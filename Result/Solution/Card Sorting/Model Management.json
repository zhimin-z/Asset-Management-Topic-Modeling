[
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":42.8373255556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy a classification TensorFlow model on AZURE from GitHub. It is getting deployed correctly which can be seen from the below logs.  <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/111656-image.png?platform=QnA\" alt=\"111656-image.png\" \/><\/p>\n<p>But I'm getting an OSError on log Stream saying saved model doesn't exist as shown below. The error msg is highlighted in red.  <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/111560-image.png?platform=QnA\" alt=\"111560-image.png\" \/><\/p>\n<p>But this is working correctly on local. This model has been checked locally.  <br \/>\nThe repository for this can be checked at <a href=\"https:\/\/github.com\/Vikeshkr-DSP\/cassava-leaf-disease-prediction\">https:\/\/github.com\/Vikeshkr-DSP\/cassava-leaf-disease-prediction<\/a>.  <br \/>\nThanks in advance for your help.<\/p>",
        "Challenge_closed_time":1625582635012,
        "Challenge_comment_count":3,
        "Challenge_created_time":1625428420640,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue while deploying a classification TensorFlow model on AZURE from GitHub. The model is getting deployed correctly, but the user is getting an OSError on log Stream saying saved model doesn't exist. The model is working correctly on local, and the repository for this can be checked at https:\/\/github.com\/Vikeshkr-DSP\/cassava-leaf-disease-prediction.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/462250\/facing-problem-in-deplying-pyhton-application-from",
        "Challenge_link_count":3,
        "Challenge_participation_count":4,
        "Challenge_readability":11.7,
        "Challenge_reading_time":12.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":42.8373255556,
        "Challenge_title":"Facing problem in deplying pyhton application from github- unable to load tensorflow saved model in AZURE.",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":105,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The above issue on <code>OSError: SavedModel file does not exist at: cassava_leaf.h5\/{saved_model.pbtxt|saved_model.pb}<\/code> was due to a bit large size of model and we did not provide an absolute path to the model location, the application could not find the same during startup.  <\/p>\n<p>The issue resolved by manually transferring the h5-model file to a location like \/home on the App Service and updated the app.py file to use an absolute path in order to refer the file.   <\/p>\n<p>Similar to: <code>model=load_model('\/home\/cassava_leaf.h5')<\/code>  <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.9,
        "Solution_reading_time":7.02,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":82.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":1.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":76.9123686111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi. \n\nIs it possible to train a machine learning model with SageMaker using a reserved instance that is already up and running instead of provisioning a new instance every time which is somewhat time consuming? I'm familiar with local mode, but I understand this is not supported when using AWS SageMaker machine learning estimators.\n\nAppreciate any suggestions for how to make the model training process in SageMaker go faster when using AWS SageMaker machine learning estimators.\n\nThanks,\nStefan",
        "Challenge_closed_time":1642148033228,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641871148701,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is looking for ways to speed up the model training process in SageMaker by using a reserved instance that is already running instead of provisioning a new instance every time. They are seeking suggestions for making the process faster when using AWS SageMaker machine learning estimators.",
        "Challenge_last_edit_time":1668610348310,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUsy3vkTMkSA2ojA1bmafDSA\/train-machine-learning-model-using-reserved-instance",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":6.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":76.9123686111,
        "Challenge_title":"Train machine learning model using reserved instance",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":354.0,
        "Challenge_word_count":84,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"As of today, it's not possible to train a machine learning model with SageMaker using a reserved instance that is already up and running instead of provisioning a new instance. The service team is currently working on it, unfortunately I don't have an ETA as to when the feature will be released.\n\nLocal Mode is supported for frameworks images (TensorFlow, MXNet, Chainer, PyTorch, and Scikit-Learn) and images you supply yourself.\n\n[Using the SageMaker Python SDK \u2014 sagemaker 2.72.3 documentation](https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode)\n\nIf you want to train Built-in algorithm models simply faster, you should check the recommendation in the SageMaker document.\n\nExample [Blazingtext-instances](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/blazingtext.html#blazingtext-instances), [Deepar-instances](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deepar.html#deepar-instances)\n\nIf the algorithm supports it, one can also try using Pipe mode or FastFile mode. These offer some fast training job startup time. [Accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker](https:\/\/aws.amazon.com\/blogs\/machine-learning\/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker\/)",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1642148033228,
        "Solution_link_count":4.0,
        "Solution_readability":18.5,
        "Solution_reading_time":16.19,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":124.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":3.9360030556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created a simply model and then registered in azure. How can I make a prediction?<\/p>\n<pre><code>from sklearn import svm\nimport joblib\nimport numpy as np\n\n# customer ages\nX_train = np.array([50, 17, 35, 23, 28, 40, 31, 29, 19, 62])\nX_train = X_train.reshape(-1, 1)\n# churn y\/n\ny_train = [&quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;]\n\nclf = svm.SVC(gamma=0.001, C=100.)\nclf.fit(X_train, y_train)\n\njoblib.dump(value=clf, filename=&quot;churn-model.pkl&quot;)\n<\/code><\/pre>\n<p>Registration:<\/p>\n<pre><code>from azureml.core import Workspace\nws = Workspace.get(name=&quot;myworkspace&quot;, subscription_id='My_subscription_id', resource_group='ML_Lingaro')\n\nfrom azureml.core.model import Model\nmodel = Model.register(workspace=ws, model_path=&quot;churn-model.pkl&quot;, model_name=&quot;churn-model-test&quot;)\n<\/code><\/pre>\n<p>Prediction:<\/p>\n<pre><code>from azureml.core.model import Model\nimport os\n\nmodel = Model(workspace=ws, name=&quot;churn-model-test&quot;)\nX_test = np.array([50, 17, 35, 23, 28, 40, 31, 29, 19, 62])\nmodel.predict(X_test) ???? \n<\/code><\/pre>\n<p>Error: <code>AttributeError: 'Model' object has no attribute 'predict'<\/code><\/p>",
        "Challenge_closed_time":1609737626627,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609722575690,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created and registered a model in Azure, but is facing an error while trying to make a prediction using the registered model. The error message states that the 'Model' object has no attribute 'predict'.",
        "Challenge_last_edit_time":1609723457016,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65556574",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":17.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":4.1808158334,
        "Challenge_title":"How to make prediction after model registration in azure?",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":540.0,
        "Challenge_word_count":120,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1605834001336,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":73.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>great question -- I also had the same misconception starting out. The missing piece is that there's a difference between model 'registration' and model 'deployment'. Registration is simply for tracking and for easy downloading at a later place and time. Deployment is what you're after, making a model available to be scored against.<\/p>\n<p>There's a <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python&amp;WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">whole section in the docs about deployment<\/a>. My suggestion would be to deploy it locally first for testing.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.8,
        "Solution_reading_time":8.06,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":75.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1619177157943,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":835.0,
        "Answerer_view_count":516.0,
        "Challenge_adjusted_solved_time":396.3547583334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Why <code> kfp.v2.dsl.Output<\/code> as function argument works without being provided?<\/p>\n<p>I am following <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/training-data-analyst\/blob\/master\/courses\/machine_learning\/deepdive2\/machine_learning_in_the_enterprise\/solutions\/create_run_vertex_pipeline.ipynb\" rel=\"nofollow noreferrer\">Create and run ML pipelines with Vertex Pipelines!<\/a> Jupyter notebook example from GCP.<\/p>\n<p>The function <code>classif_model_eval_metrics<\/code> takes <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> which have no default values.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],                # No default value set, hence must be mandatory\n    metricsc: Output[ClassificationMetrics], # No default value set, hence must be mandatory\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]): \n    # Full code at the bottom.\n<\/code><\/pre>\n<p>Hence those arguments should be mandatory, but the function is called without those arguments.<\/p>\n<pre><code>    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p>The entire pipeline code is below.<\/p>\n<pre><code>@kfp.dsl.pipeline(name=&quot;automl-tab-beans-training-v2&quot;,\n                  pipeline_root=PIPELINE_ROOT)\ndef pipeline(\n    bq_source: str = &quot;bq:\/\/aju-dev-demos.beans.beans1&quot;,\n    display_name: str = DISPLAY_NAME,\n    project: str = PROJECT_ID,\n    gcp_region: str = &quot;us-central1&quot;,\n    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str = '{&quot;auRoc&quot;: 0.95}',\n):\n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project, display_name=display_name, bq_source=bq_source\n    )\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        column_transformations=COLUMNS,\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],\n        target_column=&quot;Class&quot;,\n    )\n    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Why does it work and what are <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> of type <code>kfp.v2.dsl.Output<\/code>?<\/p>\n<hr \/>\n<h2>classif_model_eval_metrics function code<\/h2>\n<pre><code>from kfp.v2.dsl import (\n    Dataset, Model, Output, Input, \n    OutputPath, ClassificationMetrics, Metrics, component\n)\n\n@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]):  # Return parameter.\n    &quot;&quot;&quot;Renders evaluation metrics for an AutoML Tabular classification model.\n    Retrieves the classification model evaluation and render the ROC and confusion matrix\n    for the model. Determine whether the model is sufficiently accurate to deploy.\n    &quot;&quot;&quot;\n    import json\n    import logging\n    from google.cloud import aiplatform\n\n    # Fetch model eval info\n    def get_eval_info(client, model_name):\n        from google.protobuf.json_format import MessageToDict\n        response = client.list_model_evaluations(parent=model_name)\n        metrics_list = []\n        metrics_string_list = []\n        for evaluation in response:\n            metrics = MessageToDict(evaluation._pb.metrics)\n            metrics_str = json.dumps(metrics)\n            metrics_list.append(metrics)\n            metrics_string_list.append(metrics_str)\n        return (\n            evaluation.name,\n            metrics_list,\n            metrics_string_list,\n        )\n\n    def classification_thresholds_check(metrics_dict, thresholds_dict):\n        for k, v in thresholds_dict.items():\n            if k in [&quot;auRoc&quot;, &quot;auPrc&quot;]:  # higher is better\n                if metrics_dict[k] &lt; v:  # if under threshold, don't deploy\n                    return False\n        return True\n\n    def log_metrics(metrics_list, metricsc):\n        test_confusion_matrix = metrics_list[0][&quot;confusionMatrix&quot;]\n        logging.info(&quot;rows: %s&quot;, test_confusion_matrix[&quot;rows&quot;])\n        # log the ROC curve\n        fpr = [], tpr = [], thresholds = []\n        for item in metrics_list[0][&quot;confidenceMetrics&quot;]:\n            fpr.append(item.get(&quot;falsePositiveRate&quot;, 0.0))\n            tpr.append(item.get(&quot;recall&quot;, 0.0))\n            thresholds.append(item.get(&quot;confidenceThreshold&quot;, 0.0))\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        # log the confusion matrix\n        annotations = []\n        for item in test_confusion_matrix[&quot;annotationSpecs&quot;]:\n            annotations.append(item[&quot;displayName&quot;])\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n        # log textual metrics info as well\n        for metric in metrics_list[0].keys():\n            if metric != &quot;confidenceMetrics&quot;:\n                val_string = json.dumps(metrics_list[0][metric])\n                metrics.log_metric(metric, val_string)\n        # metrics.metadata[&quot;model_type&quot;] = &quot;AutoML Tabular classification&quot;\n\n    aiplatform.init(project=project)\n\n    client = aiplatform.gapic.ModelServiceClient(client_options={&quot;api_endpoint&quot;: api_endpoint})\n    eval_name, metrics_list, metrics_str_list = get_eval_info(\n        client, model.uri.replace(&quot;aiplatform:\/\/v1\/&quot;, &quot;&quot;)\n    )\n    log_metrics(metrics_list, metricsc)\n    thresholds_dict = json.loads(thresholds_dict_str)\n\n    return (&quot;true&quot;,) if classification_thresholds_check(metrics_list[0], thresholds_dict) else (&quot;false&quot;, )\n<\/code><\/pre>",
        "Challenge_closed_time":1652802212843,
        "Challenge_comment_count":5,
        "Challenge_created_time":1651375335713,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is following a GCP Jupyter notebook example and has encountered an issue where the function 'classif_model_eval_metrics' takes 'metrics: Output[Metrics]' and 'metricsc: Output[ClassificationMetrics]' as mandatory arguments, but the function is called without those arguments. The user is confused about why it works and what 'metrics: Output[Metrics]' and 'metricsc: Output[ClassificationMetrics]' of type 'kfp.v2.dsl.Output' are.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72073763",
        "Challenge_link_count":3,
        "Challenge_participation_count":6,
        "Challenge_readability":23.4,
        "Challenge_reading_time":91.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":50,
        "Challenge_solved_time":396.3547583334,
        "Challenge_title":"GCP Vertex Pipeline - Why kfp.v2.dsl.Output as function arguments work without being provided?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":358.0,
        "Challenge_word_count":467,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416648155470,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":14749.0,
        "Poster_view_count":968.0,
        "Solution_body":"<p>The custom component is defined as a Python function with a <code>@kfp.v2.dsl.component<\/code> decorator.<\/p>\n<p>The <code>@component<\/code> decorator specifies three optional arguments: the base container image to use; any packages to install; and the yaml file to which to write the component specification.<\/p>\n<p>The component function, <code>classif_model_eval_metrics<\/code>, has some input parameters.  The model parameter is an input <code>kfp.v2.dsl.Model artifact<\/code>.<\/p>\n<p>The two function args, <code>metrics<\/code> and <code>metricsc<\/code>, are component Outputs, in this case of types Metrics and ClassificationMetrics. They\u2019re not explicitly passed as inputs to the component step, but rather are automatically instantiated and can be used in the component.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-3:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;,\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,  # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n)\n<\/code><\/pre>\n<p>For example, in the function below, we\u2019re calling <code>metricsc.log_roc_curve()<\/code> and <code>metricsc.log_confusion_matrix()<\/code> to render these visualizations in the Pipelines UI. These Output params become component outputs when the component is compiled, and can be consumed by other pipeline steps.<\/p>\n<pre><code>def log_metrics(metrics_list, metricsc):\n        ...\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        ...\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n<\/code><\/pre>\n<p>For more information you can refer to this <a href=\"https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/use-vertex-pipelines-build-automl-classification-end-end-workflow\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":19.5,
        "Solution_reading_time":27.41,
        "Solution_score_count":2.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":180.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":1.8544463889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I had to stop training in the middle, which set the Trains status to <code>Aborted<\/code>.\nLater I continued it from the last checkpoint, but the status remained <code>Aborted<\/code>.\nFurthermore, automatic training metrics stopped appearing in the dashboard (though custom metrics still do).<\/p>\n<p>Can I reset the status back to <code>Running<\/code> and make Trains log training stats again?<\/p>\n<p><strong>Edit:<\/strong> When continuing training, I retrieved the task using <code>Task.get_task()<\/code> and not <code>Task.init()<\/code>. Maybe that's why training stats are not updated anymore?<\/p>\n<p><strong>Edit2:<\/strong> I also tried <code>Task.init(reuse_last_task_id=original_task_id_string)<\/code>, but it just creates a new task, and doesn't reuse the given task ID.<\/p>",
        "Challenge_closed_time":1593515579540,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593508903533,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user had to stop training in the middle, which set the Trains status to 'Aborted'. Later, when the user continued training from the last checkpoint, the status remained 'Aborted'. Additionally, automatic training metrics stopped appearing in the dashboard, though custom metrics still do. The user is looking for a way to reset the status back to 'Running' and make Trains log training stats again. The user tried using Task.get_task() and Task.init() but was unsuccessful.",
        "Challenge_last_edit_time":1609467668432,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62654203",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":11.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1.8544463889,
        "Challenge_title":"Trains: Can I reset the status of a task? (from 'Aborted' back to 'Running')",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":228.0,
        "Challenge_word_count":111,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1311330349880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tel Aviv",
        "Poster_reputation_count":3784.0,
        "Poster_view_count":342.0,
        "Solution_body":"<p>Disclaimer: I'm a member of Allegro Trains team<\/p>\n<blockquote>\n<p>When continuing training, I retrieved the task using Task.get_task() and not Task.init(). Maybe that's why training stats are not updated anymore?<\/p>\n<\/blockquote>\n<p>Yes that's the only way to continue the same exact Task.\nYou can also mark it as started with <code>task.mark_started()<\/code> , that said the automatic logging will not kick in, as <code>Task.get_task<\/code> is usually used for accessing previously executed tasks and not continuing it (if you think the continue use case is important please feel free to open a GitHub issue, I can definitely see the value there)<\/p>\n<p>You can also do something a bit different, and justcreate a new Task continuing from the last iteration the previous run ended. Notice that if you load the weights file (PyTorch\/TF\/Keras\/JobLib) it will automatically connect it with the model that was created in the previous run (assuming the model was stored is the same location, or if you have the model on https\/S3\/Gs\/Azure and you are using <code>trains.StorageManager.get_local_copy()<\/code>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>previous_run = Task.get_task()\ntask = Task.init('examples', 'continue training')\ntask.set_initial_iteration(previous_run.get_last_iteration())\ntorch.load('\/tmp\/my_previous_weights')\n<\/code><\/pre>\n<p>BTW:<\/p>\n<blockquote>\n<p>I also tried Task.init(reuse_last_task_id=original_task_id_string), but it just creates a new task, and doesn't reuse the given task ID.<\/p>\n<\/blockquote>\n<p>This is a great idea for an interface to continue a previous run, feel free to add it as GitHub issue.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1593557456892,
        "Solution_link_count":0.0,
        "Solution_readability":10.6,
        "Solution_reading_time":21.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":219.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":37.0426969444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am very new to SageMaker. Upon my first interaction, it looks like the AWS SageMaker requires you to start from its Notebook. I have a training set which is ready. Is there a way to bypass setting the Notebook and just to start by upload the training set? Or it should be done through the Notebook. If anyone knows some example fitting my need above, that will be great. <\/p>",
        "Challenge_closed_time":1520631374252,
        "Challenge_comment_count":0,
        "Challenge_created_time":1520498020543,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is new to AWS SageMaker and wants to know if there is a way to upload a training set without starting from the Notebook. They are seeking examples that fit their needs.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49168673",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.4,
        "Challenge_reading_time":5.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":37.0426969444,
        "Challenge_title":"How to load a training set in AWS SageMaker to build a model?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":700.0,
        "Challenge_word_count":83,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1305269513436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":10317.0,
        "Poster_view_count":595.0,
        "Solution_body":"<p>Amazon SageMaker is a combination of multiple services that each is independent of the others. You can use the notebook instances if you want to develop your models in the familiar Jupyter environment. But if just need to train a model, you can use the training jobs without opening a notebook instance. <\/p>\n\n<p>There a few ways to launch a training job:<\/p>\n\n<ul>\n<li>Use the high-level SDK for Python that is similar to the way that you start a training step in your python code<\/li>\n<\/ul>\n\n<p><code>kmeans.fit(kmeans.record_set(train_set[0]))<\/code><\/p>\n\n<p>Here is the link to the python library: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk<\/a><\/p>\n\n<ul>\n<li>Use the low-level API to Create-Training-Job, and you can do that using various SDK (Java, Python, JavaScript, C#...) or the CLI. <\/li>\n<\/ul>\n\n<p><code>sagemaker = boto3.client('sagemaker')\n sagemaker.create_training_job(**create_training_params)<\/code><\/p>\n\n<p>Here is a link to the documentation on these options: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html<\/a> <\/p>\n\n<ul>\n<li>Use Spark interface to launch it using a similar interface to creating an MLLib training job<\/li>\n<\/ul>\n\n<p><code>val estimator = new KMeansSageMakerEstimator(\n  sagemakerRole = IAMRole(roleArn),\n  trainingInstanceType = \"ml.p2.xlarge\",\n  trainingInstanceCount = 1,\n  endpointInstanceType = \"ml.c4.xlarge\",\n  endpointInitialInstanceCount = 1)\n  .setK(10).setFeatureDim(784)<\/code><\/p>\n\n<p><code>val model = estimator.fit(trainingData)<\/code><\/p>\n\n<p>Here is a link to the spark-sagemaker library: <a href=\"https:\/\/github.com\/aws\/sagemaker-spark\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-spark<\/a><\/p>\n\n<ul>\n<li>Create a training job in the Amazon SageMaker console using the wizard there: <a href=\"https:\/\/console.aws.amazon.com\/sagemaker\/home?region=us-east-1#\/jobs\" rel=\"nofollow noreferrer\">https:\/\/console.aws.amazon.com\/sagemaker\/home?region=us-east-1#\/jobs<\/a><\/li>\n<\/ul>\n\n<p>Please note that there a few options also to train models, either using the built-in algorithms such as K-Means, Linear Learner or XGBoost (see here for the complete list: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html<\/a>). But you can also bring your own models for pre-baked Docker images such as TensorFlow (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/tf.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/tf.html<\/a>) or MXNet (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet.html<\/a>), your own Docker image (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo.html<\/a>).  <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":16.0,
        "Solution_readability":16.7,
        "Solution_reading_time":42.09,
        "Solution_score_count":1.0,
        "Solution_sentence_count":27.0,
        "Solution_word_count":275.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":12.2807027778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Let's say I have successfully trained a model on some training data for 10 epochs. How can I then access the very same model and train for a further 10 epochs?<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-checkpoints.html\" rel=\"nofollow noreferrer\">In the docs<\/a> it suggests &quot;you need to specify a checkpoint output path through hyperparameters&quot; --&gt; how?<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># define my estimator the standard way\nhuggingface_estimator = HuggingFace(\n    entry_point='train.py',\n    source_dir='.\/scripts',\n    instance_type='ml.p3.2xlarge',\n    instance_count=1,\n    role=role,\n    transformers_version='4.10',\n    pytorch_version='1.9',\n    py_version='py38',\n    hyperparameters = hyperparameters,\n    metric_definitions=metric_definitions\n)\n\n# train the model\nhuggingface_estimator.fit(\n    {'train': training_input_path, 'test': test_input_path}\n)\n<\/code><\/pre>\n<p>If I run <code>huggingface_estimator.fit<\/code> again it will just start the whole thing over again and overwrite my previous training.<\/p>",
        "Challenge_closed_time":1649831911830,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649787701300,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to know how to access a previously trained model and train it for further epochs without re-initializing it in Sagemaker and Huggingface. The user has referred to the documentation that suggests specifying a checkpoint output path through hyperparameters but is unsure how to do so. The user has also shared a code snippet and mentioned that running the fit function again will overwrite the previous training.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71847442",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.7,
        "Challenge_reading_time":14.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":12.2807027778,
        "Challenge_title":"Train an already trained model in Sagemaker and Huggingface without re-initialising",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":110.0,
        "Challenge_word_count":111,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1437078651387,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":901.0,
        "Poster_view_count":145.0,
        "Solution_body":"<p>You can find the relevant checkpoint save\/load code in <a href=\"https:\/\/github.com\/huggingface\/notebooks\/blob\/main\/sagemaker\/05_spot_instances\/sagemaker-notebook.ipynb\" rel=\"nofollow noreferrer\">Spot Instances - Amazon SageMaker x Hugging Face Transformers<\/a>.<br \/>\n(The example enables Spot instances, but you can use on-demand).<\/p>\n<ol>\n<li>In hyperparameters you set: <code>'output_dir':'\/opt\/ml\/checkpoints'<\/code>.<\/li>\n<li>You define a <code>checkpoint_s3_uri<\/code> in the Estimator (which is unique to the series of jobs you'll run).<\/li>\n<li>You add code for train.py to support checkpointing:<\/li>\n<\/ol>\n<blockquote>\n<pre><code>from transformers.trainer_utils import get_last_checkpoint\n\n# check if checkpoint existing if so continue training\nif get_last_checkpoint(args.output_dir) is not None:\n    logger.info(&quot;***** continue training *****&quot;)\n    last_checkpoint = get_last_checkpoint(args.output_dir)\n    trainer.train(resume_from_checkpoint=last_checkpoint)\nelse:\n    trainer.train()\n<\/code><\/pre>\n<\/blockquote>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.2,
        "Solution_reading_time":13.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":91.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":32.0736591667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to use the tensorflow hub to retrain existing models, however tensorflow supports the hub library only on their 2.2 version. And The Estimator azure presents supports tf 2.0.  <\/p>\n<p>When I list tensorflow 2.2 as a required dependency as a pip package, during docker image creation the system fails - it seems like horovod is responsible, - that it cannot find the correct libraries.   <\/p>\n<p>Is this possible to be fixed? as in either an Estimator with tf 2.2 support, or an esitmator without the horovod - as I do not need a distributed system for my solution. <\/p>",
        "Challenge_closed_time":1591454811350,
        "Challenge_comment_count":3,
        "Challenge_created_time":1591339346177,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to use TensorFlow hub to retrain existing models, but Azure ML's Estimator only supports TensorFlow 2.0. When the user tries to list TensorFlow 2.2 as a required dependency, the system fails due to horovod not finding the correct libraries. The user is looking for a solution to either have an Estimator with TensorFlow 2.2 support or an Estimator without horovod as they do not need a distributed system for their solution.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/32334\/tensorflow-2-2-0-update-for-the-tensorflow-estimat",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":8.7,
        "Challenge_reading_time":8.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":32.0736591667,
        "Challenge_title":"TensorFlow 2.2.0 update for the tensorflow estimator for Azure ML, or disable horovod?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":111,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Following the pointers from <a>@romungi-MSFT<\/a>, defining estimator with gpubase image; &quot;mcr.microsoft.com\/azureml\/base-gpu:openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04&quot; solves the problem, and Tensorflow 2.2 can be included. Tensorflow uses GPU by default when available.<\/p>\n<pre><code> estimator = Estimator(source_directory=experiment_folder,\n                       compute_target=compute_target,\n                       script_params=script_params,\n                       entry_script='rps_efn_b0.py',\n                       node_count=1,        \n                       conda_packages=['ipykernel'],\n                       pip_packages = ['azureml-sdk',\n                                       'pyarrow',\n                                       'pyspark',\n                                       'azureml-mlflow',\n                                       'joblib',\n                                       'matplotlib',\n                                       'Pillow',\n                                       'tensorflow==2.2',\n                                       'tensorflow-datasets',\n                                       'tensorflow-hub',\n                                       'azureml-defaults',\n                                       'azureml-dataprep[fuse,pandas]'],\n                       custom_docker_image='mcr.microsoft.com\/azureml\/base-gpu:openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04')\n<\/code><\/pre>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":24.9,
        "Solution_reading_time":11.33,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":50.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":167.4691666667,
        "Challenge_answer_count":0,
        "Challenge_body":"![10a2a57d-e765-4359-915e-a60163bd6ec8](https:\/\/user-images.githubusercontent.com\/58698728\/205865653-bf35fb85-19cb-4e95-958e-619d13015db0.jpg)\r\n",
        "Challenge_closed_time":1670919923000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1670317034000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to call \"dvc-cc run\" as the dvc servername and url are not found. This is due to the dvc\/config file being created with whitespaces, which dvc-cc cannot read. The user has provided additional context including the versions of dvc, faice, and dvc-cc being used.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/csia-pme\/csia-pme\/issues\/39",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":24.5,
        "Challenge_reading_time":2.53,
        "Challenge_repo_contributor_count":8.0,
        "Challenge_repo_fork_count":1.0,
        "Challenge_repo_issue_count":90.0,
        "Challenge_repo_star_count":6.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":1,
        "Challenge_solved_time":167.4691666667,
        "Challenge_title":"Resolve DVC Bad request with Minio",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":6,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1437986390372,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"M\u00fcnchen, Deutschland",
        "Answerer_reputation_count":361.0,
        "Answerer_view_count":149.0,
        "Challenge_adjusted_solved_time":0.0983611111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have an sklearn k-means model. I am training the model and saving it in a pickle file so I can deploy it later using azure ml library. The model that I am training uses a custom Feature Encoder called <strong>MultiColumnLabelEncoder<\/strong>.\nThe pipeline model is defined as follow :<\/p>\n\n<pre><code># Pipeline\nkmeans = KMeans(n_clusters=3, random_state=0)\npipe = Pipeline([\n(\"encoder\", MultiColumnLabelEncoder()),\n('k-means', kmeans),\n])\n#Training the pipeline\nmodel = pipe.fit(visitors_df)\nprediction = model.predict(visitors_df)\n#save the model in pickle\/joblib format\nfilename = 'k_means_model.pkl'\njoblib.dump(model, filename)\n<\/code><\/pre>\n\n<p>The model saving works fine. The Deployment steps are the same as the steps in this link : <\/p>\n\n<p><a href=\"https:\/\/notebooks.azure.com\/azureml\/projects\/azureml-getting-started\/html\/how-to-use-azureml\/deploy-to-cloud\/model-register-and-deploy.ipynb\" rel=\"nofollow noreferrer\">https:\/\/notebooks.azure.com\/azureml\/projects\/azureml-getting-started\/html\/how-to-use-azureml\/deploy-to-cloud\/model-register-and-deploy.ipynb<\/a><\/p>\n\n<p>However the deployment always fails with this error :<\/p>\n\n<pre><code>  File \"\/var\/azureml-server\/create_app.py\", line 3, in &lt;module&gt;\n    from app import main\n  File \"\/var\/azureml-server\/app.py\", line 27, in &lt;module&gt;\n    import main as user_main\n  File \"\/var\/azureml-app\/main.py\", line 19, in &lt;module&gt;\n    driver_module_spec.loader.exec_module(driver_module)\n  File \"\/structure\/azureml-app\/score.py\", line 22, in &lt;module&gt;\n    importlib.import_module(\"multilabelencoder\")\n  File \"\/azureml-envs\/azureml_b707e8c15a41fd316cf6c660941cf3d5\/lib\/python3.6\/importlib\/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\nModuleNotFoundError: No module named 'multilabelencoder'\n<\/code><\/pre>\n\n<p>I understand that pickle\/joblib has some problems unpickling the custom function MultiLabelEncoder. That's why I defined this class in a separate python script (which I executed also). I called this custom function in the training python script, in the deployment script and in the scoring python file (score.py). The importing in the score.py file is not successful. \nSo my question is how can I import custom python module to azure ml deployment environment ?<\/p>\n\n<p>Thank you in advance.<\/p>\n\n<p>EDIT: \nThis is my .yml file<\/p>\n\n<pre><code>name: project_environment\ndependencies:\n  # The python interpreter version.\n  # Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.2\n\n- pip:\n  - multilabelencoder==1.0.4\n  - scikit-learn\n  - azureml-defaults==1.0.74.*\n  - pandas\nchannels:\n- conda-forge\n<\/code><\/pre>",
        "Challenge_closed_time":1575635052340,
        "Challenge_comment_count":1,
        "Challenge_created_time":1575463048677,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has trained an sklearn k-means model using a custom Feature Encoder called MultiColumnLabelEncoder and saved it in a pickle file for deployment using Azure ML library. However, the deployment fails with an error due to the inability to import the custom module MultiLabelEncoder. The user has defined the class in a separate python script and called it in the training, deployment, and scoring python files. The user is seeking a solution to import the custom python module to the Azure ML deployment environment.",
        "Challenge_last_edit_time":1575634698240,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59176241",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.4,
        "Challenge_reading_time":35.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":47.7787952778,
        "Challenge_title":"import custom python module in azure ml deployment environment",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2611.0,
        "Challenge_word_count":276,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1437986390372,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"M\u00fcnchen, Deutschland",
        "Poster_reputation_count":361.0,
        "Poster_view_count":149.0,
        "Solution_body":"<p>In fact, the solution was to import my customized class <strong>MultiColumnLabelEncoder<\/strong> as a pip package (You can find it through pip install multilllabelencoder==1.0.5).\nThen I passed the pip package to the .yml file or in the InferenceConfig of the azure ml environment.\nIn the score.py file, I imported the class as follows :<\/p>\n\n<pre><code>from multilabelencoder import multilabelencoder\ndef init():\n    global model\n\n    # Call the custom encoder to be used dfor unpickling the model\n    encoder = multilabelencoder.MultiColumnLabelEncoder() \n    # Get the path where the deployed model can be found.\n    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'k_means_model_45.pkl')\n    model = joblib.load(model_path)\n<\/code><\/pre>\n\n<p>Then the deployment was successful. \nOne more important thing is I had to use the same pip package (multilabelencoder) in the training pipeline as here :<\/p>\n\n<pre><code>from multilabelencoder import multilabelencoder \npipe = Pipeline([\n    (\"encoder\", multilabelencoder.MultiColumnLabelEncoder(columns)),\n    ('k-means', kmeans),\n])\n#Training the pipeline\ntrainedModel = pipe.fit(df)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.2,
        "Solution_reading_time":14.38,
        "Solution_score_count":4.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":132.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1333391842272,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"California, United States",
        "Answerer_reputation_count":1405.0,
        "Answerer_view_count":151.0,
        "Challenge_adjusted_solved_time":10042.3480277778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to build some <strong>neural network<\/strong> models for NLP and recommendation applications. The framework I want to use is <strong>TensorFlow<\/strong>. I plan to train these models and make predictions on Amazon web services. The application will be most likely <strong>distributed computing<\/strong>.<\/p>\n\n<p>I am wondering what are the pros and cons of SageMaker and EMR for TensorFlow applications?<\/p>\n\n<p>They both have TensorFlow integrated. <\/p>",
        "Challenge_closed_time":1573664904092,
        "Challenge_comment_count":0,
        "Challenge_created_time":1537510189837,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user wants to build neural network models for NLP and recommendation applications using TensorFlow and plans to train these models and make predictions on Amazon web services. They are looking for the pros and cons of using Amazon SageMaker and Amazon EMR for TensorFlow applications, as both have TensorFlow integrated and the application will most likely involve distributed computing.",
        "Challenge_last_edit_time":1537512451192,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52437599",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.3,
        "Challenge_reading_time":7.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":10042.9761819444,
        "Challenge_title":"Pros and Cons of Amazon SageMaker VS. Amazon EMR, for deploying TensorFlow-based deep learning models?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":10776.0,
        "Challenge_word_count":78,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1341967360208,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2832.0,
        "Poster_view_count":368.0,
        "Solution_body":"<p>In general terms, they serve different purposes.<\/p>\n\n<p><a href=\"https:\/\/aws.amazon.com\/emr\/\" rel=\"noreferrer\"><strong>EMR<\/strong><\/a> is when you need to process massive amounts of data and heavily rely on Spark, Hadoop, and MapReduce (EMR = Elastic MapReduce). Essentially, if your data is in large enough volume to make use of the efficiencies of Spark, Hadoop, Hive, HDFS, HBase and Pig stack then go with EMR.<\/p>\n\n<p><strong>EMR Pros:<\/strong><\/p>\n\n<ul>\n<li>Generally, low cost compared to EC2 instances<\/li>\n<li>As the name suggests Elastic meaning you can provision what you need when you need it<\/li>\n<li>Hive, Pig, and HBase out of the box<\/li>\n<\/ul>\n\n<p><strong>EMR Cons:<\/strong><\/p>\n\n<ul>\n<li>You need a very specific use case to truly benefit from all the offerings in EMR. Most don't take advantage of its entire offering<\/li>\n<\/ul>\n\n<p><a href=\"https:\/\/aws.amazon.com\/sagemaker\/\" rel=\"noreferrer\"><strong>SageMaker<\/strong><\/a> is an attempt to make Machine Learning easier and distributed. The marketplace provides out of the box algos and models for quick use. It's a great service if you conform to the workflows it enforces. Meaning creating training jobs, deploying inference endpoints <\/p>\n\n<p><strong>SageMaker Pros:<\/strong><\/p>\n\n<ul>\n<li>Easy to get up and running with Notebooks<\/li>\n<li>Rich marketplace to quickly try existing models<\/li>\n<li>Many different example notebooks for popular algorithms<\/li>\n<li>Predefined kernels that minimize configuration<\/li>\n<li>Easy to deploy models<\/li>\n<li>Allows you to distribute inference compute by deploying endpoints<\/li>\n<\/ul>\n\n<p><strong>SageMaker Cons:<\/strong><\/p>\n\n<ul>\n<li>Expensive!<\/li>\n<li>Enforces a certain workflow making it hard to be fully custom<\/li>\n<li>Expensive!<\/li>\n<\/ul>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.6,
        "Solution_reading_time":22.46,
        "Solution_score_count":10.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":229.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.4033333333,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi, I am trying to fit an MXNet model locally. I am adapting this https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/ and doing the following:\n\n    bucket = 'XXXXXXXXXXX'\n    prefix = 'sagemaker\/cifar-bench\/data'\n    \n    inputs = sagemaker_session.upload_data(\n        path='data',\n        bucket=bucket, \n        key_prefix=prefix)\n    \n    print('data sent to ' + inputs)\n    \n    \n    Inception = MXNet('gluon_cifar_net.py', \n              role=role, \n              train_instance_count=1, \n              train_instance_type='local_gpu',\n              framework_version='1.2.1',\n              base_job_name='cifar10-inception-',\n              hyperparameters={'batch_size': 256, \n                               'optimizer': 'sgd',\n                               'epochs': 100, \n                               'learning_rate': 0.1, \n                               'momentum': 0.9})\n    \n    \n    Inception.fit(inputs)\n\nwhich returns an `OSError: [Errno 2] No such file or directory`\n\nIn the error log I can see that there seems to be error at `self.latest_training_job = _TrainingJob.start_new(self, inputs)` and `self.sagemaker_client.create_training_job(**train_request)`\n\nHow can I make the local mode work?\n",
        "Challenge_closed_time":1537550695000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1537534843000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to fit an MXNet model locally using SageMaker but is encountering an \"OSError: [Errno 2] No such file or directory\" error. The error log indicates that there is an issue with \"self.latest_training_job = _TrainingJob.start_new(self, inputs)\" and \"self.sagemaker_client.create_training_job(**train_request)\". The user is seeking help to resolve the issue and make the local mode work.",
        "Challenge_last_edit_time":1668119355544,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUQu1fDak6RL2wmivZ5UJwUw\/sagemaker-mxnet-local-mode-not-working",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":16.0,
        "Challenge_reading_time":13.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":4.4033333333,
        "Challenge_title":"SageMaker MXNet local mode not working",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":259.0,
        "Challenge_word_count":93,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"It is very likely that you don't have docker-compose (or docker) installed in the box, that is why you are getting a No such file or directory.\n\nIf you want to use the GPU setup I would recommend running on a sagemaker notebook instance. Navigate to one of the example notebooks such as: https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_cifar10\/mxnet_cifar10_local_mode.ipynb\n\nAnd run the setup.sh cell. This will install and configure all the docker dependencies correctly and then you should be able to use MXNet locally on GPU without any issue.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925587140,
        "Solution_link_count":1.0,
        "Solution_readability":12.2,
        "Solution_reading_time":7.52,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":84.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1589293508567,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":833.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":0.0,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I was trying to register a model using the <code>Run<\/code> Class like this:<\/p>\n<pre><code>model = run.register_model(\n    model_name=model_name,\n    model_path=model_path)\n<\/code><\/pre>\n<p>Errors with message: <code>Could not locate the provided model_path ... in the set of files uploaded to the run...<\/code><\/p>",
        "Challenge_closed_time":1643643837027,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643643837027,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error while trying to register a model using the Run Class in AzureML. The error message stated that the provided model_path could not be located in the uploaded files.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70928761",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.9,
        "Challenge_reading_time":4.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.0,
        "Challenge_title":"AzureML Model Register",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":319.0,
        "Challenge_word_count":38,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589293508567,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":833.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>The only way I found to fix the issue was to use the <code>Model<\/code> Class instead:<\/p>\n<pre><code>        model = Model.register(\n            workspace=ws,\n            model_name=model_name,\n            model_path=model_path,\n            model_framework=Model.Framework.SCIKITLEARN,\n            model_framework_version=sklearn.__version__,\n            description='Model Deescription',\n            tags={'Name' : 'ModelName', 'Type' : 'Production'},\n            model_framework=Model.Framework.SCIKITLEARN,\n            model_framework_version='1.0'\n            )\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":20.6,
        "Solution_reading_time":6.17,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":33.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":75.75,
        "Challenge_answer_count":3,
        "Challenge_body":"Hi,\n\u00a0\nIs there a way to download my Google AutoML Transation model and use it offline once it's trained?\n\u00a0\nAnd in what format can the model be exported?\u00a0\n\u00a0\nThank you",
        "Challenge_closed_time":1664553420000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664280720000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is looking for a way to download and use their Google AutoML Translation model offline after it has been trained. They are also inquiring about the format in which the model can be exported.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/exporting-a-google-autoML-translate-model\/m-p\/471646#M607",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":6.7,
        "Challenge_reading_time":2.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":75.75,
        "Challenge_title":"exporting a google autoML translate model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":142.0,
        "Challenge_word_count":35,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"1.- No.\n\n2.- You can create a Feature Request at\u00a0Issue Tracker\u00a0and\u00a0add a description about the feature you want(Export Translation Models), and the engineer team will look at it. You can see here how it is more likely that the team prioritize the work of the Feature Request\/Issues.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.2,
        "Solution_reading_time":3.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":55.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":3.4073972222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I try to use azuremlsdk to deploy a locally trained model (a perfectly valid use case AFIK). I follow <a href=\"https:\/\/cran.r-project.org\/web\/packages\/azuremlsdk\/vignettes\/train-and-deploy-first-model.html\" rel=\"nofollow noreferrer\">this<\/a> and managed to create a ML workspace and register a &quot;model&quot; like so:<\/p>\n<pre><code>library(azuremlsdk)\n\ninteractive_auth &lt;- interactive_login_authentication(tenant_id=&quot;xxx&quot;)\nws &lt;- get_workspace(\n        name = &quot;xxx&quot;, \n        subscription_id = &quot;xxx&quot;, \n        resource_group =&quot;xxx&quot;, \n        auth = interactive_auth\n)\n\nadd &lt;- function(a, b) {\n    return(a + b)\n}\n\nadd(1,2)\n\nsaveRDS(add, file = &quot;D:\/add.rds&quot;)\n\nmodel &lt;- register_model(\n    ws, \n    model_path = &quot;D:\/add.rds&quot;, \n    model_name = &quot;add_model&quot;,\n    description = &quot;An amazing model&quot;\n)\n<\/code><\/pre>\n<p>This seemed to work fine, as I get some nice log messages telling me that the model was registered. For my sanity, I wonder where can I find this registered (&quot;materialised&quot;) model\/object\/function in the Azure UI please?<\/p>",
        "Challenge_closed_time":1621001633740,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620989367110,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has used azuremlsdk to register a locally trained model in an ML workspace. The registration process seems to have worked fine, but the user is unable to locate the registered model in the Azure UI.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67533091",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":14.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3.4073972222,
        "Challenge_title":"where are registered models in azure machine learning",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":41.0,
        "Challenge_word_count":118,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>On ml.azure.com, there is a &quot;Models&quot; option on the left-hand blade.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Y7cZe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Y7cZe.png\" alt=\"UI Sidebar\" \/><\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.1,
        "Solution_reading_time":3.17,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":199.5710802778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi.  <\/p>\n<p>I am working on an ML model in Designer.  <\/p>\n<p>I have a dataset of c. 55,000 rows.   <\/p>\n<p>When I add an &quot;ID&quot; column (unique per row - so 55,000 IDs) to my dataset for training \/ scoring, I receive the error message:  <\/p>\n<blockquote>\n<p>ModuleExceptionMessage:ColumnUniqueValuesExceeded: Number of unique values in column: &quot;ID&quot; is greater than allowed.  <\/p>\n<\/blockquote>\n<p><strong>Question:<\/strong> is this error based on a physical cap on number of rows - or capacity based on e.g. Compute power associated with the instance?  <\/p>\n<p>I can run 20k rows through the model <em>without<\/em> the ID column - so it seems the unique rows is the challenge.  <\/p>\n<p>But then - how do I keep an identifying column in the scored dataset, if there is a cap on unique values?   <\/p>\n<p>Because I need the ID column to join with other data that is not able to be used in modelling as features etc.   <\/p>\n<p>Any guidance welcome! <\/p>",
        "Challenge_closed_time":1605654456912,
        "Challenge_comment_count":6,
        "Challenge_created_time":1604936001023,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error message in Azure ML Designer when adding an \"ID\" column to a dataset for training\/scoring. The error message states that the number of unique values in the column exceeds the allowed limit. The user is unsure if this error is due to a physical cap on the number of rows or capacity based on compute power. The user needs the ID column to join with other data that cannot be used in modeling as features.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/156534\/azure-ml-id-column-for-joining-data-returns-no-of",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":6.5,
        "Challenge_reading_time":12.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":199.5710802778,
        "Challenge_title":"Azure ML: ID column for joining data returns \"No. Of unique values ... is greater than allowed\"",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":172,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>User can use Edit Metadata module to mark the ID column as &quot;ClearFeature&quot;, and thus this will not be used in Train Model. This should prevent the error. Please have a try and let me know if there is any questions. <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/edit-metadata\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/edit-metadata<\/a>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/40390-microsoftteams-image-7.png?platform=QnA\" alt=\"40390-microsoftteams-image-7.png\" \/>    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.8,
        "Solution_reading_time":8.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":52.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1542628542703,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":11.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":43.2314219444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed a Tensorflow-Model in SageMaker Studio following this tutorial:\n<a href=\"https:\/\/aws.amazon.com\/de\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/de\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/<\/a>\nThe Model needs a Multidimensional Array as input. Invoking it from the Notebook itself is working:<\/p>\n<pre><code>import numpy as np\nimport json\ndata = np.load(&quot;testValues.npy&quot;)\npred=predictor.predict(data)\n<\/code><\/pre>\n<p>But I wasnt able to invoke it from a boto 3 client using this code:<\/p>\n<pre><code>import json\nimport boto3\nimport numpy as np\nimport io\n \nclient = boto3.client('runtime.sagemaker')\ndatain = np.load(&quot;testValues.npy&quot;)\ndata=datain.tolist();\nresponse = client.invoke_endpoint(EndpointName=endpoint_name, Body=json.dumps(data))\nresponse_body = response['Body']\nprint(response_body.read())\n<\/code><\/pre>\n<p>This throws the Error:<\/p>\n<pre><code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from model with message &quot;{&quot;error&quot;: &quot;Unsupported Media Type: Unknown&quot;}&quot;.\n<\/code><\/pre>\n<p>I guess the reason is the json Media Type but i have no clue how to get it back in shape.\nI tried this:<a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/644\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/644<\/a> but it doesnt seem to change anything<\/p>",
        "Challenge_closed_time":1605773068536,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605617435417,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has deployed a Tensorflow-Model in SageMaker Studio that requires a multidimensional array as input. While invoking it from the Notebook works, the user encountered an error when trying to invoke it from a boto3 client. The error message suggests an unsupported media type, and the user suspects it is due to the JSON media type but is unsure how to fix it. The user tried a solution from a GitHub issue but it did not work.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64875623",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":16.8,
        "Challenge_reading_time":21.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":43.2314219444,
        "Challenge_title":"Invoking an endpoint in AWS with a multidimensional array",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":550.0,
        "Challenge_word_count":147,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1542628542703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":11.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>This fixed it for me:\nThe Content Type was missing.<\/p>\n<pre><code>import json\nimport boto3\nimport numpy as np\nimport io\n\nclient = boto3.client('runtime.sagemaker',aws_access_key_id=..., aws_secret_access_key=...,region_name=...)\nendpoint_name = '...'\n\ndata = np.load(&quot;testValues.npy&quot;)\n\n\npayload = json.dumps(data.tolist())\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                                  ContentType='application\/json',\n                                   Body=payload)\nresult = json.loads(response['Body'].read().decode())\nres = result['predictions']\nprint(&quot;test&quot;)\n\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":18.7,
        "Solution_reading_time":7.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":38.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":795.3458333333,
        "Challenge_answer_count":0,
        "Challenge_body":"I got this error with Azure Machine Learning. \r\n\r\nConfigException: ConfigException:\r\n\tMessage: blacklisted and whitelisted models are exactly the same. Found: {'XGBoostClassifier'}.Please remove models from the blacklist or add models to the whitelist.\r\n\r\nThe settings are as follow. 'XGBoostClassifier' is in the whitelist; and backlist is None. Would you please help with the error?\r\n\r\nautoml_settings = {\r\n    \"iteration_timeout_minutes\": 2,\r\n    \"experiment_timeout_minutes\": 20,\r\n    \"enable_early_stopping\": True,\r\n    \"primary_metric\": 'accuracy',\r\n    \"featurization\": 'auto',\r\n    \"verbosity\": logging.INFO,\r\n    \"n_cross_validations\": 5\r\n}\r\n\r\nfrom azureml.train.automl import AutoMLConfig\r\n\r\nautoml_config = AutoMLConfig(task='classification',\r\n                             enable_tf = True,\r\n                             debug_log='automated_ml_errors.log',\r\n                             X=x_train.values,\r\n                             y=y_train.values.flatten(),\r\n                             blacklist_models = None,\r\n                             whitelist_models = ['XGBoostClassifier'],\r\n                             **automl_settings)\r\n\r\n(Note: XGBoostClassifier was installed in the notebook)",
        "Challenge_closed_time":1583863460000,
        "Challenge_comment_count":8,
        "Challenge_created_time":1581000215000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an ImportError while trying to import 'AutoMLStep' from 'azureml.train.automl'.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/767",
        "Challenge_link_count":0,
        "Challenge_participation_count":8,
        "Challenge_readability":16.2,
        "Challenge_reading_time":13.22,
        "Challenge_repo_contributor_count":58.0,
        "Challenge_repo_fork_count":2387.0,
        "Challenge_repo_issue_count":1906.0,
        "Challenge_repo_star_count":3704.0,
        "Challenge_repo_watch_count":2001.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":795.3458333333,
        "Challenge_title":"Azure Machine Learning error: Can not use 'XGBoostClassifier'",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":98,
        "Discussion_body":"Hello,\r\n\r\nWe're so sorry you've encountered this issue. I've gone ahead and filed a work item to investigate and fix the issue around whitelisting XGBoost. We will reach out again here once a fix is in.\r\n\r\nThank you,\r\nSabina It could be possible that XGBoostClassifier was blacklisted by the system. We can double check if you can share your runId. In the meanwhile, we will improve the error msg for this scenario. Thanks! @waltz2u Can you please run the following line of code in your jupyter notebook and let me know what it says? \r\n\r\n`import xgboost`\r\n\r\nThanks,\r\nSabina Hi @waltz2u, I was able to reproduce and overcome this issue by double checking that import xgboost was installed correctly by trying `import xgboost`.\r\n\r\n\r\n`pip install \"py-xgboost<=0.80\"` fixed it on my end. Can you please try that and let us know if it solved the issue?  Hi @cartacioS and @jialiu103, sorry for the late reply. Yes it works now for me. Thank you very much.\r\n\r\nCD\r\n Will now proceed to close this thread. Thanks. @cartacioS I'm facing the same error, except that I'm kicking off the AutoML run from my local machine, using a remote compute as my aml compute target. Using this issue above, and this [one](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/313), it seems that I would still need to add xgboost to my env (locally) although technically I won't be using that package in my AutoML exercise? @jadhosn If you do not require XGBoost for your training, you can simply ignore this warning. But if you want XGBoost to be a potential recommended model, then yes you will need to add XGBoost to your local environment regardless of local\/remote compute.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":27.2427933334,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I think it would be beneficial to select and delete several experiments at the same time.<br>\nNow I have to delete one by one and it is very time consuming.<\/p>\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1652874850582,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652776776526,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is requesting a feature to be able to select and delete multiple experiments at once, as deleting them one by one is time-consuming.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/remove-multiple-runs-at-the-same-time\/2435",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":4.8,
        "Challenge_reading_time":2.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":27.2427933334,
        "Challenge_title":"Remove multiple runs at the same time",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":249.0,
        "Challenge_word_count":39,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey <a class=\"mention\" href=\"\/u\/lucasventura\">@lucasventura<\/a>, you can do it like <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/runs-table#filter-and-delete-unwanted-runs\">this<\/a>.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":30.0,
        "Solution_reading_time":2.67,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":0.1789469444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In my setup, I run a script that <strong>trains<\/strong> a model and starts generating checkpoints. Another script watches for new checkpoints and <strong>evaluates<\/strong> them. The scripts run in parallel, so evaluation is just a step behind training.<\/p>\n\n<p>What's the right Tracks configuration to support this scenario?<\/p>",
        "Challenge_closed_time":1591906575912,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591905931703,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for the appropriate Trains configuration to track separate train\/test processes where a script trains a model and generates checkpoints while another script evaluates them in parallel.",
        "Challenge_last_edit_time":1609771005756,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62332672",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.4,
        "Challenge_reading_time":4.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.1789469444,
        "Challenge_title":"Tracking separate train\/test processes with Trains",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":99.0,
        "Challenge_word_count":51,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1311330349880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tel Aviv",
        "Poster_reputation_count":3784.0,
        "Poster_view_count":342.0,
        "Solution_body":"<p>disclaimer: I'm part of the <a href=\"https:\/\/github.com\/allegroai\/trains\/\" rel=\"nofollow noreferrer\">allegro.ai Trains<\/a> team<\/p>\n<p>Do you have two experiments? one for testing one for training ?<\/p>\n<p>If you do have two experiments, then I would make sure the models are logged in both of them (which if they are stored on the same shared-folder\/s3\/etc will be automatic)\nThen you can quickly see the performance of each-one.<\/p>\n<p>Another option is sharing the same experiment, then the second process adds reports to the original experiment, that means that somehow you have to pass to it the experiment id.\nThen you can do:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>task = Task.get_task(task_id='training_task_id`)\ntask.get_logger().report_scalar('title', 'loss', value=0.4, iteration=1)\n<\/code><\/pre>\n<p>EDIT:\nAre the two processes always launched together, or is the checkpoint test a general purpose code ?<\/p>\n<p>EDIT2:<\/p>\n<p>Let's assume you have main script training a model. This experiment has a unique task ID:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>my_uid = Task.current_task().id\n<\/code><\/pre>\n<p>Let's also assume you have a way to pass it to your second process (If this is an actual sub-process, it inherits the os environment variables so you could do <code>os.environ['MY_TASK_ID']=my_uid<\/code>)<\/p>\n<p>Then in the evaluation script you could report directly into the main training Task like so:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>train_task = Task.get_task(task_id=os.environ['MY_TASK_ID'])\ntrain_task.get_logger().report_scalar('title', 'loss', value=0.4, iteration=1)\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1592833417200,
        "Solution_link_count":1.0,
        "Solution_readability":10.9,
        "Solution_reading_time":21.3,
        "Solution_score_count":1.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":202.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":45.3307377778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have 3 main process to perform using Amazon SageMaker.<\/p>\n\n<ol>\n<li>Using own training python script, (not using sagemaker container, inbuilt algorithm) [Train.py]<\/li>\n<\/ol>\n\n<p>-> For this, I have referred to this link:<br>\n<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/train-and-host-scikit-learn-models-in-amazon-sagemaker-by-building-a-scikit-docker-container\/\" rel=\"nofollow noreferrer\">Bring own algorithm to AWS sagemaker<\/a>\nand it seems that we can bring our own training script to sagemaker managed training setup, and model artifacts can be uploaded to s3 etc.\nNote: I am using Light GBM model for training.<\/p>\n\n<ol start=\"2\">\n<li>Writing forecast to AWS RDS DB:<\/li>\n<\/ol>\n\n<p>-> There is no need to deploy model and create endpoint, because training will happen everyday, and will create forecast as soon as training completes. (Need to generate forecast in train.py itself)<\/p>\n\n<p>-> <strong>Challenge is how can I write forecast in AWS RDS DB from train.py script. (Given that script is running in Private VPC)<\/strong><\/p>\n\n<ol start=\"3\">\n<li>Scheduling this process as daily job:<\/li>\n<\/ol>\n\n<p>--> I have gone through AWS step functions and seems to be the way to trigger daily training and write forecast to RDS.<\/p>\n\n<p>--> <strong>Challenge is how to use step function for time based trigger and not event based.<\/strong><\/p>\n\n<p>Any suggestions on how to do this? Any best practices to follow? Thank you in advance.<\/p>",
        "Challenge_closed_time":1566713256896,
        "Challenge_comment_count":0,
        "Challenge_created_time":1566547602673,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in performing three main processes using Amazon SageMaker. The first process involves using their own training python script, not using the sagemaker container, and uploading model artifacts to S3. The second process involves writing forecast to AWS RDS DB from the train.py script running in a private VPC. The third process involves scheduling this process as a daily job using AWS step functions for time-based trigger instead of event-based. The user is seeking suggestions and best practices to overcome these challenges.",
        "Challenge_last_edit_time":1566550066240,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57622122",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":19.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":46.0150619444,
        "Challenge_title":"Custom sagemaker container for training, write forecast to AWS RDS, on a daily basis",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":466.0,
        "Challenge_word_count":217,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1469183608840,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Gurugram, Haryana, India",
        "Poster_reputation_count":624.0,
        "Poster_view_count":67.0,
        "Solution_body":"<p>The way to trigger Step Functions on schedule is by using CloudWatch Events (sort of cron). Check out this tutorial: <a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/tutorial-cloudwatch-events-target.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/tutorial-cloudwatch-events-target.html<\/a><\/p>\n\n<p>Don't write to the RDS from your Python code! It is better to write the output to S3 and then \"copy\" the files from S3 into the RDS. Decoupling these batches will make a more reliable and scalable process. You can trigger the bulk copy into the RDS when the files are written to S3 or to a later time when your DB is not too busy. <\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.6,
        "Solution_reading_time":8.74,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":92.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":6.1298294445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there a way to deploy a xgboost model trained locally using amazon sagemaker? I only saw tutorial talking about both training and deploying model with amazon sagemaker.\nThanks.<\/p>",
        "Challenge_closed_time":1531778753543,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531756686157,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to deploy a locally trained xgboost model on Amazon SageMaker, as they have only found tutorials that cover both training and deployment on SageMaker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51365850",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":2.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":6.1298294445,
        "Challenge_title":"how to deploy a xgboost model on amazon sagemaker?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1641.0,
        "Challenge_word_count":37,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1530193663836,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>This <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/d5681a07611ae29567355b60b2f22500b561218b\/advanced_functionality\/xgboost_bring_your_own_model\/xgboost_bring_your_own_model.ipynb\" rel=\"nofollow noreferrer\">example notebook<\/a> is good starting point showing how to use a pre-existing scikit-learn xgboost model with the Amazon SageMaker to create a hosted endpoint for that model.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":21.1,
        "Solution_reading_time":5.55,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.9119444444,
        "Challenge_answer_count":0,
        "Challenge_body":"Logging using Weights and Biases does not differentiate between training and testing modes in `logbook.write_metric_log({  'mode': 'train' ... })`",
        "Challenge_closed_time":1583456108000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1583449225000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the writing of checkpoints to wandb during training. Despite starting a non-dry run via a notebook, no checkpoints are being uploaded to wandb. The expected behavior is for checkpoints to be uploaded whenever there is a better one available during training. The user is struggling to understand the symlinking model of wandb, which requires checkpoints to be under the project root, making it difficult to run multiple experiments simultaneously.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/shagunsodhani\/ml-logger\/issues\/25",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":2.73,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":88.0,
        "Challenge_repo_star_count":17.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1.9119444444,
        "Challenge_title":"[BUG] Weights & Biases logging does not differentiate between modes",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":25,
        "Discussion_body":"@koustuvsinha Thanks for bringing this up. Could you try the new version?\r\n\r\nWhen constructing the logbook, pass an additional parameter:\r\n\r\n```\r\nfrom ml_logger import logbook as ml_logbook\r\nlogbook_config = ml_logbook.make_config(\r\n    logger_file_path = <path to write logs>,\r\n    wandb_config = <wandb config or None>,\r\n    wandb_prefix_key = \"mode\",\r\n)\r\n```",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":30.8515688889,
        "Challenge_answer_count":1,
        "Challenge_body":"I want to train and build the model in Sagemaker studio and then be able to export the model as a container image to ECR, so I can use the model in external platform by sharing the ECR image to another account where I Can create container with the image  from ECR",
        "Challenge_closed_time":1663369533112,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663258467464,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to export a trained model from Sagemaker studio as a container image to ECR, so that it can be used in an external platform by sharing the ECR image to another account.",
        "Challenge_last_edit_time":1667926271895,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUZHWz5-hpSc-80dEIkuxwQw\/how-to-export-tresained-models-to-ecr-as-container-image",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.0,
        "Challenge_reading_time":3.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":30.8515688889,
        "Challenge_title":"How to export tresained models to ECR as container image",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":72.0,
        "Challenge_word_count":61,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The models you train in SageMaker are stored in S3 as .tar.gz files that you can use to deploy to an endpoint, or even test locally (extracting the model file from the tar file). \nIf you are using a built-in algorithm, you can share the .tar.gz file to the second account and deploy the model in the second account, since built-in algorithm containers can be accessed from any AWS account. \n\nIf you are using a custom training image ([docs here](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/adapt-training-container.html)), you can push this image to ECR and allow a [second account to pull the image](https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/secondary-account-access-ecr\/) and then use the image with the model that you have trained. However, note that Studio at this time does not support building Docker images out of the box. You can use [SageMaker Notebook Instances](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi.html) instead.\n\nI would recommend keeping the model (.tar.gz) and the image (Docker) separate, since you can easily retrain and deploy the newer versions of models without updating the image every single time.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1663369533112,
        "Solution_link_count":3.0,
        "Solution_readability":10.2,
        "Solution_reading_time":14.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":163.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1341441916656,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5985.0,
        "Answerer_view_count":161.0,
        "Challenge_adjusted_solved_time":4157.9736122222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have two separate normalized text files that I want to train my BlazingText model on.<\/p>\n\n<p>I am struggling to get this to work and the documentation is not helping.<\/p>\n\n<p>Basically I need to figure out how to supply multiple files or S3 prefixes as \"inputs\" parameter to the sagemaker.estimator.Estimator.fit() method.<\/p>\n\n<p>I first tried:<\/p>\n\n<pre><code>s3_train_data1 = 's3:\/\/{}\/{}'.format(bucket, prefix1)\ns3_train_data2 = 's3:\/\/{}\/{}'.format(bucket, prefix2)\n\ntrain_data1 = sagemaker.session.s3_input(s3_train_data1, distribution='FullyReplicated', content_type='text\/plain', s3_data_type='S3Prefix')\n\ntrain_data2 = sagemaker.session.s3_input(s3_train_data2, distribution='FullyReplicated', content_type='text\/plain', s3_data_type='S3Prefix')\n\nbt_model.fit(inputs={'train1': train_data1, 'train2': train_data2}, logs=True)\n<\/code><\/pre>\n\n<p>this doesn't work because SageMaker is looking for the key specifically to be \"train\" in the inputs parameter.<\/p>\n\n<p>So then i tried:<\/p>\n\n<pre><code>bt_model.fit(inputs={'train': train_data1, 'train': train_data2}, logs=True)\n<\/code><\/pre>\n\n<p>This trains the model only on the second dataset and ignores the first one completely.<\/p>\n\n<p>Now finally I tried using a Manifest file using the documentation here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_S3DataSource.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_S3DataSource.html<\/a><\/p>\n\n<p>(see manifest file format under \"S3Uri\" section)<\/p>\n\n<p>the documentation says the manifest file format is a JSON that looks like this example:<\/p>\n\n<pre><code>[\n\n{\"prefix\": \"s3:\/\/customer_bucket\/some\/prefix\/\"},\n\n\"relative\/path\/to\/custdata-1\",\n\n\"relative\/path\/custdata-2\"\n\n]\n<\/code><\/pre>\n\n<p>Well, I don't think this is valid JSON in the first place but what do I know, I still give it a try.<\/p>\n\n<p>When I try this:<\/p>\n\n<pre><code>s3_train_data_manifest = 'https:\/\/s3.us-east-2.amazonaws.com\/bucketpath\/myfilename.manifest'\n\ntrain_data_merged = sagemaker.session.s3_input(s3_train_data_manifest, distribution='FullyReplicated', content_type='text\/plain', s3_data_type='ManifestFile')\n\ndata_channel_merged = {'train': train_data_merged}\n\nbt_model.fit(inputs=data_channel_merged, logs=True)\n<\/code><\/pre>\n\n<p>I get an error saying:<\/p>\n\n<pre><code>ValueError: Error training blazingtext-2018-10-17-XX-XX-XX-XXX: Failed Reason: ClientError: Data download failed:Unable to parse manifest at s3:\/\/mybucketpath\/myfilename.manifest - invalid format\n<\/code><\/pre>\n\n<p>I tried replacing square brackets in my manifest file with curly braces ...but still I feel the JSON file format seems to be missing something that documentation fails to describe correctly?<\/p>",
        "Challenge_closed_time":1540883711836,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539786419823,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble training a SageMaker BlazingText model using multiple channels. They have tried supplying multiple files or S3 prefixes as \"inputs\" parameter to the sagemaker.estimator.Estimator.fit() method, but it did not work. They also tried using a Manifest file, but it resulted in an error saying \"Unable to parse manifest - invalid format\". The user is unsure if the JSON file format is missing something that the documentation fails to describe correctly.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52857309",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":15.3,
        "Challenge_reading_time":36.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":304.8033369444,
        "Challenge_title":"How to train SageMaker BlazingText model using multiple channels",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1053.0,
        "Challenge_word_count":265,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432179256928,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":17.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>You can certainly match multiple files with the same prefix, so your first attempt could have worked as long as you organize your files in your S3 bucket to suit. For e.g. the prefix: <code>s3:\/\/mybucket\/foo\/<\/code> will match the files <code>s3:\/\/mybucket\/foo\/bar\/data1.txt<\/code> and <code>s3:\/\/mybucket\/foo\/baz\/data2.txt<\/code><\/p>\n\n<p>However, if there is a third file in your bucket called <code>s3:\/\/mybucket\/foo\/qux\/data3.txt<\/code> that you <em>don't<\/em> want matched (while still matching the first two) there is no way to do achieve that with a single prefix. In these cases a manifest would work. So, in the above example, the manifest would simply be:<\/p>\n\n<pre><code>[\n  {\"prefix\": \"s3:\/\/mybucket\/foo\/\"},\n  \"bar\/data1.txt\",\n  \"baz\/data2.txt\"\n]\n<\/code><\/pre>\n\n<p><em>(and yes, this is valid json - it is an array whose first element is an object with an attribute called <code>prefix<\/code> and all subsequent elements are strings).<\/em><\/p>\n\n<p>Please double check your manifest (you didn't actually post it so I can't do that for you) and make sure it conforms to the above syntax.<\/p>\n\n<p>If you're still stuck please open up a thread on the AWS sagemaker forums - <a href=\"https:\/\/forums.aws.amazon.com\/forum.jspa?forumID=285\" rel=\"nofollow noreferrer\">https:\/\/forums.aws.amazon.com\/forum.jspa?forumID=285<\/a> and after you do that we can setup a PM to try and get to the bottom of this (never post your AWS account id in a public forum like StackOverflow or even in AWS forums).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1554755124827,
        "Solution_link_count":2.0,
        "Solution_readability":8.9,
        "Solution_reading_time":18.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":207.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1367741264647,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":169.0,
        "Answerer_view_count":30.0,
        "Challenge_adjusted_solved_time":1.3759711111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have used estimator for a pytorch model and have saved the artifacts in s3 bucket. using below code<\/p>\n<pre><code>estimator = PyTorch(\n    entry_point=&quot;train_deploy.py&quot;,\n    source_dir=&quot;code&quot;,\n    role=role,\n    framework_version=&quot;1.3.1&quot;,\n    py_version=&quot;py3&quot;,\n    instance_count=1,  # this script only support distributed training for GPU instances.\n    instance_type=&quot;ml.m5.12xlarge&quot;,\n    output_path=output_path,\n    hyperparameters={\n        &quot;epochs&quot;: 1,\n        &quot;num_labels&quot;: 7,\n        &quot;backend&quot;: &quot;gloo&quot;,\n    },\n    disable_profiler=False, # disable debugger\n)\nestimator.fit({&quot;training&quot;: inputs_train, &quot;testing&quot;: inputs_test})\n<\/code><\/pre>\n<p>The model works well and there are no issues with it. However i would like to re use this model later for inference, how do i do that. i am looking for something like below<\/p>\n<pre><code>estimator = PyTorch.load(input_path = &quot;&lt;xyz&gt;&quot;)\n<\/code><\/pre>",
        "Challenge_closed_time":1659511740763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659506787267,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has successfully used an estimator for a PyTorch model and saved the artifacts in an S3 bucket. However, they are now facing a challenge in reusing the model for inference and are looking for a way to load the estimator from the saved artifacts in the S3 bucket.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73216926",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":13.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":1.3759711111,
        "Challenge_title":"Load estimator from model artifact in s3 bucket aws",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":28.0,
        "Challenge_word_count":100,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1367741264647,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":169.0,
        "Poster_view_count":30.0,
        "Solution_body":"<p>I was able to solve this by the following steps<\/p>\n<pre><code>model_data=output_path\nfrom sagemaker.pytorch.model import PyTorchModel \n\npytorch_model = PyTorchModel(model_data=model_data,\n                             role=role,\n                             framework_version=&quot;1.3.1&quot;,\n                             source_dir=&quot;code&quot;,\n                             py_version=&quot;py3&quot;,\n                             entry_point=&quot;train_deploy.py&quot;)\n\npredictor = pytorch_model.deploy(initial_instance_count=1, instance_type=&quot;ml.m4.2xlarge&quot;)\npredictor.serializer = sagemaker.serializers.JSONSerializer()\npredictor.deserializer = sagemaker.deserializers.JSONDeserializer()\nresult = predictor.predict(&quot;&lt;text that needs to be predicted&gt;&quot;)\nprint(&quot;predicted class: &quot;, np.argmax(result, axis=1))\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":31.6,
        "Solution_reading_time":9.95,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":42.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1421803794992,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"St. Louis, MO, USA",
        "Answerer_reputation_count":373.0,
        "Answerer_view_count":56.0,
        "Challenge_adjusted_solved_time":16.7299083334,
        "Challenge_answer_count":4,
        "Challenge_body":"<p><strong>Problem:<\/strong>\nI am trying to setup a model in Sagemaker, however it fails when it comes to downloading the data.\nDoes anyone know what I am doing wrong?<\/p>\n\n<p><strong>What I did so far<\/strong>:\nIn order to avoid any mistakes on my side I decided to use the AWS tutorial:\ntensorflow_iris_dnn_classifier_using_estimators<\/p>\n\n<p>And I made only two changes:<\/p>\n\n<ol>\n<li>I copied the dataset to my own S3 instance. --> I tested if I could access \/ show the data and it worked.<\/li>\n<li>I edited the path to point to the new folder.<\/li>\n<\/ol>\n\n<p>This is the AWS source code:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators<\/a><\/p>\n\n<pre><code>%%time\nimport boto3\n\n# use the region-specific sample data bucket\nregion = boto3.Session().region_name\n#train_data_location = 's3:\/\/sagemaker-sample-data-{}\/tensorflow\/iris'.format(region)\ntrain_data_location = 's3:\/\/my-s3-bucket'\n\niris_estimator.fit(train_data_location)\n<\/code><\/pre>\n\n<p>And this is the error I get:<\/p>\n\n<pre><code>\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/IPython\/core\/interactiveshell.pyc in run_cell_magic(self, magic_name, line, cell)\n   2115             magic_arg_s = self.var_expand(line, stack_depth)\n   2116             with self.builtin_trap:\n-&gt; 2117                 result = fn(magic_arg_s, cell)\n   2118             return result\n   2119 \n\n&lt;decorator-gen-60&gt; in time(self, line, cell, local_ns)\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/IPython\/core\/magic.pyc in &lt;lambda&gt;(f, *a, **k)\n    186     # but it's overkill for just that one bit of state.\n    187     def magic_deco(arg):\n--&gt; 188         call = lambda f, *a, **k: f(*a, **k)\n    189 \n    190         if callable(arg):\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/IPython\/core\/magics\/execution.pyc in time(self, line, cell, local_ns)\n   1191         else:\n   1192             st = clock2()\n-&gt; 1193             exec(code, glob, local_ns)\n   1194             end = clock2()\n   1195             out = None\n\n&lt;timed exec&gt; in &lt;module&gt;()\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/tensorflow\/estimator.pyc in fit(self, inputs, wait, logs, job_name, run_tensorboard_locally)\n    314                 tensorboard.join()\n    315         else:\n--&gt; 316             fit_super()\n    317 \n    318     @classmethod\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/tensorflow\/estimator.pyc in fit_super()\n    293 \n    294         def fit_super():\n--&gt; 295             super(TensorFlow, self).fit(inputs, wait, logs, job_name)\n    296 \n    297         if run_tensorboard_locally and wait is False:\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/estimator.pyc in fit(self, inputs, wait, logs, job_name)\n    232         self.latest_training_job = _TrainingJob.start_new(self, inputs)\n    233         if wait:\n--&gt; 234             self.latest_training_job.wait(logs=logs)\n    235 \n    236     def _compilation_job_name(self):\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/estimator.pyc in wait(self, logs)\n    571     def wait(self, logs=True):\n    572         if logs:\n--&gt; 573             self.sagemaker_session.logs_for_job(self.job_name, wait=True)\n    574         else:\n    575             self.sagemaker_session.wait_for_job(self.job_name)\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/session.pyc in logs_for_job(self, job_name, wait, poll)\n   1126 \n   1127         if wait:\n-&gt; 1128             self._check_job_status(job_name, description, 'TrainingJobStatus')\n   1129             if dot:\n   1130                 print()\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/session.pyc in _check_job_status(self, job, desc, status_key_name)\n    826             reason = desc.get('FailureReason', '(No reason provided)')\n    827             job_type = status_key_name.replace('JobStatus', ' job')\n--&gt; 828             raise ValueError('Error for {} {}: {} Reason: {}'.format(job_type, job, status, reason))\n    829 \n    830     def wait_for_endpoint(self, endpoint, poll=5):\n\nValueError: Error for Training job sagemaker-tensorflow-2019-01-03-16-32-16-435: Failed Reason: ClientError: Data download failed:S3 key: s3:\/\/my-s3-bucket\/\/sagemaker-tensorflow-2019-01-03-14-02-39-959\/source\/sourcedir.tar.gz has an illegal char sub-sequence '\/\/' in it\n<\/code><\/pre>",
        "Challenge_closed_time":1546594981603,
        "Challenge_comment_count":2,
        "Challenge_created_time":1546534753933,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while setting up a model in AWS Sagemaker. The error occurs when the data is being downloaded and the user has copied the dataset to their own S3 instance and edited the path to point to the new folder. The error message indicates that the data download failed due to an illegal character sub-sequence in the S3 key.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54026623",
        "Challenge_link_count":2,
        "Challenge_participation_count":6,
        "Challenge_readability":15.9,
        "Challenge_reading_time":57.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":9.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":16.7299083334,
        "Challenge_title":"AWS Sagemaker - ClientError: Data download failed",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":5145.0,
        "Challenge_word_count":367,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1546261116430,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>The script is expecting 'bucket' to be bucket = Session().default_bucket() or your own. Have you tried setting bucket equal to your personal bucket?<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":1.95,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":22.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1432655047272,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":463.0,
        "Answerer_view_count":76.0,
        "Challenge_adjusted_solved_time":264.5982422222,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>What is a is the curl command to make a POST request to sage-maker and receive a ML inference?<\/p>",
        "Challenge_closed_time":1514326523572,
        "Challenge_comment_count":1,
        "Challenge_created_time":1513373969900,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to use the curl command to make a POST request to Amazon Sagemaker and receive a machine learning inference.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47840209",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":8.2,
        "Challenge_reading_time":1.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":11.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":264.5982422222,
        "Challenge_title":"How to Curl an Amazon Sagemaker Endpoint",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":6889.0,
        "Challenge_word_count":25,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443201378360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":749.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>Rather than using curl, it's recommended that you use the SageMaker Runtime client to send data and get back inferences from a SageMaker Endpoint:<\/p>\n\n<p><a href=\"http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_runtime_InvokeEndpoint.html\" rel=\"nofollow noreferrer\">http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_runtime_InvokeEndpoint.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.7,
        "Solution_reading_time":4.86,
        "Solution_score_count":4.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1593662684510,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":41.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":119.8228480555,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am fairly new to TensorFlow (and SageMaker) and am stuck in the process of deploying a SageMaker endpoint. I have just recently succeeded in creating a Saved Model type model, which is currently being used to service a sample endpoint (the model was created externally). However, when I checked the image I am using for the endpoint, it says '...\/tensorflow-inference', which is not the direction I want to go in because I want to use a SageMaker TensorFlow serving container (I followed tutorials from the official TensorFlow serving GitHub repo-using sample models, and they are deployed correcting using the TensorFlow serving framework).<\/p>\n<p>Am I encountering this issue because my Saved Model does not have the correct 'serving' tag? I have not checked my tag sets yet but wanted to know if this would be the core reason to the problem. Also, most importantly, <strong>what are the differences between the two container types<\/strong>-I think having a better understanding of these two concepts would show me why I am unable to produce the correct image.<\/p>\n<hr \/>\n<p>This is how I deployed the sample endpoint:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model = Model(model_data =...)\n\npredictor = model.deploy(initial_instance_count=...)\n<\/code><\/pre>\n<p>When I run the code, I get a model, an endpoint configuration, and an endpoint. I got the container type by clicking on model details within the AWS SageMaker console.<\/p>",
        "Challenge_closed_time":1595219623720,
        "Challenge_comment_count":3,
        "Challenge_created_time":1594709403670,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in deploying a SageMaker endpoint using TensorFlow. They have created a Saved Model type model, but the image being used for the endpoint is 'tensorflow-inference' instead of the desired SageMaker TensorFlow serving container. The user is unsure if the issue is due to incorrect tag sets and is seeking a better understanding of the differences between the two container types.",
        "Challenge_last_edit_time":1594788261467,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62889537",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":11.9,
        "Challenge_reading_time":19.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":141.7277916667,
        "Challenge_title":"TensorFlow Serving vs. TensorFlow Inference (container type for SageMaker model)",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1223.0,
        "Challenge_word_count":229,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1593662684510,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>There are different versions for the framework containers. Since the framework version I'm using is 1.15, the image I got had to be in a tensorflow-inference container. If I used versions &lt;= 1.13, then I would get sagemaker-tensorflow-serving images. The two aren't the same, but there's no 'correct' container type.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":4.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":50.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":229.9927777778,
        "Challenge_answer_count":0,
        "Challenge_body":"**Description**\r\nError when a MLflow registry model is deployed to triton using **mlflow-triton-plugin** with `--falvor=onnx` flag.\r\nThe plugin is trying to create a `config.pbtxt` in the destination folder before creating that model folder itself.\r\nEasy fix is to create that folder beforehand, but could also be handled from the plugin side.\r\n\r\n```\r\n# create a dir if not exists  \r\nif not os.exists(triton_deployment_dir):\r\n  os.mkdir(triton_deployment_dir)\r\n# then write config to that dir\r\nwith open(os.path.join(triton_deployment_dir, \"config.pbtxt\"),\r\n            \"w\") as cfile:\r\n    cfile.write(config)\r\n```\r\n\r\n**Triton Information**\r\nDocker image: `nvcr.io\/nvidia\/tritonserver:21.12-py3`\r\n\r\n**To Reproduce**\r\n\r\n0. Install mlflow-triton-plugin\r\n1. Log and register an ONNX model to MLflow model registry.\r\n2. Run a triton inference server with these flags: `--model-control-mode=explicit --strict-model-config=false`\r\n3. Create a deployment from mlflow:\r\n `mlflow deployments create -t triton --flavor onnx --name <model-name> -m \"models:\/<model-name>\/1\"`\r\n\r\nError is raised:\r\n```\r\nFile \"mlflow_triton\/deployments.py\", line 105, in create_deployment\r\n  File \"mlflow_triton\/deployments.py\", line 332, in _copy_files_to_triton_repo\r\n  File \"mlflow_triton\/deployments.py\", line 326, in _get_copy_paths\r\nFileNotFoundError: [Errno 2] No such file or directory: '<dest-folder>\/<model-name>\/config.pbtxt'\r\n```\r\n",
        "Challenge_closed_time":1649449895000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648621921000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to load a pyfunc model in the beta version of BentoML 1.0, even though the model gets successfully stored in the local model store. The loading of the model is failing with an AttributeError. The user has provided the code used to train and log the model to mlflow and the code used to load the model into BentoML.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/triton-inference-server\/server\/issues\/4130",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.7,
        "Challenge_reading_time":18.1,
        "Challenge_repo_contributor_count":101.0,
        "Challenge_repo_fork_count":1187.0,
        "Challenge_repo_issue_count":5827.0,
        "Challenge_repo_star_count":5419.0,
        "Challenge_repo_watch_count":122.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":229.9927777778,
        "Challenge_title":"error creating a triton deployment mlflow plugin",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":159,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1588516515763,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"UK",
        "Answerer_reputation_count":29087.0,
        "Answerer_view_count":3080.0,
        "Challenge_adjusted_solved_time":0.2190208333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following <a href=\"https:\/\/github.com\/mtm12\/SageMakerDemo\" rel=\"noreferrer\">this example<\/a> on how to train a machine learning model in Amazon-sagemaker.<\/p>\n<pre><code>data_location = 's3:\/\/{}\/kmeans_highlevel_example\/data'.format(bucket)\noutput_location = 's3:\/\/{}\/kmeans_highlevel_example\/output'.format(bucket)\n\nprint('training data will be uploaded to: {}'.format(data_location))\nprint('training artifacts will be uploaded to: {}'.format(output_location))\n\nkmeans = KMeans(role=role,\n                train_instance_count=2,\n                train_instance_type='ml.c4.8xlarge',\n                output_path=output_location,\n                k=10,\n                epochs=100,\n                data_location=data_location)\n<\/code><\/pre>\n<p>So after calling the fit function the model should be saved in the S3 bucket?? How can you load this model next time?<\/p>",
        "Challenge_closed_time":1595162985808,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595162197333,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is trying to train a machine learning model in Amazon Sagemaker by following a specific example. After calling the fit function, the model should be saved in the S3 bucket, but the user is unsure how to load the model for future use.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62980380",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.4,
        "Challenge_reading_time":10.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.2190208333,
        "Challenge_title":"How to load trained model in amazon sagemaker?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":4776.0,
        "Challenge_word_count":74,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1310821356880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Slovenia",
        "Poster_reputation_count":14913.0,
        "Poster_view_count":1093.0,
        "Solution_body":"<p>This can be done by using the sagemaker library combined with the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html\" rel=\"noreferrer\">Inference Model<\/a>.<\/p>\n<pre><code>model = sagemaker.model.Model(\n    image=image\n    model_data='s3:\/\/bucket\/model.tar.gz',\n    role=role_arn)\n<\/code><\/pre>\n<p>The options you're passing in are:<\/p>\n<ul>\n<li><code>image<\/code> - This is the ECR image you're using for inference (which should be for the algorithm you're trying to use). Paths are available <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html\" rel=\"noreferrer\">here<\/a>.<\/li>\n<li><code>model_data<\/code> - This is the path of where your model is stored (in a <code>tar.gz<\/code> compressed archive).<\/li>\n<li><code>role<\/code> - This is the arn of a role that is capable of both pulling the image from ECR and getting the s3 archive.<\/li>\n<\/ul>\n<p>Once you've successfully done this you will need to setup an endpoint, this can be done by performing the following in your notebook through the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html#sagemaker.model.Model.deploy\" rel=\"noreferrer\">deploy function<\/a>.<\/p>\n<pre><code>model.deploy(\n   initial_instance_count=1,\n   instance_type='ml.p2.xlarge'\n)\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.4,
        "Solution_reading_time":17.2,
        "Solution_score_count":8.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":128.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":83.8230358333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a data model consisting only of categorial features and a categorial label.<\/p>\n<p>So when I build that model manually in XGBoost, I would basically transform the features to binary columns (using LabelEncoder and OneHotEncoder), and the label into classes using LabelEncoder. I would then run a <strong>Multilabel Classification<\/strong> (multi:softmax).\nI tried that with my dataset and ended up with an accuracy around 0.4 (unfortunately can't share the dataset due to confidentiality)<\/p>\n<p>Now, if I run the same dataset in Azure AutoML, I end up with an accuracy around 0.85 in the best experiment. But what is really interesting is that the AutoML uses SparseNormalizer, XGBoostClassifier, with <strong>reg:logistic<\/strong> as objective.\nSo if I interpret this right, AzureML just normalizes the data (somehow from categorial data?) and then executes a logistic regression? Is this even possible \/ does this make sense with categorial data?<\/p>\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1593795420676,
        "Challenge_comment_count":4,
        "Challenge_created_time":1593709508303,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has encountered a strange algorithm selection issue while using Azure AutoML with XGBoostClassifier on categorical data. The user manually built a model using LabelEncoder and OneHotEncoder, but the accuracy was only around 0.4. However, when the same dataset was run in Azure AutoML, the accuracy improved to around 0.85, but the AutoML used SparseNormalizer, XGBoostClassifier, with reg:logistic as objective, which the user found confusing and wondered if it made sense with categorical data.",
        "Challenge_last_edit_time":1593766738543,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62701556",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":11.9,
        "Challenge_reading_time":13.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":23.8645480556,
        "Challenge_title":"Strange algorithm selection when using Azure AutoML with XBoostClassifier on categorial data",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":281.0,
        "Challenge_word_count":157,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1534756062256,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":87.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p><code>TL;DR<\/code> You're right that normalization doesn't make sense for training gradient-boosted decision trees (<code>GBDT<\/code>s) on categorical data, but it won't have an adverse impact. AutoML is an automated framework for modeling. In exchange for calibration control, you get ease-of-use. It is still worth verifying first that AutoML is receiving data with the columns properly encoded as categorical.<\/p>\n<p>Think of an AutoML model as effectively a <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html\" rel=\"nofollow noreferrer\">sklearn Pipeline<\/a>, which is a bundled set of pre-processing steps along with a predictive Estimator. AutoML will attempt to sample from a large swath of pre-configured Pipelines such that the most accurate Pipeline will be discovered. As <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-automated-ml#automatic-featurization-standard\" rel=\"nofollow noreferrer\">the docs<\/a> say:<\/p>\n<blockquote>\n<p>In every automated machine learning experiment, your data is automatically scaled or normalized to help algorithms perform well. During model training, one of the following scaling or normalization techniques will be applied to each model.<\/p>\n<\/blockquote>\n<p>Too see this, you can called <code>.named_steps<\/code> on your fitted model. Also check out <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train#automated-feature-engineering\" rel=\"nofollow noreferrer\"><code>fitted_model.get_featurization_summary()<\/code><\/a><\/p>\n<p>I especially empathize with your concern especially w.r.t. how <code>LightGBM<\/code> (MSFT's GBDT implementation) is levered by AutoML. <code>LightGBM<\/code> accepts categorical columns and instead of one-hot encoding, will bin them into two subsets whenever split. Despite this, AutoML will pre-process away the categorical columns by one-hot encoding, scaling, and\/or normalization; so this unique categorical approach is never utilized in AutoML.<\/p>\n<p>If you're interested in &quot;manual&quot; ML in Azure ML, I highly suggest looking into <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-train-machine-learning-model#estimators\" rel=\"nofollow noreferrer\"><code>Estimators<\/code><\/a> and <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-train-machine-learning-model#machine-learning-pipeline\" rel=\"nofollow noreferrer\"><code>Azure ML Pipelines<\/code><\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1594068501472,
        "Solution_link_count":5.0,
        "Solution_readability":16.2,
        "Solution_reading_time":32.88,
        "Solution_score_count":4.0,
        "Solution_sentence_count":21.0,
        "Solution_word_count":250.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.4330788889,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hello MS team,  <\/p>\n<p>I have registered an ML model in the AML workspace using an Azure Machine learning pipeline and triggered the main control script of the pipeline by linking the repo present in Azure DevOps to the AML workspace(using Service principal).   <\/p>\n<p>How do I download the latest version of the model from the AML workspace to the <em>&quot;Artifacts&quot;<\/em> folder in Azure DevOPs?  <\/p>\n<p>Any help is appreciated please.  <\/p>",
        "Challenge_closed_time":1643933985587,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643903626503,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking assistance in downloading the latest version of an ML model registered in the Azure Machine Learning Service Model Registry to the \"Artifacts\" folder in Azure DevOps. They have already registered the model using an Azure Machine Learning pipeline and linked the repo present in Azure DevOps to the AML workspace using Service principal.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/721792\/how-to-get-model-id-of-the-latest-version-register",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.1,
        "Challenge_reading_time":7.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":8.4330788889,
        "Challenge_title":"How to get Model ID of the Latest Version registered in Azure Machine Learning Service Model Registry using az ml cli?",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":92,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=6755dac2-30f1-48a2-9d0d-4d2c96edc5d4\">@Shivapriya Katta  <\/a>     <\/p>\n<p>I think you are mentioning how to get the latest version of model and download the model in az ml.    <\/p>\n<p>There are 2 steps, one is list the model to get the model ID you want, two is download the model.    <\/p>\n<p><strong>az ml model list<\/strong>    <br \/>\nList models in the workspace.    <\/p>\n<pre><code>az ml model list [--dataset-id]  \n                 [--latest]  \n                 [--model-name]  \n                 [--path]  \n                 [--property]  \n                 [--resource-group]  \n                 [--run-id]  \n                 [--subscription-id]  \n                 [--tag]  \n                 [--workspace-name]  \n                 [-v]  \n<\/code><\/pre>\n<p>Optional Parameters    <br \/>\n--dataset-id    <br \/>\nIf provided, will only show models with the specified dataset ID.    <\/p>\n<p>--latest -l    <br \/>\nIf provided, will only return models with the latest version.    <\/p>\n<p>--model-name -n    <br \/>\nAn optional model name to filter the list by.    <\/p>\n<p>--path    <br \/>\nPath to a project folder. Default: current directory.    <\/p>\n<p>--property    <br \/>\nKey\/value property to add (e.g. key=value ). Multiple properties can be specified with multiple --property options.    <\/p>\n<p>--resource-group -g    <br \/>\nResource group corresponding to the provided workspace.    <\/p>\n<p>--run-id    <br \/>\nIf provided, will only show models with the specified Run ID.    <\/p>\n<p>--subscription-id    <br \/>\nSpecifies the subscription Id.    <\/p>\n<p>--tag    <br \/>\nKey\/value tag to add (e.g. key=value ). Multiple tags can be specified with multiple --tag options.    <\/p>\n<p>--workspace-name -w    <br \/>\nName of the workspace containing models to list.    <\/p>\n<p>-v    <br \/>\nVerbosity flag.    <\/p>\n<p><strong>az ml model download<\/strong>    <br \/>\nDownload a model from the workspace.    <\/p>\n<pre><code>az ml model download --model-id  \n                     --target-dir  \n                     [--overwrite]  \n                     [--path]  \n                     [--resource-group]  \n                     [--subscription-id]  \n                     [--workspace-name]  \n                     [-v]  \n<\/code><\/pre>\n<p>Required Parameters    <br \/>\n--model-id -i    <br \/>\nID of model.    <\/p>\n<p>--target-dir -t    <br \/>\nTarget directory to download the model file to.    <\/p>\n<p>Optional Parameters    <br \/>\n--overwrite    <br \/>\nOverwrite if the same name file exists in target directory.    <\/p>\n<p>--path    <br \/>\nPath to a project folder. Default: current directory.    <\/p>\n<p>--resource-group -g    <br \/>\nResource group corresponding to the provided workspace.    <\/p>\n<p>--subscription-id    <br \/>\nSpecifies the subscription Id.    <\/p>\n<p>--workspace-name -w    <br \/>\nName of the workspace containing model to show.    <\/p>\n<p>-v    <br \/>\nVerbosity flag.    <\/p>\n<p>Hope this helps!     <\/p>\n<p><em>Please kindly accept the answer if you feel helpful, thank you!<\/em>    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.8,
        "Solution_reading_time":32.05,
        "Solution_score_count":0.0,
        "Solution_sentence_count":30.0,
        "Solution_word_count":344.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1460437080990,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":386.0,
        "Answerer_view_count":42.0,
        "Challenge_adjusted_solved_time":0.7767452778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to reduce my development headaches for creating a ML Webservice on Azure ML Studio. One of the things that stuck me was can we just upload .rda files in the workbench and load it via an RScript (like in the figure below). <\/p>\n\n<p><img src=\"https:\/\/raw.githubusercontent.com\/pratos\/pratos.github.io\/master\/images\/stackb1model.png\" alt=\"Do\"><\/p>\n\n<p>But can't connect directly to the R Script block. There's another way to do it (works to upload packages that aren't available in Azure's R directories) -- using zip. But there isn't really any resource out there that I found to access the .rda file in .zip.<\/p>\n\n<p>I have 2 options here, make the .zip work or any other work around where I can directly use my .rda model. If someone could guide me about how to go forward it would appreciate it.<\/p>\n\n<p>Note: Currently, I'm creating models via the \"Create RModel\" block, training them and saving it, so that I can use it to make a predictive web service. But for models like Random Forest, not sure how the randomness might create models (local versions and Azure versions are different, the setting of seed also isn't very helpful). A bit tight on schedule, Azure ML seems boxed for creating iterations and automating the ML workflow (or maybe I'm doing it wrong).<\/p>",
        "Challenge_closed_time":1486104439200,
        "Challenge_comment_count":0,
        "Challenge_created_time":1486101642917,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to upload a saved ML model in R (local) to Azure Machine Learning Studio but is facing challenges. They are unable to connect directly to the R Script block and are looking for a workaround to use their .rda model. The user is also unsure about how the randomness might create models and is looking for guidance on how to proceed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42017727",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":16.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":0.7767452778,
        "Challenge_title":"Upload Saved ML Model in R (local) to Azure Machine Learning Studio",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":730.0,
        "Challenge_word_count":221,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1479363468550,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":108.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>Here is an example of uploading a .rda file for scoring:\n<a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Womens-Health-Risk-Assessment-using-the-XGBoost-classification-algorithm-1\" rel=\"nofollow noreferrer\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Womens-Health-Risk-Assessment-using-the-XGBoost-classification-algorithm-1<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":48.9,
        "Solution_reading_time":5.01,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":15.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1389887039672,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Singapore",
        "Answerer_reputation_count":5854.0,
        "Answerer_view_count":794.0,
        "Challenge_adjusted_solved_time":58.3565702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Can SageMaker Training have training data in NVMe volumes on compatible instances? (eg G4dn and P3dn). If so, if there an appropriate way to programmatically access that data?<\/p>",
        "Challenge_closed_time":1663457094040,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663247010387,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring if SageMaker Training can use training data in NVMe volumes on compatible instances such as G4dn and P3dn, and if there is a way to programmatically access that data.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73731665",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.5,
        "Challenge_reading_time":3.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":58.3565702778,
        "Challenge_title":"Can SageMaker Training have training data in NVMe volumes on compatible instances?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":16.0,
        "Challenge_word_count":39,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1507661294190,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":98.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>Yes on all nitro-backed instances EBS volumes that are exposed as NVMe block devices.<\/p>\n<p>In the Sagemaker Python SDK, you can specify the\u00a0<code>volume_size<\/code>\u00a0of the\u00a0<code>SM_TRAINING_CHANNEL<\/code>\u00a0path - the EBS (NVMe backed) will be in that path and when you go to actually run you pass the\u00a0<code>--train_dir<\/code>\u00a0path to your code.<\/p>\n<p>Code example below:<\/p>\n<pre><code>def main(aws_region,s3_location,instance_cout):\n    estimator = TensorFlow(\n        train_instance_type='ml.p3.16xlarge',\n            **train_volume_size=200,**\n        train_instance_count=int(instance_count),\n        framework_version='2.2',\n            py_version='py3',\n        image_name=&quot;231748552833.dkr.ecr.%s.amazonaws.com\/sage-py3-tf-hvd:latest&quot;%aws_region,\n<\/code><\/pre>\n<p>And then in your entry script<\/p>\n<pre><code>train_dir = os.environ.get('SM_CHANNEL_TRAIN')\nsubprocess.call(['python','-W ignore', 'deep-learning-models\/legacy\/models\/resnet\/tensorflow2\/train_tf2_resnet.py', \\\n            &quot;--data_dir=%s&quot;%train_dir, \\\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.2,
        "Solution_reading_time":13.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":79.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1363541433296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Twin Cities, MN, USA",
        "Answerer_reputation_count":348.0,
        "Answerer_view_count":24.0,
        "Challenge_adjusted_solved_time":7.8782044444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying in Amazon Sagemaker to deploy an existing Scikit-Learn model. So a model that wasn't trained on SageMaker, but locally on my machine.<\/p>\n<p>On my local (windows) machine I've saved my model as model.joblib and tarred the model to model.tar.gz.<\/p>\n<p>Next, I've uploaded this model to my S3 bucket ('my_bucket') in the following path s3:\/\/my_bucket\/models\/model.tar.gz. I can see the tar file in S3.<\/p>\n<p>But when I'm trying to deploy the model, it keeps giving the error message &quot;Failed to extract model data archive&quot;.<\/p>\n<p>The .tar.gz is generated on my local machine by running 'tar -czf model.tar.gz model.joblib' in a powershell command window.<\/p>\n<p>The code for uploading to S3<\/p>\n<pre><code>import boto3\ns3 = boto3.client(&quot;s3&quot;, \n              region_name='eu-central-1', \n              aws_access_key_id=AWS_KEY_ID, \n              aws_secret_access_key=AWS_SECRET)\ns3.upload_file(Filename='model.tar.gz', Bucket=my_bucket, Key='models\/model.tar.gz')\n<\/code><\/pre>\n<p>The code for creating the estimator and deploying:<\/p>\n<pre><code>import boto3\nfrom sagemaker.sklearn.estimator import SKLearnModel\n\n...\n\nmodel_data = 's3:\/\/my_bucket\/models\/model.tar.gz'\nsklearn_model = SKLearnModel(model_data=model_data,\n                             role=role,\n                             entry_point=&quot;my-script.py&quot;,\n                             framework_version=&quot;0.23-1&quot;)\npredictor = sklearn_model.deploy(instance_type=&quot;ml.t2.medium&quot;, initial_instance_count=1)                             \n<\/code><\/pre>\n<p>The error message:<\/p>\n<blockquote>\n<p>error message: UnexpectedStatusException: Error hosting endpoint\nsagemaker-scikit-learn-2021-01-24-17-24-42-204: Failed. Reason: Failed\nto extract model data archive for container &quot;container_1&quot; from URL\n&quot;s3:\/\/my_bucket\/models\/model.tar.gz&quot;. Please ensure that the object\nlocated at the URL is a valid tar.gz archive<\/p>\n<\/blockquote>\n<p>Is there a way to see why the archive is invalid?<\/p>",
        "Challenge_closed_time":1611852069143,
        "Challenge_comment_count":5,
        "Challenge_created_time":1611565411923,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy an existing Scikit-Learn model on Amazon SageMaker that was not trained on SageMaker but locally on their machine. They saved the model as model.joblib and tarred it to model.tar.gz. The user uploaded the model to their S3 bucket but encountered an error message \"Failed to extract model data archive\" when trying to deploy the model. The user is seeking a way to see why the archive is invalid.",
        "Challenge_last_edit_time":1611823707607,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65881699",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":11.7,
        "Challenge_reading_time":25.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":79.6270055556,
        "Challenge_title":"SageMaker failed to extract model data archive tar.gz for container when deploying",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1859.0,
        "Challenge_word_count":208,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1472970520888,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Amersfoort, Nederland",
        "Poster_reputation_count":424.0,
        "Poster_view_count":21.0,
        "Solution_body":"<p>I had a similar issue as well, along with a similar fix to Bas (per comment above).<\/p>\n<p>I was finding I wasn't necessarily having issues with the .tar.gz step, this command does work fine:<\/p>\n<p><code>tar -czf &lt;filename&gt; .\/&lt;directory-with-files&gt;<\/code><\/p>\n<p>but rather with the uploading step.<\/p>\n<p>Manually uploading to S3 should take care of this, however, if you're doing this step programmatically, you might need to double check the steps taken. Bas appears to have had filename issues, mine were around using boto properly. Here's some code that works (Python only here, but watch for similar issues with other libraries):<\/p>\n<pre><code>bucket = 'bucket-name'\nkey = 'directory-inside-bucket'\nfile = 'the file name of the .tar.gz'\n\ns3_client = boto3.client('s3')\ns3_client.upload_file(file, bucket, key)\n<\/code><\/pre>\n<p>Docs: <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.9,
        "Solution_reading_time":14.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":120.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1428432018420,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, United States",
        "Answerer_reputation_count":301.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":45.3193069444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm pretty new to Tensorflow and SageMaker and I'm trying to figure out how to write my <code>serving_input_fn()<\/code>. I've tried a number of ways to do it, but to no avail. <\/p>\n\n<p>my input function has 3 feature columns: <code>amount_normalized, x_month and y_month<\/code>:<\/p>\n\n<pre><code>def construct_feature_columns():\n    amount_normalized = tf.feature_column.numeric_column(key='amount_normalized')\n    x_month = tf.feature_column.numeric_column(key='x_month')\n    y_month = tf.feature_column.numeric_column(key='y_month')\n    return set([amount_normalized, x_month, y_month])\n<\/code><\/pre>\n\n<p>I want to be able to call my deployed model using something like <code>deployed_model.predict([1.23,0.3,0.8])<\/code> <\/p>\n\n<p>Where the first element is <code>amount_normalized<\/code>, second is <code>x_month<\/code> third is <code>y_month<\/code><\/p>\n\n<p>I've tried this:<\/p>\n\n<pre><code>FEATURES = ['amount_normalized', 'x_month', 'y_month']\ndef serving_input_fn(params):\n    feature_placeholders = {\n      key : tf.placeholder(tf.float32, [None]) \\\n        for key in FEATURES\n    }\nreturn tf.estimator.export.build_raw_serving_input_receiver_fn(feature_placeholders)()\n<\/code><\/pre>\n\n<p>But all I get is:\n<code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"\".<\/code><\/p>\n\n<p>Any help would be <strong>really<\/strong> appreciated!<\/p>",
        "Challenge_closed_time":1525019634972,
        "Challenge_comment_count":0,
        "Challenge_created_time":1524856485467,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is new to Tensorflow and SageMaker and is struggling to write their serving_input_fn() for a model with 3 feature columns. They want to call their deployed model using deployed_model.predict([1.23,0.3,0.8]) where the first element is amount_normalized, second is x_month, and third is y_month. The user has tried to write the serving_input_fn() but is encountering an error. They are seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50068941",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.2,
        "Challenge_reading_time":18.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":45.3193069444,
        "Challenge_title":"SageMaker Tensorflow - how to write my serving_input_fn()",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1337.0,
        "Challenge_word_count":138,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1428432018420,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"New York, United States",
        "Poster_reputation_count":301.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>Posting this here in case anyone else has this issue.<\/p>\n\n<p>After a bunch of trial and error I managed to solve my issue by writing my serving input function like this:<\/p>\n\n<pre><code>FEATURES = ['amount_normalized', 'x_month', 'y_month']\ndef serving_input_fn(hyperparameters):\n    feature_spec = {\n        key : tf.FixedLenFeature(shape=[], dtype = tf.float32) \\\n          for key in FEATURES\n    }\n    return tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)()\n<\/code><\/pre>\n\n<p>I can then call my deployed model by passing in a hash:<\/p>\n\n<pre><code>deployed_model.predict({\"amount_normalized\": 2.3, \"x_month\": 0.2, \"y_month\": -0.3})\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.8,
        "Solution_reading_time":8.4,
        "Solution_score_count":3.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":68.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.8001691667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I can\u2019t find some of the basic modules from this week. Any significant change about Designer? <\/p>",
        "Challenge_closed_time":1660841903856,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660839023247,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to locate some basic modules in the Machine Learning Designer and is questioning if there have been any significant changes made to the Designer.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/972775\/change-in-machine-learning-designer",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":1.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.8001691667,
        "Challenge_title":"Change in Machine Learning Designer",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":21,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=2ce87912-8dda-483a-8ead-0e2912a6e6ef\">@Mofoch  <\/a>     <\/p>\n<p>Thanks for using Microsoft Q&amp;A platform, there is no surprising change in Azure Machine Learning Designer.    <\/p>\n<p>Based on my experience, you may use the filter so you can not see some of the modules as below screenshot.     <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/232490-image.png?platform=QnA\" alt=\"232490-image.png\" \/>    <\/p>\n<p>If this is not your case, could you please share which module you have lost? Thanks.     <\/p>\n<p>I hope this helps.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.4,
        "Solution_reading_time":9.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":91.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":22.7411330556,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I have a notebook based on <a href=\"http:\/\/wandb.me\/lit-colab\" rel=\"noopener nofollow ugc\"> Supercharge your Training with PyTorch Lightning + Weights &amp; Biases<\/a> and I\u2019m wondering what the easiest approach to load a model with the best checkpoint after training finishes.<\/p>\n<p>I\u2019m assuming that after training the \u201cmodel\u201d instance will just have the weights of the most recent epoch, which might not be the most accurate model (in case it started overfitting etc).<\/p>\n<p>Specifically I was looking for an easy way to get the directory where the checkpoints artifacts are stored, which in my case look like this: <code>.\/MnistKaggle\/1vzsgin6\/checkpoints<\/code>, where <code>1vzsgin6<\/code> is the run id auto-generated by wandb.<\/p>\n<p>One (clunky) way to do it would be:<\/p>\n<pre><code class=\"lang-auto\">wandb_logger = WandbLogger(project=\"MnistKaggle\")\ncheckpoint_dir_path = None\n\ndef my_after_save_checkpoint(checkpoint):\n  checkpoint_dir_path = checkpoint.dirpath\n\nwandb_logger.after_save_checkpoint = my_after_save_checkpoint\n\n# Now find the checkpoint file in the checkpoint_dir_path directory and load the model from that.\n<\/code><\/pre>\n<p>Is there an easier way?  I was sorta expecting the <code>WandbLogger<\/code> object to have an easy method like <code>get_save_checkpoint_dirpath()<\/code>, but I\u2019m not seeing anything.<\/p>\n<p>Thanks in advance for any help!<\/p>",
        "Challenge_closed_time":1667430011968,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667348143889,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking the easiest way to load the best model checkpoint after training with PyTorch Lightning and Weights & Biases. They are looking for a way to get the directory where the checkpoints artifacts are stored and are currently using a clunky method involving WandbLogger. They are hoping for an easier method to retrieve the checkpoint directory path.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/easiest-way-to-load-the-best-model-checkpoint-after-training-w-pytorch-lightning\/3365",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":10.9,
        "Challenge_reading_time":18.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":22.7411330556,
        "Challenge_title":"Easiest way to load the best model checkpoint after training w\/ pytorch lightning",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1455.0,
        "Challenge_word_count":182,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/tleyden\">@tleyden<\/a> , happy to help. Please review the following <a href=\"https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/extensions\/generated\/pytorch_lightning.loggers.WandbLogger.html#:~:text=(model)-,Log%20model%20checkpoints,-Log%20model%20checkpoints\" rel=\"noopener nofollow ugc\">resource<\/a> on model checkpointing and retrieval.<\/p>\n<p>A common flow would be to log a model checkpoint as in the example then to also log a \u201cbest model\u201d artifact. Since artifacts are versioned you don\u2019t have to worry about renaming the new \u201cbest model\u201d artifact. Then at the end of your run you not only have an artifact history of your model at each of the checkpoints but also a versioned history of all the best models.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.9,
        "Solution_reading_time":9.74,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":91.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":220.4830555556,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nI try to reproduce the minimal example from the Docs: a Kedro project using the starter `pandas-iris` using the `kedro-mlflow` functinality. I do not arrive at initializing the kedro-mlflow project, since the cli commands are not available.\r\n\r\n## Context\r\n\r\nIt is unclear to me if this is connected to #157 \r\nI wanted to start looking into kedro-mlflow, but got immediatle blocked by the initialization of the project. Therefore any advice on where to look to fix this would also be appreciated. \r\n\r\n## Steps to Reproduce\r\n\r\n```\r\nconda create -n kedro_mlflow python=3.8\r\nconda activate kedro_mlflow\r\npip install kedro-mlflow\r\nkedro mlflow -h\r\nkedro new --starter=pandas-iris\r\ncd mlflow_test\/\r\nkedro mlflow -h\r\n> ERROR \"No such command 'mlflow'\"\r\n```\r\n\r\n## Expected Result\r\n\r\n`kedro mlflow` is available in a project directory, i.e. `kedro mlflow -h` gives the same output inside the folder as before\r\n\r\n## Actual Result\r\n\r\ninside the project folder the `mlflow` command is unknown to Kedro\r\n\r\n```\r\n...\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/pkg_resources\/__init__.py:1130: DeprecationWarning: Use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\r\n  return get_provider(package_or_requirement).get_resource_filename(\r\n....\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/mlflow\/types\/schema.py:49: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \r\nDeprecated in NumPy 1.20; for more details and guidance: https:\/\/numpy.org\/devdocs\/release\/1.20.0-notes.html#deprecations\r\n  binary = (7, np.dtype(\"bytes\"), \"BinaryType\", np.object)\r\n2021-04-23 17:49:52,197 - root - INFO - Registered hooks from 2 installed plugin(s): kedro-mlflow-0.7.1\r\nUsage: kedro [OPTIONS] COMMAND [ARGS]...\r\nTry 'kedro -h' for help.\r\n\r\nError: No such command 'mlflow'.\r\n\r\n```\r\n\r\n## Your Environment\r\n\r\nUbuntu 18.04.5\r\n\r\n- Kedro 0.17.3\r\n- kedro-mlflow 0.7.1\r\n- python 3.8.8.\r\n- mlflow 1.15.0\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nyes",
        "Challenge_closed_time":1619987466000,
        "Challenge_comment_count":6,
        "Challenge_created_time":1619193727000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to load a previously saved KedroPipelineModel from mlflow due to a \"cannot pickle context artifacts\" error caused by a non-deepcopyable dataset (in this case, a keras tokenizer). The issue occurs with kedro and kedro-mlflow versions 0.16.5 and 0.4.0, and the bug also happens with the latest version on develop. A potential solution involves modifying a line of code in the kedro_mlflow package.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/193",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":8.4,
        "Challenge_reading_time":27.15,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":21.0,
        "Challenge_repo_issue_count":414.0,
        "Challenge_repo_star_count":145.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":220.4830555556,
        "Challenge_title":"kedro-mlflow CLI is unavailable inside a Kedro project",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":273,
        "Discussion_body":"Hi, \r\n\r\nI wil try to check it out this weekend, but the `kedro==0.17.3` version is brand new (it was released yesterday), and given my experience with past kedro versions update 2 things might have happened on kedro's side: \r\n- They have broken the auto-discovery mechanism (I've seen in the release note that they change the CLI command discovery to enale overriding project commands by plugins)\r\n- They have not updated their `pandas-iris` starter yet which does not match the new version and is only compliant with `kedro==0.17.2`. \r\n\r\nWhile I am investigating, would you please confirm that :\r\n- `kedro-mlflow` works fine with kedro==0.17.2 with your setup\r\n- `kedro-mlflow` works fine if you don't use the `pandas-iris` starter: try `kedro new` with `kedro==0.17.3` and then add one ode to test the plugin\r\n- I'd be glad to see if another plugin (e.g. `kedro-viz`) is facing the same problem that kedro-mlflow. Would you mind checking?\r\n\r\nOf course there is the possibility that the problem comes from `kedro-mlflow` itself, but I hardly believe it. I'll tell you within 2 days. I am sorry, I am quite busy for now and I will not debug this before next week. Once again, it is very likely kedro's plugin discovery mechanism has been broken in the new release, I strongly suggest you go back to `kedro==0.17.2`.\r\n\r\nNext actions: \r\n- [X] reproduce the bug -> Done, thanks for the very good reproducible example\r\n- [X] Check if it happens with other plugins (say kedro-viz) -> `kedro viz` global command is properly discovered\r\n- [X] Check if hooks are properly loaded -> everything works fine if I add a `mlflow.yml` manually in the `conf\/local` folder (or any folder in `conf\/` actually). -> **This is a short term solution for you**,e ven if it is not very convenient. You can find allowed keys [in the documentation](https:\/\/kedro-mlflow.readthedocs.io\/en\/latest\/source\/04_experimentation_tracking\/01_configuration.html#the-mlflow-yml-file) or irectly [copy paste it from the code](https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/master\/kedro_mlflow\/template\/project\/mlflow.yml)\r\n- [X] Check if the tests pass with kedro==0.17.3 -> *Some tests are failing, but not the one related to the CLI commands which seems discovered. I need to investigate further*.\r\n- [x] Check if other plugins with *local* commands are discovered\r\n- [x] Check if it also happens it an empty project (i.e. *not* a starter)\r\n First of all, thank you for looking so quickly into it!\r\n\r\nFrom how I read your second message you already know that, but to answer your questions:\r\n- detecting `kedro mlflow` works fine with `kedro==0.17.2`\r\n- the problem is consistent with kedro==0.17.3 independent if I use the pandas-iris starter or not\r\n- `kedro viz` is found also with `kedro=0.17.3`\r\n\r\nAgain, thank you for providing workarounds directly on Monday morning, I can nicely work with those! A question for my understanding of the plugin: As long as the hooks are loaded, the mlflow functionality depends only on a `mlflow.yml` to be present, and all that `kedro mlflow init` does is copy this file from the template into `conf\/local`, is this correct? TL;DR: \r\n\r\nInstall this version for now, it should make the command available again:\r\n\r\n```console\r\npip uninstall kedro-mlflow\r\npip install git+https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow.git@bug\/no-cli\r\n```\r\n**Beware:** it is very important to uninstall your existing version of kedro-mlflow before reinstalling because the patch has the same version number that the current release.\r\n\r\nIf you confirm this works for you, I will deploy the patch to PyPI before kedro provides a patch on their side.\r\n_____________________________\r\n\r\nHi, some follow-up about this bug:\r\n\r\n- I've figured out *what* is going on but not *why* it happens. The `mlflow` group of command exists both at global (`new`) and project (`init`, `ui`) levels and for an unknown reason, `kedro` takes into account only one group of command in its `0.17.3` version. This is a bug I will report to the core team. However, it does not affect their other plugins (kedro-viz, kedro-docker, kedro-airflow) because none of them has both global and project commands.\r\n- The quickest (hacky) fix is to remove the global group of command to the make the project ones available. I've done this in the branch `bug\/no-cli` of the repo.\r\n\r\nTo answer your question: \r\n\r\n> A question for my understanding of the plugin: As long as the hooks are loaded, the mlflow functionality depends only on a mlflow.yml to be present, and all that kedro mlflow init does is copy this file from the template into conf\/local, is this correct?\r\n\r\nExactly: the `init` command renders the template (i.e. copy paste it + replace the jinja tags with dynamic values like the name of your project) to a folder in your `conf\/` folder (by default `local`, but you can specify an environment like this: `kedro mlflow init --env=<your-env-folder>`). The hooks contain all the code logic  and this mlflow.yml file is just here to pass parameters to them. \r\n\r\nThe other project command is `kedro mlflow ui` which is just a wrapper of \"mlflow ui\" with the parameters (mlflow_tracking_uri, port, host) defined in your `mlflow.yml` file.\r\n thanks, form a quick test I would say: the patch works like a charm! Hi @dmb23, I've just deployed the patch to PyPI. You can use `pip install kedro_mlflow==0.7.2`` and it should be ok for now. I close the issue, but feel free to reopen if you still encounter any issue in this new version.",
        "Discussion_score_count":7.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.1856155556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I created a notebook in the workspace and when I sent the experiment for training I received error message <strong>undefined symbol: XGDMatrixSetDenseInfo<\/strong> for algorithm <strong>Xgboost<\/strong>. Do you know how to fix the problem?<\/p>\n<p><strong>Azure ML Version:<\/strong> 1.22.0  <br \/>\n<strong>Compute Instance:<\/strong> Standard_DS3_v2<\/p>\n<ul>\n<li> Code:  import logging  <br \/>\n  from azureml.train.automl import AutoMLConfig  <br \/>\n  from azureml.core.experiment import Experiment  automl_settings = {  <br \/>\n  &quot;iteration_timeout_minutes&quot;: 10,  <br \/>\n  &quot;experiment_timeout_hours&quot;: 0.3,  <br \/>\n  &quot;enable_early_stopping&quot;: True,  <br \/>\n  &quot;primary_metric&quot;: 'normalized_root_mean_squared_error',  <br \/>\n  &quot;featurization&quot;: 'auto',  <br \/>\n  &quot;verbosity&quot;: logging.INFO,  <br \/>\n  &quot;n_cross_validations&quot;: 5  <br \/>\n  }  automl_config = AutoMLConfig(task='regression',  <br \/>\n  debug_log='automated_ml_errors.log',  <br \/>\n  training_data=x_train,  <br \/>\n  label_column_name=&quot;production_time&quot;,  <br \/>\n  **automl_settings)  experiment = Experiment(ws, &quot;train-model&quot;)  <br \/>\n  local_run = experiment.submit(automl_config, show_output=True)<\/li>\n<li> Full Error Message:  ERROR: FitException:  <br \/>\n  Message: \/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo  <br \/>\n  InnerException: AttributeError: \/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo  <br \/>\n  ErrorResponse  <br \/>\n  {  <br \/>\n  &quot;error&quot;: {  <br \/>\n  &quot;code&quot;: &quot;SystemError&quot;,  <br \/>\n  &quot;message&quot;: &quot;Encountered an internal AutoML error. Error Message\/Code: FitException. Additional Info: FitException:\\n\\tMessage: \/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo\\n\\tInnerException: None\\n\\tErrorResponse \\n{\\n \\&quot;error\\&quot;: {\\n \\&quot;message\\&quot;: \\&quot;\/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo\\&quot;,\\n \\&quot;target\\&quot;: \\&quot;Xgboost\\&quot;,\\n \\&quot;reference_code\\&quot;: \\&quot;Xgboost\\&quot;\\n }\\n}&quot;,  <br \/>\n  &quot;details_uri&quot;: &quot;https:\/\/learn.microsoft.com\/azure\/machine-learning\/resource-known-issues#automated-machine-learning&quot;,  <br \/>\n  &quot;target&quot;: &quot;Xgboost&quot;,  <br \/>\n  &quot;inner_error&quot;: {  <br \/>\n  &quot;code&quot;: &quot;ClientError&quot;,  <br \/>\n  &quot;inner_error&quot;: {  <br \/>\n  &quot;code&quot;: &quot;AutoMLInternal&quot;  <br \/>\n  }  <br \/>\n  },  <br \/>\n  &quot;reference_code&quot;: &quot;Xgboost&quot;  <br \/>\n  }  <br \/>\n  }<\/li>\n<\/ul>\n<p>Best regards,  <br \/>\nCristina<\/p>\n<p><a href=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/79658-packages.txt?platform=QnA\">79658-packages.txt<\/a><\/p>",
        "Challenge_closed_time":1616196599996,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616170731780,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while running an experiment in Azure Machine Learning Services AutoML. The error message stated \"undefined symbol: XGDMatrixSetDenseInfo\" for the Xgboost algorithm. The user is seeking help to fix the problem. The Azure ML version used was 1.22.0 and the compute instance was Standard_DS3_v2. The error message and code used in the experiment are provided in the post.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/322886\/azure-machine-learning-services-automl-error-runni",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":23.9,
        "Challenge_reading_time":39.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":7.1856155556,
        "Challenge_title":"Azure Machine Learning Services - AutoMl - Error running experiment.submit: \"\/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo\"",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":197,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, can you try uninstalling and reinstalling Xgboost (try versions &lt;= 0.90 if you continue to get errors).<\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":1.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":69.0259405556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained a AutoML Object Detection model in Vertex AI (a service under AI Platform in GCP). I am trying to access model evaluation metrics for each label (precision, recall, accuracy etc.) for varying Confidence Score Threshold and IoU Threshold.<\/p>\n<p>However, I am stuck at step one, even to get model's aggerate performance metric much less to the performance metric at granular levels. I have followed <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/evaluating-automl-models?authuser=1#aggregate\" rel=\"nofollow noreferrer\">this instruction<\/a> But I cannot seem to figure out what is <code>evaluation_id<\/code> (also see the official sample code snippet <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/samples\/snippets\/model_service\/get_model_evaluation_image_object_detection_sample.py\" rel=\"nofollow noreferrer\">here<\/a>), which is:<\/p>\n<pre><code>def get_model_evaluation_image_object_detection_sample(\n    project: str,\n    model_id: str,\n    evaluation_id: str,\n    location: str = &quot;us-central1&quot;,\n    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,\n):\n    # The AI Platform services require regional API endpoints.\n    client_options = {&quot;api_endpoint&quot;: api_endpoint}\n    # Initialize client that will be used to create and send requests.\n    # This client only needs to be created once, and can be reused for multiple requests.\n    client = aiplatform.gapic.ModelServiceClient(client_options=client_options)\n    name = client.model_evaluation_path(\n        project=project, location=location, model=model_id, evaluation=evaluation_id\n    )\n    response = client.get_model_evaluation(name=name)\n    print(&quot;response:&quot;, response)\n<\/code><\/pre>\n<p>After sometime I have figured out that for model trained in EU,  and <code>api_endpoint<\/code> shall be passed as:<\/p>\n<pre><code>location: str = &quot;europe-west4&quot;\napi_endpoint: str = &quot;europe-west4-aiplatform.googleapis.com&quot;\n<\/code><\/pre>\n<p>But whatever I try for <code>evaluation_id<\/code> leads to the following errors:<\/p>\n<pre><code>InvalidArgument: 400 List of found errors:  1.Field: name; Message: Invalid ModelEvaluation resource name.\n<\/code><\/pre>\n<p>There in the documentation it says (which is seems it contains what I need):<\/p>\n<blockquote>\n<p>For the bounding box metric, Vertex AI returns an array of metric\nvalues at different IoU threshold values (between 0 and 1) and\nconfidence threshold values (between 0 and 1). For example, you can\nnarrow in on evaluation metrics at an IoU threshold of 0.85 and a\nconfidence threshold of 0.8228. By viewing these different threshold\nvalues, you can see how they affect other metrics such as precision\nand recall.<\/p>\n<\/blockquote>\n<p>Without knowing that is contained in the output array, how would that work for each class? Basically I need for each class the model metrics for varying IoU threshold values and confidence threshold.<\/p>\n<p>Also I have tried to query from AutoML API instead, like:<\/p>\n<pre><code>client_options = {'api_endpoint': 'eu-automl.googleapis.com:443'}\n\nclient = automl.AutoMlClient(client_options=client_options)\n# Get the full path of the model.\nmodel_full_id = client.model_path(project_id, &quot;europe-west4&quot;, model_id)\n\nprint(&quot;List of model evaluations:&quot;)\nfor evaluation in client.list_model_evaluations(parent=model_full_id, filter=&quot;&quot;):\n    print(&quot;Model evaluation name: {}&quot;.format(evaluation.name))\n    print(&quot;Model annotation spec id: {}&quot;.format(evaluation.annotation_spec_id))\n    print(&quot;Create Time: {}&quot;.format(evaluation.create_time))\n    print(&quot;Evaluation example count: {}&quot;.format(evaluation.evaluated_example_count))\n    print(\n        &quot;Classification model evaluation metrics: {}&quot;.format(\n            evaluation.classification_evaluation_metrics\n        )\n    )\n<\/code><\/pre>\n<p>No surprise, also this doesn't work, and leads to:<\/p>\n<pre><code>InvalidArgument: 400 List of found errors:  1.Field: parent; Message: The provided location ID doesn't match the endpoint. For automl.googleapis.com, the valid location ID is `us-central1`. For eu-automl.googleapis.com, the valid location ID is `eu`.\n<\/code><\/pre>",
        "Challenge_closed_time":1635129616532,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634902641630,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained an AutoML Object Detection model in Vertex AI and is trying to access model evaluation metrics for each label for varying Confidence Score Threshold and IoU Threshold. However, they are unable to get the model's aggregate performance metric, much less the performance metric at granular levels. They are stuck at step one and cannot figure out what the evaluation_id is. They have tried passing the correct api_endpoint for a model trained in EU, but whatever they try for evaluation_id leads to errors. They need the model metrics for varying IoU threshold values and confidence threshold for each class. They have also tried querying from AutoML API, but it doesn't work either.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69676225",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":15.3,
        "Challenge_reading_time":54.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":63.0485838889,
        "Challenge_title":"GCP AI Platform API - Object Detection Metrics at Class Level (Python)",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":298.0,
        "Challenge_word_count":441,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1490792741112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berlin, Germany",
        "Poster_reputation_count":423.0,
        "Poster_view_count":84.0,
        "Solution_body":"<p>I was able to get the response of the model evaluation using <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform.html\" rel=\"nofollow noreferrer\">aiplatform_v1<\/a> which is well documented and this is the reference linked from the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/start\/client-libraries?authuser=1#client_libraries\" rel=\"nofollow noreferrer\">Vertex AI reference page<\/a>.<\/p>\n<p>On this script I ran <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform_v1\/model_service.html#google.cloud.aiplatform_v1.services.model_service.ModelServiceClient.list_model_evaluations\" rel=\"nofollow noreferrer\">list_model_evaluations()<\/a> to get the evaluation name and used it as input for <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform_v1\/model_service.html#google.cloud.aiplatform_v1.services.model_service.ModelServiceClient.get_model_evaluation\" rel=\"nofollow noreferrer\">get_model_evaluation()<\/a> which will return the evaluation details for Confidence Score Threshold, IoU Threshold, etc.<\/p>\n<p>NOTE: I don't have a trained model in <code>europe-west4<\/code> so I used <code>us-central1<\/code> instead. But if you have trained in <code>europe-west4<\/code> you should use <code>https:\/\/europe-west4-aiplatform.googleapis.com<\/code> as <code>api_endpoint<\/code> as per <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/locations#specifying_the_location_using_the\" rel=\"nofollow noreferrer\">location document<\/a>.<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as aiplatform\n\napi_endpoint = 'us-central1-aiplatform.googleapis.com'\nclient_options = {&quot;api_endpoint&quot;: api_endpoint}\nclient_model = aiplatform.services.model_service.ModelServiceClient(client_options=client_options)\nproject_id = 'your-project-id'\nlocation = 'us-central1'\nmodel_id = '999999999999'\n\nmodel_name = f'projects\/{project_id}\/locations\/{location}\/models\/{model_id}'\nlist_eval_request = aiplatform.types.ListModelEvaluationsRequest(parent=model_name)\nlist_eval = client_model.list_model_evaluations(request=list_eval_request)\neval_name=''\nfor val in list_eval:\n    eval_name = val.name\n\nget_eval_request = aiplatform.types.GetModelEvaluationRequest(name=eval_name)\nget_eval = client_model.get_model_evaluation(request=get_eval_request)\nprint(get_eval)\n<\/code><\/pre>\n<p>See response snippet:<\/p>\n<pre><code>name: &quot;projects\/xxxxxxxxx\/locations\/us-central1\/models\/999999999999\/evaluations\/1234567890&quot;\nmetrics_schema_uri: &quot;gs:\/\/google-cloud-aiplatform\/schema\/modelevaluation\/image_object_detection_metrics_1.0.0.yaml&quot;\nmetrics {\n  struct_value {\n    fields {\n      key: &quot;boundingBoxMeanAveragePrecision&quot;\n      value {\n        number_value: 0.20201288\n      }\n    }\n    fields {\n      key: &quot;boundingBoxMetrics&quot;\n      value {\n        list_value {\n          values {\n            struct_value {\n              fields {\n                key: &quot;confidenceMetrics&quot;\n                value {\n                  list_value {\n                    values {\n                      struct_value {\n                        fields {\n                          key: &quot;confidenceThreshold&quot;\n                          value {\n                            number_value: 0.06579724\n                          }\n                        }\n                        fields {\n                          key: &quot;f1Score&quot;\n                          value {\n                            number_value: 0.15670435\n                          }\n                        }\n                        fields {\n                          key: &quot;precision&quot;\n                          value {\n                            number_value: 0.09326923\n                          }\n                        }\n                        fields {\n                          key: &quot;recall&quot;\n                          value {\n                            number_value: 0.48989898\n                          }\n                        }\n                      }\n                    }\n                    values {\n                      struct_value {\n....\n<\/code><\/pre>\n<p><strong>EDIT 1: Get response per class<\/strong><\/p>\n<p>To get metrics per class, you can use <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform_v1\/model_service.html#google.cloud.aiplatform_v1.services.model_service.ModelServiceClient.list_model_evaluation_slices\" rel=\"nofollow noreferrer\">list_model_evaluation_slices()<\/a> to get the name for each class, then use the name to <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform_v1\/model_service.html#google.cloud.aiplatform_v1.services.model_service.ModelServiceClient.get_model_evaluation_slice\" rel=\"nofollow noreferrer\">get_model_evaluation_slice()<\/a>. In this code I pushed the names to a list since I have multiple classes. Then just use the values stored in the array to get the metric per class.<\/p>\n<p>In my code I used <code>label[0]<\/code> to get a single response from this class.<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as aiplatform\n\napi_endpoint = 'us-central1-aiplatform.googleapis.com'\nclient_options = {&quot;api_endpoint&quot;: api_endpoint}\nclient_model = aiplatform.services.model_service.ModelServiceClient(client_options=client_options)\nproject_id = 'your-project-id'\nlocation = 'us-central1'\nmodel_id = '999999999999'\n\nmodel_name = f'projects\/{project_id}\/locations\/{location}\/models\/{model_id}'\nlist_eval_request = aiplatform.types.ListModelEvaluationsRequest(parent=model_name)\nlist_eval = client_model.list_model_evaluations(request=list_eval_request)\neval_name=''\nfor val in list_eval:\n    eval_name = val.name\n\nlabel=[]\nslice_eval_request = aiplatform.types.ListModelEvaluationSlicesRequest(parent=eval_name)\nslice_eval = client_model.list_model_evaluation_slices(request=slice_eval_request)\nfor data in slice_eval:\n    label.append(data.name)\n\nget_eval_slice_request = aiplatform.types.GetModelEvaluationSliceRequest(name=label[0])\nget_eval_slice = client_model.get_model_evaluation_slice(request=get_eval_slice_request)\nprint(get_eval_slice)\n<\/code><\/pre>\n<p>Print all classes:\n<a href=\"https:\/\/i.stack.imgur.com\/pinWU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/pinWU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Classes in UI:\n<a href=\"https:\/\/i.stack.imgur.com\/mXlIW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mXlIW.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Response snippet for a class:<\/p>\n<pre><code>name: &quot;projects\/xxxxxxxxx\/locations\/us-central1\/models\/999999999\/evaluations\/0000000000\/slices\/777777777&quot;\nslice_ {\n  dimension: &quot;annotationSpec&quot;\n  value: &quot;Cheese&quot;\n}\nmetrics_schema_uri: &quot;gs:\/\/google-cloud-aiplatform\/schema\/modelevaluation\/image_object_detection_metrics_1.0.0.yaml&quot;\nmetrics {\n  struct_value {\n    fields {\n      key: &quot;boundingBoxMeanAveragePrecision&quot;\n      value {\n        number_value: 0.14256561\n      }\n    }\n    fields {\n      key: &quot;boundingBoxMetrics&quot;\n      value {\n        list_value {\n          values {\n            struct_value {\n              fields {\n                key: &quot;confidenceMetrics&quot;\n                value {\n                  list_value {\n                    values {\n                      struct_value {\n                        fields {\n                          key: &quot;confidenceThreshold&quot;\n                          value {\n                            number_value: 0.06579724\n                          }\n                        }\n                        fields {\n                          key: &quot;f1Score&quot;\n                          value {\n                            number_value: 0.10344828\n                          }\n                        }\n                        fields {\n                          key: &quot;precision&quot;\n                          value {\n                            number_value: 0.06198347\n                          }\n                        }\n....\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1635151135016,
        "Solution_link_count":12.0,
        "Solution_readability":25.3,
        "Solution_reading_time":87.88,
        "Solution_score_count":2.0,
        "Solution_sentence_count":51.0,
        "Solution_word_count":407.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1620154324507,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"India",
        "Answerer_reputation_count":1169.0,
        "Answerer_view_count":2077.0,
        "Challenge_adjusted_solved_time":69.4640041667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've deployed a tensorflow model in vertex AI platform using TFX Pipelines. The model have custom serving signatures but I'm strugling to specify the signature when I'm predicting.<\/p>\n<p>I've the exact same model deployed in GCP AI Platform and I'm able to specify it.<\/p>\n<p>According to the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-custom-models?authuser=5&amp;_ga=2.16305585.-680038964.1635267137#formatting-prediction-input\" rel=\"nofollow noreferrer\">vertex documentation<\/a>, we must pass a dictionary containing the Instances (List) and the Parameters (Dict) values.<\/p>\n<p>I've submitted these arguments to <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/samples\/snippets\/prediction_service\/predict_custom_trained_model_sample.py\" rel=\"nofollow noreferrer\">this function<\/a>:<\/p>\n<pre><code>instances: [{&quot;argument_n&quot;: &quot;value&quot;}]\n\nparameters: {&quot;signature_name&quot;: &quot;name_of_signature&quot;}\n<\/code><\/pre>\n<p>Doesn't work, it still get the default signature of the model.<\/p>\n<p>In GCP AI Platform, I've been able to predict directly specifying in the body of the request the signature name:<\/p>\n<pre><code>response = service.projects().predict(\n        name=name,\n        body={&quot;instances&quot;: instances,\n        &quot;signature_name&quot;: &quot;name_of_signature&quot;},\n    ).execute()\n<\/code><\/pre>\n<p>@EDIT\nI've discovered that with the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.endpoints\/rawPredict\" rel=\"nofollow noreferrer\">rawPredict method<\/a> from gcloud it works.<\/p>\n<p>Here is an example:<\/p>\n<pre><code>!gcloud ai endpoints raw-predict {endpoint} --region=us-central1 \\\n--request='{&quot;signature_name&quot;:&quot;name_of_the_signature&quot;, \\\n&quot;instances&quot;: [{&quot;instance_0&quot;: [&quot;value_0&quot;], &quot;instance_1&quot;: [&quot;value_1&quot;]}]}'\n<\/code><\/pre>\n<p>Unfortunately, looking at <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/google\/cloud\/aiplatform\/models.py\" rel=\"nofollow noreferrer\">google api models code<\/a> it only have the predict method, not the raw_predict. So I don't know if it's available through python sdk right now.<\/p>",
        "Challenge_closed_time":1636439088550,
        "Challenge_comment_count":4,
        "Challenge_created_time":1636138079960,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble specifying the signature name when predicting with a TensorFlow model deployed on Vertex AI platform using TFX Pipelines. They have tried passing a dictionary containing instances and parameters values, but it still gets the default signature of the model. However, they were able to predict directly by specifying the signature name in the body of the request when using GCP AI Platform. The user has also discovered that the rawPredict method from gcloud works, but it is not available through the Python SDK.",
        "Challenge_last_edit_time":1636203038128,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69857932",
        "Challenge_link_count":4,
        "Challenge_participation_count":5,
        "Challenge_readability":17.4,
        "Challenge_reading_time":30.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":83.6134972223,
        "Challenge_title":"Specify signature name on Vertex AI Predict",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":508.0,
        "Challenge_word_count":192,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1606605180560,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brazil",
        "Poster_reputation_count":98.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>Vertex AI is a newer platform with limitations that will be improved over time. \u201csignature_name\u201d can be added to HTTP JSON Payload in <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.endpoints\/rawPredict\" rel=\"nofollow noreferrer\">RawPredictRequest<\/a> or from <a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/ai\/endpoints\/raw-predict\" rel=\"nofollow noreferrer\">gcloud<\/a> as you have done but right now this is not available in regular predict requests.<\/p>\n<p><strong>Using HTTP JSON payload :<\/strong><\/p>\n<p>Example:<\/p>\n<p>input.json :<\/p>\n<pre><code>{\n   &quot;instances&quot;: [\n     [&quot;male&quot;, 29.8811345124283, 26.0, 1, &quot;S&quot;, &quot;New York, NY&quot;, 0, 0],\n     [&quot;female&quot;, 48.0, 39.6, 1, &quot;C&quot;, &quot;London \/ Paris&quot;, 0, 1]],\n \n     &quot;signature_name&quot;: &lt;string&gt;\n}\n\n<\/code><\/pre>\n<pre><code>curl \\\n-X POST \\\n-H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\\n-H &quot;Content-Type: application\/json&quot; \\\nhttps:\/\/us-central1-aiplatform.googleapis.com\/v1\/projects\/${PROJECT_ID}\/locations\/us-central1\/endpoints\/${ENDPOINT_ID}:rawPredict \\\n-d &quot;@input.json&quot;\n\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1636453108543,
        "Solution_link_count":3.0,
        "Solution_readability":16.9,
        "Solution_reading_time":15.97,
        "Solution_score_count":3.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":96.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1583123749267,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Washington D.C., DC, USA",
        "Answerer_reputation_count":317.0,
        "Answerer_view_count":35.0,
        "Challenge_adjusted_solved_time":45.4293536111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When testing on a local machine in Python I would normally use the following to read a training set with sub-directories of all the classes and files\/class:<\/p>\n\n<pre><code>train_path = r\"C:\\temp\\coins\\PCGS - Gold\\train\"\n\ntrain_batches = ImageDataGenerator().flow_from_directory(train_path, target_size=(100,100), classes=['0','1',2','3' etc...], batch_size=32)\n<\/code><\/pre>\n\n<p><strong>Found 4100 images belonging to 22 classes.<\/strong><\/p>\n\n<p>but on AWS SageMaker's Jupyter notebook I am now pulling the files from an S3 bucket.  I tried the following: <\/p>\n\n<pre><code>bucket = \"coinpath\"\n\ntrain_path = 's3:\/\/{}\/{}\/train'.format(bucket, \"v1\")   #note that the directory structure is coinpath\/v1\/train where coinpath is the bucket\n\ntrain_batches = ImageDataGenerator().flow_from_directory(train_path, target_size=(100,100), classes=\n['0','1',2','3' etc...], batch_size=32)\n<\/code><\/pre>\n\n<p>but I get: ** Found 0 images belonging to 22 classes.**<\/p>\n\n<p>Looking for some guidance on the right way to pull training data from S3.<\/p>",
        "Challenge_closed_time":1589768612456,
        "Challenge_comment_count":1,
        "Challenge_created_time":1589605066783,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing issues while transitioning a TensorFlow model from a local machine to AWS SageMaker. They are unable to read the S3 bucket and are receiving an error message stating that 0 images have been found belonging to 22 classes. The user is seeking guidance on the correct way to pull training data from S3.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61832086",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.9,
        "Challenge_reading_time":14.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":45.4293536111,
        "Challenge_title":"Having issues reading S3 bucket when transitioning a tensorflow model from local machine to AWS SageMaker",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":608.0,
        "Challenge_word_count":129,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1583123749267,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Washington D.C., DC, USA",
        "Poster_reputation_count":317.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>From <a href=\"https:\/\/stackoverflow.com\/questions\/54736505\/ideal-way-to-read-data-in-bucket-stored-batches-of-data-for-keras-ml-training-in\">Ideal way to read data in bucket stored batches of data for Keras ML training in Google Cloud Platform?<\/a> \"ImageDataGenerator.flow_from_directory() currently does not allow you to stream data directly from a GCS bucket. \"<\/p>\n\n<p>I had to download the image from S3 first.  This is best for latency reasons as well. <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.0,
        "Solution_reading_time":6.04,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":54.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1452222077950,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"China",
        "Answerer_reputation_count":2187.0,
        "Answerer_view_count":894.0,
        "Challenge_adjusted_solved_time":12.8401833334,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I use next command to save output results:<\/p>\n\n<pre><code>ws.datasets.add_from_dataframe(data, 'GenericCSV', 'output.csv', 'Uotput results')\n<\/code><\/pre>\n\n<p>where <code>ws<\/code> is <code>azureml.Workspace<\/code> object and <code>data<\/code> is <code>pandas.DataFrame<\/code>.<\/p>\n\n<p>It works fine if my dataset size less than 4 mb. Otherwise I got a error:<\/p>\n\n<pre><code>AzureMLHttpError: Maximum request length exceeded.\n<\/code><\/pre>\n\n<p>As I understood this is the error raised by Azure environment limits and the maximum size of the dataset could not be changed. <\/p>\n\n<p>I could split my dataset to 4 mb parts and download them from Azure ML studio, but it is very inconvinient if size of my output dataset is more than 400 mb.<\/p>",
        "Challenge_closed_time":1457935312407,
        "Challenge_comment_count":3,
        "Challenge_created_time":1457889087747,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to save a dataset from an ipython notebook in Azure ML Studio using a command that works fine for datasets less than 4 mb, but encounters an error \"Maximum request length exceeded\" for larger datasets. The user is unable to change the maximum size limit and splitting the dataset into smaller parts is inconvenient for larger datasets.",
        "Challenge_last_edit_time":1457963855452,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35973168",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":6.9,
        "Challenge_reading_time":10.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":12.8401833334,
        "Challenge_title":"How could I save dataset from ipython notebook in Azure ML Studio?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3128.0,
        "Challenge_word_count":112,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1452426675696,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":77.0,
        "Poster_view_count":21.0,
        "Solution_body":"<p>I have read the source code in the python package <strong>azureml<\/strong>, and found out that they are using a simple request post when uploading a dataset, which has a limited content length 4194304 bytes.<\/p>\n\n<p>I tried to modify the code inside \"http.py\" within the python package <strong>azureml<\/strong>. I posted the request with a chunked data, and I got the following error:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \".\\azuremltest.py\", line 10, in &lt;module&gt;\n    ws.datasets.add_from_dataframe(frame, 'GenericCSV', 'output2.csv', 'Uotput results')\n  File \"C:\\Python34\\lib\\site-packages\\azureml\\__init__.py\", line 507, in add_from_dataframe\n    return self._upload(raw_data, data_type_id, name, description)\n  File \"C:\\Python34\\lib\\site-packages\\azureml\\__init__.py\", line 550, in _upload\nraw_data, None)\n  File \"C:\\Python34\\lib\\site-packages\\azureml\\http.py\", line 135, in upload_dataset\n    upload_result = self._send_post_req(api_path, raw_data)\n  File \"C:\\Python34\\lib\\site-packages\\azureml\\http.py\", line 197, in _send_post_req\n    raise AzureMLHttpError(response.text, response.status_code)\nazureml.errors.AzureMLHttpError: Chunked transfer encoding is not permitted. Upload size must be indicated in the Content-Length header.\nRequest ID: 7b692d82-845c-4106-b8ec-896a91ecdf2d 2016-03-14 04:32:55Z\n<\/code><\/pre>\n\n<p>The REST API in <strong>azureml<\/strong> package does not support chunked transfer encoding. Hence, I took a look at how the Azure ML studio implements this, and I found out this:<\/p>\n\n<ol>\n<li><p>It post a request with content-length=0 to <code>https:\/\/studioapi.azureml.net\/api\/resourceuploads\/workspaces\/&lt;workspace_id&gt;\/?userStorage=true&amp;dataTypeId=GenericCSV<\/code>, which will return an id in the response body.<\/p><\/li>\n<li><p>Break the .csv file into chunks less than 4194304 bytes, and post them to <code>https:\/\/studioapi.azureml.net\/api\/blobuploads\/workspaces\/&lt;workspace_id&gt;\/?numberOfBlocks=&lt;the number of chunks&gt;&amp;blockId=&lt;index of chunk&gt;&amp;uploadId=&lt;the id you get from previous request&gt;&amp;dataTypeId=GenericCSV<\/code><\/p><\/li>\n<\/ol>\n\n<p>If you really want this functionality, you can implement it with python and the above REST API.<\/p>\n\n<p>If you think it's too complicated, report the issue to <a href=\"https:\/\/github.com\/Azure\/Azure-MachineLearning-ClientLibrary-Python\/issues\" rel=\"nofollow\">this<\/a>. The <strong>azureml<\/strong> python package is still under development, so your suggestion would be very helpful for them.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1457939067983,
        "Solution_link_count":3.0,
        "Solution_readability":11.2,
        "Solution_reading_time":33.05,
        "Solution_score_count":4.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":258.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":40.1701361111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to share a model registry completely between Dev and Prod environment? So my idea is to create 10000 models in dev and maybe select 2000 from there to work in prod. I am planning to use AWS model registry. So if I do the training and testing and hyperparameter tuning in my AWS dev environment, is it possible to then share the registry in prod? The obvious reason is that it does not make sense to use the prod to do the training and testing again.<\/p>\n<p>Please advise!<\/p>\n<p>Thanks in advance!<\/p>",
        "Challenge_closed_time":1638378803110,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638234190620,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of sharing a model registry between their development and production environments using AWS model registry. They want to create 10,000 models in dev and select 2,000 for use in prod without having to repeat the training and testing process in prod. They are seeking advice on how to accomplish this.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70163094",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":6.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":40.1701361111,
        "Challenge_title":"SageMaker Model Registry Sharing",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":229.0,
        "Challenge_word_count":97,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1557333597230,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":99.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>It depends how you define Dev and Prod.<\/p>\n<ul>\n<li><p>If by Dev and Prod you mean different AWS account (which is a good practice - see <a href=\"https:\/\/docs.aws.amazon.com\/whitepapers\/latest\/organizing-your-aws-environment\/benefits-of-using-multiple-aws-accounts.html\" rel=\"nofollow noreferrer\">doc<\/a> and <a href=\"https:\/\/aws.amazon.com\/blogs\/devops\/aws-building-a-secure-cross-account-continuous-delivery-pipeline\/\" rel=\"nofollow noreferrer\">blog<\/a>), you cannot share fractions of a model registry from a given account to another account, but you can create triggers to export models from one model registry to another, as documented in this blog post <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/patterns-for-multi-account-hub-and-spoke-amazon-sagemaker-model-registry\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/patterns-for-multi-account-hub-and-spoke-amazon-sagemaker-model-registry\/<\/a><\/p>\n<\/li>\n<li><p>If your Dev and Prod live in the same AWS account and you are just looking for ways to differentiate them, you can use:<\/p>\n<ul>\n<li>Model Registry Status information<\/li>\n<li>Tags<\/li>\n<\/ul>\n<\/li>\n<\/ul>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":19.7,
        "Solution_reading_time":15.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":107.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":85.3072222222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use mlflow package in databricks to save the model into Azure Storage.<\/p>\n<p>The Script:<\/p>\n<p><code>abfss_path='abfss:\/\/mlops@dlsgdpeasdev03.dfs.core.windows.net'<\/code><\/p>\n<p><code>project = 'test'<\/code><\/p>\n<p><code>model_version = 'v1.0.1'<\/code><\/p>\n<p><code>model = {model training step}<\/code>  <br \/>\n<code>prefix_model_path = os.path.join(abfss_path, project, model_version)<\/code><\/p>\n<p><code>model_path = prefix_model_path<\/code><\/p>\n<p><code>print(model_path) # <\/code>abfss:\/\/mlops@dlsgdpeasdev03.dfs.core.windows.net\/test\/v1.0.1<\/p>\n<p><code>mlflow.sklearn.save_model(model, model_path)<\/code><\/p>\n<p>The message is successfully save the model. <\/p>\n<p>When I check the container and file does not exist, but I am able to load model with the same path. That mean the model file saved in databricks somewhere.<\/p>\n<p>I want to know where is the model file in databricks, and how to save the model directly from databricks notebook to Azure Storage.<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1673889987903,
        "Challenge_comment_count":1,
        "Challenge_created_time":1673582881903,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to save an ml model into Azure Storage Container using the mlflow package in Databricks. Although the message shows that the model is saved successfully, the user cannot find the file in the container. The user wants to know where the model file is saved in Databricks and how to save the model directly from the Databricks notebook to Azure Storage.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1160457\/how-databrick-save-ml-model-into-azure-storage-con",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.1,
        "Challenge_reading_time":13.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":85.3072222222,
        "Challenge_title":"How databrick save ml model into Azure Storage Container?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":109,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=ff486681-dc42-4832-92a7-87a687f2ec26\">@Benny Lau ,Shui Hong - Group Office  <\/a>, <\/p>\n<p>Thanks for the ask and welcome to Microsoft Q&amp;A . <\/p>\n<p>As I understand the ask here is to where the model is saved and how you can save to the blob . <\/p>\n<p>As per the document here : [https:\/\/learn.microsoft.com\/en-us\/azure\/databricks\/mlflow\/models#api-commands<\/p>\n<p>You have three option and I assume that your  model file is getting stored in the DBFS on the Azure databricks cluster .<\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/4d409166-ee35-4252-9294-6e7a4140ffab?platform=QnA\" alt=\"User's image\" \/><\/p>\n<p>Databricks can save a machine learning model to an Azure Storage Container using the <strong><code>dbutils.fs<\/code><\/strong> module. This module provides a set of functions for interacting with the Databricks file system (DBFS) and Azure Blob Storage. Here is an example of how to save a model to an Azure Storage Container:<\/p>\n<ol>\n<li> First, you will need to mount the Azure Storage Container to DBFS, this can be done using the <strong><code>dbutils.fs.mount<\/code><\/strong> function.<\/li>\n<\/ol>\n<pre><code>dbutils.fs.mount(\n  source='wasbs:\/\/&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.blob.core.windows.net',\n  mount_point='\/mnt\/&lt;your-mount-point&gt;',\n  extra_configs={\n    &quot;fs.azure.account.auth.type&quot;: &quot;OAuth&quot;,\n    &quot;fs.azure.account.oauth.provider.type&quot;: &quot;org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider&quot;,\n    &quot;fs.azure.account.oauth2.client.id&quot;: &quot;&lt;your-client-id&gt;&quot;,\n    &quot;fs.azure.account.oauth2.client.secret&quot;: &quot;&lt;your-client-secret&gt;&quot;,\n    &quot;fs.azure.account.oauth2.client.endpoint&quot;: &quot;https:\/\/login.microsoftonline.com\/&lt;your-tenant-id&gt;\/oauth2\/token&quot;\n  }\n)\n\n<\/code><\/pre>\n<ol>\n<li> Once the container is mounted, you can use the <strong><code>dbutils.fs.cp<\/code><\/strong> function to copy the model from the local file system to the mount point.<\/li>\n<\/ol>\n<p>dbutils.fs.cp(&quot;path\/to\/local\/model&quot;, &quot;\/mnt\/&lt;your-mount-point&gt;\/model&quot;)<\/p>\n<ol>\n<li> You can also use <strong><code>model.save()<\/code><\/strong> method to save the model in the mounted container path<\/li>\n<\/ol>\n<p>model.save(&quot;\/mnt\/&lt;your-mount-point&gt;\/model&quot;)<\/p>\n<p>Note: Be sure to replace the placeholders in the above code with the appropriate values for your use case.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.7,
        "Solution_reading_time":33.04,
        "Solution_score_count":0.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":231.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3547222222,
        "Challenge_answer_count":0,
        "Challenge_body":"#### Environment details\r\n\r\n  - OS: Mac M1 Pro\r\n  - Node.js version: v16.16.0\r\n  - npm version: 8.11.0\r\n  - `@google-cloud\/aiplatform` version: ^2.3.0\r\n\r\n#### Steps to reproduce\r\n\r\n  1. I've run this demo on my local computer: https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-text-classification.js\r\n  2. The process paused and shows `4 DEADLINE_EXCEEDED: Deadline exceeded` in the line: `await predictionServiceClient.predict(request);`\r\n\r\n\r\nThanks!\r\n",
        "Challenge_closed_time":1664935217000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1664933940000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered a bug where to_wandb is not sectioning by train\/test and overrides runs by checks. When running a suite with train\/test checks and duplicate checks in the suite, the expected behavior is to have sections for each dataset and be able to run a suite with a couple of checks.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/googleapis\/nodejs-ai-platform\/issues\/453",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":6.83,
        "Challenge_repo_contributor_count":20.0,
        "Challenge_repo_fork_count":16.0,
        "Challenge_repo_issue_count":561.0,
        "Challenge_repo_star_count":31.0,
        "Challenge_repo_watch_count":41.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.3547222222,
        "Challenge_title":"vertex AI endpoint prediction error, 4 DEADLINE_EXCEEDED: Deadline exceeded",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":53,
        "Discussion_body":"> \r\n\r\nWhen I upgrade the nodejs to v16.17.1 and add a call_option\r\n`\r\n      const call_options = {\r\n        timeout: 200000 \/\/ millis\r\n      }\r\n`\r\nproblem solved.\r\n",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1613062428296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":141.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":1.09315,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>We are struggling to model our data correctly for use in Kedro - we are using the recommended Raw\\Int\\Prm\\Ft\\Mst model but are struggling with some of the concepts....e.g.<\/p>\n<ul>\n<li>When is a dataset a feature rather than a primary dataset? The distinction seems vague...<\/li>\n<li>Is it OK for a primary dataset to consume data from another primary dataset?<\/li>\n<li>Is it good practice to build a feature dataset from the INT layer? or should it always pass through Primary?<\/li>\n<\/ul>\n<p>I appreciate there are no hard &amp; fast rules with data modelling but these are big modelling decisions &amp; any guidance or best practice on Kedro modelling would be really helpful, I can find just one table defining the layers in the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/12_faq\/01_faq.html#what-is-data-engineering-convention\" rel=\"nofollow noreferrer\">Kedro docs<\/a><\/p>\n<p>If anyone can offer any further advice or blogs\\docs talking about Kedro Data Modelling that would be awesome!<\/p>",
        "Challenge_closed_time":1623349806340,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623345871000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is struggling with data modeling for use in Kedro and has questions about the Raw\\Int\\Prm\\Ft\\Mst model, including when a dataset is a feature rather than a primary dataset, whether it's okay for a primary dataset to consume data from another primary dataset, and whether it's good practice to build a feature dataset from the INT layer. The user is seeking guidance and best practices for Kedro modeling.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67925860",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.7,
        "Challenge_reading_time":12.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":1.09315,
        "Challenge_title":"Kedro Data Modelling",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":180.0,
        "Challenge_word_count":145,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369054667740,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United Kingdom",
        "Poster_reputation_count":1445.0,
        "Poster_view_count":104.0,
        "Solution_body":"<p>Great question. As you say, there are no hard and fast rules here and opinions do vary, but let me share my perspective as a QB data scientist and kedro maintainer who has used the layering convention you referred to several times.<\/p>\n<p>For a start, let me emphasise that there's absolutely no reason to stick to the data engineering convention suggested by kedro if it's not suitable for your needs. 99% of users don't change the folder structure in <code>data<\/code>. This is not because the kedro default is the right structure for them but because they just don't think of changing it. You should absolutely add\/remove\/rename layers to suit yourself. The most important thing is to choose a set of layers (or even a non-layered structure) that works for your project rather than trying to shoehorn your datasets to fit the kedro default suggestion.<\/p>\n<p>Now, assuming you are following kedro's suggested structure - onto your questions:<\/p>\n<blockquote>\n<p>When is a dataset a feature rather than a primary dataset? The distinction seems vague...<\/p>\n<\/blockquote>\n<p>In the case of simple features, a feature dataset can be very similar to a primary one. The distinction is maybe clearest if you think about more complex features, e.g. formed by aggregating over time windows. A primary dataset would have a column that gives a cleaned version of the original data, but without doing any complex calculations on it, just simple transformations. Say the raw data is the colour of all cars driving past your house over a week. By the time the data is in primary, it will be clean (e.g. correcting &quot;rde&quot; to &quot;red&quot;, maybe mapping &quot;crimson&quot; and &quot;red&quot; to the same colour). Between primary and the feature layer, we will have done some less trivial calculations on it, e.g. to find one-hot encoded most common car colour each day.<\/p>\n<blockquote>\n<p>Is it OK for a primary dataset to consume data from another primary dataset?<\/p>\n<\/blockquote>\n<p>In my opinion, yes. This might be necessary if you want to join multiple primary tables together. In general if you are building complex pipelines it will become very difficult if you don't allow this. e.g. in the feature layer I might want to form a dataset containing <code>composite_feature = feature_1 * feature_2<\/code> from the two inputs <code>feature_1<\/code> and <code>feature_2<\/code>. There's no way of doing this without having multiple sub-layers within the feature layer.<\/p>\n<p>However, something that is generally worth avoiding is a node that consumes data from many different layers. e.g. a node that takes in one dataset from the feature layer and one from the intermediate layer. This seems a bit strange (why has the latter dataset not passed through the feature layer?).<\/p>\n<blockquote>\n<p>Is it good practice to build a feature dataset from the INT layer? or should it always pass through Primary?<\/p>\n<\/blockquote>\n<p>Building features from the intermediate layer isn't unheard of, but it seems a bit weird. The primary layer is typically an important one which forms the basis for all feature engineering. If your data is in a shape that you can build features then that means it's probably primary layer already. In this case, maybe you don't need an intermediate layer.<\/p>\n<p>The above points might be summarised by the following rules (which should no doubt be broken when required):<\/p>\n<ol>\n<li>The input datasets for a node in layer <code>L<\/code> should all be in the same layer, which can be either <code>L<\/code> or <code>L-1<\/code><\/li>\n<li>The output datasets for a node in layer <code>L<\/code> should all be in the same layer <code>L<\/code>, which can be either <code>L<\/code> or <code>L+1<\/code><\/li>\n<\/ol>\n<blockquote>\n<p>If anyone can offer any further advice or blogs\\docs talking about Kedro Data Modelling that would be awesome!<\/p>\n<\/blockquote>\n<p>I'm also interested in seeing what others think here! One possibly useful thing to note is that kedro was inspired by cookiecutter data science, and the kedro layer structure is an extended version of <a href=\"http:\/\/drivendata.github.io\/cookiecutter-data-science\/#directory-structure\" rel=\"nofollow noreferrer\">what's suggested there<\/a>. Maybe other projects have taken this directory structure and adapted it in different ways.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.2,
        "Solution_reading_time":53.65,
        "Solution_score_count":4.0,
        "Solution_sentence_count":37.0,
        "Solution_word_count":668.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1562055808543,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":895.0,
        "Answerer_view_count":53.0,
        "Challenge_adjusted_solved_time":3027.8199811111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to deploy a TF2.0 model to SageMaker. So far, I managed to train the model and save it into an S3 bucket but when I'm calling the <code>.deploy()<\/code> method, I get the following error from cloud Watch <\/p>\n\n<p><code>ValueError: no SavedModel bundles found!<\/code><\/p>\n\n<p>Here is my training script: <\/p>\n\n<pre><code>### Code to add in a tensorflow_estimator.py file\n\nimport argparse\nimport os\nimport pathlib\nimport tensorflow as tf\n\n\nif __name__ == '__main__':\n\n\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch_size', type=int, default=100)\n    parser.add_argument('--learning_rate', type=float, default=0.1)\n\n    # Data, model, and output directories\n    parser.add_argument('--output-data-dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n    args, _ = parser.parse_known_args()\n\n    print(\"##### ARGS ##### \\n{}\".format(args))\n\n    # Get files \n    path = pathlib.Path(args.train)\n\n    # Print out folder content \n    for item in path.iterdir():\n        print(\"##### DIRECTORIES ##### \\n{}\".format(item))\n\n    # Get all images \n    all_images = list(path.glob(\"*\/*\"))\n    all_image_paths = [str(path) for path in list(path.glob(\"*\/*\"))]\n\n    # Transform images into tensors\n    def preprocess_and_load_images(path):\n        image = tf.io.read_file(path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.resize(image, [192, 192])\n        return image\n\n    # Apply preprocessing function\n    ds_paths = tf.data.Dataset.from_tensor_slices(all_image_paths)\n    ds_images = ds_paths.map(preprocess_and_load_images)\n\n    # Map Labels\n    labels = []\n    for data in path.iterdir():  \n        if data.is_dir():               \n            labels += [data.name]    \n\n    labels_index = {}\n    for i,label in enumerate(labels):\n        labels_index[label]=i\n\n    print(\"##### Label Index ##### \\n{}\".format(labels_index))\n\n    all_image_labels = [labels_index[path.parent.name] for path in list(path.glob(\"*\/*\"))]\n\n    # Create a tf Dataset\n    labels_ds = tf.data.Dataset.from_tensor_slices(all_image_labels)\n\n    # Zip train and labeled dataset\n    full_ds = tf.data.Dataset.zip((ds_images, labels_ds))\n\n    # Shuffle Dataset and batch it \n    full_ds = full_ds.shuffle(len(all_images)).batch(args.batch_size)\n\n    # Create a pre-trained model \n    base_model = tf.keras.applications.InceptionV3(input_shape=(192,192,3), \n                                               include_top=False,\n                                               weights = \"imagenet\"\n                                               )\n\n    base_model.trainable = False\n    model = tf.keras.Sequential([\n                base_model,\n                tf.keras.layers.GlobalAveragePooling2D(),\n                tf.keras.layers.Dense(len(labels), activation=\"softmax\")\n            ])\n\n    initial_learning_rate = args.learning_rate\n\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate,\n        decay_steps=1000,\n        decay_rate=0.96,\n        staircase=True) \n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr_schedule),\n              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\n\n    # Train the model\n    model.fit(full_ds, epochs=args.epochs)\n\n    # Save the model \n    model.save(os.path.join(args.model_dir, \"tf_model\"), save_format=\"tf\")\n\ndef model_fn(model_dir):\n    classifier = tf.keras.models.load_model(os.path.join(model_dir, \"tf_model\"))\n    return classifier\n<\/code><\/pre>\n\n<p>And here is the code that I wrote into Colab <\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ntf_estimator = TensorFlow(entry_point='tensorflow_estimator.py', \n                          role=role,\n                          train_instance_count=1, \n                          train_instance_type='ml.m5.large',\n                          framework_version='2.0.0', \n                          sagemaker_session=sagemaker_session,\n                          output_path=s3_output_location,\n                          hyperparameters={'epochs': 1,\n                                           'batch_size': 30,\n                                           'learning_rate': 0.001},\n                          py_version='py3')\n\n\ntf_estimator.fit({\"train\":train_data})\n\nfrom sagemaker.tensorflow.serving import Model\n\nmodel = Model(model_data='s3:\/\/path\/to\/model.tar.gz', \n              role=role,\n              framework_version=\"2.0.0\",\n              sagemaker_session=sagemaker_session)\n\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.m5.large')\n<\/code><\/pre>\n\n<p>I already tried to look at <a href=\"https:\/\/stackoverflow.com\/questions\/57172147\/no-savedmodel-bundles-found-on-tensorflow-hub-model-deployment-to-aws-sagemak\">this thread<\/a> but I actually don't have the problem of versions in my tar.gz file as the structure is the following : <\/p>\n\n<pre><code>\u251c\u2500\u2500 assets\n\u251c\u2500\u2500 saved_model.pb\n\u2514\u2500\u2500 variables\n    \u251c\u2500\u2500 variables.data-00000-of-00001\n    \u2514\u2500\u2500 variables.index\n<\/code><\/pre>\n\n<p>I feel I might be wrong when defining <code>model_fn()<\/code> in my training script but definitely don't what to replace that with. Would you guys have an idea? <\/p>\n\n<p>Thanks a lot for your help!  <\/p>",
        "Challenge_closed_time":1579799847967,
        "Challenge_comment_count":1,
        "Challenge_created_time":1579796394240,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering a \"ValueError: no SavedModel bundles found!\" error when trying to deploy a TensorFlow 2.0 model to SageMaker. They have successfully trained and saved the model to an S3 bucket, but are encountering issues when calling the .deploy() method. The user has provided their training script and deployment code, and has already checked for version issues in their tar.gz file. They suspect that the issue may be with their model_fn() function but are unsure of how to proceed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59882941",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":16.2,
        "Challenge_reading_time":64.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":68,
        "Challenge_solved_time":0.9593686111,
        "Challenge_title":"ValueError: no SavedModel bundles found! when trying to deploy a TF2.0 model to SageMaker",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":943.0,
        "Challenge_word_count":395,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1562055808543,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":895.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>I actually tried to modify my training script to the following : <\/p>\n\n<pre><code>### Code to add in a tensorflow_estimator.py file\n\nimport argparse\nimport os\nimport pathlib\nimport tensorflow as tf\n\n\nif __name__ == '__main__':\n\n\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch_size', type=int, default=100)\n    parser.add_argument('--learning_rate', type=float, default=0.1)\n\n    # Data, model, and output directories\n    parser.add_argument('--output-data-dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n    args, _ = parser.parse_known_args()\n\n    print(\"##### ARGS ##### \\n{}\".format(args))\n\n    # Get files \n    path = pathlib.Path(args.train)\n\n    # Print out folder content \n    for item in path.iterdir():\n        print(\"##### DIRECTORIES ##### \\n{}\".format(item))\n\n    # Get all images \n    all_images = list(path.glob(\"*\/*\"))\n    all_image_paths = [str(path) for path in list(path.glob(\"*\/*\"))]\n\n    # Transform images into tensors\n    def preprocess_and_load_images(path):\n        image = tf.io.read_file(path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.resize(image, [192, 192])\n        return image\n\n    # Apply preprocessing function\n    ds_paths = tf.data.Dataset.from_tensor_slices(all_image_paths)\n    ds_images = ds_paths.map(preprocess_and_load_images)\n\n    # Map Labels\n    labels = []\n    for data in path.iterdir():  \n        if data.is_dir():               \n            labels += [data.name]    \n\n    labels_index = {}\n    for i,label in enumerate(labels):\n        labels_index[label]=i\n\n    print(\"##### Label Index ##### \\n{}\".format(labels_index))\n\n    all_image_labels = [labels_index[path.parent.name] for path in list(path.glob(\"*\/*\"))]\n\n    # Create a tf Dataset\n    labels_ds = tf.data.Dataset.from_tensor_slices(all_image_labels)\n\n    # Zip train and labeled dataset\n    full_ds = tf.data.Dataset.zip((ds_images, labels_ds))\n\n    # Shuffle Dataset and batch it \n    full_ds = full_ds.shuffle(len(all_images)).batch(args.batch_size)\n\n    # Create a pre-trained model \n    base_model = tf.keras.applications.InceptionV3(input_shape=(192,192,3), \n                                               include_top=False,\n                                               weights = \"imagenet\"\n                                               )\n\n    base_model.trainable = False\n    model = tf.keras.Sequential([\n                base_model,\n                tf.keras.layers.GlobalAveragePooling2D(),\n                tf.keras.layers.Dense(len(labels), activation=\"softmax\")\n            ])\n\n    initial_learning_rate = args.learning_rate\n\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate,\n        decay_steps=1000,\n        decay_rate=0.96,\n        staircase=True) \n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr_schedule),\n              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\n\n    # Train the model\n    model.fit(full_ds, epochs=args.epochs)\n\n    # Save the model \n    model.save(os.path.join(args.model_dir, \"tensorflow_model\/1\"), save_format=\"tf\")\n\n<\/code><\/pre>\n\n<p>It seems that it's important to have a numerical name for your folder:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Save the model\nmodel.save(os.path.join(args.model_dir, \"tensorflow_model\/1\"), save_format=\"tf\")\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1590696546172,
        "Solution_link_count":0.0,
        "Solution_readability":18.3,
        "Solution_reading_time":44.06,
        "Solution_score_count":4.0,
        "Solution_sentence_count":46.0,
        "Solution_word_count":244.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":162.8144655556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run a autoML model as follows:<\/p>\n<p>automl_settings = {  <br \/>\n&quot;n_cross_validations&quot;: 5  <br \/>\n}<\/p>\n<p>automl_config = AutoMLConfig(task = 'regression',  <br \/>\ncompute_target = compute_target,  <br \/>\ntraining_data = train_data.filter(train_data['location']==l),  <br \/>\nlabel_column_name = label,  <br \/>\n**automl_settings)<\/p>\n<p>remote_run = experiment.submit(automl_config, show_output=True)<\/p>\n<p>And I get: ValidationException: The data points should have at least 50 rows for a valid regression or classification task with cv 5.<\/p>\n<p>The data has more than 250 rows. Moreover, when I changed the cv number, nothing changed. Does anybody have any idea what might be happening?<\/p>",
        "Challenge_closed_time":1630572887363,
        "Challenge_comment_count":1,
        "Challenge_created_time":1629986755287,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a ValidationException while trying to run an autoML model for regression. The error message states that the data points should have at least 50 rows for a valid regression or classification task with cv 5. The user has more than 250 rows of data and changing the cv number did not resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/529189\/validationexception-the-data-points-should-have-at",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.4,
        "Challenge_reading_time":10.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":162.8144655556,
        "Challenge_title":"ValidationException: The data points should have at least 50 rows for a valid regression or classification task with cv 5.",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":100,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello,  <\/p>\n<p>Hope you have solved this issue. If you are still blocked by this, please feel free to let us know. We can either investigate deeper if we can have more details, or we can help you to enable a support ticket if you do not have a support plan. Thanks.  <\/p>\n<p>Regards,  <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.2,
        "Solution_reading_time":3.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":56.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1251699930780,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Santa Clara, CA, United States",
        "Answerer_reputation_count":1538.0,
        "Answerer_view_count":198.0,
        "Challenge_adjusted_solved_time":33.9835508333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been exploring using Vertex AI for my machine learning workflows. Because deploying different models to the same endpoint utilizing only one node is not possible in Vertex AI, I am considering a <a href=\"https:\/\/stackoverflow.com\/questions\/69878915\/deploying-multiple-models-to-same-endpoint-in-vertex-ai\">workaround<\/a>. With this workaround, I will be unable to use many Vertex AI features, like model monitoring, feature attribution etc., and it simply becomes, I think, a managed alternative to running the prediction application on, say, a GKE cluster. So, besides the cost difference, I am exploring if running the custom prediction container on Vertex AI vs. GKE will involve any limitations, for example, only <strong>N1<\/strong> machine types are available for prediction in Vertex AI<\/p>\n<p>There is a similar <a href=\"https:\/\/stackoverflow.com\/questions\/67930882\/google-kubernetes-engine-vs-vertex-ai-ai-platform-unified-for-serving-model-pr\">question<\/a>, but I it does not raise the specific questions I hope to have answered.<\/p>\n<ul>\n<li>I am not sure of the available disk space. In Vertex AI, one can specify the machine type, such as n1-standard-2 etc., but I am not sure what disk space will be available and if\/how one can specify it? In the custom container code, I may copy multiple model artifacts, or data from outside sources to the local directory before processing them so understanding any disk space limitations is important.<\/li>\n<li>For custom training in Vertex AI, one can use an interactive shell to inspect the container where the training code is running, as described <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/monitor-debug-interactive-shell\" rel=\"nofollow noreferrer\">here<\/a>. Is something like this possible for a custom prediction container? I have not found anything in the docs.<\/li>\n<li>For custom training, one can use a private IP for custom training as described <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-private-ip\" rel=\"nofollow noreferrer\">here<\/a>. Again, I have not found anything similar for custom prediction in the docs, is it possible?<\/li>\n<\/ul>\n<p>If you know of any other possible limitations, please post.<\/p>",
        "Challenge_closed_time":1637122254800,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636999914017,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is exploring using Vertex AI for their machine learning workflows but is facing challenges with deploying different models to the same endpoint. They are considering a workaround but it will limit their use of Vertex AI features. The user is exploring if running the custom prediction container on Vertex AI vs. GKE will involve any limitations, such as available disk space, interactive shell access, and private IP usage. The user is seeking information on any other possible limitations.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69978953",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":29.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":33.9835508333,
        "Challenge_title":"Vertex AI custom prediction vs Google Kubernetes Engine",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":304.0,
        "Challenge_word_count":299,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1471292986790,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":700.0,
        "Poster_view_count":90.0,
        "Solution_body":"<ol>\n<li>we don't specify a disk size, so default to 100GB<\/li>\n<li>I'm not aware of this right now. But if it's a custom container, you could just run it locally or on GKE for debugging purpose.<\/li>\n<li>are you looking for this? <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/using-private-endpoints\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/using-private-endpoints<\/a><\/li>\n<\/ol>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.5,
        "Solution_reading_time":5.7,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":46.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1388815483776,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":100.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":474.1775913889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm having an issue to serve a model with reference to model registry. According to help, the path should look like this: <\/p>\n\n<p>models:\/model_name\/stage<\/p>\n\n<p>When I type in terminal: <br>\n<code>mlflow models serve -m models:\/ml_test_model1\/Staging --no-conda -h 0.0.0.0 -p 5003<\/code><\/p>\n\n<p>I got the error: <br>\n<code>mlflow.exceptions.MlflowException: Not a proper models:\/ URI: models:\/ml_test_model1\/Staging\/MLmodel. Models URIs must be of the form 'models:\/&lt;model_name&gt;\/&lt;version or stage&gt;'.<\/code><\/p>\n\n<p>Model is registered and visible in db and server. <br> \nIf I put absolute path, it works (experiment_id\/run_id\/artifacts\/model_name).<\/p>\n\n<p>mlflow version: 1.4 <br>\nPython version: 3.7.3<\/p>\n\n<p>Is it matter of some environmental settings or something different?<\/p>",
        "Challenge_closed_time":1577232243980,
        "Challenge_comment_count":0,
        "Challenge_created_time":1575544859307,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while serving a model with reference to the model registry using MLflow. The error message suggests that the URI format is incorrect and should be in the form of 'models:\/<model_name>\/<version or stage>'. The model is registered and visible in the database and server, and the user is able to serve the model using an absolute path. The user is using MLflow version 1.4 and Python version 3.7.3 and is unsure if this is an environmental setting issue or something else.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59194004",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.4,
        "Challenge_reading_time":10.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":468.7179647222,
        "Challenge_title":"MLflow - Serving model by reference to model registry",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1225.0,
        "Challenge_word_count":104,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1575543357812,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>That style of referencing model artefacts is fixed from mlflow v1.5 (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/pull\/2067\" rel=\"nofollow noreferrer\">Bug Fix<\/a>).<\/p>\n\n<p>You'll need to run <code>mlflow db upgrade &lt;db uri&gt;<\/code> to refresh your schemas before restarting your mlflow server.<\/p>\n\n<p>You may find listing registered models helpful:<\/p>\n\n<p><code>&lt;server&gt;:&lt;port&gt;\/api\/2.0\/preview\/mlflow\/registered-models\/list<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1577251898636,
        "Solution_link_count":1.0,
        "Solution_readability":10.1,
        "Solution_reading_time":6.02,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":42.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":31.5461152778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to save images that I configure during training to the output bucket in sagemaker.  I've read that all the information that needs to be saved during training goes into the model.tar.gz file.  I've tried saving plots using the model_dir and the output_data_dir to no avail.  The model itself is saved properly, but the additional information is not being stored with it.  I want to reload this additional information (the saved images) during inference but have heard that storing all the information in the model.tar.gz can cause slow inference.  I would love some help.<\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>from sagemaker.pytorch import PyTorch\nestimator = PyTorch(entry_point='XXXXXXXX\/AWS\/mnist.py',\n                    role=role,\n                    py_version='py3',\n                    framework_version='1.8.0',\n                    instance_count=1,\n                    instance_type='ml.c5.xlarge',\n                    output_path='s3:\/\/XXXXX-bucket\/',\n                    )<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code># mnist.py\nimport os\nimport pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nimport argparse\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import ToTensor\nfrom torchvision.io import read_image\nfrom torch import nn\nimport matplotlib.pyplot as plt\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\n\ndef train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X.to(device))\n        loss = loss_fn(pred, y.to(device))\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}\/{size:&gt;5d}]\")\n\n\ndef test_loop(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X.to(device))\n            test_loss += loss_fn(pred, y.to(device)).item()\n            correct += (pred.argmax(1) == y.to(device)).type(torch.float).sum().item()\n\n    test_loss \/= num_batches\n    correct \/= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")\n\n# Initialize the loss function\nif __name__=='__main__':\n    # default to the value in environment variable `SM_MODEL_DIR`. Using args makes the script more portable.\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n\n    args, _ = parser.parse_known_args()\n\n    training_data = datasets.FashionMNIST(\n        root=\"data\",\n        train=True,\n        download=True,\n        transform=ToTensor()\n    )\n\n    test_data = datasets.FashionMNIST(\n        root=\"data\",\n        train=False,\n        download=True,\n        transform=ToTensor()\n    )\n\n    labels_map = {\n        0: \"T-Shirt\",\n        1: \"Trouser\",\n        2: \"Pullover\",\n        3: \"Dress\",\n        4: \"Coat\",\n        5: \"Sandal\",\n        6: \"Shirt\",\n        7: \"Sneaker\",\n        8: \"Bag\",\n        9: \"Ankle Boot\",\n    }\n\n    figure = plt.figure(figsize=(8, 8))\n    cols, rows = 3, 3\n    for i in range(1, cols * rows + 1):\n        sample_idx = torch.randint(len(training_data), size=(1,)).item()\n        img, label = training_data[sample_idx]\n        figure.add_subplot(rows, cols, i)\n        plt.title(labels_map[label])\n        plt.axis(\"off\")\n        plt.imsave(args.output_data_dir+'plot'+str(i)+'.jpg', img.squeeze(), cmap=\"gray\")\n\n    train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n    test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n\n    # Display image and label.\n    train_features, train_labels = next(iter(train_dataloader))\n    print(f\"Feature batch shape: {train_features.size()}\")\n    print(f\"Labels batch shape: {train_labels.size()}\")\n    img = train_features[0].squeeze()\n    label = train_labels[0]\n    plt.imsave(args.output_data_dir+'sample.jpg', img, cmap=\"gray\")\n    print(\"Saved img.\")\n    print(f\"Label: {label}\")\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using {device} device\")\n\n    model = NeuralNetwork().to(device)\n    print(model)\n\n    learning_rate = 1e-3\n    batch_size = 64\n    epochs = 5\n    # ... train `model`, then save it to `model_dir`\n    \n\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n    epochs = 1\n    for t in range(epochs):\n        print(f\"Epoch {t+1}\\n-------------------------------\")\n        train_loop(train_dataloader, model, loss_fn, optimizer)\n        test_loop(test_dataloader, model, loss_fn)\n    print(\"Done!\")\n\n    \n\n\n\n    with open(os.path.join(args.model_dir, 'model.pth'), 'wb') as f:\n        torch.save(model.state_dict(), f)\n        plt.plot([1,2,3,4])\n        plt.ylabel('some numbers')\n        plt.show()\n        plt.savefig('test.jpeg')<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>",
        "Challenge_closed_time":1661165052743,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661051216633,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to save images during training in Sagemaker, but is unable to do so using the model_dir and output_data_dir. They want to reload this additional information during inference, but storing all the information in the model.tar.gz can cause slow inference. The user has provided a code snippet using PyTorch to train a neural network and save the model.",
        "Challenge_last_edit_time":1661051486728,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73431378",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":68.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":63,
        "Challenge_solved_time":31.6211416667,
        "Challenge_title":"Save images from sagemaker training",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":43.0,
        "Challenge_word_count":478,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1510598465376,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Francisco, CA, United States",
        "Poster_reputation_count":1.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I suspect there is an issue with string concatenation in <code>plt.imsave<\/code> because the environment variable <code>SM_OUTPUT_DATA_DIR<\/code> by default points to <code>\/opt\/ml\/output\/data<\/code> (that's the actual value of <code>args.output_data_dir<\/code>, since you don't pass this parameter) so the outcome is something like <code>\/opt\/ml\/output\/dataplot1.jpg<\/code>. The same happen if you use the <code>model_dir<\/code> in the same way. I'd rather use something like <code>os.path.join<\/code> like you're already doing for the model. <a href=\"https:\/\/nono.ma\/sagemaker-model-dir-output-dir-and-output-data-dir-parameters\" rel=\"nofollow noreferrer\">here<\/a> a nice exaplaination about these folders and environment variables in sagemaker.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.3,
        "Solution_reading_time":9.92,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1525393797416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Philadelphia, USA",
        "Answerer_reputation_count":130.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":0.4764786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created an mlflow model with custom pyfunc. It shows the results when I send input to the loaded model in Jupyter notebook.\nHowever if I am trying to serve it to a local port<\/p>\n<pre><code>!mlflow models serve -m Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model -p 8001\n<\/code><\/pre>\n<p>I am getting this error<\/p>\n<pre><code> Traceback (most recent call last):\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/bin\/mlflow&quot;, line 10, in &lt;module&gt;\n    sys.exit(cli())\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 829, in __call__\n    return self.main(*args, **kwargs)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 782, in main\n    rv = self.invoke(ctx)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 610, in invoke\n    return callback(*args, **kwargs)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/models\/cli.py&quot;, line 56, in serve\n    install_mlflow=install_mlflow).serve(model_uri=model_uri, port=port,\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/models\/cli.py&quot;, line 163, in _get_flavor_backend\n    append_to_uri_path(underlying_model_uri, &quot;MLmodel&quot;), output_path=tmp.path())\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/tracking\/artifact_utils.py&quot;, line 76, in _download_artifact_from_uri\n    artifact_path=artifact_path, dst_path=output_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/local_artifact_repo.py&quot;, line 67, in download_artifacts\n    return super(LocalArtifactRepository, self).download_artifacts(artifact_path, dst_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py&quot;, line 140, in download_artifacts\n    return download_file(artifact_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py&quot;, line 105, in download_file\n    self._download_file(remote_file_path=fullpath, local_path=local_file_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/local_artifact_repo.py&quot;, line 95, in _download_file\n    shutil.copyfile(remote_file_path, local_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/shutil.py&quot;, line 120, in copyfile\n    with open(src, 'rb') as fsrc:\nFileNotFoundError: [Errno 2] No such file or directory: 'Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model\/MLmodel'\n<\/code><\/pre>",
        "Challenge_closed_time":1611840603100,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611838887777,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to serve an mlflow model locally and is encountering a \"FileNotFoundError\" when trying to run the model on a local port. The error message suggests that the file or directory specified in the model URI does not exist.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65937623",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":23.6,
        "Challenge_reading_time":47.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":0.4764786111,
        "Challenge_title":"Unable to serve an mlflow model locally",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1277.0,
        "Challenge_word_count":199,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1573739890560,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>From your error traceback, the model artifact can't be located. In your code, you are executing the 'mlflow' command from within a Jupyter Notebook. I would suggest trying the following:<\/p>\n<ol>\n<li>Check if your models artifacts are on the path you are using Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model<\/li>\n<li>Try opening a terminal, then <code>cd \/Home\/miniconda3\/envs<\/code> and  execute <code>mlflow models serve -m .\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model -p 8001<\/code><\/li>\n<li>MLFlow offers different solutions to serve a model, you can try to register your model and refer to it as &quot;models:\/{model_name}\/{stage}&quot; as mentioned in the Model Registry <a href=\"https:\/\/mlflow.org\/docs\/latest\/model-registry.html#serving-an-mlflow-model-from-model-registry\" rel=\"nofollow noreferrer\">docs<\/a><\/li>\n<\/ol>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.2,
        "Solution_reading_time":11.53,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":92.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1646242327867,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":27.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":98.4473672222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a data that looks like this<\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Date<\/th>\n<th>Name<\/th>\n<th>SurveyID<\/th>\n<th>Score<\/th>\n<th>Error<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>2022-02-17<\/td>\n<td>Jack<\/td>\n<td>10<\/td>\n<td>95<\/td>\n<td>Name<\/td>\n<\/tr>\n<tr>\n<td>2022-02-17<\/td>\n<td>Jack<\/td>\n<td>10<\/td>\n<td>95<\/td>\n<td>Address<\/td>\n<\/tr>\n<tr>\n<td>2022-02-16<\/td>\n<td>Tom<\/td>\n<td>9<\/td>\n<td>100<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>2022-02-16<\/td>\n<td>Carl<\/td>\n<td>8<\/td>\n<td>93<\/td>\n<td>Zip<\/td>\n<\/tr>\n<tr>\n<td>2022-02-16<\/td>\n<td>Carl<\/td>\n<td>8<\/td>\n<td>93<\/td>\n<td>Email<\/td>\n<\/tr>\n<tr>\n<td>2022-02-15<\/td>\n<td>Dan<\/td>\n<td>7<\/td>\n<td>72<\/td>\n<td>Zip<\/td>\n<\/tr>\n<tr>\n<td>2022-02-15<\/td>\n<td>Dan<\/td>\n<td>7<\/td>\n<td>72<\/td>\n<td>Email<\/td>\n<\/tr>\n<tr>\n<td>2022-02-15<\/td>\n<td>Dan<\/td>\n<td>7<\/td>\n<td>72<\/td>\n<td>Name<\/td>\n<\/tr>\n<tr>\n<td>2022-02-15<\/td>\n<td>Dan<\/td>\n<td>6<\/td>\n<td>90<\/td>\n<td>Phone<\/td>\n<\/tr>\n<tr>\n<td>2022-02-14<\/td>\n<td>Tom<\/td>\n<td>5<\/td>\n<td>98<\/td>\n<td>Gender<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div>\n<p>I wanted to have a segmentation data using the avg. score per individual.<\/p>\n<pre><code>Segment\nA:  98%-100%\nB:  95%-97%\nC:  90%-94%\nD:  80%-89%\nE:  0% -79%\n<\/code><\/pre>\n<p>I did an if else formula which is this:<\/p>\n<pre><code>ifelse(Score} &gt;= 98,'A',ifelse({Score} &gt;= 95,'B',ifelse({Score} &gt;= 90,'C',ifelse({Score} &gt;= 80,'D','E'))))\n<\/code><\/pre>\n<p>This is now the output of what I did:<\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Date<\/th>\n<th>Name<\/th>\n<th>SurveyID<\/th>\n<th>Score<\/th>\n<th>Error<\/th>\n<th>Segement<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>2022-02-17<\/td>\n<td>Jack<\/td>\n<td>10<\/td>\n<td>95<\/td>\n<td>Name<\/td>\n<td>B<\/td>\n<\/tr>\n<tr>\n<td>2022-02-17<\/td>\n<td>Jack<\/td>\n<td>10<\/td>\n<td>95<\/td>\n<td>Address<\/td>\n<td>B<\/td>\n<\/tr>\n<tr>\n<td>2022-02-16<\/td>\n<td>Tom<\/td>\n<td>9<\/td>\n<td>100<\/td>\n<td><\/td>\n<td>A<\/td>\n<\/tr>\n<tr>\n<td>2022-02-16<\/td>\n<td>Carl<\/td>\n<td>8<\/td>\n<td>93<\/td>\n<td>Zip<\/td>\n<td>C<\/td>\n<\/tr>\n<tr>\n<td>2022-02-16<\/td>\n<td>Carl<\/td>\n<td>8<\/td>\n<td>93<\/td>\n<td>Email<\/td>\n<td>C<\/td>\n<\/tr>\n<tr>\n<td>2022-02-15<\/td>\n<td>Dan<\/td>\n<td>7<\/td>\n<td>72<\/td>\n<td>Zip<\/td>\n<td>E<\/td>\n<\/tr>\n<tr>\n<td>2022-02-15<\/td>\n<td>Dan<\/td>\n<td>7<\/td>\n<td>72<\/td>\n<td>Email<\/td>\n<td>E<\/td>\n<\/tr>\n<tr>\n<td>2022-02-15<\/td>\n<td>Dan<\/td>\n<td>7<\/td>\n<td>72<\/td>\n<td>Name<\/td>\n<td>E<\/td>\n<\/tr>\n<tr>\n<td>2022-02-15<\/td>\n<td>Dan<\/td>\n<td>6<\/td>\n<td>90<\/td>\n<td>Phone<\/td>\n<td>C<\/td>\n<\/tr>\n<tr>\n<td>2022-02-14<\/td>\n<td>Tom<\/td>\n<td>5<\/td>\n<td>98<\/td>\n<td>Gender<\/td>\n<td>A<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div>\n<p>I realized that the calculation I did only applies for the score. I was expecting an output like this:<\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Name<\/th>\n<th>Average Score<\/th>\n<th>Total Survey<\/th>\n<th>Segement<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>Jack<\/td>\n<td>95<\/td>\n<td>1<\/td>\n<td>B<\/td>\n<\/tr>\n<tr>\n<td>Tom<\/td>\n<td>99<\/td>\n<td>2<\/td>\n<td>A<\/td>\n<\/tr>\n<tr>\n<td>Carl<\/td>\n<td>93<\/td>\n<td>1<\/td>\n<td>C<\/td>\n<\/tr>\n<tr>\n<td>Dan<\/td>\n<td>81<\/td>\n<td>2<\/td>\n<td>D<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div>\n<p>I have tried to create another calculated field for Average Score which is:<\/p>\n<pre><code>avgOver({Score}, [Name], PRE_AGG)\n<\/code><\/pre>\n<p>I believe I am missing a distinct count of survey IDs in that formula, that I do not know where to place. As for segmentation calculation, I cannot on my life figure that part out without getting aggregation errors on Quicksight. Please help, thank you.<\/p>",
        "Challenge_closed_time":1655843015852,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655488605330,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a segmentation data using the average score per individual in AWS Quicksight. They have created an if-else formula for segmentation, but it only applies to the score and not the average score. They have also tried to create a calculated field for average score but are unsure where to place the distinct count of survey IDs. They are also encountering aggregation errors while trying to calculate segmentation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72663159",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":22.3,
        "Challenge_reading_time":47.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":98.4473672222,
        "Challenge_title":"AWS Quicksight - question about creating a calculated field using if else and custom aggregation",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":105.0,
        "Challenge_word_count":383,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1646242327867,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":27.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Got the answer from Quicksight Community. Pasting it here.<\/p>\n<p>For segmentation, you can use the calculated field which you created for average score .<\/p>\n<pre><code>avg_score = avgOver(Score,[Name],PRE_AGG)\n<\/code><\/pre>\n<p>Segment<\/p>\n<pre><code>ifelse\n(\n    {avg_score}&gt;= 98,'A',\n    {avg_score}&gt;= 95,'B',\n    {avg_score}&gt;= 90,'C',\n    {avg_score}&gt;= 80,'D',\n    'E'\n)\n<\/code><\/pre>\n<p>The survey id can be used to get the distinct count per individual.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.5,
        "Solution_reading_time":5.93,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":52.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2939025,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,  <\/p>\n<p>I'm aware that running <strong>compute Instance<\/strong> costs us for the number of hours we used. So, we stop it when ever we don't need it.  <\/p>\n<p>Similarly, does <strong>compute cluster<\/strong> &amp; <strong>Endpoints<\/strong> also costs us if we do not delete them after use?  <\/p>\n<p>Thanks  <br \/>\nBhaskar  <\/p>",
        "Challenge_closed_time":1617896011616,
        "Challenge_comment_count":1,
        "Challenge_created_time":1617894953567,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is inquiring whether compute clusters and endpoints will continue to incur costs if they are not deleted after use, similar to how compute instances do.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/349617\/does-compute-cluster-endpoints-costs-if-we-dont-de",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.7,
        "Challenge_reading_time":4.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.2939025,
        "Challenge_title":"Does compute cluster & Endpoints costs if we dont delete after use ?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":61,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=7c0d4f65-2f38-4fdd-91df-9f513e0321c1\">@Bhaskar11  <\/a>     <\/p>\n<p>When a <strong>compute cluster is idle<\/strong>, it autoscales to 0 nodes, so you don't pay when it's not in use. A compute instance is always on and doesn't autoscale. You should stop the compute instance when you aren't using it to avoid extra cost.    <br \/>\n<strong>refer -<\/strong> <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-target\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-target<\/a>    <\/p>\n<p>If the Answer is helpful, please click <code>Accept Answer<\/code> and <strong>up-vote<\/strong>, this can be beneficial to other community members.    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.7,
        "Solution_reading_time":9.36,
        "Solution_score_count":9.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":75.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1390559330112,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, Canada",
        "Answerer_reputation_count":96.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":1183.9241880556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am building a multiclass classifier on aws Sagemaker, and would love to use the predefined linearlearner algorithm for classification. <\/p>",
        "Challenge_closed_time":1531423415907,
        "Challenge_comment_count":3,
        "Challenge_created_time":1527161288830,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to know if Sagemaker's linear learner algorithm can be used for multiclass classification.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50508217",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":13.7,
        "Challenge_reading_time":2.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":1183.9241880556,
        "Challenge_title":"Can sagemaker's linear learner be used for multiclass classification?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":410.0,
        "Challenge_word_count":29,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1527161034076,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>Yes, it is possible now.<\/p>\n\n<p>You can set the predictor_type hyper-parameter to <code>multiclass_classifier<\/code>.<\/p>\n\n<p>See the documentation here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ll_hyperparameters.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ll_hyperparameters.html<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":32.5,
        "Solution_reading_time":4.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":21.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1114.6180555556,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\ndoing\r\n\r\n`$ aim convert mlflow --tracking_uri 'file:\/\/\/Users\/aim_user\/mlruns' --experiment 61`\r\n\r\nas described here https:\/\/aimstack.readthedocs.io\/en\/latest\/quick_start\/convert_data.html#show-mlflow-logs-in-aim\r\n\r\nfails with the following error\r\n\r\n![Screenshot from 2022-02-27 02-33-17](https:\/\/user-images.githubusercontent.com\/26168435\/155864827-dc7f3acb-0c79-4fab-9c79-a599f1a954ab.png)\r\n\r\nusing the experiment name instead of the experiment id\r\n\r\n![Screenshot from 2022-02-27 02-33-55](https:\/\/user-images.githubusercontent.com\/26168435\/155864887-63c19423-865e-4540-bfb7-c034e123af80.png)\r\n\r\ni.e.\r\n\r\n`$ aim convert mlflow --tracking_uri 'file:\/\/\/Users\/aim_user\/mlruns' --experiment 'ai-vengers-collab'` \r\n\r\nworks:\r\n\r\n![Screenshot from 2022-02-27 02-31-46](https:\/\/user-images.githubusercontent.com\/26168435\/155864881-03434a11-68f8-47e3-90e3-13465cbe86b4.png)\r\n\r\n### To reproduce\r\n\r\nsee above\r\n\r\n### Expected behavior\r\n\r\nconvert the experiment by ID\r\n\r\n### Environment\r\n\r\n- Aim Version 3.6\r\n- Python 3.8.1\r\n- pip3\r\n- Ubuntu 20.04.3 LTS\r\n",
        "Challenge_closed_time":1649939186000,
        "Challenge_comment_count":6,
        "Challenge_created_time":1645926561000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while running mlflow.projects.run consistently with etag error. The error is related to Etag conflict on the environment definition.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aimhubio\/aim\/issues\/1415",
        "Challenge_link_count":4,
        "Challenge_participation_count":6,
        "Challenge_readability":13.3,
        "Challenge_reading_time":14.53,
        "Challenge_repo_contributor_count":62.0,
        "Challenge_repo_fork_count":234.0,
        "Challenge_repo_issue_count":2794.0,
        "Challenge_repo_star_count":3760.0,
        "Challenge_repo_watch_count":44.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1114.6180555556,
        "Challenge_title":"aim convert mlflow --experiment fails for experiment id, works for experiment name",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":81,
        "Discussion_body":"Hey @luisoala, thanks for reporting the issue!\r\n@devfox-se could you please take a look at this? Thanks for reporting this @luisoala, will take a look soon! Hey @luisoala! We've released `v3.6.2` containing the fix for mlflow converter. Please check it out and let me know if there are any issues. thanks @alberttorosyan working through a few other deadlines atm, aiming for a test ~ next tuesday, will share result here @luisoala Hi, have you had a chance to test this?:) Closing due to inactivity, feel free to reopen in case this still persists.",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1448314895790,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, United States",
        "Answerer_reputation_count":1769.0,
        "Answerer_view_count":272.0,
        "Challenge_adjusted_solved_time":15.0059675,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Can AWS SageMaker handle binary classification using TFidf vectorized text as prediction base?<\/p>",
        "Challenge_closed_time":1567815655550,
        "Challenge_comment_count":0,
        "Challenge_created_time":1567761634067,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring if AWS SageMaker can perform binary classification using TFidf vectorized text for prediction.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57819173",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.0,
        "Challenge_reading_time":1.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":15.0059675,
        "Challenge_title":"SageMaker AWS Binary Text Classification",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":109.0,
        "Challenge_word_count":17,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1339151552347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Amsterdam, Netherlands",
        "Poster_reputation_count":1125.0,
        "Poster_view_count":319.0,
        "Solution_body":"<p>You would have to use inference pipeline for your use case. What that means is that you will need to use a pre-processing step to featurize your text into tfidf and then feed into Sagemaker classification. Here's a <a href=\"https:\/\/stackoverflow.com\/questions\/57767899\/how-to-create-a-pipeline-in-sagemaker-with-pytorch\">SO answer<\/a> with more details around this.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.0,
        "Solution_reading_time":4.8,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":46.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":6.7927280555,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm thinking of deploying a TensorFlow model using Vertex AI in GCP. I am almost sure that the cost will be directly related to the number of queries per second (QPS) because I am going to use automatic scaling. I also know that the type of machine (with GPU, TPU, etc.) will have an impact on the cost.<\/p>\n<ul>\n<li>Do you have any estimation about the cost versus the number of queries per second?<\/li>\n<li>How does the type of virtual machine changes this cost?<\/li>\n<\/ul>\n<p>The type of model is for object detection.<\/p>",
        "Challenge_closed_time":1657283230408,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657258379287,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is planning to deploy a TensorFlow model using Vertex AI in GCP and is concerned about the cost, which they believe will be directly related to the number of queries per second and the type of machine used. They are seeking an estimation of the cost based on the number of queries per second and how the cost is affected by the type of virtual machine used. The model is for object detection.",
        "Challenge_last_edit_time":1657258776587,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72907038",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":6.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":6.9030891667,
        "Challenge_title":"Cost of deploying a TensorFlow model in GCP?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":74.0,
        "Challenge_word_count":100,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1569457921527,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Luis Potos\u00ed, S.L.P., M\u00e9xico",
        "Poster_reputation_count":41.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>Autoscaling depends on the CPU and GPU utilization which directly correlates to the QPS, as you have said. To estimate the cost based on the QPS, you can deploy a custom prediction container to a Compute Engine instance directly, then benchmark the instance by making prediction calls until the VM hits 90+ percent CPU utilization (consider GPU utilization if configured). Do this multiple times for different machine types, and determine the &quot;QPS per cost per hour&quot; of different machine types. You can re-run these experiments while benchmarking latency to find the <strong>ideal cost per QPS per your latency targets<\/strong> for your specific custom prediction container. For more information about choosing the ideal machine for your workload, refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#finding_the_ideal_machine_type\" rel=\"nofollow noreferrer\">documentation<\/a>.<\/p>\n<p>For your second question, as per the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/pricing#custom-trained_models:%7E:text=a%20specific%20job.-,Prediction%20and%20explanation,-This%20table%20provides\" rel=\"nofollow noreferrer\">Vertex AI pricing documentation<\/a> (for model deployment), cost estimation is done based on the node hours. A node hour represents the time a virtual machine spends running your prediction job or waiting in a ready state to handle prediction or explanation requests. Each type of VM offered has a specific pricing per node hour depending on the number of cores and the amount of memory. Using a VM with more resources will cost more per node hour and vice versa. To choose an ideal VM for your deployment, please follow the steps given in the first paragraph which will help you find a good trade off between cost and performance.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.4,
        "Solution_reading_time":22.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":243.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1392296244356,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Chicago, IL, USA",
        "Answerer_reputation_count":1020.0,
        "Answerer_view_count":206.0,
        "Challenge_adjusted_solved_time":17.7042919445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am finetuning multiple models using for loop as follows.<\/p>\n<pre><code>for file in os.listdir(args.data_dir):\n    finetune(args, file)\n<\/code><\/pre>\n<p>BUT <code>wandb<\/code> website shows plots and logs only for the first file i.e., <code>file1<\/code> in <code>data_dir<\/code> although it is training and saving models for other files. It feels very strange behavior.<\/p>\n<pre><code>wandb: Synced bertweet-base-finetuned-file1: https:\/\/wandb.ai\/***\/huggingface\/runs\/***\n<\/code><\/pre>\n<p>This is a small snippet of <strong>finetuning<\/strong> code with Huggingface:<\/p>\n<pre><code>def finetune(args, file):\n    training_args = TrainingArguments(\n        output_dir=f'{model_name}-finetuned-{file}',\n        overwrite_output_dir=True,\n        evaluation_strategy='no',\n        num_train_epochs=args.epochs,\n        learning_rate=args.lr,\n        weight_decay=args.decay,\n        per_device_train_batch_size=args.batch_size,\n        per_device_eval_batch_size=args.batch_size,\n        fp16=True, # mixed-precision training to boost speed\n        save_strategy='no',\n        seed=args.seed,\n        dataloader_num_workers=4,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset['train'],\n        eval_dataset=None,\n        data_collator=data_collator,\n    )\n    trainer.train()\n    trainer.save_model()\n<\/code><\/pre>",
        "Challenge_closed_time":1650469853183,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650379721377,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with the Wandb website for Huggingface Trainer, where the website is only showing plots and logs for the first model in the data directory, even though the user is training and saving models for other files. The user is using a for loop to fine-tune multiple models and has shared a code snippet for the same.",
        "Challenge_last_edit_time":1650406117732,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71926953",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":15.8,
        "Challenge_reading_time":17.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":25.0366127778,
        "Challenge_title":"Wandb website for Huggingface Trainer shows plots and logs only for the first model",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":178.0,
        "Challenge_word_count":108,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1392296244356,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Chicago, IL, USA",
        "Poster_reputation_count":1020.0,
        "Poster_view_count":206.0,
        "Solution_body":"<p><code>wandb.init(reinit=True)<\/code> and <code>run.finish()<\/code> helped me to log the models <strong>separately<\/strong> on wandb website.<\/p>\n<p>The working code looks like below:<\/p>\n<pre><code>\nfor file in os.listdir(args.data_dir):\n    finetune(args, file)\n\nimport wandb\ndef finetune(args, file):\n    run = wandb.init(reinit=True)\n    ...\n    run.finish()\n<\/code><\/pre>\n<p>Reference: <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/launch#how-do-i-launch-multiple-runs-from-one-script\" rel=\"nofollow noreferrer\">https:\/\/docs.wandb.ai\/guides\/track\/launch#how-do-i-launch-multiple-runs-from-one-script<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.5,
        "Solution_reading_time":8.14,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":40.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.6086344444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hey experts, I am looking for a document of how features in SDK V1 mapping to SDK v2 to show my team and plan how we should move to SDK v2. I cannot find a summary for that. Can you please help with this <\/p>",
        "Challenge_closed_time":1682725999300,
        "Challenge_comment_count":0,
        "Challenge_created_time":1682716608216,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is looking for a document that maps the features of SDK V1 to SDK V2 to help their team plan the transition to the newer version, but they are unable to find a summary for it. They are seeking assistance in locating such a document.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1266094\/sdk-features-mapping",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.4,
        "Challenge_reading_time":2.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":2.6086344444,
        "Challenge_title":"SDK features mapping",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":47,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"https:\/\/learn.microsoft.com\/users\/na\/?userid=9603a4b0-3119-4f80-93b6-9637337c7a94\">@otto atler<\/a> <\/p>\n<p>Thanks for reaching out to us again, please see below list: <\/p>\n<p>For workspace - <\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.workspace.workspace\">Method\/API in SDK v1 (use links to ref docs)<\/a>    <\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.entities.workspace\">Method\/API in SDK v2 (use links to ref docs)<\/a><\/p>\n<p>For compute - <\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.compute.amlcompute(class)\">Method\/API in SDK v1 (use links to ref docs)<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.entities.amlcompute\">Method\/API in SDK v2 (use links to ref docs)<\/a><\/p>\n<p>For datastore -<\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_storage_datastore.azureblobdatastore?view=azure-ml-py&amp;preserve-view=true\">azureml_blob_datastore<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.entities.azuredatalakegen1datastore\">azureml_blob_datastore<\/a><\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_data_lake_datastore.azuredatalakedatastore?view=azure-ml-py&amp;preserve-view=true\">azureml_data_lake_gen1_datastore<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.entities.azuredatalakegen1datastore\">azureml_data_lake_gen1_datastore<\/a><\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_data_lake_datastore.azuredatalakegen2datastore?view=azure-ml-py&amp;preserve-view=true\">azureml_data_lake_gen2_datastore<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.entities.azuredatalakegen2datastore\">azureml_data_lake_gen2_datastore<\/a><\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_sql_database_datastore.azuresqldatabasedatastore?view=azure-ml-py&amp;preserve-view=true\">azuremlml_sql_database_datastore<\/a><\/p>\n<p>V2 Will be supported via import &amp; export functionalities|<\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_my_sql_datastore.azuremysqldatastore?view=azure-ml-py&amp;preserve-view=true\">azuremlml_my_sql_datastore<\/a><\/p>\n<p>V2 Will be supported via import &amp; export functionalities|<\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_postgre_sql_datastore.azurepostgresqldatastore?view=azure-ml-py&amp;preserve-view=true\">azuremlml_postgre_sql_datastore<\/a><\/p>\n<p>V2 Will be supported via import &amp; export functionalities|<\/p>\n<p>For data assets - <\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data\">Method\/API in SDK v1<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.entities\">Method\/API in SDK v2<\/a><\/p>\n<p>For model assets - <\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model(class)#azureml-core-model-register\">Model.register<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.mlclient#azure-ai-ml-mlclient-create-or-update\">ml_client.models.create_or_update<\/a><\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run#azureml-core-run-run-register-model\">run.register_model<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.mlclient#azure-ai-ml-mlclient-create-or-update\">ml_client.models.create_or_update<\/a><\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model(class)#azureml-core-model-deploy\">Model.deploy<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.mlclient#azure-ai-ml-mlclient-begin-create-or-update\">ml_client.begin_create_or_update(blue_deployment)<\/a><\/p>\n<p>I hope this helps, please let me know if you have any questions.<\/p>\n<p>Regards,<\/p>\n<p>Yutong<\/p>\n<p>-Please kindly accept the answer and vote 'Yes' if you feel helpful to support he community, thanks a lot.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":22.0,
        "Solution_readability":30.1,
        "Solution_reading_time":61.37,
        "Solution_score_count":0.0,
        "Solution_sentence_count":24.0,
        "Solution_word_count":197.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1533910280492,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":49.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":468.8841677778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am beginning to train a semantic segmentation model in AWS Sagermaker and they provide the following metrics for the output. I understand mIOU, loss, and pixel accuracy, but I do not know what throughput is or how to interpret it. Please see the image below and let me know if you need additional information.\n<a href=\"https:\/\/i.stack.imgur.com\/dpsDp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/dpsDp.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1596819625616,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595129540180,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is training a semantic segmentation model in AWS Sagemaker and is unsure about how to interpret the \"throughput\" metric provided in the output. They are familiar with other metrics such as mIOU, loss, and pixel accuracy.",
        "Challenge_last_edit_time":1595131642612,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62975940",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":6.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":469.4681766667,
        "Challenge_title":"How to interpret the Throughput metric for Semantic Segmentation?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":67.0,
        "Challenge_word_count":73,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1449513251820,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":693.0,
        "Poster_view_count":56.0,
        "Solution_body":"<p>Throughput is reported in records per second (i.e. images per second). It shows how fast the algorithm can iterate over training or validation data. For example, with a throughput of 30 records\/sec it would take a minute to iterate over 1800 images.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.2,
        "Solution_reading_time":3.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":42.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1523264005616,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Yokohama, Kanagawa, Japan",
        "Answerer_reputation_count":1635.0,
        "Answerer_view_count":143.0,
        "Challenge_adjusted_solved_time":1.0014597222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm pretty new to SageMaker, so I'm sorry if I miss something obvious.<\/p>\n\n<p>I've trained a DL model which uses frames from a video to make a prediction. The current script, that runs in the SageMaker jupyter-notebook, takes a video URL as an input and uses an FFMPEG subprocess pipe to extract the frames and predict them afterwards. This works fine, but now I want to start that script from Lambda.<\/p>\n\n<p>As far as I understood, I could deploy my model with sagemaker and make predictions for every single frame from Lambda, unfortunately this is not an option, as ffprobe, ffmpeg and numpy are too large to fit into the limited lambda space.<\/p>\n\n<p>tl;dr: Is it possible to run my custom script (ffmpeg frame extraction + tensorflow model prediction) as an endpoint in SageMaker?<\/p>",
        "Challenge_closed_time":1580863000328,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580859395073,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a deep learning model that uses frames from a video to make predictions. They want to run their custom script (ffmpeg frame extraction + tensorflow model prediction) as an endpoint in SageMaker, but are unable to do so due to the limited space in Lambda.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60067075",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":10.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.0014597222,
        "Challenge_title":"SageMaker deploy custom script",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":386.0,
        "Challenge_word_count":136,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1460444230316,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":404.0,
        "Poster_view_count":127.0,
        "Solution_body":"<p>Sagemaker allows you to use a custom Docker image (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms.html\" rel=\"nofollow noreferrer\">AWS document<\/a>)<\/p>\n\n<blockquote>\n  <p>Build your own custom container image: If there is no pre-built Amazon\n  SageMaker container image that you can use or modify for an advanced\n  scenario, you can package your own script or algorithm to use with\n  Amazon SageMaker.You can use any programming language or framework to\n  develop your container<\/p>\n<\/blockquote>\n\n<ul>\n<li>Create a docker image with your code (FFmpeg, TensorFlow)<\/li>\n<li>Testing the docker container locally<\/li>\n<li>Deploying the image on Amazon ECR (Elastic Container Repository)<\/li>\n<li>Create a SageMaker model and point to the image<\/li>\n<\/ul>\n\n<p>For details, you can learn more from <a href=\"https:\/\/towardsdatascience.com\/brewing-up-custom-ml-models-on-aws-sagemaker-e09b64627722\" rel=\"nofollow noreferrer\">this tutorial<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.9,
        "Solution_reading_time":12.43,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":111.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1226984969400,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Adelaide, Australia",
        "Answerer_reputation_count":5789.0,
        "Answerer_view_count":464.0,
        "Challenge_adjusted_solved_time":21214.2534619445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to train an object detection model starting from an existing model using the new Managed Spot Training feature,  The paramters used when creating my Estimator are as follows:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>od_model = sagemaker.estimator.Estimator(get_image_uri(sagemaker.Session().boto_region_name, 'object-detection', repo_version=\"latest\"),\n                                         Config['role'],\n                                         train_instance_count = 1,\n                                         train_instance_type = 'ml.p3.16xlarge',\n                                         train_volume_size = 50,\n                                         train_max_run = (48 * 60 * 60),\n                                         train_use_spot_instances = True,\n                                         train_max_wait = (72 * 60 * 60),\n                                         input_mode = 'File',\n                                         checkpoint_s3_uri = Config['train_checkpoint_uri'],\n                                         output_path = Config['s3_output_location'],\n                                         sagemaker_session = sagemaker.Session()\n                                         )\n<\/code><\/pre>\n\n<p>(The references to <code>Config<\/code> in the above are a config data structure I'm using to extract\/centralise some parameters)<\/p>\n\n<p>When I run the above, I get the following exception:<\/p>\n\n<blockquote>\n  <p>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: MaxWaitTimeInSeconds above 3600 is not supported for the given algorithm.<\/p>\n<\/blockquote>\n\n<p>If I change <code>train_max_wait<\/code> to 3600 I get this exception instead:<\/p>\n\n<blockquote>\n  <p>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: Invalid MaxWaitTimeInSeconds. It must be present and be greater than or equal to MaxRuntimeInSeconds<\/p>\n<\/blockquote>\n\n<p>However changing <code>max_run_time<\/code> to 3600 or less isn't going to work for me as I expect this model to take several days to train (large data set), in fact a single epoch takes more than an hour.<\/p>\n\n<p>The <a href=\"https:\/\/aws.amazon.com\/blogs\/aws\/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs\/\" rel=\"nofollow noreferrer\">AWS blog post on Managed Spot Training<\/a> say that <code>MaxWaitTimeInSeconds<\/code> is limited to an 60 minutes for:<\/p>\n\n<blockquote>\n  <p>For built-in algorithms and AWS Marketplace algorithms that don\u2019t use checkpointing, we\u2019re enforcing a maximum training time of 60 minutes (MaxWaitTimeInSeconds parameter).<\/p>\n<\/blockquote>\n\n<p>Earlier, the same blog post says:<\/p>\n\n<blockquote>\n  <p>Built-in algorithms: computer vision algorithms support checkpointing (Object Detection, Semantic Segmentation, and very soon Image Classification).<\/p>\n<\/blockquote>\n\n<p>So I don't think it's that my algorithm doesn't support Checkpointing.  In fact that blog post uses object detection and max run times of 48 hours.  So I don't think it's an algorithm limitation.<\/p>\n\n<p>As you can see above, I've set up a S3 URL for the checkpoints.  The S3 bucket does exist, and the training container has access to it (it's the same bucket that the training data and model outputs are placed, and I had no problems with access to those before turning on spot training.<\/p>\n\n<p>My boto and sagemaker libraries are current versions:<\/p>\n\n<pre><code>boto3 (1.9.239)\nbotocore (1.12.239)\nsagemaker (1.42.3)\n<\/code><\/pre>\n\n<p>As best I can tell from reading various docs, I've got everything set up correctly.  My use case is almost exactly what's described in the blog post linked above, but I'm using the SageMaker Python SDK instead of the console.<\/p>\n\n<p>I'd really like to try Managed Spot Training to save some money, as I have a very long training run coming up.  But limiting timeouts to an hour isn't going to work for my use case.  Any suggestions?<\/p>\n\n<p><strong>Update:<\/strong>  If I comment out the <code>train_use_spot_instances<\/code> and <code>train_max_wait<\/code> options to train on regular on-demand instances my training job is created successfully.  If I then try to use the console to clone the job and turn on Spot instances on the clone I get the same ValidationException.<\/p>",
        "Challenge_closed_time":1570492274472,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569898766770,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to train an object detection model using SageMaker Managed Spot Training feature, but is encountering an error related to the MaxWaitTimeInSeconds parameter. The user has tried changing the parameter to 3600, but it resulted in another error. The AWS blog post on Managed Spot Training mentions that MaxWaitTimeInSeconds is limited to 60 minutes for built-in algorithms and AWS Marketplace algorithms that don't use checkpointing. However, the user's algorithm supports checkpointing, and the S3 bucket for checkpoints exists and is accessible. The user is seeking suggestions to resolve the issue and use Managed Spot Training to save costs.",
        "Challenge_last_edit_time":1569900383487,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58177548",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":49.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":164.8632505556,
        "Challenge_title":"SageMaker Managed Spot Training with Object Detection algorithm",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1234.0,
        "Challenge_word_count":490,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1226984969400,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Adelaide, Australia",
        "Poster_reputation_count":5789.0,
        "Poster_view_count":464.0,
        "Solution_body":"<p>I ran my script again today and it worked fine, no <code>botocore.exceptions.ClientError<\/code> exceptions.  Given that this issue affected both the Python SDK for Sagemaker and the console, I suspect it might have been an issue with the backend API and not my client code.<\/p>\n<p>Either way, it's working now.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1646271695950,
        "Solution_link_count":0.0,
        "Solution_readability":8.5,
        "Solution_reading_time":3.94,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":49.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1393524211332,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":745.0,
        "Answerer_view_count":71.0,
        "Challenge_adjusted_solved_time":609.6229966667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Have exhausted myself on this one so any help would be appreciated.<\/p>\n\n<p>I am trying to set up hosting my tensorflow model with Amazon Sagemaker and following the example found <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators\/iris_dnn_classifier.py\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>This example uses hard coded feature columns with known dimensionality. <\/p>\n\n<pre><code>feature_columns = [tf.feature_column.numeric_column(INPUT_TENSOR_NAME, shape=[4])]\n<\/code><\/pre>\n\n<p>I need to avoid this as my dataset changes often.<\/p>\n\n<h1>Local Machine Set Up<\/h1>\n\n<p>Now on my local machine, I define a list of columns <\/p>\n\n<pre><code>my_feature_columns = []\n<\/code><\/pre>\n\n<p>With the following strategy<\/p>\n\n<pre><code>#Define placeholder nodes based on datatype being inserted\n\nfor key in train_x.keys():\n<\/code><\/pre>\n\n<p>Where train_x is the dataset without labels.<\/p>\n\n<p>'OBJECTS' become hashed buckets as there are many possible categories<\/p>\n\n<pre><code>    if train_x[key].dtypes == 'object':\n\n        categorical_column = tf.feature_column.categorical_column_with_hash_bucket(\n                key = key,\n                hash_bucket_size = len(train_x[key].unique()))\n\n        my_feature_columns.append(tf.feature_column.embedding_column(\n                categorical_column=categorical_column,\n                dimension=5))\n<\/code><\/pre>\n\n<p>'INT64' become categorical columns as there are only two possible categories (I have recoded booleans to 0\/1)<\/p>\n\n<pre><code>    elif train_x[key].dtypes == 'int64':\n\n        categorical_column = tf.feature_column.categorical_column_with_identity(\n                key=key,\n                num_buckets=2)\n\n        my_feature_columns.append(tf.feature_column.indicator_column(categorical_column))\n<\/code><\/pre>\n\n<p>'FLOATS' become continuous columns<\/p>\n\n<pre><code>    elif train_x[key].dtypes == 'float':\n        my_feature_columns.append(\n        tf.feature_column.numeric_column(\n        key=key))\n<\/code><\/pre>\n\n<p>On the local machine this yields a nice list of all of my features that can be given as an argument when instantiating a tf.estimator.DNNClassifier. As more categories are added to each OBJECT column, this is handled by<\/p>\n\n<pre><code>hash_bucket_size = len(train_x[key].unique())\n<\/code><\/pre>\n\n<h1>Sagemaker<\/h1>\n\n<p>From the <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst#preparing-the-tensorflow-training-script\" rel=\"nofollow noreferrer\">Docs<\/a><\/p>\n\n<p><em>Preparing the TensorFlow training script\nYour TensorFlow training script must be a Python 2.7 source file. The SageMaker TensorFlow docker image uses this script by calling specifically-named functions from this script.<\/em><\/p>\n\n<p><em>The training script must contain the following:<\/em><\/p>\n\n<p><em>Exactly one of the following:\nmodel_fn: defines the model that will be trained.\nkeras_model_fn: defines the tf.keras model that will be trained.\nestimator_fn: defines the tf.estimator.Estimator that will train the model.<\/em><\/p>\n\n<p><em>train_input_fn: preprocess and load training data.<\/em><\/p>\n\n<p><em>eval_input_fn: preprocess and load evaluation data.<\/em><\/p>\n\n<p>Again, from the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators\/iris_dnn_classifier.py\" rel=\"nofollow noreferrer\">example<\/a><\/p>\n\n<pre><code>def train_input_fn(training_dir, params):\n\"\"\"Returns input function that would feed the model during training\"\"\"\nreturn _generate_input_fn(training_dir, 'iris_training.csv')\n<\/code><\/pre>\n\n<p>This function is called by the sagemaker docker image, which adds its own argument for <strong>training_dir<\/strong>, it is not a global parameter.<\/p>\n\n<p>When trying to access my training data from the estimator_fn to build a my_feature_columns list<\/p>\n\n<pre><code>NameError: global name 'training_dir' is not defined\n<\/code><\/pre>\n\n<h1>I would love to be able to do something like this.<\/h1>\n\n<pre><code>def estimator_fn(run_config, params):\n\nmy_feature_columns = []\n\ntrain_x , _ , _ , _ = datasplitter(os.path.join(training_dir, 'leads_test_frame.csv'))\n\nfor key in train_x.keys():\n    if train_x[key].dtypes == 'object':\n\n        categorical_column = tf.feature_column.categorical_column_with_hash_bucket(\n                key = key,\n                hash_bucket_size = len(train_x[key].unique()))\n\n        my_feature_columns.append(tf.feature_column.embedding_column(\n                categorical_column=categorical_column,\n                dimension=5))\n\n    elif train_x[key].dtypes == 'int64':\n\n        categorical_column = tf.feature_column.categorical_column_with_identity(\n                key=key,\n                num_buckets=2)\n\n        my_feature_columns.append(tf.feature_column.indicator_column(categorical_column))\n\n    elif train_x[key].dtypes == 'float':\n        my_feature_columns.append(\n        tf.feature_column.numeric_column(\n        key=key))\n\nreturn tf.estimator.DNNClassifier(feature_columns=my_feature_columns,\n                                  hidden_units=[10, 20, 10],\n                                  n_classes=2,\n                                  config=run_config)\n<\/code><\/pre>\n\n<p>Thanks to anyone who can help in any way. Will happily give more info if needed but feel like 4 pages is probably enough :-S<\/p>\n\n<p>Cheers!\nClem<\/p>",
        "Challenge_closed_time":1537837258768,
        "Challenge_comment_count":0,
        "Challenge_created_time":1535642615980,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to set up hosting their TensorFlow model with Amazon Sagemaker, but is facing challenges with generating feature columns. They are using a strategy on their local machine to define a list of columns, but are unable to access their training data from the estimator_fn to build a my_feature_columns list. They would like to be able to define the my_feature_columns list in the estimator_fn function.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52100549",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":16.5,
        "Challenge_reading_time":67.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":47,
        "Challenge_solved_time":609.6229966667,
        "Challenge_title":"Procedural Generation of Feature Columns for use with Sagemaker Tensorflow Instance",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":165.0,
        "Challenge_word_count":458,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1491420193248,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Windsor, UK",
        "Poster_reputation_count":163.0,
        "Poster_view_count":71.0,
        "Solution_body":"<p><strong>training_dir<\/strong> points to your training channel, i.e. <em>\/opt\/ml\/input\/data\/training<\/em>. You can hardcode this location inside your <strong>estimation_fn<\/strong>.<\/p>\n\n<p>When training starts, SageMaker makes the data for the channel available in the <em>\/opt\/ml\/input\/data\/<strong>channel_name<\/em><\/strong> directory in the Docker container.<\/p>\n\n<p>You can find more information here <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo.html#your-algorithms-training-algo-running-container\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo.html#your-algorithms-training-algo-running-container<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":26.9,
        "Solution_reading_time":9.81,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":45.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1553882107003,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":294.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":257.5221927778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to use the sagemaker processor to replace some processes we run on Amazon batch.<\/p>\n<pre><code>from sagemaker.processor import ScriptProcessor \nproc = ScriptProcessor(\n    image_uri='your-image-uri', \n    command=['python3'], \n    role=role, \n    instance_count=1, \n    instance_type='m4.4x.large',  \n    volume_size_in_gb=500,\n    base_job_name='preprocessing-test',\n)\nproc.run(\n    code='test.py',\n)\n<\/code><\/pre>\n<p>First of all, is it true that the <code>ScriptProcessing<\/code> syntax is more complicated than the <code>TrainingJob<\/code> version where you can specify the <code>source_dir<\/code> and <code>entrypoint<\/code> to upload your code to a default container?<\/p>\n<p>Secondly, this code above gives me this error<\/p>\n<pre><code>ParamValidationError: Parameter validation failed:\nInvalid bucket name &quot;sagemaker-eu-west-1-&lt;account-id&gt;\\preprocessing-test-&lt;timestamp&gt;\\input\\code&quot;: Bucket name must match the regex &quot;^[a-zA-Z0-9.\\-_]{1,255}$&quot; or be an ARN matching the regex &quot;^arn:(aws).*:s3:[a-z\\-0-9]+:[0-9]{12}:accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$&quot;\n<\/code><\/pre>\n<p>I guess this key is created internally when trying to upload my <code>test.py<\/code>, but why does it not work? :) The documentation says you can use both local and s3 paths.<\/p>",
        "Challenge_closed_time":1587103591427,
        "Challenge_comment_count":2,
        "Challenge_created_time":1586176511533,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use the Sagemaker processor to replace some processes they run on Amazon batch. They are encountering two issues: firstly, they are wondering if the ScriptProcessing syntax is more complicated than the TrainingJob version, and secondly, they are receiving an error message stating that the bucket name is invalid when trying to upload their test.py file. The user is unsure why this is happening as the documentation states that both local and S3 paths can be used.",
        "Challenge_last_edit_time":1616657799110,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61059996",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":13.8,
        "Challenge_reading_time":17.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":257.5221927778,
        "Challenge_title":"Sagemaker Processing doesn't upload",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":314.0,
        "Challenge_word_count":130,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1484838464572,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Amsterdam, Nederland",
        "Poster_reputation_count":3937.0,
        "Poster_view_count":387.0,
        "Solution_body":"<p>The bucket name `sagemaker-eu-west-1-\\preprocessing-test-\\input\\code looks like a hardcoded string. In SageMaker Python SDK, the code upload function is <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/3bf569ece9e46a097d1ab69286ee89f762931e6c\/src\/sagemaker\/processing.py#L463\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<p>Are you using a Windows environment? As Lauren noted in the comments, there have been some bug fixes there, so make sure to use the last version<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1616657708820,
        "Solution_link_count":1.0,
        "Solution_readability":12.8,
        "Solution_reading_time":6.35,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":49.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":40.3672083333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Using API Management - can I import from a Swagger JSON file definition AND where that endpoint requires the following injection to a header so that it can call\/retrieve that Swagger JSON?<\/p>\n<p>The API in question is actually an Azure Machine Learning Online Endpoint - and that contains a Swagger JSON definition. In order to see the definition you have to inject the following header records into the HTTP call...<\/p>\n<pre><code>Authorization: Bearer longRandomtokengeneratedbyAML12345\nazureml-model-deployment: nameofmyamlmodeldendpointdeployment\n<\/code><\/pre>\n<p>Using API Management - can I somehow import this Swagger JSON ? I can't find a way to reach the endpoint and specify the x2 header records it requires<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1675215836676,
        "Challenge_comment_count":1,
        "Challenge_created_time":1675070514726,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a challenge in importing a Swagger JSON file definition into API Management, which requires HTTP header records to call\/retrieve the Swagger JSON. The API in question is an Azure Machine Learning Online Endpoint, and the user needs to inject two header records into the HTTP call to see the definition. The user is unable to find a way to reach the endpoint and specify the required header records.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1165334\/api-management-can-you-import-from-swagger-json-th",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.9,
        "Challenge_reading_time":10.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":40.3672083333,
        "Challenge_title":"API Management - Can you import from Swagger JSON that requires HTTP Header records?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":114,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@<a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=0ec06fb6-513e-4f5c-9aff-281bc5e44e22\">Neil McAlister<\/a> Thanks for posting it in Microsoft Q&amp;A. Based on the statement above, it looks like you are importing Swagger JSON to APIM via portal, CLI or PowerShell with specification URL, correct?<\/p>\n<p>If so, unfortunately that's not possible right now. There is no way to specify headers with <code>--specification-url<\/code> in <a href=\"https:\/\/learn.microsoft.com\/en-us\/cli\/azure\/apim\/api?view=azure-cli-latest#az-apim-api-import\">az apim api import<\/a> (or <a href=\"https:\/\/learn.microsoft.com\/en-us\/powershell\/module\/az.apimanagement\/import-azapimanagementapi\">Import-AzApiManagementApi<\/a> - PowerShell) and you would need to download swagger JSON in your pipeline and then use it with <code>--specification-path<\/code>(check size limitation when using this parameter <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/api-management\/api-management-api-import-restrictions#openapi-specifications\">here<\/a>) or place it in another location where APIM can directly access it. <\/p>\n<p>If you are interested in this feature and like to submit feedback to our product team, please submit it via <a href=\"https:\/\/aka.ms\/apimwish\">https:\/\/aka.ms\/apimwish<\/a> and would help our product team to prioritize the features. Also, others with similar interests can upvote it too.<\/p>\n<p>Feel free to reach out if you have any other questions.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":14.5,
        "Solution_reading_time":19.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":151.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1411546424280,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Edinburgh",
        "Answerer_reputation_count":58.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":4477.9070705556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed a the universal_sentence_encoder_large_3 to an aws sagemaker.  When I am attempting to predict with the deployed model I get <code>Failed precondition: Table not initialized.<\/code> as an error. I have included the part where I save my model below:<\/p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\ndef tfhub_to_savedmodel(model_name, export_path):\n\n    model_path = '{}\/{}\/00000001'.format(export_path, model_name)\n    tfhub_uri = 'http:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/3'\n\n    with tf.Session() as sess:\n        module = hub.Module(tfhub_uri)\n        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n        input_params = module.get_input_info_dict()\n        dtype = input_params['text'].dtype\n        shape = input_params['text'].get_shape()\n\n        # define the model inputs\n        inputs = {'text': tf.placeholder(dtype, shape, 'text')}\n        output = module(inputs['text'])\n        outputs = {\n            'vector': output,\n        }\n\n        # export the model\n        tf.saved_model.simple_save(\n            sess,\n            model_path,\n            inputs=inputs,\n            outputs=outputs)  \n\n    return model_path\n<\/code><\/pre>\n\n<p>I have seen other people ask this problem but no solution has been ever posted.  It seems to be a common problem with tensorflow_hub sentence encoders<\/p>",
        "Challenge_closed_time":1580113898207,
        "Challenge_comment_count":0,
        "Challenge_created_time":1563993432753,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has deployed a universal sentence encoder to an AWS Sagemaker and is encountering an error \"Failed precondition: Table not initialized\" while attempting to predict with the deployed model. The user has shared the code used to save the model and has mentioned that this seems to be a common problem with TensorFlow Hub sentence encoders.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57189292",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":17.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":4477.9070705556,
        "Challenge_title":"Failed precondition: Table not initialized. on deployed universal sentence encoder from aws sagemaker",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":365.0,
        "Challenge_word_count":138,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1531840489147,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berkeley, CA, USA",
        "Poster_reputation_count":425.0,
        "Poster_view_count":92.0,
        "Solution_body":"<p>I was running into this exact issue earlier this week while trying to modify this example <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_serving_container\/tensorflow_serving_container.ipynb\" rel=\"nofollow noreferrer\">Sagemaker notebook<\/a>. Particularly the part where serving the model. That is, running <code>predictor.predict()<\/code> on the Sagemaker Tensorflow Estimator.<\/p>\n\n<p>The solution outlined in the issue worked perfectly for me- <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/issues\/773#issuecomment-509433290\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/issues\/773#issuecomment-509433290<\/a><\/p>\n\n<p>I think it's just because <code>tf.tables_initializer()<\/code> only runs for training but it needs to be specified through the <code>legacy_init_op<\/code> if you want to run it during prediction.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":18.5,
        "Solution_reading_time":12.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":78.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1513074641863,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Antarctica",
        "Answerer_reputation_count":155.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":14.8578630556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>My question is somehow related to <a href=\"https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/217305\/data-input-format-call-the-service-for-azure-ml-ti.html\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/217305\/data-input-format-call-the-service-for-azure-ml-ti.html<\/a> - however, the provided solution does not seem to work.<\/p>\n<p>I am constructing a simple model with heart-disease dataset but I wrap it into Pipeline as I use some featurization steps (scaling, encoding etc.) The full script below:<\/p>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport pickle\n\n# data input\ndf = pd.read_csv('heart.csv')\n\n# numerical variables\nnum_cols = ['age',\n            'trestbps',\n            'chol',\n            'thalach',\n            'oldpeak'\n]\n\n# categorical variables\ncat_cols = ['sex',\n            'cp',\n            'fbs',\n            'restecg',\n            'exang',\n            'slope',\n            'ca',\n            'thal']\n\n# changing format of the categorical variables\ndf[cat_cols] = df[cat_cols].apply(lambda x: x.astype('object'))\n\n# target variable\ny = df['target']\n\n# features\nX = df.drop(['target'], axis=1)\n\n# data split:\n\n# random seed\nnp.random.seed(42)\n\n# splitting the data\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.2,\n                                                    stratify=y)\n\n# double check\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n# pipeline for numerical data\nnum_preprocessing = Pipeline([('num_imputer', SimpleImputer(strategy='mean')), # imputing with mean\n                                                   ('minmaxscaler', MinMaxScaler())]) # scaling\n\n# pipeline for categorical data\ncat_preprocessing = Pipeline([('cat_imputer', SimpleImputer(strategy='constant', fill_value='missing')), # filling missing values\n                                                ('onehot', OneHotEncoder(drop='first', handle_unknown='error'))]) # One Hot Encoding\n\n# preprocessor - combining pipelines\npreprocessor = ColumnTransformer([\n                                  ('categorical', cat_preprocessing, cat_cols),\n                                  ('numerical', num_preprocessing, num_cols)\n                                                           ])\n\n# initial model parameters\nlog_ini_params = {'penalty': 'l2', \n                  'tol': 0.0073559740277086005, \n                  'C': 1.1592424247511928, \n                  'fit_intercept': True, \n                  'solver': 'liblinear'}\n\n# model - Pipeline\nlog_clf = Pipeline([('preprocessor', preprocessor),\n                  ('clf', LogisticRegression(**log_ini_params))])\n\nlog_clf.fit(X_train, y_train)\n\n# dumping the model\nf = 'model\/log.pkl'\nwith open(f, 'wb') as file:\n    pickle.dump(log_clf, file)\n\n# loading it\nloaded_model = joblib.load(f)\n\n# double check on a single datapoint\nnew_data = pd.DataFrame({'age': 71,\n                         'sex': 0,\n                         'cp': 0,\n                         'trestbps': 112,\n                         'chol': 203,\n                         'fbs': 0,\n                         'restecg': 1,\n                         'thalach': 185,\n                         'exang': 0,\n                         'oldpeak': 0.1,\n                         'slope': 2,\n                         'ca': 0,\n                          'thal': 2}, index=[0])\n\nloaded_model.predict(new_data)\n\n<\/code><\/pre>\n<p>...and it works just fine.  Then I deploy the model to the Azure Web Service using these steps:<\/p>\n<ol>\n<li>I create the score.py file<\/li>\n<\/ol>\n<pre><code>import joblib\nfrom azureml.core.model import Model\nimport json\n\ndef init():\n    global model\n    model_path = Model.get_model_path('log') # logistic\n    print('Model Path is  ', model_path)\n    model = joblib.load(model_path)\n\n\ndef run(data):\n    try:\n        data = json.loads(data)\n        result = model.predict(data['data'])\n        # any data type, as long as it is JSON serializable.\n        return {'data' : result.tolist() , 'message' : 'Successfully classified heart diseases'}\n    except Exception as e:\n        error = str(e)\n        return {'data' : error , 'message' : 'Failed to classify heart diseases'}\n<\/code><\/pre>\n<ol>\n<li>I deploy the model:<\/li>\n<\/ol>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core.webservice import AciWebservice\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.environment import Environment\nfrom azureml.core import Workspace\nfrom azureml.core.model import Model\nfrom azureml.core.conda_dependencies import CondaDependencies\n\nws = Workspace.from_config()\n\nmodel = Model.register(workspace = ws,\n              model_path ='model\/log.pkl',\n              model_name = 'log',\n              tags = {'version': '1'},\n              description = 'Heart disease classification',\n              )\n\n# to install required packages\nenv = Environment('env')\ncd = CondaDependencies.create(pip_packages=['pandas==1.1.5', 'azureml-defaults','joblib==0.17.0'], conda_packages = ['scikit-learn==0.23.2'])\nenv.python.conda_dependencies = cd\n\n# Register environment to re-use later\nenv.register(workspace = ws)\nprint('Registered Environment')\n\nmyenv = Environment.get(workspace=ws, name='env')\n\nmyenv.save_to_directory('.\/environ', overwrite=True)\n\naciconfig = AciWebservice.deploy_configuration(\n            cpu_cores=1,\n            memory_gb=1,\n            tags={'data':'heart disease classifier'},\n            description='Classification of heart diseases',\n            )\n\ninference_config = InferenceConfig(entry_script='score.py', environment=myenv)\n\nservice = Model.deploy(workspace=ws,\n                name='hd-model-log',\n                models=[model],\n                inference_config=inference_config,\n                deployment_config=aciconfig, \n                overwrite = True)\n\nservice.wait_for_deployment(show_output=True)\nurl = service.scoring_uri\nprint(url)\n<\/code><\/pre>\n<p>The deployment is fine:<\/p>\n<blockquote>\n<p>Succeeded\nACI service creation operation finished, operation &quot;Succeeded&quot;<\/p>\n<\/blockquote>\n<p>But I can not make any predictions with the new data. I try to use:<\/p>\n<pre><code>import pandas as pd\n\nnew_data = pd.DataFrame([[71, 0, 0, 112, 203, 0, 1, 185, 0, 0.1, 2, 0, 2],\n                         [80, 0, 0, 115, 203, 0, 1, 185, 0, 0.1, 2, 0, 0]],\n                         columns=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal'])\n<\/code><\/pre>\n<p>Following the answer from this topic (<a href=\"https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/217305\/data-input-format-call-the-service-for-azure-ml-ti.html\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/217305\/data-input-format-call-the-service-for-azure-ml-ti.html<\/a>) I transform the data:<\/p>\n<pre><code>test_sample = json.dumps({'data': new_data.to_dict(orient='records')})\n<\/code><\/pre>\n<p>And try to make some predictions:<\/p>\n<pre><code>import json\nimport requests\ndata = test_sample\nheaders = {'Content-Type':'application\/json'}\nr = requests.post(url, data=data, headers = headers)\nprint(r.status_code)\nprint(r.json())\n<\/code><\/pre>\n<p>However, I encounter an error:<\/p>\n<blockquote>\n<p>200\n{'data': &quot;Expected 2D array, got 1D array instead:\\narray=[{'age': 71, 'sex': 0, 'cp': 0, 'trestbps': 112, 'chol': 203, 'fbs': 0, 'restecg': 1, 'thalach': 185, 'exang': 0, 'oldpeak': 0.1, 'slope': 2, 'ca': 0, 'thal': &gt; 2}\\n {'age': 80, 'sex': 0, 'cp': 0, 'trestbps': 115, 'chol': 203, 'fbs': 0, 'restecg': 1, 'thalach': 185, 'exang': 0, 'oldpeak': 0.1, 'slope': 2, 'ca': 0, 'thal': 0}].\\nReshape your data either using array.reshape(-1, &gt; 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.&quot;, 'message': 'Failed to classify heart diseases'}<\/p>\n<\/blockquote>\n<p>How is it possible to adjust the input data to this form of predictions and add other output like predict_proba so I could store them in a separate output dataset?<\/p>\n<p>I know this error is somehow related either with the &quot;run&quot; part of the score.py file or the last code cell that calls the webservice, but I'm unable to find it.<\/p>\n<p>Would really appreciate some help.<\/p>",
        "Challenge_closed_time":1653558488392,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653476427160,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in making predictions with Azure Machine Learning using new data that contains headers. The user has constructed a simple model with heart-disease dataset and wrapped it into Pipeline as they use some featurization steps. The model is deployed to the Azure Web Service, but the user is unable to make any predictions with the new data. The user is encountering an error related to the input data format and is seeking help to adjust the input data to this form of predictions and add other output like predict_proba so they could store them in a separate output dataset.",
        "Challenge_last_edit_time":1653505948283,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72376401",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":12.9,
        "Challenge_reading_time":99.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":79,
        "Challenge_solved_time":22.7947866667,
        "Challenge_title":"Making predictions with Azure Machine learning with new data that contains headers (like pd.Dataframe)",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":280.0,
        "Challenge_word_count":761,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1513074641863,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Antarctica",
        "Poster_reputation_count":155.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>I believe I managed to solve the problem - even though I encountered some serious issues. :)<\/p>\n<ol>\n<li>As described here <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script\" rel=\"nofollow noreferrer\">here<\/a> - I edited the <code>score.py<\/code> script:<\/li>\n<\/ol>\n<pre><code>import joblib\nfrom azureml.core.model import Model\nimport numpy as np\nimport json\nimport pandas as pd\nimport numpy as np\n\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\nfrom inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\nfrom inference_schema.parameter_types.standard_py_parameter_type import StandardPythonParameterType\n    \ndata_sample = PandasParameterType(pd.DataFrame({'age': pd.Series([0], dtype='int64'),\n                                                'sex': pd.Series(['example_value'], dtype='object'),\n                                                'cp': pd.Series(['example_value'], dtype='object'),\n                                                'trestbps': pd.Series([0], dtype='int64'),\n                                                'chol': pd.Series([0], dtype='int64'),\n                                                'fbs': pd.Series(['example_value'], dtype='object'),\n                                                'restecg': pd.Series(['example_value'], dtype='object'),\n                                                'thalach': pd.Series([0], dtype='int64'),\n                                                'exang': pd.Series(['example_value'], dtype='object'),\n                                                'oldpeak': pd.Series([0.0], dtype='float64'),\n                                                'slope': pd.Series(['example_value'], dtype='object'),\n                                                'ca': pd.Series(['example_value'], dtype='object'),\n                                                'thal': pd.Series(['example_value'], dtype='object')}))\n\ninput_sample = StandardPythonParameterType({'data': data_sample})\nresult_sample = NumpyParameterType(np.array([0]))\noutput_sample = StandardPythonParameterType({'Results':result_sample})\n\ndef init():\n    global model\n    # Example when the model is a file\n    model_path = Model.get_model_path('log') # logistic\n    print('Model Path is  ', model_path)\n    model = joblib.load(model_path)\n\n@input_schema('Inputs', input_sample)\n@output_schema(output_sample)\ndef run(Inputs):\n    try:\n        data = Inputs['data']\n        result = model.predict_proba(data)\n        return result.tolist()\n    except Exception as e:\n        error = str(e)\n        return error\n<\/code><\/pre>\n<ol start=\"2\">\n<li>In the deployment step I adjusted the <code>CondaDependencies<\/code>:<\/li>\n<\/ol>\n<pre><code># to install required packages\nenv = Environment('env')\ncd = CondaDependencies.create(pip_packages=['pandas==1.1.5', 'azureml-defaults','joblib==0.17.0', 'inference-schema==1.3.0'], conda_packages = ['scikit-learn==0.22.2.post1'])\nenv.python.conda_dependencies = cd\n# Register environment to re-use later\nenv.register(workspace = ws)\nprint('Registered Environment')\n<\/code><\/pre>\n<p>as<\/p>\n<p>a) It is necessary to include <code>inference-schema<\/code> in the <code>Dependencies<\/code> file\nb) I downgraded <code>scikit-learn<\/code> to <code>scikit-learn==0.22.2.post1<\/code> version because of <a href=\"https:\/\/github.com\/hyperopt\/hyperopt\/issues\/668\" rel=\"nofollow noreferrer\">this issue<\/a><\/p>\n<p>Now, when I feed the model with new data:<\/p>\n<pre><code>new_data = {\n  &quot;Inputs&quot;: {\n    &quot;data&quot;: [\n      {\n        &quot;age&quot;: 71,\n        &quot;sex&quot;: &quot;0&quot;,\n        &quot;cp&quot;: &quot;0&quot;,\n        &quot;trestbps&quot;: 112,\n        &quot;chol&quot;: 203,\n        &quot;fbs&quot;: &quot;0&quot;,\n        &quot;restecg&quot;: &quot;1&quot;,\n        &quot;thalach&quot;: 185,\n        &quot;exang&quot;: &quot;0&quot;,\n        &quot;oldpeak&quot;: 0.1,\n        &quot;slope&quot;: &quot;2&quot;,\n        &quot;ca&quot;: &quot;0&quot;,\n        &quot;thal&quot;: &quot;2&quot;\n      }\n    ]\n  }\n}\n<\/code><\/pre>\n<p>And use it for prediction:<\/p>\n<pre><code>import json\nimport requests\ndata = new_data\nheaders = {'Content-Type':'application\/json'}\nr = requests.post(url, str.encode(json.dumps(data)), headers = headers)\nprint(r.status_code)\nprint(r.json())\n<\/code><\/pre>\n<p>I get:<\/p>\n<p><code>200 [[0.02325369841858338, 0.9767463015814166]]<\/code><\/p>\n<p>Uff! Maybe someone will benefit from my painful learning path! :)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1653559436590,
        "Solution_link_count":2.0,
        "Solution_readability":17.1,
        "Solution_reading_time":51.56,
        "Solution_score_count":0.0,
        "Solution_sentence_count":36.0,
        "Solution_word_count":293.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":23.4807313889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>my dataset is 3 folders (train, validation and test) of images. each folder has two subfolders (cat1 and cat2). I am using AWS sage maker to preprocess my data and train my model. we all know that we have to upload the training data to S3 bucket before starting the &quot;.fit&quot; process.\nI want to know how to upload my data set to S3<\/p>\n<pre><code># general prefix\nprefix='chest-xray'\n#unique train\/test prefixes\ntrain_prefix   = '{}\/{}'.format(prefix, 'train')\nval_prefix   = '{}\/{}'.format(prefix, 'validation')\ntest_prefix    = '{}\/{}'.format(prefix, 'test')\n\n# uploading data to S3, and saving locations\ntrain_path  = sagemaker_session.upload_data(train_data, bucket=bucket, key_prefix=train_prefix)\n<\/code><\/pre>\n<p>what the train_data parameters should look like<\/p>",
        "Challenge_closed_time":1611678605710,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611594075077,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to know how to upload their image dataset, which consists of three folders (train, validation, and test) with two subfolders each (cat1 and cat2), to S3 in AWS SageMaker for preprocessing and model training. They have provided a code snippet for uploading data to S3 but are unsure about the format of the \"train_data\" parameter.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65889143",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":10.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":23.4807313889,
        "Challenge_title":"upload image dataset to S3 sagemaker",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":444.0,
        "Challenge_word_count":104,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1477757915556,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":79.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>According to the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/utility\/session.html#sagemaker.session.Session.upload_data\" rel=\"nofollow noreferrer\">documentation<\/a> <code>train_data<\/code> is the local path of the file to upload to S3, so you need this file locally where you are launching the training job. If you are using a notebook this is not the way to do. You have instead to manually upload your dataset in a S3 bucket. I suggest to preprocess your dataset in a single file (tfrecord for example if you are using TF) and upload that file to S3. You can do it using the AWS web console or using the AWS-CLI with the <code>aws s3 cp yourfile s3:\/\/your-bucket <\/code>command.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":8.78,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":102.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":105.0398277778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a training script in Sagemaker like,<\/p>\n\n<pre><code>def train(current_host, hosts, num_cpus, num_gpus, channel_input_dirs, model_dir, hyperparameters, **kwargs):\n    ... Train a network ...\n    return net\n\ndef save(net, model_dir):\n    # save the model\n    logging.info('Saving model')\n    y = net(mx.sym.var('data'))\n    y.save('%s\/model.json' % model_dir)\n    net.collect_params().save('%s\/model.params' % model_dir)\n\ndef model_fn(model_dir):\n    symbol = mx.sym.load('%s\/model.json' % model_dir)\n    outputs = mx.symbol.softmax(data=symbol, name='softmax_label')\n    inputs = mx.sym.var('data')\n    param_dict = gluon.ParameterDict('model_')\n    net = gluon.SymbolBlock(outputs, inputs, param_dict)\n    net.load_params('%s\/model.params' % model_dir, ctx=mx.cpu())\n    return net\n<\/code><\/pre>\n\n<p>Most of which I stole from the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mnist.py\" rel=\"nofollow noreferrer\">MNIST Example<\/a>.<\/p>\n\n<p>When I train, everything goes fine, but when trying to deploy like,<\/p>\n\n<pre><code>m = MXNet(\"lstm_trainer.py\", \n          role=role, \n          train_instance_count=1, \n          train_instance_type=\"ml.c4.xlarge\",\n          hyperparameters={'batch_size': 100, \n                         'epochs': 20, \n                         'learning_rate': 0.1, \n                         'momentum': 0.9, \n                         'log_interval': 100})\nm.fit(inputs) # No errors\npredictor = m.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>I get, (<a href=\"https:\/\/gist.github.com\/aidan-plenert-macdonald\/7eb7ba7402790b61596938b5cbf605b6\" rel=\"nofollow noreferrer\">full output<\/a>)<\/p>\n\n<pre><code>INFO:sagemaker:Creating model with name: sagemaker-mxnet-py2-cpu-2018-01-17-20-52-52-599\n---------------------------------------------------------------------------\n  ... Stack dump ...\nClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not find model data at s3:\/\/sagemaker-us-west-2-01234567890\/sagemaker-mxnet-py2-cpu-2018-01-17-20-52-52-599\/output\/model.tar.gz.\n<\/code><\/pre>\n\n<p>Looking in my S3 bucket <code>s3:\/\/sagemaker-us-west-2-01234567890\/sagemaker-mxnet-py2-cpu-2018-01-17-20-52-52-599\/output\/model.tar.gz<\/code>, I in fact don't see the model.<\/p>\n\n<p>What am I missing?<\/p>",
        "Challenge_closed_time":1516602490380,
        "Challenge_comment_count":2,
        "Challenge_created_time":1516224347000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error message \"Could not find model data\" when trying to deploy their model in Sagemaker. They have checked their S3 bucket and confirmed that the model is not present. The user has provided code snippets for their training script and deployment process.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48310237",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":16.3,
        "Challenge_reading_time":30.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":105.0398277778,
        "Challenge_title":"Sagemaker \"Could not find model data\" when trying to deploy my model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2950.0,
        "Challenge_word_count":164,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1384712661608,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Diego, CA, United States",
        "Poster_reputation_count":5151.0,
        "Poster_view_count":1038.0,
        "Solution_body":"<p>When you are calling the training job you should specify the output directory:<\/p>\n\n<pre><code>#Bucket location where results of model training are saved.\nmodel_artifacts_location = 's3:\/\/&lt;bucket-name&gt;\/artifacts'\n\nm = MXNet(entry_point='lstm_trainer.py',\n          role=role,\n          output_path=model_artifacts_location,\n          ...)\n<\/code><\/pre>\n\n<p>If you don't specify the output directory the function will use a default location, that it might not have the permissions to create or write to.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.6,
        "Solution_reading_time":6.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416193017423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Gensokyo",
        "Answerer_reputation_count":880.0,
        "Answerer_view_count":111.0,
        "Challenge_adjusted_solved_time":24.3919611111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have several million images in my training folder and want to specify a subset of them for training - the way to do this seems to be with a manifest file as described here.<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/augmented-manifest.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/augmented-manifest.html<\/a><\/p>\n\n<p>But this seems to be geared towards labelled data. How can I start a sagemaker training job using sagemaker's Tensorflow <code>estimator.fit<\/code> with a list of files instead of the entire directory as input?<\/p>",
        "Challenge_closed_time":1570649242910,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570561431850,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to use a subset of several million images in their training folder for training on Sagemaker with Tensorflow. They have found a way to do this with a manifest file, but it seems to be geared towards labelled data. The user is looking for a way to start a Sagemaker training job using Tensorflow estimator.fit with a list of files instead of the entire directory as input.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58292566",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":8.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":24.3919611111,
        "Challenge_title":"How can I use a list of files as the training set on Sagemaker with Tensorflow?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":809.0,
        "Challenge_word_count":86,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416193017423,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Gensokyo",
        "Poster_reputation_count":880.0,
        "Poster_view_count":111.0,
        "Solution_body":"<p>You can use an input type pipe parameter like so: <\/p>\n\n<pre><code>hyperparameters = {'save_checkpoints_secs':None,\n                   'save_checkpoints_steps':1000}\n\ntf_estimator = TensorFlow(entry_point='.\/my-training-file', role=role,\n                          training_steps=5100, evaluation_steps=100,\n                          train_instance_count=1, train_instance_type='ml.p3.2xlarge',\n                          input_mode = 'Pipe',\n                          train_volume_size=300, output_path = 's3:\/\/sagemaker-pocs\/test-carlsoa\/kepler\/model',\n                          framework_version = '1.12.0', hyperparameters=hyperparameters, checkpoint_path = None)\n<\/code><\/pre>\n\n<p>And create the manifest file pipe as an input:<\/p>\n\n<pre><code>train_data = sagemaker.session.s3_input('s3:\/\/sagemaker-pocs\/test-carlsoa\/manifest.json',\n                                        distribution='FullyReplicated',\n                                        content_type='image\/jpeg',\n                                        s3_data_type='ManifestFile',\n                                        attribute_names=['source-ref']) \n                                        #attribute_names=['source-ref', 'annotations']) \ndata_channels = {'train': train_data}\n<\/code><\/pre>\n\n<p>Note that you can use ManifestFile or AugmentedManifestFile depending on whether you have extra data or labels to provide. Now you can use data_channels as the input to the tf estimator:<\/p>\n\n<p><code>tf_estimator.fit(inputs=data_channels, logs=True)<\/code><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":23.0,
        "Solution_reading_time":15.82,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":86.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1551797759387,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Krak\u00f3w, Poland",
        "Answerer_reputation_count":77.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":147.0432102778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained the AutoMl classification model on Vertex AI, unfortunately model does not work with batch predictions, whenever I try to score training dataset (same which was used for the successful model training) with batch predictions on Vertex AI I get a following error:<\/p>\n<p>&quot;Due to one or more errors, this training job was canceled on Nov 11, 2021 at 09:42AM&quot;.<\/p>\n<p>There is an option to get a details from this error and those say the following thing:<\/p>\n<p>&quot;Batch prediction job customer_value_label_cv_automl_gui encountered the following errors: INTERNAL&quot;<\/p>\n<p>Does anyone know what might be the reason for getting this kind of error? I am very surprised that the model cannot score the dataset that it was trained on. My dataset consists of 570 columns and about 300k of records. <a href=\"https:\/\/i.stack.imgur.com\/DRbjn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DRbjn.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/0MHrg.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0MHrg.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1637235864267,
        "Challenge_comment_count":2,
        "Challenge_created_time":1636622783290,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user trained an AutoML classification model on Vertex AI, but encountered an internal error when attempting to use batch predictions on the same training dataset. The error message states that the batch prediction job encountered an internal error, and the user is unsure of the reason for the error. The dataset used for training consists of 570 columns and about 300k records.",
        "Challenge_last_edit_time":1636706508710,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69925931",
        "Challenge_link_count":4,
        "Challenge_participation_count":3,
        "Challenge_readability":11.3,
        "Challenge_reading_time":15.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":170.3002713889,
        "Challenge_title":"Vertex AI model batch prediction failed with internal error",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":712.0,
        "Challenge_word_count":157,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1551797759387,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Krak\u00f3w, Poland",
        "Poster_reputation_count":77.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>We have been able to finally figure this out. As we were using model.batch_predict method described in the <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform.html\" rel=\"nofollow noreferrer\">official documentation<\/a> we unnecessary set the machine_type parameter. Finally, we were able to figure out that it was causing the issue, the machine was probably too weak. Once we removed this declaration this method started to use automatic resources and that solved the case. I wish Vertex AI errors were a little bit more informative because it took us a lot of trials and error to figure out.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.4,
        "Solution_reading_time":7.73,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":89.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1508924024027,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":118.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":504.0913175,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have registered a scikit learn model on my MLflow Tracking server, and I am loading it with <code>sklearn.load_model(model_uri)<\/code>.<\/p>\n<p>Now, I would like to access the signature of the model so I can get a list of the model's required inputs\/features so I can retrieve them from my feature store by name. I can't seem to find any utility or method in the <code>mlflow<\/code> API or the <code>MLFlowClient<\/code> API that will let me access a signature or inputs\/outputs attribute, even though I can see a list of inputs and outputs under each version of the model in the UI.<\/p>\n<p>I know that I can find the input sample and the model configuration in the model's artifacts, but that would require me actually downloading the artifacts and loading them manually in my script. I don't need to avoid that, but I am surprised that I can't just return the signature as a dictionary the same way I can return a run's parameters or metrics.<\/p>",
        "Challenge_closed_time":1645469817663,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643655088920,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has registered a scikit learn model on their MLflow Tracking server and is trying to access the model signature to retrieve a list of required inputs\/features. However, they are unable to find any utility or method in the mlflow API or the MLFlowClient API that will let them access the signature or inputs\/outputs attribute. They can see a list of inputs and outputs under each version of the model in the UI, but they don't want to download the artifacts and load them manually in their script.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70931309",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":12.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":504.0913175,
        "Challenge_title":"How to retrieve the model signature from the MLflow Model Registry",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":904.0,
        "Challenge_word_count":173,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1466188731112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Michigan",
        "Poster_reputation_count":414.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>The way to access the model's signature without downloading the MLModel file is under the loaded model. And then you'll access the model's attributes, such as its signature or even other Pyfunc-defined methods.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\n\nmodel = mlflow.pyfunc.load_model(&quot;runs:\/&lt;run_id&gt;\/model&quot;)\nprint(model._model_meta._signature)\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.5,
        "Solution_reading_time":5.3,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":41.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":147.1668483334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is AWS Sage Maker Auto Pilot suitable for NLP?<\/p>\n\n<p>We currently have a tensorflow model that does classification on input of a sequence of URLS (\nWe transform the URLs to Word vec and Char vec to feed it to the model).<\/p>\n\n<p>Looking at Sage Maker Auto Pilot documentation it says that it works on input in tabular form.\nI was wondering if we could use it to for our use case.<\/p>",
        "Challenge_closed_time":1578756147747,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578226347093,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is questioning whether AWS Sage Maker Auto Pilot is suitable for their Natural Language Processing (NLP) needs, as their current tensorflow model uses URL sequences transformed into Word and Char vectors for classification, while Sage Maker Auto Pilot works with tabular input.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59599721",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":5.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":147.1668483334,
        "Challenge_title":"Is AWS Sage Maker Auto Pilot suitable for NLP?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":261.0,
        "Challenge_word_count":79,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458550179920,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":383.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>No. SageMaker AutoPilot doesn't support deep learning at the moment, only classification and regression problems on tabular data. Technically, I guess you could pass embeddings in CSV format, and pray that XGBoost figures them out, but I seriously doubt that this would deliver meaningful results :)<\/p>\n\n<p>Amazon Comprehend does support fully managed custom classification models <a href=\"https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/how-document-classification.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/how-document-classification.html<\/a>. It may be worth taking a look at it.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":19.4,
        "Solution_reading_time":8.26,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":68.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1584379010350,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":18.4716680556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm having an issue with AWS when I try to create a device fleet with sagemaker :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nsagemaker_client = boto3.client('sagemaker', region_name=AWS_REGION)\nsagemaker_client.create_device_fleet(\n    DeviceFleetName=device_fleet_name,\n    RoleArn=iot_role_arn,\n    OutputConfig={\n        'S3OutputLocation': s3_device_fleet_output\n    }\n)\n\n<\/code><\/pre>\n<p>It raises the following exception:<\/p>\n<blockquote>\n<p>ClientError: An error occurred (ValidationException) when calling the CreateDeviceFleet operation: The account id &lt;my-account-id&gt; does not have ownership on bucket: &lt;bucket-name&gt;<\/p>\n<\/blockquote>\n<p>I dont get it because I created the bucket so I should be the owner. I have not found how to check or change bucket ownership.<\/p>\n<p>I tried changing the bucket policy as follows but it didn't help.<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;Statement1&quot;,\n            &quot;Principal&quot;: {\n                &quot;AWS&quot;: &quot;arn:aws:iam::&lt;id&gt;:user\/&lt;user&gt;&quot;\n            },\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: &quot;*&quot;,\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::&lt;bucket-name&gt;&quot;,\n                &quot;arn:aws:s3:::&lt;bucket-name&gt;\/*&quot;\n            ]\n        }\n    ]\n}\n\n<\/code><\/pre>\n<p>I also tried with sagemaker's GUI, it fails for the same reason (ValidationException, the account id &lt;my-account-id&gt; does not have ownership on bucket : &lt;bucket-name&gt;).<\/p>",
        "Challenge_closed_time":1641479927648,
        "Challenge_comment_count":2,
        "Challenge_created_time":1641413429643,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while trying to create a device fleet with Sagemaker on AWS. The CreateDeviceFleet operation fails with a ValidationException error message stating that the account ID does not have ownership on the bucket. The user has created the bucket and is unsure how to check or change bucket ownership. The user has also tried changing the bucket policy and using Sagemaker's GUI, but the issue persists.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70599052",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":17.7,
        "Challenge_reading_time":21.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":18.4716680556,
        "Challenge_title":"AWS CreateDeviceFleet operation fail because \"the account id does not have ownership on bucket\"",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":48.0,
        "Challenge_word_count":153,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1584379010350,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":36.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>This bucket policy made it work :<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;Statement1&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Principal&quot;: {\n                &quot;AWS&quot;: &quot;arn:aws:iam::&lt;account-id&gt;:role\/&lt;iot-role&gt;&quot;\n            },\n            &quot;Action&quot;: &quot;*&quot;,\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::&lt;bucket-name&gt;&quot;,\n                &quot;arn:aws:s3:::&lt;bucket-name&gt;\/*&quot;\n            ]\n        }\n    ]\n}\n<\/code><\/pre>\n<p>I still don't fully get it, because the role had full access on s3 buckets so i don't know why editing the bucket's policy changed something, but it works.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":29.5,
        "Solution_reading_time":9.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.5907397222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I\u2019m performing a series of experiments with AutoML and I need to see the featurized data. I mean, not just the new features names retrieved by method get_engineered_feature_names() or the featurization details retrieved by get_featurization_summary(), I refer to the whole transformed dataset, the one obtained after scaling\/normalization\/featurization that is then used to train the models.   <\/p>\n<p>Is it possible to access to this dataset or download it as a file?  <\/p>\n<p>Thanks.  <\/p>",
        "Challenge_closed_time":1618340955136,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618306428473,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is conducting experiments with AutoML and needs to access the featurized dataset, including the transformed data obtained after scaling, normalization, and featurization, which is used to train the models. They are seeking information on whether it is possible to access or download this dataset as a file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/355323\/how-to-access-to-the-featurized-dataset-in-automat",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.3,
        "Challenge_reading_time":6.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":9.5907397222,
        "Challenge_title":"How to access to the featurized dataset in Automated ML",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":82,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, currently, we don't store the dataset from scaling\/normalization\/featurization after the run is complete. This feature isn't supported at this time. Sorry for the inconvenience.<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.1,
        "Solution_reading_time":2.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1374169767267,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":548.0,
        "Answerer_view_count":70.0,
        "Challenge_adjusted_solved_time":7.0366380556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to call my SageMaker model endpoint both from Postman and the AWS CLI. The endpoint's status is &quot;in service&quot; but whenever I try to call it it gives me an error. When I try to use the predict function in the SageMaker notebook and provide it a numpy array (ex. <code>np.array([1,2,3,4])<\/code>), it successfully gives me an output. I'm unsure what I'm doing wrong.<\/p>\n<pre><code>$ aws2 sagemaker-runtime invoke-endpoint \\\n$ --endpoint-name=pytorch-model \\\n$ --body=1,2 \\\n$ --content-type=text\/csv \\\n$ --cli-binary-format=raw-in-base64-out \\\n$ output.json\n\nAn error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message &quot;tensors used as indices must be long, byte or bool tensors\nTraceback (most recent call last):\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/transformer.py&quot;, line 125, in transform\n    result = self._transform_fn(self._model, input_data, content_type, accept)\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/transformer.py&quot;, line 215, in _default_transform_fn\n    prediction = self._predict_fn(data, model)\n  File &quot;\/opt\/ml\/model\/code\/pytorch-model-reco.py&quot;, line 268, in predict_fn\n    return torch.argsort(- final_matrix[input_data, :], dim = 1)\nIndexError: tensors used as indices must be long, byte or bool tensors\n<\/code><\/pre>",
        "Challenge_closed_time":1596555829307,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596530497410,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to call their SageMaker model endpoint using Postman and AWS CLI. The endpoint's status is \"in service,\" but the error message indicates that tensors used as indices must be long, byte, or bool tensors. However, when the user tries to use the predict function in the SageMaker notebook and provides a numpy array, it successfully gives an output.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63243154",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.5,
        "Challenge_reading_time":18.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":7.0366380556,
        "Challenge_title":"Invoking SageMaker Endpoint for PyTorch Model",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":394.0,
        "Challenge_word_count":156,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The clue is in the final few lines of your stacktrace:<\/p>\n<pre><code>  File &quot;\/opt\/ml\/model\/code\/pytorch-model-reco.py&quot;, line 268, in predict_fn\n    return torch.argsort(- final_matrix[input_data, :], dim = 1)\nIndexError: tensors used as indices must be long, byte or bool tensors\n<\/code><\/pre>\n<p>In your <code>predict_fn<\/code> in <code>pytorch-model-reco.py<\/code> on line 268, you're trying to use <code>input_data<\/code> as indices for <code>final_matrix<\/code>, but <code>input_data<\/code> is the wrong type.<\/p>\n<p>I would guess there is some type casting that your <code>predict_fn<\/code> should be doing when the input type is <code>text\/csv<\/code>. This type casting is happening outside of the <code>predict_fn<\/code> when your input type is numpy data. Taking a look at the <a href=\"https:\/\/github.com\/aws\/sagemaker-inference-toolkit\/tree\/master\/src\/sagemaker_inference\" rel=\"nofollow noreferrer\"><code>sagemaker_inference<\/code><\/a> source code might reveal more.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.3,
        "Solution_reading_time":12.9,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":109.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1352206833663,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":893.0,
        "Answerer_view_count":185.0,
        "Challenge_adjusted_solved_time":28.8670294445,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I'm trying to define a Sagemaker Training Job with an existing Python class. To my understanding, I could create my own container but would rather not deal with container management.<\/p>\n\n<p>When choosing \"Algorithm Source\" there is the option of \"Your own algorithm source\" but nothing is listed under resources. Where does this come from?<\/p>\n\n<p>I know I could do this through a notebook, but I really want this defined in a job that can be invoked through an endpoint.<\/p>",
        "Challenge_closed_time":1550361262523,
        "Challenge_comment_count":0,
        "Challenge_created_time":1550257341217,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to create a Sagemaker training job with their own Tensorflow code without building a container. They are trying to define a training job with an existing Python class but do not want to deal with container management. They are looking for guidance on how to use the \"Your own algorithm source\" option under \"Algorithm Source\" as nothing is listed under resources. The user wants to define the job in a way that can be invoked through an endpoint.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54715601",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.4,
        "Challenge_reading_time":7.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":28.8670294445,
        "Challenge_title":"How do I create a Sagemaker training job with my own Tensorflow code without having to build a container?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":781.0,
        "Challenge_word_count":97,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1366768533200,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>As Bruno has said you will have to use a container somewhere, but you can use an existing container to run your own custom tensorflow code.<\/p>\n\n<p>There is a good example <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_quickstart\/tensorflow_script_mode_quickstart.ipynb\" rel=\"nofollow noreferrer\">in the sagemaker github<\/a> for how to do this.<\/p>\n\n<p>The way this works is you modify your code to have an entry point which takes argparse command line arguments, and then you point a 'Sagemaker Tensorflow estimator' to the entry point. Then when you call fit on the sagemaker estimator it will download the tensorflow container and run your custom code in there.<\/p>\n\n<p>So you start off with your own custom code that looks something like this<\/p>\n\n<pre><code># my_custom_code.py\nimport tensorflow as tf\nimport numpy as np\n\ndef build_net():\n    # single fully connected\n    image_place = tf.placeholder(tf.float32, [None, 28*28])\n    label_place = tf.placeholder(tf.int32, [None,])\n    net = tf.layers.dense(image_place, units=1024, activation=tf.nn.relu)\n    net = tf.layers.dense(net, units=10, activation=None)\n    return image_place, label_place, net\n\n\ndef process_data():\n    # load\n    (x_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()\n\n    # center\n    x_train = x_train \/ 255.0\n    m = x_train.mean()\n    x_train = x_train - m\n\n    # convert to right types\n    x_train = x_train.astype(np.float32)\n    y_train = y_train.astype(np.int32)\n\n    # reshape so flat\n    x_train = np.reshape(x_train, [-1, 28*28])\n    return x_train, y_train\n\n\ndef train_model(init_learn, epochs):\n    image_p, label_p, logit = build_net()\n    x_train, y_train = process_data()\n\n    loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n        logits=logit,\n        labels=label_p)\n    optimiser = tf.train.AdamOptimizer(init_learn)\n    train_step = optimiser.minimize(loss)\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for _ in range(epochs):\n            sess.run(train_step, feed_dict={image_p: x_train, label_p: y_train})\n\n\nif __name__ == '__main__':\n    train_model(0.001, 10)\n<\/code><\/pre>\n\n<p>To make it work with sagemaker we need to create a command line entry point, which will allow sagemaker to run it in the container it will download for us eventually.<\/p>\n\n<pre><code># entry.py\n\nimport argparse\nfrom my_custom_code import train_model\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\n        '--model_dir',\n        type=str)\n    parser.add_argument(\n        '--init_learn',\n        type=float)\n    parser.add_argument(\n        '--epochs',\n        type=int)\n    args = parser.parse_args()\n    train_model(args.init_learn, args.epochs)\n<\/code><\/pre>\n\n<p>Apart from specifying the arguments my function needs to take, we also need to provide a <code>model_dir<\/code> argument. This is always required, and is an S3 location which is where an model artifacts will be saved when the training job completes. Note that you don't need to specify what this value is (though you can) as Sagemaker will provide a default location in S3 for you.<\/p>\n\n<p>So we have modified our code, now we need to actually run it on Sagemaker. Go to the AWS console and fire up a small instance from Sagemaker. Download your custom code to the instance, and then create a jupyter notebook as follows:<\/p>\n\n<pre><code># sagemaker_run.ipyb\nimport sagemaker\nfrom sagemaker.tensorflow import TensorFlow\n\nhyperparameters = {\n    'epochs': 10,\n    'init_learn': 0.001}\n\nrole = sagemaker.get_execution_role()\nsource_dir = '\/path\/to\/folder\/with\/my\/code\/on\/instance'\nestimator = TensorFlow(\n    entry_point='entry.py',\n    source_dir=source_dir,\n    train_instance_type='ml.t2.medium',\n    train_instance_count=1,\n    hyperparameters=hyperparameters,\n    role=role,\n    py_version='py3',\n    framework_version='1.12.0',\n    script_mode=True)\n\nestimator.fit()\n<\/code><\/pre>\n\n<p>Running the above will:<\/p>\n\n<ul>\n<li>Spin up an ml.t2.medium instance<\/li>\n<li>Download the tensorflow 1.12.0 container to the instance<\/li>\n<li>Download any data we specify in fit to the newly created instance in fit (in this case nothing)<\/li>\n<li>Run our code on the instance<\/li>\n<li>upload the model artifacts to model_dir<\/li>\n<\/ul>\n\n<p>And that is pretty much it. There is of course a lot not mentioned here but you can:<\/p>\n\n<ul>\n<li>Download training\/testing data from s3<\/li>\n<li>Save checkpoint files, and tensorboard files during training and upload them to s3<\/li>\n<\/ul>\n\n<p>The best resource I found was the example I shared but here are all the things I was looking at to get this working:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_quickstart\/tensorflow_script_mode_quickstart.ipynb\" rel=\"nofollow noreferrer\">example code again<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"nofollow noreferrer\">documentation<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-containers#list-of-provided-environment-variables-by-sagemaker-containers\" rel=\"nofollow noreferrer\">explanation of environment variables<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":12.3,
        "Solution_reading_time":66.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":50.0,
        "Solution_word_count":546.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1627043269590,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Richmond, VA, USA",
        "Answerer_reputation_count":67.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":6.3228980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Suppose I have two Azure ML workspaces:<\/p>\n<ol>\n<li><p>Workspace1 - This is being used by one team (Team1) who only train the model and store the model in model registry of Workspace1<\/p>\n<\/li>\n<li><p>Workspace2 - This is used by another team  (Team2) who containerise the model, push it to ACR and then deploy the containerised model in Azure ML Compute.<\/p>\n<\/li>\n<\/ol>\n<p>Is it possible for Team2 to access the model registry of Workspace1 from their Workspace2 and retrieve the model for containerisation and subsequent deployment? Alternatively, is there any concept of a shared model registry in Azure ML where both the teams can store and access a common model registry? If none of these are possible, then what is the way for Team1 and Team2 to work together on a single model with the given responsibilities as described above?<\/p>",
        "Challenge_closed_time":1638210489528,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638197253147,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge of accessing an Azure ML Model Registry from another Azure ML Workspace. They have two workspaces, one used by Team1 to train and store the model in the registry, and the other used by Team2 to containerize and deploy the model. The user is seeking a way for Team2 to access the model registry of Workspace1 from their Workspace2 or a shared model registry where both teams can store and access a common model registry.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70156610",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":11.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":3.6767725,
        "Challenge_title":"Accessing an Azure ML Model Registry from another Azure ML Workspace",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":421.0,
        "Challenge_word_count":148,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1373471094267,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United Kingdom",
        "Poster_reputation_count":395.0,
        "Poster_view_count":66.0,
        "Solution_body":"<p>As described, I think the best solution is to use one Workspace, not two.  It sounds like you have Team 1 and Team 2 sharing contributions on a single project.  What may work better is to define user roles in the Azure ML workspace, such that Team 2 has permissions to deploy models, and Team 1 has permission to create models.<\/p>\n<p>Otherwise you can always write Python code using the ML SDK to connect to any workspace given you know the subscription, resource group, workspace name etc.<\/p>\n<pre><code>from azure.core import Workspace, Model\n\n# connect to an existing workspace\nname = 'WorkspaceName'\nsub = 'subscriptionName'\nresource_group = 'resourceGroupName'\nws = Workspace.get(name=name, subscription_id=sub, resource_group=resource_group) \n\n# retrieve existing model\nmodel = Model(ws, name='your model name')\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1638220015580,
        "Solution_link_count":0.0,
        "Solution_readability":9.8,
        "Solution_reading_time":10.42,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":116.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":992.0369444444,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\n\r\nCreate a notebook instance in one of the configured region.\r\nRan the below query and got that error\r\n\r\n```\r\nselect * from aws_sagemaker_notebook_instance;\r\nError: hydrate call listAwsSageMakerNotebookInstanceTags failed with panic interface conversion: interface {} is *sagemaker.NotebookInstanceSummary, not *sagemaker.DescribeNotebookInstanceOutput\r\n\r\n```\r\n\r\n\r\n\r\n**Steampipe version (`steampipe -v`)**\r\n: v0.4.1\r\n\r\n**Plugin version (`steampipe plugin list`)**\r\naws: v0.15.0\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1623682749000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620111416000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"the user encountered a challenge where zenml was failing to create a s3 bucket due to an incorrect regex in its name.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/turbot\/steampipe-plugin-aws\/issues\/364",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.4,
        "Challenge_reading_time":7.39,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":56.0,
        "Challenge_repo_issue_count":1763.0,
        "Challenge_repo_star_count":145.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":992.0369444444,
        "Challenge_title":"Getting an error from `aws_sagemaker_notebook_instance` table. Please see the detail below.",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":60,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1428454496052,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":35.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":94.0638861111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to serialize a list of java Objects (POJO) into RecordIO format. I have seen this BeanIO (<a href=\"http:\/\/beanio.org\/\" rel=\"nofollow noreferrer\">http:\/\/beanio.org\/<\/a>) but it seems to be outdated. Is there any other Java library that could be used or a different way to do this ?<\/p>\n\n<p>Once list of objects is serialized it will be used to train a model with SageMaker.<\/p>",
        "Challenge_closed_time":1526652887110,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526314257120,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to serialize a list of Java objects into RecordIO format for training a model with SageMaker. They have looked into using BeanIO but found it to be outdated and are seeking alternative Java libraries or methods to accomplish this task.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50334735",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":5.35,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":94.0638861111,
        "Challenge_title":"How to generate Recordio from Java object",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":160.0,
        "Challenge_word_count":69,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1428454496052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Solving my own problem. I decided to use Apache Avro instead of BeanIO. Spark allow to serialize using Avro (c.f. Spark-Avro). This seems to work however it did not fit my use case has I was trying to serialize an array of numbers.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.1,
        "Solution_reading_time":2.88,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":43.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":21.5095830556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>On the web\u201c <a href=\"https:\/\/learn.microsoft.com\/en-au\/dotnet\/machine-learning\/how-to-guides\/matchup-app-infer-net\">https:\/\/learn.microsoft.com\/en-au\/dotnet\/machine-learning\/how-to-guides\/matchup-app-infer-net<\/a> \u201dFor Infer.net Probability programming example of. In this example, there are only wins or losses, no draws. Can you give me an example of a draw, which can be used in the machine learning of E-sports Bo 2 or football, thank you<\/p>",
        "Challenge_closed_time":1593434922092,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593357487593,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking an example of a draw in Infer.net probability programming for machine learning in E-sports Bo 2 or football, as the provided example only includes wins or losses.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/40647\/can-you-give-me-an-example-of-a-draw-with-infer-ne",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":6.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":21.5095830556,
        "Challenge_title":"Can you give me an example of a draw with Infer.net",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":57,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The <a href=\"https:\/\/dotnet.github.io\/infer\/userguide\/Chess%20Analysis.html\">Chess Analysis<\/a> example in the Infer.NET documentation includes draws.  <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.8,
        "Solution_reading_time":2.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":12.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":4.8427816667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I registered a model in my AML workspace, and I can see it in the Model List:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rtL5Q.png\" rel=\"nofollow noreferrer\">Model List view<\/a><\/p>\n<p>But I cannot see it in Designer (preview), which prevents me from using the new model there.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/WpvIb.png\" rel=\"nofollow noreferrer\">Designer view<\/a><\/p>\n<p>Looks like a bug to me. Datasets work fine.<\/p>",
        "Challenge_closed_time":1596073257467,
        "Challenge_comment_count":3,
        "Challenge_created_time":1596056234913,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has registered a model in their AML workspace, but it is not showing up in Designer (preview), which is preventing them from using the new model. The user suspects it is a bug as datasets are working fine.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63162310",
        "Challenge_link_count":2,
        "Challenge_participation_count":5,
        "Challenge_readability":7.5,
        "Challenge_reading_time":6.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":4.7284872222,
        "Challenge_title":"Models registered in workspace do not show up in Designer (preview)",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":81.0,
        "Challenge_word_count":64,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1531852372996,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>This is known issue as the models registered in workspace cannot be consumed in Designer without the new custom module capability (in private preview) available.<\/p>\n<p>The models showing up in Designer today are these generated from Designer training -&gt; inference pipeline conversion and can only be used in Designer (not registered in the workspace).\nWe have an effort ongoing to reduce the confusion.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1596073668927,
        "Solution_link_count":0.0,
        "Solution_readability":12.7,
        "Solution_reading_time":5.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1354669077990,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":552.0,
        "Answerer_view_count":68.0,
        "Challenge_adjusted_solved_time":3358.3124163889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The situation:<\/p>\n\n<p>I've already created several models, trained over several days each, that we're ready to move from local testing to a serving environment.<\/p>\n\n<p>The models were saved using the function<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def save_graph_to_file(sess, graph, graph_file_name):\n    \"\"\"Saves an graph to file, creating a valid quantized one if necessary.\"\"\"\n    output_graph_def = graph_util.convert_variables_to_constants(sess, graph.as_graph_def(), [final_tensor_name])\n    with gfile.FastGFile(graph_file_name, 'wb') as f:\n        f.write(output_graph_def.SerializeToString())\n<\/code><\/pre>\n\n<p>Now when attempting to deploy to a serving environment (Sagemaker, using a correct directory structure and file naming convention), the system returns<\/p>\n\n<pre><code>2019-06-04 22:38:53.794056: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\n2019-06-04 22:38:53.798096: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:259] SavedModel load for tags { serve }; Status: fail. Took 83297 microseconds.\n2019-06-04 22:38:53.798132: E tensorflow_serving\/util\/retrier.cc:37] Loading servable: {name: model version: 1} failed: Not found: Could not find meta graph def matching supplied tags: { serve }. To inspect available tag-sets in the SavedModel, please use the SavedModel CLI: `saved_model_cli`\n<\/code><\/pre>\n\n<p>All I have are the <code>*.pb<\/code> files and their label textfiles. These work lovely across multiple computers in local environments. <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def load_graph(model_file):\n    \"\"\"\n    Code from v1.6.0 of Tensorflow's label_image.py example\n    \"\"\"\n    graph = tf.Graph()\n    graph_def = tf.GraphDef()\n    with open(model_file, \"rb\") as f:\n        graph_def.ParseFromString(f.read())\n    with graph.as_default():\n        tf.import_graph_def(graph_def)\n    return graph\n\ninputLayer = \"Mul\"\noutputLayer = \"final_result\"\ninputName = \"import\/\" + inputLayer\noutputName = \"import\/\" + outputLayer\ngraph = load_graph(modelPath)\ninputOperation = graph.get_operation_by_name(inputName)\noutputOperation = graph.get_operation_by_name(outputName)\nwith tf.Session(graph= graph) as sess:\n    # ... make a tensor t\n    results = sess.run(outputOperation.outputs[0], {\n        inputOperation.outputs[0]: t\n    })\n    # lovely functional results here\n<\/code><\/pre>\n\n<p>All I want to do is to take these existing files, add the \"serve\" tag needed, and re-save them, but everything I see seems to be related to doing this from scratch.<\/p>\n\n<p>I tried to use the builder to append a graph to a model like so:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Load the graph\ngraph = load_graph(modelPath)\nimport shutil\nif os.path.exists(exportDir):\n    shutil.rmtree(exportDir)\n# Add the serving metagraph tag\nbuilder = tf.saved_model.builder.SavedModelBuilder(exportDir)\nfrom tensorflow.saved_model import tag_constants\nwith tf.Session(graph= graph) as sess:\n    builder.add_meta_graph_and_variables(sess, [tag_constants.SERVING, tag_constants.GPU], strip_default_attrs= True)\nbuilder.save()\nprint(\"Built a SavedModel\")\n<\/code><\/pre>\n\n<p>but got the same error.<\/p>",
        "Challenge_closed_time":1560534456823,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559778036413,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has created several models and saved them using a function. However, when attempting to deploy to a serving environment, the system returns an error stating that it could not find the meta graph def matching supplied tags. The user has tried to add the \"serve\" tag needed and re-save the files using the builder, but still gets the same error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56469341",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":41.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":210.1167805556,
        "Challenge_title":"Adding a {serve} metagraph to existing Tensorflow model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1140.0,
        "Challenge_word_count":322,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1354669077990,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":552.0,
        "Poster_view_count":68.0,
        "Solution_body":"<p>Finally solved it. This contains some S3 specific code and S3 instance calls (the <code>!<\/code> commands) but you should pretty much be able to slice that out to run this.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>#!python3\n\"\"\"\nAssumes we've defined:\n\n- A directory for our working files to live in, CONTAINER_DIR\n- an arbitrary integer VERSION_INT\n- We have established local and S3 paths for our model and their labels as variables, particularly `modelLabel` and `modelPath`\n\"\"\"\n\n# Create a versioned path for the models to live in\n# See https:\/\/stackoverflow.com\/a\/54014480\/1877527\nexportDir = os.path.join(CONTAINER_DIR, VERSION_INT)\nif os.path.exists(exportDir):\n    shutil.rmtree(exportDir)\nos.mkdir(exportDir)\nimport tensorflow as tf\ndef load_graph(model_file, returnElements= None):\n    \"\"\"\n    Code from v1.6.0 of Tensorflow's label_image.py example\n    \"\"\"\n    graph = tf.Graph()\n    graph_def = tf.GraphDef()\n    with open(model_file, \"rb\") as f:\n        graph_def.ParseFromString(f.read())\n    returns = None\n    with graph.as_default():\n        returns = tf.import_graph_def(graph_def, return_elements= returnElements)\n    if returnElements is None:\n        return graph\n    return graph, returns\n\n# Add the serving metagraph tag\n# We need the inputLayerName; in Inception we're feeding the resized tensor\n# corresponding to resized_input_tensor_name\n# May be able to get away with auto-determining this if not using Inception,\n# but for Inception this is the 11th layer\ninputLayerName = \"Mul:0\"\n# Load the graph\nif inputLayerName is None:\n    graph = load_graph(modelPath)\n    inputTensor = None\nelse:\n    graph, returns = load_graph(modelPath, returnElements= [inputLayerName])\n    inputTensor = returns[0]\nwith tf.Session(graph= graph) as sess:\n    # Read the layers\n    try:\n        from tensorflow.compat.v1.saved_model import simple_save\n    except (ModuleNotFoundError, ImportError):\n        from tensorflow.saved_model import simple_save\n    with graph.as_default():\n        layers = [n.name for n in graph.as_graph_def().node]\n        outName = layers.pop() + \":0\"\n        if inputLayerName is None:\n            inputLayerName = layers.pop(0) + \":0\"\n    print(\"Checking outlayer\", outName)\n    outLayer = tf.get_default_graph().get_tensor_by_name(outName)\n    if inputTensor is None:\n        print(\"Checking inlayer\", inputLayerName)\n        inputTensor = tf.get_default_graph().get_tensor_by_name(inputLayerName)\n    inputs = {\n        inputLayerName: inputTensor\n    }\n    outputs = {\n        outName: outLayer\n    }\n    simple_save(sess, exportDir, inputs, outputs)\nprint(\"Built a SavedModel\")\n# Put the model label into the artifact dir\nmodelLabelDest = os.path.join(exportDir, \"saved_model.txt\")\n!cp {modelLabel} {modelLabelDest}\n# Prep for serving\nimport datetime as dt\nmodelArtifact = f\"livemodel_{dt.datetime.now().timestamp()}.tar.gz\"\n# Copy the version directory here to package\n!cp -R {exportDir} .\/\n# gziptar it\n!tar -czvf {modelArtifact} {VERSION_INT}\n# Shove it back to S3 for serving\n!aws s3 cp {modelArtifact} {bucketPath}\nshutil.rmtree(VERSION_INT) # Cleanup\nshutil.rmtree(exportDir) # Cleanup\n<\/code><\/pre>\n\n<p>This model is then deployable as a Sagemaker endpoint (and any other Tensorflow serving environment)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1571867961112,
        "Solution_link_count":1.0,
        "Solution_readability":12.0,
        "Solution_reading_time":39.28,
        "Solution_score_count":1.0,
        "Solution_sentence_count":32.0,
        "Solution_word_count":327.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1598606826112,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":41.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":222.6413147222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm using sagemaker 2.5.1 and tensorflow 2.3.0\nThe weird part is that the same code worked before, the only change that I could think of is the new release of the two libraries<\/p>",
        "Challenge_closed_time":1599662565023,
        "Challenge_comment_count":1,
        "Challenge_created_time":1598861056290,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to save a TensorFlow model in an S3 bucket using SageMaker 2.5.1. The error message 'KeyError: 'callable_inputs'' is being displayed. The user suspects that the issue might be related to the recent release of the two libraries.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63667022",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":5.3,
        "Challenge_reading_time":3.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":222.6413147222,
        "Challenge_title":"Getting KeyError : 'callable_inputs' when trying to save a TF model in S3 bucket",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":174.0,
        "Challenge_word_count":45,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1598606826112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>The problem is actually coming from smdebug version 0.9.1\nDowngrading to 0.8.1 solves the issue<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":1.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":15.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.1106713889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,  <br \/>\nI have been using object detection from Custom Vision to train images. Classification do not suit my goal so I'm looking at alternative methods to training images. Can I get some suggestions with using Azure for images segmentation?<\/p>",
        "Challenge_closed_time":1610955618900,
        "Challenge_comment_count":1,
        "Challenge_created_time":1610951620483,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking suggestions for using Azure to train images for segmentation, as classification does not meet their requirements.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/234232\/images-segmentation-using-azure",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.7,
        "Challenge_reading_time":3.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.1106713889,
        "Challenge_title":"Images Segmentation using Azure",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":43,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=d94e82d7-8d83-4f86-8055-f9af74b9130d\">@Nam Ly  <\/a>    <\/p>\n<p>Suggestions and refer below url for Azure for images segmentation.    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/samples\/browse\/?products=azure&amp;term=vision&amp;terms=%22Custom%20Vision%22\">Custom Vision integration sample skill for cognitive search<\/a>    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/learn\/modules\/classify-images-custom-vision\/\">Classify images with the Custom Vision service<\/a>    <\/p>\n<p>Please don\u2019t forget to <code>Accept the answer<\/code> and <code>up-vote<\/code> wherever the information provided helps you, this can be beneficial to other community members.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.2,
        "Solution_reading_time":9.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":60.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":133.7906688889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am attempting to follow this <a href=\"http:\/\/www.toptal.com\/machine-learning\/predicting-gas-prices-using-azure-machine-learning-studio\" rel=\"nofollow\">tutorial<\/a> however I was attempting to predict MPG for a set of cars rather than oil prices and have the following set up:<\/p>\n\n<ol>\n<li>MPG Sample dataset<\/li>\n<li>Remove missing values, project everything (weight, displacement, cylinders, etc) except model name<\/li>\n<li>Split 75 to train model, 25 to score model<\/li>\n<li>Train model on MPG column with neural network<\/li>\n<li>Score model which is fed by Train Model and Split<\/li>\n<li>Score model is fed to Evaluate model<\/li>\n<\/ol>\n\n<p>This all seems to run fine and without issue, so I create a scoring experiment and then publish it as a web service, however when I attempt to input values it is asking for an MPG input. My understanding is that this would be the predicted value, so it seems somewhat opposite to have to enter this as a value, or am I just understanding a basic tenet of machine learning? <\/p>\n\n<p>In short: Ideally I would like to be able to enter everything but the MPG and get a prediction on what the MPG is for a given set of value.<\/p>",
        "Challenge_closed_time":1432778316968,
        "Challenge_comment_count":0,
        "Challenge_created_time":1432296670560,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is attempting to predict MPG for a set of cars using Azure Machine Learning Prediction. They have followed a tutorial and set up a model using a neural network, which runs without issue. However, when attempting to input values, the system is asking for an MPG input, which seems opposite to the user's understanding of machine learning. The user would like to be able to enter everything but the MPG and get a prediction on what the MPG is for a given set of values.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/30396392",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":18.1,
        "Challenge_reading_time":15.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":133.7906688889,
        "Challenge_title":"Azure Machine Learning Prediction - Input and Outputs",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1527.0,
        "Challenge_word_count":190,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1352816892227,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United States",
        "Poster_reputation_count":2054.0,
        "Poster_view_count":260.0,
        "Solution_body":"<p>You could also add project columns to exclude label as part of scoring experiment and connect web service output port to the output of project columns<\/p>",
        "Solution_comment_count":8.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.3,
        "Solution_reading_time":1.94,
        "Solution_score_count":2.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":26.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1631803441500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"mexico",
        "Answerer_reputation_count":1258.0,
        "Answerer_view_count":685.0,
        "Challenge_adjusted_solved_time":23.1295969444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to set up mlops for Vertex AI, following <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/mlops-with-vertex-ai\/blob\/main\/01-dataset-management.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a>. It works until, near the end, I try:<\/p>\n<pre><code>vertex_ai.init(\nproject=PROJECT,\n    location=REGION)\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code> module 'google.cloud.aiplatform.constants' has no attribute 'SUPPORTED_REGIONS\n<\/code><\/pre>\n<p>I am using <code>us-central1<\/code> which is supported. I wondered if maybe <code>from google.cloud import aiplatform as vertex_ai<\/code> has been changed but don't know how to find out. Any help is much appreciated.<\/p>",
        "Challenge_closed_time":1652193787552,
        "Challenge_comment_count":2,
        "Challenge_created_time":1652110521003,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while setting up mlops for Vertex AI. The error message 'google.cloud.aiplatform.constants' has no attribute 'SUPPORTED_REGIONS' is displayed when trying to initialize Vertex AI with the project and location. The user is using a supported region, but is unsure if there have been any changes to the import statement.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72174602",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":11.2,
        "Challenge_reading_time":10.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":23.1295969444,
        "Challenge_title":"Why do I get 'google.cloud.aiplatform.constants' has no attribute 'SUPPORTED_REGIONS error for Vertex AI init?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":298.0,
        "Challenge_word_count":83,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>I followed the same Notebook as you, even though I didn't have any issue. What could be happening to you is that you are using an <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/releases\" rel=\"nofollow noreferrer\">older version of the library<\/a>.<\/p>\n<p>You can use the command to upgrade the library that is the following one: <code>pip3 install google-cloud-aiplatform --upgrade<\/code>.<\/p>\n<p>Sometimes this happens with the basic installation of the library; the problems could be in the <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform#installation\" rel=\"nofollow noreferrer\">dependencies, versions and indirectly permissions<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.6,
        "Solution_reading_time":8.59,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":76.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1530092504712,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":915.0,
        "Answerer_view_count":288.0,
        "Challenge_adjusted_solved_time":0.5786791667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to figure out how to store intermediate Kedro pipeline objects both locally AND on S3. In particular, say I have a dataset on S3:<\/p>\n<pre><code>my_big_dataset.hdf5:\n  type: kedro.extras.datasets.pandas.HDFDataSet\n  filepath: &quot;s3:\/\/my_bucket\/data\/04_feature\/my_big_dataset.hdf5&quot;\n<\/code><\/pre>\n<p>I want to refer to these objects in the catalog by their S3 URI so that my team can use them. HOWEVER, I want to avoid re-downloading the datasets, model weights, etc. every time I run a pipeline by keeping a local copy in addition to the S3 copy. How do I mirror files with Kedro?<\/p>",
        "Challenge_closed_time":1597010598252,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597008515007,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to find a way to store Kedro pipeline objects both locally and on S3, while avoiding re-downloading datasets and model weights every time they run a pipeline. They want to refer to objects in the catalog by their S3 URI, but also keep a local copy. The user is seeking guidance on how to mirror files with Kedro.",
        "Challenge_last_edit_time":1597058318400,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63331505",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":8.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.5786791667,
        "Challenge_title":"How to catalog datasets & models by S3 URI, but keep a local copy?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":595.0,
        "Challenge_word_count":100,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1415053264667,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"USA",
        "Poster_reputation_count":11166.0,
        "Poster_view_count":653.0,
        "Solution_body":"<p>This is a good question, Kedro has <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.CachedDataSet.html\" rel=\"nofollow noreferrer\"><code>CachedDataSet<\/code><\/a> for caching datasets within the same run, which handles caching the dataset in memory when it's used\/loaded multiple times in the same run. There isn't really the same thing that persists across runs, in general Kedro doesn't do much persistent stuff.<\/p>\n<p>That said, off the top of my head, I can think of two options that (mostly) replicates or gives this functionality:<\/p>\n<ol>\n<li>Use the same <code>catalog<\/code> in the same config environment but with the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/04_kedro_project_setup\/02_configuration.html?#templating-configuration\" rel=\"nofollow noreferrer\"><code>TemplatedConfigLoader<\/code><\/a> where your catalog datasets have their filepaths looking something like:<\/li>\n<\/ol>\n<pre><code>my_dataset:\n  filepath: ${base_data}\/01_raw\/blah.csv\n<\/code><\/pre>\n<p>and you set <code>base_data<\/code> to <code>s3:\/\/bucket\/blah<\/code> when running in &quot;production&quot; mode and with <code>local_filepath\/data<\/code> locally. You can decide how exactly you do this in your overriden <code>context<\/code> method (whether it's using <code>local\/globals.yml<\/code> (see the linked documentation above) or environment variables or what not.<\/p>\n<ol start=\"2\">\n<li>Use separate environments, likely <code>local<\/code> (it's kind of what it was made for!) where you keep a separate copy of your catalog where the filepaths are replaced with local ones.<\/li>\n<\/ol>\n<p>Otherwise, your next best bet is to write a <code>PersistentCachedDataSet<\/code> similar to <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.CachedDataSet.html\" rel=\"nofollow noreferrer\"><code>CachedDataSet<\/code><\/a> which intercepts the loading\/saving for the wrapped dataset and makes a local copy when loading for the first time in a deterministic location that you look up on subsequent loads.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":14.2,
        "Solution_reading_time":25.97,
        "Solution_score_count":4.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":227.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1384343462316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pune, Maharashtra, India",
        "Answerer_reputation_count":478.0,
        "Answerer_view_count":118.0,
        "Challenge_adjusted_solved_time":48.2131977778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I tried to create MLproject with zero parameters as:<\/p>\n\n<pre><code>name: test\n\nconda_env: conda.yaml\n\nentry_points:\n  main:\n    parameters:\n    command: \"python test.py\"\n<\/code><\/pre>\n\n<p>when I get an error:<\/p>\n\n<pre><code>  Traceback (most recent call last):\n File \"\/home\/ubuntu\/.local\/bin\/mlflow\", line 11, in &lt;module&gt;\n    sys.exit(cli())\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/cli.py\", line 137, in run\n    run_id=run_id,\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/__init__.py\", line 230, in run\n    use_conda=use_conda, storage_dir=storage_dir, synchronous=synchronous, run_id=run_id)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/__init__.py\", line 85, in _run\n    project = _project_spec.load_project(work_dir)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/_project_spec.py\", line 40, in load_project\n    entry_points[name] = EntryPoint(name, parameters, command)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/_project_spec.py\", line 87, in __init__\n    self.parameters = {k: Parameter(k, v) for (k, v) in parameters.items()}\nAttributeError: 'NoneType' object has no attribute 'items'\n<\/code><\/pre>\n\n<p>Am I missing something or mlflow does not allow project with  zero parameters?<\/p>\n\n<p>I have also posted this at my public repo of: <a href=\"https:\/\/github.com\/sameermahajan\/mlflow-try\" rel=\"nofollow noreferrer\">https:\/\/github.com\/sameermahajan\/mlflow-try<\/a> if someone would like to try out:<\/p>\n\n<pre><code>mlflow run https:\/\/github.com\/sameermahajan\/mlflow-try.git\n<\/code><\/pre>",
        "Challenge_closed_time":1562240543612,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562066976100,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create an MLproject with zero parameters using mlflow, but is encountering an error message indicating that 'NoneType' object has no attribute 'items'. The user is unsure if they are missing something or if mlflow does not allow projects with zero parameters.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56851463",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":30.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":48.2131977778,
        "Challenge_title":"How do I specify mlflow MLproject with zero parameters?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":353.0,
        "Challenge_word_count":185,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1384343462316,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":478.0,
        "Poster_view_count":118.0,
        "Solution_body":"<p>For this, you completely drop the 'parameters' section as below:<\/p>\n\n<pre><code>name: test\n\nconda_env: conda.yaml\n\nentry_points:\n  main:\n    command: \"python test.py\"\n<\/code><\/pre>\n\n<p>(I thought I had tried it earlier but I was trying too many different ways to may be miss out on this one)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.7,
        "Solution_reading_time":3.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":43.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":21.8707033333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained a machine learning model in Databricks workspace using mlflow. Model is getting registered in databricks model registry and saved in databricks file share. Now I want to download model artifacts from workspace. Currently I am transferring model to azure machine learning workspace. There I am able to download all the artifacts. How to do it from databricks workspace? <\/p>",
        "Challenge_closed_time":1665051150292,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664972415760,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has trained a machine learning model in Databricks workspace using mlflow and wants to download the model artifacts from the workspace to a local directory. The model is registered in the Databricks model registry and saved in the Databricks file share. The user is currently transferring the model to Azure machine learning workspace to download all the artifacts and is seeking guidance on how to download the artifacts directly from the Databricks workspace.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1036179\/how-to-download-mlflow-model-artifacts-from-azure",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.1,
        "Challenge_reading_time":6.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":21.8707033333,
        "Challenge_title":"How to download mlflow model artifacts from Azure Databricks workspace to local directory?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":74,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=c18ee9ee-fe4d-4f61-a87a-4da34719e169\">@Sriram Reddy  <\/a> Thanks for the question. To download a model from Databricks workspace you need to do two things:    <\/p>\n<p>Set MLFlow tracking URI to databricks using python API    <\/p>\n<p>Setup databricks authentication. I prefer authenticating by setting the following environment variables, you can also use databricks CLI to authenticate:    <\/p>\n<p>DATABRICKS_HOST    <\/p>\n<p>DATABRICKS_TOKEN    <br \/>\nHere's a basic code snippet to download a model from Databricks workspace model registry:    <\/p>\n<pre><code>import os  \nimport mlflow  \nfrom mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository  \n  \nmodel_name = &quot;example-model-name&quot;  \nmodel_stage = &quot;Staging&quot;  # Should be either 'Staging' or 'Production'  \n  \nmlflow.set_tracking_uri(&quot;databricks&quot;)  \n  \nos.makedirs(&quot;model&quot;, exist_ok=True)  \nlocal_path = ModelsArtifactRepository(  \n    f'models:\/{model_name}\/{model_stage}').download_artifacts(&quot;&quot;, dst_path=&quot;model&quot;)  \n  \nprint(f'{model_stage} Model {model_name} is downloaded at {local_path}')  \n<\/code><\/pre>\n<p>Running above python script will download an ML model in the model directory.    <\/p>\n<p>Containerizing MLFlow model serving with Docker    <\/p>\n<p>For more information you can follow this <a href=\"https:\/\/dev.to\/itachiredhair\/downloading-mlflow-model-from-databricks-and-serving-with-docker-38ip\">article<\/a> from Akshay Milmile    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.2,
        "Solution_reading_time":19.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":139.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":11.9719969444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a new experiment in Azure Machine Learning and added two datasets by manually uploading csv's.<\/p>\n\n<ul>\n<li>One is from a customer of which I'd like to predict which products he will order next. <\/li>\n<li>The second dataset has the same type of data, only then from all other customers as reference for learning.<\/li>\n<\/ul>\n\n<p>I have <code>productid<\/code>, <code>amount<\/code>, and <code>orderdate<\/code> and <code>orderid<\/code> for grouping and putting it on a timeframe.\nThe customer (dataset one) is always several months behind with ordering the latest products. therefor I added the dataset two with all other customers as reference.<\/p>\n\n<p>Also because the reference can tell which products are more popular (ordered more and by several customers) so perhaps I should add a customerid column to the dataset.<\/p>\n\n<p>I know how to start and get the data in, and I do know that it is common to split the data for training, feed it to the train model with a <code>Ilearnerdotnet<\/code> type and give the output to the score model and evaluate the model.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/qH7lb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qH7lb.png\" alt=\"current experiment\"><\/a>\nI do not know how to choose a classification type and how this can give an output for the next three months of order. I have read some tutorials, but I just need someone who can give me some pointers.<\/p>\n\n<p><strong>edit<\/strong> I have added the customerid to the dataset so that I have just one set now which I should split to focus on a specific customer.\n<strong>edit2<\/strong> found these templates. will look into it <a href=\"https:\/\/stackoverflow.com\/a\/36552849\/169714\">https:\/\/stackoverflow.com\/a\/36552849\/169714<\/a><\/p>",
        "Challenge_closed_time":1463090186116,
        "Challenge_comment_count":0,
        "Challenge_created_time":1463047086927,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has created an experiment in Azure Machine Learning to predict which products a customer will order next. They have added two datasets, one from the customer and the other from all other customers as reference for learning. The user has the necessary data, but they do not know how to choose a classification type and how to get an output for the next three months of order. They have added a customerid column to the dataset and are looking for guidance.",
        "Challenge_last_edit_time":1495541203567,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37183494",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":8.5,
        "Challenge_reading_time":22.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":11.9719969444,
        "Challenge_title":"Azure machine learning predict order for customer",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":845.0,
        "Challenge_word_count":263,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1252330528847,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Netherlands",
        "Poster_reputation_count":5867.0,
        "Poster_view_count":1692.0,
        "Solution_body":"<p>Go over this <a href=\"http:\/\/download.microsoft.com\/download\/0\/5\/A\/05AE6B94-E688-403E-90A5-6035DBE9EEC5\/machine-learning-basics-infographic-with-algorithm-examples.pdf\" rel=\"nofollow\">http:\/\/download.microsoft.com\/download\/0\/5\/A\/05AE6B94-E688-403E-90A5-6035DBE9EEC5\/machine-learning-basics-infographic-with-algorithm-examples.pdf<\/a><\/p>\n\n<p>If above infographic doesn't help, then you can try all of the learners by going over this experiment and use the one with best results - <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Algo-Evaluater-Compare-Performance-of-Multiple-Algos-against-Your-Data-1\" rel=\"nofollow\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Algo-Evaluater-Compare-Performance-of-Multiple-Algos-against-Your-Data-1<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":46.5,
        "Solution_reading_time":10.69,
        "Solution_score_count":3.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.3669444444,
        "Challenge_answer_count":0,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\n\r\nSteps:\r\n1. Create a new AzureML workspace.\r\n    - Name: `azureml-test-workspace`\r\n    - Resource group: `recommenders_project_resources`\r\n    - Location: *Make sure you have enough quota in the location you choose*\r\n2. Create two new clusters: `cpu-cluster` and `gpu-cluster`. Go to compute, then compute cluster, then new.\r\n    - Select the CPU VM base. Anything above 32GB of RAM, and 8 cores should be fine.\r\n    - Select the GPU VM base. Anything above 56GB of RAM, and 6 cores, and an NVIDIA K80 should be fine.\r\n3. Add the subscription ID to GitHub action secrets [here](https:\/\/github.com\/microsoft\/recommenders\/settings\/secrets\/actions). Create a new repository secret called `AZUREML_TEST_SUBID` and add the subscription ID as the value.\r\n4. Make sure you have installed [Azure CLI](https:\/\/learn.microsoft.com\/en-us\/cli\/azure\/install-azure-cli), and that you are logged in: `az login`.\r\n5. Select your subscription: `az account set -s $AZURE_SUBSCRIPTION_ID`.\r\n5. Create a Service Principal: `az ad sp create-for-rbac --name \"CICD\" --role contributor --scopes \/subscriptions\/$AZURE_SUBSCRIPTION_ID --sdk-auth`.\r\n6. Add the output from the Service Principal (should be a JSON blob) as an action secret `AZUREML_TEST_CREDENTIALS`.\r\n\r\n\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":1669646142000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1669630421000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a bug in AzureML where the test process does not fail even if there is an error in the tests, making it difficult to identify errors. The user wants to receive a signal in GitHub when the tests fail so that the badge turns red and they are notified.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1862",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":8.1,
        "Challenge_reading_time":25.09,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2749.0,
        "Challenge_repo_issue_count":1927.0,
        "Challenge_repo_star_count":15795.0,
        "Challenge_repo_watch_count":264.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":4.3669444444,
        "Challenge_title":"[BUG] Update test documentation to connect AzureML with GitHub actions",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":254,
        "Discussion_body":"Error:\r\n```\r\n$ az ad sp create-for-rbac --name \"CICD\" --role contributor --scopes \/subscriptions\/$AZURE_SUBSCRIPTION_ID. --sdk-auth\r\nThis command or command group has been migrated to Microsoft Graph API. Please carefully review all breaking changes introduced during this migration: https:\/\/docs.microsoft.com\/cli\/azure\/microsoft-graph-migration\r\nOption '--sdk-auth' has been deprecated and will be removed in a future release.\r\nAADSTS530003: Your device is required to be managed to access this resource.\r\nTrace ID: XXXXXXXXXXXXXXXXXXXXXXX\r\nCorrelation ID: XXXXXXXXXXXXXXXXXXXXXXX\r\nTimestamp: 2022-11-28 10:02:57Z\r\nTo re-authenticate, please run:\r\naz login --scope https:\/\/graph.microsoft.com\/\/.default\r\n```\r\n",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1221810788500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paderborn, North-Rhine-Westphalia, Germany",
        "Answerer_reputation_count":68522.0,
        "Answerer_view_count":7896.0,
        "Challenge_adjusted_solved_time":380.8509777778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>It is not clear to me if one could use mlflow to serve a model that is evolving continuously based on its previous predictions.<\/p>\n<p>I need to be able to query a model in order to make a prediction on a sample of data which is the basic use of mlflow serve. However I also want the model to be updated internaly now that it has seen new data.<\/p>\n<p>Is it possible or does it need a FR ?<\/p>",
        "Challenge_closed_time":1618124640630,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616753577110,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unsure if mlflow can be used to continuously update a model based on its previous predictions while also being able to query the model for predictions on new data. They are seeking clarification on whether this is possible or if it requires a feature request.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66814885",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":5.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":380.8509777778,
        "Challenge_title":"Serve online learning models with mlflow",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":360.0,
        "Challenge_word_count":84,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1515156959092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Fairbanks, AK, United States",
        "Poster_reputation_count":76.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>I think that you should be able to do that by implementing the custom python model or custom flavor, as it's described in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">documentation<\/a>.  In this case you need to create a class that is inherited from <code>mlflow.pyfunc.PythonModel<\/code>, and implement the <code>predict<\/code> method, and inside that method you're free to do anything.  Here is just simple example from documentation:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class AddN(mlflow.pyfunc.PythonModel):\n\n    def __init__(self, n):\n        self.n = n\n\n    def predict(self, context, model_input):\n        return model_input.apply(lambda column: column + self.n)\n<\/code><\/pre>\n<p>and this model is then could be saved &amp; loaded again just as normal models:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># Construct and save the model\nmodel_path = &quot;add_n_model&quot;\nadd5_model = AddN(n=5)\nmlflow.pyfunc.save_model(path=model_path, python_model=add5_model)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(model_path)\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.9,
        "Solution_reading_time":14.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":120.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":17.5084694445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to build and push a custom ML model with docker to Amazon SageMaker. I know things are supposed to follow the general structure of being in opt\/ml. But there's no such bucket in Amazon S3??? Am I supposed to create this directory within my container before I build and push the image to AWS? I just have no idea where to put my training data, etc.<\/p>",
        "Challenge_closed_time":1565167576867,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565104546377,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in storing their model's training data and artifacts while building and pushing a custom ML model with docker to Amazon SageMaker. They are unsure about the location of the opt\/ml bucket in Amazon S3 and are seeking guidance on where to store their training data.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57379173",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.9,
        "Challenge_reading_time":5.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":17.5084694445,
        "Challenge_title":"Where do I store my model's training data, artifacts, etc?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1022.0,
        "Challenge_word_count":77,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1399251405187,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":145.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>SageMaker is automating the deployment of the Docker image with your code using the convention of channel->local-folder. Everything that you define with a channel in your <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-inputdataconfig\" rel=\"nofollow noreferrer\">input data configuration<\/a>, will be copied to the local Docker file system under <em>\/opt\/ml\/<\/em> folder, using the name of the channel as the name of the sub-folder.<\/p>\n\n<pre><code>{\n\"train\" : {\"ContentType\":  \"trainingContentType\", \n           \"TrainingInputMode\": \"File\", \n           \"S3DistributionType\": \"FullyReplicated\", \n           \"RecordWrapperType\": \"None\"},\n\"evaluation\" : {\"ContentType\":  \"evalContentType\", \n                \"TrainingInputMode\": \"File\", \n                \"S3DistributionType\": \"FullyReplicated\", \n                \"RecordWrapperType\": \"None\"},\n\"validation\" : {\"TrainingInputMode\": \"File\", \n                \"S3DistributionType\": \"FullyReplicated\", \n                \"RecordWrapperType\": \"None\"}\n} \n<\/code><\/pre>\n\n<p>to:<\/p>\n\n<pre><code>\/opt\/ml\/input\/data\/training\n\/opt\/ml\/input\/data\/validation\n\/opt\/ml\/input\/data\/testing\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":26.0,
        "Solution_reading_time":15.0,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":18.9520191667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I wonder if it's possible to run training Amazon SageMaker object detection model on a local PC?<\/p>",
        "Challenge_closed_time":1654073043172,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654004815903,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of training an Amazon SageMaker object detection model on a local PC.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72448994",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":2.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":18.9520191667,
        "Challenge_title":"Train Amazon SageMaker object detection model on local PC",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":70.0,
        "Challenge_word_count":25,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442786553536,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kyiv",
        "Poster_reputation_count":71.0,
        "Poster_view_count":30.0,
        "Solution_body":"<p>You're probably referring to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object-detection.html\" rel=\"nofollow noreferrer\">this<\/a> object detection algorithm which is part of of <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">Amazon SageMaker built-in algorithms<\/a>. <strong>Built-in algorithms must be trained on the cloud<\/strong>.<br \/>\nIf you're bringing your own Tensorflow or PyTorch model, you could use SageMaker training jobs to train either on the cloud or locally as @kirit noted.<\/p>\n<p>I would also look at <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-jumpstart.html\" rel=\"nofollow noreferrer\">SageMaker JumpStart<\/a> for a wide variety of object detection algorithm which are TF\/PT based.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":16.0,
        "Solution_reading_time":10.43,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":79.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1450242937020,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Syria",
        "Answerer_reputation_count":235.0,
        "Answerer_view_count":36.0,
        "Challenge_adjusted_solved_time":0.2286694444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run a python program using Pycharm IDE but unable to do so without stumbling into \"Your system has run out of application memory\". After some research I came across a suggestion of using Microsoft Azure ML. Can anyone point me to some helpful links that can get me started or any other suggestions at all?<\/p>\n\n<p>Edit: I am working with a data that has 400,000 samples and ~5000 samples and I want to use chi2 feature selection but I am unable to run the program.<\/p>",
        "Challenge_closed_time":1454732632400,
        "Challenge_comment_count":1,
        "Challenge_created_time":1454731809190,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to run a large Python program on Pycharm IDE due to the error \"Your system has run out of application memory\". They are seeking suggestions for alternatives, and specifically mention working with a dataset of 400,000 samples and ~5000 features, and wanting to use chi2 feature selection.",
        "Challenge_last_edit_time":1483522935080,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35237226",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":6.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.2286694444,
        "Challenge_title":"Cannot run huge Python program",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":118.0,
        "Challenge_word_count":93,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1454200887396,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>You can use: PyPy to run your program with less memory usage and more speed. see this <a href=\"http:\/\/pypy.org\/\" rel=\"nofollow\">pypy site<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":4.8,
        "Solution_reading_time":1.88,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":21.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1528629350990,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pakistan",
        "Answerer_reputation_count":439.0,
        "Answerer_view_count":51.0,
        "Challenge_adjusted_solved_time":121.4413405556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a pre-trained <code>keras<\/code> model which I have hosted on <code>AWS<\/code> using <code>AWS SageMaker<\/code>. I've got an <code>endpoint<\/code> and I can make successful <code>predictions<\/code> using the <code>Amazon SageMaker Notebook instance<\/code>.<\/p>\n<p>What I do there is that I serve a <code>.PNG image<\/code> like the following and the model gives me correct prediction.<\/p>\n<pre><code>file= s3.Bucket(bucketname).download_file(filename_1, 'normal.png')\nfile_name_1='normal.png'\n\n\nimport sagemaker\nfrom sagemaker.tensorflow.model import TensorFlowModel\n\nendpoint = 'tensorflow-inference-0000-11-22-33-44-55-666' #endpoint\n\npredictor=sagemaker.tensorflow.model.TensorFlowPredictor(endpoint, sagemaker_session)\ndata = np.array([resize(imread(file_name), (137, 310, 3))])\npredictor.predict(data)\n<\/code><\/pre>\n<p>Now I wanted to make predictions using a <code>mobile application<\/code>. For that I have to wrote a <code>Lambda function<\/code> in python and attached an <code>API gateway<\/code> to it. My <code>Lambda function<\/code> is the following.<\/p>\n<pre><code>import os\nimport sys\n\nCWD = os.path.dirname(os.path.realpath(__file__))\nsys.path.insert(0, os.path.join(CWD, &quot;lib&quot;))\n\nimport json\nimport base64\nimport boto3\nimport numpy as np\nfrom scipy import signal\nfrom scipy.signal import butter, lfilter\nfrom scipy.io import wavfile\nimport scipy.signal as sps\nimport io\nfrom io import BytesIO\nimport matplotlib.pylab as plt\nfrom matplotlib import pyplot as plt\nimport matplotlib.image as mpimg\nfrom datetime import datetime\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom PIL import Image\n\nENDPOINT_NAME = 'tensorflow-inference-0000-11-22-33-44-55-666'\nruntime= boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    s3 = boto3.client(&quot;s3&quot;)\n    \n    # retrieving data from event.\n    get_file_content_from_postman = event[&quot;content&quot;]\n    \n    # decoding data.\n    decoded_file_name = base64.b64decode(get_file_content_from_postman)\n    \n    image = Image.open(io.BytesIO(decoded_file_name))\n\n    data = np.array([resize(imread(image), (137, 310, 3))])\n    \n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='text\/csv', Body=data)\n        \n    result = json.loads(response['Body'].read().decode())\n    \n    return result\n<\/code><\/pre>\n<p>The third last line is giving me error <code>'PngImageFile' object has no attribute 'read'<\/code>.\nAny idea what I am missing here?<\/p>",
        "Challenge_closed_time":1618052704636,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617615515810,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has a pre-trained Keras model hosted on AWS SageMaker and can make successful predictions using the Amazon SageMaker Notebook instance. However, when trying to make predictions using a mobile application, the user wrote a Lambda function in Python and attached an API gateway to it. The Lambda function is giving an error \"PngImageFile' object has no attribute 'read'\". The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":1618052912968,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66950948",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.3,
        "Challenge_reading_time":32.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":121.4413405556,
        "Challenge_title":"How to make inference to a keras model hosted on AWS SageMaker via AWS Lambda function?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":114.0,
        "Challenge_word_count":251,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1528629350990,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pakistan",
        "Poster_reputation_count":439.0,
        "Poster_view_count":51.0,
        "Solution_body":"<p>I was missing one thing which was causing this error. After receiving the image data I used python list and then <code>json.dump<\/code> that list (of lists). Below is the code for reference.<\/p>\n<pre><code>import os\nimport sys\n\nCWD = os.path.dirname(os.path.realpath(__file__))\nsys.path.insert(0, os.path.join(CWD, &quot;lib&quot;))\n\nimport json\nimport base64\nimport boto3\nimport numpy as np\nimport io\nfrom io import BytesIO\nfrom skimage.io import imread\nfrom skimage.transform import resize\n\n# grab environment variable of Lambda Function\nENDPOINT_NAME = os.environ['ENDPOINT_NAME']\nruntime= boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    s3 = boto3.client(&quot;s3&quot;)\n    \n    # retrieving data from event.\n    get_file_content_from_postman = event[&quot;content&quot;]\n    \n    # decoding data.\n    decoded_file_name = base64.b64decode(get_file_content_from_postman)\n    \n    data = np.array([resize(imread(io.BytesIO(decoded_file_name)), (137, 310, 3))])\n    \n    payload = json.dumps(data.tolist())\n    \n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\n        \n    result = json.loads(response['Body'].read().decode())\n    \n    return result\n        \n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.0,
        "Solution_reading_time":15.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":106.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":19.2317730556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Sagemaker is a great tool to train your models, and we save some money by using AWS spot instances. However, training jobs sometimes get stopped in the middle. We are using some mechanisms to continue from the latest checkpoint after a restart. See also the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-checkpoints.html\" rel=\"nofollow noreferrer\">docs<\/a>.<\/p>\n<p>Still, how do you efficiently test such a mechanism? Can you trigger it yourself? Otherwise you have to wait until the spot instance actually \u00eds restarted.<\/p>\n<p>Also, are you expected to use the linked <code>checkpoint_s3_uri<\/code> argument or the <code>model_dir<\/code> for this? E.g. the <code>TensorFlow<\/code> estimator <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/sagemaker.tensorflow.html#tensorflow-estimator\" rel=\"nofollow noreferrer\">docs<\/a> seem to suggest something <code>model_dir<\/code>for checkpoints.<\/p>",
        "Challenge_closed_time":1616662041463,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616592352363,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while using AWS Sagemaker for training models with spot instances. The training jobs sometimes get stopped in the middle, and the user is using mechanisms to continue from the latest checkpoint after a restart. However, the user is unsure how to efficiently test this mechanism and which argument to use for checkpoints - the `checkpoint_s3_uri` or the `model_dir`.",
        "Challenge_last_edit_time":1616592807080,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66782040",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":12.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":19.3580833334,
        "Challenge_title":"Reloading from checkpoing during AWS Sagemaker Training",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":286.0,
        "Challenge_word_count":110,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1484838464572,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Amsterdam, Nederland",
        "Poster_reputation_count":3937.0,
        "Poster_view_count":387.0,
        "Solution_body":"<p>Since you can't manually terminate a sagemaker instance, run an Amazon SageMaker Managed Spot training for a small number of epochs, Amazon SageMaker would have backed up your checkpoint files to S3. Check that checkpoints are there. Now run a second training run, but this time provide the first jobs\u2019 checkpoint location to <code>checkpoint_s3_uri<\/code>. Reference is <a href=\"https:\/\/towardsdatascience.com\/a-quick-guide-to-using-spot-instances-with-amazon-sagemaker-b9cfb3a44a68\" rel=\"nofollow noreferrer\">here<\/a>, this also answer your second question.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.8,
        "Solution_reading_time":7.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1544390307847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Palo Alto, CA, USA",
        "Answerer_reputation_count":151.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":270.3311944445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've seen examples of labeling data using SageMaker Ground Truth and then using that data to train off-the-shelf SageMaker models. However, am I able to use this same annotation format with TensorFlow Script Mode? <\/p>\n\n<p>More specifically, I have a tensorflow.keras model I'm training using TF Script Mode, and I'd like to take data labeled with Ground Truth and convert my script from File mode to Pipe mode.<\/p>",
        "Challenge_closed_time":1549915976767,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548942784467,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to determine if they can use the annotation format from SageMaker Ground Truth with TensorFlow Script Mode to train a tensorflow.keras model and convert their script from File mode to Pipe mode.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54462105",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":5.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":270.3311944445,
        "Challenge_title":"SageMaker Ground Truth with TensorFlow",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":454.0,
        "Challenge_word_count":72,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>I am from Amazon SageMaker Ground Truth team and happy to assist you in your experiment. Just to be clear our understanding, are you running TF model in SageMaker using TF estimator in your own container (<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst<\/a>)? <\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":19.3,
        "Solution_reading_time":5.85,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":41.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4538275,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>What are the learning paths in MS Learn for Data Science, Machine Learning, and Deep Learning with Python as the base programming language? How to get the Microsoft Certification.<\/p>",
        "Challenge_closed_time":1684304707872,
        "Challenge_comment_count":0,
        "Challenge_created_time":1684303074093,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information on the learning paths available in MS Learn for Data Science, Machine Learning, and Deep Learning with Python as the base programming language. They also want to know how to obtain Microsoft Certification.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1286478\/what-is-the-learning-path-to-became-a-data-scienti",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":2.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.4538275,
        "Challenge_title":"What is the Learning path to Became a data scientist?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":38,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The certification path for Data Scientist includes 6 exams. <a href=\"https:\/\/learn.microsoft.com\/en-us\/certifications\/browse\/?roles=data-scientist\">https:\/\/learn.microsoft.com\/en-us\/certifications\/browse\/?roles=data-scientist<\/a><\/p>\n<p>Of these 6, the core exam is DP-100, passing it will earn you <strong>Microsoft Certified: Azure Data Scientist Associate<\/strong>.<\/p>\n<p>The DP-100 exam page features the Learning path collection with all of the modules to prepare for the exam; <a href=\"https:\/\/learn.microsoft.com\/en-us\/certifications\/azure-data-scientist\/\">https:\/\/learn.microsoft.com\/en-us\/certifications\/azure-data-scientist\/<\/a><\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.7,
        "Solution_reading_time":8.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":51.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":47.2837916667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a model using sagemaker (on aws ml notebook). \nI then exported that model to s3 and a <code>.tar.gz<\/code> file was created there.<\/p>\n\n<p>Im trying to find a way to load the model object to memory in my code (without using AWS docker images and deployment) and run a prediction on it.<\/p>\n\n<p>I looked for functions to do that in the model section of the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html#\" rel=\"nofollow noreferrer\">sagemaker docs<\/a>, but everything there is tightly coupled to the AWS docker images.<\/p>\n\n<p>I then tried opening the file with <code>tarfile<\/code> and <code>shutil<\/code> packages but that was useless.<\/p>\n\n<p>Any ideas?<\/p>",
        "Challenge_closed_time":1562768843267,
        "Challenge_comment_count":1,
        "Challenge_created_time":1562675762410,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a model using Sagemaker on AWS ML Notebook and exported it to S3 as a .tar.gz file. They are trying to load the model object to memory in their code without using AWS docker images and deployment, but the functions in the Sagemaker docs are tightly coupled to the images. The user has tried opening the file with tarfile and shutil packages but has been unsuccessful. They are seeking ideas for a solution.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56952741",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.1,
        "Challenge_reading_time":9.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":25.8557936111,
        "Challenge_title":"Sagemaker export and load model to memory",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1838.0,
        "Challenge_word_count":106,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1480290282627,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2906.0,
        "Poster_view_count":266.0,
        "Solution_body":"<p>With the exception of XGBoost, built-in algorithms are implemented with Apache MXNet, so simply extract the model from the .tar.gz file and load it with MXNet: load_checkpoint() is the API to use.<\/p>\n\n<p>XGBoost models are just pickled objects. Unpickle and load in sklearn:<\/p>\n\n<pre><code>$ python3\n&gt;&gt;&gt; import sklearn, pickle\n&gt;&gt;&gt; model = pickle.load(open(\"xgboost-model\", \"rb\"))\n&gt;&gt;&gt; type(model)\n&lt;class 'xgboost.core.Booster'&gt;\n<\/code><\/pre>\n\n<p>Models trained with built-in library (Tensorflow, MXNet, Pytorch, etc.) are vanilla models that can be loaded as-is with the correct library.<\/p>\n\n<p>Hope this helps.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1562845984060,
        "Solution_link_count":0.0,
        "Solution_readability":7.3,
        "Solution_reading_time":8.33,
        "Solution_score_count":3.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":82.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416648155470,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":14749.0,
        "Answerer_view_count":968.0,
        "Challenge_adjusted_solved_time":7321.6952480556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The documentations of how to use SageMaker estimators are scattered around, sometimes obsolete, incorrect. Is there a one stop location which gives the comprehensive views of how to use SageMaker SDK Estimator to train and save models?<\/p>",
        "Challenge_closed_time":1630555307168,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630555307170,
        "Challenge_favorite_count":14.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in finding comprehensive and accurate documentation on how to use SageMaker SDK Estimator for model training and saving.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69024005",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":3.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":27.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"How to use SageMaker Estimator for model training and saving",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":6655.0,
        "Challenge_word_count":46,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416648155470,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":14749.0,
        "Poster_view_count":968.0,
        "Solution_body":"<h1>Answer<\/h1>\n<p>There is no one such resource from AWS that provides the comprehensive view of how to use SageMaker SDK Estimator to train and save models.<\/p>\n<h2>Alternative Overview Diagram<\/h2>\n<p>I put a diagram and brief explanation to get the overview on how SageMaker Estimator runs a training.<\/p>\n<ol>\n<li><p>SageMaker sets up a docker container for a training job where:<\/p>\n<ul>\n<li>Environment variables are set as in <a href=\"https:\/\/github.com\/aws\/sagemaker-containers#important-environment-variables\" rel=\"noreferrer\">SageMaker Docker Container. Environment Variables<\/a>.<\/li>\n<li>Training data is setup under <code>\/opt\/ml\/input\/data<\/code>.<\/li>\n<li>Training script codes are setup under <code>\/opt\/ml\/code<\/code>.<\/li>\n<li><code>\/opt\/ml\/model<\/code> and <code>\/opt\/ml\/output<\/code> directories are setup to store training outputs.<\/li>\n<\/ul>\n<\/li>\n<\/ol>\n<pre><code>\/opt\/ml\n\u251c\u2500\u2500 input\n\u2502   \u251c\u2500\u2500 config\n\u2502   \u2502   \u251c\u2500\u2500 hyperparameters.json  &lt;--- From Estimator hyperparameter arg\n\u2502   \u2502   \u2514\u2500\u2500 resourceConfig.json\n\u2502   \u2514\u2500\u2500 data\n\u2502       \u2514\u2500\u2500 &lt;channel_name&gt;        &lt;--- From Estimator fit method inputs arg\n\u2502           \u2514\u2500\u2500 &lt;input data&gt;\n\u251c\u2500\u2500 code\n\u2502   \u2514\u2500\u2500 &lt;code files&gt;              &lt;--- From Estimator src_dir arg\n\u251c\u2500\u2500 model\n\u2502   \u2514\u2500\u2500 &lt;model files&gt;             &lt;--- Location to save the trained model artifacts\n\u2514\u2500\u2500 output\n    \u2514\u2500\u2500 failure                   &lt;--- Training job failure logs\n<\/code><\/pre>\n<ol start=\"2\">\n<li><p>SageMaker Estimator <code>fit(inputs)<\/code> method executes the training script. Estimator <code>hyperparameters<\/code> and <code>fit<\/code> method <code>inputs<\/code> are provided as its command line arguments.<\/p>\n<\/li>\n<li><p>The training script saves the model artifacts in the <code>\/opt\/ml\/model<\/code> once the training is completed.<\/p>\n<\/li>\n<li><p>SageMaker archives the artifacts under <code>\/opt\/ml\/model<\/code> into <code>model.tar.gz<\/code> and save it to the S3 location specified to <code>output_path<\/code> Estimator parameter.<\/p>\n<\/li>\n<li><p>You can set Estimator <code>metric_definitions<\/code> parameter to extract model metrics from the training logs. Then you can monitor the training progress in the SageMaker console metrics.<\/p>\n<\/li>\n<\/ol>\n<p><a href=\"https:\/\/i.stack.imgur.com\/gi8bU.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gi8bU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I believe AWS needs to stop mass-producing verbose, redundant, wordy, scattered, and obsolete documents. AWS needs to understand <strong>A picture is worth thousand words<\/strong>.<\/p>\n<p>Have diagrams and piece document parts together in a <strong>context<\/strong> with a clear objective to achieve.<\/p>\n<hr \/>\n<h1>Problem<\/h1>\n<p>AWS documentations need serious re-design and re-structuring. Just to understand <strong>how to train and save a model<\/strong> forces us going through dozens of scattered,  fragmented, verbose, redundant documentations, which are often obsolete, incomplete, and sometime incorrect.<\/p>\n<p>It is well-summarized in <a href=\"https:\/\/nandovillalba.medium.com\/why-i-think-gcp-is-better-than-aws-ea78f9975bda\" rel=\"noreferrer\">Why I think GCP is better than AWS<\/a>:<\/p>\n<blockquote>\n<p>It\u2019s not that AWS is harder to use than GCP, it\u2019s that <strong>it is needlessly hard<\/strong>; a disjointed, sprawl of infrastructure primitives with poor cohesion between them.  <br><br>\nA challenge is nice, a confusing mess is not, and <strong>the problem with AWS is that a large part of your working hours will be spent untangling their documentation and weeding through features and products to find what you want<\/strong>, rather than focusing on cool interesting challenges.<\/p>\n<\/blockquote>\n<p>Especially the SageMaker team keeps changing implementations without updating documents. Its roll-out was also inconsistent, e.g. SDK version 2 was rolled out in the SageMaker Studio making the AWS examples in Github incompatible without announcing it. Whereas SageMaker instance still had SDK 1, hence code worked in Instance but not in Studio.<\/p>\n<p>It is mind-boggling that we have to go through these many documents below to understand how to use the SageMaker SDK Estimator for training.<\/p>\n<h2>Documents for Model Training<\/h2>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\" rel=\"noreferrer\">Train a Model with Amazon SageMaker<\/a><\/li>\n<\/ul>\n<p>This document gives 20,000 feet overview of how SageMaker training but does not give any clue what to do.<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker-workshop.com\/custom\/containers.html\" rel=\"noreferrer\">Running a container for Amazon SageMaker training<\/a><\/li>\n<\/ul>\n<p>This document gives an overview of how SageMaker training looks like. However, this is not up-to-date as it is based on <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"noreferrer\">SageMaker Containers<\/a> which is obsolete.<\/p>\n<blockquote>\n<p>WARNING: This package has been deprecated. Please use the SageMaker Training Toolkit for model training and the SageMaker Inference Toolkit for model serving.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model.html\" rel=\"noreferrer\">Step 4: Train a Model<\/a><\/li>\n<\/ul>\n<p>This document layouts the steps for training.<\/p>\n<blockquote>\n<p>The Amazon SageMaker Python SDK provides framework estimators and generic estimators to train your model while orchestrating the machine learning (ML) lifecycle accessing the SageMaker features for training and the AWS infrastructures<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#train-a-model-with-the-sagemaker-python-sdk\" rel=\"noreferrer\">Train a Model with the SageMaker Python SDK<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>To train a model by using the SageMaker Python SDK, you:<\/p>\n<ul>\n<li>Prepare a training script<\/li>\n<li>Create an estimator<\/li>\n<li>Call the fit method of the estimator<\/li>\n<\/ul>\n<\/blockquote>\n<p>Finally this document gives concrete steps and ideas. However still missing comprehensiv details about Environment Variables, Directory structure in the SageMaker docker container**, S3 for uploading code, placing data, S3 where the trained model is saved, etc.<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html\" rel=\"noreferrer\">Use TensorFlow with the SageMaker Python SDK<\/a><\/li>\n<\/ul>\n<p>This documents is focused on TensorFlow Estimator implementation steps. Use <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/frameworks\/tensorflow\/get_started_mnist_train.ipynb\" rel=\"noreferrer\">Training a Tensorflow Model on MNIST<\/a> Github example to accompany with to follow the actual implementation.<\/p>\n<h2>Documents for passing parameters and data locations<\/h2>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-inputdataconfig\" rel=\"noreferrer\">How Amazon SageMaker Provides Training Information<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>This section explains how SageMaker makes training information, such as training data, hyperparameters, and other configuration information, available to your Docker container.<\/p>\n<\/blockquote>\n<p>This document finally gives the idea of how parameters and data are passed around but again, not comprehensive.<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-containers#important-environment-variables\" rel=\"noreferrer\">SageMaker Docker Container Environment Variables<\/a><\/li>\n<\/ul>\n<p>This documentation is marked as <strong>deprecated<\/strong> but the only document which explains the SageMaker Environment Variables.<\/p>\n<blockquote>\n<h3>IMPORTANT ENVIRONMENT VARIABLES<\/h3>\n<ul>\n<li>SM_MODEL_DIR<\/li>\n<li>SM_CHANNELS<\/li>\n<li>SM_CHANNEL_{channel_name}<\/li>\n<li>SM_HPS<\/li>\n<li>SM_HP_{hyperparameter_name}<\/li>\n<li>SM_CURRENT_HOST<\/li>\n<li>SM_HOSTS<\/li>\n<li>SM_NUM_GPUS<\/li>\n<\/ul>\n<h3>List of provided environment variables by SageMaker Containers<\/h3>\n<ul>\n<li>SM_NUM_CPUS<\/li>\n<li>SM_LOG_LEVEL<\/li>\n<li>SM_NETWORK_INTERFACE_NAME<\/li>\n<li>SM_USER_ARGS<\/li>\n<li>SM_INPUT_DIR<\/li>\n<li>SM_INPUT_CONFIG_DIR<\/li>\n<li>SM_OUTPUT_DATA_DIR<\/li>\n<li>SM_RESOURCE_CONFIG<\/li>\n<li>SM_INPUT_DATA_CONFIG<\/li>\n<li>SM_TRAINING_ENV<\/li>\n<\/ul>\n<\/blockquote>\n<h2>Documents for SageMaker Docker Container Directory Structure<\/h2>\n<ul>\n<li><a href=\"https:\/\/sagemaker-workshop.com\/custom\/containers.html\" rel=\"noreferrer\">Running a container for Amazon SageMaker training<\/a><\/li>\n<\/ul>\n<pre><code>\/opt\/ml\n\u251c\u2500\u2500 input\n\u2502   \u251c\u2500\u2500 config\n\u2502   \u2502   \u251c\u2500\u2500 hyperparameters.json\n\u2502   \u2502   \u2514\u2500\u2500 resourceConfig.json\n\u2502   \u2514\u2500\u2500 data\n\u2502       \u2514\u2500\u2500 &lt;channel_name&gt;\n\u2502           \u2514\u2500\u2500 &lt;input data&gt;\n\u251c\u2500\u2500 model\n\u2502   \u2514\u2500\u2500 &lt;model files&gt;\n\u2514\u2500\u2500 output\n    \u2514\u2500\u2500 failure\n<\/code><\/pre>\n<p>This document explains the directory structure and purpose of each directory.<\/p>\n<blockquote>\n<h3>The input<\/h3>\n<ul>\n<li>\/opt\/ml\/input\/config contains information to control how your program runs. hyperparameters.json is a JSON-formatted dictionary of hyperparameter names to values. These values will always be strings, so you may need to convert them. resourceConfig.json is a JSON-formatted file that describes the network layout used for distributed training. Since scikit-learn doesn\u2019t support distributed training, we\u2019ll ignore it here.<\/li>\n<li>\/opt\/ml\/input\/data\/&lt;channel_name&gt;\/ (for File mode) contains the input data for that channel. The channels are created based on the call to CreateTrainingJob but it\u2019s generally important that channels match what the algorithm expects. The files for each channel will be copied from S3 to this directory, preserving the tree structure indicated by the S3 key structure.<\/li>\n<li>\/opt\/ml\/input\/data\/&lt;channel_name&gt;_&lt;epoch_number&gt; (for Pipe mode) is the pipe for a given epoch. Epochs start at zero and go up by one each time you read them. There is no limit to the number of epochs that you can run, but you must close each pipe before reading the next epoch.<\/li>\n<\/ul>\n<h3>The output<\/h3>\n<ul>\n<li>\/opt\/ml\/model\/ is the directory where you write the model that your algorithm generates. Your model can be in any format that you want. It can be a single file or a whole directory tree. SageMaker will package any files in this directory into a compressed tar archive file. This file will be available at the S3 location returned in the DescribeTrainingJob result.<\/li>\n<li>\/opt\/ml\/output is a directory where the algorithm can write a file failure that describes why the job failed. The contents of this file will be returned in the FailureReason field of the DescribeTrainingJob result. For jobs that succeed, there is no reason to write this file as it will be ignored.<\/li>\n<\/ul>\n<\/blockquote>\n<p>However, this is not up-to-date as it is based on <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"noreferrer\">SageMaker Containers<\/a> which is obsolete.<\/p>\n<h2>Documents for Model Saving<\/h2>\n<p>The information on where the trained model is saved and in what format are fundamentally missing. The training script needs to save the model under <code>\/opt\/ml\/model<\/code> and the format and sub-directory structure depend on the frameworks e,g TensorFlow, Pytorch. This is because SageMaker deployment uses the Framework dependent model-serving, e,g. TensorFlow Serving for TensorFlow framework.<\/p>\n<p>This is not clearly documented and causing confusions. The developer needs to specify which format to use and under which sub-directory to save.<\/p>\n<p>To use TensorFlow Estimator training and deployment:<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/aws_sagemaker_studio\/frameworks\/keras_pipe_mode_horovod\/keras_pipe_mode_horovod_cifar10.html#Deploy-the-trained-model\" rel=\"noreferrer\">Deploy the trained model<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>Because <strong>we\u2019re using TensorFlow Serving for deployment<\/strong>, our training script <strong>saves the model in TensorFlow\u2019s SavedModel format<\/strong>.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/frameworks\/tensorflow\/code\/train.py#L159-L166\" rel=\"noreferrer\">amazon-sagemaker-examples\/frameworks\/tensorflow\/code\/train.py <\/a><\/li>\n<\/ul>\n<pre><code>    # Save the model\n    # A version number is needed for the serving container\n    # to load the model\n    version = &quot;00000000&quot;\n    ckpt_dir = os.path.join(args.model_dir, version)\n    if not os.path.exists(ckpt_dir):\n        os.makedirs(ckpt_dir)\n    model.save(ckpt_dir)\n<\/code><\/pre>\n<p>The code is saving the model in <code>\/opt\/ml\/model\/00000000<\/code> because this is for TensorFlow serving.<\/p>\n<ul>\n<li><a href=\"https:\/\/www.tensorflow.org\/guide\/saved_model\" rel=\"noreferrer\">Using the SavedModel format<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>The save-path follows a convention used by TensorFlow Serving where the last path component (1\/ here) is a version number for your model - it allows tools like Tensorflow Serving to reason about the relative freshness.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/www.tensorflow.org\/tfx\/tutorials\/serving\/rest_simple#save_your_model\" rel=\"noreferrer\">Train and serve a TensorFlow model with TensorFlow Serving<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>To load our trained model into TensorFlow Serving we first need to save it in SavedModel format. This will create a protobuf file in a well-defined directory hierarchy, and will include a version number. TensorFlow Serving allows us to select which version of a model, or &quot;servable&quot; we want to use when we make inference requests. Each version will be exported to a different sub-directory under the given path.<\/p>\n<\/blockquote>\n<h2>Documents for API<\/h2>\n<p>Basically the SageMaker SDK Estimator implements the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html\" rel=\"noreferrer\">CreateTrainingJob<\/a> API for training part. Hence, better to understand how it is designed and what parameters need to be defined. Otherwise working on Estimators are like walking in the dark.<\/p>\n<hr \/>\n<h1>Example<\/h1>\n<h2>Jupyter Notebook<\/h2>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\nbucket = sagemaker_session.default_bucket()\n\nmetric_definitions = [\n    {&quot;Name&quot;: &quot;train:loss&quot;, &quot;Regex&quot;: &quot;.*loss: ([0-9\\\\.]+) - accuracy: [0-9\\\\.]+.*&quot;},\n    {&quot;Name&quot;: &quot;train:accuracy&quot;, &quot;Regex&quot;: &quot;.*loss: [0-9\\\\.]+ - accuracy: ([0-9\\\\.]+).*&quot;},\n    {\n        &quot;Name&quot;: &quot;validation:accuracy&quot;,\n        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: ([0-9\\\\.]+).*&quot;,\n    },\n    {\n        &quot;Name&quot;: &quot;validation:loss&quot;,\n        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: ([0-9\\\\.]+) - val_accuracy: [0-9\\\\.]+.*&quot;,\n    },\n    {\n        &quot;Name&quot;: &quot;sec\/sample&quot;,\n        &quot;Regex&quot;: &quot;.* - \\d+s (\\d+)[mu]s\/sample - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: [0-9\\\\.]+&quot;,\n    },\n]\n\nimport uuid\n\ncheckpoint_s3_prefix = &quot;checkpoints\/{}&quot;.format(str(uuid.uuid4()))\ncheckpoint_s3_uri = &quot;s3:\/\/{}\/{}\/&quot;.format(bucket, checkpoint_s3_prefix)\n\nfrom sagemaker.tensorflow import TensorFlow\n\n# --------------------------------------------------------------------------------\n# 'trainingJobName' msut satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}\n# --------------------------------------------------------------------------------\nbase_job_name = &quot;fashion-mnist&quot;\nhyperparameters = {\n    &quot;epochs&quot;: 2, \n    &quot;batch-size&quot;: 64\n}\nestimator = TensorFlow(\n    entry_point=&quot;fashion_mnist.py&quot;,\n    source_dir=&quot;src&quot;,\n    metric_definitions=metric_definitions,\n    hyperparameters=hyperparameters,\n    role=role,\n    input_mode='File',\n    framework_version=&quot;2.3.1&quot;,\n    py_version=&quot;py37&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    base_job_name=base_job_name,\n    checkpoint_s3_uri=checkpoint_s3_uri,\n    model_dir=False\n)\nestimator.fit()\n<\/code><\/pre>\n<h2>fashion_mnist.py<\/h2>\n<pre><code>import os\nimport argparse\nimport json\nimport multiprocessing\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nfrom tensorflow.keras import backend as K\n\nprint(&quot;TensorFlow version: {}&quot;.format(tf.__version__))\nprint(&quot;Eager execution is: {}&quot;.format(tf.executing_eagerly()))\nprint(&quot;Keras version: {}&quot;.format(tf.keras.__version__))\n\n\nimage_width = 28\nimage_height = 28\n\n\ndef load_data():\n    fashion_mnist = tf.keras.datasets.fashion_mnist\n    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n\n    number_of_classes = len(set(y_train))\n    print(&quot;number_of_classes&quot;, number_of_classes)\n\n    x_train = x_train \/ 255.0\n    x_test = x_test \/ 255.0\n    x_full = np.concatenate((x_train, x_test), axis=0)\n    print(x_full.shape)\n\n    print(type(x_train))\n    print(x_train.shape)\n    print(x_train.dtype)\n    print(y_train.shape)\n    print(y_train.dtype)\n\n    # ## Train\n    # * C: Convolution layer\n    # * P: Pooling layer\n    # * B: Batch normalization layer\n    # * F: Fully connected layer\n    # * O: Output fully connected softmax layer\n\n    # Reshape data based on channels first \/ channels last strategy.\n    # This is dependent on whether you use TF, Theano or CNTK as backend.\n    # Source: https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/mnist_cnn.py\n    if K.image_data_format() == 'channels_first':\n        x = x_train.reshape(x_train.shape[0], 1, image_width, image_height)\n        x_test = x_test.reshape(x_test.shape[0], 1, image_width, image_height)\n        input_shape = (1, image_width, image_height)\n    else:\n        x_train = x_train.reshape(x_train.shape[0], image_width, image_height, 1)\n        x_test = x_test.reshape(x_test.shape[0], image_width, image_height, 1)\n        input_shape = (image_width, image_height, 1)\n\n    return x_train, y_train, x_test, y_test, input_shape, number_of_classes\n\n# tensorboard --logdir=\/full_path_to_your_logs\n\nvalidation_split = 0.2\nverbosity = 1\nuse_multiprocessing = True\nworkers = multiprocessing.cpu_count()\n\n\ndef train(model, x, y, args):\n    # SavedModel Output\n    tensorflow_saved_model_path = os.path.join(args.model_dir, &quot;tensorflow\/saved_model\/0&quot;)\n    os.makedirs(tensorflow_saved_model_path, exist_ok=True)\n\n    # Tensorboard Logs\n    tensorboard_logs_path = os.path.join(args.model_dir, &quot;tensorboard\/&quot;)\n    os.makedirs(tensorboard_logs_path, exist_ok=True)\n\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n        log_dir=tensorboard_logs_path,\n        write_graph=True,\n        write_images=True,\n        histogram_freq=1,  # How often to log histogram visualizations\n        embeddings_freq=1,  # How often to log embedding visualizations\n        update_freq=&quot;epoch&quot;,\n    )  # How often to write logs (default: once per epoch)\n\n    model.compile(\n        optimizer='adam',\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\n        metrics=['accuracy']\n    )\n    history = model.fit(\n        x,\n        y,\n        shuffle=True,\n        batch_size=args.batch_size,\n        epochs=args.epochs,\n        validation_split=validation_split,\n        use_multiprocessing=use_multiprocessing,\n        workers=workers,\n        verbose=verbosity,\n        callbacks=[\n            tensorboard_callback\n        ]\n    )\n    return history\n\n\ndef create_model(input_shape, number_of_classes):\n    model = Sequential([\n        Conv2D(\n            name=&quot;conv01&quot;,\n            filters=32,\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=&quot;same&quot;,\n            activation='relu',\n            input_shape=input_shape\n        ),\n        MaxPooling2D(\n            name=&quot;pool01&quot;,\n            pool_size=(2, 2)\n        ),\n        Flatten(),  # 3D shape to 1D.\n        BatchNormalization(\n            name=&quot;batch_before_full01&quot;\n        ),\n        Dense(\n            name=&quot;full01&quot;,\n            units=300,\n            activation=&quot;relu&quot;\n        ),  # Fully connected layer\n        Dense(\n            name=&quot;output_softmax&quot;,\n            units=number_of_classes,\n            activation=&quot;softmax&quot;\n        )\n    ])\n    return model\n\n\ndef save_model(model, args):\n    # Save the model\n    # A version number is needed for the serving container\n    # to load the model\n    version = &quot;00000000&quot;\n    model_save_dir = os.path.join(args.model_dir, version)\n    if not os.path.exists(model_save_dir):\n        os.makedirs(model_save_dir)\n    print(f&quot;saving model at {model_save_dir}&quot;)\n    model.save(model_save_dir)\n\n\ndef parse_args():\n    # --------------------------------------------------------------------------------\n    # https:\/\/docs.python.org\/dev\/library\/argparse.html#dest\n    # --------------------------------------------------------------------------------\n    parser = argparse.ArgumentParser()\n\n    # --------------------------------------------------------------------------------\n    # hyperparameters Estimator argument are passed as command-line arguments to the script.\n    # --------------------------------------------------------------------------------\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch-size', type=int, default=64)\n\n    # \/opt\/ml\/model\n    # sagemaker.tensorflow.estimator.TensorFlow override 'model_dir'.\n    # See https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/\\\n    # sagemaker.tensorflow.html#sagemaker.tensorflow.estimator.TensorFlow\n    parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])\n\n    # \/opt\/ml\/output\n    parser.add_argument(&quot;--output_dir&quot;, type=str, default=os.environ[&quot;SM_OUTPUT_DIR&quot;])\n\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == &quot;__main__&quot;:\n    args = parse_args()\n    print(&quot;---------- key\/value args&quot;)\n    for key, value in vars(args).items():\n        print(f&quot;{key}:{value}&quot;)\n\n    x_train, y_train, x_test, y_test, input_shape, number_of_classes = load_data()\n    model = create_model(input_shape, number_of_classes)\n\n    history = train(model=model, x=x_train, y=y_train, args=args)\n    print(history)\n    \n    save_model(model, args)\n    results = model.evaluate(x_test, y_test, batch_size=100)\n    print(&quot;test loss, test accuracy:&quot;, results)\n<\/code><\/pre>\n<h2>SageMaker Console<\/h2>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ctcLy.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ctcLy.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h2>Notebook output<\/h2>\n<pre><code>2021-09-03 03:02:04 Starting - Starting the training job...\n2021-09-03 03:02:16 Starting - Launching requested ML instancesProfilerReport-1630638122: InProgress\n......\n2021-09-03 03:03:17 Starting - Preparing the instances for training.........\n2021-09-03 03:04:59 Downloading - Downloading input data\n2021-09-03 03:04:59 Training - Downloading the training image...\n2021-09-03 03:05:23 Training - Training image download completed. Training in progress.2021-09-03 03:05:23.966037: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n2021-09-03 03:05:23.969704: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n2021-09-03 03:05:24.118054: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n2021-09-03 03:05:26,842 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\n2021-09-03 03:05:26,852 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:27,734 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n\/usr\/local\/bin\/python3.7 -m pip install -r requirements.txt\nWARNING: You are using pip version 21.0.1; however, version 21.2.4 is available.\nYou should consider upgrading via the '\/usr\/local\/bin\/python3.7 -m pip install --upgrade pip' command.\n\n2021-09-03 03:05:29,028 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,045 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,062 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,072 sagemaker-training-toolkit INFO     Invoking user script\n\nTraining Env:\n\n{\n    &quot;additional_framework_parameters&quot;: {},\n    &quot;channel_input_dirs&quot;: {},\n    &quot;current_host&quot;: &quot;algo-1&quot;,\n    &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,\n    &quot;hosts&quot;: [\n        &quot;algo-1&quot;\n    ],\n    &quot;hyperparameters&quot;: {\n        &quot;batch-size&quot;: 64,\n        &quot;epochs&quot;: 2\n    },\n    &quot;input_config_dir&quot;: &quot;\/opt\/ml\/input\/config&quot;,\n    &quot;input_data_config&quot;: {},\n    &quot;input_dir&quot;: &quot;\/opt\/ml\/input&quot;,\n    &quot;is_master&quot;: true,\n    &quot;job_name&quot;: &quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,\n    &quot;log_level&quot;: 20,\n    &quot;master_hostname&quot;: &quot;algo-1&quot;,\n    &quot;model_dir&quot;: &quot;\/opt\/ml\/model&quot;,\n    &quot;module_dir&quot;: &quot;s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz&quot;,\n    &quot;module_name&quot;: &quot;fashion_mnist&quot;,\n    &quot;network_interface_name&quot;: &quot;eth0&quot;,\n    &quot;num_cpus&quot;: 4,\n    &quot;num_gpus&quot;: 0,\n    &quot;output_data_dir&quot;: &quot;\/opt\/ml\/output\/data&quot;,\n    &quot;output_dir&quot;: &quot;\/opt\/ml\/output&quot;,\n    &quot;output_intermediate_dir&quot;: &quot;\/opt\/ml\/output\/intermediate&quot;,\n    &quot;resource_config&quot;: {\n        &quot;current_host&quot;: &quot;algo-1&quot;,\n        &quot;hosts&quot;: [\n            &quot;algo-1&quot;\n        ],\n        &quot;network_interface_name&quot;: &quot;eth0&quot;\n    },\n    &quot;user_entry_point&quot;: &quot;fashion_mnist.py&quot;\n}\n\nEnvironment variables:\n\nSM_HOSTS=[&quot;algo-1&quot;]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={&quot;batch-size&quot;:64,&quot;epochs&quot;:2}\nSM_USER_ENTRY_POINT=fashion_mnist.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}\nSM_INPUT_DATA_CONFIG={}\nSM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\nSM_CHANNELS=[]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=fashion_mnist\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=\/opt\/ml\/input\nSM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\nSM_OUTPUT_DIR=\/opt\/ml\/output\nSM_NUM_CPUS=4\nSM_NUM_GPUS=0\nSM_MODEL_DIR=\/opt\/ml\/model\nSM_MODULE_DIR=s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz\nSM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;batch-size&quot;:64,&quot;epochs&quot;:2},&quot;input_config_dir&quot;:&quot;\/opt\/ml\/input\/config&quot;,&quot;input_data_config&quot;:{},&quot;input_dir&quot;:&quot;\/opt\/ml\/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1&quot;,&quot;model_dir&quot;:&quot;\/opt\/ml\/model&quot;,&quot;module_dir&quot;:&quot;s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;fashion_mnist&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:4,&quot;num_gpus&quot;:0,&quot;output_data_dir&quot;:&quot;\/opt\/ml\/output\/data&quot;,&quot;output_dir&quot;:&quot;\/opt\/ml\/output&quot;,&quot;output_intermediate_dir&quot;:&quot;\/opt\/ml\/output\/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;fashion_mnist.py&quot;}\nSM_USER_ARGS=[&quot;--batch-size&quot;,&quot;64&quot;,&quot;--epochs&quot;,&quot;2&quot;]\nSM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\nSM_HP_BATCH-SIZE=64\nSM_HP_EPOCHS=2\nPYTHONPATH=\/opt\/ml\/code:\/usr\/local\/bin:\/usr\/local\/lib\/python37.zip:\/usr\/local\/lib\/python3.7:\/usr\/local\/lib\/python3.7\/lib-dynload:\/usr\/local\/lib\/python3.7\/site-packages\n\nInvoking script with the following command:\n\n\/usr\/local\/bin\/python3.7 fashion_mnist.py --batch-size 64 --epochs 2\n\n\nTensorFlow version: 2.3.1\nEager execution is: True\nKeras version: 2.4.0\n---------- key\/value args\nepochs:2\nbatch_size:64\nmodel_dir:\/opt\/ml\/model\noutput_dir:\/opt\/ml\/output\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1656913410063,
        "Solution_link_count":25.0,
        "Solution_readability":17.3,
        "Solution_reading_time":377.84,
        "Solution_score_count":65.0,
        "Solution_sentence_count":201.0,
        "Solution_word_count":2374.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1124.7847222222,
        "Challenge_answer_count":0,
        "Challenge_body":"We should create a instance of MLflow for each project in order to see the experiments related to the current project.\r\n\r\n- [x] Create project operator to deploy a MLFlow instance for each project.\r\n- [x] Update KDL APP API to create the KDLProject custom resource in k8s.\r\n- [x] Update kdlctl.sh adding project-operator docker image building.\r\n- [x] Add project-operator to KDL server helm chart.\r\n- [x] Add github workflows to publish the project-operator in docker hub.",
        "Challenge_closed_time":1623230615000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619181390000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to create a model in MLflowCatalog due to an error message stating that the registered model with the given name does not exist.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/379",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":8.4,
        "Challenge_reading_time":6.3,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":933.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1124.7847222222,
        "Challenge_title":"All MLflow experiments are visible for any user",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":80,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1446631384107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Helsinki, Finland",
        "Answerer_reputation_count":4255.0,
        "Answerer_view_count":877.0,
        "Challenge_adjusted_solved_time":0.5821208334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am running the <code>pipeline.submit()<\/code> in AzureML, which has a <code>PythonScriptStep<\/code>.\nInside this step, I download a model from tensorflow-hub, retrain it and save it as a <code>.zip<\/code>, and finally, I would like to register it in the Azure ML.\nBut as inside the script I do not have a workspace, <code>Model.register()<\/code> is not the case.\nSo I am trying to use <code>Run.register_model()<\/code> method as below:<\/p>\n\n<pre><code>os.replace(os.path.join('.', archive_name + '.zip'), \n           os.path.join('.', 'outputs', archive_name + '.zip'))\n\nprint(os.listdir('.\/outputs'))\nprint('========================')\n\nrun_context = Run.get_context()\nfinetuning_model = run_context.register_model(model_name='finetuning_similarity_model',\n                                              model_path=os.path.join(archive_name+'.zip'),\n                                              tags={},\n                                              description=\"Finetuning Similarity model\")\n<\/code><\/pre>\n\n<p>But then I have got an error:<\/p>\n\n<blockquote>\n  <p>ErrorResponse \n  {\n      \"error\": {\n          \"message\": \"Could not locate the provided model_path retrained.zip in the set of files uploaded to the run:<\/p>\n<\/blockquote>\n\n<p>despite I have the retrained <code>.zip<\/code> in the <code>.\/outputs<\/code> dir as we can see from the log:<\/p>\n\n<pre><code>['retrained.zip']\n========================\n<\/code><\/pre>\n\n<p>I guess that I am doing something wrong?<\/p>",
        "Challenge_closed_time":1578745983587,
        "Challenge_comment_count":0,
        "Challenge_created_time":1574164584153,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is trying to register a model in Azure ML from a PythonScriptStep in a pipeline. They download a model from tensorflow-hub, retrain it, save it as a .zip, and attempt to register it using Run.register_model() method. However, they receive an error stating that the provided model_path cannot be located in the set of files uploaded to the run, despite having the .zip in the .\/outputs directory. The user is seeking assistance in resolving this issue.",
        "Challenge_last_edit_time":1578744224352,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58933565",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":17.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":1272.6109538889,
        "Challenge_title":"How to register model from the Azure ML Pipeline Script step",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3429.0,
        "Challenge_word_count":150,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1574162655727,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":75.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>I was able to fix the same issue (<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.exceptions.modelpathnotfoundexception?view=azure-ml-py\" rel=\"noreferrer\"><code>ModelPathNotFoundException<\/code><\/a>) by explicitly uploading the model into the run history record before trying to register the model:<\/p>\n\n<pre><code>run.upload_file(\"outputs\/my_model.pickle\", \"outputs\/my_model.pickle\")\n<\/code><\/pre>\n\n<p>Which I found surprising because this wasn't mentioned in many of the official examples and according to the <code>upload_file()<\/code> <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#upload-file-name--path-or-stream-\" rel=\"noreferrer\">documentation<\/a>:<\/p>\n\n<blockquote>\n  <p>Runs automatically capture file in the specified output directory, which defaults to \".\/outputs\" for most run types. Use upload_file only when additional files need to be uploaded or an output directory is not specified.<\/p>\n<\/blockquote>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1578746319987,
        "Solution_link_count":2.0,
        "Solution_readability":19.6,
        "Solution_reading_time":13.4,
        "Solution_score_count":14.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":88.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1250158552416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Challenge_adjusted_solved_time":6.8599055556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to execute Python script from Azure machine learning studio. I had a script bundle(zip file) connect to the Python script as input. There are python files, txt files and other type of files in this zip file. My question is how do I get the file path from this zip file. For example, if I have language model in this  zip file, named lm.pcl, what's the file path of this language model? \nThanks!<\/p>",
        "Challenge_closed_time":1539582490283,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539557794623,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to execute a Python script from Azure machine learning studio using a script bundle (zip file) that contains various types of files including Python and txt files. The user is seeking guidance on how to access the file path of a specific language model (lm.pcl) within the zip file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52807787",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.7,
        "Challenge_reading_time":5.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":6.8599055556,
        "Challenge_title":"Azure machine learning studio get access to the file in upload zip file",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":679.0,
        "Challenge_word_count":88,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1337362023536,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":581.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>They're available under the <code>.\/Script Bundle<\/code> directory. For example, if you were to load a pickled model from the zip file, you'd write something along these lines:<\/p>\n\n<pre><code>import pandas as pd\nimport pickle\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n\n    model = pickle.load(open(\".\/Script Bundle\/model.pkl\", \"rb\"))\n    ...\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":4.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1259808393296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, Canada",
        "Answerer_reputation_count":44706.0,
        "Answerer_view_count":4356.0,
        "Challenge_adjusted_solved_time":4.5641338889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>After AWS Autopilot creates a model, How to use that model to the training data set offline?<\/p>\n<p>How to use that .tar.gz model file?<\/p>",
        "Challenge_closed_time":1637338122312,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637321691430,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to use a model generated by AWS Autopilot for offline training data sets and how to utilize the .tar.gz model file.",
        "Challenge_last_edit_time":1658782461116,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70034212",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":2.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":4.5641338889,
        "Challenge_title":"Model generated by AWS autopilot",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":125.0,
        "Challenge_word_count":28,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The .tar.gz file is a model artifact<\/p>\n<p>To create a model, you combine the algorithm container and the model artifact<\/p>\n<p>You can do so in the Console, under Inference &gt; Models &gt; Create Model<\/p>\n<p>What do you mean by &quot;How to use that model to the training data set offline?&quot;<\/p>\n<p>If you mean, &quot;run a batch transformation&quot;, then once you create a model, you can select the model in the console, then click 'Create batch transform job'<\/p>\n<p>If you want to do it locally, then you can use the SageMaker Python SDK in <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">Local Mode<\/a> and run the transform on your local computer (requires Docker)<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1637865568940,
        "Solution_link_count":1.0,
        "Solution_readability":17.3,
        "Solution_reading_time":9.31,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":108.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":110.8842475,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to open a pickled XGBoost model I created in AWS Sagemaker to look at feature importances in the model. I'm trying to follow the answers in <a href=\"https:\/\/stackoverflow.com\/questions\/55621967\/feature-importance-for-xgboost-in-sagemaker\">this post<\/a>. However, I get an the error shown below. When I try to call <code>Booster.save_model<\/code>, I get an error saying <code>'Estimator' object has no attribute 'save_model'<\/code>. How can I resolve this? <\/p>\n\n<pre><code># Build initial model\nsess = sagemaker.Session()\ns3_input_train = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/train\/'.format(bucket, prefix), content_type='csv')\nxgb_cont = get_image_uri(region, 'xgboost', repo_version='0.90-1')\nxgb = sagemaker.estimator.Estimator(xgb_cont, role, train_instance_count=1, train_instance_type='ml.m4.4xlarge',\n                                    output_path='s3:\/\/{}\/{}'.format(bucket, prefix), sagemaker_session=sess)\nxgb.set_hyperparameters(eval_metric='rmse', objective='reg:squarederror', num_round=100)\nts = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nxgb_name = 'xgb-initial-' + ts\nxgb.set_hyperparameters(eta=0.1, alpha=0.5, max_depth=10)\nxgb.fit({'train': s3_input_train}, job_name=xgb_name)\n\n# Load model to get feature importances\nmodel_path = 's3:\/\/{}\/{}\/\/output\/model.tar.gz'.format(bucket, prefix, xgb_name)\nfs = s3fs.S3FileSystem()\nwith fs.open(model_path, 'rb') as f:\n    with tarfile.open(fileobj=f, mode='r') as tar_f:\n        with tar_f.extractfile('xgboost-model') as extracted_f:\n            model = pickle.load(extracted_f)\n\nXGBoostError: [19:16:42] \/workspace\/src\/learner.cc:682: Check failed: header == serialisation_header_: \n\n  If you are loading a serialized model (like pickle in Python) generated by older\n  XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version.  There's a simple script for helping\n  the process. See:\n\n    https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html\n\n  for reference to the script, and more details about differences between saving model and\n  serializing.\n<\/code><\/pre>",
        "Challenge_closed_time":1584357657183,
        "Challenge_comment_count":2,
        "Challenge_created_time":1583954281893,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to open a pickled XGBoost model created in AWS Sagemaker to look at feature importances in the model. The user is following the answers in a post but is getting an error saying \"'Estimator' object has no attribute 'save_model'\" when trying to call Booster.save_model. The error message suggests that the user should export the model by calling Booster.save_model from the older version first and then load it back in the current version.",
        "Challenge_last_edit_time":1583958473892,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60643094",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":14.6,
        "Challenge_reading_time":27.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":112.0486916667,
        "Challenge_title":"Unable to open pickled Sagemaker XGBoost model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":4586.0,
        "Challenge_word_count":199,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1431970105067,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":4631.0,
        "Poster_view_count":333.0,
        "Solution_body":"<p>Which version of XGBoost are you using in the notebook? The model format has changed in XGBoost 1.0. See <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html\" rel=\"nofollow noreferrer\">https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html<\/a>. Short version: if you're using 1.0 in the notebook, you can't load a pickled model. <\/p>\n\n<p>Here's a working example using XGBoost in script mode (which is much more flexible than the built in algo):<\/p>\n\n<ul>\n<li><a href=\"https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/-\/blob\/master\/sagemaker\/09-XGBoost-script-mode.ipynb\" rel=\"nofollow noreferrer\">https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/-\/blob\/master\/sagemaker\/09-XGBoost-script-mode.ipynb<\/a><\/li>\n<li><a href=\"https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/-\/blob\/master\/sagemaker\/xgb.py\" rel=\"nofollow noreferrer\">https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/-\/blob\/master\/sagemaker\/xgb.py<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":21.3,
        "Solution_reading_time":12.81,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":68.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1425802890212,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA, USA",
        "Answerer_reputation_count":41.0,
        "Answerer_view_count":19.0,
        "Challenge_adjusted_solved_time":14.3147588889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Getting the following response even when I make one request (concurrency set to 200) to a web service. <\/p>\n\n<p>{ status: 503, headers: '{\"content-length\":\"174\",\"content-type\":\"application\/json; charset=utf-8\",\"etag\":\"\\\"8ce068bf420a485c8096065ea3e4f436\\\"\",\"server\":\"Microsoft-HTTPAPI\/2.0\",\"x-ms-request-id\":\"d5c56cdd-644f-48ba-ba2b-6eb444975e4c\",\"date\":\"Mon, 15 Feb 2016 04:54:01 GMT\",\"connection\":\"close\"}',  body: '{\"error\":{\"code\":\"ServiceUnavailable\",\"message\":\"Service is temporarily unavailable.\",\"details\":[{\"code\":\"NoMoreResources\",\"message\":\"No resources available for request.\"}]}}' }<\/p>\n\n<p>The request-response web service is a recommender retraining web service with the training set containing close to 200k records. The training set is already present in my ML studio dataset, only 10-15 extra records are passed in the request. The same experiment was working flawlessly till 13th Feb 2016. I have already tried increasing the concurrency but still the same issue. I even reduced the size of the training set to 20 records, still didn't work.<\/p>\n\n<p>I have two web service both doing something similar and both aren't working since 13th Feb 2016. <\/p>\n\n<p>Finally, I created a really small experiment ( skill.csv --> split row ---> web output )   which doesn't take any input. It just has to return some part of the dataset. Did not work, response code 503.<\/p>\n\n<p>The logs I got are as follows<\/p>\n\n<p>{\n  \"version\": \"2014-10-01\",\n  \"diagnostics\": [{\n    .....\n    {\n      \"type\": \"GetResourceEndEvent\",\n      \"timestamp\": 13.1362,\n      \"resourceId\": \"5e2d653c2b214e4dad2927210af4a436.865467b9e7c5410e9ebe829abd0050cd.v1-default-111\",\n      \"status\": \"Failure\",\n      \"error\": \"The Uri for the target storage location is not specified. Please consider changing the request's location mode.\"\n    },\n    {\n      \"type\": \"InitializationSummary\",\n      \"time\": \"2016-02-15T04:46:18.3651714Z\",\n      \"status\": \"Failure\",\n      \"error\": \"The Uri for the target storage location is not specified. Please consider changing the request's location mode.\"\n    }\n  ]\n}<\/p>\n\n<p>What am I missing? Or am I doing it completely wrong?<\/p>\n\n<p>Thank you in advance.<\/p>\n\n<p>PS: Data is stored in mongoDB and then imported as CSV<\/p>",
        "Challenge_closed_time":1455597422632,
        "Challenge_comment_count":0,
        "Challenge_created_time":1455545889500,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error 503 when making requests to a web service API in Azure ML, even when only making one request with a concurrency set to 200. The web service is a recommender retraining web service with a training set containing close to 200k records. The same experiment was working flawlessly until February 13th, 2016. The user has tried increasing the concurrency and reducing the size of the training set, but the issue persists. The user has also created a small experiment that doesn't take any input, but it still didn't work. The logs show an error related to the target storage location. The data is stored in MongoDB and then imported as CSV.",
        "Challenge_last_edit_time":1456850010663,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35411741",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":28.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":14.3147588889,
        "Challenge_title":"Azure ML: Getting Error 503: NoMoreResources to any web service API even when I only make 1 request",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":283.0,
        "Challenge_word_count":272,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1425802890212,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Boston, MA, USA",
        "Poster_reputation_count":41.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>This was an Azure problem. I quote the Microsoft guy, <\/p>\n\n<blockquote>\n  <p>We believe we have isolated the issue impacting tour service and we are currently working on a fix. We will be able to deploy this in the next couple of days. The problem is impacting only the ASIA AzureML region at this time, so if this is an option for you, might I suggest using a workspace in either the US or EU region until the fix gets rolled out here.<\/p>\n<\/blockquote>\n\n<p>To view the complete discussion, click <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/985e253e-5e54-45a5-a359-5c501152c445\/getting-error-503-nomoreresources-to-any-web-service-api-even-when-i-only-make-1-request?forum=MachineLearning&amp;prof=required\" rel=\"nofollow\">here<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.7,
        "Solution_reading_time":9.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":93.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1444147981107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":126.5392991667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have the following simple setup in Azure ML. <a href=\"https:\/\/i.stack.imgur.com\/kWi0S.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kWi0S.jpg\" alt=\"ML setup\"><\/a> \nBasically the Reader is a SQL query to a DB which returns a vector called Pdelta, which is then passed to the R script for further processing  and the results are then returned back to the web service. The DB query is simple (<code>SELECT Pdelta FROM ...<\/code>) and it works fine. I have set the DB query as a web service paramater as well. <\/p>\n\n<p>Everything seems to work fine, but at the end when i publish it as a web service and test it, it somehow asks for an additional input parameter. The additional parameter gets called <code>PDELTA<\/code>.\n<a href=\"https:\/\/i.stack.imgur.com\/mnzPZ.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mnzPZ.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I am wondering why is this happening, what is it that I am overlooking? I would like to make this web service ask for only one parameter - the SQL query (Delta Query) which would then deliver the Pdeltas. <\/p>\n\n<p>Any ideas or suggestions would be grealty appreciated! <\/p>",
        "Challenge_closed_time":1444147981107,
        "Challenge_comment_count":0,
        "Challenge_created_time":1443692439630,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has set up a simple Azure ML configuration where the Reader is a SQL query to a database that returns a vector called Pdelta, which is then passed to the R script for further processing. However, when the user publishes it as a web service and tests it, an additional input parameter called PDELTA is requested, which the user wants to eliminate to make the web service ask for only one parameter - the SQL query (Delta Query) that would deliver the Pdeltas.",
        "Challenge_last_edit_time":1456850075240,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/32884296",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":15.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":126.5392991667,
        "Challenge_title":"Web service input into SQL query into R in Azure ML",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":347.0,
        "Challenge_word_count":182,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432829415467,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":501.0,
        "Poster_view_count":76.0,
        "Solution_body":"<p>You can remove the web service input block and publish the web service without it. That way the Pdelta input will be passed in only from the Reader module.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":1.97,
        "Solution_score_count":3.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":29.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1334762714136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation_count":6557.0,
        "Answerer_view_count":2005.0,
        "Challenge_adjusted_solved_time":23.5139827778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When training my model the data I start with consist of rows of json data and the expected values I would like to predict from that json data. The json data follows the schema I my deployed service will receive the input as. Before training I run a number of python functions to transform the data and extract features calculated from the raw json data. It is that transformed data which my model is trained on.<\/p>\n\n<p>I have extracted the code to transform the json data into the input my model expects into a separate python file. Now I would like to have my scoring script use that python script to prepare the input sent to the service before feeding it into my trained model.<\/p>\n\n<p>Is there a way to include the data transformation script with the scoring script when deploying my service using the cli command:<\/p>\n\n<pre><code>az ml service create realtime \n    -f &lt;scoring-script&gt;.py \n    --model-file model.pkl \n    -s service_schema.json \n    -n &lt;some-name&gt; \n    -r python \n    --collect-model-data true \n    -c aml_config\\conda_dependencies.yml\n<\/code><\/pre>\n\n<p><em>(the new lines in the above command added for clarity)<\/em><\/p>\n\n<p>The two ways I've come up with is to either:<\/p>\n\n<ul>\n<li>Create my own base docker image that contains the transformation script and use that image as the base for my service. Seems a bit cumbersome to do if I need similar (but different) data transformations for later models.<\/li>\n<li>Concatenate the transformation script with my scoring script into a single file. Seems a bit hacky.<\/li>\n<\/ul>\n\n<p><strong>Is there another way to achive my goal of having a separate data transformation script used both in training and in scoring?<\/strong><\/p>",
        "Challenge_closed_time":1524806224528,
        "Challenge_comment_count":0,
        "Challenge_created_time":1524721574190,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to include a data transformation script with the scoring script when deploying an Azure ML experimentation service. They have extracted the code to transform the json data into a separate python file and want to use it to prepare the input sent to the service before feeding it into the trained model. The user is looking for a way to achieve this goal without creating a base docker image or concatenating the transformation script with the scoring script.",
        "Challenge_last_edit_time":1524806425600,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50035628",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":21.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":23.5139827778,
        "Challenge_title":"Include additional scripts when deploying a Azure ML experimentation service",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":252.0,
        "Challenge_word_count":274,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1275401694660,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Gothenburg, Sweden",
        "Poster_reputation_count":18969.0,
        "Poster_view_count":517.0,
        "Solution_body":"<p>So running <code>az ml service create realtime -h<\/code> provides information about the <code>-d<\/code> flag.<\/p>\n\n<p><code>-d : Files and directories required by the service. Multiple dependencies can be specified with additional -d arguments.<\/code><\/p>\n\n<p>Please try using this flag and provide the additional python file that you would like to call too from your <code>score.py<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.5,
        "Solution_reading_time":5.02,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":52.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":18.8037608333,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have built an XGBoost model using Amazon Sagemaker, but I was unable to find anything which will help me interpret the model and validate if it has learned the right dependencies.<\/p>\n\n<p>Generally, we can see Feature Importance for XGBoost by get_fscore() function in the python API (<a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html\" rel=\"nofollow noreferrer\">https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html<\/a>) I see nothing of that sort in the sagemaker api(<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html<\/a>).<\/p>\n\n<p>I know I can build my own model and then deploy that using sagemaker but I am curious if anyone has faced this problem and how they overcame it.<\/p>\n\n<p>Thanks.<\/p>",
        "Challenge_closed_time":1555001695856,
        "Challenge_comment_count":0,
        "Challenge_created_time":1554934002317,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has built an XGBoost model using Amazon Sagemaker but is unable to find a way to interpret the model and validate if it has learned the right dependencies. They are specifically looking for a way to see the feature importance for XGBoost in Sagemaker, but have not found any function in the Sagemaker API to do so. The user is seeking advice on how to overcome this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55621967",
        "Challenge_link_count":4,
        "Challenge_participation_count":3,
        "Challenge_readability":14.7,
        "Challenge_reading_time":11.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":18.8037608333,
        "Challenge_title":"Feature Importance for XGBoost in Sagemaker",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3637.0,
        "Challenge_word_count":99,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1419327925267,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Mountain View, CA, USA",
        "Poster_reputation_count":123.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>SageMaker XGBoost currently does not provide interface to retrieve feature importance from the model. You can write some code to get the feature importance from the XGBoost model. You have to get the booster object artifacts from the model in S3 and then use the following snippet <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pickle as pkl\nimport xgboost\nbooster = pkl.load(open(model_file, 'rb'))\nbooster.get_score()\nbooster.get_fscore()\n<\/code><\/pre>\n\n<p>Refer <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html\" rel=\"nofollow noreferrer\">XGBoost doc<\/a> for methods to get feature importance from the Booster object such as <code>get_score()<\/code> or <code>get_fscore()<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.4,
        "Solution_reading_time":9.42,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":83.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1535695625688,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":19.0,
        "Challenge_adjusted_solved_time":355.6374158334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am learning Sagemaker and I have this entry point:<\/p>\n\n<pre><code>import os\nimport tensorflow as tf\nfrom tensorflow.python.estimator.model_fn import ModeKeys as Modes\n\nINPUT_TENSOR_NAME = 'inputs'\nSIGNATURE_NAME = 'predictions'\n\nLEARNING_RATE = 0.001\n\n\ndef model_fn(features, labels, mode, params):\n    # Input Layer\n    input_layer = tf.reshape(features[INPUT_TENSOR_NAME], [-1, 28, 28, 1])\n\n    # Convolutional Layer #1\n    conv1 = tf.layers.conv2d(\n        inputs=input_layer,\n        filters=32,\n        kernel_size=[5, 5],\n        padding='same',\n        activation=tf.nn.relu)\n\n    # Pooling Layer #1\n    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n\n    # Convolutional Layer #2 and Pooling Layer #2\n    conv2 = tf.layers.conv2d(\n        inputs=pool1,\n        filters=64,\n        kernel_size=[5, 5],\n        padding='same',\n        activation=tf.nn.relu)\n    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n\n    # Dense Layer\n    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n    dropout = tf.layers.dropout(\n        inputs=dense, rate=0.4, training=(mode == Modes.TRAIN))\n\n    # Logits Layer\n    logits = tf.layers.dense(inputs=dropout, units=10)\n\n    # Define operations\n    if mode in (Modes.PREDICT, Modes.EVAL):\n        predicted_indices = tf.argmax(input=logits, axis=1)\n        probabilities = tf.nn.softmax(logits, name='softmax_tensor')\n\n    if mode in (Modes.TRAIN, Modes.EVAL):\n        global_step = tf.train.get_or_create_global_step()\n        label_indices = tf.cast(labels, tf.int32)\n        loss = tf.losses.softmax_cross_entropy(\n            onehot_labels=tf.one_hot(label_indices, depth=10), logits=logits)\n        tf.summary.scalar('OptimizeLoss', loss)\n\n    if mode == Modes.PREDICT:\n        predictions = {\n            'classes': predicted_indices,\n            'probabilities': probabilities\n        }\n        export_outputs = {\n            SIGNATURE_NAME: tf.estimator.export.PredictOutput(predictions)\n        }\n        return tf.estimator.EstimatorSpec(\n            mode, predictions=predictions, export_outputs=export_outputs)\n\n    if mode == Modes.TRAIN:\n        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n        train_op = optimizer.minimize(loss, global_step=global_step)\n        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n\n    if mode == Modes.EVAL:\n        eval_metric_ops = {\n            'accuracy': tf.metrics.accuracy(label_indices, predicted_indices)\n        }\n        return tf.estimator.EstimatorSpec(\n            mode, loss=loss, eval_metric_ops=eval_metric_ops)\n\n\ndef serving_input_fn(params):\n    inputs = {INPUT_TENSOR_NAME: tf.placeholder(tf.float32, [None, 784])}\n    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n\n\ndef read_and_decode(filename_queue):\n    reader = tf.TFRecordReader()\n    _, serialized_example = reader.read(filename_queue)\n\n    features = tf.parse_single_example(\n        serialized_example,\n        features={\n            'image_raw': tf.FixedLenFeature([], tf.string),\n            'label': tf.FixedLenFeature([], tf.int64),\n        })\n\n    image = tf.decode_raw(features['image_raw'], tf.uint8)\n    image.set_shape([784])\n    image = tf.cast(image, tf.float32) * (1. \/ 255)\n    label = tf.cast(features['label'], tf.int32)\n\n    return image, label\n\n\ndef train_input_fn(training_dir, params):\n    return _input_fn(training_dir, 'train.tfrecords', batch_size=100)\n\n\ndef eval_input_fn(training_dir, params):\n    return _input_fn(training_dir, 'test.tfrecords', batch_size=100)\n\n\ndef _input_fn(training_dir, training_filename, batch_size=100):\n    test_file = os.path.join(training_dir, training_filename)\n    filename_queue = tf.train.string_input_producer([test_file])\n\n    image, label = read_and_decode(filename_queue)\n    images, labels = tf.train.batch(\n        [image, label], batch_size=batch_size,\n        capacity=1000 + 3 * batch_size)\n\n    return {INPUT_TENSOR_NAME: images}, labels\n\ndef neo_preprocess(payload, content_type):\n    import logging\n    import numpy as np\n    import io\n\n    logging.info('Invoking user-defined pre-processing function')\n\n    if content_type != 'application\/x-image' and content_type != 'application\/vnd+python.numpy+binary':\n        raise RuntimeError('Content type must be application\/x-image or application\/vnd+python.numpy+binary')\n\n    f = io.BytesIO(payload)\n    image = np.load(f)*255\n\n    return image\n\n### NOTE: this function cannot use MXNet\ndef neo_postprocess(result):\n    import logging\n    import numpy as np\n    import json\n\n    logging.info('Invoking user-defined post-processing function')\n\n    # Softmax (assumes batch size 1)\n    result = np.squeeze(result)\n    result_exp = np.exp(result - np.max(result))\n    result = result_exp \/ np.sum(result_exp)\n\n    response_body = json.dumps(result.tolist())\n    content_type = 'application\/json'\n\n    return response_body, content_type\n<\/code><\/pre>\n\n<p>And I am training it <\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='cnn_fashion_mnist.py',\n                       role=role,\n                       input_mode='Pipe',\n                       training_steps=1, \n                       evaluation_steps=1,\n                       train_instance_count=1,\n                       output_path=output_path,\n                       train_instance_type='ml.c5.2xlarge',\n                       base_job_name='mnist')\n<\/code><\/pre>\n\n<p>so far it is trying correctly and it tells me that everything when well, but when I check the output there is nothing there or if I try to deploy it I get the error saying it couldn't find the model because there is nothing in the bucker, any ideas or extra configurations? Thank you<\/p>",
        "Challenge_closed_time":1576626745670,
        "Challenge_comment_count":0,
        "Challenge_created_time":1575346450973,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is learning Sagemaker and has created an entry point for a Tensorflow model. They are training the model using Sagemaker, but when they check the output, there is nothing there, and they cannot deploy the model due to the error of not finding the model in the bucket. The user is seeking help to understand if there are any extra configurations required.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59150100",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":17.6,
        "Challenge_reading_time":66.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":64,
        "Challenge_solved_time":355.6374158334,
        "Challenge_title":"Sagemaker and Tensorflow model not saved",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":658.0,
        "Challenge_word_count":407,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1462770189772,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":45.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>Looks like you are using one of the older Tensorflow versions.\nWe would recommend switching to a newer more straight-forward way of running Tensorflow in SageMaker (script mode) by switching to a more recent Tensorflow version.<\/p>\n\n<p>You can read more about it in our documentation:\n<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html<\/a><\/p>\n\n<p>Here is an example that might help:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/tensorflow_script_mode_training_and_serving.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/tensorflow_script_mode_training_and_serving.ipynb<\/a> <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":25.0,
        "Solution_reading_time":12.16,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":61.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":3.3754333334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run custom python\/sklearn sagemaker script on AWS, basically learning from these examples: <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a><\/p>\n<p>All works fine, if define the arguments, train the model and output the file:<\/p>\n<pre><code>parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\nparser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\nparser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n# train the model...\n\njoblib.dump(model, os.path.join(args.model_dir, &quot;model.joblib&quot;))\n<\/code><\/pre>\n<p>And call the job with:<\/p>\n<pre><code>aws_sklearn.fit({'train': 's3:\/\/path\/to\/train', 'test': 's3:\/\/path\/to\/test'}, wait=False)\n<\/code><\/pre>\n<p>In this case model gets stored on different auto-generated bucket, which I do not want. I want to get the output (.joblib file) in the same s3 bucket I took data from. So I add the parameter <code>model-dir<\/code>:<\/p>\n<pre><code>aws_sklearn.fit({'train': 's3:\/\/path\/to\/train', 'test': 's3:\/\/path\/to\/test', `model-dir`: 's3:\/\/path\/to\/model'}, wait=False)\n<\/code><\/pre>\n<p>But it results in error:\n<code>FileNotFoundError: [Errno 2] No such file or directory: 's3:\/\/path\/to\/model\/model.joblib'<\/code><\/p>\n<p>Same happens if I hardcode the output path inside the training script.<\/p>\n<p>So the main question, how can I get the output file in the bucket of my choice?<\/p>",
        "Challenge_closed_time":1610545645387,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610533493827,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to run a custom python\/sklearn sagemaker script on AWS and wants to save the output (.joblib file) in the same S3 bucket from where the data was taken. However, when the user adds the parameter \"model-dir\" to specify the output location, it results in an error \"FileNotFoundError: [Errno 2] No such file or directory\". The user is seeking a solution to save the output file in the bucket of their choice.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65699980",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":23.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":3.3754333334,
        "Challenge_title":"Change model file save location on AWS SageMaker Training Job",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1244.0,
        "Challenge_word_count":159,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1572957474856,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":123.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>You can use parameter <code>output_path<\/code> when you define the estimator. If you use the\n<code>model_dir<\/code> I guess you have to create that bucket beforehand, but you have the advantage that artifacts can be saved in real time during the training (if the instance has rights on S3). You can take a look at my <a href=\"https:\/\/github.com\/roccopietrini\/TFSagemakerDetection\" rel=\"nofollow noreferrer\">repo<\/a> for this specific case.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.5,
        "Solution_reading_time":5.66,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.0104277778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'd like to use GPT-3 for my application.  I understand MS has licensed GPT-3 from OpenAI, and that there is pricing too.  So how do I get to use GPT-3?  <\/p>\n<p>Chris Powell<\/p>",
        "Challenge_closed_time":1605182707940,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605179070400,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information on how to access GPT-3 for their application, including details on Microsoft's licensing and pricing.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/160489\/gpt-3-access",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.2,
        "Challenge_reading_time":2.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.0104277778,
        "Challenge_title":"GPT-3 access",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":34,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=912f72cd-88e9-416f-bbe1-5a7356385053\">@Crispy  <\/a> Thanks for the question, Innovations from our GPT-3 workstreams will be incorporated in later versions of Azure. In the meantime, If you are interested in participation in the OpenAI GPT-3 and Azure Service partnership please fill out this <a href=\"https:\/\/forms.office.com\/Pages\/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRyj5DlT4gqZKgEsfbkRQK5xUQVlSVlJITkxDQkRaOVdESjJGN0dONkQzNy4u\">form<\/a> to submit a request.    <\/p>\n<p>Ignite blog announcement: <a href=\"https:\/\/blogs.microsoft.com\/ai-for-business\/ai-at-scale-ignite\/\">https:\/\/blogs.microsoft.com\/ai-for-business\/ai-at-scale-ignite\/<\/a>    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.0,
        "Solution_reading_time":9.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1399995590030,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Dubai, UAE",
        "Answerer_reputation_count":4682.0,
        "Answerer_view_count":650.0,
        "Challenge_adjusted_solved_time":6.4294858333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>It is my first time to use Azure Machine Learning...<\/p>\n\n<p>When I have trained 2 models using the same training data and testing data, when it comes to evaluate model, it shows error<\/p>\n\n<blockquote>\n  <p>All models must have the same learner type<\/p>\n<\/blockquote>\n\n<p>Do you know what is \"learner type\" of machine learning models and how to tell the learner type of a model?<\/p>\n\n<p>Below is the screenshot of my basic practice on Azure Machine Learning:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/plx4V.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/plx4V.png\" alt=\"Azure Machine Learning practice\"><\/a><\/p>",
        "Challenge_closed_time":1461249085856,
        "Challenge_comment_count":1,
        "Challenge_created_time":1461225939707,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while evaluating two machine learning models trained using the same data on Azure Machine Learning. The error message states that all models must have the same learner type. The user is seeking information on what the \"learner type\" of machine learning models is and how to determine it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36763479",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":9.4,
        "Challenge_reading_time":8.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":6.4294858333,
        "Challenge_title":"How to tell the learner type of machine learning models",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":516.0,
        "Challenge_word_count":94,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361240380856,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3349.0,
        "Poster_view_count":465.0,
        "Solution_body":"<p>The models you compare should be of the same type - binary classification, regression, multi-class classification etc. For example, you can't compare effectiveness of linear regression to the effectiveness of logistics regression. They solve absolutely different tasks.<\/p>\n\n<p>This is the case for you - you try to compare linear regression (which outputs real value) with the multiclass decision forest, which tries to classify input to some class.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.0,
        "Solution_reading_time":5.73,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":65.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1656670919183,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":76.5733497222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm new with ruby and I want to use GCP AIPlatform but I'm struggeling with the payload.<\/p>\n<p>So far, I have :<\/p>\n<pre class=\"lang-rb prettyprint-override\"><code>client = ::Google::Cloud::AIPlatform::V1::PredictionService::Client.new do |config|\n  config.endpoint = &quot;#{location}-aiplatform.googleapis.com&quot;\nend\n\nimg = File.open(imgPath, 'rb') do |img|\n  'data:image\/png;base64,' + Base64.strict_encode64(img.read)\nend\n\ninstance = Instance.new(:content =&gt; img)\n\nrequest = Google::Cloud::AIPlatform::V1::PredictRequest.new(\n  endpoint: &quot;projects\/#{project}\/locations\/#{location}\/endpoints\/#{endpoint}&quot;,\n  instances: [instance]\n)\n\nresult = client.predict request\np result\n<\/code><\/pre>\n<p>Here is my proto<\/p>\n<pre><code>message Instance {\n  required bytes content = 1;\n};\n<\/code><\/pre>\n<p>But I have the following error : <code>Invalid type Instance to assign to submessage field 'instances'<\/code><\/p>\n<p>I read the documentation but for ruby SDK it's a bit light.\nThe parameters are OK, the JS example here : <a href=\"https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-image-object-detection.js\" rel=\"nofollow noreferrer\">https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-image-object-detection.js<\/a> is working with those parameters<\/p>\n<p>What am I doing wrong ?<\/p>",
        "Challenge_closed_time":1656947266916,
        "Challenge_comment_count":1,
        "Challenge_created_time":1656671602857,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is new to Ruby and is trying to use GCP AIPlatform for image classification prediction. They are struggling with the payload and are encountering an error message \"Invalid type Instance to assign to submessage field 'instances'\". The user has provided their code and proto and is seeking help to identify the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72827960",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":15.3,
        "Challenge_reading_time":18.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":76.5733497222,
        "Challenge_title":"How to get image classification prediction from GCP AIPlatform in ruby?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":54.0,
        "Challenge_word_count":126,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1656670919183,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I managed it<\/p>\n<pre class=\"lang-rb prettyprint-override\"><code>client = Google::Cloud::AIPlatform::V1::PredictionService::Client.new do |config|\n  config.endpoint = &quot;#{location}-aiplatform.googleapis.com&quot;\nend\n\nimg = File.open(imgPath, 'rb') do |img|\n  Base64.strict_encode64(img.read)\nend\n\ninstance = Google::Protobuf::Value.new(:struct_value =&gt; {:fields =&gt; {\n  :content =&gt; {:string_value =&gt; img}\n}})\nendpoint = &quot;projects\/#{project}\/locations\/#{location}\/endpoints\/#{endpoint}&quot;\n\n\nrequest = Google::Cloud::AIPlatform::V1::PredictRequest.new(\n  endpoint: endpoint,\n  instances: [instance]\n)\n\nresult = client.predict request\np result\n<\/code><\/pre>\n<p>The use of the Google::Protobuf::Value looks ugly to me but it works<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.6,
        "Solution_reading_time":9.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":55.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1254829817772,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":2595.0,
        "Answerer_view_count":357.0,
        "Challenge_adjusted_solved_time":197.5463266667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to implement a input_handler() in inference.py for a sagemaker inference container.<\/p>\n<p>The images\/arrays are very big (3D). So I want to pass in a S3 URI, then the input_handler() function should load the image\/array from s3 and return the actual numpy array for the model (which expects a tensor):<\/p>\n<pre><code>def input_handler(data, context):\n\n    d = data.read().decode('utf-8')\n\n    body = json.loads(d)\n    s3path = body['s3_path']\n\n    s3 = S3FileSystem()\n    df = np.load(s3.open(s3path))\n\n    return df\n<\/code><\/pre>\n<p>Returning a numpy array worked with the Sagemaker python api version &lt; 1.0 and input_fn(), but does not work with the new container used by sagemaker python api &gt; 2.0 that expects input_handler().<\/p>\n<p>The actual container image is &quot;763104351884.dkr.ecr.eu-central-1.amazonaws.com\/tensorflow-inference:1.15-gpu&quot;.<\/p>\n<p>During inference, I get the following error in CloudWatch thrown by the container:<\/p>\n<pre><code>ERROR:python_service:exception handling request: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(\n\nTraceback (most recent call last):\n  File &quot;\/sagemaker\/python_service.py&quot;, line 289, in _handle_invocation_post\n    res.body, res.content_type = self._handlers(data, context)\n  File &quot;\/sagemaker\/python_service.py&quot;, line 322, in handler\n    response = requests.post(context.rest_uri, data=processed_input)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/requests\/api.py&quot;, line 116, in post\n    return request('post', url, data=data, json=json, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/requests\/api.py&quot;, line 60, in request\n    return session.request(method=method, url=url, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/requests\/sessions.py&quot;, line 512, in request\n    data=data or \n{}\n,\n<\/code><\/pre>\n<p>What is the correct return type? All examples I found were for json &amp; text...<\/p>",
        "Challenge_closed_time":1600259722303,
        "Challenge_comment_count":3,
        "Challenge_created_time":1599496754993,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is trying to implement an input_handler() function in inference.py for a Sagemaker inference container that loads a large 3D image\/array from S3 and returns a numpy array for the model. However, the function is not working with the new container used by Sagemaker python API > 2.0 that expects input_handler(). The user is getting an error during inference, which suggests that the return type is incorrect. The user is seeking guidance on the correct return type for the input_handler() function.",
        "Challenge_last_edit_time":1599548555527,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63781356",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.6,
        "Challenge_reading_time":26.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":211.9353638889,
        "Challenge_title":"How to correctly write a sagemaker tensorflow input_handler() that returns a numpy array?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":636.0,
        "Challenge_word_count":222,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1254829817772,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":2595.0,
        "Poster_view_count":357.0,
        "Solution_body":"<p>This seems to work:<\/p>\n<p><code>return json.dumps({&quot;inputs&quot;: df.tolist() }).<\/code><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":1.38,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":8.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1566789105747,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":156.0,
        "Answerer_view_count":22.0,
        "Challenge_adjusted_solved_time":24.4308544445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Where can I look for the <strong>performance metrics<\/strong> generated by <strong>Amazon SageMaker Debugger\/Profiler<\/strong>?<\/p>",
        "Challenge_closed_time":1663457554492,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663381119547,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for the location of performance metrics generated by Amazon SageMaker Debugger\/Profiler.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73751712",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":17.2,
        "Challenge_reading_time":2.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":21.2319291667,
        "Challenge_title":"Where can I find the Performance Metrics generated by SageMaker Debugger\/Profiler?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":23.0,
        "Challenge_word_count":23,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1389887039672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":5854.0,
        "Poster_view_count":794.0,
        "Solution_body":"<p>1.<code>from sagemaker.debugger import ProfilerConfig<\/code><\/p>\n<p><code>profiler_config = ProfilerConfig( framework_profile_params=FrameworkProfile(start_step=1, num_steps=2) )<\/code><\/p>\n<p>2.\n<code>from sagemaker.debugger import TensorBoardOutputConfig<\/code><\/p>\n<p><code>tensorboard_output_config = TensorBoardOutputConfig(s3_output_path= &lt;&lt; add your bucket name an folder &gt;&gt; )<\/code><\/p>\n<ol start=\"3\">\n<li><p>In your estimator - specify :  <code>profiler_config= profiler_config<\/code> and <code>tensorboard_output_config=tensorboard_output_config<\/code><\/p>\n<\/li>\n<li><p>Train your model<\/p>\n<\/li>\n<li><p>Go to the s3 bucket specified  for your training job name that is assigned in Sagemaker . You should see a report under <strong>rule-output<\/strong> &gt; <strong>ProfilerReport<\/strong> *** &gt; <strong>profiler-output\/<\/strong> &gt; <strong>profiler-report.html<\/strong><\/p>\n<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1663469070623,
        "Solution_link_count":0.0,
        "Solution_readability":23.2,
        "Solution_reading_time":12.31,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":71.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.6225,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nIs there a way to regularly checkpoint model artifact in a SageMaker training job for BYO training container?",
        "Challenge_closed_time":1586359356000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586331915000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to regularly checkpoint the model artifact during a SageMaker training job for a BYO training container.",
        "Challenge_last_edit_time":1668623326404,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUrXX2MIygS5igas27GrAhHw\/how-to-checkpoint-sagemaker-model-artifact-during-a-training-job",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":2.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":7.6225,
        "Challenge_title":"How to checkpoint SageMaker model artifact during a training job?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":403.0,
        "Challenge_word_count":28,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"If you specify a checkpoint configuration (***regardless of managed spot training***) when starting a training job, checkpointing will work. You can provide a local path and S3 path as follows (API reference):\n\n    \"CheckpointConfig\": { \n      \"LocalPath\": \"string\",\n      \"S3Uri\": \"string\"\n    }\n\nThe local path defaults to `\/opt\/ml\/checkpoints\/`, and then you specify the target path in S3 with `S3Uri`.\n\n***Given this configuration, SageMaker will configure an output channel with Continuous upload mode to Amazon S3***. At the time being, this results in running an agent on the hosts that watches the file system and continuously uploads data to Amazon S3. Similar behavior is applied when debugging is enabled, for delivering tensor data to Amazon S3.\n\nAs commented, `sagemaker-containers` implements its own code to save intermediate outputs and watching files on the file system, but I would rather rely on the functionality offered by the service to avoid dependencies on specific libraries where possible.\n\n**Note**: when using SageMaker Processing, which in my view can be considered an abstraction over training or, from another perspective, the foundation for training, you can configure an output channel to use continuous upload mode; further info [here][1].\n\n\n  [1]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ProcessingS3Output.html",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925569444,
        "Solution_link_count":1.0,
        "Solution_readability":16.1,
        "Solution_reading_time":16.88,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":188.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":212.0517972223,
        "Challenge_answer_count":1,
        "Challenge_body":"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-data-capture-endpoint.html\n\nI have followed the steps mentioned in this and it appears I cannot change the encoding for EndpointOutput in datacapture file. It's coming BASE64 for xgboost model. I am using latest version 1.2.3.\n\nFor monitor scheduler it required both EndpointOutput and EndpointInput to have the same encoding. My EndpointInput  is CSV but EndpointOutput is coming to be BASE64 and nothing can change it.\n\nThis is causing issue while run of analyzer. After baseline is generated and data is captured, when monitoring schedule runs the analyzer it throws error of Encoding mismatch. For it to run EndpointOutput and EndpointInput should have same encoding.\n\nI saw we cannot do anything to change the encoding of output. I used LightGBM, CatBoost algorithms also and found for these EndpointOuput encoding is JSON, which is readable but still not solving the purpose.\n\nIs there a way we can change EndpointOutput Encoding for DataCapture.",
        "Challenge_closed_time":1675066186694,
        "Challenge_comment_count":1,
        "Challenge_created_time":1673956972508,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with the EndpointOutput encoding in data capture files while using Amazon SageMaker. The EndpointOutput is coming as BASE64, which cannot be changed, causing an encoding mismatch error while running the analyzer. The user has tried using different algorithms, but the EndpointOutput encoding remains the same. The user is seeking a solution to change the EndpointOutput encoding for DataCapture.",
        "Challenge_last_edit_time":1674302800224,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUGSFVfrFJS_KsdrOMeepDPg\/model-monitor-capture-data-endpointoutput-encoding-is-base64",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.5,
        "Challenge_reading_time":13.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":308.1150516667,
        "Challenge_title":"Model Monitor Capture data - EndpointOutput Encoding is BASE64",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":77.0,
        "Challenge_word_count":156,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Output encoding can be configured by using the [CaptureContentTypeHeader \nin EndpointConfig.DataCaptureConfig](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DataCaptureConfig.html#sagemaker-Type-DataCaptureConfig-CaptureContentTypeHeader). I believe since this is not being set, default encoding i.e. base64 is being used. \n\nPlease try once with this attribute set as below:\n```\n\"CaptureContentTypeHeader\": { \n         \"CsvContentTypes\": [ \"text\/csv\" ]\n      }\n```\n> Assuming that content_type\/accept is \"text_csv\" for the concerned model.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1675066186694,
        "Solution_link_count":1.0,
        "Solution_readability":22.1,
        "Solution_reading_time":7.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":47.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1374162232292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":523.0,
        "Answerer_view_count":48.0,
        "Challenge_adjusted_solved_time":40.9724888889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I tried to create my own experiment using the module, but failed to make it work.\nhere is the exception i got:  <\/p>\n\n<blockquote>\n  <p>Error 0018: Training dataset of user-item-rating triples contains invalid data.\n  [Critical]     {\"InputParameters\":{\"DataTable\":[{\"Rows\":14,\"Columns\":3,\"estimatedSize\":12668928,\"ColumnTypes\":{\"System.String\":1,\"System.Int32\":1,\"System.Double\":1},\"IsComplete\":true,\"Statistics\":{\"0\":[10,0],\"1\":[5422.0,5999.0,873.0,6616.0,1758.0582820478173,7.0,0.0],\"2\":[1.0,1.0,1.0,1.0,0.0,1.0,0.0]}},{\"Rows\":2338,\"Columns\":3,\"estimatedSize\":1404928,\"ColumnTypes\":{\"System.String\":1,\"System.Int32\":1,\"System.Double\":1},\"IsComplete\":true,\"Statistics\":{\"0\":[2338,0],\"1\":[7.5367835757057318,3.0,0.0,704.0,17.738259318519511,64.0,0.0],\"2\":[3.3737234816082085,1.5,0.0,352.0,8.3956874404883841,122.0,0.0]}},{\"Rows\":2532,\"Columns\":22,\"estimatedSize\":4648960,\"ColumnTypes\":{\"System.Int32\":10,\"System.String\":5,\"System.Double\":6,\"System.Boolean\":1},\"IsComplete\":true,\"Statistics\":{\"0\":[4575.7263033175359,5326.5,539.0,6871.0,1987.9561375024909,2532.0,0.0],\"1\":[4575.7263033175359,5326.5,539.0,6871.0,1987.9561375024909,2532.0,0.0],\"2\":[613.0,613.0,613.0,613.0,0.0,1.0,0.0],\"3\":[0,2532],\"4\":[0,2532],\"5\":[4575.7263033175359,5326.5,539.0,6871.0,1987.9561375024909,2532.0,0.0],\"6\":[23.647231437598673,19.99,1.99,149.99,17.237723488320938,90.0,0.0],\"7\":[0.043827014218009476,0.0,0.0,45.99,1.3460680431173562,3.0,0.0],\"8\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"9\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"10\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"11\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"12\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"13\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"14\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"15\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"16\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"17\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"18\":[2524,0],\"19\":[242,18],\"20\":[1,0],\"21\":[2524,0]}}],\"Generic\":{\"traitCount\":10,\"iterationCount\":5,\"batchCount\":4}},\"OutputParameters\":[],\"ModuleType\":\"Microsoft.Analytics.Modules.MatchboxRecommender.Dll\",\"ModuleVersion\":\" Version=6.0.0.0\",\"AdditionalModuleInfo\":\"Microsoft.Analytics.Modules.MatchboxRecommender.Dll, Version=6.0.0.0, Culture=neutral, PublicKeyToken=69c3241e6f0468ca;Microsoft.Analytics.Modules.MatchboxRecommender.Dll.MatchboxRecommender;Train\",\"Errors\":\"Microsoft.Analytics.Exceptions.ErrorMapping+ModuleException: Error 0018: Training dataset of user-item-rating triples contains invalid data.\\r\\n   at Microsoft.Analytics.Modules.MatchboxRecommender.Dll.Utilities.UpdateRatingMetadata(DataTable dataset, String datasetName) in d:\\_Bld\\8833\\7669\\Sources\\Product\\Source\\Modules\\MatchboxRecommender.Dll\\Utilities.cs:line 179\\r\\n   at Microsoft.Analytics.Modules.MatchboxRecommender.Dll.MatchboxRecommender.TrainImpl(DataTable userItemRatingTriples, DataTable userFeatures, DataTable itemFeatures, Int32 traitCount, Int32 iterationCount, Int32 batchCount) in d:\\_Bld\\8833\\7669\\Sources\\Product\\Source\\Modules\\MatchboxRecommender.Dll\\MatchboxRecommender.cs:line 62\",\"Warnings\":[],\"Duration\":\"00:00:00.6722068\"}\n  Module finished after a runtime of 00:00:01.1250071 with exit code -2\n  Module failed due to negative exit code of -2<\/p>\n<\/blockquote>\n\n<p>i've check the input data i'm setting as input user-place-rating table, record by record (no worries it's only 14 records) here it is: <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/LjyD6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LjyD6.png\" alt=\"the input data\"><\/a><\/p>\n\n<p>Here is a screenshot of the experiment:\n<a href=\"https:\/\/i.stack.imgur.com\/I43tG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/I43tG.png\" alt=\"the experiment\"><\/a><\/p>\n\n<p>since the error message is not very informative, I don't know where to start, so, if anybody has an idea, I would be happy to hear about it.<\/p>\n\n<p>Update:\nA friend of mine suggested to add \"Edit Metadata\" module to change the \"rating\" feature into \"int\" or \"float\" types, and the two other(placeID and userID) into string features. that didn't help as well.<\/p>",
        "Challenge_closed_time":1464083199416,
        "Challenge_comment_count":0,
        "Challenge_created_time":1463926726100,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error while trying to create their own experiment using the \"Train Matchbox Recommender\" module in AzureML. The error message received was not very informative, and the user checked the input data they set as input user-place-rating table, record by record. The user also added an \"Edit Metadata\" module to change the \"rating\" feature into \"int\" or \"float\" types, and the two other (placeID and userID) into string features, but it did not help.",
        "Challenge_last_edit_time":1463935698456,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37375506",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":22.4,
        "Challenge_reading_time":56.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":43.46481,
        "Challenge_title":"AzureML: \"Train Matchbox Recommender\" is not working and does not descibe the error",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1187.0,
        "Challenge_word_count":216,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1320061998252,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":778.0,
        "Poster_view_count":89.0,
        "Solution_body":"<p>The matchbox recommender requires that ratings be numerical or categorical. Also when training, your ratings cannot all be the same.<\/p>\n\n<p>You need to use a metadata editor <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn905986.aspx\" rel=\"nofollow\">https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn905986.aspx<\/a> to convert the ratings into numerical features and you need to make sure you are using a range of ratings.<\/p>\n\n<p>Then this should work!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.0,
        "Solution_reading_time":6.1,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1512770138847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":22.3412477778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to use a local training job in SageMaker.<\/p>\n<p>Following this AWS notebook (<a href=\"http:\/\/mxnet_mnist_with_gluon_local_mode.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mxnet_mnist_with_gluon_local_mode.ipynb<\/a>) I was able to train and predict locally.<\/p>\n<p>There is any way to train locally and save the trained model in the Amazon SageMaker Training Job section?\nOtherwise, how can I properly save trained models I trained using local mode?<\/p>",
        "Challenge_closed_time":1596039571220,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595954217667,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to use a local training job in Amazon SageMaker and has successfully trained and predicted locally using an AWS notebook. However, the user is facing challenges in saving the trained model in the Amazon SageMaker Training Job section and is seeking guidance on how to properly save trained models trained using local mode.",
        "Challenge_last_edit_time":1595959142728,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63138835",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":12.9,
        "Challenge_reading_time":8.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":23.7093202778,
        "Challenge_title":"How to save models trained locally in Amazon SageMaker?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3195.0,
        "Challenge_word_count":66,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1464391892936,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Rio de Janeiro, State of Rio de Janeiro, Brazil",
        "Poster_reputation_count":2243.0,
        "Poster_view_count":148.0,
        "Solution_body":"<p>There is no way to have your local mode training jobs appear in the AWS console. The intent of local mode is to allow for faster iteration\/debugging before using SageMaker for training your model.<\/p>\n<p>You can create SageMaker Models from local model artifacts. Compress your model artifacts into a <code>.tar.gz<\/code> file, upload that file to S3, and then create the Model (with the SDK or in the console).<\/p>\n<p>Documentation:<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":18.5,
        "Solution_reading_time":12.35,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":79.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1462822911288,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5460.0,
        "Answerer_view_count":588.0,
        "Challenge_adjusted_solved_time":46.5399019445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>While training a job on a SageMaker instance using H2o AutoML a message \"This H2OFrame is empty\" has come up after running the code, what should I do to fix the problem?<\/p>\n\n<pre><code>\/opt\/ml\/input\/config\/hyperparameters.json\nAll Parameters:\n{'nfolds': '5', 'training': \"{'classification': 'true', 'target': 'y'}\", 'max_runtime_secs': '3600'}\n\/opt\/ml\/input\/config\/resourceconfig.json\nAll Resources:\n{'current_host': 'algo-1', 'hosts': ['algo-1'], 'network_interface_name': 'eth0'}\nWaiting until DNS resolves: 1\n10.0.182.83\nStarting up H2O-3\nCreating Connection to H2O-3\nAttempt 0: H2O-3 not running yet...\nConnecting to H2O server at http:\/\/127.0.0.1:54321... successful.\n-------------------------- ----------------------------------------\n\n-------------------------- ----------------------------------------\nBeginning Model Training\nParse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100%\nClassification - If you want to do a regression instead, set \"classification\":\"false\" in \"training\" params, inhyperparamters.json\nConverting specified columns to categorical values:\n[]\nAutoML progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100%\nThis H2OFrame is empty.\nException during training: Argument `model` should be a ModelBase, got NoneType None\nTraceback (most recent call last):\nFile \"\/opt\/program\/train\", line 138, in _train_model\nh2o.save_model(aml.leader, path=model_path)\nFile \"\/root\/.local\/lib\/python3.7\/site-packages\/h2o\/h2o.py\", line 969, in save_model\nassert_is_type(model, ModelBase)\nFile \"\/root\/.local\/lib\/python3.7\/site-packages\/h2o\/utils\/typechecks.py\", line 457, in assert_is_type\nskip_frames=skip_frames)\nh2o.exceptions.H2OTypeError: Argument `model` should be a ModelBase, got NoneType None\nH2O session _sid_8aba closed.\n<\/code><\/pre>\n\n<p>I'm wondering if it's a problem because of the max_runtime_secs, my data has around 500 rows and 250000 columns.<\/p>",
        "Challenge_closed_time":1568906188310,
        "Challenge_comment_count":2,
        "Challenge_created_time":1568737272183,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an issue while training a job on a SageMaker instance using H2O AutoML. After running the code, a message \"This H2OFrame is empty\" appeared, and an exception occurred during training. The user is unsure if the problem is related to the max_runtime_secs parameter or the size of the data.",
        "Challenge_last_edit_time":1568738644663,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57978333",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":12.3,
        "Challenge_reading_time":26.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":46.9211463889,
        "Challenge_title":"What should I do when H2O AutoML returns \"H2OFrame is empty\"?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":517.0,
        "Challenge_word_count":197,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509012479112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belo Horizonte, MG, Brasil",
        "Poster_reputation_count":97.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>thanks @Marcel Mendes Reis for following up on your solution in the comments. I will repost here for others to easily find:<\/p>\n\n<p><em>I realized the issue was due to the max_runtime. When I trained the model with more time I didn't have the problem.<\/em> <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":3.22,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":45.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":217.8333333333,
        "Challenge_answer_count":2,
        "Challenge_body":"Even with auto Ml, should carefully custom split my data to my satisfaction or just leave it to AutoML?\n\n\u00a0\n\nAnd what difference does it make?",
        "Challenge_closed_time":1626243360000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625459160000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is questioning whether they should custom split their image data or rely on AutoML to do it for them, and is seeking to understand the potential impact of their decision.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Should-I-custom-split-my-image-data\/m-p\/163031#M10",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":4.9,
        "Challenge_reading_time":2.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":217.8333333333,
        "Challenge_title":"Should I custom split my image data?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":489.0,
        "Challenge_word_count":31,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi Ayoola\n\nIf your data is large enough and have wide representation of each category, you may go with the automated split in AutoML. That would save time and perform well.\n\nIf you have some specific needs, such as the representation of certain observations in a specific category is important and limited within the data, you may want to make sure that it is well distributed for validation and test. And custom split would help for that. Another reason of using custom split could be for comparison of your model performance with external models so you use exactly the same training\/test datasets and make an apples to apples comparison.\n\nHere are some tips I find useful in this doc:\n\nhttps:\/\/cloud.google.com\/vision\/automl\/docs\/beginners-guide#distribute_examples_equally_across_categ...\n\nCheers\n\nTuba.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.5,
        "Solution_reading_time":10.36,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":127.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":17.6780297222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am roughly following this script <a href=\"https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/blob\/master\/keras\/05-keras-blog-post\/Fashion%20MNIST-SageMaker.ipynb\" rel=\"nofollow noreferrer\">fashion-MNIST-sagemaker<\/a>.<\/p>\n\n<p>I see that in the notebook <\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ntf_estimator = TensorFlow(entry_point='mnist_keras_tf.py', \n                          role=role,\n                          train_instance_count=1, \n                          train_instance_type='local',\n                          framework_version='1.12', \n                          py_version='py3',\n                          script_mode=True,\n                          hyperparameters={'epochs': 1}\n                         )\n<\/code><\/pre>\n\n<p>I am wondering to what extent I can and should use the <code>train_instance_count<\/code> parameter. Will it distribute training along some dimension automatically, if yes - what is the dimension?<\/p>\n\n<p>Further, does it generally make sense to distribute training horizontally in a keras (with tensorflow) based setting?<\/p>",
        "Challenge_closed_time":1577100115740,
        "Challenge_comment_count":0,
        "Challenge_created_time":1577037857400,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is following a script for horizontally scaling a TensorFlow (Keras) model in SageMaker. They are unsure about the extent to which they can use the \"train_instance_count\" parameter and whether it will distribute training automatically. They are also questioning the effectiveness of horizontally scaling training in a Keras (with TensorFlow) based setting.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59446807",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":17.5,
        "Challenge_reading_time":12.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":17.2939833333,
        "Challenge_title":"sagemaker horizontally scaling tensorflow (keras) model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":295.0,
        "Challenge_word_count":82,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1456487654208,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berlin, Germany",
        "Poster_reputation_count":1464.0,
        "Poster_view_count":62.0,
        "Solution_body":"<p>distributed training is model and framework specific. Not all models are easy to distribute, and from ML framework to ML framework things are not equally easy. <strong>It is rarely automatic, even less so with TensorFlow and Keras.<\/strong><\/p>\n\n<p>Neural nets are conceptually easy to distribute under the data-parallel paradigm, whereby the gradient computation of a given mini-batch is split among workers, which could be multiple devices in the same host (multi-device) or multiple hosts with each multiple devices (multi-device multi-host). The D2L.ai course provides an in-depth view of how neural nets are distributed <a href=\"http:\/\/d2l.ai\/chapter_computational-performance\/multiple-gpus.html\" rel=\"nofollow noreferrer\">here<\/a> and <a href=\"http:\/\/d2l.ai\/chapter_computational-performance\/parameterserver.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Keras used to be trivial to distribute in multi-device, single host fashion with the <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/utils\/multi_gpu_model\" rel=\"nofollow noreferrer\"><code>multi_gpu_model<\/code>, which will sadly get deprecated in 4 months<\/a>. In your case, you seem to refer to multi-host model (more than one machine), and that requires writing ad-hoc synchronization code such as the one seen in <a href=\"https:\/\/www.tensorflow.org\/tutorials\/distribute\/multi_worker_with_keras\" rel=\"nofollow noreferrer\">this official tutorial<\/a>. <\/p>\n\n<p>Now let's look at how does this relate to SageMaker.<\/p>\n\n<p>SageMaker comes with 3 options for algorithm development. Using distributed training may require a varying amount of custom work depending on the option you choose:<\/p>\n\n<ol>\n<li><p>The <strong>built-in algorithms<\/strong> is a library of 18 pre-written algorithms. Many of them are written to be distributed in single-host multi-GPU or multi-GPU multi-host. With that first option, you don't have anything to do apart from setting <code>train_instance_count<\/code> > 1 to distribute over multiple instances<\/p><\/li>\n<li><p>The <strong>Framework containers<\/strong> (the option you are using) are containers developed for popular frameworks (TensorFlow, PyTorch, Sklearn, MXNet) and provide pre-written docker environment in which you can write arbitrary code. In this options, some container will support one-click creation of ephemeral training clusters to do distributed training, <strong>however using <code>train_instance_count<\/code> greater than one is not enough to distribute the training of your model. It will just run your script on multiple machines. In order to distribute your training, you must write appropriate distribution and synchronization code in your <code>mnist_keras_tf.py<\/code> script.<\/strong> For some frameworks such code modification will be very simple, for example for TensorFlow and Keras, SageMaker comes with Horovod pre-installed. Horovod is a peer-to-peer ring-style communication mechanism that requires very little code modification and is highly scalable (<a href=\"https:\/\/eng.uber.com\/horovod\/\" rel=\"nofollow noreferrer\">initial annoucement from Uber<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html#training-with-horovod\" rel=\"nofollow noreferrer\">SageMaker doc<\/a>, <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_horovod\/tensorflow_script_mode_horovod.ipynb\" rel=\"nofollow noreferrer\">SageMaker example<\/a>, <a href=\"https:\/\/aws.amazon.com\/fr\/blogs\/machine-learning\/launching-tensorflow-distributed-training-easily-with-horovod-or-parameter-servers-in-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">SageMaker blog post<\/a>). My recommendation would be to try using Horovod to distribute your code. Similarly, in Apache MXNet you can easily create Parameter Stores to host model parameters in a distributed fashion and sync with them from multiple nodes. MXNet scalability and ease of distribution is one of the reason Amazon loves it.<\/p><\/li>\n<li><p>The <strong>Bring-Your-Own Container<\/strong> requires you to write both docker container and algorithm code. In this situation, you can of course distribute your training over multiple machines but you also have to write machine-to-machine communication code<\/p><\/li>\n<\/ol>\n\n<p>For your specific situation my recommendation would be to scale horizontally first in a single node with multiple GPUs over bigger and bigger machine types, because latency and complexity increase drastically as you switch from single-host to multi-host context. If truly necessary, use multi-node context and things maybe easier if that's done with Horovod.\nIn any case, things are still much easier to do with SageMaker since it manages creation of ephemeral, billed-per-second clusters with built-in, logging and metadata and artifacts persistence and also handles fast training data loading from s3, sharded over training nodes. <\/p>\n\n<p><strong>Note on the relevancy of distributed training<\/strong>: Keep in mind that when you distribute over N devices a model that was running fine on one device, you usually grow the batch size by N so that the per-device batch size stays constant and each device keeps being busy. This will disturb your model convergence, because bigger batches means a less noisy SGD. A common heuristic is to grow the learning rate by N (more info in <a href=\"https:\/\/arxiv.org\/abs\/1706.02677\" rel=\"nofollow noreferrer\">this great paper from Priya Goyal et al<\/a>), but this on the other hand induces instability at the first couple epochs, so it is sometimes associated with a learning rate warmup. Scaling SGD to work well with very large batches is still an active research problem, with new ideas coming up frequently. Reaching good model performance with very large batches sometimes require ad-hoc research and a fair amount of parameter tuning, occasionally to the extent where the extra money spent on finding how to distribute well overcome the benefits of the faster training you eventually manage to run. A situation where distributed training makes sense is when an individual record represent too much compute to form a big enough physical batch on a device, a situation seen on big input sizes (eg vision over HD pictures) or big parameter counts (eg BERT). That being said, for those models requiring very big logical batch you don't necessarily have to distribute things physically: you can run sequentially N batches through your single GPU and wait N per-device batches before doing the gradient averaging and parameter update to simulate having an N times bigger GPU. (a clever hack sometimes called <em>gradient accumulation<\/em>)<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1577101498307,
        "Solution_link_count":9.0,
        "Solution_readability":13.8,
        "Solution_reading_time":85.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":41.0,
        "Solution_word_count":853.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1342709052703,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"D\u00fcsseldorf, Germany",
        "Answerer_reputation_count":1889.0,
        "Answerer_view_count":654.0,
        "Challenge_adjusted_solved_time":25.8006916667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained the following Sagemaker model: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco<\/a><\/p>\n\n<p>I've tried both the JSON and RecordIO version. In both, the algorithm is tested on ONE sample image. However, I have a dataset of 2000 pictures, which I would like to test. I have saved the 2000 jpg pictures in a folder within an S3 bucket and I also have two .mat files (pics + ground truth). How can I apply this model to all 2000 pictures at once and then save the results, rather than doing it one picture at a time?<\/p>\n\n<p>I am using the code below to load a single picture from my S3 bucket:<\/p>\n\n<pre><code>object = bucket.Object('pictures\/pic1.jpg')\nobject.download_file('pic1.jpg')\nimg=mpimg.imread('pic1.jpg')\nimg_name = 'pic1.jpg'\nimgplot = plt.imshow(img)\nplt.show(imgplot)\n\nwith open(img_name, 'rb') as image:\n    f = image.read()\n    b = bytearray(f)\n    ne = open('n.txt','wb')\n    ne.write(b)\n\nimport json\nobject_detector.content_type = 'image\/jpeg'\nresults = object_detector.predict(b)\ndetections = json.loads(results)\nprint (detections['prediction'])\n<\/code><\/pre>",
        "Challenge_closed_time":1539965456440,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539872573950,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a Sagemaker object detection model and wants to test it on a dataset of 2000 pictures saved in an S3 bucket. They are currently using code to load a single picture from the bucket and test the model on it, but they want to know how to apply the model to all 2000 pictures at once and save the results.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52876202",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":18.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":25.8006916667,
        "Challenge_title":"How to bulk test the Sagemaker Object detection model with a .mat dataset or S3 folder of images?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":112.0,
        "Challenge_word_count":157,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1489873508190,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":509.0,
        "Poster_view_count":84.0,
        "Solution_body":"<p>I'm not sure if I understood your question correctly. However, if you want to feed multiple images to the model at once, you can create a multi-dimensional array of images (byte arrays) to feed the model.<\/p>\n\n<p>The code would look something like this.<\/p>\n\n<pre><code>import numpy as np\n...\n\n#  predict_images_list is a Python list of byte arrays\npredict_images = np.stack(predict_images_list)\n\nwith graph.as_default():\n    #  results is an list of typical results you'd get.\n    results = object_detector.predict(predict_images)\n<\/code><\/pre>\n\n<p>But, I'm not sure if it's a good idea to feed 2000 images at once. Better to batch them in 20-30 images at a time and predict. <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":8.31,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":99.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":52.1266738889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there any restriction on registering an ML pickle model into Azure Machine Learning Service in terms of the size of the pickle file?  <\/p>\n<p>Does it cause latency in realtime data processing and getting the prediction results from the pickle file if we have a  model that let's say it 5MB and the other one is 500MB (The bigger file has better performance in terms of accuracy)?  <br \/>\nThanks,  <\/p>\n<p>John<\/p>",
        "Challenge_closed_time":1599800075416,
        "Challenge_comment_count":2,
        "Challenge_created_time":1599612419390,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is inquiring about any restrictions on registering an ML pickle model into Azure Machine Learning Service in terms of the size of the pickle file. They are also concerned about the potential latency in real-time data processing and getting prediction results from a larger pickle file, even if it has better performance in terms of accuracy.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/89630\/ml-pickle-file-size-azure-machine-learning-service",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.8,
        "Challenge_reading_time":5.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":52.1266738889,
        "Challenge_title":"ML Pickle file size Azure Machine Learning Service",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":79,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=ccc37df1-9b57-4e84-920a-f92d8316aa7c\">@JJA  <\/a> Thanks, For ACI we recommend not using a model over 1GB in size.    <br \/>\nFor AKS you are limited by the memory resources that you request for your service, minus about 500mb for the running python process in the pod.    <\/p>\n<p>There will be no difference in prediction speed once the model is successfully deployed.    <br \/>\nRegistering will take longer as we have to upload the model, and deploying will take longer as the service must download the model.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.2,
        "Solution_reading_time":6.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":83.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1369156710532,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Japan",
        "Answerer_reputation_count":189.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":2.8227241667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a project in which I do several optuna studies each having around 50 trials.<\/p>\n<p>The optuna documentation suggests saving each model to a file for later use on <a href=\"https:\/\/optuna.readthedocs.io\/en\/latest\/faq.html#how-to-define-objective-functions-that-have-own-arguments\" rel=\"nofollow noreferrer\">this FAQ section<\/a><\/p>\n<p>What I want is to have all the best models of different studies in a python list. How is that possible?<\/p>\n<p>This is somewhat similar to my code:<\/p>\n<pre><code>def objective(trial, n_epochs, batch_size):\n     params = {\n              'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n              'optimizer': trial.suggest_categorical(&quot;optimizer&quot;, [&quot;Adam&quot;, &quot;RMSprop&quot;, &quot;SGD&quot;]),\n              'n_epochs': n_epochs,\n              'batch_size': batch_size\n              }\n     model = clp_network(trial)\n     accuracy, model = train_and_evaluate(params, model, trial) # THIS LINE\n     return accuracy\n<\/code><\/pre>\n<pre><code>     for i in range(50):\n           study = optuna.create_study(direction=&quot;maximize&quot;, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner())\n           study.optimize(lambda trial: objective(trial, e, b), n_trials=50, show_progress_bar=True)\n<\/code><\/pre>\n<p>I would like to either save the <code>model<\/code> variable in the line marked <strong>THIS LINE<\/strong>, or somehow get the best model as a variable from the study.<\/p>",
        "Challenge_closed_time":1661860813547,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661850651740,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is conducting several optuna studies, each with around 50 trials, and wants to save all the best models of different studies in a Python list. The user is following the optuna documentation to save each model to a file for later use but wants to save the model variable in the code or get the best model as a variable from the study.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73539873",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":16.6,
        "Challenge_reading_time":18.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":2.8227241667,
        "Challenge_title":"saving trained models in optuna to a variable",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":23.0,
        "Challenge_word_count":145,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1612517804323,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Babol, Mazandaran, Iran",
        "Poster_reputation_count":125.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>The easiest way is to define a global variable to store a model for each trial as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import optuna\nfrom collections import defaultdict\n\n\nmodels = defaultdict(dict)\n\ndef objective(t):\n    model = t.suggest_int(&quot;x&quot;, 0, 100)\n    models[t.study.study_name][t.number] = model\n    \n    return model\n\nfor _ in range(10):\n    s = optuna.create_study()\n    s.optimize(objective, n_trials=10)\n\n<\/code><\/pre>\n<p>However I reckon this approach is not scalable in terms of memory space, so I'd suggest removing non-best models after each <code>optimize<\/code> call or saving models on an external file as mentioned in Optuna's FAQ.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.8,
        "Solution_reading_time":8.49,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":82.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4770.0555555556,
        "Challenge_answer_count":0,
        "Challenge_body":"error: no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Challenge_closed_time":1632465008000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1615292808000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The SageMaker Operator Types fail the KubeBuilder V2 custom CRD definition validation check due to unescaped regex patterns. The user expected KubeBuilder to generate a CRD specification that includes AWS SageMaker Operator Types. The issue can be resolved by escaping the regex pattern with quotes.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/174",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.3,
        "Challenge_reading_time":3.66,
        "Challenge_repo_contributor_count":16.0,
        "Challenge_repo_fork_count":52.0,
        "Challenge_repo_issue_count":216.0,
        "Challenge_repo_star_count":146.0,
        "Challenge_repo_watch_count":10.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":4770.0555555556,
        "Challenge_title":"error: no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":23,
        "Discussion_body":"Thanks for using amazon-sagemaker-operator-for-k8s. Please help us with the steps to replicate the issue, especially the installation\r\n\r\nOfficial documentation for reference: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/amazon-sagemaker-operators-for-kubernetes.html I ran into this issue while myself and was resolved by making sure the SageMaker operator was applied and running by verifying with kubectl -n sagemaker-k8s-operator-system get pods Closing since there has been no activity in 90+ days",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":64.0951252778,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm using SageMaker Studio, and I have my data files as well as a requirements.txt organized under my home directory. All works fine when I run notebook kernels interactively: they can access my files just fine. However, when I create a \"notebook job\", it doesn't seem to have access to any of my files. Is there a way to give my notebook job access to the same file system as my interactive notebooks?\n\nAfter I run a job, I see that a folder for the job was created within the input S3 bucket, and within that folder there's a \"input\/\" subfolder. But I don't know how to predict the name of the temp folder created for the job, so it doesn't seem like I could myself drop additional inputs in there, even if I wanted to. And if I could, how would I find them, at run-time?\n\nCould sure use guidance on how my notebook jobs can access input files.\n\nThanks,\n\nChris",
        "Challenge_closed_time":1671466765571,
        "Challenge_comment_count":1,
        "Challenge_created_time":1671236023120,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in accessing data files and requirements.txt organized under their home directory in SageMaker Studio when running a \"notebook job\". While interactive notebook kernels can access the files, the notebook job does not seem to have access to them. The user is seeking guidance on how to give the notebook job access to the same file system as the interactive notebooks.",
        "Challenge_last_edit_time":1671582880756,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUqh6oq1d6QbKzNSszEwPm0g\/can-sagemaker-notebook-jobs-access-studio-storage",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.5,
        "Challenge_reading_time":10.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":64.0951252778,
        "Challenge_title":"Can SageMaker notebook jobs access studio storage?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":68.0,
        "Challenge_word_count":168,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi Chris, the option to use input files is to directly use the S3 URIs in the notebook itself, i.e., instead of reading from an `inputs` folder in your local EFS storage (which doesn't get copied over to `inputs` folder for the training job), read the inputs directly from the S3 URI. If the inputs will be dynamic for your notebook jobs, use parameterized executions (reference - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-auto-run-troubleshoot-override.html)",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1671466784964,
        "Solution_link_count":1.0,
        "Solution_readability":13.2,
        "Solution_reading_time":5.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":36.3344202778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to predict from my past data which has around 20 attribute columns and a label. Out of those 20, only 4 are significant for prediction. But i also want to know that if a row falls into one of the classified categories, what other important correlated columns apart from those 4 and what are their weight. I want to get that result from my deployed web service on Azure.<\/p>",
        "Challenge_closed_time":1461985174656,
        "Challenge_comment_count":1,
        "Challenge_created_time":1461854370743,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to predict using past data with 20 attribute columns and a label, but only 4 columns are significant for prediction. The user wants to know the weight of other important correlated columns apart from those 4, and wants to get the result from the deployed web service on Azure.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36917948",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.2,
        "Challenge_reading_time":5.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":36.3344202778,
        "Challenge_title":"Feature weightage from Azure Machine Learning Deployed Web Service",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":303.0,
        "Challenge_word_count":80,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1461853741067,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>You can use permutation feature importance module but that will give importance of the features across the sample set. Retrieving the weights on per call basis is not available in Azure ML.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":2.42,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":32.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":18.5281647222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to register a data set as a Python step with the Azure Machine Learning Studio designer. Here is my code:<\/p>\n<pre><code>import pandas as pd\nfrom azureml.core import Workspace, Run, Dataset\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    run = Run.get_context()\n    ws = run. experiment.workspace\n    ds = Dataset.from_pandas_dataframe(dataframe1)\n    ds.register(workspace = ws,\n                name = &quot;data set name&quot;,\n                description = &quot;example description&quot;,\n                create_new_version = True)\n    return dataframe1, \n<\/code><\/pre>\n<p>I get an error saying that &quot;create_new_version&quot; in the ds.register line was an unexpected keyword argument. However, this keyword appears in the documentation and I need it to keep track of new versions of the file.<\/p>\n<p>If I remove the argument, I get a different error: &quot;Local data source path not supported for this operation&quot;, so it still does not work. Any help is appreciated. Thanks!<\/p>",
        "Challenge_closed_time":1628119362627,
        "Challenge_comment_count":3,
        "Challenge_created_time":1628113771587,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while trying to register a data set as a Python step with the Azure Machine Learning Studio designer. The error message states that \"create_new_version\" is an unexpected keyword argument, even though it appears in the documentation. Removing the argument results in a different error message. The user needs help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68658385",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":9.3,
        "Challenge_reading_time":13.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1.5530666667,
        "Challenge_title":"Azure Machine Learning Studio designer - \"create new version\" unexpected when registering a data set",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":565.0,
        "Challenge_word_count":138,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1371499229816,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1111.0,
        "Poster_view_count":191.0,
        "Solution_body":"<h2>update<\/h2>\n<p>sharing OP's solution here for easier discovery<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom azureml.core import Workspace, Run, Dataset\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    run = Run.get_context()\n    ws = run. experiment.workspace\n    datastore = ws.get_default_datastore()\n    ds = Dataset.Tabular.register_pandas_dataframe(\n        dataframe1, datastore, 'data_set_name',\n        description = 'data set description.')\n    return dataframe1,\n<\/code><\/pre>\n<h2>original answer<\/h2>\n<p>Sorry you're struggling. You're very close!<\/p>\n<p>A few things may be the culprit here.<\/p>\n<ol>\n<li>It looks like you're using the <code>Dataset<\/code> class, which has been deprecated. I recommend trying <code>Dataset.Tabular.register_pandas_dataframe()<\/code> (<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.dataset_factory.tabulardatasetfactory?view=azure-ml-py#register-pandas-dataframe-dataframe--target--name--description-none--tags-none--show-progress-true-\" rel=\"nofollow noreferrer\">docs link<\/a>) instead of <code>Dataset.from_pandas_dataframe()<\/code>. (<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/work-with-data\/dataset-api-change-notice.md\" rel=\"nofollow noreferrer\">more about the Dataset API deprecation<\/a>)<\/li>\n<li>More conjectire here, but another thing is there might be some limitations to using dataset registration within an &quot;Execute Python Script&quot; (EPS) module due to:\n<ol>\n<li>the workspace object might not have the right permissions<\/li>\n<li>you might not be able to use the <code>register_pandas_dataframe<\/code> method inside the EPS module, but might have better luck with save the dataframe first to parquet, then calling <code>Dataset.Tabular.from_parquet_files<\/code><\/li>\n<\/ol>\n<\/li>\n<\/ol>\n<p>Hopefully something works here!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1628180472980,
        "Solution_link_count":2.0,
        "Solution_readability":14.7,
        "Solution_reading_time":25.08,
        "Solution_score_count":2.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":165.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1321893442023,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1611.0,
        "Answerer_view_count":189.0,
        "Challenge_adjusted_solved_time":367.2256972222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running a Python script with Tensorflow in Amazon Sagemaker notebook instance.  I have no trouble writing to the storage in the notebook normally, but for some reason I am unsuccessful when trying to save Tensorflow model checkpoints.  This code previously worked before it was ported to Sagemaker.<\/p>\n\n<p>Below is a reduced version of my code:<\/p>\n\n<pre><code>bucket = 'sagemaker-complaints-data'    \nprefix = 'DeepTestV2' # place to upload training files within the bucket\ntimestamp = str(int(time()))\nout_dir = os.path.abspath(os.path.join(bucket, prefix, \"runs\", timestamp))\ncheckpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"model\")\npath = saver.save(sess, checkpoint_prefix, global_step=current_step)\nprint(\"Saved model checkpoint to {}\\n\".format(path))\n<\/code><\/pre>\n\n<p>No errors are being thrown and the print statement is outputting the correct path.  I have researched whether there are any known issues with using checkpoints in Sagemaker but have come across literally no posts describing this.<\/p>",
        "Challenge_closed_time":1518715595183,
        "Challenge_comment_count":1,
        "Challenge_created_time":1517393582673,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with saving Tensorflow model checkpoints to an Amazon Sagemaker notebook instance. The code previously worked before it was ported to Sagemaker, and the user is not receiving any error messages. The print statement is outputting the correct path, and the user has researched whether there are any known issues with using checkpoints in Sagemaker but has found no relevant posts.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48539564",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":14.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":367.2256972222,
        "Challenge_title":"Tensorflow - Checkpoints not saving to Sagemaker Notebook Instance",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":690.0,
        "Challenge_word_count":136,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1321893442023,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1611.0,
        "Poster_view_count":189.0,
        "Solution_body":"<p>I have found out where this is - for some reason \"checkpoints\" seems to be a reserved word - changing the word to \"checks\" allowed me to write the folder.  Hope this helps someone!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.8,
        "Solution_reading_time":2.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":32.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1626973229036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":199.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":30.4097575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an already trained a TensorFlow model outside of SageMaker.<\/p>\n<p>I am trying to focus on deployment\/inference but I am facing issues with inference.<\/p>\n<p>For deployment I did this:<\/p>\n<pre><code>from sagemaker.tensorflow.serving import TensorFlowModel\ninstance_type = 'ml.c5.xlarge' \n\nmodel = TensorFlowModel(\n    model_data=model_data,\n    name= 'tfmodel1',\n    framework_version=&quot;2.2&quot;,\n    role=role, \n    source_dir='code',\n)\n\npredictor = model.deploy(endpoint_name='test', \n                                       initial_instance_count=1, \n                                       tags=tags,\n                                       instance_type=instance_type)\n<\/code><\/pre>\n<p>When I tried to infer the model I did this:<\/p>\n<pre><code>import PIL\nfrom PIL import Image\nimport numpy as np\nimport json\nimport boto3\n\nimage = PIL.Image.open('img_test.jpg')\nclient = boto3.client('sagemaker-runtime')\nbatch_size = 1\nimage = np.asarray(image.resize((512, 512)))\nimage = np.concatenate([image[np.newaxis, :, :]] * batch_size)\nbody = json.dumps({&quot;instances&quot;: image.tolist()})\n\nioc_predictor_endpoint_name = &quot;test&quot;\ncontent_type = 'application\/x-image'   \nioc_response = client.invoke_endpoint(\n    EndpointName=ioc_predictor_endpoint_name,\n    Body=body,\n    ContentType=content_type\n )\n<\/code><\/pre>\n<p>But I have this error:<\/p>\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from primary with message &quot;{&quot;error&quot;: &quot;Unsupported Media Type: application\/x-image&quot;}&quot;.\n<\/code><\/pre>\n<p>I also tried:<\/p>\n<pre><code>from sagemaker.predictor import Predictor\n\npredictor = Predictor(ioc_predictor_endpoint_name)\ninference_response = predictor.predict(data=body)\nprint(inference_response)\n<\/code><\/pre>\n<p>And have this error:<\/p>\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from primary with message &quot;{&quot;error&quot;: &quot;Unsupported Media Type: application\/octet-stream&quot;}&quot;.\n<\/code><\/pre>\n<p>What can I do ? I don't know if I missed something<\/p>",
        "Challenge_closed_time":1651191185627,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651077733827,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has encountered an error while trying to infer a pre-trained TensorFlow model deployed outside of SageMaker using an image. The user has deployed the model successfully but is facing issues with inference. The error message indicates an unsupported media type for the image format. The user has tried different methods for inference but still faces the same error. The user is seeking guidance on how to resolve the issue.",
        "Challenge_last_edit_time":1651081710500,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72032469",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.5,
        "Challenge_reading_time":27.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":31.5143888889,
        "Challenge_title":"How to infer a tensorflow pre trained deployed model with an image?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":145.0,
        "Challenge_word_count":191,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1606642099552,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":371.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>Have you tested this model locally? How does inference work with your TF model locally? This should show you how the input needs to be formatted for inference with that model in specific. Application\/x-image data format should be fine. Do you have a custom inference script? Check out this link here for adding an inference script with will let you control pre\/post processing and you can log each line to capture the error: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.9,
        "Solution_reading_time":7.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":77.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1357263005087,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Granada Hills, Los Angeles, CA, United States",
        "Answerer_reputation_count":6262.0,
        "Answerer_view_count":428.0,
        "Challenge_adjusted_solved_time":5.9137519445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm on a Jupyter notebook using Python3 and trying to plot a tree with code like this:<\/p>\n\n<pre><code>import xgboost as xgb\nfrom xgboost import plot_tree\n\nplot_tree(model, num_trees=4)\n<\/code><\/pre>\n\n<p>On the last line I get:<\/p>\n\n<pre><code>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/xgboost\/plotting.py in to_graphviz(booster, fmap, num_trees, rankdir, yes_color, no_color, **kwargs)\n196         from graphviz import Digraph\n197     except ImportError:\n--&gt; 198         raise ImportError('You must install graphviz to plot tree')\n199 \n200     if not isinstance(booster, (Booster, XGBModel)):\n\nImportError: You must install graphviz to plot tree\n<\/code><\/pre>\n\n<p>How do I install graphviz so I can see the plot_tree?<\/p>",
        "Challenge_closed_time":1552366926347,
        "Challenge_comment_count":0,
        "Challenge_created_time":1552345636840,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to plot a tree using xgboost in Python3 on a Jupyter notebook on AWS Sagemaker. However, they are encountering an error message that says they need to install graphviz to plot the tree. The user is seeking guidance on how to install graphviz to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55112494",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.3,
        "Challenge_reading_time":9.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":5.9137519445,
        "Challenge_title":"Install graphiz on AWS Sagemaker",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":572.0,
        "Challenge_word_count":92,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1357263005087,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Granada Hills, Los Angeles, CA, United States",
        "Poster_reputation_count":6262.0,
        "Poster_view_count":428.0,
        "Solution_body":"<p>I was finally able to learn that Conda has a package which can install it for you. I was able to get it installed by running the command:<\/p>\n\n<pre><code>!conda install python-graphviz --yes\n<\/code><\/pre>\n\n<p>Note the <code>--yes<\/code> is only needed if the installation needs to verify adding\/changing other packages since the Jupyter notebook is not interactive once it is running.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.8,
        "Solution_reading_time":4.86,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":59.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1528790837107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":610.0,
        "Answerer_view_count":203.0,
        "Challenge_adjusted_solved_time":208.4337880556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I could really use some help!<\/p>\n\n<p>The company I work for is made up of 52 very different businesses so I can't predict at the company level but instead need to predict business by business then roll up the result to give company wide prediction.<\/p>\n\n<p>I have written an ML model in studio.azureml.net\nIt works great with a 0.947 Coefficient of Determination, but this is for 1 of the businesses.\nI now need to train the model for the other 51.<\/p>\n\n<p>Is there a way to do this in a single ML model rather than having to create 52 very similar models?<\/p>\n\n<p>Any help would be much appreciated !!!<\/p>\n\n<p>Kind Regards\nMartin<\/p>",
        "Challenge_closed_time":1559897214220,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559146852583,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created an ML model in Azure ML for one of the 52 businesses in their company, but now needs to train the model for the other 51 businesses. They are seeking help to determine if there is a way to do this in a single ML model instead of creating 52 similar models.",
        "Challenge_last_edit_time":1560930522380,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56364828",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":8.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":208.4337880556,
        "Challenge_title":"Azure ML - Train a model on segments of the data-set",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":123.0,
        "Challenge_word_count":121,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1300461675980,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":189.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>You can use Ensembles, combining several models to improve predictions. The most direct is stacking when the outputs of all the models are trained on the entire dataset. \nThe method that, I think, corresponds the best to your problem is bagging (bootstrap aggregation). You need to divide the training set into different subsets (each corresponding to a certain business), then train a different model on each subset and combine the result of each classifier. \nAnother way is boosting but it is difficult to implement in Azure ML. \nYou can see an example in <a href=\"https:\/\/gallery.azure.ai\/Experiment\/b6b09fc0c26047e6b4c733ab78a86498\" rel=\"nofollow noreferrer\">Azure ML Gallery<\/a>. <\/p>\n\n<p>Quote from book:<\/p>\n\n<blockquote>\n  <p>Stacking and bagging can be easily implemented in Azure Machine\n  Learning, but other ensemble methods are more difficult. Also, it\n  turns out to be very tedious to implement in Azure Machine Learning an\n  ensemble of, say, more than five models. The experiment is filled with\n  modules and is quite difficult to maintain. Sometimes it is worthwhile\n  to use any ensemble method available in R or Python. Adding more\n  models to an ensemble written in a script can be as trivial as\n  changing a number in the code, instead of copying and pasting modules\n  into the experiment.<\/p>\n<\/blockquote>\n\n<p>You may also have a look at <a href=\"http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html\" rel=\"nofollow noreferrer\">sklearn (Python)<\/a> and caret (R) documentation for further details.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.4,
        "Solution_reading_time":18.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":220.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1280527017200,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3035.0,
        "Answerer_view_count":129.0,
        "Challenge_adjusted_solved_time":98.7025555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have started exploring AWS SageMaker starting with these <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/blazingtext_word2vec_subwords_text8\" rel=\"nofollow noreferrer\">examples provided by AWS<\/a>. I then made some modifications to this particular setup so that it uses the data from my use case for training.<\/p>\n\n<p>Now, as I continue to work on this model and tuning, after I delete the inference endpoint once, I would like to be able to recreate the same endpoint -- even after stopping and restarting the notebook instance (so the notebook \/ kernel session is no longer valid) -- using the already trained model artifacts that gets uploaded to S3 under \/output folder.<\/p>\n\n<p>Now I cannot simply jump directly to this line of code:<\/p>\n\n<pre><code>bt_endpoint = bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>I did some searching -- including <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/blazingtext_hosting_pretrained_fasttext\" rel=\"nofollow noreferrer\">amazon's own example of hosting pre-trained models<\/a>, but I am a little lost. I would appreciate any guidance, examples, or documentation that I could emulate and adapt to my case.<\/p>",
        "Challenge_closed_time":1539622672603,
        "Challenge_comment_count":2,
        "Challenge_created_time":1539267343403,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a model on AWS SageMaker using their own data and made modifications to the setup. They want to recreate the inference endpoint using the already trained model artifacts that were uploaded to S3 under the \/output folder, even after stopping and restarting the notebook instance. The user is seeking guidance, examples, or documentation to help them achieve this.",
        "Challenge_last_edit_time":1539637545790,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52762367",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":14.1,
        "Challenge_reading_time":17.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":98.7025555556,
        "Challenge_title":"Re-hosting a trained model on AWS SageMaker",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1916.0,
        "Challenge_word_count":157,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432179256928,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":17.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Your comment is correct - you can re-create an Endpoint given an existing EndpointConfiguration. This can be done via the console, the AWS CLI, or the SageMaker boto client.<\/p>\n\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-endpoint.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-endpoint.html<\/a><\/li>\n<li><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_endpoint\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_endpoint<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":32.9,
        "Solution_reading_time":9.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":38.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.8427261111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>According the following doc, I should be able to to turn on FeaturizationConfig in the settings:    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train<\/a>    <\/p>\n<p>However I'm getting the following error when I try to change the switch to 'FeaturizationConfig' when setting up the AutoML experiment:    <\/p>\n<blockquote>\n<p>ConfigException: ConfigException: Message: Invalid argument(s) 'featurizationconfig' specified. Supported value(s): 'off, auto'    <\/p>\n<\/blockquote>\n<p>The following is my settings:    <\/p>\n<blockquote>\n<p>import logging    <\/p>\n<p>automl_settings = {    <br \/>\n    &quot;iteration_timeout_minutes&quot;: 15,    <br \/>\n    &quot;experiment_timeout_hours&quot;: 0.3,    <br \/>\n    &quot;enable_early_stopping&quot;: True,    <br \/>\n    &quot;primary_metric&quot;: 'spearman_correlation',    <br \/>\n    &quot;featurization&quot;: 'FeaturizationConfig',    <br \/>\n    &quot;verbosity&quot;: logging.INFO,    <br \/>\n    &quot;n_cross_validations&quot;: 5    <br \/>\n}    <\/p>\n<\/blockquote>",
        "Challenge_closed_time":1638189983847,
        "Challenge_comment_count":1,
        "Challenge_created_time":1638154550033,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to turn on FeaturizationConfig in the settings of Azure AutoML experiment. The error message indicates that the argument 'featurizationconfig' is invalid and only 'off' and 'auto' are supported values. The user has shared their settings which include the featurization parameter set to 'FeaturizationConfig'.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/643670\/azure-automl-featurisation-error",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":20.0,
        "Challenge_reading_time":14.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":9.8427261111,
        "Challenge_title":"Azure AutoML Featurisation Error",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":94,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=6760e7be-77ee-4c41-9c63-8bbcdab1aaae\">@SoonJoo@Genting  <\/a> Thanks, Previously, it was a black-box preprocessing, with user\u2019s preprocess=True\/False setting.    <br \/>\nNew change includes deprecation of <code>preprocess<\/code> and introduction of new field <code>featurization<\/code>, where featurization = \u2018auto\u2019 (for automatic featurization, comparable to preprocess=True) \/ \u2018off\u2019 (to turn off featurization, comparable to preprocess=False) \/ FeaturizationConfig (object to pass in customized configuration on featurization setting).    <\/p>\n<p>For more information on custom featurization as well as how to construct FeaturizationConfig is in this <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-features#customize-featurization\">documentation<\/a>.    <br \/>\nWe also have a notebook available with example in our git <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/regression-explanation-featurization\/auto-ml-regression-explanation-featurization.ipynb\">repo<\/a>.    <br \/>\nUsage example:    <\/p>\n<pre><code>from azureml.automl.core.featurization import FeaturizationConfig  \n  \nfeaturization_config = FeaturizationConfig()  \nfeaturization_config.add_column_purpose('Column2', 'Categorical')  \nfeaturization_config.add_column_purpose('Column5', 'Categorical')  \n  \nautoml_config = AutoMLConfig(task = 'classification', compute_target=compute_target, featurization=featurization_config, **automl_settings )  \nremote_run = experiment.submit(automl_config, show_output = False)  \n<\/code><\/pre>\n<p>For classification &amp; regression you do have the option to turn off automatic featurization.     <\/p>\n<p>featurization    <br \/>\nstr or FeaturizationConfig    <br \/>\n'auto' \/ 'off' \/ FeaturizationConfig Indicator for whether featurization step should be done automatically or not, or whether customized featurization should be used.    <br \/>\n\u2026    <br \/>\nNote: Timeseries features are handled separately when the task type is set to forecasting independent of this parameter.    <\/p>\n<p>\u2022\t<a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-train-automl-client\/azureml.train.automl.automlconfig.automlconfig?view=azure-ml-py\">AutoMLConfig Class<\/a>    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":24.0,
        "Solution_reading_time":30.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":180.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.935,
        "Challenge_answer_count":0,
        "Challenge_body":"`TypeError: object of type 'NoneType' has no len()` happens when suggested [VSCode configuration for kedro](https:\/\/kedro.readthedocs.io\/en\/stable\/09_development\/01_set_up_vscode.html) is used for debugging. The error is due to commandline arguments being `None` when running pipeline directly through `run.py`.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/__main__.py\", line 45, in <module>\r\n    cli.main()\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/..\/debugpy\/server\/cli.py\", line 430, in main\r\n    run()\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/..\/debugpy\/server\/cli.py\", line 267, in run_file\r\n    runpy.run_path(options.target, run_name=compat.force_str(\"__main__\"))\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/pyzypad_example\/run.py\", line 75, in <module>\r\n    run_package()\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/pyzypad_example\/run.py\", line 71, in run_package\r\n    project_context.run()\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/context\/context.py\", line 725, in run\r\n    run_params=record_data, pipeline=filtered_pipeline, catalog=catalog\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/hooks.py\", line 286, in __call__\r\n    return self._hookexec(self, self.get_hookimpls(), kwargs)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/manager.py\", line 93, in _hookexec\r\n    return self._inner_hookexec(hook, methods, kwargs)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/manager.py\", line 87, in <lambda>\r\n    firstresult=hook.spec.opts.get(\"firstresult\") if hook.spec else False,\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 208, in _multicall\r\n    return outcome.get_result()\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 80, in get_result\r\n    raise ex[1].with_traceback(ex[2])\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 187, in _multicall\r\n    res = hook_impl.function(*args)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/kedro-mlflow\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py\", line 85, in before_pipeline_run\r\n    pipeline_name=run_params[\"pipeline_name\"],\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/kedro-mlflow\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py\", line 136, in _generate_kedro_command\r\n    if len(from_inputs) > 0:\r\nTypeError: object of type 'NoneType' has no len()\r\n```",
        "Challenge_closed_time":1601893558000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1601890192000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue where the plugin they are using is only compatible with kedro-mlflow version less than 0.8.0, and the `context` package has been moved or refactored, causing an exception to be thrown.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/78",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":22.2,
        "Challenge_reading_time":48.48,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":21.0,
        "Challenge_repo_issue_count":414.0,
        "Challenge_repo_star_count":145.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":34,
        "Challenge_solved_time":0.935,
        "Challenge_title":"TypeError in _generate_kedro_command when debugging run in VSCode",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":212,
        "Discussion_body":"I see its fixed now so I'm closing this issue.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":15.4171983334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am looking at Azure's training modules and it states I can learn no-code models with Azure, but it also tells me I should know python. I'm a little confused at where I should spend time training in most efficient pathway. My goal is to just do predictive modeling within Azure. I have technical\/IT literacy however coding is at a basic level.   <\/p>\n<p>Ideally id like some sort of Certification, if possible from just &quot;Create no-code predictive models with Azure Machine Learning&quot;  <\/p>\n<p>Is &quot;Microsoft Certified: Azure Data Scientist Associate&quot; going to require a lot of pre work on python\/torch\/tensor? I'd ideally like Azure to be my entry. <\/p>",
        "Challenge_closed_time":1592924808467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592869306553,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is confused about the most efficient pathway to learn code-free predictive modeling in Azure, as they have basic coding skills and want to obtain a certification. They are unsure if the \"Microsoft Certified: Azure Data Scientist Associate\" certification will require pre-work on Python\/Torch\/Tensor and would prefer Azure to be their entry point.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/38841\/pathway-for-code-free-predictive-modeling",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":8.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":15.4171983334,
        "Challenge_title":"Pathway for code free predictive modeling",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":114,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Thanks for reaching out. Azure machine learning has a drag and drop interface (Designer) that supports code free predictive modeling. <a href=\"https:\/\/learn.microsoft.com\/en-us\/learn\/paths\/create-no-code-predictive-models-azure-machine-learning\/\">Create no-code predictive models with Azure Machine Learning<\/a> training modules is a great starting point and provides a pathway for <a href=\"https:\/\/learn.microsoft.com\/en-us\/learn\/certifications\/azure-data-scientist\">Azure Data Scientist Associate certification<\/a>. However, you also need programming experience and familiarity with various data science processes\/principles to be successful on the certification exam.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.9,
        "Solution_reading_time":8.98,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":68.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.7084425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi MSFT Community,     <\/p>\n<p>I followed this guide to set up a GPU: <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/data-science-virtual-machine\/dsvm-ubuntu-intro\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/data-science-virtual-machine\/dsvm-ubuntu-intro<\/a>    <\/p>\n<p>VM: Standard NC12_Promo, 12 vCPUs, 112 Gib RAM    <br \/>\nOperating System: Linux    <br \/>\nOffer: Ubuntu-1804    <\/p>\n<p>I am ready to start deep learning training but I am confused about what to do next. I am doing a medical image classification project. I have 1 millions images store in Azure blob now. Do I need to download them to my VM in order to train? Or is it a better way to access image efficiently?    <\/p>\n<p>What are some good tutorials to set up the experiments? I've read a lot of documentation but still confused.     <\/p>\n<p>Thank you very much!    <br \/>\nBest Regards,    <br \/>\nClaire<\/p>",
        "Challenge_closed_time":1605057040680,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605040090287,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to proceed with deep learning training on Azure for a medical image classification project. They are unsure whether they need to download the 1 million images stored in Azure blob to their VM for training or if there is a more efficient way to access them. They are also looking for tutorials to help set up the experiments.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/158168\/deep-learning-training-on-azure-steps-and-tutorial",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":11.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":4.7084425,
        "Challenge_title":"Deep learning training on Azure steps and tutorial",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":127,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=d324c18d-0c6a-4b3e-878e-30b46421559e\">@gecheng  <\/a>     <br \/>\ncheck on the below AI training modules.    <br \/>\n<a href=\"https:\/\/aischool.microsoft.com\/en-us\/services\/learning-paths\">https:\/\/aischool.microsoft.com\/en-us\/services\/learning-paths<\/a>    <\/p>\n<p>AI Lab    <br \/>\n<a href=\"https:\/\/www.microsoft.com\/en-us\/ai\/ai-lab-projects\">https:\/\/www.microsoft.com\/en-us\/ai\/ai-lab-projects<\/a>    <\/p>\n<p>AI module gallery    <br \/>\n<a href=\"https:\/\/gallery.azure.ai\/browse\">https:\/\/gallery.azure.ai\/browse<\/a>    <\/p>\n<p>----------    <\/p>\n<p>Please don\u2019t forget to &quot;Accept the answer&quot; and \u201cup-vote\u201d wherever the information provided helps you, this can be beneficial to other community members.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.4,
        "Solution_reading_time":9.67,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1342709052703,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"D\u00fcsseldorf, Germany",
        "Answerer_reputation_count":1889.0,
        "Answerer_view_count":654.0,
        "Challenge_adjusted_solved_time":16.1329663889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I was playing with the AWS instances and trying to deploy some locally trained Keras models, but I find no documentation on that. Has anyone already been able to do it? <\/p>\n\n<p>I tried to use a similar approach to <a href=\"https:\/\/aws.amazon.com\/pt\/blogs\/machine-learning\/bring-your-own-pre-trained-mxnet-or-tensorflow-models-into-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/pt\/blogs\/machine-learning\/bring-your-own-pre-trained-mxnet-or-tensorflow-models-into-amazon-sagemaker\/<\/a>, but I had no success. I also found some examples for training keras models in the cloud, but I was not able to get the entry_point + artifacts right. <\/p>\n\n<p>Thanks for your time!<\/p>",
        "Challenge_closed_time":1538644957792,
        "Challenge_comment_count":0,
        "Challenge_created_time":1538586879113,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy locally trained Keras models to AWS Sagemaker but is unable to find any documentation on it. They have tried a similar approach to deploying MXNet or TensorFlow models but have had no success. They are seeking advice on how to get the entry_point and artifacts right.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52632388",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":12.3,
        "Challenge_reading_time":9.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":16.1329663889,
        "Challenge_title":"Is it possible to deploy a already trained Keras to Sagemaker?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1291.0,
        "Challenge_word_count":87,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1448029868996,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Portugal",
        "Poster_reputation_count":260.0,
        "Poster_view_count":52.0,
        "Solution_body":"<p>Yes, it is possible, and yes, the official documentation is not much of help.\nHowever, I wrote an <a href=\"https:\/\/gnomezgrave.com\/2018\/07\/05\/using-a-custom-model-for-ml-inference-with-amazon-sagemaker\" rel=\"nofollow noreferrer\">article on that<\/a>, and I hope it will help you.<\/p>\n\n<p>Let me know if you need more details. Cheers!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.7,
        "Solution_reading_time":4.39,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":40.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":13.4696980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been following along with this really helpful XGBoost tutorial on Medium (code used towards bottom of article): <a href=\"https:\/\/medium.com\/analytics-vidhya\/random-forest-and-xgboost-on-amazon-sagemaker-and-aws-lambda-29abd9467795\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/analytics-vidhya\/random-forest-and-xgboost-on-amazon-sagemaker-and-aws-lambda-29abd9467795<\/a>.<\/p>\n<p>To-date, I've been able to get data appropriately formatted for ML purposes, a model created based on training data, and then test data fed through the model to give useful results.<\/p>\n<p>Whenever I leave and come back to work more on the model or feed in new test data however, I find I need to re-run all model creation steps in order to make any further predictions. Instead I would like to just call my already created model endpoint based on the Image_URI and feed in new data.<\/p>\n<p>Current steps performed:<\/p>\n<p>Model Training<\/p>\n<pre><code>xgb = sagemaker.estimator.Estimator(containers[my_region],\n                                    role, \n                                    train_instance_count=1, \n                                    train_instance_type='ml.m4.xlarge',\n                                    output_path='s3:\/\/{}\/{}\/output'.format(bucket_name, prefix),\n                                    sagemaker_session=sess)\nxgb.set_hyperparameters(eta=0.06,\n                        alpha=0.8,\n                        lambda_bias=0.8,\n                        gamma=50,\n                        min_child_weight=6,\n                        subsample=0.5,\n                        silent=0,\n                        early_stopping_rounds=5,\n                        objective='reg:linear',\n                        num_round=1000)\n\nxgb.fit({'train': s3_input_train})\n\nxgb_predictor = xgb.deploy(initial_instance_count=1,instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n<p>Evaluation<\/p>\n<pre><code>test_data_array = test_data.drop([ 'price','id','sqft_above','date'], axis=1).values #load the data into an array\n\nxgb_predictor.serializer = csv_serializer # set the serializer type\n\npredictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\npredictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array\nprint(predictions_array.shape)\n\nfrom sklearn.metrics import r2_score\nprint(&quot;R2 score : %.2f&quot; % r2_score(test_data['price'],predictions_array))\n<\/code><\/pre>\n<p>It seems that this particular line:<\/p>\n<pre><code>predictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\n<\/code><\/pre>\n<p>needs to be re-written in order to not reference xgb.predictor but instead reference the model location.<\/p>\n<p>I have tried the following<\/p>\n<pre><code>trained_model = sagemaker.model.Model(\n    model_data='s3:\/\/{}\/{}\/output\/xgboost-2020-11-10-00-00\/output\/model.tar.gz'.format(bucket_name, prefix),\n    image_uri='XXXXXXXXXX.dkr.ecr.us-east-1.amazonaws.com\/xgboost:latest',\n    role=role)  # your role here; could be different name\n\ntrained_model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n<p>and then replaced<\/p>\n<pre><code>xgb_predictor.serializer = csv_serializer # set the serializer type\npredictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\n<\/code><\/pre>\n<p>with<\/p>\n<pre><code>trained_model.serializer = csv_serializer # set the serializer type\npredictions = trained_model.predict(test_data_array).decode('utf-8') # predict!\n<\/code><\/pre>\n<p>but I get the following error:<\/p>\n<pre><code>AttributeError: 'Model' object has no attribute 'predict'\n<\/code><\/pre>",
        "Challenge_closed_time":1605108362620,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605059871707,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created an XGBoost model on Amazon Sagemaker and wants to call the already created model endpoint based on the Image_URI and feed in new data. However, the user is facing an error while trying to replace the xgb_predictor with the trained_model.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64779388",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":17.5,
        "Challenge_reading_time":43.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":13.4696980556,
        "Challenge_title":"How to invoke Sagemaker XGBoost endpoint post model creation?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1242.0,
        "Challenge_word_count":272,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1532706046196,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":191.0,
        "Poster_view_count":29.0,
        "Solution_body":"<p>that's a good question :) I agree, many of the official tutorials tend to show the full train-to-invoke pipeline and don't emphasize enough that each step can be done separately. In your specific case, when you want to invoke an already-deployed endpoint, you can either: (A) use the invoke API call in one of the numerous SDKs (example in <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker-runtime\/invoke-endpoint.html\" rel=\"nofollow noreferrer\">CLI<\/a>, <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker-runtime.html#SageMakerRuntime.Client.invoke_endpoint\" rel=\"nofollow noreferrer\">boto3<\/a>) or (B) or instantiate a <code>predictor<\/code> with the high-level Python SDK, either the generic <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html\" rel=\"nofollow noreferrer\"><code>sagemaker.model.Model<\/code><\/a> class or its XGBoost-specific child: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/xgboost\/xgboost.html#sagemaker.xgboost.model.XGBoostPredictor\" rel=\"nofollow noreferrer\"><code>sagemaker.xgboost.model.XGBoostPredictor<\/code><\/a> as illustrated below:<\/p>\n<pre><code>from sagemaker.xgboost.model import XGBoostPredictor\n    \npredictor = XGBoostPredictor(endpoint_name='your-endpoint')\npredictor.predict('&lt;payload&gt;')\n<\/code><\/pre>\n<p>similar question <a href=\"https:\/\/stackoverflow.com\/questions\/56255154\/how-to-use-a-pretrained-model-from-s3-to-predict-some-data\/56277411#56277411\">How to use a pretrained model from s3 to predict some data?<\/a><\/p>\n<p>Note:<\/p>\n<ul>\n<li>If you want the <code>model.deploy()<\/code> call to return a predictor, your model must be instantiated with a <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html\" rel=\"nofollow noreferrer\"><code>predictor_cls<\/code><\/a>. This is optional, you can also first deploy a model, and then invoke it as a separate step with the above technique<\/li>\n<li>Endpoints create charges even if you don't invoke them; they are charged per uptime. So if you don't need an always-on endpoint, don't hesitate to shut it down to minimize costs.<\/li>\n<\/ul>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":17.5,
        "Solution_reading_time":28.73,
        "Solution_score_count":2.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":196.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":29.7938888889,
        "Challenge_answer_count":0,
        "Challenge_body":"## Which example? Describe the issue\r\n\r\nexample:  az ml online-deployment create --name blue --endpoint-name amlarc-runner-simple-849b --resource-group lt-westus2-r6-amlarc-rg --workspace-name lt-westus2-r6-arc-ws --file azureml-examples\/cli\/endpoints\/online\/\/amlarc\/blue-deployment.yml --all-traffic\r\ndescription:\r\nFile \"\/var\/azureml-server\/entry.py\", line 1, in <module>\r\n    import create_app\r\n  File \"\/var\/azureml-server\/create_app.py\", line 3, in <module>\r\n    import aml_framework\r\n  File \"\/var\/azureml-server\/aml_framework.py\", line 9, in <module>\r\n    from synchronous.framework import *\r\n  File \"\/var\/azureml-server\/synchronous\/framework.py\", line 3, in <module>\r\n    from flask import Flask, request, g, Request, Response, Blueprint\r\n  File \"\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/__init__.py\", line 21, in <module>\r\n    from .app import Flask, Request, Response\r\n  File \"\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/app.py\", line 26, in <module>\r\n    from . import cli, json\r\n  File \"\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/json\/__init__.py\", line 21, in <module>\r\n    from itsdangerous import json as _json\r\nImportError: cannot import name 'json' from 'itsdangerous' (\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/itsdangerous\/__init__.py)\r\n\r\n## Additional context\r\n\r\nhttps:\/\/ml.azure.com\/endpoints\/realtime\/amlarc-runner-simple-849b\/logs?wsid=\/subscriptions\/589c7ae9-223e-45e3-a191-98433e0821a9\/resourcegroups\/lt-westus2-r6-amlarc-rg\/workspaces\/lt-westus2-r6-arc-ws&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\r\n\r\n-\r\n",
        "Challenge_closed_time":1645604942000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1645497684000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is reporting a bug related to updating test documentation to connect AzureML with GitHub actions. The steps to replicate the issue involve creating a new AzureML workspace, creating two new clusters, adding subscription ID to GitHub action secrets, installing Azure CLI, creating a Service Principal, and adding the output from the Service Principal as an action secret. The user has not mentioned any specific platform where the issue is happening.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/azureml-examples\/issues\/977",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.3,
        "Challenge_reading_time":24.77,
        "Challenge_repo_contributor_count":167.0,
        "Challenge_repo_fork_count":908.0,
        "Challenge_repo_issue_count":2335.0,
        "Challenge_repo_star_count":1110.0,
        "Challenge_repo_watch_count":2708.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":29.7938888889,
        "Challenge_title":"ImportError: cannot import name 'json' from 'itsdangerous' (\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/itsdangerous\/__init__.py)",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":115,
        "Discussion_body":"This issue should be mitigated once the PR is merged: https:\/\/github.com\/Azure\/azureml-examples\/pull\/981",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1589738451347,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":179.0,
        "Answerer_view_count":53.0,
        "Challenge_adjusted_solved_time":115.8372675,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am training a model using AMLS. I have a training pipeline in which step 1 trains a model then saves the output in temporary datastore model_folder using<\/p>\n<pre><code>os.makedirs(output_folder, exist_ok=True)\noutput_path = output_folder + &quot;\/model.pkl&quot;\njoblib.dump(value=model, filename=output_path)\n<\/code><\/pre>\n<p>Step 2 loads the model and registers it. The model folder is defined in the pipeline as<\/p>\n<pre><code>model_folder = PipelineData(&quot;model_folder&quot;, datastore=ws.get_default_datastore())\n<\/code><\/pre>\n<p>However, step 1 fails when it tries to save the model with the following ServiceError:<\/p>\n<p>Failed to upload outputs due to Exception: Microsoft.RelInfra.Common.Exceptions.OperationFailedException: Cannot upload output xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx. ---&gt; Microsoft.WindowsAzure.Storage.StorageException: This request is not authorized to perform this operation using this permission.<\/p>\n<p>How can I solve this? Earlier in my code I had no problem interacting with the default datastore using<\/p>\n<pre><code>default_ds = ws.get_default_datastore()\ndefault_ds.upload_files(...)\n<\/code><\/pre>\n<p>My <code>70_driver_log.txt<\/code> is as follows:<\/p>\n<pre><code>[2020-08-25T04:03:27.315114] Entering context manager injector.\n[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['train_word2vec.py', '--output_folder', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\/model_folder', '--model_type', 'WO', '--training_field', 'task_title', '--regex', '1', '--stopword_removal', '1', '--tokenize_basic', '0', '--remove_punctuation', '0', '--autocorrect', '0', '--lemmatization', '1', '--word_vector_length', '152', '--model_learning_rate', '0.025', '--model_min_count', '0', '--model_window', '7', '--num_epochs', '10'])\nStarting the daemon thread to refresh tokens in background for process with pid = 113\nEntering Run History Context Manager.\nCurrent directory:  \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\nPreparing to call script [ train_word2vec.py ] with arguments: ['--output_folder', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\/model_folder', '--model_type', 'WO', '--training_field', 'task_title', '--regex', '1', '--stopword_removal', '1', '--tokenize_basic', '0', '--remove_punctuation', '0', '--autocorrect', '0', '--lemmatization', '1', '--word_vector_length', '152', '--model_learning_rate', '0.025', '--model_min_count', '0', '--model_window', '7', '--num_epochs', '10']\nAfter variable expansion, calling script [ train_word2vec.py ] with arguments: ['--output_folder', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\/model_folder', '--model_type', 'WO', '--training_field', 'task_title', '--regex', '1', '--stopword_removal', '1', '--tokenize_basic', '0', '--remove_punctuation', '0', '--autocorrect', '0', '--lemmatization', '1', '--word_vector_length', '152', '--model_learning_rate', '0.025', '--model_min_count', '0', '--model_window', '7', '--num_epochs', '10']\n\nScript type = None\n[nltk_data] Downloading package stopwords to \/root\/nltk_data...\n[nltk_data]   Unzipping corpora\/stopwords.zip.\n[nltk_data] Downloading package wordnet to \/root\/nltk_data...\n[nltk_data]   Unzipping corpora\/wordnet.zip.\nOUTPUT FOLDER: \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\/model_folder\nLoading SQL data...\nLoading abbreviation data...\n\/azureml-envs\/azureml_xxxxx\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py:1783: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#returning-a-view-versus-a-copy\n  self.obj[item_labels[indexer[info_axis]]] = value\nPre-processing data...\nSuccesfully pre-processed the the text data\nTraining Word2Vec model...\nSaving the model...\nStarting the daemon thread to refresh tokens in background for process with pid = 113\n\n\nThe experiment completed successfully. Finalizing run...\n[2020-08-25T04:03:52.293994] TimeoutHandler __init__\n[2020-08-25T04:03:52.294149] TimeoutHandler __enter__\nCleaning up all outstanding Run operations, waiting 300.0 seconds\n2 items cleaning up...\nCleanup took 0.44109439849853516 seconds\n[2020-08-25T04:03:52.818991] TimeoutHandler __exit__\n2020\/08\/25 04:04:00 logger.go:293: Process Exiting with Code:  0\n<\/code><\/pre>\n<p>My arg parse arguments include<\/p>\n<pre><code>parser.add_argument('--output_folder', type=str, dest='output_folder', default=&quot;output_folder&quot;, help='output folder')\n<\/code><\/pre>",
        "Challenge_closed_time":1598745717623,
        "Challenge_comment_count":6,
        "Challenge_created_time":1598325386190,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with joblib.dump() failing to save a model to a temporary data store in AMLS. The error message indicates that the request is not authorized to perform the operation using the given permission. The user is seeking a solution to this problem.",
        "Challenge_last_edit_time":1598328703460,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63571552",
        "Challenge_link_count":1,
        "Challenge_participation_count":8,
        "Challenge_readability":16.7,
        "Challenge_reading_time":68.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":116.7587313889,
        "Challenge_title":"joblib.dump() fails when saving model to temporary data store in AMLS",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":571.0,
        "Challenge_word_count":413,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589738451347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":179.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>Fixed this problem by adding my AMLS workspace to a 'storage blob data contributor' role in the AMLS default storage account. It seemly like usually this role is added by default, but it didn't happen in my case.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":2.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":38.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1478712810887,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Belgium",
        "Answerer_reputation_count":11153.0,
        "Answerer_view_count":888.0,
        "Challenge_adjusted_solved_time":5.928925,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've run a simple two-class neural network where I got this result in the end (eval):<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/vw2jQ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vw2jQ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I think I am going to be happy with the <code>True Positive<\/code> and <code>False Negative<\/code> results. But what does <code>False Positive<\/code> mean? <code>False Positive<\/code> means it did not correctly classify 2002 elements and missed them?<\/p>\n\n<p>The <code>Accuracy<\/code> is 66%, that's really bad right? Whats the difference between that and <code>AUC<\/code>?<\/p>\n\n<p><code>Precision<\/code> suffers because Accuracy also is bad (I hoping for a 80%+)?<\/p>\n\n<p>And how do I flip <code>Positive Label<\/code> and <code>Negative Label<\/code>? I really want to predict the classification where the target is to find <code>CANDIDATE<\/code><\/p>",
        "Challenge_closed_time":1488062187216,
        "Challenge_comment_count":0,
        "Challenge_created_time":1488042320197,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has run a two-class neural network and is trying to understand the classification results. They are unsure about the meaning of False Positive and the difference between Accuracy and AUC. They are also concerned about the low accuracy and how it affects Precision. The user wants to know how to flip Positive and Negative Labels to predict the classification for finding CANDIDATE.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42458982",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":12.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":5.5186163889,
        "Challenge_title":"Understanding classification results",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":81.0,
        "Challenge_word_count":113,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267709938320,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Norway",
        "Poster_reputation_count":13032.0,
        "Poster_view_count":1004.0,
        "Solution_body":"<p>Basically, for the false\/true positives and false\/true negatives :\nYou have detected almost all the CANDIDATE samples in your dataset, 3420 of them were correctly predicted as TRUE and 31 of them were predicted as FALSE. This information is captured in the Recall ratio : 3420\/(3420+31) = 99.1%. It is very high, so very good. <\/p>\n\n<p>However, you have predicted <strong>too many<\/strong> CANDIDATE. Indeed, in all the TRUE values predicted by the model, 3420 were actually TRUE and 2002 were actually FALSE. This makes the Precision ratio bad : 3420\/(3420+2002)=63.1%. Which is not that good. <\/p>\n\n<p>F1 is a combinaison between Precision and Recall, it summarizes them into one value, some kind of weighted average. The formula is 2*(P*R)\/(P+R). So if one of Precision or Recall is bad : the F1score will capture it. <\/p>\n\n<p>You can see that you have a total of 5999 examples in your data set. Out of those, 3451 are really TRUE and 2548 are really FALSE. So you have 57% of your data that is TRUE. If you make a really stupid classifier that classifies everything as TRUE whatever the features are, then you will get 57% accuracy. Given that, 66.1% accuracy is not really good. \nIf you look at the second column of that table, you only predict 577 FALSE out of the 5999 samples. Your classifier is heavily biased towards TRUE predictions. <\/p>\n\n<p>For the AUC, it stands for Area Under the Curve. You can read <a href=\"http:\/\/fastml.com\/what-you-wanted-to-know-about-auc\/\" rel=\"nofollow noreferrer\">more detailed info about it here<\/a>. To summarize : when you predic a value, you don't really get True or False directly. You get a real number between 0 (False) and 1 (True). The way to classify a predicted value, say 0.2, is to use a Threshold. The threshold is by default set to 0.5. So if you predict 0.2, your model will predict to classify it as a False because 0.2&lt;0.5. But you could make that treshold move between 0 and 1. If the classifier is really good, if it discriminates really well the Falses and Trues predictions, then the AUC will be close to 1. If it's really bad, it will be close to 0.5. Refer to the link if you need more information. <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1488063664327,
        "Solution_link_count":1.0,
        "Solution_readability":6.6,
        "Solution_reading_time":26.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":32.0,
        "Solution_word_count":368.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":13.6488463889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The example notebook: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/autopilot\/autopilot_customer_churn.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/autopilot\/autopilot_customer_churn.ipynb<\/a> states that in the Analyzing Data step:<\/p>\n<p><code>The dataset is analyzed and Autopilot comes up with a list of ML pipelines that should be tried out on the dataset. The dataset is also split into train and validation sets.<\/code><\/p>\n<p>Presumably, autopilot uses this validation set to select the best performing model candidates to return to the user. However, I have not found a way to manually set this validation set used by sagemaker autopilot.<\/p>\n<p>For example, google automl, allows users to add TRAIN, VALIDATE,TEST keywords to a data_split column to manually set which data points are in which set.<\/p>\n<p>Is something like this currently possible which sagemaker autopilot?<\/p>",
        "Challenge_closed_time":1600931097000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600881961153,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information on how to manually set the validation set used by Sagemaker Autopilot, as they have not found a way to do so and are aware that other platforms like Google AutoML allow users to manually set data points for the validation set.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64033258",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":13.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":13.6488463889,
        "Challenge_title":"How to explicitly set sagemaker autopilot's validation set?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":159.0,
        "Challenge_word_count":122,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1412820654710,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>I'm afraid you can't do this at the moment. The validation set is indeed built by Autopilot itself.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.4,
        "Solution_reading_time":1.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":58.8322222222,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm working on time-series modeling. I'm comparing battle-of-algorithms against the autopilot machine learning approach to identify the model that best fits my use case.\nI understand that Amazon SageMaker Autopilot doesn't work with time series.\nIs there an alternative library or algorithm in the AWS ecosystem that implements battle-of-algorithms for time series?",
        "Challenge_closed_time":1601883088000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601671292000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is working on time-series modeling and is comparing the battle-of-algorithms against the autopilot machine learning approach. However, they have found that Amazon SageMaker Autopilot does not work with time series. The user is looking for an alternative library or algorithm in the AWS ecosystem that implements battle-of-algorithms for time series.",
        "Challenge_last_edit_time":1668529103319,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUxaXaoPpqRyGdkh6aKK9uew\/can-i-use-sagemaker-autopilot-for-my-time-series-modeling",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.3,
        "Challenge_reading_time":5.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":58.8322222222,
        "Challenge_title":"Can I use SageMaker Autopilot for my time-series modeling?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":319.0,
        "Challenge_word_count":60,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Amazon SageMaker Autopilot currently supports regression, binary classification, and multi-class classification. SageMaker supports only tabular data formatted in files with comma-separated values. For more information, see [Automate model development with Amazon SageMaker Autopilot][1].\n\n[1]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/autopilot-automate-model-development.html\n\nFor your use case, you can use [Amazon Forecast][2]. Amazon Forecast includes the [AutoML][3] feature that trains different models with your target time series, related time series, and item metadata. Amazon Forecast then uses the model with the best accuracy metrics.\n\n[2]: https:\/\/docs.aws.amazon.com\/forecast\/latest\/dg\/what-is-forecast.html\n[3]: https:\/\/docs.aws.amazon.com\/forecast\/latest\/dg\/automl.html",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925587776,
        "Solution_link_count":3.0,
        "Solution_readability":18.4,
        "Solution_reading_time":10.44,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":3520.4706275,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have pretrained model artifacts stored in S3 buckets. I want to create a service that loads this model and uses it for inference.<\/p>\n\n<p>I am working in AWS ecosystem and confused between using ECS vs Sagemaker for model deployment?\nWhat are some pros\/cons for choosing one over other?<\/p>",
        "Challenge_closed_time":1578553162123,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578049726853,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has pre-trained model artifacts stored in S3 buckets and wants to create a service that loads this model and uses it for inference. They are confused between using AWS ECS and Sagemaker for model deployment and are seeking advice on the pros and cons of each option.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59577521",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":4.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":139.8431305556,
        "Challenge_title":"AWS Sagemaker vs ECS for model hosting",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3288.0,
        "Challenge_word_count":55,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1390885171883,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"College Station, TX, United States",
        "Poster_reputation_count":426.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>SageMaker has a higher price mark but it is taking a lot of the heavy lifting of deploying a machine learning model, such as wiring the pieces (load balancer, gunicorn, CloudWatch, Auto-Scaling...) and it is easier to automate the processes such as A\/B testing.<\/p>\n\n<p>If you have a strong team of DevOps that have nothing more important to do, you can build a flow that will be cheaper than the SageMaker option. ECS and EKS are doing at the same time a lot of work to make it very easy for you to automate the machine learning model deployments. However, they will always be more general purpose and SageMaker with its focus on machine learning will be easier for these use cases. <\/p>\n\n<p>The usual pattern of using the cloud is to use the managed services early on as you want to move fast and you don't really know where are your future problems. Once the system is growing and you start feeling some pains here and there, you can decide to spend the time and improve that part of the system. Therefore, if you don't know the pros\/cons, start with using the simpler options. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1590723421112,
        "Solution_link_count":0.0,
        "Solution_readability":10.5,
        "Solution_reading_time":13.09,
        "Solution_score_count":7.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":196.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1505653015243,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1128.0,
        "Answerer_view_count":71.0,
        "Challenge_adjusted_solved_time":0.1140905556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to export a machine learning model I created in Azure Machine Learning studio. One of the required input is \"Path to blob beginning with container\"<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/xvSDd.png\" alt=\"Here is the screenshoot from azure export\"><\/p>\n\n<p>How do I find this path? I have already created a blob storage but I have no idea how to find the path to the blob storage. <\/p>",
        "Challenge_closed_time":1538035308943,
        "Challenge_comment_count":0,
        "Challenge_created_time":1538034898217,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to export a machine learning model from Azure Machine Learning studio and needs to provide the \"Path to blob beginning with container\" as input. However, the user is unsure how to find this path even though they have already created a blob storage.",
        "Challenge_last_edit_time":1538038508827,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52532078",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":6.4,
        "Challenge_reading_time":5.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.1140905556,
        "Challenge_title":"How to find the path to blob?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":15096.0,
        "Challenge_word_count":70,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>you should be able to find this from the Azure portal. Open the storage account, drill down into blobs, then your container. Use properties for the context menu, the URL should be the path ?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/r5hxi.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/r5hxi.jpg\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.3,
        "Solution_reading_time":4.7,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":44.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1565022238448,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":370.0,
        "Answerer_view_count":18.0,
        "Challenge_adjusted_solved_time":6.0927694445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a simple RegisterModel.py script that uses the Azure ML Service SDK to register a fastText .bin model. This completes successfully and I can see the model in the Azure Portal UI (I cannot see what model files are in it). I then want to download the model (DownloadModel.py) and use it (for testing purposes), however it throws an error on the <strong>model.download<\/strong> method (<em>tarfile.ReadError: file could not be opened successfully<\/em>) and makes a 0 byte rjtestmodel8.tar.gz file.<\/p>\n\n<p>I then use the Azure Portal and Add Model and select the same bin model file and it uploads fine. Downloading it with the download.py script below works fine, so I am assuming something is not correct with the Register script.<\/p>\n\n<p>Here are the 2 scripts and the stacktrace - let me know if you can see anything wrong:<\/p>\n\n<p><strong>RegisterModel.py<\/strong><\/p>\n\n<pre><code>import azureml.core\nfrom azureml.core import Workspace, Model\nws = Workspace.from_config()\nmodel = Model.register(workspace=ws,\n                       model_name='rjSDKmodel10',\n                       model_path='riskModel.bin')\n<\/code><\/pre>\n\n<p><strong>DownloadModel.py<\/strong><\/p>\n\n<pre><code># Works when downloading the UI Uploaded .bin file, but not the SDK registered .bin file\nimport os\nimport azureml.core\nfrom azureml.core import Workspace, Model\n\nws = Workspace.from_config()\nmodel = Model(workspace=ws, name='rjSDKmodel10')\nmodel.download(target_dir=os.getcwd(), exist_ok=True)\n<\/code><\/pre>\n\n<p><strong>Stacktrace<\/strong><\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"...\\.vscode\\extensions\\ms-python.python-2019.9.34474\\pythonFiles\\ptvsd_launcher.py\", line 43, in &lt;module&gt;\n    main(ptvsdArgs)\n  File \"...\\.vscode\\extensions\\ms-python.python-2019.9.34474\\pythonFiles\\lib\\python\\ptvsd\\__main__.py\", line 432, in main\n    run()\n  File \"...\\.vscode\\extensions\\ms-python.python-2019.9.34474\\pythonFiles\\lib\\python\\ptvsd\\__main__.py\", line 316, in run_file\n    runpy.run_path(target, run_name='__main__')\n  File \"...\\.conda\\envs\\DoC\\lib\\runpy.py\", line 263, in run_path\n    pkg_name=pkg_name, script_name=fname)\n  File \"...\\.conda\\envs\\DoC\\lib\\runpy.py\", line 96, in _run_module_code\n    mod_name, mod_spec, pkg_name, script_name)\n  File \"...\\.conda\\envs\\DoC\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"...\\\\DownloadModel.py\", line 21, in &lt;module&gt;\n    model.download(target_dir=os.getcwd(), exist_ok=True)\n  File \"...\\.conda\\envs\\DoC\\lib\\site-packages\\azureml\\core\\model.py\", line 712, in download\n    file_paths = self._download_model_files(sas_to_relative_download_path, target_dir, exist_ok)\n  File \"...\\.conda\\envs\\DoC\\lib\\site-packages\\azureml\\core\\model.py\", line 658, in _download_model_files\n    file_paths = self._handle_packed_model_file(tar_path, target_dir, exist_ok)\n  File \"...\\.conda\\envs\\DoC\\lib\\site-packages\\azureml\\core\\model.py\", line 670, in _handle_packed_model_file\n    with tarfile.open(tar_path) as tar:\n  File \"...\\.conda\\envs\\DoC\\lib\\tarfile.py\", line 1578, in open\n    raise ReadError(\"file could not be opened successfully\")\ntarfile.ReadError: file could not be opened successfully\n<\/code><\/pre>\n\n<p><strong>Environment<\/strong><\/p>\n\n<ul>\n<li>riskModel.bin is 6 megs<\/li>\n<li>AMLS 1.0.60<\/li>\n<li>Python 3.7<\/li>\n<li>Working locally with Visual Code<\/li>\n<\/ul>",
        "Challenge_closed_time":1568054240467,
        "Challenge_comment_count":2,
        "Challenge_created_time":1568032306497,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while registering and downloading a fastText .bin model with Azure Machine Learning Service. The RegisterModel.py script completes successfully and the model is visible in the Azure Portal UI, but the DownloadModel.py script throws an error on the model.download method and creates a 0 byte rjtestmodel8.tar.gz file. The user has provided the scripts and stacktrace and is seeking help to identify the issue.",
        "Challenge_last_edit_time":1568060910630,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57854136",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":12.0,
        "Challenge_reading_time":43.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":46,
        "Challenge_solved_time":6.0927694445,
        "Challenge_title":"Registering and downloading a fastText .bin model fails with Azure Machine Learning Service",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":281.0,
        "Challenge_word_count":327,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1256089885500,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sydney, Australia",
        "Poster_reputation_count":4947.0,
        "Poster_view_count":531.0,
        "Solution_body":"<p>The Azure Machine Learning service SDK has a bug with how it interacts with Azure Storage, which causes it to upload corrupted files if it has to retry uploading. <\/p>\n\n<p>A couple workarounds:<\/p>\n\n<ol>\n<li>The bug was introduced in 1.0.60 release. If you downgrade to AzureML-SDK 1.0.55, the code should fail when there are issue uploading instead of silently corrupting data.<\/li>\n<li>It's possible that the retry is being triggered by the low timeout values that the AzureML-SDK defaults to. You could investigate changing the timeout in <code>site-packages\/azureml\/_restclient\/artifacts_client.py<\/code><\/li>\n<\/ol>\n\n<p>This bug should be fixed in the next release of the AzureML-SDK.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1568054711300,
        "Solution_link_count":0.0,
        "Solution_readability":8.9,
        "Solution_reading_time":8.71,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":100.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1480786532470,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Nagpur, Sitabuldi, Nagpur, Maharashtra, India",
        "Answerer_reputation_count":111.0,
        "Answerer_view_count":44.0,
        "Challenge_adjusted_solved_time":2013.1323233333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>This is the first time I am using amazon web services to deploy my machine learning pre-trained model. I want to deploy my pre-trained TensorFlow model to Aws-Sagemaker. I am somehow able to deploy the endpoints successfully But whenever I call the <code>predictor.predict(some_data)<\/code> method to make prediction to invoking the endpoints it's throwing an error.<\/p>\n\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"\". See https:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/sagemaker-tensorflow-2020-04-07-04-25-27-055 in account 453101909370 for more information.\n<\/code><\/pre>\n\n<p>After going through the cloud watch logs I found this error.<\/p>\n\n<pre><code>#011details = \"NodeDef mentions attr 'explicit_paddings' not in Op&lt;name=Conv2D; signature=input:T, filter:T -&gt; output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]; attr=dilations:list(int),default=[1, 1, 1, 1]&gt;; NodeDef: {{node conv1_conv\/convolution}} = Conv2D[T=DT_FLOAT, _output_shapes=[[?,112,112,64]], data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"\/job:localhost\/replica:0\/task:0\/device:CPU:0\"](conv1_pad\/Pad, conv1_conv\/kernel\/read). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\n<\/code><\/pre>\n\n<p>I don't know where I am wrong and I have wasted 2 days already to solve this error and couldn't find out the information regarding this. The detailed logs I have shared <a href=\"https:\/\/docs.google.com\/document\/d\/1NXsLRd6cfbNE55xSVq5d63ETt-cBmwZsOaic8IyF1Qw\/edit?usp=sharing\" rel=\"nofollow noreferrer\">here<\/a>. <\/p>\n\n<p>Tensorflow version of my notebook instance is 1.15<\/p>",
        "Challenge_closed_time":1593503184160,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586244384423,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to deploy a pre-trained TensorFlow model on AWS Sagemaker. The endpoints are deployed successfully, but when the user tries to make predictions by calling the predictor.predict() method, it throws a ModelError with a server error message. The user has checked the cloud watch logs and found an error related to the Conv2D operation. The user is unsure about the cause of the error and has been unable to find a solution after spending two days on it. The TensorFlow version used by the user's notebook instance is 1.15.",
        "Challenge_last_edit_time":1586255907796,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61074798",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":15.9,
        "Challenge_reading_time":29.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":2016.3332602778,
        "Challenge_title":"Deploy pre-trained tensorflow model on the aws sagemaker - ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":740.0,
        "Challenge_word_count":214,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1480786532470,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Nagpur, Sitabuldi, Nagpur, Maharashtra, India",
        "Poster_reputation_count":111.0,
        "Poster_view_count":44.0,
        "Solution_body":"<p>After a lot of searching and try &amp; error, I was able to solve this problem. In many cases, the problem arises because of the TensorFlow and Python versions.<\/p>\n<p><strong>Cause of the problem:<\/strong>\nTo deploy the endpoints, I was using the <code>TensorflowModel<\/code> on TF 1.12 and python 3 and which exactly caused the problem.<\/p>\n<blockquote>\n<pre><code>sagemaker_model = TensorFlowModel(model_data = model_data,\n                                  role = role,\n                                  framework_version = '1.12',\n                                  entry_point = 'train.py')\n<\/code><\/pre>\n<\/blockquote>\n<p>Apparently, <code>TensorFlowModel<\/code> only allows python 2 on TF version 1.11, 1.12. 2.1.0.<\/p>\n<p><strong>How I fixed it:<\/strong> There are two TensorFlow solutions that handle serving in the Python SDK. They have different class representations and documentation as shown here.<\/p>\n<ol>\n<li><strong>TensorFlowModel<\/strong> - <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/model.py#L47\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/model.py#L47<\/a><\/li>\n<\/ol>\n<ul>\n<li>Doc:\n<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/tree\/v1.12.0\/src\/sagemaker\/tensorflow#deploying-directly-from-model-artifacts\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/tree\/v1.12.0\/src\/sagemaker\/tensorflow#deploying-directly-from-model-artifacts<\/a><\/li>\n<li>Key difference: Uses a proxy GRPC client to send requests<\/li>\n<li>Container impl:\n<a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/blob\/master\/src\/tf_container\/serve.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/blob\/master\/src\/tf_container\/serve.py<\/a><\/li>\n<\/ul>\n<ol start=\"2\">\n<li><strong>Model<\/strong> - <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/serving.py#L96\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/serving.py#L96<\/a><\/li>\n<\/ol>\n<ul>\n<li>Doc: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst<\/a><\/li>\n<li>Key difference: Utilizes the TensorFlow serving rest API<\/li>\n<li>Container impl: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/blob\/master\/container\/sagemaker\/serve.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/blob\/master\/container\/sagemaker\/serve.py<\/a><\/li>\n<\/ul>\n<p>Python 3 isn't supported using the <code>TensorFlowModel<\/code> object, as the container uses the TensorFlow serving API library in conjunction with the GRPC client to handle making inferences, however, the TensorFlow serving API isn't supported in Python 3 officially, so there are only Python 2 versions of the containers when using the <code>TensorFlowModel<\/code> object.\nIf you need Python 3 then you will need to use the <code>Model<\/code> object defined in #2 above.<\/p>\n<p>Finally, I used the <code>Model<\/code> with the TensorFlow version 1.15.1.<\/p>\n<blockquote>\n<pre><code>sagemaker_model = Model(model_data = model_data,\n                        role = role,\n                        framework_version='1.15.2',\n                        entry_point = 'train.py')\n<\/code><\/pre>\n<\/blockquote>\n<p>Also, here are the successful results.\n<a href=\"https:\/\/i.stack.imgur.com\/OMsEf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/OMsEf.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":14.0,
        "Solution_readability":21.1,
        "Solution_reading_time":48.42,
        "Solution_score_count":4.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":271.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":214.3601405556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,    <\/p>\n<p>I am running an AutoML experiment for a regression task, and looking at the YAML file which is generated it seems that TensorFlowLinearRegressor and TensorFlowDNN models are listed as both 'supported_models' and 'blacklist_algos'.    <\/p>\n<p>I tried to deactivate the automatic blacklisting of models by specifying the parameter 'auto_blacklist' to False, and 'blacklist_models' and 'blacklist_algos' parameters to Null, but it doesn't change anything.    <\/p>\n<pre><code>automl_settings = {  \n    &quot;primary_metric&quot;: 'normalized_mean_absolute_error',  \n    &quot;featurization&quot;: 'auto',  \n    &quot;verbosity&quot;: logging.INFO,  \n    &quot;n_cross_validations&quot;: 5,  \n    &quot;auto_blacklist&quot;: False,  \n    &quot;blacklist_models&quot;: None,  \n    &quot;blacklist_algos&quot;: None  \n}  \nrun = experiment.submit(automl_config, show_output=True)  \n<\/code><\/pre>\n<p>The generated YAML file (excerpt):    <\/p>\n<pre><code>&quot;whitelist_models&quot;:null,  \n&quot;blacklist_algos&quot;:[&quot;TensorFlowDNN&quot;,&quot;TensorFlowLinearRegressor&quot;],  \n&quot;supported_models&quot;:[&quot;ElasticNet&quot;,&quot;GradientBoosting&quot;,&quot;LightGBM&quot;,&quot;TensorFlowLinearRegressor&quot;,&quot;TensorFlowDNN&quot;,&quot;LassoLars&quot;,&quot;DecisionTree&quot;,&quot;RandomForest&quot;,&quot;FastLinearRegressor&quot;,&quot;OnlineGradientDescentRegressor&quot;,&quot;ExtremeRandomTrees&quot;,&quot;TabnetRegressor&quot;,&quot;XGBoostRegressor&quot;,&quot;KNN&quot;,&quot;SGD&quot;],  \n&quot;private_models&quot;:[],  \n&quot;auto_blacklist&quot;:false  \n<\/code><\/pre>\n<p>Maybe the problem comes from the fact that Deep learning is set to 'Disabled' in the configuration settings, as shown on the following picture:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/205364-image.png?platform=QnA\" alt=\"205364-image.png\" \/>    <\/p>\n<p>Are deep learning models not supported anymore by AutoML?    <\/p>",
        "Challenge_closed_time":1654236226443,
        "Challenge_comment_count":3,
        "Challenge_created_time":1653464529937,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is running an AutoML experiment for a regression task and noticed that TensorFlowLinearRegressor and TensorFlowDNN models are listed as both 'supported_models' and 'blacklist_algos' in the generated YAML file. The user tried to deactivate the automatic blacklisting of models by specifying the parameter 'auto_blacklist' to False, and 'blacklist_models' and 'blacklist_algos' parameters to Null, but it didn't work. The user wonders if deep learning models are not supported anymore by AutoML as Deep learning is set to 'Disabled' in the configuration settings.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/863297\/automl-tensorflowdnn-and-tensorflowlinearregressor",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":26.5,
        "Challenge_reading_time":26.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":214.3601405556,
        "Challenge_title":"AutoML : TensorFlowDNN and TensorFlowLinearRegressor are blacklisted by default",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":142,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@ThierryL-3166  Thanks for the question.     <\/p>\n<p>As mentioned in the below document The following support models in AutoML TensorFlowDNN, TensorFlowLinearRegressor are deprecated.     <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-automl-core\/azureml.automl.core.shared.constants.supportedmodels.regression?view=azure-ml-py\">https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-automl-core\/azureml.automl.core.shared.constants.supportedmodels.regression?view=azure-ml-py<\/a><\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":32.5,
        "Solution_reading_time":6.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    }
]