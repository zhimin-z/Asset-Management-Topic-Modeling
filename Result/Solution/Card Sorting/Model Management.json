[
    {
        "Answerer_created_time":1411546424280,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Edinburgh",
        "Answerer_reputation_count":58.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed a the universal_sentence_encoder_large_3 to an aws sagemaker.  When I am attempting to predict with the deployed model I get <code>Failed precondition: Table not initialized.<\/code> as an error. I have included the part where I save my model below:<\/p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\ndef tfhub_to_savedmodel(model_name, export_path):\n\n    model_path = '{}\/{}\/00000001'.format(export_path, model_name)\n    tfhub_uri = 'http:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/3'\n\n    with tf.Session() as sess:\n        module = hub.Module(tfhub_uri)\n        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n        input_params = module.get_input_info_dict()\n        dtype = input_params['text'].dtype\n        shape = input_params['text'].get_shape()\n\n        # define the model inputs\n        inputs = {'text': tf.placeholder(dtype, shape, 'text')}\n        output = module(inputs['text'])\n        outputs = {\n            'vector': output,\n        }\n\n        # export the model\n        tf.saved_model.simple_save(\n            sess,\n            model_path,\n            inputs=inputs,\n            outputs=outputs)  \n\n    return model_path\n<\/code><\/pre>\n\n<p>I have seen other people ask this problem but no solution has been ever posted.  It seems to be a common problem with tensorflow_hub sentence encoders<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1563993432753,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57189292",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":17.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"Failed precondition: Table not initialized. on deployed universal sentence encoder from aws sagemaker",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":365.0,
        "Challenge_word_count":138,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1531840489147,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berkeley, CA, USA",
        "Poster_reputation_count":425.0,
        "Poster_view_count":92.0,
        "Solution_body":"<p>I was running into this exact issue earlier this week while trying to modify this example <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_serving_container\/tensorflow_serving_container.ipynb\" rel=\"nofollow noreferrer\">Sagemaker notebook<\/a>. Particularly the part where serving the model. That is, running <code>predictor.predict()<\/code> on the Sagemaker Tensorflow Estimator.<\/p>\n\n<p>The solution outlined in the issue worked perfectly for me- <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/issues\/773#issuecomment-509433290\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/issues\/773#issuecomment-509433290<\/a><\/p>\n\n<p>I think it's just because <code>tf.tables_initializer()<\/code> only runs for training but it needs to be specified through the <code>legacy_init_op<\/code> if you want to run it during prediction.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":18.5,
        "Solution_reading_time":12.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":78.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi, I've made a model and it's predicting prices of cars. hooray! I cannot find the the equation for Azure's Regression Linear model anywhere. I made this model using Designer GUI. For example, in R, the coefficients are returned by running summary(mymodel)  <br \/>\n= y-intercept + (slope * miles) + (slope * year)  <br \/>\n= 21022.96 + (-0.0249*98500) + (-6.5668*2016)  <br \/>\nsomething like this equation for a line is what I'm looking for in Azure.<\/p>\n<p>what I've tried:  <\/p>\n<ol>\n<li> If it was only 1 feature, I could solve for an equation using (y2-y1) \/ (miles2-miles1) to find slope and the solve to y intercept. But this model uses miles and year as variables.<\/li>\n<\/ol>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1626236136657,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/474924\/where-to-find-the-equation-for-the-line-after-maki",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.1,
        "Challenge_reading_time":9.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"where to find the equation for the line after making Azure ML linear regression model, 2 slopes and 1 y intercept",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":127,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@<a href=\"\/users\/na\/?userid=dfab4fce-bbe8-4d93-a6fe-317f22fb2756\">@MikeRichardson-3493  <\/a> Thanks, We currently do not have coefficients for regression models, but we will forward this with our data science team to check on this. We are working on an interface to surface models that compose ensembles, model weights and more. While not  as involved of an interface, some of this information is available today within the model details tags sections:.     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/114674-a.png?platform=QnA\" alt=\"114674-a.png\" \/>    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.3,
        "Solution_reading_time":7.54,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":68.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1440734188430,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"India",
        "Answerer_reputation_count":1491.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a custom algorithm for text prediction. I want to deploy that in sagemaker. I am following this tutorial.\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/tf-example1.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/tf-example1.html<\/a>\n<br>\nThe only change from the tutorial is.<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\niris_estimator = TensorFlow(entry_point='\/home\/ec2-user\/SageMaker\/sagemaker.py',\n                        role=role,\n                        output_path=model_artifacts_location,\n                        code_location=custom_code_upload_location,\n                        train_instance_count=1,\n                        train_instance_type='ml.c4.xlarge',\n                        training_steps=1000,\n                        evaluation_steps=100, source_dir=\".\/\", requirements_file=\"requirements.txt\")\n<\/code><\/pre>\n\n<p>.<\/p>\n\n<pre><code>%%time\nimport boto3\n\ntrain_data_location = 's3:\/\/sagemaker-&lt;my bucket&gt;'\n\niris_estimator.fit(train_data_location)\n<\/code><\/pre>\n\n<p>INFO: the dataset is at the root of the bucket.<\/p>\n\n<p>error log<\/p>\n\n<pre><code>ValueError: Error training sagemaker-tensorflow-2018-06-19-07-11-13-634: Failed Reason: AlgorithmError: uncaught exception during training: Import by filename is not supported.\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/training.py\", line 36, in start\n    fw.train()\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/tf_container\/train_entry_point.py\", line 143, in train\n    customer_script = env.import_user_module()\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/environment.py\", line 101, in import_user_module\n    user_module = importlib.import_module(script)\n  File \"\/usr\/lib\/python2.7\/importlib\/__init__.py\", line 37, in import_module\n    __import__(name)\nImportError: Import by filename is not supported.\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1529399160327,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50924494",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":19.9,
        "Challenge_reading_time":24.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker ImportError: Import by filename is not supported",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":470.0,
        "Challenge_word_count":136,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1440734188430,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":1491.0,
        "Poster_view_count":112.0,
        "Solution_body":"<p>I solved this issue, The problem was using absolute path for <code>entry_point<\/code>. \n<br>\nwhen you use a <code>source_dir<\/code> parameter the path to the <code>entry_point<\/code> should be relative to the <code>source_dir<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.1,
        "Solution_reading_time":3.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":30.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to open a pickled XGBoost model I created in AWS Sagemaker to look at feature importances in the model. I'm trying to follow the answers in <a href=\"https:\/\/stackoverflow.com\/questions\/55621967\/feature-importance-for-xgboost-in-sagemaker\">this post<\/a>. However, I get an the error shown below. When I try to call <code>Booster.save_model<\/code>, I get an error saying <code>'Estimator' object has no attribute 'save_model'<\/code>. How can I resolve this? <\/p>\n\n<pre><code># Build initial model\nsess = sagemaker.Session()\ns3_input_train = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/train\/'.format(bucket, prefix), content_type='csv')\nxgb_cont = get_image_uri(region, 'xgboost', repo_version='0.90-1')\nxgb = sagemaker.estimator.Estimator(xgb_cont, role, train_instance_count=1, train_instance_type='ml.m4.4xlarge',\n                                    output_path='s3:\/\/{}\/{}'.format(bucket, prefix), sagemaker_session=sess)\nxgb.set_hyperparameters(eval_metric='rmse', objective='reg:squarederror', num_round=100)\nts = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nxgb_name = 'xgb-initial-' + ts\nxgb.set_hyperparameters(eta=0.1, alpha=0.5, max_depth=10)\nxgb.fit({'train': s3_input_train}, job_name=xgb_name)\n\n# Load model to get feature importances\nmodel_path = 's3:\/\/{}\/{}\/\/output\/model.tar.gz'.format(bucket, prefix, xgb_name)\nfs = s3fs.S3FileSystem()\nwith fs.open(model_path, 'rb') as f:\n    with tarfile.open(fileobj=f, mode='r') as tar_f:\n        with tar_f.extractfile('xgboost-model') as extracted_f:\n            model = pickle.load(extracted_f)\n\nXGBoostError: [19:16:42] \/workspace\/src\/learner.cc:682: Check failed: header == serialisation_header_: \n\n  If you are loading a serialized model (like pickle in Python) generated by older\n  XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version.  There's a simple script for helping\n  the process. See:\n\n    https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html\n\n  for reference to the script, and more details about differences between saving model and\n  serializing.\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1583954281893,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1583958473892,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60643094",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":14.6,
        "Challenge_reading_time":27.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":null,
        "Challenge_title":"Unable to open pickled Sagemaker XGBoost model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":4586.0,
        "Challenge_word_count":199,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1431970105067,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":4631.0,
        "Poster_view_count":333.0,
        "Solution_body":"<p>Which version of XGBoost are you using in the notebook? The model format has changed in XGBoost 1.0. See <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html\" rel=\"nofollow noreferrer\">https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html<\/a>. Short version: if you're using 1.0 in the notebook, you can't load a pickled model. <\/p>\n\n<p>Here's a working example using XGBoost in script mode (which is much more flexible than the built in algo):<\/p>\n\n<ul>\n<li><a href=\"https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/-\/blob\/master\/sagemaker\/09-XGBoost-script-mode.ipynb\" rel=\"nofollow noreferrer\">https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/-\/blob\/master\/sagemaker\/09-XGBoost-script-mode.ipynb<\/a><\/li>\n<li><a href=\"https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/-\/blob\/master\/sagemaker\/xgb.py\" rel=\"nofollow noreferrer\">https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/-\/blob\/master\/sagemaker\/xgb.py<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":21.3,
        "Solution_reading_time":12.81,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":68.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554860971800,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tokyo, Japan",
        "Answerer_reputation_count":301.0,
        "Answerer_view_count":65.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I would like to get the best model to use later in the notebook to predict using a different test batch.<\/p>\n\n<p>reproducible example (taken from Optuna Github) :<\/p>\n\n<pre><code>import lightgbm as lgb\nimport numpy as np\nimport sklearn.datasets\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\n\nimport optuna\n\n\n# FYI: Objective functions can take additional arguments\n# (https:\/\/optuna.readthedocs.io\/en\/stable\/faq.html#objective-func-additional-args).\ndef objective(trial):\n    data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)\n    train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)\n    dtrain = lgb.Dataset(train_x, label=train_y)\n    dvalid = lgb.Dataset(valid_x, label=valid_y)\n\n    param = {\n        \"objective\": \"binary\",\n        \"metric\": \"auc\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n        \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n    }\n\n    # Add a callback for pruning.\n    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"auc\")\n    gbm = lgb.train(\n        param, dtrain, valid_sets=[dvalid], verbose_eval=False, callbacks=[pruning_callback]\n    )\n\n    preds = gbm.predict(valid_x)\n    pred_labels = np.rint(preds)\n    accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n    return accuracy\n\n<\/code><\/pre>\n\n<p>my understanding is that the study below will tune for accuracy. I would like to somehow retrieve the best model from the study (not just the parameters) without saving it as a pickle, I just want to use the model somewhere else in my notebook. <\/p>\n\n<pre><code>\nif __name__ == \"__main__\":\n    study = optuna.create_study(\n        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=\"maximize\"\n    )\n    study.optimize(objective, n_trials=100)\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n\n<\/code><\/pre>\n\n<p>desired output would be <\/p>\n\n<pre><code>best_model = ~model from above~\nnew_target_pred = best_model.predict(new_data_test)\nmetrics.accuracy_score(new_target_test, new__target_pred)\n\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591072505577,
        "Challenge_favorite_count":9.0,
        "Challenge_last_edit_time":1591072893872,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62144904",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":13.2,
        "Challenge_reading_time":33.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":14.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":null,
        "Challenge_title":"Python: How to retrive the best model from Optuna LightGBM study?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":8921.0,
        "Challenge_word_count":229,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1529932143432,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Melbourne VIC, Australia",
        "Poster_reputation_count":525.0,
        "Poster_view_count":54.0,
        "Solution_body":"<p>I think you can use the <code>callback<\/code> argument of <a href=\"https:\/\/optuna.readthedocs.io\/en\/latest\/reference\/study.html#optuna.study.Study.optimize\" rel=\"noreferrer\"><code>Study.optimize<\/code><\/a> to save the best model. In the following code example, the callback checks if a given trial is corresponding to the best trial and saves the model as a global variable <code>best_booster<\/code>.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>best_booster = None\ngbm = None\n\ndef objective(trial):\n    global gbm\n    # ...\n\ndef callback(study, trial):\n    global best_booster\n    if study.best_trial == trial:\n        best_booster = gbm\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(\n        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=\"maximize\"\n    )\n    study.optimize(objective, n_trials=100, callbacks=[callback])\n\n<\/code><\/pre>\n\n<p>If you define your objective function as a class, you can remove the global variables. I created a notebook as a code example. Please take a look at it:\n<a href=\"https:\/\/colab.research.google.com\/drive\/1ssjXp74bJ8bCAbvXFOC4EIycBto_ONp_?usp=sharing\" rel=\"noreferrer\">https:\/\/colab.research.google.com\/drive\/1ssjXp74bJ8bCAbvXFOC4EIycBto_ONp_?usp=sharing<\/a><\/p>\n\n<blockquote>\n  <p>I would like to somehow retrieve the best model from the study (not just the parameters) without saving it as a pickle<\/p>\n<\/blockquote>\n\n<p>FYI, if you can pickle the boosters, I think you can make the code simple by following <a href=\"https:\/\/optuna.readthedocs.io\/en\/latest\/faq.html#how-to-save-machine-learning-models-trained-in-objective-functions\" rel=\"noreferrer\">this FAQ<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":15.3,
        "Solution_reading_time":21.14,
        "Solution_score_count":8.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":152.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1627764232887,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained a SageMaker semantic segmentation model, using the built-in sagemaker semantic segmentation algorithm. This deploys ok to a SageMaker endpoint and I can run inference in the cloud  successfully from it.\nI would like to use the model on a edge device (AWS Panorama Appliance) which should just mean compiling the model with SageMaker Neo to the specifications of the target device.<\/p>\n<p>However, regardless of what my target device is (the Neo settings), I cant seem to compile the model with Neo as I get the following error:<\/p>\n<pre><code>ClientError: InputConfiguration: No valid Mxnet model file -symbol.json found\n<\/code><\/pre>\n<p>The model.tar.gz for semantic segmentation models contains hyperparams.json, model_algo-1, model_best.params. According to the docs, model_algo-1 is the serialized mxnet model. Aren't gluon models supported by Neo?<\/p>\n<p>Incidentally I encountered the exact same problem with another SageMaker built-in algorithm, the k-Nearest Neighbour (k-NN). It too seems to be compiled without a -symbol.json.<\/p>\n<p>Is there some scripts I can run to recreated a -symbol.json file or convert the compiled sagemaker model?<\/p>\n<p>After building my model with an Estimator, I got to compile it in SageMaker Neo with code:<\/p>\n<pre><code>optimized_ic = my_estimator.compile_model(\n target_instance_family=&quot;ml_c5&quot;,\n target_platform_os=&quot;LINUX&quot;,\n target_platform_arch=&quot;ARM64&quot;,\n input_shape={&quot;data&quot;: [1,3,512,512]},  \n output_path=s3_optimized_output_location,\n framework=&quot;mxnet&quot;,\n framework_version=&quot;1.8&quot;, \n)\n<\/code><\/pre>\n<p>I would expect this to compile ok, but that is where I get the error saying the model is missing the *-symbol.json file.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647989575803,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1648020944223,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71579883",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":23.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"Missing -symbol.json error when trying to compile a SageMaker semantic segmentation model (built-in algorithm) with SageMaker Neo",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":52.0,
        "Challenge_word_count":236,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1586928819952,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":25.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>For some reason, AWS has decided to not make its built-in algorithms directly compatible with Neo... However, you can re-engineer the network parameters using the model.tar.gz output file and then compile.<\/p>\n<p>Step 1: Extract model from tar file<\/p>\n<pre><code>import tarfile\n#path to local tar file\nmodel = 'ss_model.tar.gz'\n\n#extract tar file \nt = tarfile.open(model, 'r:gz')\nt.extractall()\n<\/code><\/pre>\n<p>This should output two files:\nmodel_algo-1, model_best.params<\/p>\n<ol start=\"2\">\n<li>Load weights into network from gluon model zoo for the architecture that you chose<\/li>\n<\/ol>\n<p>In this case I used DeepLabv3 with resnet50<\/p>\n<pre><code>import gluoncv\nimport mxnet as mx\nfrom gluoncv import model_zoo\nfrom gluoncv.data.transforms.presets.segmentation import test_transform\n\nmodel = model_zoo.DeepLabV3(nclass=2, backbone='resnet50', pretrained_base=False, height=800, width=1280, crop_size=240)\nmodel.load_parameters(&quot;model_algo-1&quot;)\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Check the parameters have loaded correctly by making a prediction with new model<\/li>\n<\/ol>\n<p>Use an image that was used for training.<\/p>\n<pre><code>#use cpu\nctx = mx.cpu(0)\n#decode image bytes of loaded file\nimg = image.imdecode(imbytes)\n\n#transform image\nimg = test_transform(img, ctx)\nimg = img.astype('float32')\nprint('tranformed image shape: ', img.shape)\n\n#get prediction\noutput = model.predict(img)\n<\/code><\/pre>\n<ol start=\"4\">\n<li>Hybridise model into output required by Sagemaker Neo<\/li>\n<\/ol>\n<p>Additional check for image shape compatibility<\/p>\n<pre><code>model.hybridize()\nmodel(mx.nd.ones((1,3,800,1280)))\nexport_block('deeplabv3-res50', model, data_shape=(3,800,1280), preprocess=None, layout='CHW')\n<\/code><\/pre>\n<ol start=\"5\">\n<li>Recompile model into tar.gz format<\/li>\n<\/ol>\n<p>This contains the params and json file which Neo looks for.<\/p>\n<pre><code>tar = tarfile.open(&quot;comp_model.tar.gz&quot;, &quot;w:gz&quot;)\nfor name in [&quot;deeplabv3-res50-0000.params&quot;, &quot;deeplabv3-res50-symbol.json&quot;]:\n    tar.add(name)\ntar.close()\n<\/code><\/pre>\n<ol start=\"6\">\n<li>Save tar.gz file to s3 and then compile using Neo GUI<\/li>\n<\/ol>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":28.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":231.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1351154914716,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2564.0,
        "Answerer_view_count":451.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I defined a training job:<\/p>\n<pre><code>job = aiplatform.AutoMLTextTrainingJob(...\n<\/code><\/pre>\n<p>then I created a model by running the job:<\/p>\n<pre><code>model = job.run(...\n<\/code><\/pre>\n<p>It worked fine but it is now the next day and the variable <code>model<\/code> was in a Jupyter notebook and no longer exists. I have tried to get it back with:<\/p>\n<pre><code>from google.cloud import aiplatform_v1beta1\n\ndef sample_get_model():\n    client = aiplatform_v1beta1.ModelServiceClient()\n\n    model_id=id_of_training_pipeline\n    name= f'projects\/{PROJECT}\/locations\/{REGION}\/models\/{model_id}'\n    \n    request = aiplatform_v1beta1.GetModelRequest(name=name)\n    response = client.get_model(request=request)\n    print(response)\n\nsample_get_model()\n<\/code><\/pre>\n<p>I have also tried the id of v1 of the model created in place of <code>id_of_training_pipeline<\/code> and I have tried <code>\/pipelines\/pipeline_id<\/code><\/p>\n<p>but I get:\n<code>E0805 15:12:36.784008212   28406 hpack_parser.cc:1234]       Error parsing metadata: error=invalid value key=content-type value=text\/html; charset=UTF-8<\/code><\/p>\n<p>(<code>PROJECT<\/code> and <code>REGION<\/code> are set correctly).<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659709186783,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73251212",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.4,
        "Challenge_reading_time":15.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"How do I retrieve a model in Vertex AI?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":45.0,
        "Challenge_word_count":119,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>Found <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/samples\/aiplatform-get-model-sample#aiplatform_get_model_sample-python\" rel=\"nofollow noreferrer\">this<\/a> Google code which works.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":31.0,
        "Solution_reading_time":2.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":9.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1501114346136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Australia",
        "Answerer_reputation_count":37.0,
        "Answerer_view_count":8.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I built a multiclass SVM model in R and used Create R model module from azure to train and predict my testing dataset. Here are the trainer and the score R scripts.<\/p>\n\n<p><strong>Trainer R script:<\/strong> <\/p>\n\n<pre><code>library(e1071)\nfeatures &lt;- get.feature.columns(dataset)\nlabels   &lt;- as.factor(get.label.column(dataset))\ntrain.data &lt;- data.frame(features, labels)\nfeature.names &lt;- get.feature.column.names(dataset)\nnames(train.data) &lt;- c(feature.names, \"Class\")\nmodel &lt;- svm(Class ~ . , train.data)\n<\/code><\/pre>\n\n<p><strong>Scores R script:<\/strong><\/p>\n\n<pre><code>library(e1071)    \nclasses &lt;- predict(model, dataset)\nclasses &lt;- as.factor(classes)\nres &lt;- data.frame(classes, probabilities = 0.5)\nprint(str(res))\nprint(res)\nscores &lt;- res\n<\/code><\/pre>\n\n<p>Note in my code, I hardcoded the probability values to simplify the code.<\/p>\n\n<p>Here is my component design in Azure: <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/hjMC4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hjMC4.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>When I run the experiment, all the components work fine. However, in the score model, the scored dataset port does not show the predicted values. It only shows feature values from the testing dataset. I checked the output log of <em>Score model<\/em> and I could see the model has nicely predicted the testing data (note I added print commands in the Scores R script). But this is not enough and I need the prediction returned from the score model so I can pass it via API.<\/p>\n\n<p>Has anyone faced this issue before?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533539693247,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1533560847647,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51702359",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":21.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":null,
        "Challenge_title":"In Azure ML Studio, score model doesn't return predicted values from an R model",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":657.0,
        "Challenge_word_count":216,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501114346136,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Australia",
        "Poster_reputation_count":37.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>I found an answer for this. In fact, I cannot see the result in the outcome of the scoring model but when I linked it to a <em>select column in the dataset<\/em> module, I see the predicted columns there.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.5,
        "Solution_reading_time":2.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<!-- \r\n### Common bugs:\r\n1. Tensorboard not showing in Jupyter-notebook see [issue 79](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/79).    \r\n2. PyTorch 1.1.0 vs 1.2.0 support [see FAQ](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning#faq)    \r\n-->\r\n\r\n## \ud83d\udc1b Bug\r\n\r\nWhen using the MLFlow logger with Hydra, because the parameters passed to the LightningModule is a `DictConfig`, the condition in the `logger\/base.py` is not met.\r\n\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/8211256c46430e43e0c27e4f078c72085bb4ea34\/pytorch_lightning\/loggers\/base.py#L177\r\n\r\n### To Reproduce\r\n\r\nUse Hydra and MLFlow together. \r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n```python\r\nTraceback (most recent call last):\r\n  File \"\/home\/siavash\/KroniKare\/kwae2\/kwae_ma\/models\/pl_train_segmentation_model.py\", line 115, in <module>\r\n    main()\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/main.py\", line 24, in decorated_main\r\n    strict=strict,\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/_internal\/utils.py\", line 174, in run_hydra\r\n    overrides=args.overrides,\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/_internal\/hydra.py\", line 86, in run\r\n    job_subdir_key=None,\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/plugins\/common\/utils.py\", line 109, in run_job\r\n    ret.return_value = task_function(task_cfg)\r\n  File \"\/home\/siavash\/KroniKare\/kwae2\/kwae_ma\/models\/pl_train_segmentation_model.py\", line 111, in main\r\n    trainer.fit(wound_seg_pl)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 765, in fit\r\n    self.single_gpu_train(model)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/distrib_parts.py\", line 492, in single_gpu_train\r\n    self.run_pretrain_routine(model)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 843, in run_pretrain_routine\r\n    self.logger.log_hyperparams(ref_model.hparams)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 275, in log_hyperparams\r\n    [logger.log_hyperparams(params) for logger in self._logger_iterable]\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 275, in <listcomp>\r\n    [logger.log_hyperparams(params) for logger in self._logger_iterable]\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 10, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/mlflow.py\", line 105, in log_hyperparams\r\n    self.experiment.log_param(self.run_id, k, v)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/mlflow\/tracking\/client.py\", line 206, in log_param\r\n    self._tracking_client.log_param(run_id, key, value)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 177, in log_param\r\n    _validate_param_name(key)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/mlflow\/utils\/validation.py\", line 120, in _validate_param_name\r\n    INVALID_PARAMETER_VALUE)\r\nmlflow.exceptions.MlflowException: Invalid parameter name: ''. Names may be treated as files in certain cases, and must not resolve to other names when treated as such. This name would resolve to '.'\r\n```\r\n\r\n### Expected behavior\r\n\r\nCheck whether the instance if `dict` or `DictConfig` in the given line. \r\n",
        "Challenge_closed_time":1592925.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591172197000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/2058",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":19.8,
        "Challenge_reading_time":50.14,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":null,
        "Challenge_title":"Hydra MLFlow Clash",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":248,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi! thanks for your contribution!, great first issue! > Check whether the instance if `dict` or `DictConfig` in the given line.\r\n\r\n@ssakhavi that sounds reasonable solution, mind sending a PR - fix and its test?",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.2,
        "Solution_reading_time":2.56,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":33.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1613062428296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":141.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>We are struggling to model our data correctly for use in Kedro - we are using the recommended Raw\\Int\\Prm\\Ft\\Mst model but are struggling with some of the concepts....e.g.<\/p>\n<ul>\n<li>When is a dataset a feature rather than a primary dataset? The distinction seems vague...<\/li>\n<li>Is it OK for a primary dataset to consume data from another primary dataset?<\/li>\n<li>Is it good practice to build a feature dataset from the INT layer? or should it always pass through Primary?<\/li>\n<\/ul>\n<p>I appreciate there are no hard &amp; fast rules with data modelling but these are big modelling decisions &amp; any guidance or best practice on Kedro modelling would be really helpful, I can find just one table defining the layers in the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/12_faq\/01_faq.html#what-is-data-engineering-convention\" rel=\"nofollow noreferrer\">Kedro docs<\/a><\/p>\n<p>If anyone can offer any further advice or blogs\\docs talking about Kedro Data Modelling that would be awesome!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623345871000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67925860",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.7,
        "Challenge_reading_time":12.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Kedro Data Modelling",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":180.0,
        "Challenge_word_count":145,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369054667740,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United Kingdom",
        "Poster_reputation_count":1445.0,
        "Poster_view_count":104.0,
        "Solution_body":"<p>Great question. As you say, there are no hard and fast rules here and opinions do vary, but let me share my perspective as a QB data scientist and kedro maintainer who has used the layering convention you referred to several times.<\/p>\n<p>For a start, let me emphasise that there's absolutely no reason to stick to the data engineering convention suggested by kedro if it's not suitable for your needs. 99% of users don't change the folder structure in <code>data<\/code>. This is not because the kedro default is the right structure for them but because they just don't think of changing it. You should absolutely add\/remove\/rename layers to suit yourself. The most important thing is to choose a set of layers (or even a non-layered structure) that works for your project rather than trying to shoehorn your datasets to fit the kedro default suggestion.<\/p>\n<p>Now, assuming you are following kedro's suggested structure - onto your questions:<\/p>\n<blockquote>\n<p>When is a dataset a feature rather than a primary dataset? The distinction seems vague...<\/p>\n<\/blockquote>\n<p>In the case of simple features, a feature dataset can be very similar to a primary one. The distinction is maybe clearest if you think about more complex features, e.g. formed by aggregating over time windows. A primary dataset would have a column that gives a cleaned version of the original data, but without doing any complex calculations on it, just simple transformations. Say the raw data is the colour of all cars driving past your house over a week. By the time the data is in primary, it will be clean (e.g. correcting &quot;rde&quot; to &quot;red&quot;, maybe mapping &quot;crimson&quot; and &quot;red&quot; to the same colour). Between primary and the feature layer, we will have done some less trivial calculations on it, e.g. to find one-hot encoded most common car colour each day.<\/p>\n<blockquote>\n<p>Is it OK for a primary dataset to consume data from another primary dataset?<\/p>\n<\/blockquote>\n<p>In my opinion, yes. This might be necessary if you want to join multiple primary tables together. In general if you are building complex pipelines it will become very difficult if you don't allow this. e.g. in the feature layer I might want to form a dataset containing <code>composite_feature = feature_1 * feature_2<\/code> from the two inputs <code>feature_1<\/code> and <code>feature_2<\/code>. There's no way of doing this without having multiple sub-layers within the feature layer.<\/p>\n<p>However, something that is generally worth avoiding is a node that consumes data from many different layers. e.g. a node that takes in one dataset from the feature layer and one from the intermediate layer. This seems a bit strange (why has the latter dataset not passed through the feature layer?).<\/p>\n<blockquote>\n<p>Is it good practice to build a feature dataset from the INT layer? or should it always pass through Primary?<\/p>\n<\/blockquote>\n<p>Building features from the intermediate layer isn't unheard of, but it seems a bit weird. The primary layer is typically an important one which forms the basis for all feature engineering. If your data is in a shape that you can build features then that means it's probably primary layer already. In this case, maybe you don't need an intermediate layer.<\/p>\n<p>The above points might be summarised by the following rules (which should no doubt be broken when required):<\/p>\n<ol>\n<li>The input datasets for a node in layer <code>L<\/code> should all be in the same layer, which can be either <code>L<\/code> or <code>L-1<\/code><\/li>\n<li>The output datasets for a node in layer <code>L<\/code> should all be in the same layer <code>L<\/code>, which can be either <code>L<\/code> or <code>L+1<\/code><\/li>\n<\/ol>\n<blockquote>\n<p>If anyone can offer any further advice or blogs\\docs talking about Kedro Data Modelling that would be awesome!<\/p>\n<\/blockquote>\n<p>I'm also interested in seeing what others think here! One possibly useful thing to note is that kedro was inspired by cookiecutter data science, and the kedro layer structure is an extended version of <a href=\"http:\/\/drivendata.github.io\/cookiecutter-data-science\/#directory-structure\" rel=\"nofollow noreferrer\">what's suggested there<\/a>. Maybe other projects have taken this directory structure and adapted it in different ways.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.2,
        "Solution_reading_time":53.65,
        "Solution_score_count":4.0,
        "Solution_sentence_count":37.0,
        "Solution_word_count":668.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1521856385820,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation_count":820.0,
        "Answerer_view_count":165.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to get the name of the experiment that contains the run that created a registered MLflow model. How can I do this using MLflow, if I just have the name of the model and the version?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658855429667,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73127303",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.7,
        "Challenge_reading_time":2.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Get Experiment that Created Model in MLflow",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":44.0,
        "Challenge_word_count":44,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521856385820,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sri Lanka",
        "Poster_reputation_count":820.0,
        "Poster_view_count":165.0,
        "Solution_body":"<p>As @Andre has said, I had to write my own function to achieve this,<\/p>\n<pre><code>def get_model_experiment(model_name, model_version):\n    # get run_id of the model version\n    run_id = mlflow_client.get_model_version(model_name, model_version).run_id\n\n    # get the experiment_id from the run_id\n    experiment_id = mlflow_client.get_run(run_id).info.experiment_id\n\n    # get the experiment name from the experiment_id\n    return mlflow_client.get_experiment(experiment_id).name\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":19.6,
        "Solution_reading_time":6.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":44.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there any restriction on registering an ML pickle model into Azure Machine Learning Service in terms of the size of the pickle file?  <\/p>\n<p>Does it cause latency in realtime data processing and getting the prediction results from the pickle file if we have a  model that let's say it 5MB and the other one is 500MB (The bigger file has better performance in terms of accuracy)?  <br \/>\nThanks,  <\/p>\n<p>John<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1599612419390,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/89630\/ml-pickle-file-size-azure-machine-learning-service",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.8,
        "Challenge_reading_time":5.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"ML Pickle file size Azure Machine Learning Service",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":79,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=ccc37df1-9b57-4e84-920a-f92d8316aa7c\">@JJA  <\/a> Thanks, For ACI we recommend not using a model over 1GB in size.    <br \/>\nFor AKS you are limited by the memory resources that you request for your service, minus about 500mb for the running python process in the pod.    <\/p>\n<p>There will be no difference in prediction speed once the model is successfully deployed.    <br \/>\nRegistering will take longer as we have to upload the model, and deploying will take longer as the service must download the model.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.2,
        "Solution_reading_time":6.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":83.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416648155470,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":14749.0,
        "Answerer_view_count":968.0,
        "Challenge_adjusted_solved_time":6386.0465466667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy a model trained with sklearn to an endpoint and serve it as an API for predictions. All I want to use sagemaker for, is to deploy and server model I had serialised using <code>joblib<\/code>, nothing more. every blog I have read and sagemaker python documentation showed that sklearn model had to be trained on sagemaker in order to be deployed in sagemaker.<\/p>\n<p>When I was going through the SageMaker documentation I learned that sagemaker does let users <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#load-a-model\" rel=\"nofollow noreferrer\">load a serialised model<\/a> stored in S3 as shown below:<\/p>\n<pre><code>def model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))\n    return clf\n<\/code><\/pre>\n<p>And this is what documentation says about the argument <code>model_dir<\/code>:<\/p>\n<blockquote>\n<p>SageMaker will inject the directory where your model files and\nsub-directories, saved by save, have been mounted. Your model function\nshould return a model object that can be used for model serving.<\/p>\n<\/blockquote>\n<p>This again means that training has to be done on sagemaker.<\/p>\n<p>So, is there a way I can just specify the S3 location of my serialised model and have sagemaker de-serialise(or load) the model from S3 and use it for inference?<\/p>\n<h2>EDIT 1:<\/h2>\n<p>I used code in the answer to my application and I got below error when trying to deploy from notebook of SageMaker studio. I believe SageMaker is screaming that training wasn't done on SageMaker.<\/p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-4-6662bbae6010&gt; in &lt;module&gt;\n      1 predictor = model.deploy(\n      2     initial_instance_count=1,\n----&gt; 3     instance_type='ml.m4.xlarge'\n      4 )\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in deploy(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, use_compiled_model, wait, model_name, kms_key, data_capture_config, tags, **kwargs)\n    770         &quot;&quot;&quot;\n    771         removed_kwargs(&quot;update_endpoint&quot;, kwargs)\n--&gt; 772         self._ensure_latest_training_job()\n    773         self._ensure_base_job_name()\n    774         default_name = name_from_base(self.base_job_name)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in _ensure_latest_training_job(self, error_message)\n   1128         &quot;&quot;&quot;\n   1129         if self.latest_training_job is None:\n-&gt; 1130             raise ValueError(error_message)\n   1131 \n   1132     delete_endpoint = removed_function(&quot;delete_endpoint&quot;)\n\nValueError: Estimator is not associated with a training job\n<\/code><\/pre>\n<p>My code:<\/p>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\n# from sagemaker.pytorch import PyTorchModel\nfrom sagemaker.sklearn import SKLearn\nfrom sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n\nsm_role = sagemaker.get_execution_role()  # IAM role to run SageMaker, access S3 and ECR\n\nmodel_file = &quot;s3:\/\/sagemaker-manual-bucket\/sm_model_artifacts\/model.tar.gz&quot;   # Must be &quot;.tar.gz&quot; suffix\n\nclass AnalysisClass(RealTimePredictor):\n    def __init__(self, endpoint_name, sagemaker_session):\n        super().__init__(\n            endpoint_name,\n            sagemaker_session=sagemaker_session,\n            serializer=json_serializer,\n            deserializer=json_deserializer,   # To be able to use JSON serialization\n            content_type='application\/json'   # To be able to send JSON as HTTP body\n        )\n\nmodel = SKLearn(model_data=model_file,\n                entry_point='inference.py',\n                name='rf_try_1',\n                role=sm_role,\n                source_dir='code',\n                framework_version='0.20.0',\n                instance_count=1,\n                instance_type='ml.m4.xlarge',\n                predictor_cls=AnalysisClass)\npredictor = model.deploy(initial_instance_count=1,\n                         instance_type='ml.m4.xlarge')\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607263332140,
        "Challenge_favorite_count":3.0,
        "Challenge_last_edit_time":1607399645212,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65168915",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":15.5,
        "Challenge_reading_time":51.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS SageMaker - How to load trained sklearn model to serve for inference?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3221.0,
        "Challenge_word_count":393,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559910246180,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":2046.0,
        "Poster_view_count":369.0,
        "Solution_body":"<p>Yes you can. AWS documentation focuses on end-to-end from training to deployment in SageMaker which makes the impression that training has to be done on sagemaker. AWS documentation and examples should have clear separation among Training in Estimator, Saving and loading model, and Deployment model to SageMaker Endpoint.<\/p>\n<h2>SageMaker Model<\/h2>\n<p>You need to create the <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-model.html\" rel=\"nofollow noreferrer\">AWS::SageMaker::Model<\/a> resource which refers to the &quot;model&quot; you have trained <strong>and more<\/strong>. AWS::SageMaker::Model is in CloudFormation document but it is only to explain what AWS resource you need.<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\" rel=\"nofollow noreferrer\">CreateModel<\/a> API creates a SageMaker model resource. The parameters specifie the docker image to use, model location in S3, IAM role to use, etc. See <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html#your-algorithms-inference-code-load-artifacts\" rel=\"nofollow noreferrer\">How SageMaker Loads Your Model Artifacts<\/a>.<\/p>\n<h3>Docker image<\/h3>\n<p>Obviously you need the framework e.g. ScikitLearn, TensorFlow, PyTorch, etc that you used to train your model to get inferences. You need a docker image that has the framework, and HTTP front end to respond to the prediction calls. See <a href=\"https:\/\/github.com\/aws\/sagemaker-inference-toolkit\" rel=\"nofollow noreferrer\">SageMaker Inference Toolkit<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/amazon-sagemaker-toolkits.html\" rel=\"nofollow noreferrer\">Using the SageMaker Training and Inference Toolkits<\/a>.<\/p>\n<p>To build the image is not easy. Hence AWS provides pre-built images called <a href=\"https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/deep-learning-containers-images.html\" rel=\"nofollow noreferrer\">AWS Deep Learning Containers<\/a> and available images are in <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">Github<\/a>.<\/p>\n<p>If your framework and the version is listed there, you can use it as the image. Otherwise you need to build by yourself. See <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-mlops-workshop\/blob\/master\/lab\/01_CreateAlgorithmContainer\/01_Creating%20a%20Classifier%20Container.ipynb\" rel=\"nofollow noreferrer\">Building a docker container for training\/deploying our classifier<\/a>.<\/p>\n<h2>SageMaker Python SDK for Frameworks<\/h2>\n<p>Create SageMaker Model by yourself using API is hard. Hence AWS SageMaker Python SDK has provided utilities to create the SageMaker models for several frameworks. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/index.html\" rel=\"nofollow noreferrer\">Frameworks<\/a> for available frameworks. If it is not there, you may still be able to use <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html#sagemaker.model.FrameworkModel\" rel=\"nofollow noreferrer\">sagemaker.model.FrameworkModel<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html\" rel=\"nofollow noreferrer\">Model<\/a> to load your trained model. For your case, see <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html\" rel=\"nofollow noreferrer\">Using Scikit-learn with the SageMaker Python SDK<\/a>.<\/p>\n<h3>model.tar.gz<\/h3>\n<p>For instance if you used PyTorch and save the model as model.pth. To load the model and the inference code to get the prediction from the model, you need to create a model.tar.gz file. The structure inside the model.tar.gz is explained in <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#model-directory-structure\" rel=\"nofollow noreferrer\">Model Directory Structure<\/a>. If you use Windows, beware of the CRLF to LF. AWS SageMaker runs in *NIX environment. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-the-directory-structure-for-your-model-files\" rel=\"nofollow noreferrer\">Create the directory structure for your model files<\/a>.<\/p>\n<pre><code>|- model.pth        # model file is inside \/ directory.\n|- code\/            # Code artefacts must be inside \/code\n  |- inference.py   # Your inference code for the framework\n  |- requirements.txt  # only for versions 1.3.1 and higher. Name must be &quot;requirements.txt&quot;\n<\/code><\/pre>\n<p>Save the tar.gz file in S3. Make sure of the IAM role to access the S3 bucket and objects.<\/p>\n<h3>Loading model and get inference<\/h3>\n<p>See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-a-pytorchmodel-object\" rel=\"nofollow noreferrer\">Create a PyTorchModel object<\/a>. When instantiating the PyTorchModel class, SageMaker automatically selects the AWS Deep Learning Container image for PyTorch for the version specified in <strong>framework_version<\/strong>. If the image for the version does not exist, then it fails. This has not been documented in AWS but need to be aware of. SageMaker then internally calls the CreateModel API with the S3 model file location and the AWS Deep Learning Container image URL.<\/p>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.pytorch import PyTorchModel\nfrom sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n\nrole = sagemaker.get_execution_role()  # IAM role to run SageMaker, access S3 and ECR\nmodel_file = &quot;s3:\/\/YOUR_BUCKET\/YOUR_FOLDER\/model.tar.gz&quot;   # Must be &quot;.tar.gz&quot; suffix\n\n\nclass AnalysisClass(RealTimePredictor):\n    def __init__(self, endpoint_name, sagemaker_session):\n        super().__init__(\n            endpoint_name,\n            sagemaker_session=sagemaker_session,\n            serializer=json_serializer,\n            deserializer=json_deserializer,   # To be able to use JSON serialization\n            content_type='application\/json'   # To be able to send JSON as HTTP body\n        )\n\nmodel = PyTorchModel(\n    model_data=model_file,\n    name='YOUR_MODEL_NAME_WHATEVER',\n    role=role,\n    entry_point='inference.py',\n    source_dir='code',              # Location of the inference code\n    framework_version='1.5.0',      # Availble AWS Deep Learning PyTorch container version must be specified\n    predictor_cls=AnalysisClass     # To specify the HTTP request body format (application\/json)\n)\n\npredictor = model.deploy(\n    initial_instance_count=1,\n    instance_type='ml.m5.xlarge'\n)\n\ntest_data = {&quot;body&quot;: &quot;YOUR PREDICTION REQUEST&quot;}\nprediction = predictor.predict(test_data)\n<\/code><\/pre>\n<p>By default, SageMaker uses NumPy as the serialization format. To be able to use JSON, need to specify the serializer and content_type. Instead of using RealTimePredictor class, you can specify them to predictor.<\/p>\n<pre><code>predictor.serializer=json_serializer\npredictor.predict(test_data)\n<\/code><\/pre>\n<p>Or<\/p>\n<pre><code>predictor.serializer=None # As the serializer is None, predictor won't serialize the data\nserialized_test_data=json.dumps(test_data) \npredictor.predict(serialized_test_data)\n<\/code><\/pre>\n<h3>Inference code sample<\/h3>\n<p>See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#process-model-input\" rel=\"nofollow noreferrer\">Process Model Input<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#get-predictions-from-a-pytorch-model\" rel=\"nofollow noreferrer\">Get Predictions from a PyTorch Model<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#process-model-output\" rel=\"nofollow noreferrer\">Process Model Output<\/a>. The prediction request is sent as JSON in HTTP request body in this example.<\/p>\n<pre><code>import os\nimport sys\nimport datetime\nimport json\nimport torch\nimport numpy as np\n\nCONTENT_TYPE_JSON = 'application\/json'\n\ndef model_fn(model_dir):\n    # SageMaker automatically load the model.tar.gz from the S3 and \n    # mount the folders inside the docker container. The  'model_dir'\n    # points to the root of the extracted tar.gz file.\n\n    model_path = f'{model_dir}\/'\n    \n    # Load the model\n    # You can load whatever from the Internet, S3, wherever &lt;--- Answer to your Question\n    # NO Need to use the model in tar.gz. You can place a dummy model file.\n    ...\n\n    return model\n\n\ndef predict_fn(input_data, model):\n    # Do your inference\n    ...\n\ndef input_fn(serialized_input_data, content_type=CONTENT_TYPE_JSON):\n    input_data = json.loads(serialized_input_data)\n    return input_data\n\n\ndef output_fn(prediction_output, accept=CONTENT_TYPE_JSON):\n    if accept == CONTENT_TYPE_JSON:\n        return json.dumps(prediction_output), accept\n    raise Exception('Unsupported content type') \n<\/code><\/pre>\n<h2>Related<\/h2>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker\" rel=\"nofollow noreferrer\">Using Models Trained Outside of Amazon SageMaker\n<\/a><\/li>\n<\/ul>\n<h2>Note<\/h2>\n<p>SageMaker team keeps changing the implementations and the documentations are frequently obsolete. When you are sure you did follow the documents and it does not work, obsolete documentation is quite likely. In such case, need to clarify with AWS support, or open an issue in the Github.<\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1630389412780,
        "Solution_link_count":19.0,
        "Solution_readability":16.5,
        "Solution_reading_time":122.15,
        "Solution_score_count":7.0,
        "Solution_sentence_count":93.0,
        "Solution_word_count":900.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"In this article, learn how to build an end-to-end data to AI solution on Google Cloud, including a practical example of a real-time fraud detection system and the architecture behind it. You'll also discover how to train, deploy, and monitor machine learning models in production.\n\nThis article is based on a recent Cloud OnBoard session.\u00a0Register here to watch on demand.\u00a0\u00a0\u00a0\n\nIf you have any questions, please leave a comment on the blog (or below) and someone from the Community or Google Cloud team will be happy to help.\n\nRead the blog",
        "Challenge_closed_time":1684833.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1684756920000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-build-a-data-to-AI-solution-with-BigQuery-and-Vertex-AI\/m-p\/595708#M1991",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.9,
        "Challenge_reading_time":7.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How to build a data to AI solution with BigQuery and Vertex AI",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":111.0,
        "Challenge_word_count":104,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Great read!\u00a0\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":0.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":7.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I try to use azuremlsdk to deploy a locally trained model (a perfectly valid use case AFIK). I follow <a href=\"https:\/\/cran.r-project.org\/web\/packages\/azuremlsdk\/vignettes\/train-and-deploy-first-model.html\" rel=\"nofollow noreferrer\">this<\/a> and managed to create a ML workspace and register a &quot;model&quot; like so:<\/p>\n<pre><code>library(azuremlsdk)\n\ninteractive_auth &lt;- interactive_login_authentication(tenant_id=&quot;xxx&quot;)\nws &lt;- get_workspace(\n        name = &quot;xxx&quot;, \n        subscription_id = &quot;xxx&quot;, \n        resource_group =&quot;xxx&quot;, \n        auth = interactive_auth\n)\n\nadd &lt;- function(a, b) {\n    return(a + b)\n}\n\nadd(1,2)\n\nsaveRDS(add, file = &quot;D:\/add.rds&quot;)\n\nmodel &lt;- register_model(\n    ws, \n    model_path = &quot;D:\/add.rds&quot;, \n    model_name = &quot;add_model&quot;,\n    description = &quot;An amazing model&quot;\n)\n<\/code><\/pre>\n<p>This seemed to work fine, as I get some nice log messages telling me that the model was registered. For my sanity, I wonder where can I find this registered (&quot;materialised&quot;) model\/object\/function in the Azure UI please?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620989367110,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67533091",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":14.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"where are registered models in azure machine learning",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":41.0,
        "Challenge_word_count":118,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>On ml.azure.com, there is a &quot;Models&quot; option on the left-hand blade.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Y7cZe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Y7cZe.png\" alt=\"UI Sidebar\" \/><\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.1,
        "Solution_reading_time":3.17,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Training a ml model with mlflow in azure environment.<\/p>\n<pre><code>import mlflow\nfrom mlflow import MlflowClient\nfrom azureml.core import Experiment, Workspace\n\nexperiment_name = 'housing-lin-mlflow'\n\nexperiment = Experiment(ws, experiment_name)\n\nruns = mlflow.search_runs(experiment_ids=[ experiment.id ])\n\n<\/code><\/pre>\n<p>While fetching runs from search_runs getting this error :<\/p>\n<pre><code>RestException: BAD_REQUEST: For input string: &quot;5b649b3c-3b8f-497a-bb4f&quot;\n<\/code><\/pre>\n<p>MLflow version : 1.28.0\nIn Azure studio jobs have been created and successfully run.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661517215980,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1661625379892,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73501103",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":8.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Getting Bad request while searching run in mlflow",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":56.0,
        "Challenge_word_count":65,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1582101477803,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":171.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>The bad request in MLFlow after successful running the job is because of not giving proper API permissions for the application.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Search for <strong>MLFLOW<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Scroll down<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/s50AL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s50AL.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Click on View API Permissions<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Under API permissions, assign the permissions according to the application running region and requirements. Checkout the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-models-mlflow\" rel=\"nofollow noreferrer\">document<\/a> for further information.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":9.0,
        "Solution_readability":17.0,
        "Solution_reading_time":16.36,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":94.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1436818579270,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Eugene, OR, USA",
        "Answerer_reputation_count":474.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using the Score Matchbox Recommender set to recommend items from unrated items. This module will run for over 3 hours (I haven't tried longer) and not finish. It will work fine when I'm recommending from rated items to evaluate the recommender, but as soon as I switch to unrated it will run indefinitely. I'm currently using the split data module on already split data to get an even smaller sample of about 20,000 rows. Is this too much for this module to handle? <\/p>\n\n<p>If I try to take the sample even smaller using the partition and sample module (yes I know it's not a recommender split), I immediately get an Exception 0000: Internal system error. <\/p>\n\n<p>Any idea why it's taking so long\/how to fix it?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1529692532740,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50993858",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.7,
        "Challenge_reading_time":9.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Score Matchbox Recommender Stuck or Throwing an Error",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":215.0,
        "Challenge_word_count":135,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436818579270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Eugene, OR, USA",
        "Poster_reputation_count":474.0,
        "Poster_view_count":28.0,
        "Solution_body":"<p>Using the \"Filter Based Feature Selection\" module and then removing all columns besides those found significant and the identifiers seemed to fix the issue. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.0,
        "Solution_reading_time":2.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":6,
        "Challenge_body":"## Description\r\n\r\nI try to reproduce the minimal example from the Docs: a Kedro project using the starter `pandas-iris` using the `kedro-mlflow` functinality. I do not arrive at initializing the kedro-mlflow project, since the cli commands are not available.\r\n\r\n## Context\r\n\r\nIt is unclear to me if this is connected to #157 \r\nI wanted to start looking into kedro-mlflow, but got immediatle blocked by the initialization of the project. Therefore any advice on where to look to fix this would also be appreciated. \r\n\r\n## Steps to Reproduce\r\n\r\n```\r\nconda create -n kedro_mlflow python=3.8\r\nconda activate kedro_mlflow\r\npip install kedro-mlflow\r\nkedro mlflow -h\r\nkedro new --starter=pandas-iris\r\ncd mlflow_test\/\r\nkedro mlflow -h\r\n> ERROR \"No such command 'mlflow'\"\r\n```\r\n\r\n## Expected Result\r\n\r\n`kedro mlflow` is available in a project directory, i.e. `kedro mlflow -h` gives the same output inside the folder as before\r\n\r\n## Actual Result\r\n\r\ninside the project folder the `mlflow` command is unknown to Kedro\r\n\r\n```\r\n...\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/pkg_resources\/__init__.py:1130: DeprecationWarning: Use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\r\n  return get_provider(package_or_requirement).get_resource_filename(\r\n....\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/mlflow\/types\/schema.py:49: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \r\nDeprecated in NumPy 1.20; for more details and guidance: https:\/\/numpy.org\/devdocs\/release\/1.20.0-notes.html#deprecations\r\n  binary = (7, np.dtype(\"bytes\"), \"BinaryType\", np.object)\r\n2021-04-23 17:49:52,197 - root - INFO - Registered hooks from 2 installed plugin(s): kedro-mlflow-0.7.1\r\nUsage: kedro [OPTIONS] COMMAND [ARGS]...\r\nTry 'kedro -h' for help.\r\n\r\nError: No such command 'mlflow'.\r\n\r\n```\r\n\r\n## Your Environment\r\n\r\nUbuntu 18.04.5\r\n\r\n- Kedro 0.17.3\r\n- kedro-mlflow 0.7.1\r\n- python 3.8.8.\r\n- mlflow 1.15.0\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nyes",
        "Challenge_closed_time":1619987.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619193727000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/193",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":8.4,
        "Challenge_reading_time":27.15,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":null,
        "Challenge_title":"kedro-mlflow CLI is unavailable inside a Kedro project",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":273,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi, \r\n\r\nI wil try to check it out this weekend, but the `kedro==0.17.3` version is brand new (it was released yesterday), and given my experience with past kedro versions update 2 things might have happened on kedro's side: \r\n- They have broken the auto-discovery mechanism (I've seen in the release note that they change the CLI command discovery to enale overriding project commands by plugins)\r\n- They have not updated their `pandas-iris` starter yet which does not match the new version and is only compliant with `kedro==0.17.2`. \r\n\r\nWhile I am investigating, would you please confirm that :\r\n- `kedro-mlflow` works fine with kedro==0.17.2 with your setup\r\n- `kedro-mlflow` works fine if you don't use the `pandas-iris` starter: try `kedro new` with `kedro==0.17.3` and then add one ode to test the plugin\r\n- I'd be glad to see if another plugin (e.g. `kedro-viz`) is facing the same problem that kedro-mlflow. Would you mind checking?\r\n\r\nOf course there is the possibility that the problem comes from `kedro-mlflow` itself, but I hardly believe it. I'll tell you within 2 days. I am sorry, I am quite busy for now and I will not debug this before next week. Once again, it is very likely kedro's plugin discovery mechanism has been broken in the new release, I strongly suggest you go back to `kedro==0.17.2`.\r\n\r\nNext actions: \r\n- [X] reproduce the bug -> Done, thanks for the very good reproducible example\r\n- [X] Check if it happens with other plugins (say kedro-viz) -> `kedro viz` global command is properly discovered\r\n- [X] Check if hooks are properly loaded -> everything works fine if I add a `mlflow.yml` manually in the `conf\/local` folder (or any folder in `conf\/` actually). -> **This is a short term solution for you**,e ven if it is not very convenient. You can find allowed keys [in the documentation](https:\/\/kedro-mlflow.readthedocs.io\/en\/latest\/source\/04_experimentation_tracking\/01_configuration.html#the-mlflow-yml-file) or irectly [copy paste it from the code](https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/master\/kedro_mlflow\/template\/project\/mlflow.yml)\r\n- [X] Check if the tests pass with kedro==0.17.3 -> *Some tests are failing, but not the one related to the CLI commands which seems discovered. I need to investigate further*.\r\n- [x] Check if other plugins with *local* commands are discovered\r\n- [x] Check if it also happens it an empty project (i.e. *not* a starter)\r\n First of all, thank you for looking so quickly into it!\r\n\r\nFrom how I read your second message you already know that, but to answer your questions:\r\n- detecting `kedro mlflow` works fine with `kedro==0.17.2`\r\n- the problem is consistent with kedro==0.17.3 independent if I use the pandas-iris starter or not\r\n- `kedro viz` is found also with `kedro=0.17.3`\r\n\r\nAgain, thank you for providing workarounds directly on Monday morning, I can nicely work with those! A question for my understanding of the plugin: As long as the hooks are loaded, the mlflow functionality depends only on a `mlflow.yml` to be present, and all that `kedro mlflow init` does is copy this file from the template into `conf\/local`, is this correct? TL;DR: \r\n\r\nInstall this version for now, it should make the command available again:\r\n\r\n```console\r\npip uninstall kedro-mlflow\r\npip install git+https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow.git@bug\/no-cli\r\n```\r\n**Beware:** it is very important to uninstall your existing version of kedro-mlflow before reinstalling because the patch has the same version number that the current release.\r\n\r\nIf you confirm this works for you, I will deploy the patch to PyPI before kedro provides a patch on their side.\r\n_____________________________\r\n\r\nHi, some follow-up about this bug:\r\n\r\n- I've figured out *what* is going on but not *why* it happens. The `mlflow` group of command exists both at global (`new`) and project (`init`, `ui`) levels and for an unknown reason, `kedro` takes into account only one group of command in its `0.17.3` version. This is a bug I will report to the core team. However, it does not affect their other plugins (kedro-viz, kedro-docker, kedro-airflow) because none of them has both global and project commands.\r\n- The quickest (hacky) fix is to remove the global group of command to the make the project ones available. I've done this in the branch `bug\/no-cli` of the repo.\r\n\r\nTo answer your question: \r\n\r\n> A question for my understanding of the plugin: As long as the hooks are loaded, the mlflow functionality depends only on a mlflow.yml to be present, and all that kedro mlflow init does is copy this file from the template into conf\/local, is this correct?\r\n\r\nExactly: the `init` command renders the template (i.e. copy paste it + replace the jinja tags with dynamic values like the name of your project) to a folder in your `conf\/` folder (by default `local`, but you can specify an environment like this: `kedro mlflow init --env=<your-env-folder>`). The hooks contain all the code logic  and this mlflow.yml file is just here to pass parameters to them. \r\n\r\nThe other project command is `kedro mlflow ui` which is just a wrapper of \"mlflow ui\" with the parameters (mlflow_tracking_uri, port, host) defined in your `mlflow.yml` file.\r\n thanks, form a quick test I would say: the patch works like a charm! Hi @dmb23, I've just deployed the patch to PyPI. You can use `pip install kedro_mlflow==0.7.2`` and it should be ok for now. I close the issue, but feel free to reopen if you still encounter any issue in this new version.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":7.8,
        "Solution_reading_time":66.71,
        "Solution_score_count":null,
        "Solution_sentence_count":48.0,
        "Solution_word_count":849.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created a simply model and then registered in azure. How can I make a prediction?<\/p>\n<pre><code>from sklearn import svm\nimport joblib\nimport numpy as np\n\n# customer ages\nX_train = np.array([50, 17, 35, 23, 28, 40, 31, 29, 19, 62])\nX_train = X_train.reshape(-1, 1)\n# churn y\/n\ny_train = [&quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;]\n\nclf = svm.SVC(gamma=0.001, C=100.)\nclf.fit(X_train, y_train)\n\njoblib.dump(value=clf, filename=&quot;churn-model.pkl&quot;)\n<\/code><\/pre>\n<p>Registration:<\/p>\n<pre><code>from azureml.core import Workspace\nws = Workspace.get(name=&quot;myworkspace&quot;, subscription_id='My_subscription_id', resource_group='ML_Lingaro')\n\nfrom azureml.core.model import Model\nmodel = Model.register(workspace=ws, model_path=&quot;churn-model.pkl&quot;, model_name=&quot;churn-model-test&quot;)\n<\/code><\/pre>\n<p>Prediction:<\/p>\n<pre><code>from azureml.core.model import Model\nimport os\n\nmodel = Model(workspace=ws, name=&quot;churn-model-test&quot;)\nX_test = np.array([50, 17, 35, 23, 28, 40, 31, 29, 19, 62])\nmodel.predict(X_test) ???? \n<\/code><\/pre>\n<p>Error: <code>AttributeError: 'Model' object has no attribute 'predict'<\/code><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609722575690,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1609723457016,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65556574",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":17.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"How to make prediction after model registration in azure?",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":540.0,
        "Challenge_word_count":120,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1605834001336,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":73.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>great question -- I also had the same misconception starting out. The missing piece is that there's a difference between model 'registration' and model 'deployment'. Registration is simply for tracking and for easy downloading at a later place and time. Deployment is what you're after, making a model available to be scored against.<\/p>\n<p>There's a <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python&amp;WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">whole section in the docs about deployment<\/a>. My suggestion would be to deploy it locally first for testing.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.8,
        "Solution_reading_time":8.06,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":75.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a dataset where each row is a data sample, and there is a a column indicating a group this sample came from. So, each group has several data points, and each one is a row in the dataframe. I would like to run the cross-validation so that at each fold, the data points from one group are used as the validation set, and the data points from other groups as the training test. Is this currently somehow possible in Azure AutoML ?  <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618488817010,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/358763\/leave-one-group-out-cross-validation-in-azure-auto",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":5.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Leave-one-group-out cross-validation in Azure AutoML",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":88,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Yes, you can specify custom cross-validation data folds based on columns. More details are provided in the following <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-cross-validation-data-splits#specify-custom-cross-validation-data-folds\">document<\/a>. Hope this helps.    <\/p>\n<p><strong>Example:<\/strong>    <\/p>\n<pre><code>automl_config = AutoMLConfig(compute_target = aml_remote_compute,  \n                             task = 'classification',  \n                             primary_metric = 'AUC_weighted',  \n                             training_data = dataset,  \n                             label_column_name = 'y',  \n                             cv_split_column_names = ['cv1', 'cv2']  \n                            )  \n<\/code><\/pre>\n",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":25.0,
        "Solution_reading_time":7.89,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":41.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1388815483776,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":100.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":474.1775913889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm having an issue to serve a model with reference to model registry. According to help, the path should look like this: <\/p>\n\n<p>models:\/model_name\/stage<\/p>\n\n<p>When I type in terminal: <br>\n<code>mlflow models serve -m models:\/ml_test_model1\/Staging --no-conda -h 0.0.0.0 -p 5003<\/code><\/p>\n\n<p>I got the error: <br>\n<code>mlflow.exceptions.MlflowException: Not a proper models:\/ URI: models:\/ml_test_model1\/Staging\/MLmodel. Models URIs must be of the form 'models:\/&lt;model_name&gt;\/&lt;version or stage&gt;'.<\/code><\/p>\n\n<p>Model is registered and visible in db and server. <br> \nIf I put absolute path, it works (experiment_id\/run_id\/artifacts\/model_name).<\/p>\n\n<p>mlflow version: 1.4 <br>\nPython version: 3.7.3<\/p>\n\n<p>Is it matter of some environmental settings or something different?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1575544859307,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59194004",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.4,
        "Challenge_reading_time":10.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"MLflow - Serving model by reference to model registry",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1225.0,
        "Challenge_word_count":104,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1575543357812,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>That style of referencing model artefacts is fixed from mlflow v1.5 (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/pull\/2067\" rel=\"nofollow noreferrer\">Bug Fix<\/a>).<\/p>\n\n<p>You'll need to run <code>mlflow db upgrade &lt;db uri&gt;<\/code> to refresh your schemas before restarting your mlflow server.<\/p>\n\n<p>You may find listing registered models helpful:<\/p>\n\n<p><code>&lt;server&gt;:&lt;port&gt;\/api\/2.0\/preview\/mlflow\/registered-models\/list<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1577251898636,
        "Solution_link_count":1.0,
        "Solution_readability":10.1,
        "Solution_reading_time":6.02,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":42.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"I want to train and build the model in Sagemaker studio and then be able to export the model as a container image to ECR, so I can use the model in external platform by sharing the ECR image to another account where I Can create container with the image  from ECR",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663258467464,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1667926271895,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUZHWz5-hpSc-80dEIkuxwQw\/how-to-export-tresained-models-to-ecr-as-container-image",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.0,
        "Challenge_reading_time":3.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"How to export tresained models to ECR as container image",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":72.0,
        "Challenge_word_count":61,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The models you train in SageMaker are stored in S3 as .tar.gz files that you can use to deploy to an endpoint, or even test locally (extracting the model file from the tar file). \nIf you are using a built-in algorithm, you can share the .tar.gz file to the second account and deploy the model in the second account, since built-in algorithm containers can be accessed from any AWS account. \n\nIf you are using a custom training image ([docs here](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/adapt-training-container.html)), you can push this image to ECR and allow a [second account to pull the image](https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/secondary-account-access-ecr\/) and then use the image with the model that you have trained. However, note that Studio at this time does not support building Docker images out of the box. You can use [SageMaker Notebook Instances](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi.html) instead.\n\nI would recommend keeping the model (.tar.gz) and the image (Docker) separate, since you can easily retrain and deploy the newer versions of models without updating the image every single time.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1663369533112,
        "Solution_link_count":3.0,
        "Solution_readability":10.2,
        "Solution_reading_time":14.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":163.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to use the tensorflow hub to retrain existing models, however tensorflow supports the hub library only on their 2.2 version. And The Estimator azure presents supports tf 2.0.  <\/p>\n<p>When I list tensorflow 2.2 as a required dependency as a pip package, during docker image creation the system fails - it seems like horovod is responsible, - that it cannot find the correct libraries.   <\/p>\n<p>Is this possible to be fixed? as in either an Estimator with tf 2.2 support, or an esitmator without the horovod - as I do not need a distributed system for my solution. <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1591339346177,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/32334\/tensorflow-2-2-0-update-for-the-tensorflow-estimat",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":8.7,
        "Challenge_reading_time":8.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"TensorFlow 2.2.0 update for the tensorflow estimator for Azure ML, or disable horovod?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":111,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Following the pointers from <a>@romungi-MSFT<\/a>, defining estimator with gpubase image; &quot;mcr.microsoft.com\/azureml\/base-gpu:openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04&quot; solves the problem, and Tensorflow 2.2 can be included. Tensorflow uses GPU by default when available.<\/p>\n<pre><code> estimator = Estimator(source_directory=experiment_folder,\n                       compute_target=compute_target,\n                       script_params=script_params,\n                       entry_script='rps_efn_b0.py',\n                       node_count=1,        \n                       conda_packages=['ipykernel'],\n                       pip_packages = ['azureml-sdk',\n                                       'pyarrow',\n                                       'pyspark',\n                                       'azureml-mlflow',\n                                       'joblib',\n                                       'matplotlib',\n                                       'Pillow',\n                                       'tensorflow==2.2',\n                                       'tensorflow-datasets',\n                                       'tensorflow-hub',\n                                       'azureml-defaults',\n                                       'azureml-dataprep[fuse,pandas]'],\n                       custom_docker_image='mcr.microsoft.com\/azureml\/base-gpu:openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04')\n<\/code><\/pre>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":24.9,
        "Solution_reading_time":11.33,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":50.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1436184843608,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":305.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>So I have been playing around with Azure ML lately, and I got one dataset where I have multiple values I want to predict. All of them uses different algorithms and when I try to train multiple models within one experiment; it says the \u201ctrain model can only predict one value\u201d, and there are not enough input ports on the train-model to take in multiple values even if I was to use the same algorithm for each measure. I tried launching the column selector and making rules, but I get the same error as mentioned. How do I predict multiple values and later put the predicted columns together for the web service output so I don\u2019t have to have multiple API\u2019s?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1465894075710,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37807158",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.2,
        "Challenge_reading_time":8.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Train multiple models with various measures and accumulate predictions",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1763.0,
        "Challenge_word_count":129,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1463041289043,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":25.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>What you would want to do is to train each model and save them as already trained models.\nSo create a new experiment, train your models and save them by right clicking on each model and they will show up in the left nav bar in the Studio. Now you are able to drag your models into the canvas and have them score predictions where you eventually make them end up in the same output as I have done in my example through the \u201cAdd columns\u201d module. I made this example for Ronaldo (Real Madrid CF player) on how he will perform in match after training day. You can see my demo on <a href=\"http:\/\/ronaldoinform.azurewebsites.net\" rel=\"nofollow noreferrer\">http:\/\/ronaldoinform.azurewebsites.net<\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ZwzUy.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZwzUy.png\" alt=\"Ronaldo InForm\"><\/a><\/p>\n\n<p>For more detailed explanation on how to save the models and train multiple values; you can check out Raymond Langaeian (MSFT) answer in the comment section on this link:\n<a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-convert-training-experiment-to-scoring-experiment\/\" rel=\"nofollow noreferrer\">https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-convert-training-experiment-to-scoring-experiment\/<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":13.0,
        "Solution_reading_time":17.07,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":157.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In my setup, I run a script that <strong>trains<\/strong> a model and starts generating checkpoints. Another script watches for new checkpoints and <strong>evaluates<\/strong> them. The scripts run in parallel, so evaluation is just a step behind training.<\/p>\n\n<p>What's the right Tracks configuration to support this scenario?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591905931703,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1609771005756,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62332672",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.4,
        "Challenge_reading_time":4.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Tracking separate train\/test processes with Trains",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":99.0,
        "Challenge_word_count":51,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1311330349880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tel Aviv",
        "Poster_reputation_count":3784.0,
        "Poster_view_count":342.0,
        "Solution_body":"<p>disclaimer: I'm part of the <a href=\"https:\/\/github.com\/allegroai\/trains\/\" rel=\"nofollow noreferrer\">allegro.ai Trains<\/a> team<\/p>\n<p>Do you have two experiments? one for testing one for training ?<\/p>\n<p>If you do have two experiments, then I would make sure the models are logged in both of them (which if they are stored on the same shared-folder\/s3\/etc will be automatic)\nThen you can quickly see the performance of each-one.<\/p>\n<p>Another option is sharing the same experiment, then the second process adds reports to the original experiment, that means that somehow you have to pass to it the experiment id.\nThen you can do:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>task = Task.get_task(task_id='training_task_id`)\ntask.get_logger().report_scalar('title', 'loss', value=0.4, iteration=1)\n<\/code><\/pre>\n<p>EDIT:\nAre the two processes always launched together, or is the checkpoint test a general purpose code ?<\/p>\n<p>EDIT2:<\/p>\n<p>Let's assume you have main script training a model. This experiment has a unique task ID:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>my_uid = Task.current_task().id\n<\/code><\/pre>\n<p>Let's also assume you have a way to pass it to your second process (If this is an actual sub-process, it inherits the os environment variables so you could do <code>os.environ['MY_TASK_ID']=my_uid<\/code>)<\/p>\n<p>Then in the evaluation script you could report directly into the main training Task like so:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>train_task = Task.get_task(task_id=os.environ['MY_TASK_ID'])\ntrain_task.get_logger().report_scalar('title', 'loss', value=0.4, iteration=1)\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1592833417200,
        "Solution_link_count":1.0,
        "Solution_readability":10.9,
        "Solution_reading_time":21.3,
        "Solution_score_count":1.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":202.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am finetuning multiple models using for loop as follows.<\/p>\n<pre><code class=\"lang-auto\">for file in os.listdir(args.data_dir):\n    finetune(args, file)\n<\/code><\/pre>\n<p>BUT <code>wandb<\/code> shows logs only for the first file in <code>data_dir<\/code> although it is training and saving models for other files. It feels very strange behavior.<\/p>\n<pre><code class=\"lang-auto\">wandb: Synced bertweet-base-finetuned-file1: https:\/\/wandb.ai\/***\/huggingface\/runs\/***\n<\/code><\/pre>\n<p>This is a small snippet of <strong>finetuning<\/strong> code with Huggingface:<\/p>\n<pre><code class=\"lang-auto\">def finetune(args, file):\n    training_args = TrainingArguments(\n        output_dir=f'{model_name}-finetuned-{file}',\n        overwrite_output_dir=True,\n        evaluation_strategy='no',\n        num_train_epochs=args.epochs,\n        learning_rate=args.lr,\n        weight_decay=args.decay,\n        per_device_train_batch_size=args.batch_size,\n        per_device_eval_batch_size=args.batch_size,\n        fp16=True, # mixed-precision training to boost speed\n        save_strategy='no',\n        seed=args.seed,\n        dataloader_num_workers=4,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset['train'],\n        eval_dataset=None,\n        data_collator=data_collator,\n    )\n    trainer.train()\n    trainer.save_model()\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650439111790,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-for-huggingface-trainer-saves-only-first-model\/2270",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":17.1,
        "Challenge_reading_time":17.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Wandb for Huggingface Trainer saves only first model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":207.0,
        "Challenge_word_count":100,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><code>wandb.init(reinit=True)<\/code> and <code>run.finish()<\/code> helped me to log the models <strong>separately<\/strong> on wandb website.<\/p>\n<p>The working code looks like below:<\/p>\n<pre><code class=\"lang-auto\">\nfor file in os.listdir(args.data_dir):\n    finetune(args, file)\n\nimport wandb\ndef finetune(args, file):\n    run = wandb.init(reinit=True)\n    ...\n    run.finish()\n<\/code><\/pre>\n<p>Reference: <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/launch#how-do-i-launch-multiple-runs-from-one-script\" class=\"inline-onebox\">Launch Experiments with wandb.init - Documentation<\/a><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.4,
        "Solution_reading_time":7.73,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":44.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi.  <\/p>\n<p>I am working on an ML model in Designer.  <\/p>\n<p>I have a dataset of c. 55,000 rows.   <\/p>\n<p>When I add an &quot;ID&quot; column (unique per row - so 55,000 IDs) to my dataset for training \/ scoring, I receive the error message:  <\/p>\n<blockquote>\n<p>ModuleExceptionMessage:ColumnUniqueValuesExceeded: Number of unique values in column: &quot;ID&quot; is greater than allowed.  <\/p>\n<\/blockquote>\n<p><strong>Question:<\/strong> is this error based on a physical cap on number of rows - or capacity based on e.g. Compute power associated with the instance?  <\/p>\n<p>I can run 20k rows through the model <em>without<\/em> the ID column - so it seems the unique rows is the challenge.  <\/p>\n<p>But then - how do I keep an identifying column in the scored dataset, if there is a cap on unique values?   <\/p>\n<p>Because I need the ID column to join with other data that is not able to be used in modelling as features etc.   <\/p>\n<p>Any guidance welcome! <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":6,
        "Challenge_created_time":1604936001023,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/156534\/azure-ml-id-column-for-joining-data-returns-no-of",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":6.5,
        "Challenge_reading_time":12.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML: ID column for joining data returns \"No. Of unique values ... is greater than allowed\"",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":172,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>User can use Edit Metadata module to mark the ID column as &quot;ClearFeature&quot;, and thus this will not be used in Train Model. This should prevent the error. Please have a try and let me know if there is any questions. <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/edit-metadata\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/edit-metadata<\/a>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/40390-microsoftteams-image-7.png?platform=QnA\" alt=\"40390-microsoftteams-image-7.png\" \/>    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.8,
        "Solution_reading_time":8.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":52.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to predict from my past data which has around 20 attribute columns and a label. Out of those 20, only 4 are significant for prediction. But i also want to know that if a row falls into one of the classified categories, what other important correlated columns apart from those 4 and what are their weight. I want to get that result from my deployed web service on Azure.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1461854370743,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36917948",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.2,
        "Challenge_reading_time":5.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Feature weightage from Azure Machine Learning Deployed Web Service",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":303.0,
        "Challenge_word_count":80,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1461853741067,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>You can use permutation feature importance module but that will give importance of the features across the sample set. Retrieving the weights on per call basis is not available in Azure ML.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":2.42,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":32.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1415722650716,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Verona, VR, Italy",
        "Answerer_reputation_count":4811.0,
        "Answerer_view_count":713.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am trying out <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning-service\/\" rel=\"nofollow noreferrer\">Azure Machine Learning Service<\/a> for ML deployment.<\/p>\n\n<p>I have already trained a model on a compute VM and saved it as pickle, and now would like to deploy it (I am using Python on Azure notebooks for the purpose as of now).<\/p>\n\n<p>From the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml#register-model\" rel=\"nofollow noreferrer\">guide<\/a>, it looks like I need to I need a <code>run<\/code> object to be existing in my session to execute the \"model registration\" step:<\/p>\n\n<pre><code># register model \nmodel = run.register_model(model_name='my_model', model_path='outputs\/my_model.pkl')\nprint(model.name, model.id, model.version, sep = '\\t')\n<\/code><\/pre>\n\n<p>However, I haven't created any <code>run<\/code> object as I haven't <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml#submit-the-job-to-the-cluster\" rel=\"nofollow noreferrer\">executed<\/a> any experiment for training, I am just starting off with my pickled model.<\/p>\n\n<p>I also tried to register a model by uploading it via the Azure Portal (see screenshot below), but (as the model file is quite large, I assume) it fails with a <code>ajax error 413.<\/code> as in <a href=\"https:\/\/stackoverflow.com\/questions\/55064123\/unable-to-register-an-onnx-model-in-azure-machine-learning-service-workspace\">Unable to register an ONNX model in azure machine learning service workspace<\/a>.  <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/tVKcV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tVKcV.png\" alt=\"model registering\"><\/a> <\/p>\n\n<p>Is there any way to register and then deploy a pretrained pickled mode (without the need of submitting a <code>run<\/code>, if that makes sense)?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1553160837153,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1554642485923,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55277334",
        "Challenge_link_count":6,
        "Challenge_participation_count":3,
        "Challenge_readability":13.9,
        "Challenge_reading_time":25.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I register in Azure ML Service a machine learning model trained locally?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1466.0,
        "Challenge_word_count":212,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1415722650716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Verona, VR, Italy",
        "Poster_reputation_count":4811.0,
        "Poster_view_count":713.0,
        "Solution_body":"<p>Model registration can be done with <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none-\" rel=\"nofollow noreferrer\">Model.register<\/a>, without the need of using a <code>run<\/code> object<\/p>\n\n<pre><code>model = Model.register(model_name='my_model', model_path='my_model.pkl', workspace = ws)\n<\/code><\/pre>\n\n<p>for the deployment one can follow steps as outlined in the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml#deploy-as-a-web-service\" rel=\"nofollow noreferrer\">Azure ML service doc<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":26.6,
        "Solution_reading_time":9.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":42.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":365.0732141667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Looking at the following source code taken from <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"nofollow noreferrer\">here<\/a> (SDK v2):<\/p>\n<pre><code>import boto3\nimport sagemaker\nfrom sagemaker.xgboost.estimator import XGBoost\nfrom sagemaker.session import Session\nfrom sagemaker.inputs import TrainingInput\n\n# initialize hyperparameters\nhyperparameters = {\n        &quot;max_depth&quot;:&quot;5&quot;,\n        &quot;eta&quot;:&quot;0.2&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;verbosity&quot;:&quot;1&quot;,\n        &quot;objective&quot;:&quot;reg:linear&quot;,\n        &quot;num_round&quot;:&quot;50&quot;}\n\n# set an output path where the trained model will be saved\nbucket = sagemaker.Session().default_bucket()\nprefix = 'DEMO-xgboost-as-a-framework'\noutput_path = 's3:\/\/{}\/{}\/{}\/output'.format(bucket, prefix, 'abalone-xgb-framework')\n\n# construct a SageMaker XGBoost estimator\n# specify the entry_point to your xgboost training script\nestimator = XGBoost(entry_point = &quot;your_xgboost_abalone_script.py&quot;, \n                    framework_version='1.2-2',\n                    hyperparameters=hyperparameters,\n                    role=sagemaker.get_execution_role(),\n                    instance_count=1,\n                    instance_type='ml.m5.2xlarge',\n                    output_path=output_path)\n\n# define the data type and paths to the training and validation datasets\ncontent_type = &quot;libsvm&quot;\ntrain_input = TrainingInput(&quot;s3:\/\/{}\/{}\/{}\/&quot;.format(bucket, prefix, 'train'), content_type=content_type)\nvalidation_input = TrainingInput(&quot;s3:\/\/{}\/{}\/{}\/&quot;.format(bucket, prefix, 'validation'), content_type=content_type)\n\n# execute the XGBoost training job\nestimator.fit({'train': train_input, 'validation': validation_input})\n<\/code><\/pre>\n<p>I wonder where the your_xgboost_abalone_script.py file has to be placed please? So far I used XGBoost as a built-in algorithm from my local machine with similar code (i.e. I span up a training job remotely). Thanks!<\/p>\n<p>PS:<\/p>\n<p>Looking at <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html\" rel=\"nofollow noreferrer\">this<\/a>, and source_dir, I wonder if one can upload Python files to S3. In this case, I take it is has to be tar.gz? Thanks!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653478466700,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1653482394636,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72376872",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.1,
        "Challenge_reading_time":30.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":null,
        "Challenge_title":"entry_point file using XGBoost as a framework in sagemaker",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":74.0,
        "Challenge_word_count":191,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p><code>your_xgboost_abalone_script.py<\/code> can be created locally. The path you provide is relative to where the code is running.<\/p>\n<p>I.e. <code>your_xgboost_abalone_script.py<\/code> can be located in the same directory where you are running the SageMaker SDK (&quot;source code&quot;).<\/p>\n<p>For example if you have <code>your_xgboost_abalone_script.py<\/code> in the same directory as the source code:<\/p>\n<pre><code>.\n\u251c\u2500\u2500 source_code.py\n\u2514\u2500\u2500 your_xgboost_abalone_script.py\n<\/code><\/pre>\n<p>Then you can point to this file exactly how the documentation depicts:<\/p>\n<pre><code>estimator = XGBoost(entry_point = &quot;your_xgboost_abalone_script.py&quot;, \n.\n.\n.\n)\n<\/code><\/pre>\n<p>The SDK will take <code>your_xgboost_abalone_script.py<\/code> repackage it into a model tar ball and upload it to S3 on your behalf.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1654796658207,
        "Solution_link_count":0.0,
        "Solution_readability":12.3,
        "Solution_reading_time":10.69,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":89.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1388051464987,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":13496.0,
        "Answerer_view_count":1691.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm a newbie trying out Azure's Machine Learning (ML) Studio module. I own a standard subscription level account which grants me an experimental duration of \"Up to 7 days per experiment with a maximum of 24 hours per module\" according to the <a href=\"https:\/\/azure.microsoft.com\/en-us\/pricing\/details\/machine-learning-studio\/\" rel=\"nofollow noreferrer\">ML Studio's pricing site<\/a>.<\/p>\n\n<p>However, since my dataset is extremely large, I would need a much longer training duration than the allocated 24 hours (I have tried and it timeout-ed even with the simplest NN architecture). Is there workaround for this issue? <\/p>\n\n<p>Thank you in advance. <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1546418864137,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54003442",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.0,
        "Challenge_reading_time":8.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning Studio training 24 hour timeout",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":228.0,
        "Challenge_word_count":100,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1455696062412,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":327.0,
        "Poster_view_count":46.0,
        "Solution_body":"<p>I would suggest to stop using <code>Azure Machine Learning Studio<\/code> and switch to \"real\" Azure ML with <code>Azure Machine Learning Services<\/code>, where you will have much more control on your compute needs.<\/p>\n\n<p>Azure ML Studio roadmap is really limited and the purpose of this solution was to help people coming to Machine Learning. If you have a real use-case, use Azure Machine Learning Services.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.4,
        "Solution_reading_time":5.19,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1267440784443,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Somewhere",
        "Answerer_reputation_count":15705.0,
        "Answerer_view_count":2150.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I can train a XGBoost model using Sagemaker images like so:<\/p>\n<pre><code>import boto3\nimport sagemaker\nfrom sagemaker.inputs import TrainingInput\nimport os\n\nfolder = r&quot;C:\\Somewhere&quot;\nos.chdir(folder)\n\ns3_prefix = 'some_model'\ns3_bucket_name = 'the_bucket'\ntrain_file_name = 'train.csv'\nval_file_name = 'val.csv'\nrole_arn = 'arn:aws:iam::482777693429:role\/bla_instance_role'\n\nregion_name = boto3.Session().region_name\n\ns3_input_train = TrainingInput(s3_data='s3:\/\/{}\/{}\/{}'.format(s3_bucket_name, s3_prefix, train_file_name), content_type='csv')\ns3_input_val = TrainingInput(s3_data='s3:\/\/{}\/{}\/{}'.format(s3_bucket_name, s3_prefix, val_file_name), content_type='csv')\n\nprint(type(s3_input_train))\n\nhyperparameters = {\n        &quot;max_depth&quot;:&quot;13&quot;,\n        &quot;eta&quot;:&quot;0.15&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;objective&quot;:&quot;reg:squarederror&quot;,\n        &quot;num_round&quot;:&quot;50&quot;}\n\noutput_path = 's3:\/\/{}\/{}\/output'.format(s3_bucket_name, s3_prefix)\n\n# 1.5-1\n# 1.3-1\nestimator = sagemaker.estimator.Estimator(image_uri=sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region_name, &quot;1.2-2&quot;), \n                                          hyperparameters=hyperparameters,\n                                          role=role_arn,\n                                          instance_count=1, \n                                          instance_type='ml.m5.2xlarge',\n                                          #instance_type='local', \n                                          volume_size=1, # 1 GB \n                                          output_path=output_path)\n\nestimator.fit({'train': s3_input_train, 'validation': s3_input_val})\n<\/code><\/pre>\n<p>This work for all versions 1.2-2, 1.3-1 and 1.5-1. Unfortunately the following code only works for version 1.2-2:<\/p>\n<pre><code>import boto3\nimport os\nimport pickle as pkl \nimport tarfile\nimport pandas as pd\nimport xgboost as xgb\n\nfolder = r&quot;C:\\Somewhere&quot;\nos.chdir(folder)\n\ns3_prefix = 'some_model'\ns3_bucket_name = 'the_bucket'\nmodel_path = 'output\/sagemaker-xgboost-2022-04-30-10-52-29-877\/output\/model.tar.gz'\nsession = boto3.Session(profile_name='default')\nsession.resource('s3').Bucket(s3_bucket_name).download_file('{}\/{}'.format(s3_prefix, model_path), 'model.tar.gz')\nt = tarfile.open('model.tar.gz', 'r:gz')\nt.extractall()\n\nmodel_file_name = 'xgboost-model'\nwith open(model_file_name, &quot;rb&quot;) as input_file:\ne = pkl.load(input_file) \n<\/code><\/pre>\n<p>Otherwise I get a:<\/p>\n<pre><code>_pickle.UnpicklingError: unpickling stack underflow\n<\/code><\/pre>\n<p>Am I missing something? Is my &quot;pickle loading code wrong&quot;?<\/p>\n<p>The version of xgboost is 1.6.0 where I run the pickle code.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651317702787,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72068059",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":17.4,
        "Challenge_reading_time":34.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":null,
        "Challenge_title":"cannot load pickle files for xgboost images of version > 1.2-2 in sagemaker - UnpicklingError",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":140.0,
        "Challenge_word_count":185,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>I found the solution <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/2952\" rel=\"nofollow noreferrer\">here<\/a>. I will leave it in case someone come accross the same issue.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.8,
        "Solution_reading_time":2.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":20.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"Hi,\n\nI created my models with Auto ML (image classification or object detection).\n\nNow, I would like to use these in my application, on local (disconnected).\n\nIs it possible to extract a model file from Auto ML that I can use (.pb for instance) ?\n\nAfter some researches, it seems to me that it is not possible but I would like to be sure.\n\nElse, how?\n\nRegards.",
        "Challenge_closed_time":1671501.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1671155520000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Can-I-export-a-my-model-for-internal-usage\/m-p\/500236#M951",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.0,
        "Challenge_reading_time":4.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Can I export a my model for internal usage ?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":421.0,
        "Challenge_word_count":75,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I just didn't understand that I need to select the \"edge\" option to have the \"export model\" available.\n\nThank you for the useful documentation .\n\nAs a additionnal question : can I know the version of Tensorflow used for these export model? (I have some incompatibiliy to use these in my software).\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":3.97,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":55.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Someone should add \"net#\" as a tag. I'm trying to improve my neural network in Azure Machine Learning Studio by turning it into a convolution neural net using this tutorial:<\/p>\n\n<p><a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Neural-Network-Convolution-and-pooling-deep-net-2\" rel=\"noreferrer\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Neural-Network-Convolution-and-pooling-deep-net-2<\/a><\/p>\n\n<p>The differences between mine and the tutorial is I'm doing regression with 35 features and 1 label and they're doing classification with 28x28 features and 10 labels. <\/p>\n\n<p>I start with the basic and 2nd example and get them working with:<\/p>\n\n<pre><code>input Data [35];\n\nhidden H1 [100]\n    from Data all;\n\nhidden H2 [100]\n    from H1 all;\n\noutput Result [1] linear\n    from H2 all;\n<\/code><\/pre>\n\n<p>Now the transformation to convolution I misunderstand. In the tutorial and documentation here: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-azure-ml-netsharp-reference-guide\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-azure-ml-netsharp-reference-guide<\/a> it doesn't mention how the node tuple values are calculated for the hidden layers. The tutorial says:<\/p>\n\n<pre><code>hidden C1 [5, 12, 12]\n  from Picture convolve {\n    InputShape  = [28, 28];\n    KernelShape = [ 5,  5];\n    Stride      = [ 2,  2];\n    MapCount = 5;\n  }\n\nhidden C2 [50, 4, 4]\n   from C1 convolve {\n     InputShape  = [ 5, 12, 12];\n     KernelShape = [ 1,  5,  5];\n     Stride      = [ 1,  2,  2];\n     Sharing     = [ F,  T,  T];\n     MapCount = 10;\n  }\n<\/code><\/pre>\n\n<p>Seems like the [5, 12, 12] and [50,4,4] pop out of no where along with the KernalShape, Stride, and MapCount. How do I know what values are valid for my example? I tried using the same values, but it didn't work and I have a feeling since he has a [28,28] input and I have a [35], I need tuples with 2 integers not 3. <\/p>\n\n<p>I just tried with random values that seem to correlate with the tutorial:<\/p>\n\n<pre><code>const { T = true; F = false; }\n\ninput Data [35];\n\nhidden C1 [7, 23]\n  from Data convolve {\n    InputShape  = [35];\n    KernelShape = [7];\n    Stride      = [2];\n    MapCount = 7;\n  }\n\nhidden C2 [200, 6]\n   from C1 convolve {\n     InputShape  = [ 7, 23];\n     KernelShape = [ 1,  7];\n     Stride      = [ 1,  2];\n     Sharing     = [ F,  T];\n     MapCount = 14;\n  }\n\nhidden H3 [100]\n  from C2 all;\n\noutput Result [1] linear\n  from H3 all;\n<\/code><\/pre>\n\n<p>Right now it seems impossible to debug because the only error code Azure Machine Learning Studio ever gives is:<\/p>\n\n<pre><code>Exception\":{\"ErrorId\":\"LibraryException\",\"ErrorCode\":\"1000\",\"ExceptionType\":\"ModuleException\",\"Message\":\"Error 1000: TLC library exception: Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown.\",\"Exception\":{\"Library\":\"TLC\",\"ExceptionType\":\"LibraryException\",\"Message\":\"Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown.\"}}}Error: Error 1000: TLC library exception: Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown. Process exited with error code -2\n<\/code><\/pre>\n\n<p>Lastly my setup is <a href=\"https:\/\/i.stack.imgur.com\/PBN9L.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PBN9L.png\" alt=\"Azure Machine Learning Setup\"><\/a> <\/p>\n\n<p>Thanks for the help!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1502257048253,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1502686418056,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45582412",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":14.9,
        "Challenge_reading_time":41.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":9.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"How to build a Convolution Neural Net in Azure Machine Learning?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1268.0,
        "Challenge_word_count":393,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1418505926276,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Missouri",
        "Poster_reputation_count":1454.0,
        "Poster_view_count":328.0,
        "Solution_body":"<p>The correct network definition for 35-column length input with given kernels and strides would be following:<\/p>\n\n<pre><code>const { T = true; F = false; }\n\ninput Data [35];\n\nhidden C1 [7, 15]\n  from Data convolve {\n    InputShape  = [35];\n    KernelShape = [7];\n    Stride      = [2];\n    MapCount = 7;\n  }\n\nhidden C2 [14, 7, 5]\n   from C1 convolve {\n     InputShape  = [ 7, 15];\n     KernelShape = [ 1,  7];\n     Stride      = [ 1,  2];\n     Sharing     = [ F,  T];\n     MapCount = 14;\n  }\n\nhidden H3 [100]\n  from C2 all;\n\noutput Result [1] linear\n  from H3 all;\n<\/code><\/pre>\n\n<p>First, the C1 = [7,15]. The first dimension is simply the MapCount. For the second dimension, the kernel shape defines the length of the \"window\" that's used to scan the input columns, and the stride defines how much it moves at each step. So the kernel windows would cover columns 1-7, 3-9, 5-11,...,29-35, yielding the second dimension of 15 when you tally the windows.<\/p>\n\n<p>Next, the C2 = [14,7,5]. The first dimension is again the MapCount. For the second and third dimension, the 1-by-7 kernel \"window\" has to cover the input size of 7-by-15, using steps of 1 and 2 along corresponding dimensions. <\/p>\n\n<p>Note that you could specify C2 hidden layer shape of [98,5] or even [490], if you wanted to flatten the outputs. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.5,
        "Solution_reading_time":14.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":199.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1399995590030,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Dubai, UAE",
        "Answerer_reputation_count":4682.0,
        "Answerer_view_count":650.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>It is my first time to use Azure Machine Learning...<\/p>\n\n<p>When I have trained 2 models using the same training data and testing data, when it comes to evaluate model, it shows error<\/p>\n\n<blockquote>\n  <p>All models must have the same learner type<\/p>\n<\/blockquote>\n\n<p>Do you know what is \"learner type\" of machine learning models and how to tell the learner type of a model?<\/p>\n\n<p>Below is the screenshot of my basic practice on Azure Machine Learning:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/plx4V.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/plx4V.png\" alt=\"Azure Machine Learning practice\"><\/a><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1461225939707,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36763479",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":9.4,
        "Challenge_reading_time":8.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"How to tell the learner type of machine learning models",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":516.0,
        "Challenge_word_count":94,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361240380856,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3349.0,
        "Poster_view_count":465.0,
        "Solution_body":"<p>The models you compare should be of the same type - binary classification, regression, multi-class classification etc. For example, you can't compare effectiveness of linear regression to the effectiveness of logistics regression. They solve absolutely different tasks.<\/p>\n\n<p>This is the case for you - you try to compare linear regression (which outputs real value) with the multiclass decision forest, which tries to classify input to some class.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.0,
        "Solution_reading_time":5.73,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":65.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1568318861627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":486.0,
        "Answerer_view_count":75.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm working on a credit fraud dataset on sagemaker. I'm using a linear learner binary classification algorithm. I divided the data into training and test sets and got the results for test set. When I tried to evaluate model performance characteristics on training set, I'm getting the following error<\/p>\n\n<pre><code> An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error \n(400) from model with message \"unable to evaluate payload provided\".\n<\/code><\/pre>\n\n<p>I mentioned the code below<\/p>\n\n<pre><code>train_data = 's3:\/\/{}\/{}\/{}'.format(bucket, prefix, 'train\/examples') #making train_data\n\n#making test_data\ntest_key = \"{}\/test\/examples\".format(prefix)\ns3.Bucket(bucket).download_file(test_key, 'test_data')\n\n#preparing train channels for training the data\ntrain_channel = sagemaker.session.s3_input(train_data, content_type='text\/csv')\n\n#training the data\nlinear.fit(inputs=train_channel,  logs=True)\n\n#creating the endpoint\nlinear_predictor = linear.deploy(initial_instance_count=1,\n                             instance_type='ml.m4.xlarge')\n\n#getting the results on test_data\nl = []\nwith open('test_data', 'r') as f:\nfor j in range(0,56962):\n    single_test = f.readline()\n    result = linear_predictor.predict(single_test)\n    l.append(result)\n    if j%10000 ==0 :\n        print(j)\nprint(l[0:10])\n\n#getting the results on train_data\n#THE CODE BELOW IS THROWING THE ABOVE MENTIONED ERROR\nq =[]\nwith open('train_data', 'r') as f:\nfor j in range(0,56962):\n    single_test = f.readline()\n    result = linear_predictor.predict(single_test)\n    q.append(result)\n    if j%10000 ==0 :\n        print(j)\nprint(q[0:10])\n<\/code><\/pre>\n\n<p>I'm getting the results on test data. I stored it in list l. For getting the results on the training set, I followed the similar procedure, but I'm getting the above mentioned error. Can someone please offer a resolution for this?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1573477497977,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1573548102688,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58801976",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":24.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"Evaluating payload provided when calling invoke endpoint operation",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1769.0,
        "Challenge_word_count":219,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1568318861627,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hyderabad, Telangana, India",
        "Poster_reputation_count":486.0,
        "Poster_view_count":75.0,
        "Solution_body":"<p>\"unable to evaluate payload provided\" occurs only when the input data format is not compatible with the ML model you created. In this case, to get results on training set, we need to remove the last column(label column) before passing it to endpoint<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.7,
        "Solution_reading_time":3.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":43.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"Even with auto Ml, should carefully custom split my data to my satisfaction or just leave it to AutoML?\n\n\u00a0\n\nAnd what difference does it make?",
        "Challenge_closed_time":1626243.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625459160000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Should-I-custom-split-my-image-data\/m-p\/163031#M10",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":4.9,
        "Challenge_reading_time":2.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Should I custom split my image data?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":489.0,
        "Challenge_word_count":31,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi Ayoola\n\nIf your data is large enough and have wide representation of each category, you may go with the automated split in AutoML. That would save time and perform well.\n\nIf you have some specific needs, such as the representation of certain observations in a specific category is important and limited within the data, you may want to make sure that it is well distributed for validation and test. And custom split would help for that. Another reason of using custom split could be for comparison of your model performance with external models so you use exactly the same training\/test datasets and make an apples to apples comparison.\n\nHere are some tips I find useful in this doc:\n\nhttps:\/\/cloud.google.com\/vision\/automl\/docs\/beginners-guide#distribute_examples_equally_across_categ...\n\nCheers\n\nTuba.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.5,
        "Solution_reading_time":10.36,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":127.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1645048145367,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"S\u00e3o Paulo, Brazil",
        "Answerer_reputation_count":146.0,
        "Answerer_view_count":18.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently training some models via Googles AutoML feature contained within their Vertex AI products.<\/p>\n<p>The normal pipeline is creating a dataset, which I do by creating a table in Bigquery, and then starting the training process.<\/p>\n<p>This has normally worked before but for my latest dataset I get the following error message:<\/p>\n<blockquote>\n<p>Training pipeline failed with error message: The size of source BigQuery table is larger than 107374182400 bytes.<\/p>\n<\/blockquote>\n<p>While it seemed unlikely to me that the table is actually too large for AutoML, I tried re-training on a new dataset that's a 50% sample of the original table but the same error occured.<\/p>\n<p>Is my dataset really to large for AutoML to handle or is there another issue?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658768743937,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73112914",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":10.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Google Vertex AI fails AutoML training due to large BigQuery dataset being too large",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":61.0,
        "Challenge_word_count":136,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1455621115903,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":1179.0,
        "Poster_view_count":92.0,
        "Solution_body":"<p>There are some perspectives of limits for AutoML Tables -- not only size in bytes (100GB as maximum supported size), but also number of rows (~200bi lines) and number of columns (up to 1000 columns).<\/p>\n<p>You can find more details on <a href=\"https:\/\/cloud.google.com\/automl-tables\/docs\/quotas#limits\" rel=\"nofollow noreferrer\">AutoML Tables limits<\/a> documentation.<\/p>\n<p>Is your source data within those limits?<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.9,
        "Solution_reading_time":5.44,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am very new to SageMaker. Upon my first interaction, it looks like the AWS SageMaker requires you to start from its Notebook. I have a training set which is ready. Is there a way to bypass setting the Notebook and just to start by upload the training set? Or it should be done through the Notebook. If anyone knows some example fitting my need above, that will be great. <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1520498020543,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49168673",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.4,
        "Challenge_reading_time":5.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How to load a training set in AWS SageMaker to build a model?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":700.0,
        "Challenge_word_count":83,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1305269513436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":10317.0,
        "Poster_view_count":595.0,
        "Solution_body":"<p>Amazon SageMaker is a combination of multiple services that each is independent of the others. You can use the notebook instances if you want to develop your models in the familiar Jupyter environment. But if just need to train a model, you can use the training jobs without opening a notebook instance. <\/p>\n\n<p>There a few ways to launch a training job:<\/p>\n\n<ul>\n<li>Use the high-level SDK for Python that is similar to the way that you start a training step in your python code<\/li>\n<\/ul>\n\n<p><code>kmeans.fit(kmeans.record_set(train_set[0]))<\/code><\/p>\n\n<p>Here is the link to the python library: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk<\/a><\/p>\n\n<ul>\n<li>Use the low-level API to Create-Training-Job, and you can do that using various SDK (Java, Python, JavaScript, C#...) or the CLI. <\/li>\n<\/ul>\n\n<p><code>sagemaker = boto3.client('sagemaker')\n sagemaker.create_training_job(**create_training_params)<\/code><\/p>\n\n<p>Here is a link to the documentation on these options: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html<\/a> <\/p>\n\n<ul>\n<li>Use Spark interface to launch it using a similar interface to creating an MLLib training job<\/li>\n<\/ul>\n\n<p><code>val estimator = new KMeansSageMakerEstimator(\n  sagemakerRole = IAMRole(roleArn),\n  trainingInstanceType = \"ml.p2.xlarge\",\n  trainingInstanceCount = 1,\n  endpointInstanceType = \"ml.c4.xlarge\",\n  endpointInitialInstanceCount = 1)\n  .setK(10).setFeatureDim(784)<\/code><\/p>\n\n<p><code>val model = estimator.fit(trainingData)<\/code><\/p>\n\n<p>Here is a link to the spark-sagemaker library: <a href=\"https:\/\/github.com\/aws\/sagemaker-spark\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-spark<\/a><\/p>\n\n<ul>\n<li>Create a training job in the Amazon SageMaker console using the wizard there: <a href=\"https:\/\/console.aws.amazon.com\/sagemaker\/home?region=us-east-1#\/jobs\" rel=\"nofollow noreferrer\">https:\/\/console.aws.amazon.com\/sagemaker\/home?region=us-east-1#\/jobs<\/a><\/li>\n<\/ul>\n\n<p>Please note that there a few options also to train models, either using the built-in algorithms such as K-Means, Linear Learner or XGBoost (see here for the complete list: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html<\/a>). But you can also bring your own models for pre-baked Docker images such as TensorFlow (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/tf.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/tf.html<\/a>) or MXNet (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet.html<\/a>), your own Docker image (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo.html<\/a>).  <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":16.0,
        "Solution_readability":16.7,
        "Solution_reading_time":42.09,
        "Solution_score_count":1.0,
        "Solution_sentence_count":27.0,
        "Solution_word_count":275.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1221810788500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paderborn, North-Rhine-Westphalia, Germany",
        "Answerer_reputation_count":68522.0,
        "Answerer_view_count":7896.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>It is not clear to me if one could use mlflow to serve a model that is evolving continuously based on its previous predictions.<\/p>\n<p>I need to be able to query a model in order to make a prediction on a sample of data which is the basic use of mlflow serve. However I also want the model to be updated internaly now that it has seen new data.<\/p>\n<p>Is it possible or does it need a FR ?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616753577110,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66814885",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":5.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Serve online learning models with mlflow",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":360.0,
        "Challenge_word_count":84,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1515156959092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Fairbanks, AK, United States",
        "Poster_reputation_count":76.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>I think that you should be able to do that by implementing the custom python model or custom flavor, as it's described in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">documentation<\/a>.  In this case you need to create a class that is inherited from <code>mlflow.pyfunc.PythonModel<\/code>, and implement the <code>predict<\/code> method, and inside that method you're free to do anything.  Here is just simple example from documentation:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class AddN(mlflow.pyfunc.PythonModel):\n\n    def __init__(self, n):\n        self.n = n\n\n    def predict(self, context, model_input):\n        return model_input.apply(lambda column: column + self.n)\n<\/code><\/pre>\n<p>and this model is then could be saved &amp; loaded again just as normal models:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># Construct and save the model\nmodel_path = &quot;add_n_model&quot;\nadd5_model = AddN(n=5)\nmlflow.pyfunc.save_model(path=model_path, python_model=add5_model)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(model_path)\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.9,
        "Solution_reading_time":14.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":120.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1499498135632,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":675.0,
        "Answerer_view_count":26.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We are trying to execute and check what kind of output is provided by Predictive Maintenance Using Machine Learning on AWS sample data. We are referring <a href=\"https:\/\/aws.amazon.com\/solutions\/predictive-maintenance-using-machine-learning\/\" rel=\"nofollow noreferrer\">Predictive Maintenance Using Machine Learning<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/solutions\/latest\/predictive-maintenance-using-machine-learning\/welcome.html\" rel=\"nofollow noreferrer\">AWS Guide<\/a> to launch the sample template provided by the AWS. The template is executed properly and we can see the resources in account. Whenever we run the sagemaker notebook for the given example we are getting the error in CloudWatch logs as follows<\/p>\n\n<pre><code>ImportError: cannot import name 'replace_file' on line from mxnet.gluon.utils import download, check_sha1, _get_repo_file_url, replace_file.\n<\/code><\/pre>\n\n<p>This is the stage where the invoke the training job. We have tried following options to resolve the issue.<\/p>\n\n<ul>\n<li>Upgrading the mxnet module<\/li>\n<li>Upgrading the tensorflow module<\/li>\n<\/ul>\n\n<p>But no success.<\/p>\n\n<p>Thanks in advance.<\/p>\n\n<p>Error Traceback is as follows<\/p>\n\n<pre><code>  File \"\/usr\/lib\/python3.5\/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"\/usr\/lib\/python3.5\/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"\/opt\/ml\/code\/sagemaker_predictive_maintenance_entry_point.py\", line 10, in &lt;module&gt;\n    import gluonnlp\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gluonnlp\/__init__.py\", line 25, in &lt;module&gt;\n    from . import data\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gluonnlp\/data\/__init__.py\", line 23, in &lt;module&gt;\n    from . import (batchify, candidate_sampler, conll, corpora, dataloader,\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gluonnlp\/data\/question_answering.py\", line 31, in &lt;module&gt;\n    from mxnet.gluon.utils import download, check_sha1, _get_repo_file_url, replace_file\n    ImportError: cannot import name 'replace_file'\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1587652058893,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61389632",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":27.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":null,
        "Challenge_title":"Import error while Executing AWS Predictive Maintenance Using Machine Learning Sample",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":398.0,
        "Challenge_word_count":213,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1473770138816,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>A fix for this issue is being deployed to the official solution. In the meantime, you can make the changes described <a href=\"https:\/\/github.com\/awslabs\/predictive-maintenance-using-machine-learning\/pull\/7\/files\" rel=\"nofollow noreferrer\">here<\/a> in your SageMaker environment by following the instructions below:<\/p>\n\n<p>1) In the notebook, please change the <code>framework_version<\/code> to <code>1.6.0<\/code>.<\/p>\n\n<pre><code>MXNet(entry_point='sagemaker_predictive_maintenance_entry_point.py',\n          source_dir='sagemaker_predictive_maintenance_entry_point',\n          py_version='py3',\n          role=role, \n          train_instance_count=1, \n          train_instance_type=train_instance_type,\n          output_path=output_location,\n          hyperparameters={'num-datasets' : len(train_df),\n                           'num-gpus': 1,\n                           'epochs': 500,\n                           'optimizer': 'adam',\n                           'batch-size':1,\n                           'log-interval': 100},\n         input_mode='File',\n         train_max_run=7200,\n         framework_version='1.6.0')  &lt;- Change this to 1.6.0.\n<\/code><\/pre>\n\n<p>2) This will likely fix things, but just to be sure you don't have any stale packages, change the <code>requirements.txt<\/code> file as well.<\/p>\n\n<p>You'll need to open up a terminal in SageMaker.\n<a href=\"https:\/\/i.stack.imgur.com\/0Vn6l.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0Vn6l.png\" alt=\"enter image description here\"><\/a>\nimage taken from <a href=\"https:\/\/medium.com\/swlh\/jupyter-notebook-on-amazon-sagemaker-getting-started-55489f500439\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/swlh\/jupyter-notebook-on-amazon-sagemaker-getting-started-55489f500439<\/a><\/p>\n\n<p>and run<\/p>\n\n<pre><code>cd SageMaker\/sagemaker_predictive_maintenance_entry_point\/\nsudo vim requirements.txt  # (or sudo nano requirements.txt)\n<\/code><\/pre>\n\n<p>Change the contents to:<\/p>\n\n<pre><code>gluonnlp==0.9.1\npandas==0.22\n<\/code><\/pre>\n\n<p>Save it, and then run the example again.<\/p>\n\n<p>Feel free to comment on the issue as well:\n<a href=\"https:\/\/github.com\/awslabs\/predictive-maintenance-using-machine-learning\/issues\/6\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/predictive-maintenance-using-machine-learning\/issues\/6<\/a> <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":16.4,
        "Solution_reading_time":28.12,
        "Solution_score_count":2.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":160.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1221810788500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paderborn, North-Rhine-Westphalia, Germany",
        "Answerer_reputation_count":68522.0,
        "Answerer_view_count":7896.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I can load a specific version of a model using the mlflow client:<\/p>\n<pre><code>import mlflow\n\nmodel_version = 1\n\nmodel = mlflow.pyfunc.load_model(\n    model_uri=f&quot;models:\/c3760a15e6ac48f88ad7e5af940047d4\/{model_version}&quot;\n)\n<\/code><\/pre>\n<p>But is there a way to load the latest model version?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637843541287,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1637850687888,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70111193",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":4.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I load the latest model version from MLflow model registry?",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1873.0,
        "Challenge_word_count":43,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351080779276,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Leuven, Belgium",
        "Poster_reputation_count":3126.0,
        "Poster_view_count":262.0,
        "Solution_body":"<p>There is no such thing, like load <code>latest<\/code>, but:<\/p>\n<ul>\n<li>You can specify the stage (<code>staging<\/code>, <code>production<\/code>) - see <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/concepts.html#referencing-artifacts\" rel=\"nofollow noreferrer\">docs<\/a><\/li>\n<li>You can find latest version using the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.get_latest_versions\" rel=\"nofollow noreferrer\">get_latest_versions<\/a> function - but it will also return latest per stage<\/li>\n<\/ul>\n<p>So you need to define what <code>latest<\/code> means for you.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.3,
        "Solution_reading_time":8.4,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":53.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>After applying Principal Component Analysis PCA to my data set in order to achieve better model accuracy. The 13 features dimensions, I am reducing it to 10 features using PCA. Everything is fine till here.    <\/p>\n<p>After implementing the model in WebApp, it is building &amp; seems fine in the studio.    <\/p>\n<p>In the testing phase of model prediction, Instead of displaying 10 features as an input, the UI system is showing the original features which is 13 &amp; the output is showing 10 featu<a href=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/70351-01.pdf?platform=QnA\">70351-01.pdf<\/a>res which does not have any feature names for the newly generated features which are 10. And also prediction is not working at all after executing it.\\    <\/p>\n<p>Attached are the screenshots, Please refer.    <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613865635337,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/281600\/error-webapp-implementation-with-principal-compone",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":11.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Error - WebApp Implementation with Principal Component Analysis(PCA)",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":126,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@RakshitSidd-7739 After updating and running the training experiment did you try to update the prediction experiment.     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/70512-image.png?platform=QnA\" alt=\"70512-image.png\" \/>    <\/p>\n<p>After the prediction experiment is updated you can update the web service and check if it shows up correctly.     <\/p>\n",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.5,
        "Solution_reading_time":4.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":41.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a training script in Sagemaker like,<\/p>\n\n<pre><code>def train(current_host, hosts, num_cpus, num_gpus, channel_input_dirs, model_dir, hyperparameters, **kwargs):\n    ... Train a network ...\n    return net\n\ndef save(net, model_dir):\n    # save the model\n    logging.info('Saving model')\n    y = net(mx.sym.var('data'))\n    y.save('%s\/model.json' % model_dir)\n    net.collect_params().save('%s\/model.params' % model_dir)\n\ndef model_fn(model_dir):\n    symbol = mx.sym.load('%s\/model.json' % model_dir)\n    outputs = mx.symbol.softmax(data=symbol, name='softmax_label')\n    inputs = mx.sym.var('data')\n    param_dict = gluon.ParameterDict('model_')\n    net = gluon.SymbolBlock(outputs, inputs, param_dict)\n    net.load_params('%s\/model.params' % model_dir, ctx=mx.cpu())\n    return net\n<\/code><\/pre>\n\n<p>Most of which I stole from the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mnist.py\" rel=\"nofollow noreferrer\">MNIST Example<\/a>.<\/p>\n\n<p>When I train, everything goes fine, but when trying to deploy like,<\/p>\n\n<pre><code>m = MXNet(\"lstm_trainer.py\", \n          role=role, \n          train_instance_count=1, \n          train_instance_type=\"ml.c4.xlarge\",\n          hyperparameters={'batch_size': 100, \n                         'epochs': 20, \n                         'learning_rate': 0.1, \n                         'momentum': 0.9, \n                         'log_interval': 100})\nm.fit(inputs) # No errors\npredictor = m.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>I get, (<a href=\"https:\/\/gist.github.com\/aidan-plenert-macdonald\/7eb7ba7402790b61596938b5cbf605b6\" rel=\"nofollow noreferrer\">full output<\/a>)<\/p>\n\n<pre><code>INFO:sagemaker:Creating model with name: sagemaker-mxnet-py2-cpu-2018-01-17-20-52-52-599\n---------------------------------------------------------------------------\n  ... Stack dump ...\nClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not find model data at s3:\/\/sagemaker-us-west-2-01234567890\/sagemaker-mxnet-py2-cpu-2018-01-17-20-52-52-599\/output\/model.tar.gz.\n<\/code><\/pre>\n\n<p>Looking in my S3 bucket <code>s3:\/\/sagemaker-us-west-2-01234567890\/sagemaker-mxnet-py2-cpu-2018-01-17-20-52-52-599\/output\/model.tar.gz<\/code>, I in fact don't see the model.<\/p>\n\n<p>What am I missing?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1516224347000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48310237",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":16.3,
        "Challenge_reading_time":30.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker \"Could not find model data\" when trying to deploy my model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2950.0,
        "Challenge_word_count":164,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1384712661608,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Diego, CA, United States",
        "Poster_reputation_count":5151.0,
        "Poster_view_count":1038.0,
        "Solution_body":"<p>When you are calling the training job you should specify the output directory:<\/p>\n\n<pre><code>#Bucket location where results of model training are saved.\nmodel_artifacts_location = 's3:\/\/&lt;bucket-name&gt;\/artifacts'\n\nm = MXNet(entry_point='lstm_trainer.py',\n          role=role,\n          output_path=model_artifacts_location,\n          ...)\n<\/code><\/pre>\n\n<p>If you don't specify the output directory the function will use a default location, that it might not have the permissions to create or write to.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.6,
        "Solution_reading_time":6.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":47.2837916667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a model using sagemaker (on aws ml notebook). \nI then exported that model to s3 and a <code>.tar.gz<\/code> file was created there.<\/p>\n\n<p>Im trying to find a way to load the model object to memory in my code (without using AWS docker images and deployment) and run a prediction on it.<\/p>\n\n<p>I looked for functions to do that in the model section of the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html#\" rel=\"nofollow noreferrer\">sagemaker docs<\/a>, but everything there is tightly coupled to the AWS docker images.<\/p>\n\n<p>I then tried opening the file with <code>tarfile<\/code> and <code>shutil<\/code> packages but that was useless.<\/p>\n\n<p>Any ideas?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1562675762410,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56952741",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.1,
        "Challenge_reading_time":9.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker export and load model to memory",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1838.0,
        "Challenge_word_count":106,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1480290282627,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2906.0,
        "Poster_view_count":266.0,
        "Solution_body":"<p>With the exception of XGBoost, built-in algorithms are implemented with Apache MXNet, so simply extract the model from the .tar.gz file and load it with MXNet: load_checkpoint() is the API to use.<\/p>\n\n<p>XGBoost models are just pickled objects. Unpickle and load in sklearn:<\/p>\n\n<pre><code>$ python3\n&gt;&gt;&gt; import sklearn, pickle\n&gt;&gt;&gt; model = pickle.load(open(\"xgboost-model\", \"rb\"))\n&gt;&gt;&gt; type(model)\n&lt;class 'xgboost.core.Booster'&gt;\n<\/code><\/pre>\n\n<p>Models trained with built-in library (Tensorflow, MXNet, Pytorch, etc.) are vanilla models that can be loaded as-is with the correct library.<\/p>\n\n<p>Hope this helps.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1562845984060,
        "Solution_link_count":0.0,
        "Solution_readability":7.3,
        "Solution_reading_time":8.33,
        "Solution_score_count":3.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":82.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1588516515763,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"UK",
        "Answerer_reputation_count":29087.0,
        "Answerer_view_count":3080.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following <a href=\"https:\/\/github.com\/mtm12\/SageMakerDemo\" rel=\"noreferrer\">this example<\/a> on how to train a machine learning model in Amazon-sagemaker.<\/p>\n<pre><code>data_location = 's3:\/\/{}\/kmeans_highlevel_example\/data'.format(bucket)\noutput_location = 's3:\/\/{}\/kmeans_highlevel_example\/output'.format(bucket)\n\nprint('training data will be uploaded to: {}'.format(data_location))\nprint('training artifacts will be uploaded to: {}'.format(output_location))\n\nkmeans = KMeans(role=role,\n                train_instance_count=2,\n                train_instance_type='ml.c4.8xlarge',\n                output_path=output_location,\n                k=10,\n                epochs=100,\n                data_location=data_location)\n<\/code><\/pre>\n<p>So after calling the fit function the model should be saved in the S3 bucket?? How can you load this model next time?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595162197333,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62980380",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.4,
        "Challenge_reading_time":10.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"How to load trained model in amazon sagemaker?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":4776.0,
        "Challenge_word_count":74,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1310821356880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Slovenia",
        "Poster_reputation_count":14913.0,
        "Poster_view_count":1093.0,
        "Solution_body":"<p>This can be done by using the sagemaker library combined with the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html\" rel=\"noreferrer\">Inference Model<\/a>.<\/p>\n<pre><code>model = sagemaker.model.Model(\n    image=image\n    model_data='s3:\/\/bucket\/model.tar.gz',\n    role=role_arn)\n<\/code><\/pre>\n<p>The options you're passing in are:<\/p>\n<ul>\n<li><code>image<\/code> - This is the ECR image you're using for inference (which should be for the algorithm you're trying to use). Paths are available <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html\" rel=\"noreferrer\">here<\/a>.<\/li>\n<li><code>model_data<\/code> - This is the path of where your model is stored (in a <code>tar.gz<\/code> compressed archive).<\/li>\n<li><code>role<\/code> - This is the arn of a role that is capable of both pulling the image from ECR and getting the s3 archive.<\/li>\n<\/ul>\n<p>Once you've successfully done this you will need to setup an endpoint, this can be done by performing the following in your notebook through the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html#sagemaker.model.Model.deploy\" rel=\"noreferrer\">deploy function<\/a>.<\/p>\n<pre><code>model.deploy(\n   initial_instance_count=1,\n   instance_type='ml.p2.xlarge'\n)\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.4,
        "Solution_reading_time":17.2,
        "Solution_score_count":8.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":128.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1550233102176,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":621.0,
        "Answerer_view_count":103.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to save a model to MLFlow, but as I have a custom prediction pipeline to retrieve data, I need to save extra metadata into the model.<\/p>\n<p>I tried using my custom signature class, which It does the job correctly and saves the model with the extra metadata in the MLModel file (YAML format). But when want to load the model from the MLFlow registry, the signature is not easy accesible.<\/p>\n<pre><code>mlflow.sklearn.log_model(model, &quot;model&quot;, signature = signature)\n<\/code><\/pre>\n<p>I've also tried to save an extra dictionary at the log_model function, but it saves it in the conda.yaml file:<\/p>\n<pre><code>mlflow.sklearn.log_model(model, &quot;model&quot;, {&quot;metadata1&quot;:&quot;value1&quot;, &quot;metadata2&quot;:&quot;value2&quot;})\n<\/code><\/pre>\n<p>Should I make my own flavour? Or my own Model inheritance? I've seen <a href=\"https:\/\/github1s.com\/mlflow\/mlflow\/blob\/HEAD\/mlflow\/pyfunc\/__init__.py\" rel=\"nofollow noreferrer\">here<\/a> that the PyFuncModel recieves some metadata class and an implementation to solve this, but I don't know where should I pass my own implementations to PyFuncModel on an experiment script. Here's a minimal example:<\/p>\n<pre><code>import mlflow\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\nmetadata_dic = {&quot;metadata1&quot;: &quot;value1&quot;, \n                &quot;metadata2&quot;: &quot;value2&quot;}\n\nX = np.array([[-2, -1, 0, 1, 2, 1],[-2, -1, 0, 1, 2, 1]]).T\ny = np.array([0, 0, 1, 1, 1, 0])\n\nX = pd.DataFrame(X, columns=[&quot;X1&quot;, &quot;X2&quot;])\ny = pd.DataFrame(y, columns=[&quot;y&quot;])\n\n\nmodel = LogisticRegression()\nmodel.fit(X, y)\n\nmlflow.sklearn.log_model(model, &quot;model&quot;)\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638276045390,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70169519",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.9,
        "Challenge_reading_time":22.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I save more metadata on an MLFlow model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":323.0,
        "Challenge_word_count":215,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1550233102176,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":621.0,
        "Poster_view_count":103.0,
        "Solution_body":"<p>Finally, I made a class that contains every metadata and saved it as an model argument:<\/p>\n<pre><code>model = LogisticRegression()\nmodel.fit(X, y)\nmodel.metadata = ModelMetadata(**metadata_dic)\nmlflow.sklearn.log_model(model, &quot;model&quot;)\n<\/code><\/pre>\n<p>Here I lost the customizable <code>predict<\/code> process, but after reading the <code>MLFlow<\/code> documentation is not very clear how to proceed.<\/p>\n<p>If anyone finds a good approach It would be very appreciated.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.2,
        "Solution_reading_time":6.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":56.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1342709052703,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"D\u00fcsseldorf, Germany",
        "Answerer_reputation_count":1889.0,
        "Answerer_view_count":654.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained the following Sagemaker model: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco<\/a><\/p>\n\n<p>I've tried both the JSON and RecordIO version. In both, the algorithm is tested on ONE sample image. However, I have a dataset of 2000 pictures, which I would like to test. I have saved the 2000 jpg pictures in a folder within an S3 bucket and I also have two .mat files (pics + ground truth). How can I apply this model to all 2000 pictures at once and then save the results, rather than doing it one picture at a time?<\/p>\n\n<p>I am using the code below to load a single picture from my S3 bucket:<\/p>\n\n<pre><code>object = bucket.Object('pictures\/pic1.jpg')\nobject.download_file('pic1.jpg')\nimg=mpimg.imread('pic1.jpg')\nimg_name = 'pic1.jpg'\nimgplot = plt.imshow(img)\nplt.show(imgplot)\n\nwith open(img_name, 'rb') as image:\n    f = image.read()\n    b = bytearray(f)\n    ne = open('n.txt','wb')\n    ne.write(b)\n\nimport json\nobject_detector.content_type = 'image\/jpeg'\nresults = object_detector.predict(b)\ndetections = json.loads(results)\nprint (detections['prediction'])\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539872573950,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52876202",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":18.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"How to bulk test the Sagemaker Object detection model with a .mat dataset or S3 folder of images?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":112.0,
        "Challenge_word_count":157,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1489873508190,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":509.0,
        "Poster_view_count":84.0,
        "Solution_body":"<p>I'm not sure if I understood your question correctly. However, if you want to feed multiple images to the model at once, you can create a multi-dimensional array of images (byte arrays) to feed the model.<\/p>\n\n<p>The code would look something like this.<\/p>\n\n<pre><code>import numpy as np\n...\n\n#  predict_images_list is a Python list of byte arrays\npredict_images = np.stack(predict_images_list)\n\nwith graph.as_default():\n    #  results is an list of typical results you'd get.\n    results = object_detector.predict(predict_images)\n<\/code><\/pre>\n\n<p>But, I'm not sure if it's a good idea to feed 2000 images at once. Better to batch them in 20-30 images at a time and predict. <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":8.31,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":99.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have run automated ML experiments with imbalanced data (10:1, 20:1, sometimes 30:1) and deployed the best models which all showed fantastic results.    <\/p>\n<p>When I looked up the link    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-manage-ml-pitfalls#identify-models-with-imbalanced-data\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-manage-ml-pitfalls#identify-models-with-imbalanced-data<\/a>    <br \/>\n, it says Azure automated ML can properly handle imbalance of up to 20:1.     <br \/>\nI started to wonder where the ratio 20:1 came from.     <\/p>\n<p>As far as I understand, Azure automated ML doesn't use upsampling, downsampling or resampling, and is more focused on a column of weights to make a class more or less important, and a performance metric dealing better with imbalanced data.    <\/p>\n<ul>\n<li> Does this 20:1 come from some theory? or from tons of experiments already conducted?    <\/li>\n<\/ul>\n<p>Azure automated ML shows the result with warning when I use 30:1(or more) imbalanced data, but I still wonder why it is 20:1.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1593407718737,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/40727\/azure-automated-ml(interface)-how-do-models-create",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":13.5,
        "Challenge_reading_time":15.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Automated ML(interface) how do models created from an Automated ML experiment handle Imbalanced Data?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":155,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>In AutoML we use 5% minority class as threshold to classify imbalance\/non-imbalance. This is a heuristic, and is one guideline produced in the Guardrails to the question \u201cAt x% threshold level is the dataset balanced?\u201d. Since it is not possible to absolutely classify imbalance in all cases (depending on the dataset and its size and distribution, 5% or 10% or even higher may mean imbalance, whereas for very large datasets the minority class may have sufficient training samples for model to learn and get a reasonable imbalance-appropriate metric such as weighted AUC or balanced accuracy),  current Guardrails serve the goal of surfacing \u201csubstantial\u201d imbalance to user so the user can take any of the following measures:  <\/p>\n<p>\u2022\tWhen the user knows (either from their knowledge of their own data or from guardrails) that there is imbalance, Automated ML provides an option in the Automated ML config to provide sample weights \u2013 a user-specified weight array where user can specify to weight each sample with a weight. That way they can weigh the minority class more when submitting the data into Automated ML config. We will soon provide weighting option for imbalance classes from within AutoML that will be activated automatically when imbalance is detected.  <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":19.3,
        "Solution_reading_time":15.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":205.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1341441916656,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5985.0,
        "Answerer_view_count":161.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am trying to train a model using Amazon Sagemaker (xgboost: eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com\/xgboost:latest'). But I always get the same error message shortly after starting the training job:<\/p>\n\n<blockquote>\n  <p>\"ClientError: Hidden file found in the data path! Remove that before\n  training.\"<\/p>\n<\/blockquote>\n\n<p>The S3 console shows that output path is empty (I also tried to create a new directory to no avail). Versioning is not enabled for the bucket.<\/p>\n\n<p>Surprisingly, google finds nothing under this error message.<\/p>\n\n<p>I have configured the input and outputs as follows:<\/p>\n\n<pre><code>   \"InputDataConfig\": [\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": \"s3:\/\/{}\/{}-inputdata\/train\".format(s3_utils.bucket, LABEL)\n                }\n            },\n            \"ContentType\": \"csv\",\n            \"CompressionType\": \"None\"\n        },\n        {\n            \"ChannelName\": \"validation\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": \"s3:\/\/{}\/{}-inputdata\/validation\".format(s3_utils.bucket, LABEL)\n                }\n            },\n            \"ContentType\": \"csv\",\n            \"CompressionType\": \"None\"\n        }\n    ],\n    \"OutputDataConfig\": {\n        \"S3OutputPath\": \"s3:\/\/{}\/{}-xgboost-output\".format(s3_utils.bucket, LABEL)        },\n<\/code><\/pre>\n\n<p>The field<\/p>\n\n<pre><code>    \"RoleArn\": role,\n<\/code><\/pre>\n\n<p>where role comes from<\/p>\n\n<pre><code>    from sagemaker import get_execution_role\n    role = get_execution_role()\n<\/code><\/pre>\n\n<p>and is<\/p>\n\n<pre><code>    arn:aws:iam::&lt;ACCOUNT&gt;:role\/service-role\/AmazonSageMaker-ExecutionRole-&lt;HIDDEN&gt;\n<\/code><\/pre>\n\n<p>Here is a screenshot showing the data-path:\n<a href=\"https:\/\/i.stack.imgur.com\/bs8kl.png\" rel=\"nofollow noreferrer\">S3 dashboard view of data-path<\/a>. The two csv files is all there is. In particular, there is no empty \"directory\" which might be what \"hidden file\" could mean.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531339613883,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1531415699343,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51293471",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":13.0,
        "Challenge_reading_time":24.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS Sagemaker - \"Hidden file found in the data path! Remove that before training.\"",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1578.0,
        "Challenge_word_count":191,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1531338135003,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Netherlands",
        "Poster_reputation_count":23.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>Ok, the prefix you set in the <code>S3Uri<\/code> matters here. Based on your screenshot I think your bucket looks something like this (in tree form):<\/p>\n\n<pre><code>s3:\/\/bucket\n\u2514\u2500\u2500 LABEL-inputdata\n    \u251c\u2500\u2500 train.csv\n    \u2514\u2500\u2500 validation.csv\n<\/code><\/pre>\n\n<p>Based on your <code>InputDataConfig<\/code> above, SageMaker has to download it to folders on the filesystem for the <code>xgboost<\/code> training algorithm to run. It does so based on the channel names and on the <code>S3Uri<\/code> prefix you provided. The prefix is chopped off to determine the name of the folder\/file to download to. So, in your example, the <code>train<\/code> channel gets downloaded as:<\/p>\n\n<pre><code>\/opt\/ml\/input\/data\/train\/.csv\n<\/code><\/pre>\n\n<p>Finally, the <code>xgboost<\/code> implementation sees the <code>.csv<\/code> file as a hidden file and complains about it.<\/p>\n\n<p>To get it to work you could rearrange your data in s3 like so...<\/p>\n\n<pre><code>s3:bucket\n\u2514\u2500\u2500 LABEL-inputdata\n    \u251c\u2500\u2500 train\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 data.csv\n    \u2514\u2500\u2500 validation\n        \u2514\u2500\u2500 data.csv\n<\/code><\/pre>\n\n<p>.. and change your input data config to:<\/p>\n\n<pre><code>   \"InputDataConfig\": [\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": \"s3:\/\/{}\/{}-inputdata\/train\/\".format(s3_utils.bucket, LABEL)\n                }\n            },\n            \"ContentType\": \"csv\",\n            \"CompressionType\": \"None\"\n        },\n        {\n            \"ChannelName\": \"validation\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": \"s3:\/\/{}\/{}-inputdata\/validation\/\".format(s3_utils.bucket, LABEL)\n                }\n            },\n            \"ContentType\": \"csv\",\n            \"CompressionType\": \"None\"\n        }\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":20.04,
        "Solution_score_count":5.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":168.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I work with Azure Machine Learning Service for modeling. To track and analyze the result of a binary classification problem, I use a method named <strong>score-classification<\/strong> in <em>azureml.training.tabular.score.scoring<\/em> library. I invoke the method like this:<\/p>\n<pre><code>metrics = score_classification( y_test, y_pred_probs, metrics_names_list, class_labels, train_labels, sample_weight=sample_weights, use_binary=True)\n\n<\/code><\/pre>\n<p>Input arguments are: <\/p>\n<ul>\n<li> <em>y_test<\/em> is an array of 0 and 1. <\/li>\n<li> <em>y_pred<\/em> is an array of float values for each item. <\/li>\n<li> <em>metrics_names_list<\/em> is the list of the name of the metrics I want to calculate:['f1_score_classwise', 'confusion_matrix']. <\/li>\n<li> <em>class_labels<\/em> is a two-item array of [0, 1].<\/li>\n<li> <em>train_labels<\/em> is a two-item list of ['False', 'True']. <\/li>\n<\/ul>\n<p>When it calculates the metrics I sent as <em>metrics_names_list<\/em>, the results are shown in the Azure ML portal in the metrics page. <\/p>\n<p>Confusion matrix is one of the metrics I draw each time. It has a combo box for the representation. This combo box could be set as <strong>Raw<\/strong> to show the number of items for each cell, and <strong>Normalized<\/strong> to show the percentage of the cells.<\/p>\n<p>The problem is that I see float value for the Raw configuration of this matrix! I do not know how to handle this issue? <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/92357b82-b9f4-4cc4-8630-9619d4584bfa?platform=QnA\" alt=\"enter image description here\" \/><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1681125906966,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1213742\/how-to-fix-the-bug-for-float-values-in-confusion-m",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":21.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"How to fix the bug for float values in confusion matrix in Azure ML service?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":223,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=e2b5cca4-3304-4fb3-9c9d-7d0f840d76b8\">@Elahe Dorani  <\/a><\/p>\n<p>Thanks for reaching out to us, I am not very clear about your question, so if I am not in the right way, please let me know. It sounds like the issue you are experiencing is that the confusion matrix is being displayed as float values instead of integers when you select the &quot;Raw&quot; option in the combo box.\nOne possible explanation for this behavior is that the <strong><code>score_classification<\/code><\/strong> function is returning the confusion matrix as a numpy array of float values instead of integers. This could happen if the function is doing some kind of normalization or scaling of the values.<\/p>\n<p>To address this issue, you could try converting the confusion matrix to integers before passing it to the <strong><code>score_classification<\/code><\/strong> function. You can use the numpy <strong><code>round<\/code><\/strong> function to round the float values to the nearest integer:<\/p>\n<pre><code>pythonCopy code\nconfusion_matrix = np.round(confusion_matrix).astype(int)\n<\/code><\/pre>\n<p>Then, when you call the <strong><code>score_classification<\/code><\/strong> function, pass in the rounded confusion matrix instead of the original one.<\/p>\n<p>If this does not work, another option is to modify the <strong><code>score_classification<\/code><\/strong> function to return the confusion matrix as integers instead of floats. You can do this by using the numpy <strong><code>astype<\/code><\/strong> function to convert the matrix to the <strong><code>int<\/code><\/strong> data type:<\/p>\n<pre><code>arduinoCopy code\nconfusion_matrix = confusion_matrix.astype(int)\n<\/code><\/pre>\n<p>I hope this helps.<\/p>\n<p>Regards,\nYutong<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.8,
        "Solution_reading_time":22.39,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":219.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1338385871600,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":118.0,
        "Answerer_view_count":52.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using the Azure ML model available at <a href=\"https:\/\/gallery.azure.ai\/Experiment\/Weather-prediction-model-1\" rel=\"nofollow noreferrer\">https:\/\/gallery.azure.ai\/Experiment\/Weather-prediction-model-1<\/a> to design a prediction mechanism based on temperature and humidity. I haven't done any changes to the existing model and feeding in data from a simulator. The prediction output is stuck at 0.489944100379944. I have taken over 17k samples and still, the prediction is constant at this value. <\/p>\n\n<p>Any help will be highly appreciated.<\/p>\n\n<p><em>N.B. - This is my first stint with ML<\/em><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":10,
        "Challenge_created_time":1524929526273,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50078161",
        "Challenge_link_count":2,
        "Challenge_participation_count":11,
        "Challenge_readability":10.3,
        "Challenge_reading_time":8.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML Prediction Is Constant",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":163.0,
        "Challenge_word_count":80,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1338385871600,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":118.0,
        "Poster_view_count":52.0,
        "Solution_body":"<p>This was caused by the training dataset. The dataset had characters in the humidity and temperature columns. This led to the model expecting characters but operating on floating point numbers. I cleaned the dataset and ensured that there are only floats in the temperature and humidity columns. Then I used this training data for the model and phew!!!! Everything's working now. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.3,
        "Solution_reading_time":4.77,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":62.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1569638810883,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5255.0,
        "Answerer_view_count":306.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am fairly new to AWS and Sagemaker and have decided to follow some of the tutorials Amazon has to familiarize myself with it. I've been following this one (<a href=\"https:\/\/aws.amazon.com\/getting-started\/hands-on\/semantic-content-recommendation-system-amazon-sagemaker\/5\/\" rel=\"nofollow noreferrer\">tutorial<\/a>) and I've realized that it's an older tutorial using Sagemaker v1. I've been able to look up and change whatever is needed for the tutorial to work in v2 but I became stuck at this part for storing the training data in a S3 bucket to deploy the model.<\/p>\n<pre><code>import io\nimport sagemaker.amazon.common as smac\n\nprint('train_features shape = ', predictions.shape)\nprint('train_labels shape = ', labels.shape)\nbuf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(buf, predictions, labels)\nbuf.seek(0)\n\nbucket = BUCKET\nprefix = PREFIX\nkey = 'knn\/train'\nfname = os.path.join(prefix, key)\nprint(fname)\nboto3.resource('s3').Bucket(bucket).Object(fname).upload_fileobj(buf)\ns3_train_data = 's3:\/\/{}\/{}\/{}'.format(bucket, prefix, key)\nprint('uploaded training data location: {}'.format(s3_train_data))\n<\/code><\/pre>\n<p>It returns this error<\/p>\n<pre><code>NameError Traceback (most recent call \nlast)\n&lt;ipython-input-20-9e52dd949332&gt; in &lt;module&gt;\n 3\n 4\n----&gt; 5 print('train_features shape = ', predictions.shape)\n 6 print('train_labels shape = ', labels.shape)\n 7 buf = io.BytesIO()\nNameError: name 'predictions' is not defined\n<\/code><\/pre>\n<p>I'm curious as to why this would have worked in Sagemaker v1 and not v2 if predictions is not defined and if anyone could point me in the right direction for correcting this.<\/p>\n<p>Thanks.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623014565190,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1623040885867,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67863816",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":22.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"semantic content recommendation system with Amazon SageMaker, storing in S3",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":76.0,
        "Challenge_word_count":198,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1623013767550,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>It looks like they've left some of the code out, or changed the terminology and left in predictions by accident. predictions is an object that is defined on this page <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-test-model.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-test-model.html<\/a><\/p>\n<p>You'll have to work out what predictions is in your case.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.4,
        "Solution_reading_time":5.44,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":45.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1367741264647,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":169.0,
        "Answerer_view_count":30.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have used estimator for a pytorch model and have saved the artifacts in s3 bucket. using below code<\/p>\n<pre><code>estimator = PyTorch(\n    entry_point=&quot;train_deploy.py&quot;,\n    source_dir=&quot;code&quot;,\n    role=role,\n    framework_version=&quot;1.3.1&quot;,\n    py_version=&quot;py3&quot;,\n    instance_count=1,  # this script only support distributed training for GPU instances.\n    instance_type=&quot;ml.m5.12xlarge&quot;,\n    output_path=output_path,\n    hyperparameters={\n        &quot;epochs&quot;: 1,\n        &quot;num_labels&quot;: 7,\n        &quot;backend&quot;: &quot;gloo&quot;,\n    },\n    disable_profiler=False, # disable debugger\n)\nestimator.fit({&quot;training&quot;: inputs_train, &quot;testing&quot;: inputs_test})\n<\/code><\/pre>\n<p>The model works well and there are no issues with it. However i would like to re use this model later for inference, how do i do that. i am looking for something like below<\/p>\n<pre><code>estimator = PyTorch.load(input_path = &quot;&lt;xyz&gt;&quot;)\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659506787267,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73216926",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":13.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Load estimator from model artifact in s3 bucket aws",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":28.0,
        "Challenge_word_count":100,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1367741264647,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":169.0,
        "Poster_view_count":30.0,
        "Solution_body":"<p>I was able to solve this by the following steps<\/p>\n<pre><code>model_data=output_path\nfrom sagemaker.pytorch.model import PyTorchModel \n\npytorch_model = PyTorchModel(model_data=model_data,\n                             role=role,\n                             framework_version=&quot;1.3.1&quot;,\n                             source_dir=&quot;code&quot;,\n                             py_version=&quot;py3&quot;,\n                             entry_point=&quot;train_deploy.py&quot;)\n\npredictor = pytorch_model.deploy(initial_instance_count=1, instance_type=&quot;ml.m4.2xlarge&quot;)\npredictor.serializer = sagemaker.serializers.JSONSerializer()\npredictor.deserializer = sagemaker.deserializers.JSONDeserializer()\nresult = predictor.predict(&quot;&lt;text that needs to be predicted&gt;&quot;)\nprint(&quot;predicted class: &quot;, np.argmax(result, axis=1))\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":31.6,
        "Solution_reading_time":9.95,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":42.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to save images that I configure during training to the output bucket in sagemaker.  I've read that all the information that needs to be saved during training goes into the model.tar.gz file.  I've tried saving plots using the model_dir and the output_data_dir to no avail.  The model itself is saved properly, but the additional information is not being stored with it.  I want to reload this additional information (the saved images) during inference but have heard that storing all the information in the model.tar.gz can cause slow inference.  I would love some help.<\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>from sagemaker.pytorch import PyTorch\nestimator = PyTorch(entry_point='XXXXXXXX\/AWS\/mnist.py',\n                    role=role,\n                    py_version='py3',\n                    framework_version='1.8.0',\n                    instance_count=1,\n                    instance_type='ml.c5.xlarge',\n                    output_path='s3:\/\/XXXXX-bucket\/',\n                    )<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code># mnist.py\nimport os\nimport pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nimport argparse\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import ToTensor\nfrom torchvision.io import read_image\nfrom torch import nn\nimport matplotlib.pyplot as plt\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\n\ndef train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X.to(device))\n        loss = loss_fn(pred, y.to(device))\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}\/{size:&gt;5d}]\")\n\n\ndef test_loop(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X.to(device))\n            test_loss += loss_fn(pred, y.to(device)).item()\n            correct += (pred.argmax(1) == y.to(device)).type(torch.float).sum().item()\n\n    test_loss \/= num_batches\n    correct \/= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")\n\n# Initialize the loss function\nif __name__=='__main__':\n    # default to the value in environment variable `SM_MODEL_DIR`. Using args makes the script more portable.\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n\n    args, _ = parser.parse_known_args()\n\n    training_data = datasets.FashionMNIST(\n        root=\"data\",\n        train=True,\n        download=True,\n        transform=ToTensor()\n    )\n\n    test_data = datasets.FashionMNIST(\n        root=\"data\",\n        train=False,\n        download=True,\n        transform=ToTensor()\n    )\n\n    labels_map = {\n        0: \"T-Shirt\",\n        1: \"Trouser\",\n        2: \"Pullover\",\n        3: \"Dress\",\n        4: \"Coat\",\n        5: \"Sandal\",\n        6: \"Shirt\",\n        7: \"Sneaker\",\n        8: \"Bag\",\n        9: \"Ankle Boot\",\n    }\n\n    figure = plt.figure(figsize=(8, 8))\n    cols, rows = 3, 3\n    for i in range(1, cols * rows + 1):\n        sample_idx = torch.randint(len(training_data), size=(1,)).item()\n        img, label = training_data[sample_idx]\n        figure.add_subplot(rows, cols, i)\n        plt.title(labels_map[label])\n        plt.axis(\"off\")\n        plt.imsave(args.output_data_dir+'plot'+str(i)+'.jpg', img.squeeze(), cmap=\"gray\")\n\n    train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n    test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n\n    # Display image and label.\n    train_features, train_labels = next(iter(train_dataloader))\n    print(f\"Feature batch shape: {train_features.size()}\")\n    print(f\"Labels batch shape: {train_labels.size()}\")\n    img = train_features[0].squeeze()\n    label = train_labels[0]\n    plt.imsave(args.output_data_dir+'sample.jpg', img, cmap=\"gray\")\n    print(\"Saved img.\")\n    print(f\"Label: {label}\")\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using {device} device\")\n\n    model = NeuralNetwork().to(device)\n    print(model)\n\n    learning_rate = 1e-3\n    batch_size = 64\n    epochs = 5\n    # ... train `model`, then save it to `model_dir`\n    \n\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n    epochs = 1\n    for t in range(epochs):\n        print(f\"Epoch {t+1}\\n-------------------------------\")\n        train_loop(train_dataloader, model, loss_fn, optimizer)\n        test_loop(test_dataloader, model, loss_fn)\n    print(\"Done!\")\n\n    \n\n\n\n    with open(os.path.join(args.model_dir, 'model.pth'), 'wb') as f:\n        torch.save(model.state_dict(), f)\n        plt.plot([1,2,3,4])\n        plt.ylabel('some numbers')\n        plt.show()\n        plt.savefig('test.jpeg')<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661051216633,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1661051486728,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73431378",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":68.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":63,
        "Challenge_solved_time":null,
        "Challenge_title":"Save images from sagemaker training",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":43.0,
        "Challenge_word_count":478,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1510598465376,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Francisco, CA, United States",
        "Poster_reputation_count":1.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I suspect there is an issue with string concatenation in <code>plt.imsave<\/code> because the environment variable <code>SM_OUTPUT_DATA_DIR<\/code> by default points to <code>\/opt\/ml\/output\/data<\/code> (that's the actual value of <code>args.output_data_dir<\/code>, since you don't pass this parameter) so the outcome is something like <code>\/opt\/ml\/output\/dataplot1.jpg<\/code>. The same happen if you use the <code>model_dir<\/code> in the same way. I'd rather use something like <code>os.path.join<\/code> like you're already doing for the model. <a href=\"https:\/\/nono.ma\/sagemaker-model-dir-output-dir-and-output-data-dir-parameters\" rel=\"nofollow noreferrer\">here<\/a> a nice exaplaination about these folders and environment variables in sagemaker.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.3,
        "Solution_reading_time":9.92,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am building a recommender system using sagemaker's built-in factorisation machine model.<\/p>\n\n<p>My desired result is to have a <strong>rating matrix<\/strong> where I can look up a predicted score by a <strong>user id<\/strong> and an <strong>item id<\/strong>.<\/p>\n\n<p>I understand that there is a <strong>predict API<\/strong> provided by the model: <\/p>\n\n<pre><code>result = fm_predictor.predict(X_test[1000:1010].toarray())\n<\/code><\/pre>\n\n<p>But I am not sure how I can use it to achieve the desired purpose. If I want to know, say, if user#123 is interested in movie#456, how can I use the above API?<\/p>\n\n<p>Reference: \n<a href=\"https:\/\/medium.com\/@julsimon\/building-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker-cedbfc8c93d8\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/@julsimon\/building-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker-cedbfc8c93d8<\/a><\/p>\n\n<p><a href=\"https:\/\/www.slideshare.net\/AmazonWebServices\/building-a-recommender-system-on-aws-aws-summit-sydney-2018\" rel=\"nofollow noreferrer\">https:\/\/www.slideshare.net\/AmazonWebServices\/building-a-recommender-system-on-aws-aws-summit-sydney-2018<\/a> (p.41,43)<\/p>\n\n<hr>\n\n<p>Updated:<\/p>\n\n<p>I think I understand how to use the API now, you have to build another one-hot encoded dataset as input, for example:<\/p>\n\n<pre><code>X_new = lil_matrix((1, nbFeatures)).astype('float32')\nX_new[0, 935] = 1\nX_new[0, 1600] = 1\n\nprediction2 = X_new[0].toarray()\nresult2 = fm_predictor.predict(prediction2)\n\nprint(result2)\n<\/code><\/pre>\n\n<p>But it seems it would be quite inefficient to fill out the recommendation matrix this way. What would be the best practice?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548212721507,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1548236309036,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54319471",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":14.7,
        "Challenge_reading_time":22.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"Amazon SageMaker factorisation machine rating matrix and endpoint",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":199.0,
        "Challenge_word_count":165,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1394122990683,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":163.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>I think one could think about 2 scenarios:<\/p>\n\n<p>1) if you need very low latency, you can fill up the matrix indeed, i.e. compute all recos for all users, and store it in a key\/value backend queried by your app. You can definitely predict multiple users at a time, using the one-hot encoded technique above.<\/p>\n\n<p>2) predict on-demand by invoking the endpoint directly from the app. This is quite simpler, at the cost of a little latency.<\/p>\n\n<p>Hope this helps.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":5.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1424310919952,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Lynnwood, WA",
        "Answerer_reputation_count":451.0,
        "Answerer_view_count":102.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created an experiment in azure machine learning studio, this experiment is multi-class classification problem using multi-class neural network algorithm, I have also add 'feature hashing' module to transform a stream of English text into a set of features represented as integers. I have successfully run the experiment but when i publish it as web service endpoint i got message \"Reduce the total number of input and output columns to less than 1000 and try publishing again.\"\nI understood after some research that feature hashing convert text into thousands of feature but the problem is how i publish it as web service? and i don't want to remove 'feature hashing' module.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1442415823127,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/32612311",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.8,
        "Challenge_reading_time":9.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Publish azure machine learning service with feature hashing",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":405.0,
        "Challenge_word_count":119,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1422680560156,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Ahmedabad, Gujarat, India",
        "Poster_reputation_count":410.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>It sounds like you are trying to output all those thousands of columns as an output. What you really only need is the scored probability or the scored label. To solve this, just drop all the feature hashed columns from the score model module. To do this add in a project columns module, and tell it to start with \"no columns\" then \"include\" by \"column names\", and just add predicted column (scored probability\/scored label). <\/p>\n\n<p>Then hook up the output of that project columns module to your web service output module. Your web service should now be returning only 1-3 columns rather than thousands.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.7,
        "Solution_reading_time":7.45,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":104.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Using Amazon Sagemaker, I created an Xgboost model. After unpacking the resulting tar.gz file, I end up with a file \"xgboost-model\". <\/p>\n\n<p>The next step will be to upload the model directly from my S3 bucket, without downloading it using <em>pickle<\/em>. Here is what I tried:<\/p>\n\n<pre><code>obj = client.get_object(Bucket='...',Key='xgboost-model')\n\nxgb_model = pkl.load(open((obj['Body'].read())),\"rb\")\n<\/code><\/pre>\n\n<p>But it throws me the error:<\/p>\n\n<pre><code>TypeError: embedded NUL character\n<\/code><\/pre>\n\n<p>Also tried this:<\/p>\n\n<pre><code>xgb_model = pkl.loads(open((obj['Body'].read())),\"rb\")\n<\/code><\/pre>\n\n<p>the outcome was the same.<\/p>\n\n<p>Another approach:<\/p>\n\n<pre><code>bucket='...'\nkey='xgboost-model'\n\nwith s3io.open('s3:\/\/{0}\/{1}'.format(bucket, key),mode='w') as s3_file:\n  pkl.dump(mdl, s3_file)\n<\/code><\/pre>\n\n<p>This giving the error:<\/p>\n\n<pre><code>CertificateError: hostname bucket doesn't match either of '*.s3.amazonaws.com', 's3.amazonaws.com'\n<\/code><\/pre>\n\n<p>This although the bucket is the same.<\/p>\n\n<p>How Can I upload the model in a pickle object so I can then use it it for predictions?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531399214863,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1531400147427,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51305956",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.2,
        "Challenge_reading_time":15.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"S3 read Sagemaker trained model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2742.0,
        "Challenge_word_count":129,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432680790120,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":455.0,
        "Poster_view_count":58.0,
        "Solution_body":"<p>My assumption is you have trained the model using Sagemaker XGBoost built-in algorithm. You would like to use that model and do the predictions in your own hosting environment (not Sagemaker hosting).<\/p>\n\n<p><code>pickle.load(file)<\/code> reads a pickled object from the open file object file and <code>pickle.loads(bytes_object)<\/code> reads a pickled object from a bytes object and returns the deserialized object. Since you have the S3 object already downloaded (into memory) as bytes, you can use <code>pickle.loads<\/code> without using <code>open<\/code><\/p>\n\n<pre><code>xgb_model = pkl.loads(obj['Body'].read())\n<\/code><\/pre>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.8,
        "Solution_reading_time":8.11,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"We should create a instance of MLflow for each project in order to see the experiments related to the current project.\r\n\r\n- [x] Create project operator to deploy a MLFlow instance for each project.\r\n- [x] Update KDL APP API to create the KDLProject custom resource in k8s.\r\n- [x] Update kdlctl.sh adding project-operator docker image building.\r\n- [x] Add project-operator to KDL server helm chart.\r\n- [x] Add github workflows to publish the project-operator in docker hub.",
        "Challenge_closed_time":1623230.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619181390000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/379",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":8.4,
        "Challenge_reading_time":6.3,
        "Challenge_repo_contributor_count":16.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":909.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"All MLflow experiments are visible for any user",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":80,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1476987111723,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Canada",
        "Answerer_reputation_count":2021.0,
        "Answerer_view_count":134.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have 50TB of uncompressed data (images) that is in dozens of tar.gz files in S3. I'm training tensorflow models with a dozen of these tar.gz files at a time. I would like to use a Sagemaker training job to pull this data and unpack it before training. Is this possible? Do I have to change the way that the data is stored before running training?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611852886873,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65941675",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":3.5,
        "Challenge_reading_time":4.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Can gzip tar files be used for training data in Sagemaker?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":433.0,
        "Challenge_word_count":76,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1476987111723,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Canada",
        "Poster_reputation_count":2021.0,
        "Poster_view_count":134.0,
        "Solution_body":"<p><strong>Short Answer<\/strong> : No<\/p>\n<p><strong>Long Answer<\/strong>:\nThe recommended way to use Sagemaker with very large datasets is to use the Pipe API (as opposed to the File Api) which streams data to the training image rather than downloading the data. To take advantage of the Pipe API the data will need to be in one of the supported file types: <strong>text records, TFRecord or Protobuf<\/strong><\/p>\n<p>The benefits are<\/p>\n<ol>\n<li>reducing delay when the container is launched<\/li>\n<li>not needing to scale the instance storage to the size of the training data<\/li>\n<li>increasing throughput by moving most preprocessing before model training<\/li>\n<\/ol>\n<p>References:<\/p>\n<ol>\n<li><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/<\/a><\/li>\n<li><a href=\"https:\/\/julsimon.medium.com\/making-amazon-sagemaker-and-tensorflow-work-for-you-893365184233\" rel=\"nofollow noreferrer\">https:\/\/julsimon.medium.com\/making-amazon-sagemaker-and-tensorflow-work-for-you-893365184233<\/a> (This is a fantastic resource which answers a lot of questions regarding using Sagemaker on very large datasets)<\/li>\n<li><a href=\"https:\/\/julsimon.medium.com\/deep-dive-on-tensorflow-training-with-amazon-sagemaker-and-amazon-s3-12038828075c\" rel=\"nofollow noreferrer\">https:\/\/julsimon.medium.com\/deep-dive-on-tensorflow-training-with-amazon-sagemaker-and-amazon-s3-12038828075c<\/a><\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":20.1,
        "Solution_reading_time":21.09,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":132.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1523264005616,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Yokohama, Kanagawa, Japan",
        "Answerer_reputation_count":1635.0,
        "Answerer_view_count":143.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm pretty new to SageMaker, so I'm sorry if I miss something obvious.<\/p>\n\n<p>I've trained a DL model which uses frames from a video to make a prediction. The current script, that runs in the SageMaker jupyter-notebook, takes a video URL as an input and uses an FFMPEG subprocess pipe to extract the frames and predict them afterwards. This works fine, but now I want to start that script from Lambda.<\/p>\n\n<p>As far as I understood, I could deploy my model with sagemaker and make predictions for every single frame from Lambda, unfortunately this is not an option, as ffprobe, ffmpeg and numpy are too large to fit into the limited lambda space.<\/p>\n\n<p>tl;dr: Is it possible to run my custom script (ffmpeg frame extraction + tensorflow model prediction) as an endpoint in SageMaker?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580859395073,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60067075",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":10.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker deploy custom script",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":386.0,
        "Challenge_word_count":136,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1460444230316,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":404.0,
        "Poster_view_count":127.0,
        "Solution_body":"<p>Sagemaker allows you to use a custom Docker image (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms.html\" rel=\"nofollow noreferrer\">AWS document<\/a>)<\/p>\n\n<blockquote>\n  <p>Build your own custom container image: If there is no pre-built Amazon\n  SageMaker container image that you can use or modify for an advanced\n  scenario, you can package your own script or algorithm to use with\n  Amazon SageMaker.You can use any programming language or framework to\n  develop your container<\/p>\n<\/blockquote>\n\n<ul>\n<li>Create a docker image with your code (FFmpeg, TensorFlow)<\/li>\n<li>Testing the docker container locally<\/li>\n<li>Deploying the image on Amazon ECR (Elastic Container Repository)<\/li>\n<li>Create a SageMaker model and point to the image<\/li>\n<\/ul>\n\n<p>For details, you can learn more from <a href=\"https:\/\/towardsdatascience.com\/brewing-up-custom-ml-models-on-aws-sagemaker-e09b64627722\" rel=\"nofollow noreferrer\">this tutorial<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.9,
        "Solution_reading_time":12.43,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":111.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1459917054448,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Frankfurt, Germany",
        "Answerer_reputation_count":9168.0,
        "Answerer_view_count":675.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created a time series predictor with Keras and  Dockerized the model with with Flash and Gunicorn as per AWS docs. I am loading the serialized model with this code.<\/p>\n\n<pre><code>@classmethod\ndef get_model(cls):\n    if cls.model == None:\n        cls.model = load_model('\/opt\/ml\/bitcoin_model.h5')\n    return cls.model\n<\/code><\/pre>\n\n<p>Then I used the predict method to produce the results , the dockerized container is working perfectly in the local environment , but when I try to host the model in sagemaker it produces this error.<\/p>\n\n<pre><code>ValueError: Tensor Tensor(\"dense_1\/BiasAdd:0\", shape=(?, 1), dtype=float32) is not an element of this graph.\n<\/code><\/pre>\n\n<p>So how can I resolve this issue ?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1541480239330,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1541480860787,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53165953",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":10.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"ValueError: Tensor is not an element of this graph, when hosting a model in Sagemaker with Gunicorn and Flask and Keras",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":223.0,
        "Challenge_word_count":117,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459917054448,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Frankfurt, Germany",
        "Poster_reputation_count":9168.0,
        "Poster_view_count":675.0,
        "Solution_body":"<p>The issue was resolved by calling _make_predict_function() method in the model load phase.<\/p>\n\n<pre><code>@classmethod\ndef get_model(cls):\n    if cls.model == None:\n        cls.model = load_model('\/opt\/ml\/bitcoin_model.h5')\n        cls.model._make_predict_function()\n    return cls.model\n<\/code><\/pre>\n\n<p>Bug Reference : <a href=\"https:\/\/github.com\/keras-team\/keras\/issues\/6462\" rel=\"nofollow noreferrer\">https:\/\/github.com\/keras-team\/keras\/issues\/6462<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.3,
        "Solution_reading_time":5.96,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1595479476676,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Massachusetts, USA",
        "Answerer_reputation_count":246.0,
        "Answerer_view_count":21.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to create an MLOps Pipeline using Azure DevOps and Azure Databricks. From Azure DevOps, I am submitting a Databricks job to a cluster, which trains a Machine Learning Model and saves it into MLFlow Model Registry with a custom flavour (using PyFunc Custom Model).<\/p>\n<p>Now after the job gets over, I want to export this MLFlow Object (with all dependencies - Conda dependencies, two model files - one <code>.pkl<\/code> and one <code>.h5<\/code>, the Python Class with <code>load_context()<\/code> and <code>predict()<\/code> functions defined so that after exporting I can import it and call predict as we do with MLFlow Models).<\/p>\n<p>How do I export this entire MLFlow Model and save it as an AzureDevOps Artifact to be used in the CD phase (where I will deploy it to an AKS cluster with a custom base image)?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629179063687,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68812238",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.4,
        "Challenge_reading_time":11.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How to export a MLFlow Model from Azure Databricks as an Azure DevOps Artifacts for CD phase?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":575.0,
        "Challenge_word_count":151,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>There is no official way to export a Databricks MLflow run from one workspace to another. However, there is an &quot;unofficial&quot; tool that does most of the job with the main limitation being that notebook revisions linked to a run cannot be exported due to lack of a REST API endpoint for this.<\/p>\n<p><a href=\"https:\/\/github.com\/amesar\/mlflow-export-import\" rel=\"nofollow noreferrer\">https:\/\/github.com\/amesar\/mlflow-export-import<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.0,
        "Solution_reading_time":5.74,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":57.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1342709052703,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"D\u00fcsseldorf, Germany",
        "Answerer_reputation_count":1889.0,
        "Answerer_view_count":654.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I was playing with the AWS instances and trying to deploy some locally trained Keras models, but I find no documentation on that. Has anyone already been able to do it? <\/p>\n\n<p>I tried to use a similar approach to <a href=\"https:\/\/aws.amazon.com\/pt\/blogs\/machine-learning\/bring-your-own-pre-trained-mxnet-or-tensorflow-models-into-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/pt\/blogs\/machine-learning\/bring-your-own-pre-trained-mxnet-or-tensorflow-models-into-amazon-sagemaker\/<\/a>, but I had no success. I also found some examples for training keras models in the cloud, but I was not able to get the entry_point + artifacts right. <\/p>\n\n<p>Thanks for your time!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1538586879113,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52632388",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":12.3,
        "Challenge_reading_time":9.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Is it possible to deploy a already trained Keras to Sagemaker?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1291.0,
        "Challenge_word_count":87,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1448029868996,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Portugal",
        "Poster_reputation_count":260.0,
        "Poster_view_count":52.0,
        "Solution_body":"<p>Yes, it is possible, and yes, the official documentation is not much of help.\nHowever, I wrote an <a href=\"https:\/\/gnomezgrave.com\/2018\/07\/05\/using-a-custom-model-for-ml-inference-with-amazon-sagemaker\" rel=\"nofollow noreferrer\">article on that<\/a>, and I hope it will help you.<\/p>\n\n<p>Let me know if you need more details. Cheers!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.7,
        "Solution_reading_time":4.39,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":40.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1540534565632,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":161.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":0.4110722223,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using AWS Sagemaker for model training and deployment, this is sample example for model training <\/p>\n\n<pre><code>from sagemaker.estimator import Estimator\nhyperparameters = {'train-steps': 10}\ninstance_type = 'ml.m4.xlarge'\n\nestimator = Estimator(role=role,\n                      train_instance_count=1,\n                      train_instance_type=instance_type,\n                      image_name=ecr_image,\n                      hyperparameters=hyperparameters)\n\nestimator.fit(data_location)\n<\/code><\/pre>\n\n<p>The docker image mentioned here is a tensorflow system. <\/p>\n\n<p>Suppose it will take 1000 seconds to train the model, now I will increase the instance count to 5 then the training time will increase 5 times i.e. 5000 seconds. As per my understanding the training job will be distributed to 5 machines so ideally it will take 200 seconds per machine but seems its doing separate training on each machine. Can someone please let me know its working over distributed system in general or with Tensorflow.<\/p>\n\n<p>I tried to find out the answer on this documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-dg.pdf\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-dg.pdf<\/a> but seems the way of working on distributed machines is not mentioned here.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1546584216850,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54034172",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.7,
        "Challenge_reading_time":17.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS Sagemaker | Why multiple instances training taking time multiplied to instance number",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1386.0,
        "Challenge_word_count":160,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501403168107,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":1370.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>Are you using <a href=\"https:\/\/www.tensorflow.org\/guide\/estimators\" rel=\"nofollow noreferrer\">TensorFlow estimator APIs<\/a> in your script? If yes, I think you should run the script by wrapping it in <code>sagemaker.tensorflow.TensorFlow<\/code> class as described <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst#training-with-tensorflow-estimator\" rel=\"nofollow noreferrer\">in the documentation here<\/a>. If you run training that way, parallelization and communication between instances should work out-of-the-box.<\/p>\n\n<p>But note that scaling will not be linear when you increase the number of instances. Communicating between instances takes time and there could be non-parallelizable bottlenecks in your script like loading data to memory.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1546585696710,
        "Solution_link_count":2.0,
        "Solution_readability":14.5,
        "Solution_reading_time":10.56,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":84.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1521856385820,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation_count":820.0,
        "Answerer_view_count":165.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>With the MLflow client library, it is possible to transition models through stages as shown below,<\/p>\n<pre><code>client = MlflowClient()\nclient.transition_model_version_stage(\n    name=&quot;sk-learn-random-forest-reg-model&quot;,\n    version=3,\n    stage=&quot;Production&quot;\n)\n<\/code><\/pre>\n<p>Upon doing some testing, I noticed that this does not in any way affect the model(s) that are currently in the stage that the model was transitioned to.<\/p>\n<p>For example, let's say version 2 of a given model is in Production. This will remain tagged as a Production model, even if I were to move version 3 to Production as well.<\/p>\n<p>Is there any way that I can control what happens to models that exist in a stage when making transitions using the above code?<\/p>\n<p>Basically, I only want one version of a model to be in a given stage at a time.<\/p>\n<p>This functionality is available when transitioning models through the Databricks UI,\n<a href=\"https:\/\/i.stack.imgur.com\/aycvC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/aycvC.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656383133240,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72780102",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.6,
        "Challenge_reading_time":14.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Treatment of Existing Models in a Stage When Transitioning Models",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":26.0,
        "Challenge_word_count":156,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521856385820,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sri Lanka",
        "Poster_reputation_count":820.0,
        "Poster_view_count":165.0,
        "Solution_body":"<p>I have missed the <code>archive_existing_versions=True<\/code> that comes with the <code>transition_model_version_stage<\/code> function.<\/p>\n<p>This flag defaults to <code>False<\/code>.<\/p>\n<p>The documentation is available here,\n<br>\n<a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.transition_model_version_stage\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.transition_model_version_stage<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":24.7,
        "Solution_reading_time":7.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":26.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1595479476676,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Massachusetts, USA",
        "Answerer_reputation_count":246.0,
        "Answerer_view_count":21.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use API workflow (python code) to find a model version that has the best metric (for instance, \u201caccuracy\u201d) among several model versions. I understand we can use web UI to do so, but I would love to write python code to achieve this. Could someone help me?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593542659820,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62664183",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":4.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"MLflow: find model version with best metric using python code",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":445.0,
        "Challenge_word_count":60,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1419619351727,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":125.0,
        "Poster_view_count":16.0,
        "Solution_body":"<pre><code>import mlflow \nclient = mlflow.tracking.MlflowClient()\nruns = client.search_runs(&quot;my_experiment_id&quot;, &quot;&quot;, order_by=[&quot;metrics.rmse DESC&quot;], max_results=1)\nbest_run = runs[0]\n<\/code><\/pre>\n<p><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.search_runs\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.search_runs<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":29.4,
        "Solution_reading_time":6.76,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":17.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1463515446747,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"India",
        "Answerer_reputation_count":15335.0,
        "Answerer_view_count":1991.0,
        "Challenge_adjusted_solved_time":4.5067230556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>please be easy on me, I am new to ML. I am sure somebody will request to close this as subjective but I cannot find my specific answer and don't know how else to ask. <\/p>\n\n<p>If I have a shop, with three areas of the shop. I have sensors to detect when people come in or out of each area. This happens every 15 seconds. So, in my db, I have a count of the occupancy, per room, every 15 seconds. <\/p>\n\n<p>Using this data, I want to predict the occupancy, per room, in the future but also, if somebody comes in the door, predict most likely room they will go to. <\/p>\n\n<p>Is it possible to predict future occupancy per room and also probability of where people will go when the walk in using a dataset that simply lists the rooms and the occupancy of each room every 15 seconds? Is this a regression model?<\/p>\n\n<p>Thanks!<\/p>\n\n<p>Mike<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544840911773,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53789057",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.6,
        "Challenge_reading_time":10.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Is it possible to get two different types of results from dataset",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":67.0,
        "Challenge_word_count":170,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1455012473430,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Temecula, CA, United States",
        "Poster_reputation_count":1652.0,
        "Poster_view_count":198.0,
        "Solution_body":"<p>Predicting the most likely room, which they would walk in. :<\/p>\n\n<p>This falls under the classification problem. The output falls under a set of categories, in this case it is different rooms.<\/p>\n\n<p>Predicting the Occpancy of each room :\nAs mentioned by @poorna is a regression problem. <\/p>\n\n<p>Two ways you can look at this problem, <\/p>\n\n<ol>\n<li><p>Multi- target regression problem with occupancy of each room as one target and past occupancies of all rooms as input. <\/p><\/li>\n<li><p>Independent forecast problem for each room with past occupancies of corresponding room as input.<\/p><\/li>\n<\/ol>\n\n<p>For learning the basics of machine learning, you can go through this <a href=\"https:\/\/scikit-learn.org\/stable\/tutorial\/basic\/tutorial.html\" rel=\"nofollow noreferrer\">link<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1544857135976,
        "Solution_link_count":1.0,
        "Solution_readability":9.8,
        "Solution_reading_time":9.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":109.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1425426748316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":91.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I played a bit around with Azure ML studio. So as I understand the process goes like this:<\/p>\n\n<p>a) Create training experiment. Train it with data. <\/p>\n\n<p>b) Create Scoring experiment. This will include the trained model from the training experiment. Expose this as a service to be consumed over REST.<\/p>\n\n<p>Maybe a stupid question but what is the recommended way to get the complete experience like the one i get when I use an app like <a href=\"https:\/\/datamarket.azure.com\/dataset\/amla\/mba\" rel=\"nofollow\">https:\/\/datamarket.azure.com\/dataset\/amla\/mba<\/a> (Frequently Bought Together API built with Azure Machine Learning). <\/p>\n\n<p>I mean the following:<\/p>\n\n<p>a) Expose 2 or more services - one to train the model and the other to consume (test) the trained model. <\/p>\n\n<p>b) User periodically sends training data to train the model <\/p>\n\n<p>c) The trained model\/models now gets saved available for consumption<\/p>\n\n<p>d) User is now able to send a dataframe to get the predicted results.<\/p>\n\n<p>Is there an additional wrapper that needs to be built?<\/p>\n\n<p>If there is a link documenting this please point me to the same. <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1424282724780,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1424286272932,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/28590690",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":8.8,
        "Challenge_reading_time":14.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML App - Complete Experince - Train automatically and Consume",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":769.0,
        "Challenge_word_count":182,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1424282355667,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>The Azure ML retraining API is designed to handle the workflow you describe:<\/p>\n\n<p><a href=\"http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-retrain-models-programmatically\/\" rel=\"nofollow\">http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-retrain-models-programmatically\/<\/a><\/p>\n\n<p>Hope this helps,<\/p>\n\n<p>Roope - Microsoft Azure ML Team<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":29.2,
        "Solution_reading_time":5.44,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":24.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1373375969332,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":126.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>We have a huge set of data in CSV format, containing a few numeric elements, like this:<\/p>\n\n<pre><code>Year,BinaryDigit,NumberToPredict,JustANumber, ...other stuff\n1954,1,762,16, ...other stuff\n1965,0,142,16, ...other stuff\n1977,1,172,16, ...other stuff\n<\/code><\/pre>\n\n<p>The thing here is that there is a strong correlation between the third column and the columns before that. So I have pre-processed the data and it's now available in a format I think is perfect:<\/p>\n\n<pre><code>1954,1,762\n1965,0,142\n1977,1,172\n<\/code><\/pre>\n\n<p>What I want is a predicition on the value in the third column, using the first two as input. So in the case above, I want the input 1965,0 to return 142. In real life this file is thousands of rows, but since there's a pattern, I'd like to retrieve the most possible value.<\/p>\n\n<p>So far I've setup a train job on the CSV file using the L<em>inear Learner<\/em> algorithm, with the following settings:<\/p>\n\n<pre><code>label_size = 1\nfeature_dim = 2\npredictor_type = regression\n<\/code><\/pre>\n\n<p>I've also created a model from it, and setup an endpoint. When I invoke it, I get a score in return.<\/p>\n\n<pre><code>    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                   ContentType='text\/csv',\n                                   Body=payload)\n<\/code><\/pre>\n\n<p>My goal here is to get the third column prediction instead. How can I achieve that? I have read a lot of the documentation regarding this, but since I'm not very familiar with AWS, I might as well have used the wrong algorithms for what I am trying to do.<\/p>\n\n<p>(Please feel free to edit this question to better suit AWS terminology)<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1552553455710,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1552895653107,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55158307",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.4,
        "Challenge_reading_time":21.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"Can I make Amazon SageMaker deliver a recommendation based on historic data instead of a probability score?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":271.0,
        "Challenge_word_count":256,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1411464641600,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"\u00d6rebro, Sverige",
        "Poster_reputation_count":205.0,
        "Poster_view_count":38.0,
        "Solution_body":"<p>For csv input, the label should be in the first column, as mentioned <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/cdf-training.html\" rel=\"nofollow noreferrer\">here<\/a>:  So you should preprocess your data to put the label (the column you want to predict) on the left.<\/p>\n\n<p>Next, you need to decide whether this is a regression problem or a classification problem. <\/p>\n\n<p>If you want to predict a number that's as close as possible to the true number, that's regression. For example, the truth might be 4, and the model might predict 4.15. If you need an integer prediction, you could round the model's output.<\/p>\n\n<p>If you want the prediction to be one of a few categories, then you have a classification problem. For example, we might encode 'North America' = 0, 'Europe' = 1, 'Africa' = 2, and so on. In this case, a fractional prediction wouldn't make sense. <\/p>\n\n<p>For regression, use <code>'predictor_type' = 'regressor'<\/code> and for classification with more than 2 classes, use <code>'predictor_type' = 'multiclass_classifier'<\/code> as documented <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ll_hyperparameters.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>The output of regression will contain only a <code>'score'<\/code> field, which is the model's prediction. The output of multiclass classification will contain a <code>'predicted_label'<\/code> field, which is the model's prediction, as well as a <code>'score'<\/code> field, which is a vector of probabilities representing the model's confidence. The index with the highest probability will be the one that's predicted as the <code>'predicted_label'<\/code>. The output formats are documented <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/LL-in-formats.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":11.3,
        "Solution_reading_time":23.09,
        "Solution_score_count":2.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":230.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1334762714136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation_count":6557.0,
        "Answerer_view_count":2005.0,
        "Challenge_adjusted_solved_time":10264.0195763889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have deployed a regression model on azure ML , is it possible to get the model weights\/coefficients of the model programatically from azure, rather than getting predicted value? .<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1458651323657,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36154971",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.9,
        "Challenge_reading_time":2.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"AZURE ML getting model weights",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1044.0,
        "Challenge_word_count":33,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1422821109972,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":1238.0,
        "Poster_view_count":172.0,
        "Solution_body":"<p>No. Currently we do not feature exporting weights from the models including with Azure Machine Learning. <\/p>\n\n<p>If you have a method for extracting weights from Python models, you may be able to work this out using the execute Python Script module.<\/p>\n\n<p>The primary purpose of Azure Machine Learning is to make deployable and scalable web services from the machine learning modules. Though the authoring experience for creating ML models is great, it is not intended to be a place to create and export models, but instead a place to create and operationalize your models. <\/p>\n\n<p><em>UPDATE<\/em> New features may make this answer outdated. <\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1495601794132,
        "Solution_link_count":0.0,
        "Solution_readability":10.3,
        "Solution_reading_time":8.02,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":105.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"**Description**\r\nError when a MLflow registry model is deployed to triton using **mlflow-triton-plugin** with `--falvor=onnx` flag.\r\nThe plugin is trying to create a `config.pbtxt` in the destination folder before creating that model folder itself.\r\nEasy fix is to create that folder beforehand, but could also be handled from the plugin side.\r\n\r\n```\r\n# create a dir if not exists  \r\nif not os.exists(triton_deployment_dir):\r\n  os.mkdir(triton_deployment_dir)\r\n# then write config to that dir\r\nwith open(os.path.join(triton_deployment_dir, \"config.pbtxt\"),\r\n            \"w\") as cfile:\r\n    cfile.write(config)\r\n```\r\n\r\n**Triton Information**\r\nDocker image: `nvcr.io\/nvidia\/tritonserver:21.12-py3`\r\n\r\n**To Reproduce**\r\n\r\n0. Install mlflow-triton-plugin\r\n1. Log and register an ONNX model to MLflow model registry.\r\n2. Run a triton inference server with these flags: `--model-control-mode=explicit --strict-model-config=false`\r\n3. Create a deployment from mlflow:\r\n `mlflow deployments create -t triton --flavor onnx --name <model-name> -m \"models:\/<model-name>\/1\"`\r\n\r\nError is raised:\r\n```\r\nFile \"mlflow_triton\/deployments.py\", line 105, in create_deployment\r\n  File \"mlflow_triton\/deployments.py\", line 332, in _copy_files_to_triton_repo\r\n  File \"mlflow_triton\/deployments.py\", line 326, in _get_copy_paths\r\nFileNotFoundError: [Errno 2] No such file or directory: '<dest-folder>\/<model-name>\/config.pbtxt'\r\n```\r\n",
        "Challenge_closed_time":1649449.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648621921000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/triton-inference-server\/server\/issues\/4130",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.7,
        "Challenge_reading_time":18.1,
        "Challenge_repo_contributor_count":94.0,
        "Challenge_repo_fork_count":1046.0,
        "Challenge_repo_issue_count":5133.0,
        "Challenge_repo_star_count":4495.0,
        "Challenge_repo_watch_count":116.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"error creating a triton deployment mlflow plugin",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":159,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1583123749267,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Washington D.C., DC, USA",
        "Answerer_reputation_count":317.0,
        "Answerer_view_count":35.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When testing on a local machine in Python I would normally use the following to read a training set with sub-directories of all the classes and files\/class:<\/p>\n\n<pre><code>train_path = r\"C:\\temp\\coins\\PCGS - Gold\\train\"\n\ntrain_batches = ImageDataGenerator().flow_from_directory(train_path, target_size=(100,100), classes=['0','1',2','3' etc...], batch_size=32)\n<\/code><\/pre>\n\n<p><strong>Found 4100 images belonging to 22 classes.<\/strong><\/p>\n\n<p>but on AWS SageMaker's Jupyter notebook I am now pulling the files from an S3 bucket.  I tried the following: <\/p>\n\n<pre><code>bucket = \"coinpath\"\n\ntrain_path = 's3:\/\/{}\/{}\/train'.format(bucket, \"v1\")   #note that the directory structure is coinpath\/v1\/train where coinpath is the bucket\n\ntrain_batches = ImageDataGenerator().flow_from_directory(train_path, target_size=(100,100), classes=\n['0','1',2','3' etc...], batch_size=32)\n<\/code><\/pre>\n\n<p>but I get: ** Found 0 images belonging to 22 classes.**<\/p>\n\n<p>Looking for some guidance on the right way to pull training data from S3.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1589605066783,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61832086",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.9,
        "Challenge_reading_time":14.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Having issues reading S3 bucket when transitioning a tensorflow model from local machine to AWS SageMaker",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":608.0,
        "Challenge_word_count":129,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1583123749267,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Washington D.C., DC, USA",
        "Poster_reputation_count":317.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>From <a href=\"https:\/\/stackoverflow.com\/questions\/54736505\/ideal-way-to-read-data-in-bucket-stored-batches-of-data-for-keras-ml-training-in\">Ideal way to read data in bucket stored batches of data for Keras ML training in Google Cloud Platform?<\/a> \"ImageDataGenerator.flow_from_directory() currently does not allow you to stream data directly from a GCS bucket. \"<\/p>\n\n<p>I had to download the image from S3 first.  This is best for latency reasons as well. <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.0,
        "Solution_reading_time":6.04,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":54.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1528790837107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":610.0,
        "Answerer_view_count":203.0,
        "Challenge_adjusted_solved_time":221.7831972222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In Azure ML Studio, we have the option of choosing a number of inbuilt ML models like Classification, Regression, etc. , which we can drag and drop to our workflow.<\/p>\n\n<p>My question is, can I upload a custom ML model that I have built locally on my system in Python, and add it to the workflow?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1565761282107,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1565767277470,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57488706",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":6.9,
        "Challenge_reading_time":4.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Deploying custom model on Azure ML Studio",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1187.0,
        "Challenge_word_count":61,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1565761178208,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":10.0,
        "Solution_body":"<ol>\n<li>Take the model.pkl file, zip it, and upload it into Azure Machine Learning Studio. Click the \u201cNew\u201d icon in the bottom left:\n<a href=\"https:\/\/i.stack.imgur.com\/Iwvhi.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Iwvhi.jpg\" alt=\"\"><\/a><\/li>\n<li>In the pane that comes up, click on dataset, and then \u201cFrom Local File\u201d:\n<a href=\"https:\/\/i.stack.imgur.com\/DvyjO.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DvyjO.jpg\" alt=\"\"><\/a><\/li>\n<li>Select the zip file where you stored your serialized model and click the tick. You expirement should look like this:\n<a href=\"https:\/\/i.stack.imgur.com\/0efka.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0efka.jpg\" alt=\"\"><\/a><\/li>\n<li>Put the following code to run your classification experiment:<\/li>\n<\/ol>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport sys\nimport pickle\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    sys.path.insert(0,\".\\Script Bundle\")\n    model = pickle.load(open(\".\\Script Bundle\\model.pkl\", 'rb'))\n    pred = model.predict(dataframe1)\n    return pd.DataFrame([pred[0]])\n<\/code><\/pre>\n\n<p><strong>Update<\/strong> \nIf you want to declare this experiment as an API you need to add web input and output to the Python script module.\n<a href=\"https:\/\/i.stack.imgur.com\/eqV8W.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eqV8W.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":1566565696980,
        "Solution_link_count":8.0,
        "Solution_readability":11.5,
        "Solution_reading_time":19.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":144.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1443426419048,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation_count":842.0,
        "Answerer_view_count":219.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working on my thesis (making the traffic lights system work more efficiently by letting them learn) and in my first part of this research, which is how to predict the traffic intensities of the next fifteen minutes, I have to predict the traffic (car) intensities for each lane of the intersection.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Jk1RO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Jk1RO.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The figure above is a sketch of a real intersection and there are 12 different lanes of which I have historical intensity data. I want to use a regression model algorithm by predicting the intensities of each lane for the next 15 minutes.<\/p>\n\n<p>The data set I have generated, is a CSV-file containing all the lanes in the headers and the intensities for each lane in the Xth-15 minutes (3 months long). Below here I will show the generated data set.\n<a href=\"https:\/\/i.stack.imgur.com\/ltrTC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ltrTC.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I wrote a <em>Console Application<\/em> in <code>C#<\/code> to write and generate this output and I uploaded this data set on my <strong>Azure ML project<\/strong>.<\/p>\n\n<p>When I want to train a model, I can only select a single column, which means I can only train one model for a lane. My question is, as I have read in another question, should I train a new model for each lane and save it, or can I somehow convert my data to a more efficient data set and eventually train it as one train model?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1492432364613,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/43451568",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":20.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"How should I train my train models (multiple or single) with Azure Machine Learning?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":432.0,
        "Challenge_word_count":257,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442585745260,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":491.0,
        "Poster_view_count":73.0,
        "Solution_body":"<p>Yes. For this you should train separate models for each column you going to predict values. If the predicted value has an effect with the intensities of other columns use them as inputs for building the predictive model. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.7,
        "Solution_reading_time":2.79,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1231266594036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, United States",
        "Answerer_reputation_count":2800.0,
        "Answerer_view_count":262.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I just started using aws sagemaker for running and maintaining models, experiments. just wanted to know is there any persistent layer for the sagemaker from where i can get data of my experiments\/models instead of looking into the sagemaker studio. Does sagemaker saves the experiments or its data like s3 location in any table  something like modelsdb? <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1584418712033,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1584419644403,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60716202",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":4.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker experiments store",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":229.0,
        "Challenge_word_count":60,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1427130497092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":187.0,
        "Poster_view_count":151.0,
        "Solution_body":"<p>SageMaker Studio is using the SageMaker API to pull all of the data its displaying.  Essentially there's no secret API here getting invoked.<\/p>\n\n<p>Quite a bit of what's being displayed with respect to experiments is from the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_Search.html\" rel=\"nofollow noreferrer\">search results<\/a>, the rest coming from either List* or Describe* calls.  Studio is taking the results from the search request and displaying it in the table format that you're seeing.  Search results when searching over resource ExperimentTrialComponent that have a source (such as a training job) will be enhanced with the original sources data ([result]::SourceDetail::TrainingJob) if supported (work is ongoing to add additional source detail resource types).<\/p>\n\n<p>All of the metadata that is related to resources in SageMaker is available via the APIs; there is no other location (in the cloud) like s3 for that data.<\/p>\n\n<p>As of this time there is no effort to determine if it's possible to add support into <a href=\"https:\/\/github.com\/VertaAI\/modeldb\" rel=\"nofollow noreferrer\">modeldb<\/a> for SageMaker that I'm aware of.  Given that modeldb appears to make some assumptions about it's talking to a relational database it would appear unlikely to be something that would be doable. (I only read the overview very quickly so this might be inaccurate.)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.6,
        "Solution_reading_time":17.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":201.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1334762714136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation_count":6557.0,
        "Answerer_view_count":2005.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When training my model the data I start with consist of rows of json data and the expected values I would like to predict from that json data. The json data follows the schema I my deployed service will receive the input as. Before training I run a number of python functions to transform the data and extract features calculated from the raw json data. It is that transformed data which my model is trained on.<\/p>\n\n<p>I have extracted the code to transform the json data into the input my model expects into a separate python file. Now I would like to have my scoring script use that python script to prepare the input sent to the service before feeding it into my trained model.<\/p>\n\n<p>Is there a way to include the data transformation script with the scoring script when deploying my service using the cli command:<\/p>\n\n<pre><code>az ml service create realtime \n    -f &lt;scoring-script&gt;.py \n    --model-file model.pkl \n    -s service_schema.json \n    -n &lt;some-name&gt; \n    -r python \n    --collect-model-data true \n    -c aml_config\\conda_dependencies.yml\n<\/code><\/pre>\n\n<p><em>(the new lines in the above command added for clarity)<\/em><\/p>\n\n<p>The two ways I've come up with is to either:<\/p>\n\n<ul>\n<li>Create my own base docker image that contains the transformation script and use that image as the base for my service. Seems a bit cumbersome to do if I need similar (but different) data transformations for later models.<\/li>\n<li>Concatenate the transformation script with my scoring script into a single file. Seems a bit hacky.<\/li>\n<\/ul>\n\n<p><strong>Is there another way to achive my goal of having a separate data transformation script used both in training and in scoring?<\/strong><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1524721574190,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1524806425600,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50035628",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":21.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"Include additional scripts when deploying a Azure ML experimentation service",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":252.0,
        "Challenge_word_count":274,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1275401694660,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Gothenburg, Sweden",
        "Poster_reputation_count":18969.0,
        "Poster_view_count":517.0,
        "Solution_body":"<p>So running <code>az ml service create realtime -h<\/code> provides information about the <code>-d<\/code> flag.<\/p>\n\n<p><code>-d : Files and directories required by the service. Multiple dependencies can be specified with additional -d arguments.<\/code><\/p>\n\n<p>Please try using this flag and provide the additional python file that you would like to call too from your <code>score.py<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.5,
        "Solution_reading_time":5.02,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":52.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I have a notebook based on <a href=\"http:\/\/wandb.me\/lit-colab\" rel=\"noopener nofollow ugc\"> Supercharge your Training with PyTorch Lightning + Weights &amp; Biases<\/a> and I\u2019m wondering what the easiest approach to load a model with the best checkpoint after training finishes.<\/p>\n<p>I\u2019m assuming that after training the \u201cmodel\u201d instance will just have the weights of the most recent epoch, which might not be the most accurate model (in case it started overfitting etc).<\/p>\n<p>Specifically I was looking for an easy way to get the directory where the checkpoints artifacts are stored, which in my case look like this: <code>.\/MnistKaggle\/1vzsgin6\/checkpoints<\/code>, where <code>1vzsgin6<\/code> is the run id auto-generated by wandb.<\/p>\n<p>One (clunky) way to do it would be:<\/p>\n<pre><code class=\"lang-auto\">wandb_logger = WandbLogger(project=\"MnistKaggle\")\ncheckpoint_dir_path = None\n\ndef my_after_save_checkpoint(checkpoint):\n  checkpoint_dir_path = checkpoint.dirpath\n\nwandb_logger.after_save_checkpoint = my_after_save_checkpoint\n\n# Now find the checkpoint file in the checkpoint_dir_path directory and load the model from that.\n<\/code><\/pre>\n<p>Is there an easier way?  I was sorta expecting the <code>WandbLogger<\/code> object to have an easy method like <code>get_save_checkpoint_dirpath()<\/code>, but I\u2019m not seeing anything.<\/p>\n<p>Thanks in advance for any help!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667348143889,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/easiest-way-to-load-the-best-model-checkpoint-after-training-w-pytorch-lightning\/3365",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":10.9,
        "Challenge_reading_time":18.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Easiest way to load the best model checkpoint after training w\/ pytorch lightning",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1455.0,
        "Challenge_word_count":182,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/tleyden\">@tleyden<\/a> , happy to help. Please review the following <a href=\"https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/extensions\/generated\/pytorch_lightning.loggers.WandbLogger.html#:~:text=(model)-,Log%20model%20checkpoints,-Log%20model%20checkpoints\" rel=\"noopener nofollow ugc\">resource<\/a> on model checkpointing and retrieval.<\/p>\n<p>A common flow would be to log a model checkpoint as in the example then to also log a \u201cbest model\u201d artifact. Since artifacts are versioned you don\u2019t have to worry about renaming the new \u201cbest model\u201d artifact. Then at the end of your run you not only have an artifact history of your model at each of the checkpoints but also a versioned history of all the best models.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.9,
        "Solution_reading_time":9.74,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":91.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1400869861950,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Rio de Janeiro, Brazil",
        "Answerer_reputation_count":135.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a pretrained spacy model on a local folder that I can easily read with <code>m = spacy.load(&quot;path\/model\/&quot;)<\/code><\/p>\n<p>But now I have to upload it as a .tar.gz file to use as a Sagemaker model artifact.\nHow can I read this .tar.gz file?<\/p>\n<p>Ideally I want to read the unzipped folder from memory. Without extracting all to disk and then reading it again<\/p>\n<p>My question is almost a duplicate of this one <a href=\"https:\/\/stackoverflow.com\/questions\/49274650\/directly-load-spacy-model-from-packaged-tar-gz-file\">Directly load spacy model from packaged tar.gz file<\/a>. But the answers don't explain how to untar unzip the folder into memory<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652738161967,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72266041",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.4,
        "Challenge_reading_time":9.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Loading a spacy .tar.gz model artifact from s3 Sagemaker",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":184.0,
        "Challenge_word_count":102,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1400869861950,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Rio de Janeiro, Brazil",
        "Poster_reputation_count":135.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>Turns out Sagemaker already decompress the <code>.tar.gz<\/code> file automatically.\nSo I can just read the folder exactly like before.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":1.81,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi all,    <\/p>\n<p>I have trained an image classifier with the customvision service, which worked as charm. Now I would like to run the model inference locally with a python script on my PC. Therefore I have been following the tutorial on <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/custom-vision-service\/export-model-python\">https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/custom-vision-service\/export-model-python<\/a>    <\/p>\n<p>I am having troubles with sess.graph.get_tensor_by_name('Placeholder:0').shape.as_list()    <\/p>\n<p>Could you please provide some information on the system requirements and the python package versions? An openCV 4.5.1 C++ code snippet on how to consume the downloaded model would be also great if possible.    <\/p>\n<p>I am using Python 3.8.5    <\/p>\n<p>Thank you<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1621939072900,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/408585\/customvision-run-trained-tensorflow-model-in-pytho",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.0,
        "Challenge_reading_time":12.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Customvision run trained tensorflow model in Python: Placeholder:0 refers to a non existing tensor in image classification",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":111,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Thank you, will do.  <\/p>\n<p>I have solved the issue with using C++ openCV instead and WinML also helps with rapid prototyping.   <\/p>\n<p>This was a particularly good example I have found:  <\/p>\n<p><a href=\"https:\/\/github.com\/Azure-Samples\/cognitive-services-onnx-customvision-sample\">https:\/\/github.com\/Azure-Samples\/cognitive-services-onnx-customvision-sample<\/a>  <\/p>\n<p>Would be great to have more of those.  <\/p>\n<p>Best.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.5,
        "Solution_reading_time":5.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":46.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1528790837107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":610.0,
        "Answerer_view_count":203.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Azure ML provides client libraries (e.g. azureml for Python) for dataset management and model deploying. From what I understand, the custom algorithm would be serialized as a Pickle file, but I'm not sure what happens after that. If I have a custom model with a deep NN architecture and set up a web service for training and another for scoring, do I still need the machine that the model was developed on for the web services to run? I found this on the azureml documentation that was helpful:<\/p>\n<blockquote>\n<p>If a function has no source file associated with it (for example, you're developing inside of a REPL environment) then the functions byte code is serialized. If the function refers to any global variables those will also be serialized using Pickle. In this mode all of the state which you're referring to needs to be already defined (e.g. your published function should come after any other functions you are calling).<\/p>\n<p>If a function is saved on disk then the entire module the function is defined in will be serialized and re-executed on the server to get the function back. In this mode the entire contents of the file is serialized and the order of the function definitions don't matter.<\/p>\n<\/blockquote>\n<p>What if the function uses a library like TensorFlow or Keras? Can someone explain what happens after the Pickle model is created?<\/p>\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1530819659027,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51198775",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":17.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"How does Azure web service deployment work locally?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":126.0,
        "Challenge_word_count":239,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1338127253383,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Minneapolis, MN, United States",
        "Poster_reputation_count":115.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>You need to take the model.pkl file, zip it, and upload it into Azure Machine Learning Studio as a new dataset. Then add the python module and connect it to your newly generated zip.<\/p>\n\n<p>You can now use it inside the AML Studio experiment. To use the model add the following code in your python module:<\/p>\n\n<pre><code>import pandas as pd\nimport sys\nimport pickle\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    sys.path.insert(0,\".\\Script Bundle\")\n    model = pickle.load(open(\".\\Script Bundle\\model.pkl\", 'rb'))\n    pred = model.predict(dataframe1)\n    return pd.DataFrame([pred[0]]),\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/blogs.technet.microsoft.com\/uktechnet\/2018\/04\/25\/deploying-externally-generated-pythonr-models-as-web-services-using-azure-machine-learning-studio\/\" rel=\"nofollow noreferrer\">You may find this post useful<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.5,
        "Solution_reading_time":10.9,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":89.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run custom python\/sklearn sagemaker script on AWS, basically learning from these examples: <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a><\/p>\n<p>All works fine, if define the arguments, train the model and output the file:<\/p>\n<pre><code>parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\nparser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\nparser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n# train the model...\n\njoblib.dump(model, os.path.join(args.model_dir, &quot;model.joblib&quot;))\n<\/code><\/pre>\n<p>And call the job with:<\/p>\n<pre><code>aws_sklearn.fit({'train': 's3:\/\/path\/to\/train', 'test': 's3:\/\/path\/to\/test'}, wait=False)\n<\/code><\/pre>\n<p>In this case model gets stored on different auto-generated bucket, which I do not want. I want to get the output (.joblib file) in the same s3 bucket I took data from. So I add the parameter <code>model-dir<\/code>:<\/p>\n<pre><code>aws_sklearn.fit({'train': 's3:\/\/path\/to\/train', 'test': 's3:\/\/path\/to\/test', `model-dir`: 's3:\/\/path\/to\/model'}, wait=False)\n<\/code><\/pre>\n<p>But it results in error:\n<code>FileNotFoundError: [Errno 2] No such file or directory: 's3:\/\/path\/to\/model\/model.joblib'<\/code><\/p>\n<p>Same happens if I hardcode the output path inside the training script.<\/p>\n<p>So the main question, how can I get the output file in the bucket of my choice?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610533493827,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65699980",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":23.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"Change model file save location on AWS SageMaker Training Job",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1244.0,
        "Challenge_word_count":159,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1572957474856,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":123.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>You can use parameter <code>output_path<\/code> when you define the estimator. If you use the\n<code>model_dir<\/code> I guess you have to create that bucket beforehand, but you have the advantage that artifacts can be saved in real time during the training (if the instance has rights on S3). You can take a look at my <a href=\"https:\/\/github.com\/roccopietrini\/TFSagemakerDetection\" rel=\"nofollow noreferrer\">repo<\/a> for this specific case.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.5,
        "Solution_reading_time":5.66,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1437986390372,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"M\u00fcnchen, Deutschland",
        "Answerer_reputation_count":361.0,
        "Answerer_view_count":149.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have an sklearn k-means model. I am training the model and saving it in a pickle file so I can deploy it later using azure ml library. The model that I am training uses a custom Feature Encoder called <strong>MultiColumnLabelEncoder<\/strong>.\nThe pipeline model is defined as follow :<\/p>\n\n<pre><code># Pipeline\nkmeans = KMeans(n_clusters=3, random_state=0)\npipe = Pipeline([\n(\"encoder\", MultiColumnLabelEncoder()),\n('k-means', kmeans),\n])\n#Training the pipeline\nmodel = pipe.fit(visitors_df)\nprediction = model.predict(visitors_df)\n#save the model in pickle\/joblib format\nfilename = 'k_means_model.pkl'\njoblib.dump(model, filename)\n<\/code><\/pre>\n\n<p>The model saving works fine. The Deployment steps are the same as the steps in this link : <\/p>\n\n<p><a href=\"https:\/\/notebooks.azure.com\/azureml\/projects\/azureml-getting-started\/html\/how-to-use-azureml\/deploy-to-cloud\/model-register-and-deploy.ipynb\" rel=\"nofollow noreferrer\">https:\/\/notebooks.azure.com\/azureml\/projects\/azureml-getting-started\/html\/how-to-use-azureml\/deploy-to-cloud\/model-register-and-deploy.ipynb<\/a><\/p>\n\n<p>However the deployment always fails with this error :<\/p>\n\n<pre><code>  File \"\/var\/azureml-server\/create_app.py\", line 3, in &lt;module&gt;\n    from app import main\n  File \"\/var\/azureml-server\/app.py\", line 27, in &lt;module&gt;\n    import main as user_main\n  File \"\/var\/azureml-app\/main.py\", line 19, in &lt;module&gt;\n    driver_module_spec.loader.exec_module(driver_module)\n  File \"\/structure\/azureml-app\/score.py\", line 22, in &lt;module&gt;\n    importlib.import_module(\"multilabelencoder\")\n  File \"\/azureml-envs\/azureml_b707e8c15a41fd316cf6c660941cf3d5\/lib\/python3.6\/importlib\/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\nModuleNotFoundError: No module named 'multilabelencoder'\n<\/code><\/pre>\n\n<p>I understand that pickle\/joblib has some problems unpickling the custom function MultiLabelEncoder. That's why I defined this class in a separate python script (which I executed also). I called this custom function in the training python script, in the deployment script and in the scoring python file (score.py). The importing in the score.py file is not successful. \nSo my question is how can I import custom python module to azure ml deployment environment ?<\/p>\n\n<p>Thank you in advance.<\/p>\n\n<p>EDIT: \nThis is my .yml file<\/p>\n\n<pre><code>name: project_environment\ndependencies:\n  # The python interpreter version.\n  # Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.2\n\n- pip:\n  - multilabelencoder==1.0.4\n  - scikit-learn\n  - azureml-defaults==1.0.74.*\n  - pandas\nchannels:\n- conda-forge\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1575463048677,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1575634698240,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59176241",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.4,
        "Challenge_reading_time":35.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":null,
        "Challenge_title":"import custom python module in azure ml deployment environment",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2611.0,
        "Challenge_word_count":276,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1437986390372,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"M\u00fcnchen, Deutschland",
        "Poster_reputation_count":361.0,
        "Poster_view_count":149.0,
        "Solution_body":"<p>In fact, the solution was to import my customized class <strong>MultiColumnLabelEncoder<\/strong> as a pip package (You can find it through pip install multilllabelencoder==1.0.5).\nThen I passed the pip package to the .yml file or in the InferenceConfig of the azure ml environment.\nIn the score.py file, I imported the class as follows :<\/p>\n\n<pre><code>from multilabelencoder import multilabelencoder\ndef init():\n    global model\n\n    # Call the custom encoder to be used dfor unpickling the model\n    encoder = multilabelencoder.MultiColumnLabelEncoder() \n    # Get the path where the deployed model can be found.\n    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'k_means_model_45.pkl')\n    model = joblib.load(model_path)\n<\/code><\/pre>\n\n<p>Then the deployment was successful. \nOne more important thing is I had to use the same pip package (multilabelencoder) in the training pipeline as here :<\/p>\n\n<pre><code>from multilabelencoder import multilabelencoder \npipe = Pipeline([\n    (\"encoder\", multilabelencoder.MultiColumnLabelEncoder(columns)),\n    ('k-means', kmeans),\n])\n#Training the pipeline\ntrainedModel = pipe.fit(df)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.2,
        "Solution_reading_time":14.38,
        "Solution_score_count":4.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":132.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1441425578580,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":433.0,
        "Answerer_view_count":56.0,
        "Challenge_adjusted_solved_time":12869.3964275,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I trained my model in Amazon-SageMaker and downloaded it to my local computer. Unfortunately, I don't have any idea how to run the model locally.<\/p>\n\n<p>The Model is in a directory with files like:<\/p>\n\n<pre><code>image-classification-0001.params\nimage-classification-0002.params\nimage-classification-0003.params\nimage-classification-0004.params\nimage-classification-0005.params\nimage-classification-symbol.json\nmodel-shapes.json\n<\/code><\/pre>\n\n<p>Would anyone know how to run this locally with Python, or be able to point me to a resource that could help? I am trying to avoid calling the model using the Amazon API.<\/p>\n\n<p>Edit: The model I used was created with code very similar to this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-fulltraining.ipynb\" rel=\"nofollow noreferrer\">example<\/a>.<\/p>\n\n<p>Any help is appreciated, I will award the bounty to whoever is most helpful, even if they don't completely solve the question. <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1520225638010,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1536012091088,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49103679",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":15.7,
        "Challenge_reading_time":14.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"How to Deploy Amazon-SageMaker Locally in Python",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3742.0,
        "Challenge_word_count":122,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1441425578580,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":433.0,
        "Poster_view_count":56.0,
        "Solution_body":"<p>Following SRC's advice, I was able to get it to work by following the instructions in this <a href=\"https:\/\/stackoverflow.com\/questions\/47190614\/how-to-load-a-trained-mxnet-model\">question<\/a> and this <a href=\"http:\/\/mxnet.incubator.apache.org\/tutorials\/python\/predict_image.html\" rel=\"nofollow noreferrer\">doc<\/a> which describe how to load a MXnet model.<\/p>\n\n<p>I loaded the model like so:<\/p>\n\n<pre><code>lenet_model = mx.mod.Module.load('model_directory\/image-classification',5)\nimage_l = 64\nimage_w = 64\nlenet_model.bind(for_training=False, data_shapes=[('data',(1,3,image_l,image_w))],label_shapes=lenet_model._label_shapes)\n<\/code><\/pre>\n\n<p>Then predicted using the slightly modified helper functions in the previously linked documentation:<\/p>\n\n<pre><code>import mxnet as mx\nimport matplotlib.pyplot as plot\nimport cv2\nimport numpy as np\nfrom mxnet.io import DataBatch\n\ndef get_image(url, show=False):\n    # download and show the image\n    fname = mx.test_utils.download(url)\n    img = cv2.cvtColor(cv2.imread(fname), cv2.COLOR_BGR2RGB)\n    if img is None:\n         return None\n    if show:\n         plt.imshow(img)\n         plt.axis('off')\n    # convert into format (batch, RGB, width, height)\n    img = cv2.resize(img, (64, 64))\n    img = np.swapaxes(img, 0, 2)\n    img = np.swapaxes(img, 1, 2)\n    img = img[np.newaxis, :]\n    return img\n\ndef predict(url, labels):\n    img = get_image(url, show=True)\n    # compute the predict probabilities\n    lenet_model.forward(DataBatch([mx.nd.array(img)]))\n    prob = lenet_model.get_outputs()[0].asnumpy()\n\n    # print the top-5\n    prob = np.squeeze(prob)\n    a = np.argsort(prob)[::-1]\n\n    for i in a[0:5]:\n       print('probability=%f, class=%s' %(prob[i], labels[i]))\n<\/code><\/pre>\n\n<p>Finally I called the prediction with this code:<\/p>\n\n<pre><code>labels = ['a','b','c', 'd','e', 'f']\npredict('https:\/\/eximagesite\/img_tst_a.jpg', labels )\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1582341918227,
        "Solution_link_count":3.0,
        "Solution_readability":12.5,
        "Solution_reading_time":23.62,
        "Solution_score_count":3.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":169.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1529408888483,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":79.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained a Pytorch model using SageMaker and the model is now stored in an S3 bucket.\nI am trying to retrieve that model and deploying it.<\/p>\n\n<p>This is the code I am using:<\/p>\n\n<pre><code>estimator = sagemaker.model.FrameworkModel(\n    model_data= #link to  model location in s3\n    image=  # image\n    role=role,\n    entry_point='train.py', \n    source_dir='pytorch_source',\n    sagemaker_session = sagemaker_session\n) \n\npredictor = estimator.deploy(initial_instance_count=1, instance_type=\"ml.p2.xlarge\")\n<\/code><\/pre>\n\n<p>But after the deployment process (that seems to run smoothly), the predictor is just a NoneType.\nI haven't found any weird message in the logs...<\/p>\n\n<p>I have also made another attempt with the following code:<\/p>\n\n<pre><code>estimator = PyTorchModel(model_data= #link to model location in s3 \n                             role=role,\n                             image= #image\n                             entry_point='pytorch_source\/train.py',\n                            predictor_cls = 'pytorch_source\/train.py',\n                           framework_version = '1.1.0')\n\npredictor = estimator.deploy(initial_instance_count=1, instance_type=\"ml.p2.xlarge\")\n<\/code><\/pre>\n\n<p>But it doesn't even complete the deployment.<\/p>\n\n<p>Can anyone help with this?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583334481857,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1583398637700,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60529048",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":15.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"How to deploy an existing pytorch model previously trained with Amazon Sagemaker and stored in S3 bucket",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":360.0,
        "Challenge_word_count":138,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1529408888483,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":79.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I actually solved using PyTorchModel with the following settings:<\/p>\n\n<pre><code>estimator = PyTorchModel(model_data='#path to model, \n                             role=role,\n                             source_dir='pytorch_source',\n                             entry_point='deploy.py',\n                            predictor_cls = ImgPredictor,\n                           framework_version = '1.1.0')\n<\/code><\/pre>\n\n<p>where ImgPredictor is<\/p>\n\n<pre><code>from sagemaker.predictor import RealTimePredictor, json_deserializer\n\nclass ImgPredictor(RealTimePredictor):\n    def __init__(self, endpoint_name, sagemaker_session):\n        super(ImgPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='application\/x-image', \n                                           deserializer = json_deserializer ,accept='application\/json')\n<\/code><\/pre>\n\n<p>and deploy.py contains the required functions input_fn, output_fn, model_fn and predict_fn.\nAlso, a requirements.txt file was missing in the source directory.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":22.9,
        "Solution_reading_time":11.09,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I wonder if it's possible to run training Amazon SageMaker object detection model on a local PC?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654004815903,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72448994",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":2.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Train Amazon SageMaker object detection model on local PC",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":70.0,
        "Challenge_word_count":25,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442786553536,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kyiv",
        "Poster_reputation_count":71.0,
        "Poster_view_count":30.0,
        "Solution_body":"<p>You're probably referring to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object-detection.html\" rel=\"nofollow noreferrer\">this<\/a> object detection algorithm which is part of of <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">Amazon SageMaker built-in algorithms<\/a>. <strong>Built-in algorithms must be trained on the cloud<\/strong>.<br \/>\nIf you're bringing your own Tensorflow or PyTorch model, you could use SageMaker training jobs to train either on the cloud or locally as @kirit noted.<\/p>\n<p>I would also look at <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-jumpstart.html\" rel=\"nofollow noreferrer\">SageMaker JumpStart<\/a> for a wide variety of object detection algorithm which are TF\/PT based.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":16.0,
        "Solution_reading_time":10.43,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":79.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1428654714763,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":596.0,
        "Answerer_view_count":80.0,
        "Challenge_adjusted_solved_time":18.9362980555,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to build an API using an MLflow model.<\/p>\n<p>the funny thing is it works from one location on my PC and not from another. So, the reason for doing I wanted to change my repo etc.<\/p>\n<p>So, the simple code of<\/p>\n<pre><code>from mlflow.pyfunc import load_model\nMODEL_ARTIFACT_PATH = &quot;.\/model\/model_name\/&quot;\nMODEL = load_model(MODEL_ARTIFACT_PATH)\n<\/code><\/pre>\n<p>now fails with<\/p>\n<pre><code>ERROR:    Traceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 540, in lifespan\n    async for item in self.lifespan_context(app):\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 481, in default_lifespan\n    await self.startup()\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 516, in startup\n    await handler()\n  File &quot;\/code\/.\/app\/main.py&quot;, line 32, in startup_load_model\n    MODEL = load_model(MODEL_ARTIFACT_PATH)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 733, in load_model\n    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/spark.py&quot;, line 737, in _load_pyfunc\n    return _PyFuncModelWrapper(spark, _load_model(model_uri=path))\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/spark.py&quot;, line 656, in _load_model\n    return PipelineModel.load(model_uri)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/util.py&quot;, line 332, in load\n    return cls.read().load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/pipeline.py&quot;, line 258, in load\n    return JavaMLReader(self.cls).load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/util.py&quot;, line 282, in load\n    java_obj = self._jread.load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/py4j\/java_gateway.py&quot;, line 1321, in __call__\n    return_value = get_return_value(\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/sql\/utils.py&quot;, line 117, in deco\n    raise converted from None\npyspark.sql.utils.AnalysisException: Unable to infer schema for Parquet. It must be specified manually.\n<\/code><\/pre>\n<p>The model artifacts are already downloaded to the folder \/model folder which has the following structure.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oqxRW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oqxRW.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>the load model call is in the main.py file\nAs I mentioned it works from another directory, but there is no reference to any absolute paths. Also, I have made sure that my package references are identical. e,g I have pinned them all down<\/p>\n<pre><code># Model\nmlflow==1.25.1\nprotobuf==3.20.1\npyspark==3.2.1\nscipy==1.6.2\nsix==1.15.0\n<\/code><\/pre>\n<p>also, the same docker file is used both places, which among other things, makes sure that the final resulting folder structure is the same<\/p>\n<pre><code>......other stuffs\n\nCOPY .\/app \/code\/app\nCOPY .\/model \/code\/model\n<\/code><\/pre>\n<p>what can explain it throwing this exception whereas in another location (on my PC), it works (same model artifacts) ?<\/p>\n<p>Since it uses load_model function, it should be able to read the parquet files ?<\/p>\n<p>Any question and I can explain.<\/p>\n<p>EDIT1: I have debugged this a little more in the docker container and it seems the parquet files in the itemFactors folder (listed in my screenshot above) are not getting copied over to my image , even though I have the copy command to copy all files under the model folder. It is copying the _started , _committed and _SUCCESS files, just not the parquet files. Anyone knows why would that be? I DO NOT have a .dockerignore file. Why are those files ignored while copying?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655130099670,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1655134670387,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72604450",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":50.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":46,
        "Challenge_solved_time":null,
        "Challenge_title":"MLflow load model fails Python",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":109.0,
        "Challenge_word_count":417,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1428654714763,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":596.0,
        "Poster_view_count":80.0,
        "Solution_body":"<p>I found the problem. Like I wrote in the EDIT1 of my post, with further observations, the parquet files were missing in the docker container. That was strange because I was copying the entire folder in my Dockerfile.<\/p>\n<p>I then realized that I was hitting this problem <a href=\"https:\/\/github.com\/moby\/buildkit\/issues\/1366\" rel=\"nofollow noreferrer\">mentioned here<\/a>. File paths exceeding 260 characters, silently fail and do not get copied over to the docker container. This was really frustrating because nothing failed during build and then during run, it gave me that cryptic error of &quot;unable to infer schema for parquet&quot;, essentially because the parquet files were not copied over during docker build.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1655202841060,
        "Solution_link_count":1.0,
        "Solution_readability":9.3,
        "Solution_reading_time":9.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":107.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1333391842272,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"California, United States",
        "Answerer_reputation_count":1405.0,
        "Answerer_view_count":151.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to build some <strong>neural network<\/strong> models for NLP and recommendation applications. The framework I want to use is <strong>TensorFlow<\/strong>. I plan to train these models and make predictions on Amazon web services. The application will be most likely <strong>distributed computing<\/strong>.<\/p>\n\n<p>I am wondering what are the pros and cons of SageMaker and EMR for TensorFlow applications?<\/p>\n\n<p>They both have TensorFlow integrated. <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1537510189837,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1537512451192,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52437599",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.3,
        "Challenge_reading_time":7.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Pros and Cons of Amazon SageMaker VS. Amazon EMR, for deploying TensorFlow-based deep learning models?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":10776.0,
        "Challenge_word_count":78,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1341967360208,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2832.0,
        "Poster_view_count":368.0,
        "Solution_body":"<p>In general terms, they serve different purposes.<\/p>\n\n<p><a href=\"https:\/\/aws.amazon.com\/emr\/\" rel=\"noreferrer\"><strong>EMR<\/strong><\/a> is when you need to process massive amounts of data and heavily rely on Spark, Hadoop, and MapReduce (EMR = Elastic MapReduce). Essentially, if your data is in large enough volume to make use of the efficiencies of Spark, Hadoop, Hive, HDFS, HBase and Pig stack then go with EMR.<\/p>\n\n<p><strong>EMR Pros:<\/strong><\/p>\n\n<ul>\n<li>Generally, low cost compared to EC2 instances<\/li>\n<li>As the name suggests Elastic meaning you can provision what you need when you need it<\/li>\n<li>Hive, Pig, and HBase out of the box<\/li>\n<\/ul>\n\n<p><strong>EMR Cons:<\/strong><\/p>\n\n<ul>\n<li>You need a very specific use case to truly benefit from all the offerings in EMR. Most don't take advantage of its entire offering<\/li>\n<\/ul>\n\n<p><a href=\"https:\/\/aws.amazon.com\/sagemaker\/\" rel=\"noreferrer\"><strong>SageMaker<\/strong><\/a> is an attempt to make Machine Learning easier and distributed. The marketplace provides out of the box algos and models for quick use. It's a great service if you conform to the workflows it enforces. Meaning creating training jobs, deploying inference endpoints <\/p>\n\n<p><strong>SageMaker Pros:<\/strong><\/p>\n\n<ul>\n<li>Easy to get up and running with Notebooks<\/li>\n<li>Rich marketplace to quickly try existing models<\/li>\n<li>Many different example notebooks for popular algorithms<\/li>\n<li>Predefined kernels that minimize configuration<\/li>\n<li>Easy to deploy models<\/li>\n<li>Allows you to distribute inference compute by deploying endpoints<\/li>\n<\/ul>\n\n<p><strong>SageMaker Cons:<\/strong><\/p>\n\n<ul>\n<li>Expensive!<\/li>\n<li>Enforces a certain workflow making it hard to be fully custom<\/li>\n<li>Expensive!<\/li>\n<\/ul>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.6,
        "Solution_reading_time":22.46,
        "Solution_score_count":10.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":229.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi Azure ML users! My Regression model, based on this learning Path:  <br \/>\nCreate a Regression Model with Azure Machine Learning designer --&gt; Deploy a predictive service.  <br \/>\nIt predicts a car's price. I've added Edit Meta Data in Designer to clear features of other columns selected. This displays other details about each car, like engine, manual or automatic, title status and general notes. For prediction AzureML only uses two columns, miles and year, to predict price. 6 columns ClearFeatured, 2 features and one label column for price, 9 total columns selected.  <\/p>\n<ol>\n<li> I enter data manually, CSV, click submit realtime inference and AzureML predicts a price, success.  <\/li>\n<li> I click deploy, and then click Test on the deployed endpoint, AzureML success again.  <\/li>\n<li> I click New Notebook and paste in python script to predict a price, sending 9 columns, like this, but keep getting a schema error.<\/li>\n<\/ol>\n<p>&quot;price&quot;: 5500,  <br \/>\n&quot;year&quot;: 2013,  <br \/>\n&quot;car&quot;: &quot;Mini Cooper&quot;,  <br \/>\n&quot;miles&quot;: 74000,  <br \/>\n&quot;model&quot;: &quot;Sport&quot;,  <br \/>\n&quot;engine&quot;: 1,  <br \/>\n&quot;manual&quot;: &quot;manual&quot;,  <br \/>\n&quot;title&quot;: &quot;rebuilt&quot;,  <br \/>\n&quot;notes&quot;: &quot;silver black lines to 5500 started at 6500&quot;,<\/p>\n<p>-- Here is the deploy error --  <br \/>\nThe request failed with status code: 400  <br \/>\nAccess-Control-Allow-Origin: *  <br \/>\nContent-Length: 1271  <br \/>\nContent-Type: application\/json  <br \/>\nDate: Thu, 15 Jul 2021 03:17:53 GMT  <br \/>\nServer: nginx\/1.14.0 (Ubuntu)  <br \/>\nX-Ms-Request-Id: 6d440216-81bc-441f-aeae-c5190c486028  <br \/>\nX-Ms-Run-Function-Failed: False  <br \/>\nConnection: close<\/p>\n<p>{'error': {'code': 400, 'message': 'Input Data Error. Input data are inconsistent with schema.\\nSchema: {\\'columnAttributes\\': [{\\'name\\': \\'price\\', \\'type\\': \\'Numeric\\', \\'isFeature\\': True, \\'elementType\\': {\\'typeName\\': \\'int64\\', \\'isNullable\\': False}}, {\\'name\\': \\'year\\', \\'type\\': \\'Numeric\\', \\'isFeature\\': True, \\'elementType\\': {\\'typeName\\': \\'int64\\', \\'isNullable\\': False}}, {\\'n\\nData: defaultdict(&lt;class \\'list\\'&gt;, {\\'price\\': [5500], \\'year\\': [2013], \\'car\\': [\\'Mini Cooper\\'], \\'miles\\': [74000], \\'model\\': [\\'Sport\\'], \\'engine\\': <a href=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/114777-azure-ml-deploy-has-schema-error.png?platform=QnA\">1<\/a>, \\'manual\\': [\\'manual\\'], \\'title\\': [\\'rebuilt\\'], \\'notes\\': [\\'silver black lines now down to 5800 started at 6500\\']})\\nTraceback (most recent call last):\\n File &quot;\/azureml-envs\/azureml_d04391a4e9e93a56aa2beac2c36d4d02\/lib\/python3.6\/site-packages\/azureml\/designer\/serving\/dagengine\/dag.py&quot;, line 167, in execute\\n input_data = create_dfd_from_dict(raw_input, schema)\\n File &quot;\/azureml-envs\/azureml_d04391a4e9e93a56aa2beac2c36d4d02\/lib\/python3.6\/site-packages\/azureml\/designer\/serving\/dagengine\/converter.py&quot;, line 19, in create_dfd_from_dict\\n raise ValueError(f\\'Input json_data must have the same column names as the meta data. \\'\\nValueError: Input json_data must have the same column names as the meta data. Different columns are: {\\'more1\\'}\\n', 'details': ''}}<\/p>\n<p>:: problem ::  <br \/>\nWhy is the notebook failing? but the Test of the endpoint has succcess? What am I doing wrong with the schema?<\/p>\n<p>Thank you.<\/p>\n<p>-- more details below, if you interested --  <br \/>\nmy incoming data will match the schema of the original training data, 9 columns, so I did not do this step below.<\/p>\n<p>Learning Path says:  <br \/>\n&quot;The inference pipeline assumes that new data will match the schema of the original training data, so the Automobile price data (Raw) dataset from the training pipeline is included. However, this input data includes the price label that the model predicts, which is unintuitive to include in new car data for which a price prediction has not yet been made.&quot;<\/p>\n<p>Learning Path also says:  <br \/>\n&quot;Now that you've changed the schema of the incoming data to exclude the price field, you need to remove any explicit uses of this field in the remaining modules. Select the Select Columns in Dataset module and then in the settings pane, edit the columns to remove the price field.&quot;<\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/114777-azure-ml-deploy-has-schema-error.png?platform=QnA\" alt=\"114777-azure-ml-deploy-has-schema-error.png\" \/><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1626321175607,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/476616\/400-schema-error-in-azureml-regression-model-when",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":58.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":29,
        "Challenge_solved_time":null,
        "Challenge_title":"400 schema error in AzureML regression model when using a notebook python script, but endpoint test success",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":544,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>it's working now. I needed to add the more1 column. this was a null column in my dataset, the last column in CSV file.     <br \/>\nin my training pipeline this was omitted from Select Columns in Dataset. But in the Creating Inference,mistakenly put it back in, all my columns in the Enter Data Manually asset. 10 columns in Notebook now matches 10 column names for inference pipeline.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/116545-azure-fix-cars-schema1.png?platform=QnA\" alt=\"116545-azure-fix-cars-schema1.png\" \/>    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.7,
        "Solution_reading_time":6.99,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":72.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1542628542703,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":11.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed a Tensorflow-Model in SageMaker Studio following this tutorial:\n<a href=\"https:\/\/aws.amazon.com\/de\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/de\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/<\/a>\nThe Model needs a Multidimensional Array as input. Invoking it from the Notebook itself is working:<\/p>\n<pre><code>import numpy as np\nimport json\ndata = np.load(&quot;testValues.npy&quot;)\npred=predictor.predict(data)\n<\/code><\/pre>\n<p>But I wasnt able to invoke it from a boto 3 client using this code:<\/p>\n<pre><code>import json\nimport boto3\nimport numpy as np\nimport io\n \nclient = boto3.client('runtime.sagemaker')\ndatain = np.load(&quot;testValues.npy&quot;)\ndata=datain.tolist();\nresponse = client.invoke_endpoint(EndpointName=endpoint_name, Body=json.dumps(data))\nresponse_body = response['Body']\nprint(response_body.read())\n<\/code><\/pre>\n<p>This throws the Error:<\/p>\n<pre><code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from model with message &quot;{&quot;error&quot;: &quot;Unsupported Media Type: Unknown&quot;}&quot;.\n<\/code><\/pre>\n<p>I guess the reason is the json Media Type but i have no clue how to get it back in shape.\nI tried this:<a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/644\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/644<\/a> but it doesnt seem to change anything<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605617435417,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64875623",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":16.8,
        "Challenge_reading_time":21.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Invoking an endpoint in AWS with a multidimensional array",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":550.0,
        "Challenge_word_count":147,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1542628542703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":11.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>This fixed it for me:\nThe Content Type was missing.<\/p>\n<pre><code>import json\nimport boto3\nimport numpy as np\nimport io\n\nclient = boto3.client('runtime.sagemaker',aws_access_key_id=..., aws_secret_access_key=...,region_name=...)\nendpoint_name = '...'\n\ndata = np.load(&quot;testValues.npy&quot;)\n\n\npayload = json.dumps(data.tolist())\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                                  ContentType='application\/json',\n                                   Body=payload)\nresult = json.loads(response['Body'].read().decode())\nres = result['predictions']\nprint(&quot;test&quot;)\n\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":18.7,
        "Solution_reading_time":7.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":38.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":83.8230358333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a data model consisting only of categorial features and a categorial label.<\/p>\n<p>So when I build that model manually in XGBoost, I would basically transform the features to binary columns (using LabelEncoder and OneHotEncoder), and the label into classes using LabelEncoder. I would then run a <strong>Multilabel Classification<\/strong> (multi:softmax).\nI tried that with my dataset and ended up with an accuracy around 0.4 (unfortunately can't share the dataset due to confidentiality)<\/p>\n<p>Now, if I run the same dataset in Azure AutoML, I end up with an accuracy around 0.85 in the best experiment. But what is really interesting is that the AutoML uses SparseNormalizer, XGBoostClassifier, with <strong>reg:logistic<\/strong> as objective.\nSo if I interpret this right, AzureML just normalizes the data (somehow from categorial data?) and then executes a logistic regression? Is this even possible \/ does this make sense with categorial data?<\/p>\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":4,
        "Challenge_created_time":1593709508303,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1593766738543,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62701556",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":11.9,
        "Challenge_reading_time":13.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Strange algorithm selection when using Azure AutoML with XBoostClassifier on categorial data",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":281.0,
        "Challenge_word_count":157,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1534756062256,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":87.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p><code>TL;DR<\/code> You're right that normalization doesn't make sense for training gradient-boosted decision trees (<code>GBDT<\/code>s) on categorical data, but it won't have an adverse impact. AutoML is an automated framework for modeling. In exchange for calibration control, you get ease-of-use. It is still worth verifying first that AutoML is receiving data with the columns properly encoded as categorical.<\/p>\n<p>Think of an AutoML model as effectively a <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html\" rel=\"nofollow noreferrer\">sklearn Pipeline<\/a>, which is a bundled set of pre-processing steps along with a predictive Estimator. AutoML will attempt to sample from a large swath of pre-configured Pipelines such that the most accurate Pipeline will be discovered. As <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-automated-ml#automatic-featurization-standard\" rel=\"nofollow noreferrer\">the docs<\/a> say:<\/p>\n<blockquote>\n<p>In every automated machine learning experiment, your data is automatically scaled or normalized to help algorithms perform well. During model training, one of the following scaling or normalization techniques will be applied to each model.<\/p>\n<\/blockquote>\n<p>Too see this, you can called <code>.named_steps<\/code> on your fitted model. Also check out <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train#automated-feature-engineering\" rel=\"nofollow noreferrer\"><code>fitted_model.get_featurization_summary()<\/code><\/a><\/p>\n<p>I especially empathize with your concern especially w.r.t. how <code>LightGBM<\/code> (MSFT's GBDT implementation) is levered by AutoML. <code>LightGBM<\/code> accepts categorical columns and instead of one-hot encoding, will bin them into two subsets whenever split. Despite this, AutoML will pre-process away the categorical columns by one-hot encoding, scaling, and\/or normalization; so this unique categorical approach is never utilized in AutoML.<\/p>\n<p>If you're interested in &quot;manual&quot; ML in Azure ML, I highly suggest looking into <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-train-machine-learning-model#estimators\" rel=\"nofollow noreferrer\"><code>Estimators<\/code><\/a> and <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-train-machine-learning-model#machine-learning-pipeline\" rel=\"nofollow noreferrer\"><code>Azure ML Pipelines<\/code><\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1594068501472,
        "Solution_link_count":5.0,
        "Solution_readability":16.2,
        "Solution_reading_time":32.88,
        "Solution_score_count":4.0,
        "Solution_sentence_count":21.0,
        "Solution_word_count":250.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1585824581960,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Berlin",
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running a <em>Training Job<\/em> using the Sagemaker API. The code for configuring the estimator looks as follows (I shrinked the full path names a bit):<\/p>\n<pre><code>s3_input = &quot;s3:\/\/sagemaker-studio-****\/training-inputs&quot;.format(bucket)\ns3_images = &quot;s3:\/\/sagemaker-studio-****\/dataset&quot;\ns3_labels = &quot;s3:\/\/sagemaker-studio-****\/labels&quot;\ns3_output = 's3:\/\/sagemaker-studio-****\/output'.format(bucket)\n\ncfg='{}\/input\/models\/'.format(s3_input)\nweights='{}\/input\/data\/weights\/'.format(s3_input)\noutpath='{}\/'.format(s3_output)\nimages='{}\/'.format(s3_images)\nlabels='{}\/'.format(s3_labels)\n\nhyperparameters = {\n    &quot;epochs&quot;: 1,\n    &quot;batch-size&quot;: 2\n}\n\ninputs = {\n    &quot;cfg&quot;: TrainingInput(cfg),\n    &quot;images&quot;: TrainingInput(images),\n    &quot;weights&quot;: TrainingInput(weights),\n    &quot;labels&quot;: TrainingInput(labels)\n}\n\nestimator = PyTorch(\n    entry_point='train.py',\n    source_dir='s3:\/\/sagemaker-studio-****\/input\/input.tar.gz',\n    image_uri=container,\n    role=get_execution_role(),\n    instance_count=1,\n    instance_type='ml.g4dn.xlarge',\n    input_mode='File',\n    output_path=outpath,\n    train_output=outpath,\n    base_job_name='visualsearch',\n    hyperparameters=hyperparameters,\n    framework_version='1.9',\n    py_version='py38'\n)\n\nestimator.fit(inputs)\n<\/code><\/pre>\n<p>Everything runs fine and I get the success message:<\/p>\n<pre><code>Results saved to #033[1mruns\/train\/exp#033[0m\n2022-07-08 08:38:35,766 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n2022-07-08 08:38:35,766 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n2022-07-08 08:38:35,767 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n\n2022-07-08 08:39:08 Uploading - Uploading generated training model\n2022-07-08 08:39:08 Completed - Training job completed\nProfilerReport-1657268881: IssuesFound\nTraining seconds: 558\nBillable seconds: 558\nCPU times: user 1.34 s, sys: 146 ms, total: 1.48 s\nWall time: 11min 20s\n<\/code><\/pre>\n<p>When I call <code>estimator.model_data<\/code> I get a path poiting to a model.tar.gz file <code>s3:\/\/sagemaker-studio-****\/output\/...\/model.tar.gz<\/code><\/p>\n<p>Sagemaker generated subfoldes into the output folder (which in turn contain a lot of json files and other artifacts):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/WymlH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WymlH.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But the file <code>model.tar.gz<\/code> is missing. This file is nowhere to be found. Is there anything I need to change or to add, in order to obtain my model?<\/p>\n<p>Any help is much appreciated.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657270994073,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72909085",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":15.6,
        "Challenge_reading_time":36.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker creates output folders but no model.tar.gz after successful completion of the Training Job",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":94.0,
        "Challenge_word_count":248,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1640612109920,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Zedtwitz, Germany",
        "Poster_reputation_count":215.0,
        "Poster_view_count":46.0,
        "Solution_body":"<p>you need to make sure to store your model output to the right location inside the training container. Sagemaker will upload everything that is stored in the MODEL_DIR directory. You can find the location in the ENV of the training job:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model_dir = os.environ.get(&quot;SM_MODEL_DIR&quot;)\n<\/code><\/pre>\n<p>Normally it is set to <code>opt\/ml\/model<\/code><\/p>\n<p>Ref:<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\/blob\/master\/ENVIRONMENT_VARIABLES.md#sm_model_dir\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-training-toolkit\/blob\/master\/ENVIRONMENT_VARIABLES.md#sm_model_dir<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-output.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-output.html<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":23.3,
        "Solution_reading_time":12.49,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":63.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1593662684510,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":41.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am fairly new to TensorFlow (and SageMaker) and am stuck in the process of deploying a SageMaker endpoint. I have just recently succeeded in creating a Saved Model type model, which is currently being used to service a sample endpoint (the model was created externally). However, when I checked the image I am using for the endpoint, it says '...\/tensorflow-inference', which is not the direction I want to go in because I want to use a SageMaker TensorFlow serving container (I followed tutorials from the official TensorFlow serving GitHub repo-using sample models, and they are deployed correcting using the TensorFlow serving framework).<\/p>\n<p>Am I encountering this issue because my Saved Model does not have the correct 'serving' tag? I have not checked my tag sets yet but wanted to know if this would be the core reason to the problem. Also, most importantly, <strong>what are the differences between the two container types<\/strong>-I think having a better understanding of these two concepts would show me why I am unable to produce the correct image.<\/p>\n<hr \/>\n<p>This is how I deployed the sample endpoint:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model = Model(model_data =...)\n\npredictor = model.deploy(initial_instance_count=...)\n<\/code><\/pre>\n<p>When I run the code, I get a model, an endpoint configuration, and an endpoint. I got the container type by clicking on model details within the AWS SageMaker console.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1594709403670,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1594788261467,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62889537",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":11.9,
        "Challenge_reading_time":19.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"TensorFlow Serving vs. TensorFlow Inference (container type for SageMaker model)",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1223.0,
        "Challenge_word_count":229,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1593662684510,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>There are different versions for the framework containers. Since the framework version I'm using is 1.15, the image I got had to be in a tensorflow-inference container. If I used versions &lt;= 1.13, then I would get sagemaker-tensorflow-serving images. The two aren't the same, but there's no 'correct' container type.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":4.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":50.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am attempting to develop an Azure ML experiment that uses R to perform predictions of a continuous response variable. The initial experiment is relatively simple, incorporating only a few experiment items, including \"Create R Model\", \"Train Model\" and \"Score Model\", along with some data input.<\/p>\n\n<p>I have written a training script and a scoring script, both of which appear to execute without errors when I run the experiment within ML Studio. However, when I examine the scored dataset, the score values are all missing values. So I am concerned that my scoring script could be returning scores incorrectly. Can anyone advise what type I should be returning? Is it meant to be a single column data.frame, or something else?<\/p>\n\n<p>It is also possible that my scores are not being properly calculated within the scoring script, although I have run the training and scoring scripts within R Studio, which shows the expected results. It would also be helpful if someone could suggest how to perform debugging of my scoring script in some way, so that I could determine whereabouts the code is failing to behave as expected.<\/p>\n\n<p>Thanks, Paul<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1464593030317,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37519858",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.8,
        "Challenge_reading_time":14.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"What type should the returned scores from an R scoring script?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":238.0,
        "Challenge_word_count":199,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1464571689107,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Try using this sample and compare with yours - <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Compare-Sample-5-in-R-vs-Azure-ML-1\" rel=\"nofollow\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Compare-Sample-5-in-R-vs-Azure-ML-1<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":41.8,
        "Solution_reading_time":3.6,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1360164540016,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Columbus, OH",
        "Answerer_reputation_count":11190.0,
        "Answerer_view_count":365.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>This is a hard situation to describe.<\/p>\n\n<p>I have a python model train script at:<\/p>\n\n<p><code>myproject\/opt\/program\/train<\/code><\/p>\n\n<p>This gets a file at <code>.\/opt\/ml\/input\/data\/external\/train.csv<\/code><\/p>\n\n<p>When I do <code>python3 opt\/program\/train<\/code> the training runs fine locally.<\/p>\n\n<p>Then I containerize the project and copy <code>opt<\/code> to <code>\/opt<\/code> in my Dockerfile.<\/p>\n\n<p>Now when I run <code>docker run &lt;image name&gt; train<\/code> it also trains fine.<\/p>\n\n<p>Then I deploy the image to SageMaker, create an estimator, and call <code>model.fit(my_data)<\/code> I get:<\/p>\n\n<p><code>Exception during training: [Errno 2] File b'.\/opt\/ml\/input\/data\/external\/train.csv' does not exist<\/code><\/p>\n\n<p>It's definitely there, I was able to train by running the container myself.  Also running the container and exploring the file system I can find the file.<\/p>\n\n<p>So I think I have some filesystem misunderstanding.  From the root of the container, all of these seem to have equivalent outputs.<\/p>\n\n<pre><code>root@798ffe7364c6:\/# ls opt\nml  program\nroot@798ffe7364c6:\/# ls \/opt\nml  program\nroot@798ffe7364c6:\/# ls .\/opt\nml  program\n<\/code><\/pre>\n\n<p>I'm trying to come up with a way to have one path that will work locally, in the container, and on AWS.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1573920322537,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1573920635263,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58892606",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":16.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker can't find paths in container",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2448.0,
        "Challenge_word_count":175,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1360164540016,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Columbus, OH",
        "Poster_reputation_count":11190.0,
        "Poster_view_count":365.0,
        "Solution_body":"<p>I was missing the fact that SageMaker looks for your data channels in S3 and copies those to your container at <code>\/opt\/ml\/input\/data<\/code><\/p>\n\n<p>By default it seems to use <code>training<\/code> and <code>validation<\/code> as the channel names.  Therefore, in my example above, it would have never copied data from my <code>external<\/code> folder on S3 to the right <code>external<\/code> folder in my container.  In fact, I discovered it was copying it instead to <code>\/opt\/ml\/input\/data\/training\/external\/train.csv<\/code>.<\/p>\n\n<p>To resolve this, I would have either had to change my folder names, or use <code>InputDataConfig<\/code> to define other channels.  I chose the later and was able to get it working.<\/p>\n\n<p>More info on <code>InputDataConfig<\/code> here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.2,
        "Solution_reading_time":12.56,
        "Solution_score_count":3.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":111.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1267440784443,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Somewhere",
        "Answerer_reputation_count":15705.0,
        "Answerer_view_count":2150.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I get a model from Sagemaker of type:<\/p>\n<pre><code>&lt;class 'xgboost.core.Booster'&gt;\n<\/code><\/pre>\n<p>I can score this locally which is great but some google searches have shown that it may not be possible to do &quot;standard&quot; things like this taken from <a href=\"https:\/\/mljar.com\/blog\/feature-importance-xgboost\/\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<pre><code>plt.barh(boston.feature_names, xgb.feature_importances_)\n<\/code><\/pre>\n<p>Is it possible to tranform xgboost.core.Booster to XGBRegressor? Maybe one could use the save_raw method looking at <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#xgboost.Booster\" rel=\"nofollow noreferrer\">this<\/a>? Thanks!<\/p>\n<p>So far I tried:<\/p>\n<pre><code>xgb_reg = xgb.XGBRegressor() \nxgb_reg._Boster = model\nxgb_reg.feature_importances_\n<\/code><\/pre>\n<p>but this reults in:<\/p>\n<pre><code>NotFittedError: need to call fit or load_model beforehand\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1651569762327,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1651579039263,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72097296",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":12.7,
        "Challenge_reading_time":13.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"cast xgboost.Booster class to XGBRegressor or load XGBRegressor from xgboost.Booster",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":172.0,
        "Challenge_word_count":100,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>Something along those lines appears to work fine:<\/p>\n<pre><code>local_model_path = &quot;model.tar.gz&quot;\nwith tarfile.open(local_model_path) as tar:\n    tar.extractall()\n\nmodel = xgb.XGBRegressor() \nmodel.load_model(model_file_name)     \n<\/code><\/pre>\n<p>model can then be used as usual - model.tar.gz is an artifcat coming from sagemaker.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.5,
        "Solution_reading_time":4.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I estimated a factorization machine model in sagemaker and it saved a file <code>model.tar.gz<\/code> into an s3 folder.<\/p>\n\n<p>Is there a way I can load this file in Python and access the parameter of the model, i.e. the factors, directly?<\/p>\n\n<p>Thanks<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1575827808930,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1575835104230,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59238265",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":3.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"sagemaker - factorization machines - deserialize model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":164.0,
        "Challenge_word_count":45,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1456487654208,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berlin, Germany",
        "Poster_reputation_count":1464.0,
        "Poster_view_count":62.0,
        "Solution_body":"<p>As of April 2019: yes. An official AWS blog post was created to show how to open the SageMaker Factorization Machines artifact and extract its parameters: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/extending-amazon-sagemaker-factorization-machines-algorithm-to-predict-top-x-recommendations\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/extending-amazon-sagemaker-factorization-machines-algorithm-to-predict-top-x-recommendations\/<\/a><\/p>\n\n<p>That being said, be aware that Amazon SageMaker built-in algorithm are primarily built for deployment on AWS, and only <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"nofollow noreferrer\">SageMaker XGBoost<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/blazingtext.html\" rel=\"nofollow noreferrer\">SageMaker BlazingText<\/a> are designed to produce artifacts interoperable with their open-source equivalent.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":26.7,
        "Solution_reading_time":12.79,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":4.8427816667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I registered a model in my AML workspace, and I can see it in the Model List:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rtL5Q.png\" rel=\"nofollow noreferrer\">Model List view<\/a><\/p>\n<p>But I cannot see it in Designer (preview), which prevents me from using the new model there.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/WpvIb.png\" rel=\"nofollow noreferrer\">Designer view<\/a><\/p>\n<p>Looks like a bug to me. Datasets work fine.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1596056234913,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63162310",
        "Challenge_link_count":2,
        "Challenge_participation_count":5,
        "Challenge_readability":7.5,
        "Challenge_reading_time":6.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Models registered in workspace do not show up in Designer (preview)",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":81.0,
        "Challenge_word_count":64,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1531852372996,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>This is known issue as the models registered in workspace cannot be consumed in Designer without the new custom module capability (in private preview) available.<\/p>\n<p>The models showing up in Designer today are these generated from Designer training -&gt; inference pipeline conversion and can only be used in Designer (not registered in the workspace).\nWe have an effort ongoing to reduce the confusion.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1596073668927,
        "Solution_link_count":0.0,
        "Solution_readability":12.7,
        "Solution_reading_time":5.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1446631384107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Helsinki, Finland",
        "Answerer_reputation_count":4255.0,
        "Answerer_view_count":877.0,
        "Challenge_adjusted_solved_time":0.5821208334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am running the <code>pipeline.submit()<\/code> in AzureML, which has a <code>PythonScriptStep<\/code>.\nInside this step, I download a model from tensorflow-hub, retrain it and save it as a <code>.zip<\/code>, and finally, I would like to register it in the Azure ML.\nBut as inside the script I do not have a workspace, <code>Model.register()<\/code> is not the case.\nSo I am trying to use <code>Run.register_model()<\/code> method as below:<\/p>\n\n<pre><code>os.replace(os.path.join('.', archive_name + '.zip'), \n           os.path.join('.', 'outputs', archive_name + '.zip'))\n\nprint(os.listdir('.\/outputs'))\nprint('========================')\n\nrun_context = Run.get_context()\nfinetuning_model = run_context.register_model(model_name='finetuning_similarity_model',\n                                              model_path=os.path.join(archive_name+'.zip'),\n                                              tags={},\n                                              description=\"Finetuning Similarity model\")\n<\/code><\/pre>\n\n<p>But then I have got an error:<\/p>\n\n<blockquote>\n  <p>ErrorResponse \n  {\n      \"error\": {\n          \"message\": \"Could not locate the provided model_path retrained.zip in the set of files uploaded to the run:<\/p>\n<\/blockquote>\n\n<p>despite I have the retrained <code>.zip<\/code> in the <code>.\/outputs<\/code> dir as we can see from the log:<\/p>\n\n<pre><code>['retrained.zip']\n========================\n<\/code><\/pre>\n\n<p>I guess that I am doing something wrong?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1574164584153,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1578744224352,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58933565",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":17.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"How to register model from the Azure ML Pipeline Script step",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3429.0,
        "Challenge_word_count":150,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1574162655727,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":75.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>I was able to fix the same issue (<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.exceptions.modelpathnotfoundexception?view=azure-ml-py\" rel=\"noreferrer\"><code>ModelPathNotFoundException<\/code><\/a>) by explicitly uploading the model into the run history record before trying to register the model:<\/p>\n\n<pre><code>run.upload_file(\"outputs\/my_model.pickle\", \"outputs\/my_model.pickle\")\n<\/code><\/pre>\n\n<p>Which I found surprising because this wasn't mentioned in many of the official examples and according to the <code>upload_file()<\/code> <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#upload-file-name--path-or-stream-\" rel=\"noreferrer\">documentation<\/a>:<\/p>\n\n<blockquote>\n  <p>Runs automatically capture file in the specified output directory, which defaults to \".\/outputs\" for most run types. Use upload_file only when additional files need to be uploaded or an output directory is not specified.<\/p>\n<\/blockquote>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1578746319987,
        "Solution_link_count":2.0,
        "Solution_readability":19.6,
        "Solution_reading_time":13.4,
        "Solution_score_count":14.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":88.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to convert a numpy array to an amazon protobuf record using <code>sagemaker.amazon.amazon_estimator.AmazonAlgorithmEstimatorBase.record_set()<\/code> However, this is taking a really long time. <\/p>\n\n<p>I'm wondering how the function actually performs and how long it should take<\/p>\n\n<pre><code>from sagemaker import LinearLearner\nimport numpy as np\n\nmodel=LinearLearner(role=get_execution_role(),\n                             train_instance_count=len(train_features),\n                             train_instance_type='ml.t2.medium',\n                             predictor_type='binary_classifier',\n                                )\n<\/code><\/pre>\n\n<pre><code>numpy_array = np.array([[7.4727994e-01 9.5506465e-01 7.6940370e-01 8.2015032e-01 1.8113719e-01\n  7.8720862e-01 2.9677063e-01 2.6711187e-01 7.9498607e-01 4.4924998e-01\n  4.9533784e-01 2.6846960e-01 7.0506859e-01 4.1573554e-01 6.5843487e-01\n  3.2448095e-01 4.3870610e-01 7.2739214e-01 6.0914969e-01 5.5108833e-01\n  5.8835250e-01 5.5872935e-01 4.4392920e-01 6.8353373e-01 4.7664520e-01\n  5.6887656e-01 4.7034043e-01 4.1631639e-01 3.1357434e-01 5.5933639e-04]\n [5.7815754e-01 9.5828843e-01 7.7824914e-01 8.3188844e-01 2.3287645e-01\n  7.7196079e-01 2.5512937e-01 2.7032304e-01 7.8349811e-01 5.0130588e-01\n  4.8345023e-01 3.8397798e-01 5.9922373e-01 4.7720599e-01 6.7832541e-01\n  2.7788603e-01 4.6435007e-01 7.6100332e-01 7.7771670e-01 5.1536995e-01\n  5.8536130e-01 5.6407303e-01 5.0898582e-01 6.7815554e-01 3.0614817e-01\n  5.7353836e-01 3.8981739e-01 4.1474316e-01 3.1389123e-01 3.5031504e-04]]) \n<\/code><\/pre>\n\n<pre><code>record=model.record_set(numpy_array)\n<\/code><\/pre>\n\n<h2>Expected output<\/h2>\n\n<p>I expect the variable record to container a record ready for training with linearlearning model<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1563161763097,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57032981",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.6,
        "Challenge_reading_time":23.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"how long does converting to amazon protobuf record using .record_set() take to complete",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":119.0,
        "Challenge_word_count":143,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1515320453680,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangkok",
        "Poster_reputation_count":1848.0,
        "Poster_view_count":206.0,
        "Solution_body":"<p>I believe this is the problem:<\/p>\n\n<pre><code>train_instance_count=len(train_features)\n<\/code><\/pre>\n\n<p>This parameter is about infrastructure (how many SageMaker instances you want to train on), not about features. You should set it to 1.<\/p>\n\n<pre><code>import sagemaker\nfrom sagemaker import LinearLearner\nimport numpy as np\n\nmodel=LinearLearner(role=sagemaker.get_execution_role(),\n                             train_instance_count=1,\n                             train_instance_type='ml.t2.medium',\n                             predictor_type='binary_classifier')\n\nnumpy_array = np.array(...)\n\nrecord=model.record_set(numpy_array)\n# This takes &lt;100 ms on my t3 notebook instance\n\nprint(record)\n\n(&lt;class 'sagemaker.amazon.amazon_estimator.RecordSet'&gt;, {'s3_data':\n's3:\/\/sagemaker-eu-west-1-123456789012\/sagemaker-record-sets\/LinearLearner-\n2019-07-18-09-48-21-639\/.amazon.manifest', 'feature_dim': 30, 'num_records': 2,\n's3_data_type': 'ManifestFile', 'channel': 'train'})\n<\/code><\/pre>\n\n<p>The manifest file lists the protobuf-encoded file(s):<\/p>\n\n<pre><code>[{\"prefix\": \"s3:\/\/sagemaker-eu-west-1-123456789012\/sagemaker-record-sets\/LinearLearner-2019-07-18-09-48-21-639\/\"}, \"matrix_0.pbr\"]\n<\/code><\/pre>\n\n<p>You can now use it for the training channel when you call fit(), re: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_S3DataSource.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_S3DataSource.html<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":19.0,
        "Solution_reading_time":18.8,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":101.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm thinking of deploying a TensorFlow model using Vertex AI in GCP. I am almost sure that the cost will be directly related to the number of queries per second (QPS) because I am going to use automatic scaling. I also know that the type of machine (with GPU, TPU, etc.) will have an impact on the cost.<\/p>\n<ul>\n<li>Do you have any estimation about the cost versus the number of queries per second?<\/li>\n<li>How does the type of virtual machine changes this cost?<\/li>\n<\/ul>\n<p>The type of model is for object detection.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657258379287,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1657258776587,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72907038",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":6.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Cost of deploying a TensorFlow model in GCP?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":74.0,
        "Challenge_word_count":100,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1569457921527,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Luis Potos\u00ed, S.L.P., M\u00e9xico",
        "Poster_reputation_count":41.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>Autoscaling depends on the CPU and GPU utilization which directly correlates to the QPS, as you have said. To estimate the cost based on the QPS, you can deploy a custom prediction container to a Compute Engine instance directly, then benchmark the instance by making prediction calls until the VM hits 90+ percent CPU utilization (consider GPU utilization if configured). Do this multiple times for different machine types, and determine the &quot;QPS per cost per hour&quot; of different machine types. You can re-run these experiments while benchmarking latency to find the <strong>ideal cost per QPS per your latency targets<\/strong> for your specific custom prediction container. For more information about choosing the ideal machine for your workload, refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#finding_the_ideal_machine_type\" rel=\"nofollow noreferrer\">documentation<\/a>.<\/p>\n<p>For your second question, as per the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/pricing#custom-trained_models:%7E:text=a%20specific%20job.-,Prediction%20and%20explanation,-This%20table%20provides\" rel=\"nofollow noreferrer\">Vertex AI pricing documentation<\/a> (for model deployment), cost estimation is done based on the node hours. A node hour represents the time a virtual machine spends running your prediction job or waiting in a ready state to handle prediction or explanation requests. Each type of VM offered has a specific pricing per node hour depending on the number of cores and the amount of memory. Using a VM with more resources will cost more per node hour and vice versa. To choose an ideal VM for your deployment, please follow the steps given in the first paragraph which will help you find a good trade off between cost and performance.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.4,
        "Solution_reading_time":22.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":243.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1601729162436,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bengaluru, Karnataka, India",
        "Answerer_reputation_count":887.0,
        "Answerer_view_count":130.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Azure Machine Learning Service's Model Artifact has the ability to store references to the Datasets associated with the model. We can use <code>azureml.core.model.Model.add_dataset_references([('relation-as-a-string', Dataset)])<\/code> to add these dataset references.\nHow do we retrieve a Dataset from the references stored in this Model class by using a reference to the Model Class?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623658430013,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67966905",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.2,
        "Challenge_reading_time":5.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How to extract a dataset from azureml.core.model.Model Class?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":225.0,
        "Challenge_word_count":58,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>Consider that a Dataset was added as a reference to a Model with the name <code>'training_dataset'<\/code><\/p>\n<p>In order to get a reference to this Dataset we use:<\/p>\n<pre><code>model = Model(workspace, name)\ndataset_id = next(dictionary['id'] for dictionary in model.serialize()['datasets'] if dictionary[&quot;name&quot;] == 'training_dataset')\ndataset_reference = Dataset.get_by_id(workspace, dataset_id )\n<\/code><\/pre>\n<p>After this step we can use <code>dataset_reference<\/code> as any other AzureML Dataset Class object.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.1,
        "Solution_reading_time":6.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":57.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1460437080990,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":386.0,
        "Answerer_view_count":42.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to reduce my development headaches for creating a ML Webservice on Azure ML Studio. One of the things that stuck me was can we just upload .rda files in the workbench and load it via an RScript (like in the figure below). <\/p>\n\n<p><img src=\"https:\/\/raw.githubusercontent.com\/pratos\/pratos.github.io\/master\/images\/stackb1model.png\" alt=\"Do\"><\/p>\n\n<p>But can't connect directly to the R Script block. There's another way to do it (works to upload packages that aren't available in Azure's R directories) -- using zip. But there isn't really any resource out there that I found to access the .rda file in .zip.<\/p>\n\n<p>I have 2 options here, make the .zip work or any other work around where I can directly use my .rda model. If someone could guide me about how to go forward it would appreciate it.<\/p>\n\n<p>Note: Currently, I'm creating models via the \"Create RModel\" block, training them and saving it, so that I can use it to make a predictive web service. But for models like Random Forest, not sure how the randomness might create models (local versions and Azure versions are different, the setting of seed also isn't very helpful). A bit tight on schedule, Azure ML seems boxed for creating iterations and automating the ML workflow (or maybe I'm doing it wrong).<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1486101642917,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42017727",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":16.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"Upload Saved ML Model in R (local) to Azure Machine Learning Studio",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":730.0,
        "Challenge_word_count":221,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1479363468550,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":108.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>Here is an example of uploading a .rda file for scoring:\n<a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Womens-Health-Risk-Assessment-using-the-XGBoost-classification-algorithm-1\" rel=\"nofollow noreferrer\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Womens-Health-Risk-Assessment-using-the-XGBoost-classification-algorithm-1<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":48.9,
        "Solution_reading_time":5.01,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":15.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1468179475927,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pasadena, CA, United States",
        "Answerer_reputation_count":795.0,
        "Answerer_view_count":210.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I built a linear regression algorithm in Azure ML. On the &quot;Score Model&quot; module I can actually see the predictions and the rest of the features. However, when I deploy this project as a web service, the service is expecting the actual label of the data (e.g. I'm trying to predict a house's price and it asks me for the price of the house to make the prediction), which doesn't make any sense to me... What am I doing wrong? On the &quot;Train Model&quot; module I set that the label column is the HousePrice, which is what I'm trying to predict...<\/p>\n<p>This is my model:\n<a href=\"https:\/\/i.stack.imgur.com\/kI8qu.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kI8qu.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I tried leaving that field blank but the prediction returns null...<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1476382562787,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40028165",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":7.3,
        "Challenge_reading_time":10.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML's web service asking for label?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1012.0,
        "Challenge_word_count":132,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1401729936860,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1102.0,
        "Poster_view_count":120.0,
        "Solution_body":"<p>The input schema (names\/types of required input) based on the location in the graph where you attach the \"Web Service Input\" module. To get the schema you want, you will need to find -- or if necessary, create -- a place in the experiment where the data has the column names\/types you desire.<\/p>\n\n<p>Consider this simple example experiment that predicts whether a field called \"income\" will be above or below $50k\/year:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/nWaN2.png\" rel=\"nofollow\"><img src=\"https:\/\/i.stack.imgur.com\/nWaN2.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>When we click \"Set up web service\", the following graph is automatically generated:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/NMMpV.png\" rel=\"nofollow\"><img src=\"https:\/\/i.stack.imgur.com\/NMMpV.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Since the input dataset and \"Web service input\" modules are connected to the same port, the web service schema will perfectly match the schema of the input dataset. This is unfortunate because the input dataset contains a column called \"income\", which is what our web service is supposed to predict -- this is equivalent to the problem that you are having.<\/p>\n\n<p>To get around it, we need to create a place in our experiment graph where we've dropped the unneeded \"income\" field from the input dataset, and attach the \"Web service input\" module there:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/WPeSB.png\" rel=\"nofollow\"><img src=\"https:\/\/i.stack.imgur.com\/WPeSB.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>With this arrangement, the web service only requests the features actually needed to score the model. I'm sure you can use a similar method to create a predictive experiment with whatever input schema you need for your own work.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":12.2,
        "Solution_reading_time":22.59,
        "Solution_score_count":3.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":238.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1565022238448,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":370.0,
        "Answerer_view_count":18.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a simple RegisterModel.py script that uses the Azure ML Service SDK to register a fastText .bin model. This completes successfully and I can see the model in the Azure Portal UI (I cannot see what model files are in it). I then want to download the model (DownloadModel.py) and use it (for testing purposes), however it throws an error on the <strong>model.download<\/strong> method (<em>tarfile.ReadError: file could not be opened successfully<\/em>) and makes a 0 byte rjtestmodel8.tar.gz file.<\/p>\n\n<p>I then use the Azure Portal and Add Model and select the same bin model file and it uploads fine. Downloading it with the download.py script below works fine, so I am assuming something is not correct with the Register script.<\/p>\n\n<p>Here are the 2 scripts and the stacktrace - let me know if you can see anything wrong:<\/p>\n\n<p><strong>RegisterModel.py<\/strong><\/p>\n\n<pre><code>import azureml.core\nfrom azureml.core import Workspace, Model\nws = Workspace.from_config()\nmodel = Model.register(workspace=ws,\n                       model_name='rjSDKmodel10',\n                       model_path='riskModel.bin')\n<\/code><\/pre>\n\n<p><strong>DownloadModel.py<\/strong><\/p>\n\n<pre><code># Works when downloading the UI Uploaded .bin file, but not the SDK registered .bin file\nimport os\nimport azureml.core\nfrom azureml.core import Workspace, Model\n\nws = Workspace.from_config()\nmodel = Model(workspace=ws, name='rjSDKmodel10')\nmodel.download(target_dir=os.getcwd(), exist_ok=True)\n<\/code><\/pre>\n\n<p><strong>Stacktrace<\/strong><\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"...\\.vscode\\extensions\\ms-python.python-2019.9.34474\\pythonFiles\\ptvsd_launcher.py\", line 43, in &lt;module&gt;\n    main(ptvsdArgs)\n  File \"...\\.vscode\\extensions\\ms-python.python-2019.9.34474\\pythonFiles\\lib\\python\\ptvsd\\__main__.py\", line 432, in main\n    run()\n  File \"...\\.vscode\\extensions\\ms-python.python-2019.9.34474\\pythonFiles\\lib\\python\\ptvsd\\__main__.py\", line 316, in run_file\n    runpy.run_path(target, run_name='__main__')\n  File \"...\\.conda\\envs\\DoC\\lib\\runpy.py\", line 263, in run_path\n    pkg_name=pkg_name, script_name=fname)\n  File \"...\\.conda\\envs\\DoC\\lib\\runpy.py\", line 96, in _run_module_code\n    mod_name, mod_spec, pkg_name, script_name)\n  File \"...\\.conda\\envs\\DoC\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"...\\\\DownloadModel.py\", line 21, in &lt;module&gt;\n    model.download(target_dir=os.getcwd(), exist_ok=True)\n  File \"...\\.conda\\envs\\DoC\\lib\\site-packages\\azureml\\core\\model.py\", line 712, in download\n    file_paths = self._download_model_files(sas_to_relative_download_path, target_dir, exist_ok)\n  File \"...\\.conda\\envs\\DoC\\lib\\site-packages\\azureml\\core\\model.py\", line 658, in _download_model_files\n    file_paths = self._handle_packed_model_file(tar_path, target_dir, exist_ok)\n  File \"...\\.conda\\envs\\DoC\\lib\\site-packages\\azureml\\core\\model.py\", line 670, in _handle_packed_model_file\n    with tarfile.open(tar_path) as tar:\n  File \"...\\.conda\\envs\\DoC\\lib\\tarfile.py\", line 1578, in open\n    raise ReadError(\"file could not be opened successfully\")\ntarfile.ReadError: file could not be opened successfully\n<\/code><\/pre>\n\n<p><strong>Environment<\/strong><\/p>\n\n<ul>\n<li>riskModel.bin is 6 megs<\/li>\n<li>AMLS 1.0.60<\/li>\n<li>Python 3.7<\/li>\n<li>Working locally with Visual Code<\/li>\n<\/ul>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1568032306497,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1568060910630,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57854136",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":12.0,
        "Challenge_reading_time":43.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":46,
        "Challenge_solved_time":null,
        "Challenge_title":"Registering and downloading a fastText .bin model fails with Azure Machine Learning Service",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":281.0,
        "Challenge_word_count":327,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1256089885500,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sydney, Australia",
        "Poster_reputation_count":4947.0,
        "Poster_view_count":531.0,
        "Solution_body":"<p>The Azure Machine Learning service SDK has a bug with how it interacts with Azure Storage, which causes it to upload corrupted files if it has to retry uploading. <\/p>\n\n<p>A couple workarounds:<\/p>\n\n<ol>\n<li>The bug was introduced in 1.0.60 release. If you downgrade to AzureML-SDK 1.0.55, the code should fail when there are issue uploading instead of silently corrupting data.<\/li>\n<li>It's possible that the retry is being triggered by the low timeout values that the AzureML-SDK defaults to. You could investigate changing the timeout in <code>site-packages\/azureml\/_restclient\/artifacts_client.py<\/code><\/li>\n<\/ol>\n\n<p>This bug should be fixed in the next release of the AzureML-SDK.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1568054711300,
        "Solution_link_count":0.0,
        "Solution_readability":8.9,
        "Solution_reading_time":8.71,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":100.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1512770138847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to use a local training job in SageMaker.<\/p>\n<p>Following this AWS notebook (<a href=\"http:\/\/mxnet_mnist_with_gluon_local_mode.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mxnet_mnist_with_gluon_local_mode.ipynb<\/a>) I was able to train and predict locally.<\/p>\n<p>There is any way to train locally and save the trained model in the Amazon SageMaker Training Job section?\nOtherwise, how can I properly save trained models I trained using local mode?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595954217667,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1595959142728,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63138835",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":12.9,
        "Challenge_reading_time":8.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How to save models trained locally in Amazon SageMaker?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3195.0,
        "Challenge_word_count":66,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1464391892936,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Rio de Janeiro, State of Rio de Janeiro, Brazil",
        "Poster_reputation_count":2243.0,
        "Poster_view_count":148.0,
        "Solution_body":"<p>There is no way to have your local mode training jobs appear in the AWS console. The intent of local mode is to allow for faster iteration\/debugging before using SageMaker for training your model.<\/p>\n<p>You can create SageMaker Models from local model artifacts. Compress your model artifacts into a <code>.tar.gz<\/code> file, upload that file to S3, and then create the Model (with the SDK or in the console).<\/p>\n<p>Documentation:<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":18.5,
        "Solution_reading_time":12.35,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":79.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1280527017200,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3035.0,
        "Answerer_view_count":129.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to use the ML model I created in AWS in my QuickSight reports.<\/p>\n\n<ul>\n<li>Is there a way to consume the ML endpoint in order to run batch predictions in QuickSight?<\/li>\n<li>Can I define a 'calculated field' in order to do that?<\/li>\n<\/ul>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533640683263,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51725489",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.6,
        "Challenge_reading_time":3.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"QuickSight using ML",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":490.0,
        "Challenge_word_count":48,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501420854056,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":507.0,
        "Poster_view_count":78.0,
        "Solution_body":"<p>At this time there is no direct integration with AWS SageMaker and QuickSight, however you can use utilize SageMaker's batch transform jobs to convert data outside of QuickSight and then import this information into QuickSight for visualization. The output format for SageMaker's batch transform jobs is S3, which is a supported input data source for QuickSight.<\/p>\n\n<ul>\n<li><a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2018\/07\/amazon-sagemaker-supports-high-throughput-batch-transform-jobs-for-non-real-time-inferencing\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/about-aws\/whats-new\/2018\/07\/amazon-sagemaker-supports-high-throughput-batch-transform-jobs-for-non-real-time-inferencing\/<\/a><\/li>\n<\/ul>\n\n<p>Depending on how fancy you want to be, you can also integrate calls to AWS services such as AWS Lambda or AWS SageMaker as a user-defined function (UDF) within your datastore. Here are a few resources that may help:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/redshift\/latest\/dg\/user-defined-functions.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/redshift\/latest\/dg\/user-defined-functions.html<\/a><\/li>\n<li><a href=\"https:\/\/aws.amazon.com\/blogs\/big-data\/from-sql-to-microservices-integrating-aws-lambda-with-relational-databases\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/big-data\/from-sql-to-microservices-integrating-aws-lambda-with-relational-databases\/<\/a><\/li>\n<\/ul>\n\n<p>Calculated fields will probably not help you in this regard - calculated fields are restricted to a relatively small set of operations, and none of these operations support calls to external sources.<\/p>\n\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/quicksight\/latest\/user\/calculated-field-reference.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/quicksight\/latest\/user\/calculated-field-reference.html<\/a> <\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":8.0,
        "Solution_readability":20.8,
        "Solution_reading_time":25.18,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":149.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1520413126203,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":37123.0,
        "Answerer_view_count":4058.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I trained a model in AZURE ML. Now i want to use that model in my ios app to predict the output\u00a0.<\/p>\n\n<p>How to download the model from AZURE and use it my swift code.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1525678856837,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1558224843256,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50209284",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":3.0,
        "Challenge_reading_time":2.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"How to use the trained model developed in AZURE ML",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":516.0,
        "Challenge_word_count":45,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1510206999776,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":81.0,
        "Poster_view_count":41.0,
        "Solution_body":"<p>As far as I know, the model could run in <strong>Azure Machine Learning Studio<\/strong>.It seems that you are unable to download it, the model could do nothing outside of Azure ML. <\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/41236871\/how-to-download-the-trained-models-from-azure-machine-studio\">Here<\/a> is a similar post for you to refer, I have also tried @Ahmet's \nmethod, but result is like @mrjrdnthms says.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1525850503192,
        "Solution_link_count":1.0,
        "Solution_readability":8.6,
        "Solution_reading_time":5.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've built models using the AutoML function and I'm trying to call the best model to deploy into production. The AutoML function ran correctly and produced the ~35 models. My goal is to pull the best model. Here is the code:  <\/p>\n<p>best_run, fitted_model = remote_run.get_output()  <br \/>\nfitted_model  <\/p>\n<p>When runnning the code, I get the following error:   <\/p>\n<p>AttributeError: 'DataTransformer' object has no attribute 'enable_dnn'  <\/p>\n<p>Any help would be much appreciated.   <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613083790957,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/270011\/remote-run-model-unable-to-be-saved",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":6.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Remote run model unable to be saved",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":79,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=f0b99777-9086-42ac-b2e2-2b069641e943\">@Bernardo Jaccoud  <\/a> Did your run configure enable_dnn i.e bert <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-features#bert-integration-in-automated-ml\">settings<\/a> of automated ML? I am curious to understand what the status of your run is directly on the portal ml.azure.com?    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.9,
        "Solution_reading_time":5.3,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I developed a designer to implement regression models in azure machine learning studio. I have taken the data set pill and then split the data set into train and test in prescribed manner. When I am trying to implement the evaluation metrics and run the pipeline, it was showing a warning and error in the moment I called the dataset for the operation. I am bit confused, with the same implementation, when i tried to run with linear regression and it worked as shown in the image. If the same approach is used to implement logistic regression it was showing some warning and error in building the evaluation metrics.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/DbEeq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DbEeq.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>the above success is in linear regression. When it comes to logistic regression it was showing the warning and error in pipeline.<\/p>\n<p>Any help is appreciated.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663970010913,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73833320",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":13.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"parameters error in azure ML designer in evaluation metrics in regression model",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":29.0,
        "Challenge_word_count":156,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1652172570283,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":19.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>Creating a sample pipeline with designer with mathematical format.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/kCx3A.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kCx3A.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>We need to create a compute instance.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/7bQA5.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7bQA5.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/NQWZV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NQWZV.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/MpxPY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/MpxPY.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bAfEv.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bAfEv.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Assign the compute instance and click on create<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wIS0d.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wIS0d.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Now the import data warning will be removed. In the same manner, we will be getting similar error in other pills too.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/zFK74.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zFK74.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Yk5gY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Yk5gY.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Create a mathematical format. If not needed for your case, try to remove that math operation and give the remaining.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/uhKlv.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uhKlv.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Assign the column set. Select any option according to the requirement.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/mcNZe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mcNZe.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Finally, we can find the pills which have no warning or error.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":20.0,
        "Solution_readability":14.1,
        "Solution_reading_time":30.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":189.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Sagemaker is a great tool to train your models, and we save some money by using AWS spot instances. However, training jobs sometimes get stopped in the middle. We are using some mechanisms to continue from the latest checkpoint after a restart. See also the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-checkpoints.html\" rel=\"nofollow noreferrer\">docs<\/a>.<\/p>\n<p>Still, how do you efficiently test such a mechanism? Can you trigger it yourself? Otherwise you have to wait until the spot instance actually \u00eds restarted.<\/p>\n<p>Also, are you expected to use the linked <code>checkpoint_s3_uri<\/code> argument or the <code>model_dir<\/code> for this? E.g. the <code>TensorFlow<\/code> estimator <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/sagemaker.tensorflow.html#tensorflow-estimator\" rel=\"nofollow noreferrer\">docs<\/a> seem to suggest something <code>model_dir<\/code>for checkpoints.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616592352363,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1616592807080,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66782040",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":12.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Reloading from checkpoing during AWS Sagemaker Training",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":286.0,
        "Challenge_word_count":110,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1484838464572,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Amsterdam, Nederland",
        "Poster_reputation_count":3937.0,
        "Poster_view_count":387.0,
        "Solution_body":"<p>Since you can't manually terminate a sagemaker instance, run an Amazon SageMaker Managed Spot training for a small number of epochs, Amazon SageMaker would have backed up your checkpoint files to S3. Check that checkpoints are there. Now run a second training run, but this time provide the first jobs\u2019 checkpoint location to <code>checkpoint_s3_uri<\/code>. Reference is <a href=\"https:\/\/towardsdatascience.com\/a-quick-guide-to-using-spot-instances-with-amazon-sagemaker-b9cfb3a44a68\" rel=\"nofollow noreferrer\">here<\/a>, this also answer your second question.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.8,
        "Solution_reading_time":7.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to training TensorFlow model on AWS Sagemaker.\nI created container with external lib for that (Use Your Own Algorithms or Models with Amazon SageMaker).<\/p>\n\n<p>we run a training job with TensorFlow API<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.tensorflow import TensorFlow\nestimator = TensorFlow(\n  entry_point=\"entry.py\",             # entry script\n  role=role,\n  framework_version=\"1.13.0\",   \n  py_version='py3',\n  hyperparameters=hyperparameters,\n  train_instance_count=1,                   # \"The number of GPUs instances to use\"\n  train_instance_type=train_instance_type,\n  image_name=my_image\n\n)\nestimator.fit({'train': train_s3, 'eval': eval_s3})\n<\/code><\/pre>\n\n<p>and got an error:<\/p>\n\n<pre><code>09:06:46\n2019-07-23 09:06:45,463 INFO - root - running container entrypoint \uf141\n09:06:46\n2019-07-23 09:06:45,463 INFO - root - starting train task \uf141\n09:06:46\n2019-07-23 09:06:45,476 INFO - container_support.training - Training starting \uf141\n09:06:46\n2019-07-23 09:06:45,479 ERROR - container_support.training - uncaught exception during training: No module named 'tf_container'\n\uf141\n09:06:46\nTraceback (most recent call last): File \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/environment.py\", line 136, in load_framework return importlib.import_module('mxnet_container') File \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File \"&lt;frozen importlib._bootstrap&gt;\", line 994, in _gcd_i \uf141\n09:06:46\nModuleNotFoundError: No module named 'mxnet_container'\n\uf141\n09:06:46\nDuring handling of the above exception, another exception occurred:\n\uf141\n09:06:46\nTraceback (most recent call last): File \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/training.py\", line 35, in start fw = TrainingEnvironment.load_framework() File \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/environment.py\", line 138, in load_framework return importlib.import_module('tf_container') File \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, \uf141\n09:06:46\nModuleNotFoundError: No module named 'tf_container'\n<\/code><\/pre>\n\n<p>What can I do to solve this issue? how can I debug this case?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1563885123080,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57164290",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":29.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":null,
        "Challenge_title":"Training with TensorFlow on Sagemaker No module named 'tf_container'",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1191.0,
        "Challenge_word_count":206,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1471952551663,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I'm guessing that you used your own TF container, not the SageMaker one at <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container<\/a><\/p>\n\n<p>If that's the case, your container is missing the support code needed to use the TensorFlow estimator ('tf_container' package).<\/p>\n\n<p>The solution is to start from the SageMaker container, customize it, push it back to ECR and pass the image name to the SageMaker estimator with the 'image_name' parameter.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.4,
        "Solution_reading_time":7.04,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1392296244356,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Chicago, IL, USA",
        "Answerer_reputation_count":1020.0,
        "Answerer_view_count":206.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am finetuning multiple models using for loop as follows.<\/p>\n<pre><code>for file in os.listdir(args.data_dir):\n    finetune(args, file)\n<\/code><\/pre>\n<p>BUT <code>wandb<\/code> website shows plots and logs only for the first file i.e., <code>file1<\/code> in <code>data_dir<\/code> although it is training and saving models for other files. It feels very strange behavior.<\/p>\n<pre><code>wandb: Synced bertweet-base-finetuned-file1: https:\/\/wandb.ai\/***\/huggingface\/runs\/***\n<\/code><\/pre>\n<p>This is a small snippet of <strong>finetuning<\/strong> code with Huggingface:<\/p>\n<pre><code>def finetune(args, file):\n    training_args = TrainingArguments(\n        output_dir=f'{model_name}-finetuned-{file}',\n        overwrite_output_dir=True,\n        evaluation_strategy='no',\n        num_train_epochs=args.epochs,\n        learning_rate=args.lr,\n        weight_decay=args.decay,\n        per_device_train_batch_size=args.batch_size,\n        per_device_eval_batch_size=args.batch_size,\n        fp16=True, # mixed-precision training to boost speed\n        save_strategy='no',\n        seed=args.seed,\n        dataloader_num_workers=4,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset['train'],\n        eval_dataset=None,\n        data_collator=data_collator,\n    )\n    trainer.train()\n    trainer.save_model()\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650379721377,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1650406117732,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71926953",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":15.8,
        "Challenge_reading_time":17.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Wandb website for Huggingface Trainer shows plots and logs only for the first model",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":178.0,
        "Challenge_word_count":108,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1392296244356,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Chicago, IL, USA",
        "Poster_reputation_count":1020.0,
        "Poster_view_count":206.0,
        "Solution_body":"<p><code>wandb.init(reinit=True)<\/code> and <code>run.finish()<\/code> helped me to log the models <strong>separately<\/strong> on wandb website.<\/p>\n<p>The working code looks like below:<\/p>\n<pre><code>\nfor file in os.listdir(args.data_dir):\n    finetune(args, file)\n\nimport wandb\ndef finetune(args, file):\n    run = wandb.init(reinit=True)\n    ...\n    run.finish()\n<\/code><\/pre>\n<p>Reference: <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/launch#how-do-i-launch-multiple-runs-from-one-script\" rel=\"nofollow noreferrer\">https:\/\/docs.wandb.ai\/guides\/track\/launch#how-do-i-launch-multiple-runs-from-one-script<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.5,
        "Solution_reading_time":8.14,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":40.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How is the output of Fisher Linear Discriminant Analysis experiment interpreted now that the column labels in the output are replaced with Col1, Col2, Col3.......etc? How can the model be used to predict clusters of other input data as deployed web service requires even the dependent valuable(the same same ones we wish to predict)?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1621855005240,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/407053\/fisher-linear-discriminant-analysis-azure",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":4.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Fisher Linear Discriminant Analysis Azure",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":58,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Are you referring to the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/latent-dirichlet-allocation#lda-transformed-dataset\">categories<\/a> generated from LDA module? If so, then that's expected. LDA is an unsupervised technique, it groups words into categories\/topics and it's up to the analyst to interpret it by observing the results and transforming the output dataset accordingly. Here's are some <a href=\"https:\/\/gallery.azure.ai\/browse?s=lda\">examples<\/a> of LDA approach in Azure AI Gallery. Hope this helps.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.4,
        "Solution_reading_time":7.45,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":61.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1556553177963,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Singapore",
        "Answerer_reputation_count":347.0,
        "Answerer_view_count":93.0,
        "Challenge_adjusted_solved_time":465.8558136111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a mismatch in shapes between inputs and the model of my reinforcement learning project.<\/p>\n\n<p>I have been closely following the AWS examples, specifically the cartpole example. However I have built my own custom environment. What I am struggling to understand is how to change my environment so that it is able to work with the prebuilt Ray RLEstimator.<\/p>\n\n<p>Here is the code for the environment:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from enum import Enum\nimport math\n\nimport gym\nfrom gym import error, spaces, utils, wrappers\nfrom gym.utils import seeding\nfrom gym.envs.registration import register\nfrom gym.spaces import Discrete, Box\n\n\nimport numpy as np\n\n# from float_space import FloatSpace\n\n\ndef sigmoid_price_fun(x, maxcust, gamma):\n    return maxcust \/ (1 + math.exp(gamma * max(0, x)))\n\n\nclass Actions(Enum):\n    DECREASE_PRICE = 0\n    INCREASE_PRICE = 1\n    HOLD = 2\n\n\nPRICE_ADJUSTMENT = {\n    Actions.DECREASE_PRICE: -0.25,\n    Actions.INCREASE_PRICE: 0.25,\n    Actions.HOLD: 0\n}\n\n\nclass ArrivalSim(gym.Env):\n    \"\"\" Simple environment for price optimising RL learner. \"\"\"\n\n\n    def __init__(self, price):\n        \"\"\"\n        Parameters\n        ----------\n        price : float\n            The initial price to use.\n        \"\"\"\n        super().__init__()\n        self.price = price\n        self.revenue = 0\n        self.action_space = Discrete(3)  # [0, 1, 2]  #increase or decrease\n        self.observation_space = Box(np.array(0.0),np.array(1000))\n#         self.observation_space = FloatSpace(price)\n\n    def step(self, action):\n        \"\"\" Enacts the specified action in the environment.\n\n        Returns the new price, reward, whether we're finished and an empty dict for compatibility with Gym's\n        interface. \"\"\"\n\n        self._take_action(Actions(action))\n        next_state = self.price\n#         next_state = self.observation_space.sample()\n        reward = self._get_reward()\n        done = False\n\n        if next_state &lt; 0 or reward == 0:\n            done = True\n\n        print(next_state, reward, done, {})\n\n        return np.array(next_state), reward, done, {}\n\n    def reset(self):\n        \"\"\" Resets the environment, selecting a random initial price. Returns the price. \"\"\"\n\n#         self.observation_space.value = np.random.rand()\n#         return self.observation_space.sample()\n        self.price = np.random.rand()\n        return self.price\n\n    def _take_action(self, action):\n#         self.observation_space.value += PRICE_ADJUSTMENT[action]\n        self.price += PRICE_ADJUSTMENT[action]\n\n    def _get_reward(self,price):\n#         price = self.observation_space.value\n#         return max(np.random.poisson(sigmoid_price_fun(price, 50, 0.5)) * price, 0)\n        self.revenue = max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)\n        return max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)\n\n\n#     def render(self, mode='human'):\n#         super().render(mode)\n\ndef testEnv():\n    register(\n        id='ArrivalSim-v0',\n        entry_point='env:ArrivalSim',\n        kwargs= {'price' : 40}\n    )\n    env = gym.make('ArrivalSim-v0')\n\n    env.reset()\n    for _ in range(20):\n        test = env.action_space.sample()\n        print(test)\n        print(env.observation_space)\n        env.step(test)  # take a random action\n    env.close()\n\n\n\nif __name__ =='__main__':\n\n    testEnv()\n\n<\/code><\/pre>\n\n<p>Here is the training script<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import json\nimport os\n\nimport gym\nimport ray\nfrom ray.tune import run_experiments\nfrom ray.tune.registry import register_env\nfrom gym.envs.registration import register\n\nfrom sagemaker_rl.ray_launcher import SageMakerRayLauncher\n\n\ndef create_environment(env_config):\n    import gym\n#     from gym.spaces import Space\n    from gym.envs.registration import register\n\n    # This import must happen inside the method so that worker processes import this code\n    register(\n        id='ArrivalSim-v0',\n        entry_point='env:ArrivalSim',\n        kwargs= {'price' : 40}\n    )\n    return gym.make('ArrivalSim-v0')\n\n\n\nclass MyLauncher(SageMakerRayLauncher):\n\n    def register_env_creator(self):\n        register_env(\"ArrivalSim-v0\", create_environment)\n\n    def get_experiment_config(self):\n        return {\n          \"training\": {\n            \"env\": \"ArrivalSim-v0\",\n            \"run\": \"PPO\",\n            \"stop\": {\n              \"episode_reward_mean\": 5000,\n            },\n            \"config\": {\n              \"gamma\": 0.995,\n              \"kl_coeff\": 1.0,\n              \"num_sgd_iter\": 10,\n              \"lr\": 0.0001,\n              \"sgd_minibatch_size\": 32768,\n              \"train_batch_size\": 320000,\n              \"monitor\": False,  # Record videos.\n              \"model\": {\n                \"free_log_std\": False\n              },\n              \"use_gae\": False,\n              \"num_workers\": (self.num_cpus-1),\n              \"num_gpus\": self.num_gpus,\n              \"batch_mode\": \"complete_episodes\"\n\n            }\n          }\n        }\n\nif __name__ == \"__main__\":\n    MyLauncher().train_main()\n<\/code><\/pre>\n\n<p>Here is the code I run in Jupyter:<\/p>\n\n<pre><code>metric_definitions = RLEstimator.default_metric_definitions(RLToolkit.RAY)\nenvironment = env = {\n    'SAGEMAKER_REQUIREMENTS': 'requirements.txt', # path relative to `source_dir` below.\n}\n\nestimator = RLEstimator(entry_point=\"train.py\",\n                        source_dir='.',\n                        toolkit=RLToolkit.RAY,\n                        toolkit_version='0.6.5',\n                        framework=RLFramework.TENSORFLOW,\n                        dependencies=[\"sagemaker_rl\"],\n#                         image_name='price-response-ray-cpu',\n                        role=role,\n#                         train_instance_type=\"ml.c5.2xlarge\",\n                        train_instance_type='local',\n                        train_instance_count=1,\n#                         output_path=s3_output_path,\n#                         base_job_name=job_name_prefix,\n                        metric_definitions=metric_definitions\n#                         hyperparameters={\n                          # Attention scientists!  You can override any Ray algorithm parameter here:\n                          #\"rl.training.config.horizon\": 5000,\n                          #\"rl.training.config.num_sgd_iter\": 10,\n                        #}\n                    )\n\nestimator.fit(wait=True)\njob_name = estimator.latest_training_job.job_name\nprint(\"Training job: %s\" % job_name)\n<\/code><\/pre>\n\n<p>The error message I have been receiving has been the following:<\/p>\n\n<pre><code>algo-1-dxwxx_1  | == Status ==\nalgo-1-dxwxx_1  | Using FIFO scheduling algorithm.\nalgo-1-dxwxx_1  | Resources requested: 0\/3 CPUs, 0\/0 GPUs\nalgo-1-dxwxx_1  | Memory usage on this node: 1.1\/4.1 GB\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | == Status ==\nalgo-1-dxwxx_1  | Using FIFO scheduling algorithm.\nalgo-1-dxwxx_1  | Resources requested: 2\/3 CPUs, 0\/0 GPUs\nalgo-1-dxwxx_1  | Memory usage on this node: 1.4\/4.1 GB\nalgo-1-dxwxx_1  | Result logdir: \/opt\/ml\/output\/intermediate\/training\nalgo-1-dxwxx_1  | Number of trials: 1 ({'RUNNING': 1})\nalgo-1-dxwxx_1  | RUNNING trials:\nalgo-1-dxwxx_1  |  - PPO_ArrivalSim-v0_0:   RUNNING\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | (pid=72) 2019-08-30 09:35:13,030  WARNING ppo.py:172 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\nalgo-1-dxwxx_1  | 2019-08-30 09:35:13,063   ERROR trial_runner.py:460 -- Error processing event.\nalgo-1-dxwxx_1  | Traceback (most recent call last):\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/tune\/trial_runner.py\", line 409, in _process_trial\nalgo-1-dxwxx_1  |     result = self.trial_executor.fetch_result(trial)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/tune\/ray_trial_executor.py\", line 314, in fetch_result\nalgo-1-dxwxx_1  |     result = ray.get(trial_future[0])\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/worker.py\", line 2316, in get\nalgo-1-dxwxx_1  |     raise value\nalgo-1-dxwxx_1  | ray.exceptions.RayTaskError: ray_worker (pid=72, host=b9b15d495b68)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/model.py\", line 83, in __init__\nalgo-1-dxwxx_1  |     restored, num_outputs, options)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/model.py\", line 135, in _build_layers_v2\nalgo-1-dxwxx_1  |     raise NotImplementedError\nalgo-1-dxwxx_1  | NotImplementedError\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | During handling of the above exception, another exception occurred:\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | ray_worker (pid=72, host=b9b15d495b68)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 276, in __init__\nalgo-1-dxwxx_1  |     Trainable.__init__(self, config, logger_creator)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/tune\/trainable.py\", line 88, in __init__\nalgo-1-dxwxx_1  |     self._setup(copy.deepcopy(self.config))\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 373, in _setup\nalgo-1-dxwxx_1  |     self._init()\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/ppo\/ppo.py\", line 77, in _init\nalgo-1-dxwxx_1  |     self.env_creator, self._policy_graph)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 506, in make_local_evaluator\nalgo-1-dxwxx_1  |     extra_config or {}))\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 714, in _make_evaluator\nalgo-1-dxwxx_1  |     async_remote_worker_envs=config[\"async_remote_worker_envs\"])\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/evaluation\/policy_evaluator.py\", line 288, in __init__\nalgo-1-dxwxx_1  |     self._build_policy_map(policy_dict, policy_config)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/evaluation\/policy_evaluator.py\", line 661, in _build_policy_map\nalgo-1-dxwxx_1  |     policy_map[name] = cls(obs_space, act_space, merged_conf)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/ppo\/ppo_policy_graph.py\", line 176, in __init__\nalgo-1-dxwxx_1  |     seq_lens=existing_seq_lens)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/catalog.py\", line 215, in get_model\nalgo-1-dxwxx_1  |     seq_lens)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/catalog.py\", line 255, in _get_model\nalgo-1-dxwxx_1  |     num_outputs, options)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/model.py\", line 86, in __init__\nalgo-1-dxwxx_1  |     input_dict[\"obs\"], num_outputs, options)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/fcnet.py\", line 37, in _build_layers\nalgo-1-dxwxx_1  |     scope=label)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/contrib\/framework\/python\/ops\/arg_scope.py\", line 182, in func_with_args\nalgo-1-dxwxx_1  |     return func(*args, **current_args)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/contrib\/layers\/python\/layers\/layers.py\", line 1854, in fully_connected\nalgo-1-dxwxx_1  |     outputs = layer.apply(inputs)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/base_layer.py\", line 817, in apply\nalgo-1-dxwxx_1  |     return self.__call__(inputs, *args, **kwargs)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/layers\/base.py\", line 374, in __call__\nalgo-1-dxwxx_1  |     outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/base_layer.py\", line 730, in __call__\nalgo-1-dxwxx_1  |     self._assert_input_compatibility(inputs)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/base_layer.py\", line 1493, in _assert_input_compatibility\nalgo-1-dxwxx_1  |     str(x.shape.as_list()))\nalgo-1-dxwxx_1  | ValueError: Input 0 of layer default\/fc1 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [None]\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | 2019-08-30 09:35:13,064   INFO ray_trial_executor.py:178 -- Destroying actor for trial PPO_ArrivalSim-v0_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\nalgo-1-dxwxx_1  | 2019-08-30 09:35:13,076   INFO trial_runner.py:497 -- Attempting to recover trial state from last checkpoint.\nalgo-1-dxwxx_1  | (pid=72) 2019-08-30 09:35:13,041  INFO policy_evaluator.py:278 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n<\/code><\/pre>\n\n<p>I am not sure how to change the input the environment gives to the model or the models setup itself. It seems the documentations are quite obscure. I have a hunch that problem lies with the observation and action spaces<\/p>\n\n<p>Here is the reference to the original aws project example:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/reinforcement_learning\/rl_roboschool_ray\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/reinforcement_learning\/rl_roboschool_ray<\/a><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1567158570927,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1567160893503,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57724414",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":17.2,
        "Challenge_reading_time":159.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":119,
        "Challenge_solved_time":null,
        "Challenge_title":"How to make the inputs and model have the same shape (RLlib Ray Sagemaker reinforcement learning)",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1492.0,
        "Challenge_word_count":999,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1565097950652,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p><strong>Possible reason:<\/strong><\/p>\n\n<p>The error message:<\/p>\n\n<p><code>ValueError: Input 0 of layer default\/fc1 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [None]<\/code><\/p>\n\n<p>Your original environment obs space is <code>self.observation_space = Box(np.array(0.0),np.array(1000))<\/code>.<\/p>\n\n<p>Displaying the shape of your environment obs space gives:<\/p>\n\n<p><code>print(Box(np.array(0.0), np.array(1000), dtype=np.float32).shape)<\/code> = <code>()<\/code><\/p>\n\n<p>This could be indicated by <code>Full shape received: [None]<\/code> in the error message.<\/p>\n\n<p>If you pass the shape <code>(1,1)<\/code> into <code>np.zeros<\/code>, you get the expected  <code>min_ndim=2<\/code>:<\/p>\n\n<p><code>x = np.zeros((1, 1))\nprint(x)\n[[0.]]\nprint(x.ndim)\n2<\/code><\/p>\n\n<p><strong>Suggested solution:<\/strong><\/p>\n\n<p>I assume that you want your environment obs space to range from 0.0 to 1000.0 as indicated by the <code>self.price = np.random.rand()<\/code> in your <code>reset<\/code> function.<\/p>\n\n<p>Try using the following for your environment obs space:<\/p>\n\n<p><code>self.observation_space = Box(0.0, 1000.0, shape=(1,1), dtype=np.float32)<\/code><\/p>\n\n<p>I hope that by setting the <code>Box<\/code> with an explicit <code>shape<\/code> helps.<\/p>\n\n<p><strike>\n<strong>EDIT (20190903):<\/strong><\/p>\n\n<p>I have modified your training script. This modification includes new imports, custom model class, model registration &amp; addition of registered custom model to config. For readability, only sections added are shown below. The entire modified training script is available in this <a href=\"https:\/\/gist.github.com\/ChuaCheowHuan\/ddf70654bd928d70e5415c947d4d43f3\" rel=\"nofollow noreferrer\">gist<\/a>. Please run with the proposed obs space as describe above.<\/p>\n\n<p>New additional imports:<\/p>\n\n<pre><code># new imports\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom ray.rllib.models import ModelCatalog\nfrom ray.rllib.models.tf.tf_modelv2 import TFModelV2\nfrom ray.rllib.models.tf.fcnet_v2 import FullyConnectedNetwork\n\nfrom ray.rllib.utils import try_import_tf\nfrom ray.tune import grid_search\n\ntf = try_import_tf()\n# end new imports\n<\/code><\/pre>\n\n<p>Custom model class:<\/p>\n\n<pre><code># Custom model class (fcnet)\nclass CustomModel(TFModelV2):\n    \"\"\"Example of a custom model that just delegates to a fc-net.\"\"\"\n\n    def __init__(self, obs_space, action_space, num_outputs, model_config,\n                 name):\n        super(CustomModel, self).__init__(obs_space, action_space, num_outputs,\n                                          model_config, name)\n        self.model = FullyConnectedNetwork(obs_space, action_space,\n                                           num_outputs, model_config, name)\n        self.register_variables(self.model.variables())\n\n    def forward(self, input_dict, state, seq_lens):\n        return self.model.forward(input_dict, state, seq_lens)\n\n    def value_function(self):\n        return self.model.value_function()\n<\/code><\/pre>\n\n<p>Registered &amp; add custom model:<\/p>\n\n<pre><code>    def get_experiment_config(self):\n\n\n        # Register custom model\n        ModelCatalog.register_custom_model(\"my_model\", CustomModel)\n\n\n        return {\n          \"training\": {\n            \"env\": \"ArrivalSim-v0\",\n            \"run\": \"PPO\",\n            \"stop\": {\n              \"episode_reward_mean\": 5000,\n            },\n            \"config\": {\n\n\n              \"model\": {\"custom_model\": \"my_model\"}, # Add registered custom model\n\n\n              \"gamma\": 0.995,\n              \"kl_coeff\": 1.0,\n              \"num_sgd_iter\": 10,\n              \"lr\": 0.0001,\n              \"sgd_minibatch_size\": 32768,\n              \"train_batch_size\": 320000,\n              \"monitor\": False,  # Record videos.\n              \"model\": {\n                \"free_log_std\": False\n              },\n              \"use_gae\": False,\n              \"num_workers\": (self.num_cpus-1),\n              \"num_gpus\": self.num_gpus,\n              \"batch_mode\": \"complete_episodes\"\n            }\n          }\n        }\n<\/code><\/pre>\n\n<p><\/strike><\/p>\n\n<p><strong>EDIT 2 (20190910):<\/strong><\/p>\n\n<p>To show that it works, truncated output from Sagemaker (Jupyter notebook instance):<\/p>\n\n<pre><code>.\n.\n.\nalgo-1-y2ayw_1  | price b = 0.439261780930142\nalgo-1-y2ayw_1  | price a = 0.439261780930142\nalgo-1-y2ayw_1  | (self.price).shape = (1,)\nalgo-1-y2ayw_1  | [0.43926178] 10.103020961393266 False {}\nalgo-1-y2ayw_1  | price b = 0.439261780930142\nalgo-1-y2ayw_1  | price a = 0.439261780930142\nalgo-1-y2ayw_1  | (self.price).shape = (1,)\nalgo-1-y2ayw_1  | [0.43926178] 9.663759180463124 False {}\nalgo-1-y2ayw_1  | price b = 0.439261780930142\nalgo-1-y2ayw_1  | price a = 0.189261780930142\nalgo-1-y2ayw_1  | (self.price).shape = (1,)\nalgo-1-y2ayw_1  | [0.18926178] 5.67785342790426 False {}\nalgo-1-y2ayw_1  | price b = 0.189261780930142\nalgo-1-y2ayw_1  | price a = -0.06073821906985799\nalgo-1-y2ayw_1  | (self.price).shape = (1,)\nalgo-1-y2ayw_1  | [-0.06073822] 0 True {}\nalgo-1-y2ayw_1  | Result for PPO_ArrivalSim-v0_0:\nalgo-1-y2ayw_1  |   date: 2019-09-10_11-51-13\nalgo-1-y2ayw_1  |   done: true\nalgo-1-y2ayw_1  |   episode_len_mean: 126.72727272727273\nalgo-1-y2ayw_1  |   episode_reward_max: 15772.677709596366\nalgo-1-y2ayw_1  |   episode_reward_mean: 2964.4609668691965\nalgo-1-y2ayw_1  |   episode_reward_min: 0.0\nalgo-1-y2ayw_1  |   episodes: 5\nalgo-1-y2ayw_1  |   experiment_id: 5d3b9f2988854a0db164a2e5e9a7550f\nalgo-1-y2ayw_1  |   hostname: 2dae585dcc65\nalgo-1-y2ayw_1  |   info:\nalgo-1-y2ayw_1  |     cur_lr: 4.999999873689376e-05\nalgo-1-y2ayw_1  |     entropy: 1.0670874118804932\nalgo-1-y2ayw_1  |     grad_time_ms: 1195.066\nalgo-1-y2ayw_1  |     kl: 3.391784191131592\nalgo-1-y2ayw_1  |     load_time_ms: 44.725\nalgo-1-y2ayw_1  |     num_steps_sampled: 463\nalgo-1-y2ayw_1  |     num_steps_trained: 463\nalgo-1-y2ayw_1  |     policy_loss: -0.05383850634098053\nalgo-1-y2ayw_1  |     sample_time_ms: 621.282\nalgo-1-y2ayw_1  |     total_loss: 2194493.5\nalgo-1-y2ayw_1  |     update_time_ms: 145.352\nalgo-1-y2ayw_1  |     vf_explained_var: -5.519390106201172e-05\nalgo-1-y2ayw_1  |     vf_loss: 2194492.5\nalgo-1-y2ayw_1  |   iterations_since_restore: 2\nalgo-1-y2ayw_1  |   node_ip: 172.18.0.2\nalgo-1-y2ayw_1  |   pid: 77\nalgo-1-y2ayw_1  |   policy_reward_mean: {}\nalgo-1-y2ayw_1  |   time_since_restore: 4.55129861831665\nalgo-1-y2ayw_1  |   time_this_iter_s: 1.3484764099121094\nalgo-1-y2ayw_1  |   time_total_s: 4.55129861831665\nalgo-1-y2ayw_1  |   timestamp: 1568116273\nalgo-1-y2ayw_1  |   timesteps_since_restore: 463\nalgo-1-y2ayw_1  |   timesteps_this_iter: 234\nalgo-1-y2ayw_1  |   timesteps_total: 463\nalgo-1-y2ayw_1  |   training_iteration: 2\nalgo-1-y2ayw_1  |\nalgo-1-y2ayw_1  | A worker died or was killed while executing task 00000000781a7b5b94a203683f8f789e593abbb1.\nalgo-1-y2ayw_1  | A worker died or was killed while executing task 00000000d3507bc6b41ee1c9fc36292eeae69557.\nalgo-1-y2ayw_1  | == Status ==\nalgo-1-y2ayw_1  | Using FIFO scheduling algorithm.\nalgo-1-y2ayw_1  | Resources requested: 0\/3 CPUs, 0\/0 GPUs\nalgo-1-y2ayw_1  | Result logdir: \/opt\/ml\/output\/intermediate\/training\nalgo-1-y2ayw_1  | TERMINATED trials:\nalgo-1-y2ayw_1  |  - PPO_ArrivalSim-v0_0:   TERMINATED [pid=77], 4 s, 2 iter, 463 ts, 2.96e+03 rew\nalgo-1-y2ayw_1  |\nalgo-1-y2ayw_1  | Saved model configuration.\nalgo-1-y2ayw_1  | Saved the checkpoint file \/opt\/ml\/output\/intermediate\/training\/PPO_ArrivalSim-v0_0_2019-09-10_11-50-53vd32vlux\/checkpoint-2.extra_data as \/opt\/ml\/model\/checkpoint.extra_data\nalgo-1-y2ayw_1  | Saved the checkpoint file \/opt\/ml\/output\/intermediate\/training\/PPO_ArrivalSim-v0_0_2019-09-10_11-50-53vd32vlux\/checkpoint-2.tune_metadata as \/opt\/ml\/model\/checkpoint.tune_metadata\nalgo-1-y2ayw_1  | Created LogSyncer for \/root\/ray_results\/PPO_ArrivalSim-v0_2019-09-10_11-51-13xdn_5i34 -&gt; None\nalgo-1-y2ayw_1  | 2019-09-10 11:51:13.941718: I tensorflow\/core\/common_runtime\/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\nalgo-1-y2ayw_1  | reset -&gt; (self.price).shape =  (1,)\nalgo-1-y2ayw_1  | LocalMultiGPUOptimizer devices ['\/cpu:0']\nalgo-1-y2ayw_1  | reset -&gt; (self.price).shape =  (1,)\nalgo-1-y2ayw_1  | INFO:tensorflow:No assets to save.\nalgo-1-y2ayw_1  | No assets to save.\nalgo-1-y2ayw_1  | INFO:tensorflow:No assets to write.\nalgo-1-y2ayw_1  | No assets to write.\nalgo-1-y2ayw_1  | INFO:tensorflow:SavedModel written to: \/opt\/ml\/model\/1\/saved_model.pb\nalgo-1-y2ayw_1  | SavedModel written to: \/opt\/ml\/model\/1\/saved_model.pb\nalgo-1-y2ayw_1  | Saved TensorFlow serving model!\nalgo-1-y2ayw_1  | A worker died or was killed while executing task 00000000f352d985b807ca399460941fe2264899.\n\nalgo-1-y2ayw_1  | 2019-09-10 11:51:20,075 sagemaker-containers INFO\n\n Reporting training SUCCESS\n\ntmpwwb4b358_algo-1-y2ayw_1 exited with code 0\n\nAborting on container exit...\nFailed to delete: \/tmp\/tmpwwb4b358\/algo-1-y2ayw Please remove it manually.\n\n===== Job Complete =====\n<\/code><\/pre>\n\n<p>This time I make edits in all 3 files. Your environment, training script &amp; the Jupyter notebook but it turns out that there isn't a need to define custom models for your custom environment. However, that remains viable. And you're right, the main cause of the issue is still in the obs space.<\/p>\n\n<p>I set <code>self.price<\/code> to be a 1D numpy array to make it talk better with Ray RLlib. The creation of the custom environment in the training script was done in a simpler way as shown below. As for the notebook, I used version 0.5.3 instead of 0.6.5 for toolkit_version &amp; the training is done in local mode (in the docker container on the Sagemaker Jupyter notebook instance, still on AWS) with CPU only. However, it will also work with any ML instance (e.g ml.m4.xlarge) with GPU.<\/p>\n\n<p>The entire package along with all dependencies is in <a href=\"https:\/\/github.com\/ChuaCheowHuan\/sagemaker_Ray_RLlib_custom_env\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>The edited env:<\/p>\n\n<pre><code># new\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n# end new\n\n\nfrom enum import Enum\nimport math\n\nimport gym\nfrom gym import error, spaces, utils, wrappers\nfrom gym.utils import seeding\nfrom gym.envs.registration import register\nfrom gym.spaces import Discrete, Box\n\nimport numpy as np\n\n\ndef sigmoid_price_fun(x, maxcust, gamma):\n    return maxcust \/ (1 + math.exp(gamma * max(0, x)))\n\n\nclass Actions(Enum):\n    DECREASE_PRICE = 0\n    INCREASE_PRICE = 1\n    HOLD = 2\n\n\nPRICE_ADJUSTMENT = {\n    Actions.DECREASE_PRICE: -0.25,\n    Actions.INCREASE_PRICE: 0.25,\n    Actions.HOLD: 0\n}\n\n\nclass ArrivalSim(gym.Env):\n    \"\"\" Simple environment for price optimising RL learner. \"\"\"\n\n    def __init__(self, price):\n        \"\"\"\n        Parameters\n        ----------\n        price : float\n            The initial price to use.\n        \"\"\"\n        super().__init__()\n\n        self.price = price\n        self.revenue = 0\n        self.action_space = Discrete(3)  # [0, 1, 2]  #increase or decrease\n        # original obs space:\n        #self.observation_space = Box(0.0, 1000.0, shape=(1,1), dtype=np.float32)\n        # obs space initially suggested:\n        #self.observation_space = Box(0.0, 1000.0, shape=(1,1), dtype=np.float32)\n        # obs space suggested in this edit:\n        self.observation_space = spaces.Box(np.array([0.0]), np.array([1000.0]), dtype=np.float32)\n\n    def step(self, action):\n        \"\"\" Enacts the specified action in the environment.\n\n        Returns the new price, reward, whether we're finished and an empty dict for compatibility with Gym's\n        interface. \"\"\"\n\n        self._take_action(Actions(action))\n\n        next_state = self.price\n        print('(self.price).shape =', (self.price).shape)\n        #next_state = self.observation_space.sample()\n\n        reward = self._get_reward()\n        done = False\n\n        if next_state &lt; 0 or reward == 0:\n            done = True\n\n        print(next_state, reward, done, {})\n\n        return np.array(next_state), reward, done, {}\n\n    def reset(self):\n        \"\"\" Resets the environment, selecting a random initial price. Returns the price. \"\"\"\n        #self.observation_space.value = np.random.rand()\n        #return self.observation_space.sample()\n\n        self.price = np.random.rand(1)\n\n        print('reset -&gt; (self.price).shape = ', (self.price).shape)\n\n        return self.price\n\n    def _take_action(self, action):\n#         self.observation_space.value += PRICE_ADJUSTMENT[action]\n        #print('price b =', self.price)\n        print('price b =', self.price[0])\n        #print('price b =', self.price[[0]])\n        #self.price += PRICE_ADJUSTMENT[action]\n        self.price[0] += PRICE_ADJUSTMENT[action]\n        #self.price[[0]] += PRICE_ADJUSTMENT[action]\n        #print('price a =', self.price)\n        print('price a =', self.price[0])\n        #print('price a =', self.price[[0]])\n\n    #def _get_reward(self, price):\n    def _get_reward(self):\n#         price = self.observation_space.value\n#         return max(np.random.poisson(sigmoid_price_fun(price, 50, 0.5)) * price, 0)\n        #self.revenue = max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)\n        #return max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)\n        self.revenue = max(np.random.poisson(sigmoid_price_fun(self.price[0], 50, 0.5)) * self.price[0], 0)\n        return max(np.random.poisson(sigmoid_price_fun(self.price[0], 50, 0.5)) * self.price[0], 0)\n\n#     def render(self, mode='human'):\n#         super().render(mode)\n\ndef testEnv():\n    \"\"\"\n    register(\n        id='ArrivalSim-v0',\n        entry_point='env:ArrivalSim',\n        kwargs= {'price' : 40.0}\n    )\n    env = gym.make('ArrivalSim-v0')\n    \"\"\"\n    env = ArrivalSim(30.0)\n\n    val = env.reset()\n    print('val.shape = ', val.shape)\n\n    for _ in range(5):\n        print('env.observation_space =', env.observation_space)\n        act = env.action_space.sample()\n        print('\\nact =', act)\n        next_state, reward, done, _ = env.step(act)  # take a random action\n        print('next_state = ', next_state)\n    env.close()\n\n\n\nif __name__ =='__main__':\n\n    testEnv()\n<\/code><\/pre>\n\n<p>The edited training script:<\/p>\n\n<pre><code>import json\nimport os\n\nimport gym\nimport ray\nfrom ray.tune import run_experiments\nimport ray.rllib.agents.a3c as a3c\nimport ray.rllib.agents.ppo as ppo\nfrom ray.tune.registry import register_env\nfrom mod_op_env import ArrivalSim\n\nfrom sagemaker_rl.ray_launcher import SageMakerRayLauncher\n\n\"\"\"\ndef create_environment(env_config):\n    import gym\n#     from gym.spaces import Space\n    from gym.envs.registration import register\n\n    # This import must happen inside the method so that worker processes import this code\n    register(\n        id='ArrivalSim-v0',\n        entry_point='env:ArrivalSim',\n        kwargs= {'price' : 40}\n    )\n    return gym.make('ArrivalSim-v0')\n\"\"\"\ndef create_environment(env_config):\n    price = 30.0\n    # This import must happen inside the method so that worker processes import this code\n    from mod_op_env import ArrivalSim\n    return ArrivalSim(price)\n\n\nclass MyLauncher(SageMakerRayLauncher):\n    def __init__(self):        \n        super(MyLauncher, self).__init__()\n        self.num_gpus = int(os.environ.get(\"SM_NUM_GPUS\", 0))\n        self.hosts_info = json.loads(os.environ.get(\"SM_RESOURCE_CONFIG\"))[\"hosts\"]\n        self.num_total_gpus = self.num_gpus * len(self.hosts_info)\n\n    def register_env_creator(self):\n        register_env(\"ArrivalSim-v0\", create_environment)\n\n    def get_experiment_config(self):\n        return {\n          \"training\": {\n            \"env\": \"ArrivalSim-v0\",\n            \"run\": \"PPO\",\n            \"stop\": {\n              \"training_iteration\": 3,\n            },\n\n            \"local_dir\": \"\/opt\/ml\/model\/\",\n            \"checkpoint_freq\" : 3,\n\n            \"config\": {                                \n              #\"num_workers\": max(self.num_total_gpus-1, 1),\n              \"num_workers\": max(self.num_cpus-1, 1),\n              #\"use_gpu_for_workers\": False,\n              \"train_batch_size\": 128, #5,\n              \"sample_batch_size\": 32, #1,\n              \"gpu_fraction\": 0.3,\n              \"optimizer\": {\n                \"grads_per_step\": 10\n              },\n            },\n            #\"trial_resources\": {\"cpu\": 1, \"gpu\": 0, \"extra_gpu\": max(self.num_total_gpus-1, 1), \"extra_cpu\": 0},\n            #\"trial_resources\": {\"cpu\": 1, \"gpu\": 0, \"extra_gpu\": max(self.num_total_gpus-1, 0),\n            #                    \"extra_cpu\": max(self.num_cpus-1, 1)},\n            \"trial_resources\": {\"cpu\": 1,\n                                \"extra_cpu\": max(self.num_cpus-1, 1)},              \n          }\n        }\n\nif __name__ == \"__main__\":\n    os.environ[\"LC_ALL\"] = \"C.UTF-8\"\n    os.environ[\"LANG\"] = \"C.UTF-8\"\n    os.environ[\"RAY_USE_XRAY\"] = \"1\"\n    print(ppo.DEFAULT_CONFIG)\n    MyLauncher().train_main()\n\n<\/code><\/pre>\n\n<p>The notebook code:<\/p>\n\n<pre><code>!\/bin\/bash .\/setup.sh\n\nfrom time import gmtime, strftime\nimport sagemaker \nrole = sagemaker.get_execution_role()\n\nsage_session = sagemaker.session.Session()\ns3_bucket = sage_session.default_bucket()  \ns3_output_path = 's3:\/\/{}\/'.format(s3_bucket)\nprint(\"S3 bucket path: {}\".format(s3_output_path))\n\njob_name_prefix = 'ArrivalSim'\n\nfrom sagemaker.rl import RLEstimator, RLToolkit, RLFramework\n\nestimator = RLEstimator(entry_point=\"mod_op_train.py\", # Our launcher code\n                        source_dir='src', # Directory where the supporting files are at. All of this will be\n                                          # copied into the container.\n                        dependencies=[\"common\/sagemaker_rl\"], # some other utils files.\n                        toolkit=RLToolkit.RAY, # We want to run using the Ray toolkit against the ray container image.\n                        framework=RLFramework.TENSORFLOW, # The code is in tensorflow backend.\n                        toolkit_version='0.5.3', # Toolkit version. This will also choose an apporpriate tf version.                                               \n                        #toolkit_version='0.6.5', # Toolkit version. This will also choose an apporpriate tf version.                        \n                        role=role, # The IAM role that we created at the begining.\n                        #train_instance_type=\"ml.m4.xlarge\", # Since we want to run fast, lets run on GPUs.\n                        train_instance_type=\"local\", # Since we want to run fast, lets run on GPUs.\n                        train_instance_count=1, # Single instance will also work, but running distributed makes things \n                                                # fast, particularly in the case of multiple rollout training.\n                        output_path=s3_output_path, # The path where we can expect our trained model.\n                        base_job_name=job_name_prefix, # This is the name we setup above to be to track our job.\n                        hyperparameters = {      # Some hyperparameters for Ray toolkit to operate.\n                          \"s3_bucket\": s3_bucket,\n                          \"rl.training.stop.training_iteration\": 2, # Number of iterations.\n                          \"rl.training.checkpoint_freq\": 2,\n                        },\n                        #metric_definitions=metric_definitions, # This will bring all the logs out into the notebook.\n                    )\n\nestimator.fit()\n<\/code><\/pre>",
        "Solution_comment_count":19.0,
        "Solution_last_edit_time":1568837974432,
        "Solution_link_count":2.0,
        "Solution_readability":12.1,
        "Solution_reading_time":222.55,
        "Solution_score_count":1.0,
        "Solution_sentence_count":215.0,
        "Solution_word_count":1601.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1299276420380,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":649.0,
        "Answerer_view_count":134.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm new to Sagemaker and am running some tests to measure the performance of NTM and LDA on AWS compared with LDA mallet and native Gensim LDA model.<\/p>\n\n<p>I'm wanting to inspect the trained models on Sagemaker and look at stuff like what words have the highest contribution for each topic. And also to get a measure of model coherence.<\/p>\n\n<p>I have been able to successfully get what words have the highest contribution for each topic for NTM on Sagemaker by downloading the output file untarring it and unzipping to expose 3 files params, symbol.json and meta.json. <\/p>\n\n<p>However, when I try to do the same process for LDA, the untarred output file cannot be unzipped.<\/p>\n\n<p>Maybe I'm missing something or should do something different for LDA compared with NTM but I have not been able to find any documentation on this. Also, anyone found a simple way to calculate model coherence?<\/p>\n\n<p>Any assistance would be greatly appreciated!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1551353848073,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54924835",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":13.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker LDA topic model - how to access the params of the trained model? Also is there a simple way to capture coherence",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":521.0,
        "Challenge_word_count":179,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1359732456992,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, United Kingdom",
        "Poster_reputation_count":401.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/scientific_details_of_algorithms\/lda_topic_modeling\/LDA-Science.ipynb\" rel=\"nofollow noreferrer\">This SageMaker notebook<\/a>, which dives into the scientific details of LDA, also demonstrates how to inspect the model artifacts. Specifically, how to obtain the estimates for the Dirichlet prior <code>alpha<\/code> and the topic-word distribution matrix <code>beta<\/code>. You can find the instructions in the section titled <em>\"Inspecting the Trained Model\"<\/em>. For convenience, I will reproduce the relevant code here:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import tarfile\nimport mxnet as mx\n\n# extract the tarball\ntarflie_fname = FILENAME_PREFIX + 'model.tar.gz' # wherever the tarball is located\nwith tarfile.open(tarfile_fname) as tar:\n    tar.extractall()\n\n# obtain the model file (should be the only file starting with \"model_\")\nmodel_list = [\n    fname\n    for fname in os.listdir(FILENAME_PREFIX)\n    if fname.startswith('model_')\n]\nmodel_fname = model_list[0]\n\n# load the contents of the model file into MXNet arrays\nalpha, beta = mx.ndarray.load(model_fname)\n<\/code><\/pre>\n\n<p>That should get you the model data. Note that the topics, which are stored as rows of <code>beta<\/code>, are not presented in any particular order.<\/p>",
        "Solution_comment_count":11.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.2,
        "Solution_reading_time":16.95,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":146.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1637398374483,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":176.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>i esttablished a function of optuna to find out best model of gbm and xgboost for my data but i was wondering if i can take the best model and apply it directly into my notebook(extracting best model as an object to reuse it later)\nhere is my objective function:<\/p>\n<pre><code>import lightgbm as lgb \nimport optuna\nimport sklearn.metrics\nfrom xgboost import XGBRegressor\nfrom optuna.integration import XGBoostPruningCallback\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nbest_booster = None\ngbm = None\ndef objective(trial,random_state=22,n_jobs=1,early_stopping_rounds=50):\n    \n    regrosser_name = trial.suggest_categorical(&quot;regressor&quot;, [&quot;XGBoost&quot;, &quot;lightgbm&quot;])\n    train_x, valid_x, train_y, valid_y = train_test_split(X_train, y_train, test_size=0.25)\n    dtrain = lgb.Dataset(train_x, label=train_y)\n    # Step 2. Setup values for the hyperparameters:\n    if regrosser_name == 'XGBoost':\n        params = {\n        &quot;verbosity&quot;: 0,  # 0 (silent) - 3 (debug)\n        &quot;objective&quot;: &quot;reg:squarederror&quot;,\n        &quot;n_estimators&quot;: 10000,\n        &quot;max_depth&quot;: trial.suggest_int(&quot;max_depth&quot;, 4, 12),\n        &quot;learning_rate&quot;: trial.suggest_loguniform(&quot;learning_rate&quot;, 0.005, 0.05),\n        &quot;colsample_bytree&quot;: trial.suggest_loguniform(&quot;colsample_bytree&quot;, 0.2, 0.6),\n        &quot;subsample&quot;: trial.suggest_loguniform(&quot;subsample&quot;, 0.4, 0.8),\n        &quot;alpha&quot;: trial.suggest_loguniform(&quot;alpha&quot;, 0.01, 10.0),\n        &quot;lambda&quot;: trial.suggest_loguniform(&quot;lambda&quot;, 1e-8, 10.0),\n        &quot;gamma&quot;: trial.suggest_loguniform(&quot;lambda&quot;, 1e-8, 10.0),\n        &quot;min_child_weight&quot;: trial.suggest_loguniform(&quot;min_child_weight&quot;, 10, 1000),\n        &quot;seed&quot;: random_state,\n        &quot;n_jobs&quot;: n_jobs,\n        }\n        model = XGBRegressor(**params)\n        model.fit(train_x, train_y)\n        y_pred = model.predict(X_val)\n        accuracy_rf = sklearn.metrics.mean_absolute_error(valid_y, y_pred)\n        return accuracy_rf\n    \n        print(rf_max_depth)\n        print(rf_n_estimators)\n        \n    else:\n        param = {\n        &quot;objective&quot;: &quot;binary&quot;,\n        &quot;metric&quot;: &quot;binary_logloss&quot;,\n        &quot;verbosity&quot;: -1,\n        &quot;boosting_type&quot;: &quot;gbdt&quot;,\n        &quot;lambda_l1&quot;: trial.suggest_float(&quot;lambda_l1&quot;, 1e-8, 10.0, log=True),\n        &quot;lambda_l2&quot;: trial.suggest_float(&quot;lambda_l2&quot;, 1e-8, 10.0, log=True),\n        &quot;num_leaves&quot;: trial.suggest_int(&quot;num_leaves&quot;, 2, 256),\n        &quot;feature_fraction&quot;: trial.suggest_float(&quot;feature_fraction&quot;, 0.4, 1.0),\n        &quot;bagging_fraction&quot;: trial.suggest_float(&quot;bagging_fraction&quot;, 0.4, 1.0),\n        &quot;bagging_freq&quot;: trial.suggest_int(&quot;bagging_freq&quot;, 1, 7),\n        &quot;min_child_samples&quot;: trial.suggest_int(&quot;min_child_samples&quot;, 5, 100),\n        }\n        gbm = lgb.train(param, dtrain)\n        preds_gbm = gbm.predict(valid_x)\n        pred_labels_gbm = np.rint(preds_gbm)\n        accuracy_gbm = sklearn.metrics.mean_absolute_error(valid_y, pred_labels_gbm)\n        return accuracy_gbm\n<\/code><\/pre>\n<p>and here is how i tried to solve this issue:<\/p>\n<pre><code>def callback(study, trial):\n    global best_booster\n    if study.best_trial == trial:\n        best_booster = gbm\nif __name__ == &quot;__main__&quot;:\n    study = optuna.create_study(direction=&quot;maximize&quot;)\n    study.optimize(objective, n_trials=100, callbacks=[callback])\n<\/code><\/pre>\n<p>i think its about importing somthing, and if there is any tips on my optuna function please state it<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648541981733,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71658687",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.8,
        "Challenge_reading_time":50.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":null,
        "Challenge_title":"can i take best parameters and best model of optuna function and apply this model directly in my notebook?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":585.0,
        "Challenge_word_count":295,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1548633918320,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":79.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>If I understood your question correctly, then yes, that's what models are for.<\/p>\n<p>Like bring your saved model to your notebook, feed it data that has the same structure  as what you used to train it, and it should serve its purpose.  Or use it in a pipeline.<\/p>\n<p>Even 1 line of the same structure as an np array can be used.  For example, my model predicts whether a loan should be approved or not.<\/p>\n<p>For example, a bank customer wants a loan and submits his information.  The bank officer inputs this info in the system.  The system transforms this information into a single np array with the same structure as the dataset used to train the model.<\/p>\n<p>The model is then used by the system to predict whether the loan should be approved or not.<\/p>\n<p>I save my optuna xgb models as json, e.g.<\/p>\n<p>my_model.get_booster().save_model(f'{savepath}my_model.json')<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":10.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":146.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1512770138847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>If I am not using the notebook on AWS but instead just the Sagemaker CLI and want to train a model, can I specify a local path to read from and write to?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1530312595250,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51110274",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.4,
        "Challenge_reading_time":2.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Can I use AWS Sagemaker without S3",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":826.0,
        "Challenge_word_count":39,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1528500562963,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>If you use local mode with the SageMaker Python SDK, you can train using local data:<\/p>\n\n<pre><code>from sagemaker.mxnet import MXNet\n\nmxnet_estimator = MXNet('train.py',\n                        train_instance_type='local',\n                        train_instance_count=1)\n\nmxnet_estimator.fit('file:\/\/\/tmp\/my_training_data')\n<\/code><\/pre>\n\n<p>However, this only works if you are training a model locally, not on SageMaker. If you want to train on SageMaker, then yes, you do need to use S3.<\/p>\n\n<p>For more about local mode: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#local-mode\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#local-mode<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.3,
        "Solution_reading_time":8.2,
        "Solution_score_count":4.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":63.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have built an XGBoost model using Amazon Sagemaker, but I was unable to find anything which will help me interpret the model and validate if it has learned the right dependencies.<\/p>\n\n<p>Generally, we can see Feature Importance for XGBoost by get_fscore() function in the python API (<a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html\" rel=\"nofollow noreferrer\">https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html<\/a>) I see nothing of that sort in the sagemaker api(<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html<\/a>).<\/p>\n\n<p>I know I can build my own model and then deploy that using sagemaker but I am curious if anyone has faced this problem and how they overcame it.<\/p>\n\n<p>Thanks.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1554934002317,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55621967",
        "Challenge_link_count":4,
        "Challenge_participation_count":3,
        "Challenge_readability":14.7,
        "Challenge_reading_time":11.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Feature Importance for XGBoost in Sagemaker",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3637.0,
        "Challenge_word_count":99,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1419327925267,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Mountain View, CA, USA",
        "Poster_reputation_count":123.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>SageMaker XGBoost currently does not provide interface to retrieve feature importance from the model. You can write some code to get the feature importance from the XGBoost model. You have to get the booster object artifacts from the model in S3 and then use the following snippet <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pickle as pkl\nimport xgboost\nbooster = pkl.load(open(model_file, 'rb'))\nbooster.get_score()\nbooster.get_fscore()\n<\/code><\/pre>\n\n<p>Refer <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html\" rel=\"nofollow noreferrer\">XGBoost doc<\/a> for methods to get feature importance from the Booster object such as <code>get_score()<\/code> or <code>get_fscore()<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.4,
        "Solution_reading_time":9.42,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":83.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a Tensorflow model which is working perfectly fine on my laptop (Tf 1.8 on OS HighSierra). However, I wanted to scale my operations up and use Amazon's Virtual Machine to run predictions faster. What is the best way to use my saved model and classify images in jpeg format which are stored locally? Thank you! <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1532515840557,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51517103",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.4,
        "Challenge_reading_time":4.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Deploy my own tensorflow model on a virtual machine with AWS",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":196.0,
        "Challenge_word_count":68,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1530739110150,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":391.0,
        "Poster_view_count":57.0,
        "Solution_body":"<p>you have two options:<\/p>\n\n<p>1) Start a virtual machine on AWS (known as an Amazon EC2 instance). You can pick from many different instance types, including GPU instances. You'll have full administrative access on this machine, meaning that you can copy you TF model to it and predict just like you would on your own machine. <\/p>\n\n<p>More details on getting started with EC2 here: <a href=\"https:\/\/aws.amazon.com\/ec2\/getting-started\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/ec2\/getting-started\/<\/a> <\/p>\n\n<p>I would also recommend using the Deep Learning Amazon Machine Image, which bundles all the popular ML\/DL tools as well as the NVIDIA environment for GPU training\/prediction : <a href=\"https:\/\/aws.amazon.com\/machine-learning\/amis\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/machine-learning\/amis\/<\/a><\/p>\n\n<p>2) If you don't want to manage virtual machines, I'd recommend looking at Amazon SageMaker. You'll be able to import your TF model and to deploy it on fully-managed infrastructure for prediction. <\/p>\n\n<p>Here's a sample notebook showing you how to bring your own TF model to SageMaker: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/tensorflow_iris_byom\/tensorflow_BYOM_iris.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/tensorflow_iris_byom\/tensorflow_BYOM_iris.ipynb<\/a><\/p>\n\n<p>Hope this helps.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":14.3,
        "Solution_reading_time":19.14,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":156.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1390559330112,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, Canada",
        "Answerer_reputation_count":96.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am building a multiclass classifier on aws Sagemaker, and would love to use the predefined linearlearner algorithm for classification. <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1527161288830,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50508217",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":13.7,
        "Challenge_reading_time":2.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Can sagemaker's linear learner be used for multiclass classification?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":410.0,
        "Challenge_word_count":29,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1527161034076,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>Yes, it is possible now.<\/p>\n\n<p>You can set the predictor_type hyper-parameter to <code>multiclass_classifier<\/code>.<\/p>\n\n<p>See the documentation here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ll_hyperparameters.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ll_hyperparameters.html<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":32.5,
        "Solution_reading_time":4.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":21.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi, does SageMaker PyTorch Hosting 1.6 works only with artifacts named model.pth ?\nI'm trying [this sample with 1.6][1] and the deployment fails with error\n\n    FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/model\/model.pth'\n\nthe documentation doesn't mention such a constraint\n\n  [1]: https:\/\/github.com\/aws-samples\/amazon-sagemaker-bert-classify-pytorch",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610118101000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668602387619,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUn-IyC9nySDKKibLL3Lvw3A\/sagemaker-pytorch-hosting-1-6-works-only-with-artifacts-named-model-pth",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":5.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker PyTorch Hosting 1.6 works only with artifacts named model.pth ?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":336.0,
        "Challenge_word_count":52,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Yes - from the thread on this open issue: https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/issues\/86\n\nIn the issue thread they note that the new Pytorch 1.6 image requires that the model filename is `model.pth`, linking to the relevant code where this default is set: https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/9a6869e\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L121\n\nAlso noted in the thread is that users have successfully adapted their code to use torchserve in Pytorch 1.6 by changing it to save their model in a file named `model.pth`. Once renamed, they were still able to use custom inference scripts to load their model by defining a custom `model_fn`: https:\/\/github.com\/data-science-on-aws\/workshop\/blob\/374329adf15bf1810bfc4a9e73501ee5d3b4e0f5\/09_deploy\/wip\/pytorch\/code\/inference.py",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1667925547320,
        "Solution_link_count":3.0,
        "Solution_readability":11.9,
        "Solution_reading_time":10.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":91.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"The following sample notebook fails \r\n### img-classification-part1-training.ipynb\r\n\r\nwhen running:\r\n\r\n### from azureml.core import Dataset\r\n\r\nfrom azureml.core import Dataset\r\nfrom azureml.opendatasets import MNIST\r\n\r\ndata_folder = os.path.join(os.getcwd(), 'data')\r\nos.makedirs(data_folder, exist_ok=True)\r\n\r\nmnist_file_dataset = MNIST.get_file_dataset()\r\nmnist_file_dataset.download(data_folder, overwrite=True)\r\n\r\nmnist_file_dataset = mnist_file_dataset.register(workspace=ws,\r\n                                                 name='mnist_opendataset',\r\n                                                 description='training and test dataset',\r\n                                                 create_new_version=True)\r\n\r\n\r\n**Here is the error**\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-5-ac2e91b46eec> in <module>\r\n----> 1 from azureml.core import Dataset\r\n      2 from azureml.opendatasets import MNIST\r\n      3 \r\n      4 data_folder = os.path.join(os.getcwd(), 'data')\r\n      5 os.makedirs(data_folder, exist_ok=True)\r\n\r\nImportError: cannot import name 'Dataset'\r\n\r\nreference: yml file:\r\nname: img-classification-part1-training\r\ndependencies:\r\n- pip:\r\n  - azureml-sdk\r\n  - azureml-widgets\r\n  - matplotlib\r\n  - sklearn\r\n  - pandas\r\n  - azureml-opendatasets\r\n\r\nAzure ML SDK Version:  1.0.17\r\nPython 3.6 - AzureML\r\n\r\n@microsoft\r\nPlease kindly investigate.\r\nMany thanks :)",
        "Challenge_closed_time":1581440.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1581438052000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/787",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.7,
        "Challenge_reading_time":17.47,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"Import Error - from azureml.core import Dataset  - ImportError: cannot import name 'Dataset'",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":110,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@andrewkinsella, version `1.0.17` is from [almost a year ago](https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/azure-machine-learning-release-notes#2019-02-25). During that time, the `Datasets` class has evolved significantly (for the better). Can you try upgrading the SDK to the newest version and trying again? @MayMSFT  Thank you very much @swanderz \r\nI will try your recommendation.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.0,
        "Solution_reading_time":5.1,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1522076554448,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"S\u00e3o Paulo, State of S\u00e3o Paulo, Brazil",
        "Answerer_reputation_count":96.0,
        "Answerer_view_count":31.0,
        "Challenge_adjusted_solved_time":4030.6928727778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to retrieve the pickle off my trained model, which I know is in the run file inside my experiments in Databricks.<\/p>\n<p>It seems that the <code>mlflow.pyfunc.load_model<\/code> can only do the <code>predict<\/code> method.<\/p>\n<p>There is an option to directly access the pickle?<\/p>\n<p>I also tried to use the path in the run using the <code>pickle.load(path)<\/code> (example of path: dbfs:\/databricks\/mlflow-tracking\/20526156406\/92f3ec23bf614c9d934dd0195\/artifacts\/model\/model.pkl).<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628544411170,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68718719",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.9,
        "Challenge_reading_time":7.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I retrive the model.pkl in the experiment in Databricks",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3081.0,
        "Challenge_word_count":70,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1522076554448,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"S\u00e3o Paulo, State of S\u00e3o Paulo, Brazil",
        "Poster_reputation_count":96.0,
        "Poster_view_count":31.0,
        "Solution_body":"<p>I recently found the solution which can be done by the following two approaches:<\/p>\n<ol>\n<li>Use the customized predict function at the moment of saving the model (check <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">databricks<\/a> documentation for more details).<\/li>\n<\/ol>\n<p>example give by Databricks<\/p>\n<pre><code>class AddN(mlflow.pyfunc.PythonModel):\n\n    def __init__(self, n):\n        self.n = n\n\n    def predict(self, context, model_input):\n        return model_input.apply(lambda column: column + self.n)\n# Construct and save the model\nmodel_path = &quot;add_n_model&quot;\nadd5_model = AddN(n=5)\nmlflow.pyfunc.save_model(path=model_path, python_model=add5_model)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(model_path)\n<\/code><\/pre>\n<ol start=\"2\">\n<li>Load the model artefacts as we are downloading the artefact:<\/li>\n<\/ol>\n<pre><code>from mlflow.tracking import MlflowClient\n\nclient = MlflowClient()\n\ntmp_path = client.download_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path='model\/model.pkl')\n\nf = open(tmp_path,'rb')\n\nmodel = pickle.load(f)\n\nf.close()\n\n \n\nclient.list_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path=&quot;&quot;)\n\nclient.list_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path=&quot;model&quot;)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1643054905512,
        "Solution_link_count":1.0,
        "Solution_readability":16.0,
        "Solution_reading_time":18.3,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":109.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1627043269590,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Richmond, VA, USA",
        "Answerer_reputation_count":67.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":6.3228980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Suppose I have two Azure ML workspaces:<\/p>\n<ol>\n<li><p>Workspace1 - This is being used by one team (Team1) who only train the model and store the model in model registry of Workspace1<\/p>\n<\/li>\n<li><p>Workspace2 - This is used by another team  (Team2) who containerise the model, push it to ACR and then deploy the containerised model in Azure ML Compute.<\/p>\n<\/li>\n<\/ol>\n<p>Is it possible for Team2 to access the model registry of Workspace1 from their Workspace2 and retrieve the model for containerisation and subsequent deployment? Alternatively, is there any concept of a shared model registry in Azure ML where both the teams can store and access a common model registry? If none of these are possible, then what is the way for Team1 and Team2 to work together on a single model with the given responsibilities as described above?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638197253147,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70156610",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":11.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Accessing an Azure ML Model Registry from another Azure ML Workspace",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":421.0,
        "Challenge_word_count":148,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1373471094267,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United Kingdom",
        "Poster_reputation_count":395.0,
        "Poster_view_count":66.0,
        "Solution_body":"<p>As described, I think the best solution is to use one Workspace, not two.  It sounds like you have Team 1 and Team 2 sharing contributions on a single project.  What may work better is to define user roles in the Azure ML workspace, such that Team 2 has permissions to deploy models, and Team 1 has permission to create models.<\/p>\n<p>Otherwise you can always write Python code using the ML SDK to connect to any workspace given you know the subscription, resource group, workspace name etc.<\/p>\n<pre><code>from azure.core import Workspace, Model\n\n# connect to an existing workspace\nname = 'WorkspaceName'\nsub = 'subscriptionName'\nresource_group = 'resourceGroupName'\nws = Workspace.get(name=name, subscription_id=sub, resource_group=resource_group) \n\n# retrieve existing model\nmodel = Model(ws, name='your model name')\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1638220015580,
        "Solution_link_count":0.0,
        "Solution_readability":9.8,
        "Solution_reading_time":10.42,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":116.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1429270603900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"S\u00e3o Jos\u00e9 dos Campos, Sao Jose dos Campos - State of S\u00e3o Paulo, Brazil",
        "Answerer_reputation_count":138.0,
        "Answerer_view_count":38.0,
        "Challenge_adjusted_solved_time":714.527975,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have following this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/semantic_segmentation_pascalvoc\/semantic_segmentation_pascalvoc.ipynb\" rel=\"nofollow noreferrer\">tutorial<\/a>, which is mainly for jupyter notebook, and made some minimal modification for external processing. I've created a project that could prepare my dataset locally, upload it to S3, train, and finally deploy the model predictor to the same bucket. Perfect!<\/p>\n<p>So, after to train and saved it in S3 bucket:<\/p>\n<pre><code> ss_model.fit(inputs=data_channels, logs=True)\n<\/code><\/pre>\n<p>it failed while deploying as an endpoint. So, I have found tricks to host an endpoint in many ways, but not from a model already saved in S3. Because in order to host, you probably need to get the estimator, which in normal way is something like:<\/p>\n<pre><code> self.estimator = sagemaker.estimator.Estimator(self.training_image,\n                                                role,\n                                                train_instance_count=1,\n                                                train_instance_type='ml.p3.2xlarge',\n                                                train_volume_size=50,\n                                                train_max_run=360000,\n                                                output_path=output,\n                                                base_job_name='ss-training',\n                                                sagemaker_session=sess)\n<\/code><\/pre>\n<p>My question is: is there a way to load an estimator from a model saved in S3 (.tar)? Or, anyway, to create an endpoint without train it again?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1593283354330,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62614143",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":12.5,
        "Challenge_reading_time":17.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS SageMaker: Create an endpoint using a trained model hosted in S3",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1360.0,
        "Challenge_word_count":160,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1429270603900,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"S\u00e3o Jos\u00e9 dos Campos, Sao Jose dos Campos - State of S\u00e3o Paulo, Brazil",
        "Poster_reputation_count":138.0,
        "Poster_view_count":38.0,
        "Solution_body":"<p>So, after to run on many pages, just found a clue <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/blazingtext_hosting_pretrained_fasttext\/blazingtext_hosting_pretrained_fasttext.ipynb\" rel=\"nofollow noreferrer\">here<\/a>. And I finally found out how to load the model and create the endpoint:<\/p>\n<pre><code>def create_endpoint(self):\n    sess = sagemaker.Session()\n    training_image = get_image_uri(sess.boto_region_name, 'semantic-segmentation', repo_version=&quot;latest&quot;)        \n    role = &quot;YOUR_ROLE_ARN_WITH_SAGEMAKER_EXECUTION&quot;\n    model = &quot;s3:\/\/BUCKET\/PREFIX\/...\/output\/model.tar.gz&quot;\n\n    sm_model = sagemaker.Model(model_data=model, image=training_image, role=role, sagemaker_session=sess)\n    sm_model.deploy(initial_instance_count=1, instance_type='ml.p3.2xlarge')\n<\/code><\/pre>\n<p><strong>Please, do not forget to disable your endpoint after using. This is really important! Endpoints are charged by &quot;running&quot; not only by the use<\/strong><\/p>\n<p>I hope it also can help you out!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1595855655040,
        "Solution_link_count":1.0,
        "Solution_readability":19.4,
        "Solution_reading_time":14.32,
        "Solution_score_count":3.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":81.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use mlflow package in databricks to save the model into Azure Storage.<\/p>\n<p>The Script:<\/p>\n<p><code>abfss_path='abfss:\/\/mlops@dlsgdpeasdev03.dfs.core.windows.net'<\/code><\/p>\n<p><code>project = 'test'<\/code><\/p>\n<p><code>model_version = 'v1.0.1'<\/code><\/p>\n<p><code>model = {model training step}<\/code>  <br \/>\n<code>prefix_model_path = os.path.join(abfss_path, project, model_version)<\/code><\/p>\n<p><code>model_path = prefix_model_path<\/code><\/p>\n<p><code>print(model_path) # <\/code>abfss:\/\/mlops@dlsgdpeasdev03.dfs.core.windows.net\/test\/v1.0.1<\/p>\n<p><code>mlflow.sklearn.save_model(model, model_path)<\/code><\/p>\n<p>The message is successfully save the model. <\/p>\n<p>When I check the container and file does not exist, but I am able to load model with the same path. That mean the model file saved in databricks somewhere.<\/p>\n<p>I want to know where is the model file in databricks, and how to save the model directly from databricks notebook to Azure Storage.<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1673582881903,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1160457\/how-databrick-save-ml-model-into-azure-storage-con",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.1,
        "Challenge_reading_time":13.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"How databrick save ml model into Azure Storage Container?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":109,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=ff486681-dc42-4832-92a7-87a687f2ec26\">@Benny Lau ,Shui Hong - Group Office  <\/a>, <\/p>\n<p>Thanks for the ask and welcome to Microsoft Q&amp;A . <\/p>\n<p>As I understand the ask here is to where the model is saved and how you can save to the blob . <\/p>\n<p>As per the document here : [https:\/\/learn.microsoft.com\/en-us\/azure\/databricks\/mlflow\/models#api-commands<\/p>\n<p>You have three option and I assume that your  model file is getting stored in the DBFS on the Azure databricks cluster .<\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/4d409166-ee35-4252-9294-6e7a4140ffab?platform=QnA\" alt=\"User's image\" \/><\/p>\n<p>Databricks can save a machine learning model to an Azure Storage Container using the <strong><code>dbutils.fs<\/code><\/strong> module. This module provides a set of functions for interacting with the Databricks file system (DBFS) and Azure Blob Storage. Here is an example of how to save a model to an Azure Storage Container:<\/p>\n<ol>\n<li> First, you will need to mount the Azure Storage Container to DBFS, this can be done using the <strong><code>dbutils.fs.mount<\/code><\/strong> function.<\/li>\n<\/ol>\n<pre><code>dbutils.fs.mount(\n  source='wasbs:\/\/&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.blob.core.windows.net',\n  mount_point='\/mnt\/&lt;your-mount-point&gt;',\n  extra_configs={\n    &quot;fs.azure.account.auth.type&quot;: &quot;OAuth&quot;,\n    &quot;fs.azure.account.oauth.provider.type&quot;: &quot;org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider&quot;,\n    &quot;fs.azure.account.oauth2.client.id&quot;: &quot;&lt;your-client-id&gt;&quot;,\n    &quot;fs.azure.account.oauth2.client.secret&quot;: &quot;&lt;your-client-secret&gt;&quot;,\n    &quot;fs.azure.account.oauth2.client.endpoint&quot;: &quot;https:\/\/login.microsoftonline.com\/&lt;your-tenant-id&gt;\/oauth2\/token&quot;\n  }\n)\n\n<\/code><\/pre>\n<ol>\n<li> Once the container is mounted, you can use the <strong><code>dbutils.fs.cp<\/code><\/strong> function to copy the model from the local file system to the mount point.<\/li>\n<\/ol>\n<p>dbutils.fs.cp(&quot;path\/to\/local\/model&quot;, &quot;\/mnt\/&lt;your-mount-point&gt;\/model&quot;)<\/p>\n<ol>\n<li> You can also use <strong><code>model.save()<\/code><\/strong> method to save the model in the mounted container path<\/li>\n<\/ol>\n<p>model.save(&quot;\/mnt\/&lt;your-mount-point&gt;\/model&quot;)<\/p>\n<p>Note: Be sure to replace the placeholders in the above code with the appropriate values for your use case.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.7,
        "Solution_reading_time":33.04,
        "Solution_score_count":0.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":231.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1393524211332,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":745.0,
        "Answerer_view_count":71.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Have exhausted myself on this one so any help would be appreciated.<\/p>\n\n<p>I am trying to set up hosting my tensorflow model with Amazon Sagemaker and following the example found <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators\/iris_dnn_classifier.py\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>This example uses hard coded feature columns with known dimensionality. <\/p>\n\n<pre><code>feature_columns = [tf.feature_column.numeric_column(INPUT_TENSOR_NAME, shape=[4])]\n<\/code><\/pre>\n\n<p>I need to avoid this as my dataset changes often.<\/p>\n\n<h1>Local Machine Set Up<\/h1>\n\n<p>Now on my local machine, I define a list of columns <\/p>\n\n<pre><code>my_feature_columns = []\n<\/code><\/pre>\n\n<p>With the following strategy<\/p>\n\n<pre><code>#Define placeholder nodes based on datatype being inserted\n\nfor key in train_x.keys():\n<\/code><\/pre>\n\n<p>Where train_x is the dataset without labels.<\/p>\n\n<p>'OBJECTS' become hashed buckets as there are many possible categories<\/p>\n\n<pre><code>    if train_x[key].dtypes == 'object':\n\n        categorical_column = tf.feature_column.categorical_column_with_hash_bucket(\n                key = key,\n                hash_bucket_size = len(train_x[key].unique()))\n\n        my_feature_columns.append(tf.feature_column.embedding_column(\n                categorical_column=categorical_column,\n                dimension=5))\n<\/code><\/pre>\n\n<p>'INT64' become categorical columns as there are only two possible categories (I have recoded booleans to 0\/1)<\/p>\n\n<pre><code>    elif train_x[key].dtypes == 'int64':\n\n        categorical_column = tf.feature_column.categorical_column_with_identity(\n                key=key,\n                num_buckets=2)\n\n        my_feature_columns.append(tf.feature_column.indicator_column(categorical_column))\n<\/code><\/pre>\n\n<p>'FLOATS' become continuous columns<\/p>\n\n<pre><code>    elif train_x[key].dtypes == 'float':\n        my_feature_columns.append(\n        tf.feature_column.numeric_column(\n        key=key))\n<\/code><\/pre>\n\n<p>On the local machine this yields a nice list of all of my features that can be given as an argument when instantiating a tf.estimator.DNNClassifier. As more categories are added to each OBJECT column, this is handled by<\/p>\n\n<pre><code>hash_bucket_size = len(train_x[key].unique())\n<\/code><\/pre>\n\n<h1>Sagemaker<\/h1>\n\n<p>From the <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst#preparing-the-tensorflow-training-script\" rel=\"nofollow noreferrer\">Docs<\/a><\/p>\n\n<p><em>Preparing the TensorFlow training script\nYour TensorFlow training script must be a Python 2.7 source file. The SageMaker TensorFlow docker image uses this script by calling specifically-named functions from this script.<\/em><\/p>\n\n<p><em>The training script must contain the following:<\/em><\/p>\n\n<p><em>Exactly one of the following:\nmodel_fn: defines the model that will be trained.\nkeras_model_fn: defines the tf.keras model that will be trained.\nestimator_fn: defines the tf.estimator.Estimator that will train the model.<\/em><\/p>\n\n<p><em>train_input_fn: preprocess and load training data.<\/em><\/p>\n\n<p><em>eval_input_fn: preprocess and load evaluation data.<\/em><\/p>\n\n<p>Again, from the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators\/iris_dnn_classifier.py\" rel=\"nofollow noreferrer\">example<\/a><\/p>\n\n<pre><code>def train_input_fn(training_dir, params):\n\"\"\"Returns input function that would feed the model during training\"\"\"\nreturn _generate_input_fn(training_dir, 'iris_training.csv')\n<\/code><\/pre>\n\n<p>This function is called by the sagemaker docker image, which adds its own argument for <strong>training_dir<\/strong>, it is not a global parameter.<\/p>\n\n<p>When trying to access my training data from the estimator_fn to build a my_feature_columns list<\/p>\n\n<pre><code>NameError: global name 'training_dir' is not defined\n<\/code><\/pre>\n\n<h1>I would love to be able to do something like this.<\/h1>\n\n<pre><code>def estimator_fn(run_config, params):\n\nmy_feature_columns = []\n\ntrain_x , _ , _ , _ = datasplitter(os.path.join(training_dir, 'leads_test_frame.csv'))\n\nfor key in train_x.keys():\n    if train_x[key].dtypes == 'object':\n\n        categorical_column = tf.feature_column.categorical_column_with_hash_bucket(\n                key = key,\n                hash_bucket_size = len(train_x[key].unique()))\n\n        my_feature_columns.append(tf.feature_column.embedding_column(\n                categorical_column=categorical_column,\n                dimension=5))\n\n    elif train_x[key].dtypes == 'int64':\n\n        categorical_column = tf.feature_column.categorical_column_with_identity(\n                key=key,\n                num_buckets=2)\n\n        my_feature_columns.append(tf.feature_column.indicator_column(categorical_column))\n\n    elif train_x[key].dtypes == 'float':\n        my_feature_columns.append(\n        tf.feature_column.numeric_column(\n        key=key))\n\nreturn tf.estimator.DNNClassifier(feature_columns=my_feature_columns,\n                                  hidden_units=[10, 20, 10],\n                                  n_classes=2,\n                                  config=run_config)\n<\/code><\/pre>\n\n<p>Thanks to anyone who can help in any way. Will happily give more info if needed but feel like 4 pages is probably enough :-S<\/p>\n\n<p>Cheers!\nClem<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1535642615980,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52100549",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":16.5,
        "Challenge_reading_time":67.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":47,
        "Challenge_solved_time":null,
        "Challenge_title":"Procedural Generation of Feature Columns for use with Sagemaker Tensorflow Instance",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":165.0,
        "Challenge_word_count":458,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1491420193248,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Windsor, UK",
        "Poster_reputation_count":163.0,
        "Poster_view_count":71.0,
        "Solution_body":"<p><strong>training_dir<\/strong> points to your training channel, i.e. <em>\/opt\/ml\/input\/data\/training<\/em>. You can hardcode this location inside your <strong>estimation_fn<\/strong>.<\/p>\n\n<p>When training starts, SageMaker makes the data for the channel available in the <em>\/opt\/ml\/input\/data\/<strong>channel_name<\/em><\/strong> directory in the Docker container.<\/p>\n\n<p>You can find more information here <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo.html#your-algorithms-training-algo-running-container\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo.html#your-algorithms-training-algo-running-container<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":26.9,
        "Solution_reading_time":9.81,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":45.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1604093818187,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Krakow, Poland",
        "Answerer_reputation_count":1200.0,
        "Answerer_view_count":263.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm struggling to correctly set Vertex AI pipeline which does the following:<\/p>\n<ol>\n<li>read data from API and store to GCS and as as input for batch prediction.<\/li>\n<li>get an existing model (Video classification on Vertex AI)<\/li>\n<li>create Batch prediction job with input from point 1.<br \/>\nAs it will be seen, I don't have much experience with Vertex Pipelines\/Kubeflow thus I'm asking for help\/advice, hope it's just some beginner mistake.\nthis is the gist of the code I'm using as pipeline<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>from google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom kfp.v2 import dsl\n\nfrom kfp.v2.dsl import component\nfrom kfp.v2.dsl import (\n    Output,\n    Artifact,\n    Model,\n)\n\nPROJECT_ID = 'my-gcp-project'\nBUCKET_NAME = &quot;mybucket&quot;\nPIPELINE_ROOT = &quot;{}\/pipeline_root&quot;.format(BUCKET_NAME)\n\n\n@component\ndef get_input_data() -&gt; str:\n    # getting data from API, save to Cloud Storage\n    # return GS URI\n    gcs_batch_input_path = 'gs:\/\/somebucket\/file'\n    return gcs_batch_input_path\n\n\n@component(\n    base_image=&quot;python:3.9&quot;,\n    packages_to_install=['google-cloud-aiplatform==1.8.0']\n)\ndef load_ml_model(project_id: str, model: Output[Artifact]):\n    &quot;&quot;&quot;Load existing Vertex model&quot;&quot;&quot;\n    import google.cloud.aiplatform as aip\n\n    model_id = '1234'\n    model = aip.Model(model_name=model_id, project=project_id, location='us-central1')\n\n\n\n@dsl.pipeline(\n    name=&quot;batch-pipeline&quot;, pipeline_root=PIPELINE_ROOT,\n)\ndef pipeline(gcp_project: str):\n    input_data = get_input_data()\n    ml_model = load_ml_model(gcp_project)\n\n    gcc_aip.ModelBatchPredictOp(\n        project=PROJECT_ID,\n        job_display_name=f'test-prediction',\n        model=ml_model.output,\n        gcs_source_uris=[input_data.output],  # this doesn't work\n        # gcs_source_uris=['gs:\/\/mybucket\/output\/'],  # hardcoded gs uri works\n        gcs_destination_output_uri_prefix=f'gs:\/\/{PIPELINE_ROOT}\/prediction_output\/'\n    )\n\n\nif __name__ == '__main__':\n    from kfp.v2 import compiler\n    import google.cloud.aiplatform as aip\n    pipeline_export_filepath = 'test-pipeline.json'\n    compiler.Compiler().compile(pipeline_func=pipeline,\n                                package_path=pipeline_export_filepath)\n    # pipeline_params = {\n    #     'gcp_project': PROJECT_ID,\n    # }\n    # job = aip.PipelineJob(\n    #     display_name='test-pipeline',\n    #     template_path=pipeline_export_filepath,\n    #     pipeline_root=f'gs:\/\/{PIPELINE_ROOT}',\n    #     project=PROJECT_ID,\n    #     parameter_values=pipeline_params,\n    # )\n\n    # job.run()\n<\/code><\/pre>\n<p>When running the pipeline it throws this exception when running Batch prediction:<br \/>\n<code>details = &quot;List of found errors: 1.Field: batch_prediction_job.model; Message: Invalid Model resource name. <\/code>\nso I'm not sure what could be wrong. I tried to load model in the notebook (outside of component) and it correctly returns.<\/p>\n<p>Second issue I'm having is referencing GCS URI as output from component to batch job input.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>   input_data = get_input_data2()\n   gcc_aip.ModelBatchPredictOp(\n        project=PROJECT_ID,\n        job_display_name=f'test-prediction',\n        model=ml_model.output,\n        gcs_source_uris=[input_data.output],  # this doesn't work\n        # gcs_source_uris=['gs:\/\/mybucket\/output\/'],  # hardcoded gs uri works\n        gcs_destination_output_uri_prefix=f'gs:\/\/{PIPELINE_ROOT}\/prediction_output\/'\n    )\n<\/code><\/pre>\n<p>During compilation, I get following exception <code>TypeError: Object of type PipelineParam is not JSON serializable<\/code>, though I think this could be issue of ModelBatchPredictOp component.<\/p>\n<p>Again any help\/advice appreciated, I'm dealing with this from yesterday, so maybe I missed something obvious.<\/p>\n<p>libraries I'm using:<\/p>\n<pre><code>google-cloud-aiplatform==1.8.0  \ngoogle-cloud-pipeline-components==0.2.0  \nkfp==1.8.10  \nkfp-pipeline-spec==0.1.13  \nkfp-server-api==1.7.1\n<\/code><\/pre>\n<p><strong>UPDATE<\/strong>\nAfter comments, some research and tuning, for referencing model this works:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@component\ndef load_ml_model(project_id: str, model: Output[Artifact]):\n    region = 'us-central1'\n    model_id = '1234'\n    model_uid = f'projects\/{project_id}\/locations\/{region}\/models\/{model_id}'\n    model.uri = model_uid\n    model.metadata['resourceName'] = model_uid\n<\/code><\/pre>\n<p>and then I can use it as intended:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>batch_predict_op = gcc_aip.ModelBatchPredictOp(\n        project=gcp_project,\n        job_display_name=f'batch-prediction-test',\n        model=ml_model.outputs['model'],\n        gcs_source_uris=[input_batch_gcs_path],\ngcs_destination_output_uri_prefix=f'gs:\/\/{BUCKET_NAME}\/prediction_output\/test'\n    )\n<\/code><\/pre>\n<p><strong>UPDATE 2<\/strong>\nregarding GCS path, a workaround is to define path outside of the component and pass it as an input parameter, for example (abbreviated):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@dsl.pipeline(\n    name=&quot;my-pipeline&quot;,\n    pipeline_root=PIPELINE_ROOT,\n)\ndef pipeline(\n        gcp_project: str,\n        region: str,\n        bucket: str\n):\n    ts = datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)\n    \n    gcs_prediction_input_path = f'gs:\/\/{BUCKET_NAME}\/prediction_input\/video_batch_prediction_input_{ts}.jsonl'\n    batch_input_data_op = get_input_data(gcs_prediction_input_path)  # this loads input data to GCS path\n\n    batch_predict_op = gcc_aip.ModelBatchPredictOp(\n        project=gcp_project,\n        model=training_job_run_op.outputs[&quot;model&quot;],\n        job_display_name='batch-prediction',\n        # gcs_source_uris=[batch_input_data_op.output],\n        gcs_source_uris=[gcs_prediction_input_path],\n        gcs_destination_output_uri_prefix=f'gs:\/\/{BUCKET_NAME}\/prediction_output\/',\n    ).after(batch_input_data_op)  # we need to add 'after' so it runs after input data is prepared since get_input_data doesn't returns anything\n\n<\/code><\/pre>\n<p>still not sure, why it doesn't work\/compile when I return GCS path from <code>get_input_data<\/code> component<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":7,
        "Challenge_created_time":1639525350357,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1640079991423,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70356856",
        "Challenge_link_count":0,
        "Challenge_participation_count":8,
        "Challenge_readability":17.4,
        "Challenge_reading_time":78.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":null,
        "Challenge_title":"Vertex AI Model Batch prediction, issue with referencing existing model and input file on Cloud Storage",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1202.0,
        "Challenge_word_count":495,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1372196724860,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Prague, Czech Republic",
        "Poster_reputation_count":276.0,
        "Poster_view_count":47.0,
        "Solution_body":"<p>I'm glad you solved most of your main issues and found a workaround for model declaration.<\/p>\n<p>For your <code>input.output<\/code> observation on <code>gcs_source_uris<\/code>, the reason behind it is because the way the function\/class returns the value. If you dig inside the class\/methods of <code>google_cloud_pipeline_components<\/code>  you will find that it implements a structure that will allow you to use <code>.outputs<\/code> from the returned value of the function called.<\/p>\n<p>If you go to the implementation of one of the components of the pipeline you will find that it returns an output array from <code>convert_method_to_component<\/code> function. So, in order to have that implemented in your custom class\/function your function should return a value which can be called as an attribute. Below is a basic implementation of it.<\/p>\n<pre><code>class CustomClass():\n     def __init__(self):\n       self.return_val = {'path':'custompath','desc':'a desc'}\n      \n     @property\n     def output(self):\n       return self.return_val \n\nhello = CustomClass()\nprint(hello.output['path'])\n<\/code><\/pre>\n<p>If you want to dig more about it you can go to the following pages:<\/p>\n<ul>\n<li><p><a href=\"https:\/\/github.com\/bharathdsce\/kubeflow\/blob\/fcd627714664956b2c280b0109b64633bc99fa05\/components\/google-cloud\/google_cloud_pipeline_components\/aiplatform\/utils.py#L383\" rel=\"nofollow noreferrer\">convert_method_to_component<\/a>, which is the implementation of <code>convert_method_to_component<\/code><\/p>\n<\/li>\n<li><p><a href=\"https:\/\/www.programiz.com\/python-programming\/property\" rel=\"nofollow noreferrer\">Properties<\/a>, basics of property in python.<\/p>\n<\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.6,
        "Solution_reading_time":21.46,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":177.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1507058830472,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I used AutoML in Databricks Notebooks for a binary classification problem and the winning model flavor was XGBoost (big surprise).<\/p>\n<p>The outputted model is of this variety:<\/p>\n<pre><code>mlflow.pyfunc.loaded_model:\n      artifact_path: model\n      flavor: mlflow.sklearn\n      run_id: 123456789\n<\/code><\/pre>\n<p>Any idea why when I use <code>model.predict_proba(X)<\/code>, I get this response?<\/p>\n<p><code>AttributeError: 'PyFuncModel' object has no attribute 'predict_proba'<\/code><\/p>\n<p>I know it is possible to get the probabilities because ROC\/AUC is a metric used for tuning the model. Any help would be amazing!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640912523883,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70538098",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":8.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Databricks MLFlow AutoML XGBoost can't predict_proba()",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":451.0,
        "Challenge_word_count":82,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1562828557216,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Francisco, CA, USA",
        "Poster_reputation_count":77.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>I had the same issue with catboost model.\nThe way I solved it was by saving the artifacts in a local dir<\/p>\n<pre><code>import os\nfrom mlflow.tracking import MlflowClient\nclient = MlflowClient()\nlocal_dir = &quot;\/dbfs\/FileStore\/user\/models&quot;\nlocal_path = client.download_artifacts('run_id', &quot;model&quot;, local_dir)```\n\n```model_path = '\/dbfs\/FileStore\/user\/models\/model\/model.cb'\nmodel = CatBoostClassifier()\nmodel = model.load_model(model_path)\nmodel.predict_proba(test_set)```\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.2,
        "Solution_reading_time":6.7,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":44.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1512770138847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a <code>.npy<\/code> file for each one of the training instances. All of these files are available on S3 in <code>train_data<\/code> folder. I want to train a tensorflow model on these training instances. To do that, I wish to spin up separate aws training instance for each training job which could access the files from s3 and train the model on it. What changes in the training script are required for doing this?<\/p>\n<p>I have following config in the training script:<\/p>\n<pre><code>parser.add_argument('--gpu-count', type=int, default=os.environ['SM_NUM_GPUS'])\nparser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\nparser.add_argument('--train_channel', type=str, default=os.environ['SM_CHANNELS'])\n<\/code><\/pre>\n<p>I have created the training estimator in jupyter instance as:<\/p>\n<pre><code>tf_estimator = TensorFlow(entry_point = 'my_model.py', \n                          role = role, \n                          train_instance_count = 1, \n                          train_instance_type = 'local_gpu', \n                          framework_version = '1.15.2', \n                          py_version = 'py3', \n                          hyperparameters = {'epochs': 1})\n<\/code><\/pre>\n<p>I am calling the fit function of the estimator as:<\/p>\n<pre><code>tf_estimator.fit({'train_channel':'s3:\/\/sagemaker-ml\/train_data\/'})\n<\/code><\/pre>\n<p>where <code>train_data<\/code> folder on S3 contains the <code>.npy<\/code> files of training instances.<\/p>\n<p>But when I call the fit function, I get an error:<\/p>\n<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '[&quot;train_channel&quot;]\/train_data_12.npy'\n<\/code><\/pre>\n<p>Not sure what am I missing here, as I can see the file mentioned above on S3.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593203486503,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1593203849583,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62602435",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":21.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"How to train tensorflow on sagemaker in script mode when the data resides in multiple files on s3?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":340.0,
        "Challenge_word_count":198,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1378268842847,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, India",
        "Poster_reputation_count":4616.0,
        "Poster_view_count":592.0,
        "Solution_body":"<p><code>SM_CHANNELS<\/code> returns a list of channel names. What you're looking for is <code>SM_CHANNEL_TRAIN_CHANNEL<\/code> (&quot;SM_CHANNEL&quot; + your channel name), which provides the filesystem location for the channel:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>parser.add_argument('--train_channel', type=str, default=os.environ['SM_CHANNEL_TRAIN_CHANNEL'])\n<\/code><\/pre>\n<p>docs: <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\/blob\/master\/ENVIRONMENT_VARIABLES.md#sm_channel_channel_name\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-training-toolkit\/blob\/master\/ENVIRONMENT_VARIABLES.md#sm_channel_channel_name<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":27.3,
        "Solution_reading_time":9.24,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":36.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I figured out that there are some primary metrics I can choose when I run an automated ML experiment. Yet the number of primary metrics is fewer than the run metrics in the result page. I want to deal with imbalanced data(10:1 or 20:1) and    <\/p>\n<p>looked up the links below:    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-manage-ml-pitfalls#identify-models-with-imbalanced-data\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-manage-ml-pitfalls#identify-models-with-imbalanced-data<\/a>    <br \/>\nand    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train<\/a>    <\/p>\n<p>It seems F1 score is recommended to evaluate each model with imbalanced data.    <\/p>\n<p>Here are my questions:    <\/p>\n<ul>\n<li> Is there any way to set F1 score or multiple measures as a primary metric?     <\/li>\n<li> If there is no such way, should I do it manually?     <\/li>\n<li> Of all the given primary metrics, which primary metric is the most appropriate(to build a Classification model with imbalanced data)?    <\/li>\n<\/ul>\n<p>Thanks.    <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1593398061863,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/40792\/azure-automated-ml(interface)-choosing-primary-met",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":13.6,
        "Challenge_reading_time":16.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Automated ML(interface) choosing primary metrics to handle imbalanced data",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":144,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>For imbalanced data, it is preferred to choose AUC Weighted. Also user should then choose a metric that is appropriate to work well for imbalance. E.g. F1, micro averaged AUC, balanced accuracy for model evaluation. For primary metric (metric used for model optimization) the user should preferably choose AUC Weighted instead of accuracy.    <br \/>\nCurrently from the ml.azure.com the following metrics are supported. To add F1 score metric forwarded to product team to check on this.    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train#primary-metric\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train#primary-metric<\/a>    <br \/>\n<img src=\"\/answers\/storage\/temp\/10986-screenshot-162.png\" alt=\"10986-screenshot-162.png\" \/>    <\/p>\n",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.0,
        "Solution_reading_time":10.62,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":84.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1525393797416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Philadelphia, USA",
        "Answerer_reputation_count":130.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created an mlflow model with custom pyfunc. It shows the results when I send input to the loaded model in Jupyter notebook.\nHowever if I am trying to serve it to a local port<\/p>\n<pre><code>!mlflow models serve -m Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model -p 8001\n<\/code><\/pre>\n<p>I am getting this error<\/p>\n<pre><code> Traceback (most recent call last):\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/bin\/mlflow&quot;, line 10, in &lt;module&gt;\n    sys.exit(cli())\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 829, in __call__\n    return self.main(*args, **kwargs)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 782, in main\n    rv = self.invoke(ctx)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 610, in invoke\n    return callback(*args, **kwargs)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/models\/cli.py&quot;, line 56, in serve\n    install_mlflow=install_mlflow).serve(model_uri=model_uri, port=port,\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/models\/cli.py&quot;, line 163, in _get_flavor_backend\n    append_to_uri_path(underlying_model_uri, &quot;MLmodel&quot;), output_path=tmp.path())\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/tracking\/artifact_utils.py&quot;, line 76, in _download_artifact_from_uri\n    artifact_path=artifact_path, dst_path=output_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/local_artifact_repo.py&quot;, line 67, in download_artifacts\n    return super(LocalArtifactRepository, self).download_artifacts(artifact_path, dst_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py&quot;, line 140, in download_artifacts\n    return download_file(artifact_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py&quot;, line 105, in download_file\n    self._download_file(remote_file_path=fullpath, local_path=local_file_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/local_artifact_repo.py&quot;, line 95, in _download_file\n    shutil.copyfile(remote_file_path, local_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/shutil.py&quot;, line 120, in copyfile\n    with open(src, 'rb') as fsrc:\nFileNotFoundError: [Errno 2] No such file or directory: 'Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model\/MLmodel'\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611838887777,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65937623",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":23.6,
        "Challenge_reading_time":47.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":null,
        "Challenge_title":"Unable to serve an mlflow model locally",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1277.0,
        "Challenge_word_count":199,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1573739890560,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>From your error traceback, the model artifact can't be located. In your code, you are executing the 'mlflow' command from within a Jupyter Notebook. I would suggest trying the following:<\/p>\n<ol>\n<li>Check if your models artifacts are on the path you are using Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model<\/li>\n<li>Try opening a terminal, then <code>cd \/Home\/miniconda3\/envs<\/code> and  execute <code>mlflow models serve -m .\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model -p 8001<\/code><\/li>\n<li>MLFlow offers different solutions to serve a model, you can try to register your model and refer to it as &quot;models:\/{model_name}\/{stage}&quot; as mentioned in the Model Registry <a href=\"https:\/\/mlflow.org\/docs\/latest\/model-registry.html#serving-an-mlflow-model-from-model-registry\" rel=\"nofollow noreferrer\">docs<\/a><\/li>\n<\/ol>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.2,
        "Solution_reading_time":11.53,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":92.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1528629350990,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pakistan",
        "Answerer_reputation_count":439.0,
        "Answerer_view_count":51.0,
        "Challenge_adjusted_solved_time":5616.0211319444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a pre-trained model which I am loading in AWS SageMaker Notebook Instance from S3 Bucket and upon providing a test image for prediction from S3 bucket it gives me the accurate results as required. I want to deploy it so that I can have an endpoint which I can further integrate with AWS Lambda Function and AWS API GateWay so that I can use the model with real time application.\nAny idea how can I deploy the model from AWS Sagemaker Notebook Instance and get its endpoint?\nCode inside the <code>.ipynb<\/code> file is given below for reference.<\/p>\n<pre><code>import boto3\nimport pandas as pd\nimport sagemaker\n#from sagemaker import get_execution_role\nfrom skimage.io import imread\nfrom skimage.transform import resize\nimport numpy as np\nfrom keras.models import load_model\nimport os\nimport time\nimport json\n#role = get_execution_role()\nrole = sagemaker.get_execution_role()\n\nbucketname = 'bucket' # bucket where the model is hosted\nfilename = 'test_model.h5' # name of the model\ns3 = boto3.resource('s3')\nimage= s3.Bucket(bucketname).download_file(filename, 'test_model_new.h5')\nmodel= 'test_model_new.h5'\n\nmodel = load_model(model)\n\nbucketname = 'bucket' # name of the bucket where the test image is hosted\nfilename = 'folder\/image.png' # prefix\ns3 = boto3.resource('s3')\nfile= s3.Bucket(bucketname).download_file(filename, 'image.png')\nfile_name='image.png'\n\ntest=np.array([resize(imread(file_name), (137, 310, 3))])\n\ntest_predict = model.predict(test)\n\nprint ((test_predict &gt; 0.5).astype(np.int))\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608787488573,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65434323",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":20.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"How to deploy a Pre-Trained model using AWS SageMaker Notebook Instance?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1387.0,
        "Challenge_word_count":202,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1528629350990,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pakistan",
        "Poster_reputation_count":439.0,
        "Poster_view_count":51.0,
        "Solution_body":"<p>Here is the solution that worked for me. Simply follow the following steps.<\/p>\n<p>1 - Load your model in the SageMaker's jupyter environment with the help of<\/p>\n<pre><code>from keras.models import load_model\n\nmodel = load_model (&lt;Your Model name goes here&gt;) #In my case it's model.h5\n<\/code><\/pre>\n<p>2 - Now that the model is loaded convert it into the <code>protobuf format<\/code> that is required by <code>AWS<\/code> with the help of<\/p>\n<pre><code>def convert_h5_to_aws(loaded_model):\n\nfrom tensorflow.python.saved_model import builder\nfrom tensorflow.python.saved_model.signature_def_utils import predict_signature_def\nfrom tensorflow.python.saved_model import tag_constants\n\nmodel_version = '1'\nexport_dir = 'export\/Servo\/' + model_version\n# Build the Protocol Buffer SavedModel at 'export_dir'\nbuilder = builder.SavedModelBuilder(export_dir)\n# Create prediction signature to be used by TensorFlow Serving Predict API\nsignature = predict_signature_def(\n    inputs={&quot;inputs&quot;: loaded_model.input}, outputs={&quot;score&quot;: loaded_model.output})\nfrom keras import backend as K\n\nwith K.get_session() as sess:\n    # Save the meta graph and variables\n    builder.add_meta_graph_and_variables(\n        sess=sess, tags=[tag_constants.SERVING], signature_def_map={&quot;serving_default&quot;: signature})\n    builder.save()\nimport tarfile\nwith tarfile.open('model.tar.gz', mode='w:gz') as archive:\n    archive.add('export', recursive=True)\nimport sagemaker\n\nsagemaker_session = sagemaker.Session()\ninputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='model')\nconvert_h5_to_aws(model):\n<\/code><\/pre>\n<p>3 - And now you can deploy your model with the help of<\/p>\n<pre><code>!touch train.py\nfrom sagemaker.tensorflow.model import TensorFlowModel\nsagemaker_model = TensorFlowModel(model_data = 's3:\/\/' + sagemaker_session.default_bucket() + '\/model\/model.tar.gz',\n                                  role = role,\n                                  framework_version = '1.15.2',\n                                  entry_point = 'train.py')\n%%timelog\npredictor = sagemaker_model.deploy(initial_instance_count=1,\n                                   instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n<p>This will generate the endpoint which can be seen in the Inference section of the Amazon SageMaker and with the help of that endpoint you can now make predictions from the jupyter notebook as well as from web and mobile applications.\nThis <a href=\"https:\/\/www.youtube.com\/watch?v=RPnvfxR5DY8\" rel=\"nofollow noreferrer\">Youtube tutorial<\/a> by Liam and <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">AWS blog<\/a> by Priya helped me alot.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1629005164648,
        "Solution_link_count":2.0,
        "Solution_readability":13.8,
        "Solution_reading_time":34.23,
        "Solution_score_count":3.0,
        "Solution_sentence_count":28.0,
        "Solution_word_count":244.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1631803441500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"mexico",
        "Answerer_reputation_count":1258.0,
        "Answerer_view_count":685.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am new to google cloud service VERTEX AI.<\/p>\n<p>I am looking to Create, train, and deploy an AutoML text classification model through .Net application. I did not find anything for .Net with Vertex AI. If someone can please guide my to the location or any .Net code samples, will be really helpful.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647958547627,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1647980919087,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71573565",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.2,
        "Challenge_reading_time":4.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Google Cloud Vertex AI with .Net",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":158.0,
        "Challenge_word_count":58,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1462469556836,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":509.0,
        "Poster_view_count":65.0,
        "Solution_body":"<p>You can check <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AutoML.V1\/latest\" rel=\"nofollow noreferrer\">Google.Cloud.AutoML.V1<\/a> NuGet package for .NET. Additionally, check the <a href=\"https:\/\/github.com\/googleapis\/google-cloud-dotnet\/tree\/main\/apis\/Google.Cloud.AutoML.V1\" rel=\"nofollow noreferrer\">Github of Google.Cloud.AutoML.V1<\/a> NuGet Package where you can see the sample codes.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":19.3,
        "Solution_reading_time":5.74,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":29.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I\u2019m performing a series of experiments with AutoML and I need to see the featurized data. I mean, not just the new features names retrieved by method get_engineered_feature_names() or the featurization details retrieved by get_featurization_summary(), I refer to the whole transformed dataset, the one obtained after scaling\/normalization\/featurization that is then used to train the models.   <\/p>\n<p>Is it possible to access to this dataset or download it as a file?  <\/p>\n<p>Thanks.  <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618306428473,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/355323\/how-to-access-to-the-featurized-dataset-in-automat",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.3,
        "Challenge_reading_time":6.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"How to access to the featurized dataset in Automated ML",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":82,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, currently, we don't store the dataset from scaling\/normalization\/featurization after the run is complete. This feature isn't supported at this time. Sorry for the inconvenience.<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.1,
        "Solution_reading_time":2.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1321893442023,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1611.0,
        "Answerer_view_count":189.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running a Python script with Tensorflow in Amazon Sagemaker notebook instance.  I have no trouble writing to the storage in the notebook normally, but for some reason I am unsuccessful when trying to save Tensorflow model checkpoints.  This code previously worked before it was ported to Sagemaker.<\/p>\n\n<p>Below is a reduced version of my code:<\/p>\n\n<pre><code>bucket = 'sagemaker-complaints-data'    \nprefix = 'DeepTestV2' # place to upload training files within the bucket\ntimestamp = str(int(time()))\nout_dir = os.path.abspath(os.path.join(bucket, prefix, \"runs\", timestamp))\ncheckpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"model\")\npath = saver.save(sess, checkpoint_prefix, global_step=current_step)\nprint(\"Saved model checkpoint to {}\\n\".format(path))\n<\/code><\/pre>\n\n<p>No errors are being thrown and the print statement is outputting the correct path.  I have researched whether there are any known issues with using checkpoints in Sagemaker but have come across literally no posts describing this.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1517393582673,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48539564",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":14.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Tensorflow - Checkpoints not saving to Sagemaker Notebook Instance",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":690.0,
        "Challenge_word_count":136,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1321893442023,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1611.0,
        "Poster_view_count":189.0,
        "Solution_body":"<p>I have found out where this is - for some reason \"checkpoints\" seems to be a reserved word - changing the word to \"checks\" allowed me to write the folder.  Hope this helps someone!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.8,
        "Solution_reading_time":2.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":32.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1626973229036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":199.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an already trained a TensorFlow model outside of SageMaker.<\/p>\n<p>I am trying to focus on deployment\/inference but I am facing issues with inference.<\/p>\n<p>For deployment I did this:<\/p>\n<pre><code>from sagemaker.tensorflow.serving import TensorFlowModel\ninstance_type = 'ml.c5.xlarge' \n\nmodel = TensorFlowModel(\n    model_data=model_data,\n    name= 'tfmodel1',\n    framework_version=&quot;2.2&quot;,\n    role=role, \n    source_dir='code',\n)\n\npredictor = model.deploy(endpoint_name='test', \n                                       initial_instance_count=1, \n                                       tags=tags,\n                                       instance_type=instance_type)\n<\/code><\/pre>\n<p>When I tried to infer the model I did this:<\/p>\n<pre><code>import PIL\nfrom PIL import Image\nimport numpy as np\nimport json\nimport boto3\n\nimage = PIL.Image.open('img_test.jpg')\nclient = boto3.client('sagemaker-runtime')\nbatch_size = 1\nimage = np.asarray(image.resize((512, 512)))\nimage = np.concatenate([image[np.newaxis, :, :]] * batch_size)\nbody = json.dumps({&quot;instances&quot;: image.tolist()})\n\nioc_predictor_endpoint_name = &quot;test&quot;\ncontent_type = 'application\/x-image'   \nioc_response = client.invoke_endpoint(\n    EndpointName=ioc_predictor_endpoint_name,\n    Body=body,\n    ContentType=content_type\n )\n<\/code><\/pre>\n<p>But I have this error:<\/p>\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from primary with message &quot;{&quot;error&quot;: &quot;Unsupported Media Type: application\/x-image&quot;}&quot;.\n<\/code><\/pre>\n<p>I also tried:<\/p>\n<pre><code>from sagemaker.predictor import Predictor\n\npredictor = Predictor(ioc_predictor_endpoint_name)\ninference_response = predictor.predict(data=body)\nprint(inference_response)\n<\/code><\/pre>\n<p>And have this error:<\/p>\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from primary with message &quot;{&quot;error&quot;: &quot;Unsupported Media Type: application\/octet-stream&quot;}&quot;.\n<\/code><\/pre>\n<p>What can I do ? I don't know if I missed something<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651077733827,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1651081710500,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72032469",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.5,
        "Challenge_reading_time":27.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"How to infer a tensorflow pre trained deployed model with an image?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":145.0,
        "Challenge_word_count":191,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1606642099552,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":371.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>Have you tested this model locally? How does inference work with your TF model locally? This should show you how the input needs to be formatted for inference with that model in specific. Application\/x-image data format should be fine. Do you have a custom inference script? Check out this link here for adding an inference script with will let you control pre\/post processing and you can log each line to capture the error: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.9,
        "Solution_reading_time":7.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":77.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1508924024027,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":118.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have registered a scikit learn model on my MLflow Tracking server, and I am loading it with <code>sklearn.load_model(model_uri)<\/code>.<\/p>\n<p>Now, I would like to access the signature of the model so I can get a list of the model's required inputs\/features so I can retrieve them from my feature store by name. I can't seem to find any utility or method in the <code>mlflow<\/code> API or the <code>MLFlowClient<\/code> API that will let me access a signature or inputs\/outputs attribute, even though I can see a list of inputs and outputs under each version of the model in the UI.<\/p>\n<p>I know that I can find the input sample and the model configuration in the model's artifacts, but that would require me actually downloading the artifacts and loading them manually in my script. I don't need to avoid that, but I am surprised that I can't just return the signature as a dictionary the same way I can return a run's parameters or metrics.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643655088920,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70931309",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":12.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"How to retrieve the model signature from the MLflow Model Registry",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":904.0,
        "Challenge_word_count":173,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1466188731112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Michigan",
        "Poster_reputation_count":414.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>The way to access the model's signature without downloading the MLModel file is under the loaded model. And then you'll access the model's attributes, such as its signature or even other Pyfunc-defined methods.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\n\nmodel = mlflow.pyfunc.load_model(&quot;runs:\/&lt;run_id&gt;\/model&quot;)\nprint(model._model_meta._signature)\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.5,
        "Solution_reading_time":5.3,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":41.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1403885399456,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":402.0,
        "Answerer_view_count":90.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I registered a model in an Azure ML notebook along with its datasets. In ML Studio I can see the model listed under the dataset, but no dataset gets listed under the model. What should I do to have datasets listed under models?<\/p>\n<ul>\n<li>Model listed under dataset:<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fh3bV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fh3bV.png\" alt=\"model dataset\" \/><\/a><\/p>\n<ul>\n<li>Dataset not listed under the model:<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Ueys2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Ueys2.png\" alt=\"dataset model\" \/><\/a><\/p>\n<ul>\n<li>Notebook code:<\/li>\n<\/ul>\n<pre><code>import pickle\nimport sys\nfrom azureml.core import Workspace, Dataset, Model\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import assert_all_finite\n\nworkspace = Workspace('&lt;snip&gt;', '&lt;snip&gt;', '&lt;snip&gt;')\ndataset = Dataset.get_by_name(workspace, name='creditcard')\ndata = dataset.to_pandas_dataframe()\ndata.dropna(inplace=True)\nX = data.drop(labels=[&quot;Class&quot;], axis=1, inplace=False)\ny = data[&quot;Class&quot;]\n\nmodel = make_pipeline(StandardScaler(), GradientBoostingClassifier())\nmodel.fit(X, y)\n\nwith open('creditfraud_sklearn_model.pkl', 'wb') as outfile:\n    pickle.dump(model, outfile)\n\nModel.register(\n    Workspace = workspace,\n    model_name = 'creditfraud_sklearn_model',\n    model_path = 'creditfraud_sklearn_model.pkl',\n    description = 'Gradient Boosting classifier for Kaggle credit-card fraud',\n    model_framework = Model.Framework.SCIKITLEARN,\n    model_framework_version = sys.modules['sklearn'].__version__,\n    sample_input_dataset = dataset,\n    sample_output_dataset = dataset)\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1626199230667,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68367348",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":24.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML Studio not showing datasets under models",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":29.0,
        "Challenge_word_count":162,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403885399456,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":402.0,
        "Poster_view_count":90.0,
        "Solution_body":"<p>It looks like <code>add_dataset_references()<\/code> needs to be called to have datasets displayed under models:<\/p>\n<pre><code>model_registration.add_dataset_references([(&quot;input dataset&quot;, dataset)])\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.9,
        "Solution_reading_time":3.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>For the best run and fitted model from a previously run experiment, Looking for the python code.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1590161357353,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61958473",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":1.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Code for Best Run and Model from previous experiment",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":103.0,
        "Challenge_word_count":25,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Below is the code you can reuse\n<a href=\"https:\/\/github.com\/microsoft\/MLOpsPython\/blob\/master\/diabetes_regression\/evaluate\/evaluate_model.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/microsoft\/MLOpsPython\/blob\/master\/diabetes_regression\/evaluate\/evaluate_model.py<\/a><\/p>\n\n<p>Assuming in each previous experiment run, a model was registered with a tag that contains a metric of interest (test_mae for example), below is the code to retrieve the version with lowest mae.<\/p>\n\n<pre><code>from azureml.core.model import Model\n\nmodel_name = \"YOUR_MODEL_NAME\"\nmodel_path = \"LOCAL_PATH\u201d\nmodel_version_list = [(model.version,float(model.tags[\"test_mae\"])) for model in Model.list(workspace = ws,name =model_name)]\nmodel_version_list.sort(key = lambda a: a[0])\nlowest_mae_version =model_version_list[0][0]\nprint(\"best version is {} with mae at {}\".format(lowest_mae_version,model_version_list[0][1]))\nmodel = Model(name = model_name,workspace = ws, version =lowest_mae_version)\nmodel.download(model_path, exist_ok=True)\n<\/code><\/pre>\n\n<p>when the model has not been registered,models in an automl run and  would like to get all the models and compare the results depending on featurization, method used, and metrics, also with other data sets. The models are all inside the workspace with the GUI you can see them and download them by hand. <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.2,
        "Solution_reading_time":17.54,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":137.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nIs there a way to regularly checkpoint model artifact in a SageMaker training job for BYO training container?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586331915000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668623326404,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUrXX2MIygS5igas27GrAhHw\/how-to-checkpoint-sagemaker-model-artifact-during-a-training-job",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":2.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"How to checkpoint SageMaker model artifact during a training job?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":403.0,
        "Challenge_word_count":28,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"If you specify a checkpoint configuration (***regardless of managed spot training***) when starting a training job, checkpointing will work. You can provide a local path and S3 path as follows (API reference):\n\n    \"CheckpointConfig\": { \n      \"LocalPath\": \"string\",\n      \"S3Uri\": \"string\"\n    }\n\nThe local path defaults to `\/opt\/ml\/checkpoints\/`, and then you specify the target path in S3 with `S3Uri`.\n\n***Given this configuration, SageMaker will configure an output channel with Continuous upload mode to Amazon S3***. At the time being, this results in running an agent on the hosts that watches the file system and continuously uploads data to Amazon S3. Similar behavior is applied when debugging is enabled, for delivering tensor data to Amazon S3.\n\nAs commented, `sagemaker-containers` implements its own code to save intermediate outputs and watching files on the file system, but I would rather rely on the functionality offered by the service to avoid dependencies on specific libraries where possible.\n\n**Note**: when using SageMaker Processing, which in my view can be considered an abstraction over training or, from another perspective, the foundation for training, you can configure an output channel to use continuous upload mode; further info [here][1].\n\n\n  [1]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ProcessingS3Output.html",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925569444,
        "Solution_link_count":1.0,
        "Solution_readability":16.1,
        "Solution_reading_time":16.88,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":188.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1595690872796,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"D\u00fcsseldorf, Deutschland",
        "Answerer_reputation_count":699.0,
        "Answerer_view_count":46.0,
        "Challenge_adjusted_solved_time":78.6193025,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>For a project I am working on, which uses annual financial reports data (of multiple categories) from companies which have been successful or gone bust\/into liquidation, I previously created a (fairly well performing) model on AWS Sagemaker using a multiple linear regression algorithm (specifically, the AWS stock algorithm for logistic regression\/classification problems - the 'Linear Learner' algorithm)<\/p>\n<p>This model just produces a simple &quot;company is in good health&quot; or &quot;company looks like it will go bust&quot; binary prediction, based on one set of annual data fed in; e.g.<\/p>\n<pre><code>query input: {data:[{\n&quot;Gross Revenue&quot;: -4000,\n&quot;Balance Sheet&quot;: 10000,\n&quot;Creditors&quot;: 4000,\n&quot;Debts&quot;: 1000000 \n}]}\n\ninference output: &quot;in good health&quot; \/ &quot;in bad health&quot;\n<\/code><\/pre>\n<p>I trained this model by just ignoring what year for each company the values were from and pilling in all of the annual financial reports data (i.e. one years financial data for one company = one input line) for the training, along with the label of &quot;good&quot; or &quot;bad&quot; - a good company was one which has existed for a while, but hasn't gone bust, a bad company is one which was found to have eventually gone bust; e.g.:<\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>label<\/th>\n<th>Gross Revenue<\/th>\n<th>Balance Sheet<\/th>\n<th>Creditors<\/th>\n<th>Debts<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>good<\/td>\n<td>10000<\/td>\n<td>20000<\/td>\n<td>0<\/td>\n<td>0<\/td>\n<\/tr>\n<tr>\n<td>bad<\/td>\n<td>0<\/td>\n<td>5<\/td>\n<td>100<\/td>\n<td>10000<\/td>\n<\/tr>\n<tr>\n<td>bad<\/td>\n<td>20000<\/td>\n<td>0<\/td>\n<td>4<\/td>\n<td>100000000<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div>\n<p>I hence used these multiple features (gross revenue, balance sheet...) along with the label (good\/bad) in my training input, to create my first model.<\/p>\n<p>I would like to use the same features as before as input (gross revenue, balance sheet..) but over multiple years; e.g take the values from 2020 &amp; 2019 and use these (along with the eventual company status of &quot;good&quot; or &quot;bad&quot;) as the singular input for my new model. However I'm unsure of the following:<\/p>\n<ul>\n<li>is this an inappropriate use of logistic regression Machine learning? i.e. is there a more suitable algorithm I should consider?<\/li>\n<li>is it fine, or terribly wrong to try and just use the same technique as before, but combine the data for both years into one input line like:<\/li>\n<\/ul>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>label<\/th>\n<th>Gross Revenue(2019)<\/th>\n<th>Balance Sheet(2019)<\/th>\n<th>Creditors(2019)<\/th>\n<th>Debts(2019)<\/th>\n<th>Gross Revenue(2020)<\/th>\n<th>Balance Sheet(2020)<\/th>\n<th>Creditors(2020)<\/th>\n<th>Debts(2020)<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>good<\/td>\n<td>10000<\/td>\n<td>20000<\/td>\n<td>0<\/td>\n<td>0<\/td>\n<td>30000<\/td>\n<td>10000<\/td>\n<td>40<\/td>\n<td>500<\/td>\n<\/tr>\n<tr>\n<td>bad<\/td>\n<td>100<\/td>\n<td>50<\/td>\n<td>200<\/td>\n<td>50000<\/td>\n<td>100<\/td>\n<td>5<\/td>\n<td>100<\/td>\n<td>10000<\/td>\n<\/tr>\n<tr>\n<td>bad<\/td>\n<td>5000<\/td>\n<td>0<\/td>\n<td>2000<\/td>\n<td>800000<\/td>\n<td>2000<\/td>\n<td>0<\/td>\n<td>4<\/td>\n<td>100000000<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div>\n<p>I would personally expect that a company which has gotten worse over time (i.e. companies finances are worse in 2020 than in 2019) should be more likely to be found to be a &quot;bad&quot;\/likely to go bust, so I would hope that, if I feed in data like in the above example (i.e. earlier years data comes before later years data, on an input line) my training job ends up creating a model which gives greater weighting to the earlier years data, when making predictions<\/p>\n<p>Any advice or tips would be greatly appreciated - I'm pretty new to machine learning and would like to learn more<\/p>\n<p>UPDATE:<\/p>\n<p>Using Long-Short-Term-Memory Recurrent Neural Networks (LSTM RNN) is one potential route I think I could try taking, but this seems to commonly just be used with multivariate data over many dates; my data only has 2 or 3 dates worth of multivariate data, per company. I would want to try using the data I have for all the companies, over the few dates worth of data there are, in training<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1613315731557,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1614168553343,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66196815",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":17.6,
        "Challenge_reading_time":54.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"Using Logistic Regression For Timeseries Data in Amazon SageMaker",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":183.0,
        "Challenge_word_count":597,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1566839876032,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":65.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>I once developed a so called Genetic Time Series in R. I used a Genetic Algorithm which sorted out the best solutions from multivariate data, which were fitted on a VAR in differences or a VECM. Your data seems more macro economic or financial than user-centric and VAR or VECM seems appropriate. (Surely it is possible to treat time-series data in the same way so that we can use LSTM or other approaches, but these are very common) However, I do not know if VAR in differences or VECM works with binary classified labels. Perhaps if you would calculate a metric outcome, which you later label encode to a categorical feature (or label it first to a categorical) than VAR or VECM may also be appropriate.<\/p>\n<p>However you may add all yearly data points to one data points per firm to forecast its survival, but you would loose a lot of insight. If you are interested in time series ML which works a little bit different than for neural networks or elastic net (which could also be used with time series) let me know. And we can work something out. Or I'll paste you some sources.<\/p>\n<p>Summary:\n1.)\nIt is possible to use LSTM, elastic NEt (time points may be dummies or treated as cross sectional panel) or you use VAR in differences and VECM with a slightly different out come variable<\/p>\n<p>2.)\nIt is possible but you will loose information over time.<\/p>\n<p>All the best,\nPatrick<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1614451582832,
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":16.86,
        "Solution_score_count":2.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":247.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1401922736652,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Brisbane",
        "Answerer_reputation_count":748.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I'm looking to make some hyper parameters available to the serving endpoint in SageMaker. The training instances is given access to input parameters using hyperparameters in:<\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='autocat.py',\n                       role=role,\n                       output_path=params['output_path'],\n                       code_location=params['code_location'],\n                       train_instance_count=1,\n                       train_instance_type='ml.c4.xlarge',\n                       training_steps=10000,\n                       evaluation_steps=None,\n                       hyperparameters=params)\n<\/code><\/pre>\n\n<p>However, when the endpoint is deployed, there is no way to pass in parameters that are used to control the data processing in the <code>input_fn(serialized_input, content_type)<\/code> function.<\/p>\n\n<p>What would be the best way to pass parameters to the serving instance?? Is the <code>source_dir<\/code> parameter defined in the <code>sagemaker.tensorflow.TensorFlow<\/code> class copied to the serving instance? If so, I could use a config.yml or similar.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1521754623920,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1521754933880,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49438903",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":13.4,
        "Challenge_reading_time":13.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"How to make parameters available to SageMaker Tensorflow Endpoint",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1426.0,
        "Challenge_word_count":108,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443201378360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":749.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>Ah i have had a similar problem to you where I needed to download something off S3 to use in the input_fn for inference. In my case it was a dictionary.<\/p>\n\n<p>Three options:<\/p>\n\n<ol>\n<li>use your config.yml approach, and download and import the s3 file from within your entrypoint file before any function declarations. This would make it available to the input_fn <\/li>\n<li>Keep using the hyperparameter approach, download and import the vectorizer in <code>serving_input_fn<\/code> and make it available via a global variable so that <code>input_fn<\/code> has access to it.<\/li>\n<li>Download the file from s3 before training and include it in the source_dir directly.<\/li>\n<\/ol>\n\n<p>Option 3 would only work if you didnt need to make changes to the vectorizer seperately after initial training.<\/p>\n\n<p>Whatever you do, don't download the file directly in input_fn. I made that mistake and the performance is terrible as each invoking of the endpoint would result in the s3 file being downloaded.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.1,
        "Solution_reading_time":12.44,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":157.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1337868050812,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":483.0,
        "Answerer_view_count":105.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I can't find the proper way to add dependencies to my Azure Container Instance for ML Inference.<\/p>\n<p>I basically started by following this tutorial : <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-train-deploy-notebook\" rel=\"nofollow noreferrer\">Train and deploy an image classification model with an example Jupyter Notebook<\/a><\/p>\n<p>It works fine.<\/p>\n<p>Now I want to deploy my trained TensorFlow model for inference. I tried many ways, but I was never able to add python dependencies to the Environment.<\/p>\n<h1>From the TensorFlow curated environment<\/h1>\n<p>Using <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/resource-curated-environments#inference-curated-environments-and-prebuilt-docker-images\" rel=\"nofollow noreferrer\">AzureML-tensorflow-2.4-ubuntu18.04-py37-cpu-inference<\/a> :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace\n\n\n# connect to your workspace\nws = Workspace.from_config()\n\n# names\nexperiment_name = &quot;my-experiment&quot;\nmodel_name = &quot;my-model&quot;\nenv_version=&quot;1&quot;\nenv_name=&quot;my-env-&quot;+env_version\nservice_name = str.lower(model_name + &quot;-service-&quot; + env_version)\n\n\n# create environment for the deploy\nfrom azureml.core.environment import Environment, DEFAULT_CPU_IMAGE\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.webservice import AciWebservice\n\n# get a curated environment\nenv = Environment.get(\n    workspace=ws, \n    name=&quot;AzureML-tensorflow-2.4-ubuntu18.04-py37-cpu-inference&quot;,\n# )\ncustom_env = env.clone(env_name)\ncustom_env.inferencing_stack_version='latest'\n\n# add packages\nconda_dep = CondaDependencies()\npython_packages = ['joblib', 'numpy', 'os', 'json', 'tensorflow']\nfor package in python_packages:\n    conda_dep.add_pip_package(package)\n    conda_dep.add_conda_package(package)\n\n# Adds dependencies to PythonSection of env\ncustom_env.python.user_managed_dependencies=True\ncustom_env.python.conda_dependencies=conda_dep\n\ncustom_env.register(workspace=ws)\n\n# create deployment config i.e. compute resources\naciconfig = AciWebservice.deploy_configuration(\n    cpu_cores=1,\n    memory_gb=1,\n    tags={&quot;experiment&quot;: experiment_name, &quot;model&quot;: model_name},\n)\n\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.model import Model\n\n# get the registered model\nmodel = Model(ws, model_name)\n\n# create an inference config i.e. the scoring script and environment\ninference_config = InferenceConfig(entry_script=&quot;score.py&quot;, environment=custom_env)\n\n# deploy the service\nservice = Model.deploy(\n    workspace=ws,\n    name=service_name,\n    models=[model],\n    inference_config=inference_config,\n    deployment_config=aciconfig,\n)\n\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n<p>I get the following log :<\/p>\n<pre><code>\nAzureML image information: tensorflow-2.4-ubuntu18.04-py37-cpu-inference:20220110.v1\n\n\nPATH environment variable: \/opt\/miniconda\/envs\/amlenv\/bin:\/opt\/miniconda\/bin:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\nPYTHONPATH environment variable: \n\nPip Dependencies\n---------------\nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n2022-01-24T10:21:09,855130300+00:00 - iot-server\/finish 1 0\n2022-01-24T10:21:09,856870100+00:00 - Exit code 1 is normal. Not restarting iot-server.\nabsl-py==0.15.0\napplicationinsights==0.11.10\nastunparse==1.6.3\nazureml-inference-server-http==0.4.2\ncachetools==4.2.4\ncertifi==2021.10.8\ncharset-normalizer==2.0.10\nclick==8.0.3\nFlask==1.0.3\nflatbuffers==1.12\ngast==0.3.3\ngoogle-auth==2.3.3\ngoogle-auth-oauthlib==0.4.6\ngoogle-pasta==0.2.0\ngrpcio==1.32.0\ngunicorn==20.1.0\nh5py==2.10.0\nidna==3.3\nimportlib-metadata==4.10.0\ninference-schema==1.3.0\nitsdangerous==2.0.1\nJinja2==3.0.3\nKeras-Preprocessing==1.1.2\nMarkdown==3.3.6\nMarkupSafe==2.0.1\nnumpy==1.19.5\noauthlib==3.1.1\nopt-einsum==3.3.0\npandas==1.1.5\nprotobuf==3.19.1\npyasn1==0.4.8\npyasn1-modules==0.2.8\npython-dateutil==2.8.2\npytz==2021.3\nrequests==2.27.1\nrequests-oauthlib==1.3.0\nrsa==4.8\nsix==1.15.0\ntensorboard==2.7.0\ntensorboard-data-server==0.6.1\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.4.0\ntensorflow-estimator==2.4.0\ntermcolor==1.1.0\ntyping-extensions==3.7.4.3\nurllib3==1.26.8\nWerkzeug==2.0.2\nwrapt==1.12.1\nzipp==3.7.0\n\n\nEntry script directory: \/var\/azureml-app\/.\n\nDynamic Python package installation is disabled.\nStarting AzureML Inference Server HTTP.\n\nAzure ML Inferencing HTTP server v0.4.2\n\n\nServer Settings\n---------------\nEntry Script Name: score.py\nModel Directory: \/var\/azureml-app\/azureml-models\/my-model\/1\nWorker Count: 1\nWorker Timeout (seconds): 300\nServer Port: 31311\nApplication Insights Enabled: false\nApplication Insights Key: None\n\n\nServer Routes\n---------------\nLiveness Probe: GET   127.0.0.1:31311\/\nScore:          POST  127.0.0.1:31311\/score\n\nStarting gunicorn 20.1.0\nListening at: http:\/\/0.0.0.0:31311 (69)\nUsing worker: sync\nBooting worker with pid: 100\nException in worker process\nTraceback (most recent call last):\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 589, in spawn_worker\n    worker.init_process()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 134, in init_process\n    self.load_wsgi()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 146, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in load\n    return self.load_wsgiapp()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 48, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 359, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/azureml_inference_server_http\/server\/entry.py&quot;, line 1, in &lt;module&gt;\n    import create_app\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/azureml_inference_server_http\/server\/create_app.py&quot;, line 4, in &lt;module&gt;\n    from routes_common import main\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/azureml_inference_server_http\/server\/routes_common.py&quot;, line 32, in &lt;module&gt;\n    from aml_blueprint import AMLBlueprint\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/azureml_inference_server_http\/server\/aml_blueprint.py&quot;, line 28, in &lt;module&gt;\n    main_module_spec.loader.exec_module(main)\n  File &quot;\/var\/azureml-app\/score.py&quot;, line 4, in &lt;module&gt;\n    import joblib\nModuleNotFoundError: No module named 'joblib'\nWorker exiting (pid: 100)\nShutting down: Master\nReason: Worker failed to boot.\n2022-01-24T10:21:13,851467800+00:00 - gunicorn\/finish 3 0\n2022-01-24T10:21:13,853259700+00:00 - Exit code 3 is not normal. Killing image.\n<\/code><\/pre>\n<h1>From a Conda specification<\/h1>\n<p>Same as before, but with a fresh environment from Conda specification and changing the <code>env_version<\/code> number :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># ...\n\n\nenv_version=&quot;2&quot;\n\n# ...\n\ncustom_env = Environment.from_conda_specification(name=env_name, file_path=&quot;my-env.yml&quot;)\ncustom_env.docker.base_image = DEFAULT_CPU_IMAGE\n\n# ...\n\n<\/code><\/pre>\n<p>with <code>my-env.yml<\/code> :<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>name: my-env\ndependencies:\n- python\n\n- pip:\n  - azureml-defaults\n  - azureml-sdk\n  - sklearn\n  - numpy\n  - matplotlib\n  - joblib\n  - uuid\n  - requests\n  - tensorflow\n\n<\/code><\/pre>\n<p>I get this log :<\/p>\n<pre><code>2022-01-24T11:06:54,887886931+00:00 - iot-server\/run \n2022-01-24T11:06:54,891839877+00:00 - rsyslog\/run \n2022-01-24T11:06:54,893640998+00:00 - gunicorn\/run \n2022-01-24T11:06:54,912032812+00:00 - nginx\/run \nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n2022-01-24T11:06:55,398420960+00:00 - iot-server\/finish 1 0\n2022-01-24T11:06:55,414425146+00:00 - Exit code 1 is normal. Not restarting iot-server.\n\nPATH environment variable: \/opt\/miniconda\/bin:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\nPYTHONPATH environment variable: \n\nPip Dependencies\n---------------\nbrotlipy==0.7.0\ncertifi==2020.6.20\ncffi @ file:\/\/\/tmp\/build\/80754af9\/cffi_1605538037615\/work\nchardet @ file:\/\/\/tmp\/build\/80754af9\/chardet_1605303159953\/work\nconda==4.9.2\nconda-package-handling @ file:\/\/\/tmp\/build\/80754af9\/conda-package-handling_1603018138503\/work\ncryptography @ file:\/\/\/tmp\/build\/80754af9\/cryptography_1605544449973\/work\nidna @ file:\/\/\/tmp\/build\/80754af9\/idna_1593446292537\/work\npycosat==0.6.3\npycparser @ file:\/\/\/tmp\/build\/80754af9\/pycparser_1594388511720\/work\npyOpenSSL @ file:\/\/\/tmp\/build\/80754af9\/pyopenssl_1605545627475\/work\nPySocks @ file:\/\/\/tmp\/build\/80754af9\/pysocks_1594394576006\/work\nrequests @ file:\/\/\/tmp\/build\/80754af9\/requests_1592841827918\/work\nruamel-yaml==0.15.87\nsix @ file:\/\/\/tmp\/build\/80754af9\/six_1605205313296\/work\ntqdm @ file:\/\/\/tmp\/build\/80754af9\/tqdm_1605303662894\/work\nurllib3 @ file:\/\/\/tmp\/build\/80754af9\/urllib3_1603305693037\/work\n\nStarting HTTP server\n2022-01-24T11:06:59,701365128+00:00 - gunicorn\/finish 127 0\n.\/run: line 127: exec: gunicorn: not found\n2022-01-24T11:06:59,706177784+00:00 - Exit code 127 is not normal. Killing image.\n    \n<\/code><\/pre>\n<p>I really don't know what I'm missing, and I've been searching for too long already (Azure docs, SO, ...).<\/p>\n<p>Thanks for your help !<\/p>\n<p>Edit : Non-exhaustive list of solutions I tried :<\/p>\n<ul>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/65159778\/how-to-create-azureml-environement-and-add-required-packages\">How to create AzureML environement and add required packages<\/a><\/li>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/65159308\/how-to-use-existing-conda-environment-as-a-azureml-environment\">how to use existing conda environment as a AzureML environment<\/a><\/li>\n<li>...<\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-environments#environment-building-caching-and-reuse\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-environments#environment-building-caching-and-reuse<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments#add-packages-to-an-environment\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments#add-packages-to-an-environment<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-inferencing-gpus\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-inferencing-gpus<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python#define-a-deployment-configuration\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python#define-a-deployment-configuration<\/a><\/li>\n<li>...<\/li>\n<\/ul>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643025448957,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1643163454172,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70833499",
        "Challenge_link_count":13,
        "Challenge_participation_count":3,
        "Challenge_readability":19.5,
        "Challenge_reading_time":162.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":94,
        "Challenge_solved_time":null,
        "Challenge_title":"AzureML Environment for Inference : can't add pip packages to dependencies",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":902.0,
        "Challenge_word_count":789,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1337868050812,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Paris, France",
        "Poster_reputation_count":483.0,
        "Poster_view_count":105.0,
        "Solution_body":"<p>OK, I got it working : I started over from scratch and it worked.<\/p>\n<p>I have no idea what was wrong in all my preceding tries, and that is terrible.<\/p>\n<p>Multiple problems and how I (think I) solved them :<\/p>\n<ul>\n<li><code>joblib<\/code> : I actually didn't need it to load my Keras model. But the problem was not with this specific library, rather that I couldn't add dependencies to the inference environment.<\/li>\n<li><code>Environment<\/code> : finally, I was only able to make things work with a custom env : <code>Environment.from_conda_specification(name=version, file_path=&quot;conda_dependencies.yml&quot;)<\/code> . I haven't been able to add my libraries (or specify a specific package version) to a &quot;currated environment&quot;. I don't know why though...<\/li>\n<li><code>TensorFlow<\/code> : last problem I had was that I trained and registered my model in AzureML Notebook's <code>azureml_py38_PT_TF<\/code> kernel (<code>tensorflow==2.7.0<\/code>), and tried to load it in the inference Docker image (<code>tensorflow==2.4.0<\/code>). So I had to specify the version of TensorFlow I wanted to use in the inference image (which required the previous point to be solved).<\/li>\n<\/ul>\n<p>What finally worked :<\/p>\n<ul>\n<li>notebook.ipynb<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>import uuid\nfrom azureml.core import Workspace, Environment, Model\nfrom azureml.core.webservice import AciWebservice\nfrom azureml.core.model import InferenceConfig\n\n\nversion = &quot;test-&quot;+str(uuid.uuid4())[:8]\n\nenv = Environment.from_conda_specification(name=version, file_path=&quot;conda_dependencies.yml&quot;)\ninference_config = InferenceConfig(entry_script=&quot;score.py&quot;, environment=env)\n\nws = Workspace.from_config()\nmodel = Model(ws, model_name)\n\naci_config = AciWebservice.deploy_configuration(\n    cpu_cores=1,\n    memory_gb=1,\n)\n\nservice = Model.deploy(\n    workspace=ws,\n    name=version,\n    models=[model],\n    inference_config=inference_config,\n    deployment_config=aci_config,\n    overwrite=True,\n)\n\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n<ul>\n<li>conda_dependencies.yml<\/li>\n<\/ul>\n<pre class=\"lang-yaml prettyprint-override\"><code>channels:\n- conda-forge\ndependencies:\n- python=3.8\n- pip:\n  - azureml-defaults\n  - azureml-sdk\n  - numpy\n  - tensorflow==2.7.0\n\n<\/code><\/pre>\n<ul>\n<li>score.py<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nimport numpy as np\nimport tensorflow as tf\n\n\ndef init():\n    global model\n\n    model_path = os.path.join(os.getenv(&quot;AZUREML_MODEL_DIR&quot;), &quot;model\/data\/model&quot;)\n    model = tf.keras.models.load_model(model_path)\n\n\n\ndef run(raw_data):\n    data = np.array(json.loads(raw_data)[&quot;data&quot;])\n    y_hat = model.predict(data)\n\n    return y_hat.tolist()\n\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.8,
        "Solution_reading_time":35.99,
        "Solution_score_count":1.0,
        "Solution_sentence_count":30.0,
        "Solution_word_count":267.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1472967821507,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tokyo, Japan",
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Similar to the issue of <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/200\" rel=\"nofollow noreferrer\">The trained model can be deployed on the other platform without dependency of sagemaker or aws service?<\/a>.<\/p>\n\n<p>I have trained a model on AWS SageMaker by using the built-in algorithm <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/semantic-segmentation.html\" rel=\"nofollow noreferrer\">Semantic Segmentation<\/a>. This trained model named as <code>model.tar.gz<\/code> is stored on S3. So I want to download this file from S3 and then use it to make inference on my local PC without using AWS SageMaker anymore. Since the built-in algorithm Semantic Segmentation is built using the <a href=\"https:\/\/github.com\/dmlc\/gluon-cv\" rel=\"nofollow noreferrer\">MXNet Gluon framework and the Gluon CV toolkit<\/a>, so I try to refer the documentation of <a href=\"https:\/\/mxnet.apache.org\/\" rel=\"nofollow noreferrer\">mxnet<\/a> and <a href=\"https:\/\/gluon-cv.mxnet.io\/\" rel=\"nofollow noreferrer\">gluon-cv<\/a> to make inference on local PC.<\/p>\n\n<p>It's easy to download this file from S3, and then I unzip this file to get three files:<\/p>\n\n<ol>\n<li><strong>hyperparams.json<\/strong>: includes the parameters for network architecture, data inputs, and training. Refer to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/segmentation-hyperparameters.html\" rel=\"nofollow noreferrer\">Semantic Segmentation Hyperparameters<\/a>.<\/li>\n<li><strong>model_algo-1<\/strong><\/li>\n<li><strong>model_best.params<\/strong><\/li>\n<\/ol>\n\n<p>Both <strong>model_algo-1<\/strong> and <strong>model_best.params<\/strong> are the trained models, and I think it's the output from <code>net.save_parameters<\/code> (Refer to <a href=\"https:\/\/beta.mxnet.io\/guide\/getting-started\/crash-course\/4-train.html\" rel=\"nofollow noreferrer\">Train the neural network<\/a>). I can also load them with the function <code>mxnet.ndarray.load<\/code>.<\/p>\n\n<p>Refer to <a href=\"https:\/\/beta.mxnet.io\/guide\/getting-started\/crash-course\/5-predict.html\" rel=\"nofollow noreferrer\">Predict with a pre-trained model<\/a>. I found there are two necessary things:<\/p>\n\n<ol>\n<li>Reconstruct the network for making inference.<\/li>\n<li>Load the trained parameters.<\/li>\n<\/ol>\n\n<p>As for reconstructing the network for making inference, since I have used PSPNet from training, so I can use the class <code>gluoncv.model_zoo.PSPNet<\/code> to reconstruct the network. And I know how to use some services of AWS SageMaker, for example batch transform jobs, to make inference. I want to reproduce it on my local PC. If I use the class <code>gluoncv.model_zoo.PSPNet<\/code> to reconstruct the network, I can't make sure whether the parameters for this network are same those used on AWS SageMaker while making inference. Because I can't see the image <code>501404015308.dkr.ecr.ap-northeast-1.amazonaws.com\/semantic-segmentation:latest<\/code> in detail. <\/p>\n\n<p>As for loading the trained parameters, I can use the <code>load_parameters<\/code>. But as for <strong>model_algo-1<\/strong> and <strong>model_best.params<\/strong>, I don't know which one I should use.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582681249783,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1582681944470,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60405600",
        "Challenge_link_count":8,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":42.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":37,
        "Challenge_solved_time":null,
        "Challenge_title":"How to make inference on local PC with the model trained on AWS SageMaker by using the built-in algorithm Semantic Segmentation?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":351.0,
        "Challenge_word_count":365,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1472967821507,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tokyo, Japan",
        "Poster_reputation_count":61.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>The following code works well for me.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import mxnet as mx\nfrom mxnet import image\nfrom gluoncv.data.transforms.presets.segmentation import test_transform\nimport gluoncv\n\n# use cpu\nctx = mx.cpu(0)\n\n# load test image\nimg = image.imread('.\/img\/IMG_4015.jpg')\nimg = test_transform(img, ctx)\nimg = img.astype('float32')\n\n# reconstruct the PSP network model\nmodel = gluoncv.model_zoo.PSPNet(2)\n\n# load the trained model\nmodel.load_parameters('.\/model\/model_algo-1')\n\n# make inference\noutput = model.predict(img)\npredict = mx.nd.squeeze(mx.nd.argmax(output, 1)).asnumpy()\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.0,
        "Solution_reading_time":8.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":57.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416193017423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Gensokyo",
        "Answerer_reputation_count":880.0,
        "Answerer_view_count":111.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have several million images in my training folder and want to specify a subset of them for training - the way to do this seems to be with a manifest file as described here.<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/augmented-manifest.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/augmented-manifest.html<\/a><\/p>\n\n<p>But this seems to be geared towards labelled data. How can I start a sagemaker training job using sagemaker's Tensorflow <code>estimator.fit<\/code> with a list of files instead of the entire directory as input?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570561431850,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58292566",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":8.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I use a list of files as the training set on Sagemaker with Tensorflow?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":809.0,
        "Challenge_word_count":86,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416193017423,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Gensokyo",
        "Poster_reputation_count":880.0,
        "Poster_view_count":111.0,
        "Solution_body":"<p>You can use an input type pipe parameter like so: <\/p>\n\n<pre><code>hyperparameters = {'save_checkpoints_secs':None,\n                   'save_checkpoints_steps':1000}\n\ntf_estimator = TensorFlow(entry_point='.\/my-training-file', role=role,\n                          training_steps=5100, evaluation_steps=100,\n                          train_instance_count=1, train_instance_type='ml.p3.2xlarge',\n                          input_mode = 'Pipe',\n                          train_volume_size=300, output_path = 's3:\/\/sagemaker-pocs\/test-carlsoa\/kepler\/model',\n                          framework_version = '1.12.0', hyperparameters=hyperparameters, checkpoint_path = None)\n<\/code><\/pre>\n\n<p>And create the manifest file pipe as an input:<\/p>\n\n<pre><code>train_data = sagemaker.session.s3_input('s3:\/\/sagemaker-pocs\/test-carlsoa\/manifest.json',\n                                        distribution='FullyReplicated',\n                                        content_type='image\/jpeg',\n                                        s3_data_type='ManifestFile',\n                                        attribute_names=['source-ref']) \n                                        #attribute_names=['source-ref', 'annotations']) \ndata_channels = {'train': train_data}\n<\/code><\/pre>\n\n<p>Note that you can use ManifestFile or AugmentedManifestFile depending on whether you have extra data or labels to provide. Now you can use data_channels as the input to the tf estimator:<\/p>\n\n<p><code>tf_estimator.fit(inputs=data_channels, logs=True)<\/code><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":23.0,
        "Solution_reading_time":15.82,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":86.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Logging using Weights and Biases does not differentiate between training and testing modes in `logbook.write_metric_log({  'mode': 'train' ... })`",
        "Challenge_closed_time":1583456.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583449225000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/shagunsodhani\/ml-logger\/issues\/25",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":2.73,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":88.0,
        "Challenge_repo_star_count":17.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] Weights & Biases logging does not differentiate between modes",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":25,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@koustuvsinha Thanks for bringing this up. Could you try the new version?\r\n\r\nWhen constructing the logbook, pass an additional parameter:\r\n\r\n```\r\nfrom ml_logger import logbook as ml_logbook\r\nlogbook_config = ml_logbook.make_config(\r\n    logger_file_path = <path to write logs>,\r\n    wandb_config = <wandb config or None>,\r\n    wandb_prefix_key = \"mode\",\r\n)\r\n```",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":4.26,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":40.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1464391892936,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Rio de Janeiro, State of Rio de Janeiro, Brazil",
        "Answerer_reputation_count":2243.0,
        "Answerer_view_count":148.0,
        "Challenge_adjusted_solved_time":48.6672925,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained a credit-fraud data set on AWS Sagemaker and created an endpoint of the model. Suppose I want to provide it as a service to my friend. He has some credit data and wanted to know whether the transaction is fraud or not. He wishes to use my endpoint. How do I share it?<\/p>\n\n<ol>\n<li>Should I share my ARN for endpoint? I don't think its the right way. without a common account he won't be able to use it.<\/li>\n<li>Or is there another way<\/li>\n<\/ol>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1573479066647,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58802366",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.6,
        "Challenge_reading_time":6.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Deploying the sagemaker endpoint created as a service",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":226.0,
        "Challenge_word_count":95,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1568318861627,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hyderabad, Telangana, India",
        "Poster_reputation_count":486.0,
        "Poster_view_count":75.0,
        "Solution_body":"<p>To share your model as an endpoint, you should use lambda and API Gateway to create your API.<\/p>\n\n<ol>\n<li>Create an API gateway that triggers a Lambda with the HTTP POST method;<\/li>\n<li>your lambda should instantiate the SageMaker endpoint, get the requested parameter in the event, call the SageMaker endpoint and return the predicted value. you can also create a DynamoDB to store commonly requested parameters with their answers;<\/li>\n<li>Send the API Gateway Endpoint to your friend.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/qLss4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qLss4.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1573654268900,
        "Solution_link_count":2.0,
        "Solution_readability":10.1,
        "Solution_reading_time":8.56,
        "Solution_score_count":6.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":87.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1250158552416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Challenge_adjusted_solved_time":40.8553036111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have successfully trained a model on Azure Machine Learning Service using Hyperdrive that has now yielded a hyperdrive run instance<\/p>\n\n<pre><code>hyperdrive_run = exp.submit(config=hypertune_config)\nhyperdrive_run\nbest_run = hyperdrive_run.get_best_run_by_primary_metric()\n<\/code><\/pre>\n\n<p>As a next step, I would like to register a model while adding a description to the model.:<\/p>\n\n<pre><code>pumps_rf = best_run.register_model(model_name='pumps_rf', model_path='outputs\/rf.pkl')\n<\/code><\/pre>\n\n<p>There is a <code>description<\/code> column in the Models section of my AML Workspace on Azure portal but the <code>register_model<\/code> method does not seem to have a <code>description<\/code> flag. So how do I go about adding a description to the model so I see it in Azure Portal?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1550539380057,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54757598",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.7,
        "Challenge_reading_time":11.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Add model description when registering model after hyperdrive successful run",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":453.0,
        "Challenge_word_count":105,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408574571227,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, Canada",
        "Poster_reputation_count":2754.0,
        "Poster_view_count":124.0,
        "Solution_body":"<p>Good question :).<\/p>\n\n<p>Looking at the current version of the API, it doesn't look like you can add the description using <code>Run.register_model<\/code>, as confirmed <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#register-model-model-name--model-path-none--tags-none--properties-none----kwargs-\" rel=\"nofollow noreferrer\">by the docs<\/a>. <\/p>\n\n<p>You can go around this however by registering the model using the <code>Model.register<\/code> method which, fortunately, includes an argument for <code>description<\/code> as detailed <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none-\" rel=\"nofollow noreferrer\">here<\/a>. In your case, you also need to <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run(class)?view=azure-ml-py#download-file-name--output-file-path-none-\" rel=\"nofollow noreferrer\">download the files<\/a> first.<\/p>\n\n<p>In short, use something like:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>best_run.download_file('outputs\/rf.pkl', output_file_path='.\/rf.pkl')\n\nModel.register(workspace=ws, model_path='.\/rf.pkl', model_name=\"pumps_rf\", description=\"There are many models like it, but this one is mine.\")\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1550686459150,
        "Solution_link_count":3.0,
        "Solution_readability":21.2,
        "Solution_reading_time":18.95,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":96.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1480786532470,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Nagpur, Sitabuldi, Nagpur, Maharashtra, India",
        "Answerer_reputation_count":111.0,
        "Answerer_view_count":44.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>This is the first time I am using amazon web services to deploy my machine learning pre-trained model. I want to deploy my pre-trained TensorFlow model to Aws-Sagemaker. I am somehow able to deploy the endpoints successfully But whenever I call the <code>predictor.predict(some_data)<\/code> method to make prediction to invoking the endpoints it's throwing an error.<\/p>\n\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"\". See https:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/sagemaker-tensorflow-2020-04-07-04-25-27-055 in account 453101909370 for more information.\n<\/code><\/pre>\n\n<p>After going through the cloud watch logs I found this error.<\/p>\n\n<pre><code>#011details = \"NodeDef mentions attr 'explicit_paddings' not in Op&lt;name=Conv2D; signature=input:T, filter:T -&gt; output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]; attr=dilations:list(int),default=[1, 1, 1, 1]&gt;; NodeDef: {{node conv1_conv\/convolution}} = Conv2D[T=DT_FLOAT, _output_shapes=[[?,112,112,64]], data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"\/job:localhost\/replica:0\/task:0\/device:CPU:0\"](conv1_pad\/Pad, conv1_conv\/kernel\/read). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\n<\/code><\/pre>\n\n<p>I don't know where I am wrong and I have wasted 2 days already to solve this error and couldn't find out the information regarding this. The detailed logs I have shared <a href=\"https:\/\/docs.google.com\/document\/d\/1NXsLRd6cfbNE55xSVq5d63ETt-cBmwZsOaic8IyF1Qw\/edit?usp=sharing\" rel=\"nofollow noreferrer\">here<\/a>. <\/p>\n\n<p>Tensorflow version of my notebook instance is 1.15<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586244384423,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1586255907796,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61074798",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":15.9,
        "Challenge_reading_time":29.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"Deploy pre-trained tensorflow model on the aws sagemaker - ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":740.0,
        "Challenge_word_count":214,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1480786532470,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Nagpur, Sitabuldi, Nagpur, Maharashtra, India",
        "Poster_reputation_count":111.0,
        "Poster_view_count":44.0,
        "Solution_body":"<p>After a lot of searching and try &amp; error, I was able to solve this problem. In many cases, the problem arises because of the TensorFlow and Python versions.<\/p>\n<p><strong>Cause of the problem:<\/strong>\nTo deploy the endpoints, I was using the <code>TensorflowModel<\/code> on TF 1.12 and python 3 and which exactly caused the problem.<\/p>\n<blockquote>\n<pre><code>sagemaker_model = TensorFlowModel(model_data = model_data,\n                                  role = role,\n                                  framework_version = '1.12',\n                                  entry_point = 'train.py')\n<\/code><\/pre>\n<\/blockquote>\n<p>Apparently, <code>TensorFlowModel<\/code> only allows python 2 on TF version 1.11, 1.12. 2.1.0.<\/p>\n<p><strong>How I fixed it:<\/strong> There are two TensorFlow solutions that handle serving in the Python SDK. They have different class representations and documentation as shown here.<\/p>\n<ol>\n<li><strong>TensorFlowModel<\/strong> - <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/model.py#L47\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/model.py#L47<\/a><\/li>\n<\/ol>\n<ul>\n<li>Doc:\n<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/tree\/v1.12.0\/src\/sagemaker\/tensorflow#deploying-directly-from-model-artifacts\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/tree\/v1.12.0\/src\/sagemaker\/tensorflow#deploying-directly-from-model-artifacts<\/a><\/li>\n<li>Key difference: Uses a proxy GRPC client to send requests<\/li>\n<li>Container impl:\n<a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/blob\/master\/src\/tf_container\/serve.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/blob\/master\/src\/tf_container\/serve.py<\/a><\/li>\n<\/ul>\n<ol start=\"2\">\n<li><strong>Model<\/strong> - <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/serving.py#L96\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/serving.py#L96<\/a><\/li>\n<\/ol>\n<ul>\n<li>Doc: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst<\/a><\/li>\n<li>Key difference: Utilizes the TensorFlow serving rest API<\/li>\n<li>Container impl: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/blob\/master\/container\/sagemaker\/serve.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/blob\/master\/container\/sagemaker\/serve.py<\/a><\/li>\n<\/ul>\n<p>Python 3 isn't supported using the <code>TensorFlowModel<\/code> object, as the container uses the TensorFlow serving API library in conjunction with the GRPC client to handle making inferences, however, the TensorFlow serving API isn't supported in Python 3 officially, so there are only Python 2 versions of the containers when using the <code>TensorFlowModel<\/code> object.\nIf you need Python 3 then you will need to use the <code>Model<\/code> object defined in #2 above.<\/p>\n<p>Finally, I used the <code>Model<\/code> with the TensorFlow version 1.15.1.<\/p>\n<blockquote>\n<pre><code>sagemaker_model = Model(model_data = model_data,\n                        role = role,\n                        framework_version='1.15.2',\n                        entry_point = 'train.py')\n<\/code><\/pre>\n<\/blockquote>\n<p>Also, here are the successful results.\n<a href=\"https:\/\/i.stack.imgur.com\/OMsEf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/OMsEf.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":14.0,
        "Solution_readability":21.1,
        "Solution_reading_time":48.42,
        "Solution_score_count":4.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":271.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is AWS Sage Maker Auto Pilot suitable for NLP?<\/p>\n\n<p>We currently have a tensorflow model that does classification on input of a sequence of URLS (\nWe transform the URLs to Word vec and Char vec to feed it to the model).<\/p>\n\n<p>Looking at Sage Maker Auto Pilot documentation it says that it works on input in tabular form.\nI was wondering if we could use it to for our use case.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578226347093,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59599721",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":5.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Is AWS Sage Maker Auto Pilot suitable for NLP?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":261.0,
        "Challenge_word_count":79,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458550179920,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":383.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>No. SageMaker AutoPilot doesn't support deep learning at the moment, only classification and regression problems on tabular data. Technically, I guess you could pass embeddings in CSV format, and pray that XGBoost figures them out, but I seriously doubt that this would deliver meaningful results :)<\/p>\n\n<p>Amazon Comprehend does support fully managed custom classification models <a href=\"https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/how-document-classification.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/how-document-classification.html<\/a>. It may be worth taking a look at it.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":19.4,
        "Solution_reading_time":8.26,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":68.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hello,  <\/p>\n<p>I have made an experiment in Azure ML with a coefficient of detemination reaching 86% (regression). I would like to improve it using the rounding of several features (columns). I would like to round some columns to &quot;xx-ten&quot; example: 1854 =&gt; 1850 (up and down if possible)  <br \/>\nI have used ceil functions before  to avoid decimal numbers but here this is another case. I cannot see how to do this.  <\/p>\n<p>Can anyone help in this please?  <\/p>\n<p>Kind Regards,  <\/p>\n<p>Mohamed.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610454807547,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/227626\/round-columns-in-azure-ml",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.9,
        "Challenge_reading_time":6.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Round columns in Azure ML",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":89,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello Dear,  <\/p>\n<p>Thank you for your advice. I have found in the rounding area the &quot;tomultiple&quot; function where I can decide to round a number to tens, hundreds... (example: tomultiple(2583,10) =&gt; 2580).  <\/p>\n<p>Thank you :)  <\/p>\n<p>Mohamed.  <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.9,
        "Solution_reading_time":3.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1468179475927,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pasadena, CA, United States",
        "Answerer_reputation_count":795.0,
        "Answerer_view_count":210.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been following this <a href=\"https:\/\/gallery.azureml.net\/Experiment\/370c80490e774a6cb26edba69c583c9b\" rel=\"nofollow\">gallery sample<\/a> but I just can't seem to get batch execution to return multiple scores in one job.<\/p>\n\n<p>Everything works fine i.e. can deploy the prediction web API and request a single scoring. But whenever I send a batch execution job (using the <a href=\"https:\/\/studio.azureml.net\/apihelp\/workspaces\/9dbdce0846e64a5f9c925116e0cb6388\/webservices\/7df2a06ad50348d78f0e1cb81f3742ab\/endpoints\/87bfd4ea0615412bac19c34422ced730\/jobs\" rel=\"nofollow\">sample C# codes<\/a>) with more than one request e.g.:<\/p>\n\n<pre><code>ID1,ID2\n1,2\n3,1\n5,1\n<\/code><\/pre>\n\n<p>Azure ML will only return the prediction scores for the first request <code>1,2<\/code> but not for the other rows.<\/p>\n\n<p>I'm not sure where I'm doing wrong but I should be expecting results for all three requests. Any help would be appreciated!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1437353901330,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1437413945292,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/31507547",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":8.9,
        "Challenge_reading_time":12.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning - batch execution partially working",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":269.0,
        "Challenge_word_count":113,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1316278453427,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":1365.0,
        "Poster_view_count":310.0,
        "Solution_body":"<p>It looks like you've chosen an unfortunate example: the custom scripts in the Retail Forecasting web service explicitly drop all but the first ID pair. To see this, try loading the \"Retail Forecasting: Step 6A of 6\" experiment and check out the code in the \"Create a complete time series. Add future time stamps\" module. You will find the following:<\/p>\n\n<pre><code>all.time &lt;- data.frame(ID1 = data$ID1[1], ID2 = data$ID2[1], time = all.time)\ndata &lt;- join(all.time, data, by = c(\"ID1\", \"ID2\", \"time\"), type = \"left\")\nmaml.mapOutputPort(\"data\");\n<\/code><\/pre>\n\n<p>The left join statement will ignore any rows where data$ID1 != data$ID1[1]  and data$ID2 != data$ID2[1]. That is why you are losing everything but the first ID pair.<\/p>\n\n<p>It appears batch prediction for multiple ID pairs in a single job was not a use case that the custom script authors envisioned for their web service. If you are proficient in R and particularly interested in this use case, you could modify the scripts in this experiment to support processing multiple time series concurrently. Otherwise, you might want to simply try another example experiment.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.9,
        "Solution_reading_time":14.18,
        "Solution_score_count":2.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":171.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1402755934688,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Porto, Portugal",
        "Answerer_reputation_count":5998.0,
        "Answerer_view_count":426.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm deploying a <code>tensorflow.serving<\/code> endpoint with a custom <code>inference.py<\/code> script via the <code>entry point<\/code> parameter<\/p>\n<pre><code>model = Model(role='xxx',\n              framework_version='2.2.0',\n              entry_point='inference.py',\n              model_data='xxx')\n\npredictor = model.deploy(instance_type='xxx',\n                         initial_instance_count=1,\n                         endpoint_name='xxx')\n<\/code><\/pre>\n<p>inference.py constains an <code>input_handler<\/code> and an <code>output_handler<\/code> functions, but when i call predict with:<\/p>\n<pre><code>model = Predictor(endpoint_name='xxx')\nurl = 'xxx'\n\ninput = {\n    'instances': [url]\n}\n\npredictions = model.predict(input)\n<\/code><\/pre>\n<p>I'm getting the following <code>error<\/code>:<\/p>\n<p><em>botocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message &quot;{&quot;error&quot;: &quot;Failed to process element: 0 of 'instances' list. Error: Invalid argument: JSON Value: &quot;xxx&quot; Type: String is not of expected type: float&quot; }&quot;<\/em><\/p>\n<p>It seems the function is never calling the <code>input_handler<\/code> function in inference.py script. Do you know why this might be happening?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1595851293460,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1595931884943,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63114905",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":13.8,
        "Challenge_reading_time":17.35,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker tensorflow endpoint not calling the input_handler when being invoked for a prediction",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":474.0,
        "Challenge_word_count":129,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1402755934688,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Porto, Portugal",
        "Poster_reputation_count":5998.0,
        "Poster_view_count":426.0,
        "Solution_body":"<p>Found the problem thanks to AWS support:<\/p>\n<p>I was creating an endpoint that already had an endpoint configuration with the same name and the new configuration wasn't being utilized.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.6,
        "Solution_reading_time":2.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":29.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1521189557296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":160.0,
        "Answerer_view_count":27.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>We have created an experiment in Azure ML Studio to predict some scheduling activities based on the system data and user data. System data consists of the CPU time, Heap Usage and other system parameters while user data has active sessions of the user and some user-specific data.\nOur experiment is working fine and returning the results quite similar to what we are expecting, but we are struggling with the following:-<\/p>\n\n<p>1) Our experiment is not considering the updated data for training its models.<\/p>\n\n<p>2) Every time we are required to upload the data and retrain the models manually.<\/p>\n\n<p>I wonder if it is really possible to feed in live data to the azure experiments using some web-services or by using Azure DB. We are trying to update the data in CSV file that we have created in Azure storage. That probably would solve our 1st query.<\/p>\n\n<p>Now, this updated data should be considered to train the model periodically automatically.<\/p>\n\n<p>It would be great if someone could help us out with it?<\/p>\n\n<p>Note: We are using our model using the web services created with the help of Azure studio.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1550724960020,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1550942353507,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54799512",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.5,
        "Challenge_reading_time":14.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Retrain the classification model automatically based on updated data set",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":772.0,
        "Challenge_word_count":198,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Step 1 : Create 2 web services with Azure ML Studio ( One for the training model and one for the predictive model)<\/p>\n\n<p>Step 2: Create endpoint through the web service with the link Manage Endpoint on Azure ML Studio for each web service<\/p>\n\n<p>Step 3: Create 2 new connections on Azure Data Factory \/ Find Azure ML (on compute tab) and copy the Endpoint key and API Key that you will find under the Consume tab in the endpoint configuration (the one that you created on step 2) Endpoint Key = Batch Requests Key and API Key = Primary Key <\/p>\n\n<p>Set Disable Update Resource for the training model endpoint\nSet Enable Update Resource for the predictive model endpoint ( Update Resource End Point = Patch key )<\/p>\n\n<p>Step 4 : Create a pipeline with 2 activities ( ML Batch Execution and ML Update Resource)\nSet the AML Linked service for the ML batch Execution with the connection that has  disable Update Resource<\/p>\n\n<p>Set the AML Linked service for the ML Update Resource with the connection that has  Enable Update Resource<\/p>\n\n<p>Step 5 : Set the Web Service Inputs and Outputs<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":73.1,
        "Solution_reading_time":13.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":182.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>If I have a trained model in Using pickle, or Joblib.\nLets say its Logistic regression or XGBoost.<\/p>\n<p>I would like to host that model in AWS Sagemaker as endpoint without running a training job.\nHow to achieve that.<\/p>\n<pre><code>#Lets Say myBucketName contains model.pkl\nmodel = joblib.load('filename.pkl')  \n# X_test = Numpy Array \nmodel.predict(X_test)  \n<\/code><\/pre>\n<p>I am not interested to <code>sklearn_estimator.fit('S3 Train, S3 Validate' )<\/code> , I have the trained model<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599661148083,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63813624",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":7.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Load a Picked or Joblib Pre trained ML Model to Sagemaker and host as endpoint",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1622.0,
        "Challenge_word_count":80,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1370509408500,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1573.0,
        "Poster_view_count":194.0,
        "Solution_body":"<p>For Scikit Learn for example, you can get inspiration from this public demo <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a><\/p>\n<p>Step 1: Save your artifact (eg the joblib) compressed in S3 at <code>s3:\/\/&lt;your path&gt;\/model.tar.gz<\/code><\/p>\n<p>Step 2: Create an inference script with the deserialization function <code>model_fn<\/code>. (Note that you could also add custom inference functions <code>input_fn<\/code>, <code>predict_fn<\/code>, <code>output_fn<\/code> but for scikit the defaults function work fine)<\/p>\n<pre><code>%%writefile inference_script.py. # Jupiter command to create file in case you're in Jupiter\n\nimport joblib\nimport os\n\ndef model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))\n    return clf\n<\/code><\/pre>\n<p>Step 3: Create a model associating the artifact with the right container<\/p>\n<pre><code>from sagemaker.sklearn.model import SKLearnModel\n\nmodel = SKLearnModel(\n    model_data='s3:\/\/&lt;your path&gt;\/model.tar.gz',\n    role='&lt;your role&gt;',\n    entry_point='inference_script.py',\n    framework_version='0.23-1')\n<\/code><\/pre>\n<p>Step 4: Deploy!<\/p>\n<pre><code>model.deploy(\n    instance_type='ml.c5.large',  # choose the right instance type\n    initial_instance_count=1)\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.5,
        "Solution_reading_time":20.38,
        "Solution_score_count":5.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":122.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have successfully built a Sagemaker endpoint using a Tensorflow model. The pre and post processing is done inside &quot;inference.py&quot; which calls a handler function based on this tutorial: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html#how-to-implement-the-pre-and-or-post-processing-handler-s\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html#how-to-implement-the-pre-and-or-post-processing-handler-s<\/a><\/p>\n<p>My questions are:<\/p>\n<ul>\n<li>Which method is good for validating user input data within inference.py?<\/li>\n<li>If such validation tests fail (e.g. wrong data types or data not in allowed range, etc.), how is it possible to return appropriate error messages with status codes to the user?<\/li>\n<li>How is this compatible with the API gateway placed above the endpoint?<\/li>\n<\/ul>\n<p>Here is the structure of the inference.py with the desired validation check as a comment:<\/p>\n<pre><code>import json\nimport requests\n\n\ndef handler(data, context):\n    &quot;&quot;&quot;Handle request.\n    Args:\n        data (obj): the request data\n        context (Context): an object containing request and configuration details\n    Returns:\n        (bytes, string): data to return to client, (optional) response content type\n    &quot;&quot;&quot;\n    processed_input = _process_input(data, context)\n    response = requests.post(context.rest_uri, data=processed_input)\n    return _process_output(response, context)\n\n\ndef _process_input(data, context):\n    if context.request_content_type == 'application\/json':\n        # pass through json (assumes it's correctly formed)\n        d = data.read().decode('utf-8')\n        data_dict = json.loads(data)\n\n\n        # -----&gt;   if data_dict['input_1'] &gt; 25000:\n        # -----&gt;       return some error specific message with status code 123\n\n\n        return some_preprocessing_function(data_dict)\n\n    raise ValueError('{{&quot;error&quot;: &quot;unsupported content type {}&quot;}}'.format(\n        context.request_content_type or &quot;unknown&quot;))\n\n\ndef _process_output(data, context):\n    if data.status_code != 200:\n        raise ValueError(data.content.decode('utf-8'))\n\n    response_content_type = context.accept_header\n    prediction = data.content\n    return prediction, response_content_type\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644929807890,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1644931522732,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71126832",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.6,
        "Challenge_reading_time":30.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":null,
        "Challenge_title":"Amazon Sagemaker: User Input data validation in Inference Endpoint",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":245.0,
        "Challenge_word_count":220,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1613661928947,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":160.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>I will answer your questions inline below:<\/p>\n<ol>\n<li><em>Which method is good for validating user input data within inference.py?<\/em><\/li>\n<\/ol>\n<p>Seeing that you have a <code>handler<\/code> function, <code>input_handler<\/code> and <code>output_handler<\/code> are ignored. Thus, inside your <code>handler<\/code> function (as you are correctly doing) you can have the validation logic.<\/p>\n<ol start=\"2\">\n<li><em>If such validation tests fail (e.g. wrong data types or data not in allowed range, etc.), how is it possible to return appropriate error messages with status codes to the user?<\/em><\/li>\n<\/ol>\n<p>I like to think of my SageMaker endpoint as a web server. Thus, you can return any valid HTTP response code with a response message. Please see this example <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_batch_transform\/tensorflow_cifar-10_with_inference_script\/code\/inference.py#L47\" rel=\"nofollow noreferrer\">inference.py<\/a> file that I found as a reference.<\/p>\n<pre><code>_return_error(\n            415, 'Unsupported content type &quot;{}&quot;'.format(context.request_content_type or &quot;Unknown&quot;)\n        )\n\ndef _return_error(code, message):\n    raise ValueError(&quot;Error: {}, {}&quot;.format(str(code), message))\n<\/code><\/pre>\n<ol start=\"3\">\n<li><em>How is this compatible with the API gateway placed above the endpoint?<\/em><\/li>\n<\/ol>\n<p>Please see this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">link<\/a> for details on Creating a machine learning-powered REST API with Amazon API Gateway mapping templates and Amazon SageMaker.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.5,
        "Solution_reading_time":22.68,
        "Solution_score_count":1.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":178.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1302799407107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Menorca",
        "Answerer_reputation_count":2618.0,
        "Answerer_view_count":337.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to apply sagemaker's kMeans algorithm to a sparse matrix, obtained with <code>TfidfVectorizer<\/code> from <code>sklearn<\/code>'s library.<\/p>\n\n<p>Ideally I would like to provide the input data to Sagemaker's kMeans implementation as sparse matrix <code>scipy.sparse.csr.csr_matrix<\/code>, but when I this (<code>kmeans.fit(kmeans.record_set(train_data))<\/code>) I get the following error:<\/p>\n\n<p><code>TypeError: must be real number, not csr_matrix<\/code><\/p>\n\n<p>Of course, if I pass a dense matrix the algorithm will work (<code>train_data.toarray()<\/code>) but the amount of memory it would need is ginormous. Any possible alternatives before I incur into using supersized amazon instances?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1544912532777,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1545067466160,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53797757",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":12.4,
        "Challenge_reading_time":10.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Amazon SageMaker kMeans won't take sparse matrix (csr_matrix) as input, any alternatives before using a dense matrix?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":315.0,
        "Challenge_word_count":101,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1302799407107,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Menorca",
        "Poster_reputation_count":2618.0,
        "Poster_view_count":337.0,
        "Solution_body":"<p>The key was in the SageMaker python SDK. There you can find a function that transforms a scipy sparse matrix to a sparse tensor (<code>write_spmatrix_to_sparse_tensor<\/code>).<\/p>\n\n<p>The complete code that solved the problem without having to incur into a dense matrix is the following:<\/p>\n\n<pre><code>from sagemaker.amazon.common import write_spmatrix_to_sparse_tensor\n\ntfidf_matrix = tfidf_vectorizer.fit_transform('your_train_data') # output: sparse scipy matrix\nsagemaker_bucket = 'your-bucket' \ndata_key = 'kmeans_lowlevel\/data'\ndata_location = f\"s3:\/\/{sagemaker_bucket}\/{data_key}\"\nbuf = io.BytesIO()\nwrite_spmatrix_to_sparse_tensor(buf, tfidf_matrix)\nbuf.seek(0)\nboto3.resource('s3').Bucket(sagemaker_bucket).Object(data_key).upload_fileobj(buf)\n<\/code><\/pre>\n\n<p>After doing this, in the <code>create_training_params<\/code> configuration you'll have to feed the S3Uri field with the data location you have provided to store the sparse matrix in S3:<\/p>\n\n<pre><code>create_training_params = \\\n{\n    ... # all other params\n\n    \"InputDataConfig\": [\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": data_location, # YOUR_DATA_LOCATION_GOES_HERE\n                    \"S3DataDistributionType\": \"FullyReplicated\"\n                }\n            },\n            \"CompressionType\": \"None\",\n            \"RecordWrapperType\": \"None\"\n        }\n    ]\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.2,
        "Solution_reading_time":17.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":115.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What are the types of Regression algorithm? Are there any kinds of regression called &quot;non-linear regression&quot;?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1608719172483,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/208685\/types-of-regression-algorithm",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.3,
        "Challenge_reading_time":2.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Types of Regression Algorithm",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":19,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=4bd5b942-e3cc-4aa2-8ef5-03d77bfd1721\">@Sanniddha Chakrabarti  <\/a> Please follow the below document for Regression.    <br \/>\n<a href=\"https:\/\/www.analyticsvidhya.com\/blog\/2015\/08\/comprehensive-guide-regression\/#:%7E:text=Regression%20analysis%20is%20a%20form,effect%20relationship%20between%20the%20variables\">https:\/\/www.analyticsvidhya.com\/blog\/2015\/08\/comprehensive-guide-regression\/#:~:text=Regression%20analysis%20is%20a%20form,effect%20relationship%20between%20the%20variables<\/a>.    <\/p>\n<p>Types of Regressions:    <br \/>\nLinear Regression    <br \/>\nLogistic Regression    <br \/>\nPolynomial Regression    <br \/>\nStepwise Regression    <br \/>\nRidge Regression    <br \/>\nLasso Regression    <br \/>\nElasticNet Regression    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":33.3,
        "Solution_reading_time":10.09,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":40.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1363541433296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Twin Cities, MN, USA",
        "Answerer_reputation_count":348.0,
        "Answerer_view_count":24.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying in Amazon Sagemaker to deploy an existing Scikit-Learn model. So a model that wasn't trained on SageMaker, but locally on my machine.<\/p>\n<p>On my local (windows) machine I've saved my model as model.joblib and tarred the model to model.tar.gz.<\/p>\n<p>Next, I've uploaded this model to my S3 bucket ('my_bucket') in the following path s3:\/\/my_bucket\/models\/model.tar.gz. I can see the tar file in S3.<\/p>\n<p>But when I'm trying to deploy the model, it keeps giving the error message &quot;Failed to extract model data archive&quot;.<\/p>\n<p>The .tar.gz is generated on my local machine by running 'tar -czf model.tar.gz model.joblib' in a powershell command window.<\/p>\n<p>The code for uploading to S3<\/p>\n<pre><code>import boto3\ns3 = boto3.client(&quot;s3&quot;, \n              region_name='eu-central-1', \n              aws_access_key_id=AWS_KEY_ID, \n              aws_secret_access_key=AWS_SECRET)\ns3.upload_file(Filename='model.tar.gz', Bucket=my_bucket, Key='models\/model.tar.gz')\n<\/code><\/pre>\n<p>The code for creating the estimator and deploying:<\/p>\n<pre><code>import boto3\nfrom sagemaker.sklearn.estimator import SKLearnModel\n\n...\n\nmodel_data = 's3:\/\/my_bucket\/models\/model.tar.gz'\nsklearn_model = SKLearnModel(model_data=model_data,\n                             role=role,\n                             entry_point=&quot;my-script.py&quot;,\n                             framework_version=&quot;0.23-1&quot;)\npredictor = sklearn_model.deploy(instance_type=&quot;ml.t2.medium&quot;, initial_instance_count=1)                             \n<\/code><\/pre>\n<p>The error message:<\/p>\n<blockquote>\n<p>error message: UnexpectedStatusException: Error hosting endpoint\nsagemaker-scikit-learn-2021-01-24-17-24-42-204: Failed. Reason: Failed\nto extract model data archive for container &quot;container_1&quot; from URL\n&quot;s3:\/\/my_bucket\/models\/model.tar.gz&quot;. Please ensure that the object\nlocated at the URL is a valid tar.gz archive<\/p>\n<\/blockquote>\n<p>Is there a way to see why the archive is invalid?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":5,
        "Challenge_created_time":1611565411923,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1611823707607,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65881699",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":11.7,
        "Challenge_reading_time":25.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker failed to extract model data archive tar.gz for container when deploying",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1859.0,
        "Challenge_word_count":208,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1472970520888,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Amersfoort, Nederland",
        "Poster_reputation_count":424.0,
        "Poster_view_count":21.0,
        "Solution_body":"<p>I had a similar issue as well, along with a similar fix to Bas (per comment above).<\/p>\n<p>I was finding I wasn't necessarily having issues with the .tar.gz step, this command does work fine:<\/p>\n<p><code>tar -czf &lt;filename&gt; .\/&lt;directory-with-files&gt;<\/code><\/p>\n<p>but rather with the uploading step.<\/p>\n<p>Manually uploading to S3 should take care of this, however, if you're doing this step programmatically, you might need to double check the steps taken. Bas appears to have had filename issues, mine were around using boto properly. Here's some code that works (Python only here, but watch for similar issues with other libraries):<\/p>\n<pre><code>bucket = 'bucket-name'\nkey = 'directory-inside-bucket'\nfile = 'the file name of the .tar.gz'\n\ns3_client = boto3.client('s3')\ns3_client.upload_file(file, bucket, key)\n<\/code><\/pre>\n<p>Docs: <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.9,
        "Solution_reading_time":14.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":120.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1589738451347,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":179.0,
        "Answerer_view_count":53.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am training a model using AMLS. I have a training pipeline in which step 1 trains a model then saves the output in temporary datastore model_folder using<\/p>\n<pre><code>os.makedirs(output_folder, exist_ok=True)\noutput_path = output_folder + &quot;\/model.pkl&quot;\njoblib.dump(value=model, filename=output_path)\n<\/code><\/pre>\n<p>Step 2 loads the model and registers it. The model folder is defined in the pipeline as<\/p>\n<pre><code>model_folder = PipelineData(&quot;model_folder&quot;, datastore=ws.get_default_datastore())\n<\/code><\/pre>\n<p>However, step 1 fails when it tries to save the model with the following ServiceError:<\/p>\n<p>Failed to upload outputs due to Exception: Microsoft.RelInfra.Common.Exceptions.OperationFailedException: Cannot upload output xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx. ---&gt; Microsoft.WindowsAzure.Storage.StorageException: This request is not authorized to perform this operation using this permission.<\/p>\n<p>How can I solve this? Earlier in my code I had no problem interacting with the default datastore using<\/p>\n<pre><code>default_ds = ws.get_default_datastore()\ndefault_ds.upload_files(...)\n<\/code><\/pre>\n<p>My <code>70_driver_log.txt<\/code> is as follows:<\/p>\n<pre><code>[2020-08-25T04:03:27.315114] Entering context manager injector.\n[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['train_word2vec.py', '--output_folder', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\/model_folder', '--model_type', 'WO', '--training_field', 'task_title', '--regex', '1', '--stopword_removal', '1', '--tokenize_basic', '0', '--remove_punctuation', '0', '--autocorrect', '0', '--lemmatization', '1', '--word_vector_length', '152', '--model_learning_rate', '0.025', '--model_min_count', '0', '--model_window', '7', '--num_epochs', '10'])\nStarting the daemon thread to refresh tokens in background for process with pid = 113\nEntering Run History Context Manager.\nCurrent directory:  \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\nPreparing to call script [ train_word2vec.py ] with arguments: ['--output_folder', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\/model_folder', '--model_type', 'WO', '--training_field', 'task_title', '--regex', '1', '--stopword_removal', '1', '--tokenize_basic', '0', '--remove_punctuation', '0', '--autocorrect', '0', '--lemmatization', '1', '--word_vector_length', '152', '--model_learning_rate', '0.025', '--model_min_count', '0', '--model_window', '7', '--num_epochs', '10']\nAfter variable expansion, calling script [ train_word2vec.py ] with arguments: ['--output_folder', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\/model_folder', '--model_type', 'WO', '--training_field', 'task_title', '--regex', '1', '--stopword_removal', '1', '--tokenize_basic', '0', '--remove_punctuation', '0', '--autocorrect', '0', '--lemmatization', '1', '--word_vector_length', '152', '--model_learning_rate', '0.025', '--model_min_count', '0', '--model_window', '7', '--num_epochs', '10']\n\nScript type = None\n[nltk_data] Downloading package stopwords to \/root\/nltk_data...\n[nltk_data]   Unzipping corpora\/stopwords.zip.\n[nltk_data] Downloading package wordnet to \/root\/nltk_data...\n[nltk_data]   Unzipping corpora\/wordnet.zip.\nOUTPUT FOLDER: \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\/model_folder\nLoading SQL data...\nLoading abbreviation data...\n\/azureml-envs\/azureml_xxxxx\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py:1783: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#returning-a-view-versus-a-copy\n  self.obj[item_labels[indexer[info_axis]]] = value\nPre-processing data...\nSuccesfully pre-processed the the text data\nTraining Word2Vec model...\nSaving the model...\nStarting the daemon thread to refresh tokens in background for process with pid = 113\n\n\nThe experiment completed successfully. Finalizing run...\n[2020-08-25T04:03:52.293994] TimeoutHandler __init__\n[2020-08-25T04:03:52.294149] TimeoutHandler __enter__\nCleaning up all outstanding Run operations, waiting 300.0 seconds\n2 items cleaning up...\nCleanup took 0.44109439849853516 seconds\n[2020-08-25T04:03:52.818991] TimeoutHandler __exit__\n2020\/08\/25 04:04:00 logger.go:293: Process Exiting with Code:  0\n<\/code><\/pre>\n<p>My arg parse arguments include<\/p>\n<pre><code>parser.add_argument('--output_folder', type=str, dest='output_folder', default=&quot;output_folder&quot;, help='output folder')\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":6,
        "Challenge_created_time":1598325386190,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1598328703460,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63571552",
        "Challenge_link_count":1,
        "Challenge_participation_count":8,
        "Challenge_readability":16.7,
        "Challenge_reading_time":68.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":null,
        "Challenge_title":"joblib.dump() fails when saving model to temporary data store in AMLS",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":571.0,
        "Challenge_word_count":413,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589738451347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":179.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>Fixed this problem by adding my AMLS workspace to a 'storage blob data contributor' role in the AMLS default storage account. It seemly like usually this role is added by default, but it didn't happen in my case.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":2.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":38.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1250158552416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to execute Python script from Azure machine learning studio. I had a script bundle(zip file) connect to the Python script as input. There are python files, txt files and other type of files in this zip file. My question is how do I get the file path from this zip file. For example, if I have language model in this  zip file, named lm.pcl, what's the file path of this language model? \nThanks!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539557794623,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52807787",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.7,
        "Challenge_reading_time":5.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure machine learning studio get access to the file in upload zip file",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":679.0,
        "Challenge_word_count":88,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1337362023536,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":581.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>They're available under the <code>.\/Script Bundle<\/code> directory. For example, if you were to load a pickled model from the zip file, you'd write something along these lines:<\/p>\n\n<pre><code>import pandas as pd\nimport pickle\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n\n    model = pickle.load(open(\".\/Script Bundle\/model.pkl\", \"rb\"))\n    ...\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":4.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"Hi,\n\u00a0\nIs there a way to download my Google AutoML Transation model and use it offline once it's trained?\n\u00a0\nAnd in what format can the model be exported?\u00a0\n\u00a0\nThank you",
        "Challenge_closed_time":1664553.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664280720000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/exporting-a-google-autoML-translate-model\/m-p\/471646#M607",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":6.7,
        "Challenge_reading_time":2.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"exporting a google autoML translate model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":142.0,
        "Challenge_word_count":35,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"1.- No.\n\n2.- You can create a Feature Request at\u00a0Issue Tracker\u00a0and\u00a0add a description about the feature you want(Export Translation Models), and the engineer team will look at it. You can see here how it is more likely that the team prioritize the work of the Feature Request\/Issues.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.2,
        "Solution_reading_time":3.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":55.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1598606826112,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":41.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm using sagemaker 2.5.1 and tensorflow 2.3.0\nThe weird part is that the same code worked before, the only change that I could think of is the new release of the two libraries<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1598861056290,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63667022",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":5.3,
        "Challenge_reading_time":3.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Getting KeyError : 'callable_inputs' when trying to save a TF model in S3 bucket",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":174.0,
        "Challenge_word_count":45,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1598606826112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>The problem is actually coming from smdebug version 0.9.1\nDowngrading to 0.8.1 solves the issue<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":1.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":15.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to share a model registry completely between Dev and Prod environment? So my idea is to create 10000 models in dev and maybe select 2000 from there to work in prod. I am planning to use AWS model registry. So if I do the training and testing and hyperparameter tuning in my AWS dev environment, is it possible to then share the registry in prod? The obvious reason is that it does not make sense to use the prod to do the training and testing again.<\/p>\n<p>Please advise!<\/p>\n<p>Thanks in advance!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638234190620,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70163094",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":6.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker Model Registry Sharing",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":229.0,
        "Challenge_word_count":97,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1557333597230,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":99.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>It depends how you define Dev and Prod.<\/p>\n<ul>\n<li><p>If by Dev and Prod you mean different AWS account (which is a good practice - see <a href=\"https:\/\/docs.aws.amazon.com\/whitepapers\/latest\/organizing-your-aws-environment\/benefits-of-using-multiple-aws-accounts.html\" rel=\"nofollow noreferrer\">doc<\/a> and <a href=\"https:\/\/aws.amazon.com\/blogs\/devops\/aws-building-a-secure-cross-account-continuous-delivery-pipeline\/\" rel=\"nofollow noreferrer\">blog<\/a>), you cannot share fractions of a model registry from a given account to another account, but you can create triggers to export models from one model registry to another, as documented in this blog post <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/patterns-for-multi-account-hub-and-spoke-amazon-sagemaker-model-registry\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/patterns-for-multi-account-hub-and-spoke-amazon-sagemaker-model-registry\/<\/a><\/p>\n<\/li>\n<li><p>If your Dev and Prod live in the same AWS account and you are just looking for ways to differentiate them, you can use:<\/p>\n<ul>\n<li>Model Registry Status information<\/li>\n<li>Tags<\/li>\n<\/ul>\n<\/li>\n<\/ul>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":19.7,
        "Solution_reading_time":15.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":107.0,
        "Tool":"Amazon SageMaker"
    }
]