[
    {
        "Answerer_created_time":1342709052703,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"D\u00fcsseldorf, Germany",
        "Answerer_reputation_count":1889.0,
        "Answerer_view_count":654.0,
        "Challenge_adjusted_solved_time":16.1329663889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I was playing with the AWS instances and trying to deploy some locally trained Keras models, but I find no documentation on that. Has anyone already been able to do it? <\/p>\n\n<p>I tried to use a similar approach to <a href=\"https:\/\/aws.amazon.com\/pt\/blogs\/machine-learning\/bring-your-own-pre-trained-mxnet-or-tensorflow-models-into-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/pt\/blogs\/machine-learning\/bring-your-own-pre-trained-mxnet-or-tensorflow-models-into-amazon-sagemaker\/<\/a>, but I had no success. I also found some examples for training keras models in the cloud, but I was not able to get the entry_point + artifacts right. <\/p>\n\n<p>Thanks for your time!<\/p>",
        "Challenge_closed_time":1538644957792,
        "Challenge_comment_count":0,
        "Challenge_created_time":1538586879113,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52632388",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":12.3,
        "Challenge_reading_time":9.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":16.1329663889,
        "Challenge_title":"Is it possible to deploy a already trained Keras to Sagemaker?",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1291.0,
        "Challenge_word_count":87,
        "Platform":"Stack Overflow",
        "Poster_created_time":1448029868996,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Portugal",
        "Poster_reputation_count":260.0,
        "Poster_view_count":52.0,
        "Solution_body":"<p>Yes, it is possible, and yes, the official documentation is not much of help.\nHowever, I wrote an <a href=\"https:\/\/gnomezgrave.com\/2018\/07\/05\/using-a-custom-model-for-ml-inference-with-amazon-sagemaker\" rel=\"nofollow noreferrer\">article on that<\/a>, and I hope it will help you.<\/p>\n\n<p>Let me know if you need more details. Cheers!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.7,
        "Solution_reading_time":4.39,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":40.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1483472837568,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bangalore, Karnataka, India",
        "Answerer_reputation_count":72.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":67.5576083334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I've just deployed an ML model on Google vertex AI, it can make predictions using vertex AI web interface. But is it possible to send a request from a browser, for example, to this deployed model. Something like<\/p>\n<pre><code>http:\/\/myapp.cloud.google.com\/input=&quot;features of an example&quot; \n<\/code><\/pre>\n<p>and get the prediction as output.\nThanks<\/p>",
        "Challenge_closed_time":1631252937240,
        "Challenge_comment_count":0,
        "Challenge_created_time":1631189210517,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69117885",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.9,
        "Challenge_reading_time":5.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":17.7018675,
        "Challenge_title":"Sending http request Google Vertex AI end point",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":929.0,
        "Challenge_word_count":57,
        "Platform":"Stack Overflow",
        "Poster_created_time":1377724133300,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":349.0,
        "Poster_view_count":47.0,
        "Solution_body":"<p>Yes, you can send using endpoint URL as.<\/p>\n<pre><code>https:\/\/us-central1-aiplatform.googleapis.com\/v1beta1\/projects\/&lt;PROJECT_ID&gt;\/locations\/us-central1\/endpoints\/&lt;ENDPOINT_ID&gt;:predict\n<\/code><\/pre>\n<p>Data should be given as in POST parameter.<\/p>\n<pre><code>{\n  &quot;instances&quot;: \n    [1.4838871833555929,\n 1.8659883497083019,\n 2.234620276849616,\n 1.0187816540094903,\n -2.530890710602246,\n -1.6046416850441676,\n -0.4651483719733302,\n -0.4952254087173721,\n 0.774676376873553]\n}\n<\/code><\/pre>\n<p>URL should be Region Based.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1631432417907,
        "Solution_link_count":1.0,
        "Solution_readability":12.6,
        "Solution_reading_time":7.32,
        "Solution_score_count":4.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":35.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":26.7248641667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have keras training script on my machine. I am experimenting to run my script on AWS sagemaker container. For that I have used below code.<\/p>\n<pre><code>from sagemaker.tensorflow import TensorFlow\nest = TensorFlow(\n    entry_point=&quot;caller.py&quot;,\n    source_dir=&quot;.\/&quot;,\n    role='role_arn',\n    framework_version=&quot;2.3.1&quot;,\n    py_version=&quot;py37&quot;,\n    instance_type='ml.m5.large',\n    instance_count=1,\n    hyperparameters={'batch': 8, 'epochs': 10},\n)\n\nest.fit()\n<\/code><\/pre>\n<p>here <code>caller.py<\/code> is my entry point. After executing the above code I am getting <code>keras is not installed<\/code>. Here is the stacktrace.<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;executor.py&quot;, line 14, in &lt;module&gt;\n    est.fit()\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/estimator.py&quot;, line 682, in fit\n    self.latest_training_job.wait(logs=logs)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/estimator.py&quot;, line 1625, in wait\n    self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/session.py&quot;, line 3681, in logs_for_job\n    self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/session.py&quot;, line 3240, in _check_job_status\n    raise exceptions.UnexpectedStatusException(\nsagemaker.exceptions.UnexpectedStatusException: Error for Training job tensorflow-training-2021-06-09-07-14-01-778: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/usr\/local\/bin\/python3.7 caller.py --batch 4 --epochs 10\n\nModuleNotFoundError: No module named 'keras'\n\n<\/code><\/pre>\n<ol>\n<li>Which instance has pre-installed keras?<\/li>\n<li>Is there any way I can install the python package to the AWS container? or any workaround for the issue?<\/li>\n<\/ol>\n<p>Note: I have tried with my own container uploading to ECR and successfully run my code. I am looking for AWS's existing container capability.<\/p>",
        "Challenge_closed_time":1623320398007,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623223568407,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1623224188496,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67899421",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.0,
        "Challenge_reading_time":28.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":26.8971111111,
        "Challenge_title":"Training keras model in AWS Sagemaker",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":316.0,
        "Challenge_word_count":191,
        "Platform":"Stack Overflow",
        "Poster_created_time":1426675778223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Coimbatore, Tamil Nadu, India",
        "Poster_reputation_count":10189.0,
        "Poster_view_count":1471.0,
        "Solution_body":"<p>Keras is now part of tensorflow, so you can just reformat your code to use <code>tf.keras<\/code> instead of <code>keras<\/code>. Since version 2.3.0 of tensorflow they are in sync, so it should not be that difficult.\nYou container is <a href=\"https:\/\/aws.amazon.com\/releasenotes\/aws-deep-learning-containers-for-tensorflow-2-3-1-with-cuda-11-0\/\" rel=\"nofollow noreferrer\">this<\/a>, as you can see from the list of the packages, there is no <code>Keras<\/code>.\nIf you instead want to extend a pre-built container you can take a look <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/prebuilt-containers-extend.html\" rel=\"nofollow noreferrer\">here<\/a> but I don't recommend in this specific use-case, because also for future code maintainability you should go for <code>tf.keras<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.4,
        "Solution_reading_time":10.34,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":93.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1441651557140,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Parker, CO, USA",
        "Answerer_reputation_count":851.0,
        "Answerer_view_count":71.0,
        "Challenge_adjusted_solved_time":0.9014630556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to follow AWS Sagemaker tutorial to train a machine learning model with a Jupyter notebook environment. <\/p>\n\n<p>According to the tutorial, I'm supposed to copy the following code and run it to import required libraries and set environment variables. <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># import libraries\nimport boto3, re, sys, math, json, os, sagemaker, urllib.request\nfrom sagemaker import get_execution_role\nimport numpy as np                                \nimport pandas as pd                               \nimport matplotlib.pyplot as plt                   \nfrom IPython.display import Image                 \nfrom IPython.display import display               \nfrom time import gmtime, strftime                 \nfrom sagemaker.predictor import csv_serializer   \n\n# Define IAM role\nrole = get_execution_role()\nprefix = 'sagemaker\/DEMO-xgboost-dm'\ncontainers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com\/xgboost:latest',\n              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com\/xgboost:latest',\n              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest',\n              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com\/xgboost:latest'} # each region has its XGBoost container\nmy_region = boto3.session.Session().region_name # set the region of the instance\nprint(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + containers[my_region] + \" container for your SageMaker endpoint.\")\n<\/code><\/pre>\n\n<p>And the expected outcome is below.  <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/wLJ2j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wLJ2j.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>However, I am getting this error.<\/p>\n\n<blockquote>\n  <p>KeyError                                  Traceback (most recent call last)\n   in ()\n       18               'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com\/xgboost:latest'} # each region has its XGBoost container\n       19 my_region = boto3.session.Session().region_name # set the region of the instance\n  ---> 20 print(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + containers[my_region] + \" container for your SageMaker endpoint.\")<\/p>\n  \n  <p>KeyError: 'ap-northeast-2'<\/p>\n<\/blockquote>\n\n<p>I assume that this is happening because my region is <strong>\"ap-northeast-2\"<\/strong>. \nI have a feeling that I need to change the containers for my region.  <\/p>\n\n<p><strong>If my guess is correct, how can I find containers for my region?<\/strong><br>\n<strong>Also, am I overlooking anything else?<\/strong> <\/p>",
        "Challenge_closed_time":1576464384063,
        "Challenge_comment_count":1,
        "Challenge_created_time":1576462664100,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59349805",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.8,
        "Challenge_reading_time":32.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":0.4777675,
        "Challenge_title":"How to find XGBoost containers for different regions in AWS Sagemaker",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2197.0,
        "Challenge_word_count":264,
        "Platform":"Stack Overflow",
        "Poster_created_time":1539556112483,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seoul, South Korea",
        "Poster_reputation_count":2954.0,
        "Poster_view_count":1143.0,
        "Solution_body":"<p>I expect your rational is correct. There isn't an entry for your region in the code. I don't know if there's a list of these containers per region. That being said, you find them in ECR (Elastic Container Registry). <\/p>\n\n<p>Keep in mind, that you can probably fix this quickly by switching to one of the supported regions. Otherwise:<\/p>\n\n<p>If AWS doesn't have a publicly listed container in your region, you can register the container yourself in AWS with ECR. You'll need to login to ECR using the AWS CLI and docker login.<\/p>\n\n<p>You can use the command <code>aws ecr get-login --region ap-northeast-2<\/code> in order to get the token you'll need for docker login.<\/p>\n\n<p>Then, clone this repo: <a href=\"https:\/\/github.com\/aws\/sagemaker-xgboost-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-xgboost-container<\/a><\/p>\n\n<p>You can build this image locally and push it up to ECR. After that, login to the AWS console (or use the AWS CLI) and find the ARN of the image. It should match the format of the others in your code. <\/p>\n\n<p>After that, just add another key\/value entry into the code for your <code>containers<\/code> variable and use <code>'ap-northeast-2': '&lt;ARN of the docker image&gt;'<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1576465909367,
        "Solution_link_count":2.0,
        "Solution_readability":7.3,
        "Solution_reading_time":15.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":187.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":6.4760219444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi,  <br \/>\nI have stored some ml model in my ADLS and I want to register the model to Azure ML using databricks.  <br \/>\nTried to use the following codes to register my ml model but keep encountering an error that the path cannot be found.<\/p>\n<p>import urllib.request  <br \/>\nfrom azureml.core.model import Model<\/p>\n<h1 id=\"register-a-model\">Register a model<\/h1>\n<p>model = Model.register(model_path = 'dbfs:\/mnt\/machinelearning\/classifier.joblib',  <br \/>\nmodel_name = &quot;pretrained-classifier&quot;,  <br \/>\ndescription = &quot;Pretrained Classifier&quot;,  <br \/>\nworkspace=ws)<\/p>",
        "Challenge_closed_time":1642438310976,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642414997297,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/697789\/import-ml-model-from-adls-to-azure-ml-using-databr",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.2,
        "Challenge_reading_time":8.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":6.4760219444,
        "Challenge_title":"Import ML Model from ADLS to Azure ML using Databricks",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":78,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=ad14abdc-d75c-4489-859e-28e2aba507a8\">@Yuzu  <\/a> Using the databricks file path for registering a model is not supported. When using the <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#azureml-core-model-model-register\">model.register()<\/a> you need to download the model locally and then use the path of the model or the folder in which the model is present to register the same.     <\/p>\n<blockquote>\n<p>model_path    <\/p>\n<p>The path on the local file system where the model assets are located. This can be a direct pointer to a single file or folder. If pointing to a folder, the child_paths parameter can be used to specify individual files to bundle together as the Model object, as opposed to using the entire contents of the folder.    <\/p>\n<\/blockquote>\n<p>This sample <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-to-cloud\/model-register-and-deploy.ipynb\">notebook<\/a> should help you with using the method.    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":14.5,
        "Solution_reading_time":18.8,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":149.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.6225,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nIs there a way to regularly checkpoint model artifact in a SageMaker training job for BYO training container?",
        "Challenge_closed_time":1586359356000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586331915000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668623326404,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUrXX2MIygS5igas27GrAhHw\/how-to-checkpoint-sagemaker-model-artifact-during-a-training-job",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":2.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":7.6225,
        "Challenge_title":"How to checkpoint SageMaker model artifact during a training job?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":392.0,
        "Challenge_word_count":28,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"If you specify a checkpoint configuration (***regardless of managed spot training***) when starting a training job, checkpointing will work. You can provide a local path and S3 path as follows (API reference):\n\n    \"CheckpointConfig\": { \n      \"LocalPath\": \"string\",\n      \"S3Uri\": \"string\"\n    }\n\nThe local path defaults to `\/opt\/ml\/checkpoints\/`, and then you specify the target path in S3 with `S3Uri`.\n\n***Given this configuration, SageMaker will configure an output channel with Continuous upload mode to Amazon S3***. At the time being, this results in running an agent on the hosts that watches the file system and continuously uploads data to Amazon S3. Similar behavior is applied when debugging is enabled, for delivering tensor data to Amazon S3.\n\nAs commented, `sagemaker-containers` implements its own code to save intermediate outputs and watching files on the file system, but I would rather rely on the functionality offered by the service to avoid dependencies on specific libraries where possible.\n\n**Note**: when using SageMaker Processing, which in my view can be considered an abstraction over training or, from another perspective, the foundation for training, you can configure an output channel to use continuous upload mode; further info [here][1].\n\n\n  [1]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ProcessingS3Output.html",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925569444,
        "Solution_link_count":1.0,
        "Solution_readability":16.1,
        "Solution_reading_time":16.88,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":188.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1566789105747,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":156.0,
        "Answerer_view_count":22.0,
        "Challenge_adjusted_solved_time":24.4308544445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Where can I look for the <strong>performance metrics<\/strong> generated by <strong>Amazon SageMaker Debugger\/Profiler<\/strong>?<\/p>",
        "Challenge_closed_time":1663457554492,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663381119547,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73751712",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":17.2,
        "Challenge_reading_time":2.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":21.2319291667,
        "Challenge_title":"Where can I find the Performance Metrics generated by SageMaker Debugger\/Profiler?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":23.0,
        "Challenge_word_count":23,
        "Platform":"Stack Overflow",
        "Poster_created_time":1389887039672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":5854.0,
        "Poster_view_count":794.0,
        "Solution_body":"<p>1.<code>from sagemaker.debugger import ProfilerConfig<\/code><\/p>\n<p><code>profiler_config = ProfilerConfig( framework_profile_params=FrameworkProfile(start_step=1, num_steps=2) )<\/code><\/p>\n<p>2.\n<code>from sagemaker.debugger import TensorBoardOutputConfig<\/code><\/p>\n<p><code>tensorboard_output_config = TensorBoardOutputConfig(s3_output_path= &lt;&lt; add your bucket name an folder &gt;&gt; )<\/code><\/p>\n<ol start=\"3\">\n<li><p>In your estimator - specify :  <code>profiler_config= profiler_config<\/code> and <code>tensorboard_output_config=tensorboard_output_config<\/code><\/p>\n<\/li>\n<li><p>Train your model<\/p>\n<\/li>\n<li><p>Go to the s3 bucket specified  for your training job name that is assigned in Sagemaker . You should see a report under <strong>rule-output<\/strong> &gt; <strong>ProfilerReport<\/strong> *** &gt; <strong>profiler-output\/<\/strong> &gt; <strong>profiler-report.html<\/strong><\/p>\n<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1663469070623,
        "Solution_link_count":0.0,
        "Solution_readability":23.2,
        "Solution_reading_time":12.31,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":71.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1523264005616,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Yokohama, Kanagawa, Japan",
        "Answerer_reputation_count":1635.0,
        "Answerer_view_count":143.0,
        "Challenge_adjusted_solved_time":20.8984680556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a sagemaker model that is trained on a specific dataset, and training job is created. Now i have a new dataset that the model has to be trained on, how do I retrain the model on new data from the already existing model ? Can we have the model checkpoints saved ?<\/p>",
        "Challenge_closed_time":1584462704472,
        "Challenge_comment_count":1,
        "Challenge_created_time":1584387469987,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60712225",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.5,
        "Challenge_reading_time":4.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":20.8984680556,
        "Challenge_title":"How to retrain a sagemaker model on new data, after a training job is created",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":660.0,
        "Challenge_word_count":67,
        "Platform":"Stack Overflow",
        "Poster_created_time":1500501171967,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"New York, NY, United States",
        "Poster_reputation_count":33.0,
        "Poster_view_count":8.0,
        "Solution_body":"<blockquote>\n  <p>Only three built-in algorithms currently support incremental training: Object Detection Algorithm, Image Classification Algorithm, and Semantic Segmentation Algorithm.<\/p>\n<\/blockquote>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/incremental-training.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/incremental-training.html<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":33.6,
        "Solution_reading_time":5.48,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":24.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":15.4171983334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am looking at Azure's training modules and it states I can learn no-code models with Azure, but it also tells me I should know python. I'm a little confused at where I should spend time training in most efficient pathway. My goal is to just do predictive modeling within Azure. I have technical\/IT literacy however coding is at a basic level.   <\/p>\n<p>Ideally id like some sort of Certification, if possible from just &quot;Create no-code predictive models with Azure Machine Learning&quot;  <\/p>\n<p>Is &quot;Microsoft Certified: Azure Data Scientist Associate&quot; going to require a lot of pre work on python\/torch\/tensor? I'd ideally like Azure to be my entry. <\/p>",
        "Challenge_closed_time":1592924808467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592869306553,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/38841\/pathway-for-code-free-predictive-modeling",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":8.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":15.4171983334,
        "Challenge_title":"Pathway for code free predictive modeling",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":114,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Thanks for reaching out. Azure machine learning has a drag and drop interface (Designer) that supports code free predictive modeling. <a href=\"https:\/\/learn.microsoft.com\/en-us\/learn\/paths\/create-no-code-predictive-models-azure-machine-learning\/\">Create no-code predictive models with Azure Machine Learning<\/a> training modules is a great starting point and provides a pathway for <a href=\"https:\/\/learn.microsoft.com\/en-us\/learn\/certifications\/azure-data-scientist\">Azure Data Scientist Associate certification<\/a>. However, you also need programming experience and familiarity with various data science processes\/principles to be successful on the certification exam.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.9,
        "Solution_reading_time":8.98,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":68.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":105.0398277778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a training script in Sagemaker like,<\/p>\n\n<pre><code>def train(current_host, hosts, num_cpus, num_gpus, channel_input_dirs, model_dir, hyperparameters, **kwargs):\n    ... Train a network ...\n    return net\n\ndef save(net, model_dir):\n    # save the model\n    logging.info('Saving model')\n    y = net(mx.sym.var('data'))\n    y.save('%s\/model.json' % model_dir)\n    net.collect_params().save('%s\/model.params' % model_dir)\n\ndef model_fn(model_dir):\n    symbol = mx.sym.load('%s\/model.json' % model_dir)\n    outputs = mx.symbol.softmax(data=symbol, name='softmax_label')\n    inputs = mx.sym.var('data')\n    param_dict = gluon.ParameterDict('model_')\n    net = gluon.SymbolBlock(outputs, inputs, param_dict)\n    net.load_params('%s\/model.params' % model_dir, ctx=mx.cpu())\n    return net\n<\/code><\/pre>\n\n<p>Most of which I stole from the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mnist.py\" rel=\"nofollow noreferrer\">MNIST Example<\/a>.<\/p>\n\n<p>When I train, everything goes fine, but when trying to deploy like,<\/p>\n\n<pre><code>m = MXNet(\"lstm_trainer.py\", \n          role=role, \n          train_instance_count=1, \n          train_instance_type=\"ml.c4.xlarge\",\n          hyperparameters={'batch_size': 100, \n                         'epochs': 20, \n                         'learning_rate': 0.1, \n                         'momentum': 0.9, \n                         'log_interval': 100})\nm.fit(inputs) # No errors\npredictor = m.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>I get, (<a href=\"https:\/\/gist.github.com\/aidan-plenert-macdonald\/7eb7ba7402790b61596938b5cbf605b6\" rel=\"nofollow noreferrer\">full output<\/a>)<\/p>\n\n<pre><code>INFO:sagemaker:Creating model with name: sagemaker-mxnet-py2-cpu-2018-01-17-20-52-52-599\n---------------------------------------------------------------------------\n  ... Stack dump ...\nClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not find model data at s3:\/\/sagemaker-us-west-2-01234567890\/sagemaker-mxnet-py2-cpu-2018-01-17-20-52-52-599\/output\/model.tar.gz.\n<\/code><\/pre>\n\n<p>Looking in my S3 bucket <code>s3:\/\/sagemaker-us-west-2-01234567890\/sagemaker-mxnet-py2-cpu-2018-01-17-20-52-52-599\/output\/model.tar.gz<\/code>, I in fact don't see the model.<\/p>\n\n<p>What am I missing?<\/p>",
        "Challenge_closed_time":1516602490380,
        "Challenge_comment_count":2,
        "Challenge_created_time":1516224347000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48310237",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":16.3,
        "Challenge_reading_time":30.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":105.0398277778,
        "Challenge_title":"Sagemaker \"Could not find model data\" when trying to deploy my model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2950.0,
        "Challenge_word_count":164,
        "Platform":"Stack Overflow",
        "Poster_created_time":1384712661608,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Diego, CA, United States",
        "Poster_reputation_count":5151.0,
        "Poster_view_count":1038.0,
        "Solution_body":"<p>When you are calling the training job you should specify the output directory:<\/p>\n\n<pre><code>#Bucket location where results of model training are saved.\nmodel_artifacts_location = 's3:\/\/&lt;bucket-name&gt;\/artifacts'\n\nm = MXNet(entry_point='lstm_trainer.py',\n          role=role,\n          output_path=model_artifacts_location,\n          ...)\n<\/code><\/pre>\n\n<p>If you don't specify the output directory the function will use a default location, that it might not have the permissions to create or write to.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.6,
        "Solution_reading_time":6.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1431724059856,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":66.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":199.7542319444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><strong>NOTICE: Azure Machine Learning Workbench (Preview) is deprecated. The workflow for deploying models, images and services has been updated since this question was posted.<\/strong><\/p>\n\n<p>I have been developing a Machine Learning model for Azure Machine Learning Services using Azure Machine Learning Workbench (Preview). I successfully managed to deploy the model as a web service, as instructed in <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/desktop-workbench\/model-management-service-deploy\" rel=\"nofollow noreferrer\">Azure Machine Learning Documentation (Preview)<\/a>. I have managed to get the service up and running, and the model, manifest and images are all configured correctly. So far so good.<\/p>\n\n<p>But now I have come to the phase where I want to be able to update the service with new configurations. And this is where I find myself with more questions than answers. <\/p>\n\n<p>I have figured out that I can<\/p>\n\n<ol>\n<li>configure a new model<\/li>\n<li>configure a new manifest pointing to that model<\/li>\n<li>configure a new image pointing to that manifest<\/li>\n<li>update an existing (or create a new) service to point to the new image<\/li>\n<\/ol>\n\n<p>This seems reasonable enough. But what If I just need to update the manifest, would it be possible to skip the configuration of a new model (1), and just begin the update from (2) above, and let it point to an existing model instead of a new one?<\/p>\n\n<p>I have of course tried this by calling the following from the CLI, and I get stuck with the following output:<\/p>\n\n<pre><code>&gt;&gt; az ml manifest create --manifest-name manifestname -f score.py -r python -c aml_config\/conda_dependencies.yml -s outputs\/schema.json -i [existing-model-id]\nCreating new driver at \/var\/folders\/tmp\/tmp.py\nSuccessfully created manifest\nId: [manifest-id]\n&gt;&gt; az ml image create -n imagename --manifest-id [manifest-id-from-above]\nCreating image............................................Done.\nImage ID: [image-id]\n&gt;&gt; az ml service update realtime -i [existing-service-id] --image-id [image-id-from-above] -v\nUpdating service..................................Failed\nFound default kubeconfig in \/Users\/username\/.kube\/config using it\nUsing kubeconfig file: \/Users\/username\/.kube\/config\nKubectl exists in default location, adding it to PATH\nloading kubeconfig file\nGetting Replica sets from default namespace\nGot hash ####\n{\n    \"Azure-cli-ml Version\": null,\n    \"Error\": \"Error occurred\",\n    \"Response Content\": {\n        \"CreatedTime\": \"2018-09-17T13:31:22.4230543Z\",\n        \"EndTime\": \"2018-09-17T13:34:18.0774994Z\",\n        \"Error\": {\n            \"Code\": \"KubernetesDeploymentFailed\",\n            \"Details\": [\n                {\n                    \"Code\": \"CrashLoopBackOff\",\n                    \"Message\": \"Back-off 40s restarting failed container=### pod=###\"\n                }\n            ],\n            \"Message\": \"Kubernetes Deployment failed\",\n            \"StatusCode\": 400\n        },\n        \"Id\": \"###\",\n        \"OperationType\": \"Service\",\n        \"ResourceLocation\": \"###\",\n        \"State\": \"Failed\"\n    },\n    \"Response Headers\": {\n        \"Connection\": \"keep-alive\",\n        \"Content-Encoding\": \"gzip\",\n        \"Content-Type\": \"application\/json; charset=utf-8\",\n        \"Date\": \"Mon, 17 Sep 2018 13:34:22 GMT\",\n        \"Strict-Transport-Security\": \"max-age=15724800; includeSubDomains; preload\",\n        \"Transfer-Encoding\": \"chunked\",\n        \"X-Content-Type-Options\": \"nosniff\",\n        \"X-Frame-Options\": \"SAMEORIGIN\",\n        \"api-supported-versions\": \"2017-09-01-preview, 2018-04-01-preview\",\n        \"x-ms-client-request-id\": \"###\",\n        \"x-ms-client-session-id\": \"\"\n    }\n}\n<\/code><\/pre>\n\n<p>If I try to rollback to the previous manifest, there is no error message, and everything works just fine. This makes me assume there is something wrong with my new manifest and\/or image. There is no warning or error when creating them, however.<\/p>\n\n<p>I have tried searching for the error messages but I find nothing.<\/p>",
        "Challenge_closed_time":1537914767652,
        "Challenge_comment_count":0,
        "Challenge_created_time":1537195652417,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1538554289270,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52370381",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.4,
        "Challenge_reading_time":48.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":199.7542319444,
        "Challenge_title":"Azure ML ModelManagement web service update",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":537.0,
        "Challenge_word_count":449,
        "Platform":"Stack Overflow",
        "Poster_created_time":1463756509236,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Uppsala, Sverige",
        "Poster_reputation_count":400.0,
        "Poster_view_count":43.0,
        "Solution_body":"<p>CrashLoopBackOff error normally means that the init() function of your score.py file has a problem, for example, finding or loading the model. It could also mean you are using a library that hasn't been imported.\nAzure ML just announced an update to the preview with an updated Python SDK (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/quickstart-get-started\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/quickstart-get-started<\/a>). \nThere are tutorials and notebooks that show the process in more details with examples. I would start there.<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":14.8,
        "Solution_reading_time":11.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":75.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1309439921808,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3050.0,
        "Answerer_view_count":106.0,
        "Challenge_adjusted_solved_time":3196.0599155556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have wrapped my keras-tf-model into a Sklearn Pipeline, which also does some pre- and postprocessing. I want to serialize this model and capture its dependencies via MLflow.<\/p>\n\n<p>I have tried <code>mlflow.keras.save_model()<\/code>, which seems not appropriate. (it's not a \"pure\" keras model and as no <code>save()<\/code> attribute)<\/p>\n\n<p>I also tried <code>mlflow.sklearn.save_model()<\/code> and <code>mlflow.pyfunc.save_model()<\/code>, which both lead my to the same error: <\/p>\n\n<p><code>NotImplementedError: numpy() is only available when eager execution is enabled.<\/code><\/p>\n\n<p>(This error seems to stem from a clash between python and tensorflow, maybe?)<\/p>\n\n<p>I wonder, should it already\/ generally be possible to serialize these kind of \"hybrid\" models with mlflow?<\/p>\n\n<h3>Please finde a minimal example below<\/h3>\n\n<pre><code># In[1]:\n\n\nfrom mlflow.sklearn import save_model\nimport mlflow.sklearn\nfrom sklearn.datasets import load_iris\nfrom sklearn import tree\n\nfrom tensorflow.keras.models import Sequential\n\nimport numpy as np\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\n\n\n# ### Save Keras Model\n\n# In[2]:\n\n\niris_data = load_iris() \n\nx = iris_data.data\ny_ = iris_data.target.reshape(-1, 1)\n\n# One Hot encode the class labels\nencoder = OneHotEncoder(sparse=False)\ny = encoder.fit_transform(y_)\n\n# Split the data for training and testing\ntrain_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.20)\n\n# Build the model\nmodel = Sequential()\n\nmodel.add(Dense(10, input_shape=(4,), activation='relu', name='fc1'))\nmodel.add(Dense(10, activation='relu', name='fc2'))\nmodel.add(Dense(3, activation='softmax', name='output'))\n\noptimizer = Adam(lr=0.001)\nmodel.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(train_x, train_y, verbose=2, batch_size=5, epochs=20)\n\n\n# In[3]:\n\n\nimport mlflow.keras\n\nmlflow.keras.save_model(model, \"modelstorage\/model40\")\n\n\n# ### Save Minimal SKlearn-Pipeline (with Keras)\n\n# In[4]:\n\n\nfrom category_encoders.target_encoder import TargetEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\n\n# In[5]:\n\n\ndef define_model():\n    \"\"\"\n    Create fully connected network with given parameters.\n    \"\"\"\n    keras_model = Sequential()\n\n    keras_model.add(Dense(10, input_shape=(4,), activation='relu', name='fc1'))\n    keras_model.add(Dense(10, activation='relu', name='fc2'))\n    keras_model.add(Dense(3, activation='softmax', name='output'))\n\n    optimizer = Adam(lr=0.001)\n    keras_model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\n# In[6]:\n\n\n# target_encoder = TargetEncoder() \nscaler = StandardScaler()\nkeras_model = KerasClassifier(define_model, batch_size=5, epochs=20)\n\n\n# In[7]:\n\n\npipeline = Pipeline([\n#     ('encoding', target_encoder),\n    ('scaling', scaler),\n    ('modeling', keras_model)\n])\n\n\n# In[8]:\n\n\npipeline.fit(train_x, train_y)\n\n\n# In[9]:\n\n\nmlflow.keras.save_model(pipeline, \"modelstorage\/model42\")   #not working\n\n\n# In[10]:\n\n\nimport mlflow.sklearn\n\nmlflow.sklearn.save_model(pipeline, \"modelstorage\/model43\")\n\nOutput from modelstorage\/model43\/conda.yaml:\n\n======================\nchannels:\n- defaults\ndependencies:\n- python=3.6.7\n- scikit-learn=0.21.2\n- pip:\n  - mlflow\n  - cloudpickle==1.2.1\nname: mlflow-env\n======================\n\nDoesn't seem to capture Tensorflow.\n<\/code><\/pre>",
        "Challenge_closed_time":1572628704763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1561111222150,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1561122889067,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56701139",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.1,
        "Challenge_reading_time":47.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":47,
        "Challenge_solved_time":3199.3007258334,
        "Challenge_title":"Model-logging for \"hybrid models\" (e.g. SKlearn Pipeline including KerasWrapper) possible?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1470.0,
        "Challenge_word_count":334,
        "Platform":"Stack Overflow",
        "Poster_created_time":1336936830510,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":948.0,
        "Poster_view_count":132.0,
        "Solution_body":"<p>You can add extra dependencies when you save your model, for example if you have a keras step in your pipeline you can add keras &amp; tensorflow:<\/p>\n\n<pre><code>  conda_env = mlflow.sklearn.get_default_conda_env()\n  conda_env[\"dependencies\"] = ['keras==2.2.4', 'tensorflow==1.14.0'] + conda_env[\"dependencies\"]\n  mlflow.sklearn.log_model(pipeline, \"modelstorage\/model43\", conda_env = conda_env)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.8,
        "Solution_reading_time":5.36,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":39.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":17.8105555556,
        "Challenge_answer_count":1,
        "Challenge_body":"Where can I find the actual references to API definitions and descriptions for ModelBiasMonitor and ModelExplainabilityMonitor Classes?\n\nI can a find a few mentions in the Amazon SageMaker documentation in the following links.\nhttps:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model_monitor.html\nhttps:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_model_monitor\/fairness_and_explainability\/SageMaker-Model-Monitor-Fairness-and-Explainability.html\n\nWhere can I find the actual reference and the code implementation for these Classes?",
        "Challenge_closed_time":1611571671000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611507553000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1667925795096,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU9SPQKzelSUu3dr-D4zaXHQ\/api-definition-for-modelbiasmonitor-and-modelexplainabilitymonitor",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":27.4,
        "Challenge_reading_time":8.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":17.8105555556,
        "Challenge_title":"API definition for ModelBiasMonitor and ModelExplainabilityMonitor",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":45.0,
        "Challenge_word_count":54,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The actual reference to the classes can be found here: \n[https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/model_monitor\/clarify_model_monitoring.py ]()  \nIt encapsulates the definitions and descriptions for all of SageMaker Clarify related monitoring classes.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1613663015963,
        "Solution_link_count":1.0,
        "Solution_readability":24.7,
        "Solution_reading_time":3.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1529408888483,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":79.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":28.0548311111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained a Pytorch model using SageMaker and the model is now stored in an S3 bucket.\nI am trying to retrieve that model and deploying it.<\/p>\n\n<p>This is the code I am using:<\/p>\n\n<pre><code>estimator = sagemaker.model.FrameworkModel(\n    model_data= #link to  model location in s3\n    image=  # image\n    role=role,\n    entry_point='train.py', \n    source_dir='pytorch_source',\n    sagemaker_session = sagemaker_session\n) \n\npredictor = estimator.deploy(initial_instance_count=1, instance_type=\"ml.p2.xlarge\")\n<\/code><\/pre>\n\n<p>But after the deployment process (that seems to run smoothly), the predictor is just a NoneType.\nI haven't found any weird message in the logs...<\/p>\n\n<p>I have also made another attempt with the following code:<\/p>\n\n<pre><code>estimator = PyTorchModel(model_data= #link to model location in s3 \n                             role=role,\n                             image= #image\n                             entry_point='pytorch_source\/train.py',\n                            predictor_cls = 'pytorch_source\/train.py',\n                           framework_version = '1.1.0')\n\npredictor = estimator.deploy(initial_instance_count=1, instance_type=\"ml.p2.xlarge\")\n<\/code><\/pre>\n\n<p>But it doesn't even complete the deployment.<\/p>\n\n<p>Can anyone help with this?<\/p>",
        "Challenge_closed_time":1583499635092,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583334481857,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1583398637700,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60529048",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":15.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":45.8758986111,
        "Challenge_title":"How to deploy an existing pytorch model previously trained with Amazon Sagemaker and stored in S3 bucket",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":360.0,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_created_time":1529408888483,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":79.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I actually solved using PyTorchModel with the following settings:<\/p>\n\n<pre><code>estimator = PyTorchModel(model_data='#path to model, \n                             role=role,\n                             source_dir='pytorch_source',\n                             entry_point='deploy.py',\n                            predictor_cls = ImgPredictor,\n                           framework_version = '1.1.0')\n<\/code><\/pre>\n\n<p>where ImgPredictor is<\/p>\n\n<pre><code>from sagemaker.predictor import RealTimePredictor, json_deserializer\n\nclass ImgPredictor(RealTimePredictor):\n    def __init__(self, endpoint_name, sagemaker_session):\n        super(ImgPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='application\/x-image', \n                                           deserializer = json_deserializer ,accept='application\/json')\n<\/code><\/pre>\n\n<p>and deploy.py contains the required functions input_fn, output_fn, model_fn and predict_fn.\nAlso, a requirements.txt file was missing in the source directory.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":22.9,
        "Solution_reading_time":11.09,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":12.2807027778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Let's say I have successfully trained a model on some training data for 10 epochs. How can I then access the very same model and train for a further 10 epochs?<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-checkpoints.html\" rel=\"nofollow noreferrer\">In the docs<\/a> it suggests &quot;you need to specify a checkpoint output path through hyperparameters&quot; --&gt; how?<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># define my estimator the standard way\nhuggingface_estimator = HuggingFace(\n    entry_point='train.py',\n    source_dir='.\/scripts',\n    instance_type='ml.p3.2xlarge',\n    instance_count=1,\n    role=role,\n    transformers_version='4.10',\n    pytorch_version='1.9',\n    py_version='py38',\n    hyperparameters = hyperparameters,\n    metric_definitions=metric_definitions\n)\n\n# train the model\nhuggingface_estimator.fit(\n    {'train': training_input_path, 'test': test_input_path}\n)\n<\/code><\/pre>\n<p>If I run <code>huggingface_estimator.fit<\/code> again it will just start the whole thing over again and overwrite my previous training.<\/p>",
        "Challenge_closed_time":1649831911830,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649787701300,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71847442",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.7,
        "Challenge_reading_time":14.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":12.2807027778,
        "Challenge_title":"Train an already trained model in Sagemaker and Huggingface without re-initialising",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":110.0,
        "Challenge_word_count":111,
        "Platform":"Stack Overflow",
        "Poster_created_time":1437078651387,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":901.0,
        "Poster_view_count":145.0,
        "Solution_body":"<p>You can find the relevant checkpoint save\/load code in <a href=\"https:\/\/github.com\/huggingface\/notebooks\/blob\/main\/sagemaker\/05_spot_instances\/sagemaker-notebook.ipynb\" rel=\"nofollow noreferrer\">Spot Instances - Amazon SageMaker x Hugging Face Transformers<\/a>.<br \/>\n(The example enables Spot instances, but you can use on-demand).<\/p>\n<ol>\n<li>In hyperparameters you set: <code>'output_dir':'\/opt\/ml\/checkpoints'<\/code>.<\/li>\n<li>You define a <code>checkpoint_s3_uri<\/code> in the Estimator (which is unique to the series of jobs you'll run).<\/li>\n<li>You add code for train.py to support checkpointing:<\/li>\n<\/ol>\n<blockquote>\n<pre><code>from transformers.trainer_utils import get_last_checkpoint\n\n# check if checkpoint existing if so continue training\nif get_last_checkpoint(args.output_dir) is not None:\n    logger.info(&quot;***** continue training *****&quot;)\n    last_checkpoint = get_last_checkpoint(args.output_dir)\n    trainer.train(resume_from_checkpoint=last_checkpoint)\nelse:\n    trainer.train()\n<\/code><\/pre>\n<\/blockquote>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.2,
        "Solution_reading_time":13.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":91.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1525393797416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Philadelphia, USA",
        "Answerer_reputation_count":130.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":0.4764786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created an mlflow model with custom pyfunc. It shows the results when I send input to the loaded model in Jupyter notebook.\nHowever if I am trying to serve it to a local port<\/p>\n<pre><code>!mlflow models serve -m Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model -p 8001\n<\/code><\/pre>\n<p>I am getting this error<\/p>\n<pre><code> Traceback (most recent call last):\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/bin\/mlflow&quot;, line 10, in &lt;module&gt;\n    sys.exit(cli())\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 829, in __call__\n    return self.main(*args, **kwargs)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 782, in main\n    rv = self.invoke(ctx)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 610, in invoke\n    return callback(*args, **kwargs)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/models\/cli.py&quot;, line 56, in serve\n    install_mlflow=install_mlflow).serve(model_uri=model_uri, port=port,\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/models\/cli.py&quot;, line 163, in _get_flavor_backend\n    append_to_uri_path(underlying_model_uri, &quot;MLmodel&quot;), output_path=tmp.path())\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/tracking\/artifact_utils.py&quot;, line 76, in _download_artifact_from_uri\n    artifact_path=artifact_path, dst_path=output_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/local_artifact_repo.py&quot;, line 67, in download_artifacts\n    return super(LocalArtifactRepository, self).download_artifacts(artifact_path, dst_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py&quot;, line 140, in download_artifacts\n    return download_file(artifact_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py&quot;, line 105, in download_file\n    self._download_file(remote_file_path=fullpath, local_path=local_file_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/local_artifact_repo.py&quot;, line 95, in _download_file\n    shutil.copyfile(remote_file_path, local_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/shutil.py&quot;, line 120, in copyfile\n    with open(src, 'rb') as fsrc:\nFileNotFoundError: [Errno 2] No such file or directory: 'Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model\/MLmodel'\n<\/code><\/pre>",
        "Challenge_closed_time":1611840603100,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611838887777,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65937623",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":23.6,
        "Challenge_reading_time":47.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":0.4764786111,
        "Challenge_title":"Unable to serve an mlflow model locally",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1277.0,
        "Challenge_word_count":199,
        "Platform":"Stack Overflow",
        "Poster_created_time":1573739890560,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>From your error traceback, the model artifact can't be located. In your code, you are executing the 'mlflow' command from within a Jupyter Notebook. I would suggest trying the following:<\/p>\n<ol>\n<li>Check if your models artifacts are on the path you are using Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model<\/li>\n<li>Try opening a terminal, then <code>cd \/Home\/miniconda3\/envs<\/code> and  execute <code>mlflow models serve -m .\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model -p 8001<\/code><\/li>\n<li>MLFlow offers different solutions to serve a model, you can try to register your model and refer to it as &quot;models:\/{model_name}\/{stage}&quot; as mentioned in the Model Registry <a href=\"https:\/\/mlflow.org\/docs\/latest\/model-registry.html#serving-an-mlflow-model-from-model-registry\" rel=\"nofollow noreferrer\">docs<\/a><\/li>\n<\/ol>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.2,
        "Solution_reading_time":11.53,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":92.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1280527017200,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3035.0,
        "Answerer_view_count":129.0,
        "Challenge_adjusted_solved_time":98.7025555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have started exploring AWS SageMaker starting with these <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/blazingtext_word2vec_subwords_text8\" rel=\"nofollow noreferrer\">examples provided by AWS<\/a>. I then made some modifications to this particular setup so that it uses the data from my use case for training.<\/p>\n\n<p>Now, as I continue to work on this model and tuning, after I delete the inference endpoint once, I would like to be able to recreate the same endpoint -- even after stopping and restarting the notebook instance (so the notebook \/ kernel session is no longer valid) -- using the already trained model artifacts that gets uploaded to S3 under \/output folder.<\/p>\n\n<p>Now I cannot simply jump directly to this line of code:<\/p>\n\n<pre><code>bt_endpoint = bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>I did some searching -- including <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/blazingtext_hosting_pretrained_fasttext\" rel=\"nofollow noreferrer\">amazon's own example of hosting pre-trained models<\/a>, but I am a little lost. I would appreciate any guidance, examples, or documentation that I could emulate and adapt to my case.<\/p>",
        "Challenge_closed_time":1539622672603,
        "Challenge_comment_count":2,
        "Challenge_created_time":1539267343403,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1539637545790,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52762367",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":14.1,
        "Challenge_reading_time":17.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":98.7025555556,
        "Challenge_title":"Re-hosting a trained model on AWS SageMaker",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1916.0,
        "Challenge_word_count":157,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432179256928,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":17.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Your comment is correct - you can re-create an Endpoint given an existing EndpointConfiguration. This can be done via the console, the AWS CLI, or the SageMaker boto client.<\/p>\n\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-endpoint.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-endpoint.html<\/a><\/li>\n<li><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_endpoint\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_endpoint<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":32.9,
        "Solution_reading_time":9.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":38.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1221810788500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paderborn, North-Rhine-Westphalia, Germany",
        "Answerer_reputation_count":68522.0,
        "Answerer_view_count":7896.0,
        "Challenge_adjusted_solved_time":2.6447536111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I would like to create a databricks multi-task with following sequence:<\/p>\n<ul>\n<li>notebook task 1: train model with results logged to MLflow tracking server<\/li>\n<li>notebook task 2: use mlflow run_id from task 1 to register model in model registry<\/li>\n<\/ul>\n<p>Is it possible to pass run_id from task 1 to task 2 and if so is there any documentation on how this could be done?<\/p>",
        "Challenge_closed_time":1637341239876,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637331718763,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1654760709848,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70036318",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":14.6,
        "Challenge_reading_time":5.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":2.6447536111,
        "Challenge_title":"Databricks multi-task jobs - pass MLflow run_id from one task to next task",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":198.0,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436447644056,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":818.0,
        "Poster_view_count":94.0,
        "Solution_body":"<p>As of <em>right now<\/em> (it may change), it's impossible to pass results between jobs if you use multi-task job.<\/p>\n<p>But you can call another notebook as a child job if you use <a href=\"https:\/\/docs.databricks.com\/notebooks\/notebook-workflows.html\" rel=\"nofollow noreferrer\">notebook workflows<\/a>  and function <code>dbutils.notebooks.run<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># notebook 1\n... training code ...\ndbutils.notebooks.run(&quot;notebook2&quot;, 300, {&quot;run_id&quot;: run_id})\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.1,
        "Solution_reading_time":7.07,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":52.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1324351066392,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Christchurch, New Zealand",
        "Answerer_reputation_count":1306.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":65.1553044444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using the azure automl python sdk to download and save a model then reload it. I get the following error:<\/p>\n<pre><code>anaconda3\\envs\\automl_21\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator Pipeline from version 0.22.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n  UserWarning)\n<\/code><\/pre>\n<p>How can I ensure that the versions match?<\/p>",
        "Challenge_closed_time":1618176409272,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617943623520,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67015185",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":6.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":64.6627088889,
        "Challenge_title":"How can I match my local azure automl python sdk version to the remote version?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":62.0,
        "Challenge_word_count":73,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324351066392,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Christchurch, New Zealand",
        "Poster_reputation_count":1306.0,
        "Poster_view_count":81.0,
        "Solution_body":"<p>My Microsoft contact says -<\/p>\n<p>&quot;For this, their best  bet is probably to see what the training env was pinned to and install those same pins. They can get that env by running child_run.get_environment() and then pip install all the pkgs listed in there with the pins listed there.&quot;<\/p>\n<p>A useful code snippet.<\/p>\n<pre><code>for run in experiment.get_runs():\n    tags_dictionary = run.get_tags()\n    best_run = AutoMLRun(experiment, tags_dictionary['automl_best_child_run_id'])\n    env = best_run.get_environment()\n    print(env.python.conda_dependencies.serialize_to_string())\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1618178182616,
        "Solution_link_count":0.0,
        "Solution_readability":9.3,
        "Solution_reading_time":7.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":70.62626,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hello to all, <\/p>\n<p>I am working on a machine learning project, I have trained my model on azure auto ml studio, I would like to import it in onnx format, but it is not in the download options, so I want to modify the resulting code directly in azure auto ml, in the script.py I have modified the recording format of the model but I can't find how to execute this script because it tells me that the script.py is not found, <\/p>\n<p>Could you help me please ? <\/p>\n<p>thank you <\/p>\n<p>Lysa<\/p>",
        "Challenge_closed_time":1680506244446,
        "Challenge_comment_count":0,
        "Challenge_created_time":1680251989910,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1195000\/how-to-modify-the-template-script-on-azure-auto-ml",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.8,
        "Challenge_reading_time":6.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":70.62626,
        "Challenge_title":"how to modify the template script on azure auto ml?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":103,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=2d96c782-8477-4987-b5a4-5ea497b49eb9\">AMROUN Lysa<\/a> I believe you are looking to create a model wihich is ONNX compatible with AutoML as per your previous thread. In this case in the automl config if you setup <code>enable_onnx_compatible_models<\/code> to true in the automl config the model should be available for download. Something similar to what is provided as guidance in this <a href=\"https:\/\/github.com\/Azure\/azureml-examples\/blob\/main\/v1\/python-sdk\/tutorials\/automl-with-azureml\/classification-bank-marketing-all-features\/auto-ml-classification-bank-marketing-all-features.ipynb\">notebook<\/a>.<\/p>\n<p>I would recommend running through this notebook and then change your experiment settings in a similar way to retrieve the best onnx format model. <\/p>\n<pre><code>automl_settings = {\n    &quot;experiment_timeout_hours&quot;: 0.3,\n    &quot;enable_early_stopping&quot;: True,\n    &quot;iteration_timeout_minutes&quot;: 5,\n    &quot;max_concurrent_iterations&quot;: 4,\n    &quot;max_cores_per_iteration&quot;: -1,\n    # &quot;n_cross_validations&quot;: 2,\n    &quot;primary_metric&quot;: &quot;AUC_weighted&quot;,\n    &quot;featurization&quot;: &quot;auto&quot;,\n    &quot;verbosity&quot;: logging.INFO,\n    &quot;enable_code_generation&quot;: True,\n}\n\nautoml_config = AutoMLConfig(\n    task=&quot;classification&quot;,\n    debug_log=&quot;automl_errors.log&quot;,\n    compute_target=compute_target,\n    experiment_exit_score=0.9984,\n    blocked_models=[&quot;KNN&quot;, &quot;LinearSVM&quot;],\n    enable_onnx_compatible_models=True,\n    training_data=train_data,\n    label_column_name=label,\n    validation_data=validation_dataset,\n    **automl_settings,\n)\n\n<\/code><\/pre>\n<p>If this answers your query, do click <code>Accept Answer<\/code> and <code>Yes<\/code> for was this answer helpful. And, if you have any further query do let us know.<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":20.8,
        "Solution_reading_time":24.75,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":146.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1259808393296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, Canada",
        "Answerer_reputation_count":44706.0,
        "Answerer_view_count":4356.0,
        "Challenge_adjusted_solved_time":4.5641338889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>After AWS Autopilot creates a model, How to use that model to the training data set offline?<\/p>\n<p>How to use that .tar.gz model file?<\/p>",
        "Challenge_closed_time":1637338122312,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637321691430,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1658782461116,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70034212",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":2.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":4.5641338889,
        "Challenge_title":"Model generated by AWS autopilot",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":125.0,
        "Challenge_word_count":28,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The .tar.gz file is a model artifact<\/p>\n<p>To create a model, you combine the algorithm container and the model artifact<\/p>\n<p>You can do so in the Console, under Inference &gt; Models &gt; Create Model<\/p>\n<p>What do you mean by &quot;How to use that model to the training data set offline?&quot;<\/p>\n<p>If you mean, &quot;run a batch transformation&quot;, then once you create a model, you can select the model in the console, then click 'Create batch transform job'<\/p>\n<p>If you want to do it locally, then you can use the SageMaker Python SDK in <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">Local Mode<\/a> and run the transform on your local computer (requires Docker)<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1637865568940,
        "Solution_link_count":1.0,
        "Solution_readability":17.3,
        "Solution_reading_time":9.31,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":108.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1432655047272,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":463.0,
        "Answerer_view_count":76.0,
        "Challenge_adjusted_solved_time":989.9238102778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I've been using Amazon Sagemaker Notebooks to build a pytorch model for an NLP task.\nI know you can use Sagemaker to train, deploy, hyper parameter tuning, and model monitoring.<\/p>\n<p>However, it looks like you have to create an inference endpoint in order to monitor the model's inference performance.<\/p>\n<p>I already have a EC2 instance setup to perform inference tasks on our model, which is currently on a development box and rather not use an endpoint to make<\/p>\n<p>Is it possible to use Sagemaker to train, run hyperparam tuning and model eval without creating an endpoint.<\/p>",
        "Challenge_closed_time":1604012509390,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600448783673,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63960011",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.6,
        "Challenge_reading_time":8.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":989.9238102778,
        "Challenge_title":"Using AWS Sagemaker for model performance without creating endpoint",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":494.0,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1285379271580,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":625.0,
        "Poster_view_count":110.0,
        "Solution_body":"<p>If you don't want to keep an inference endpoint up, one option is to use SageMaker Processing to run a job that takes your trained model and test dataset as input, performs inference and computes evaluation metrics, and saves them to S3 in a JSON file.<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.ipynb\" rel=\"nofollow noreferrer\">This Jupyter notebook example<\/a> steps through (1) preprocessing training and test data, (2) training a model, then (3) evaluating the model<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":18.3,
        "Solution_reading_time":8.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":103.6265825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have started working with <strong>AWS SageMaker<\/strong> recently with the examples provided by AWS. I used this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/deepar_electricity\/DeepAR-Electricity.ipynb\" rel=\"nofollow noreferrer\">example<\/a> (<strong>DeepAR<\/strong> Model) in order to forecast a time series. After training, a model artifacts file has been created in my S3 bucket. <\/p>\n\n<p><strong>My question:<\/strong> Is there a way to host that trained model in a own hosting environment? (client premises)<\/p>",
        "Challenge_closed_time":1561921541280,
        "Challenge_comment_count":0,
        "Challenge_created_time":1561548485583,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56771758",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":13.8,
        "Challenge_reading_time":8.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":103.6265825,
        "Challenge_title":"On-Premises Hosting of Trained DeepAR Model built on AWS SageMaker",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":379.0,
        "Challenge_word_count":71,
        "Platform":"Stack Overflow",
        "Poster_created_time":1439993560103,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lebanon",
        "Poster_reputation_count":155.0,
        "Poster_view_count":76.0,
        "Solution_body":"<p>Except SageMaker XGBoost, SageMaker built-in algorithms are not designed to be used out of Amazon. That does not mean that it's impossible, for example you can find here and there snippets peeking inside model artifacts (eg for <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/extending-amazon-sagemaker-factorization-machines-algorithm-to-predict-top-x-recommendations\/\" rel=\"nofollow noreferrer\">Factorization Machines<\/a> and <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_applying_machine_learning\/ntm_20newsgroups_topic_modeling\/ntm_20newsgroups_topic_model.ipynb\" rel=\"nofollow noreferrer\">Neural Topic Model<\/a>) but these things can be hacky and are usually not part of official service features. Regarding DeepAR specifically, the model was open-sourced couple weeks ago as part of <code>gluon-ts<\/code> python package (<a href=\"https:\/\/aws.amazon.com\/blogs\/opensource\/gluon-time-series-open-source-time-series-modeling-toolkit\/\" rel=\"nofollow noreferrer\">blog post<\/a>, <a href=\"https:\/\/gluon-ts.mxnet.io\/\" rel=\"nofollow noreferrer\">code<\/a>) so if you develop a model specifically for your own hosting environment I'd recommend to use that gluon-ts code in the MXNet container, so that you'll be able to open and read the artifact out of SageMaker.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":19.8,
        "Solution_reading_time":17.48,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":126.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":41.2473602778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello. Does anyone have a current diagram like the attached schematic on Cortana Intelligence?  This diagram seems to be outdated.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/97284-cortana-intelligence.png?platform=QnA\" alt=\"97284-cortana-intelligence.png\" \/>    <\/p>",
        "Challenge_closed_time":1621443861350,
        "Challenge_comment_count":0,
        "Challenge_created_time":1621295370853,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/398494\/cortana-intelligence-current-diagram",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":21.4,
        "Challenge_reading_time":4.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":41.2473602778,
        "Challenge_title":"Cortana Intelligence Current Diagram",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":28,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello,    <\/p>\n<p>I have seen a lot of similar version but I can't find any official date for it. Could you please share where\/ which conference you found this so that we can check with related team for more information?    <\/p>\n<p><img src=\"https:\/\/msdnshared.blob.core.windows.net\/media\/2016\/09\/Session-2-Cortana-Intelligence-Suite-Overview-1024x576.png\" alt=\"Session-2-Cortana-Intelligence-Suite-Overview-1024x576.png\" \/>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/97928-2555cas.jpg?platform=QnA\" alt=\"97928-2555cas.jpg\" \/>    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.3,
        "Solution_reading_time":7.84,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":53.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.0222919444,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Could wandb be used in the inference stage? I\u2019m specifically interested in collecting system metrics during inference.  I\u2019m working with DeepSpeed and torch profile and would to know if wandb could be useful in the inference as well.<\/p>",
        "Challenge_closed_time":1647296287748,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647292607497,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/could-wandb-be-used-in-the-inference\/2077",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.6,
        "Challenge_reading_time":3.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.0222919444,
        "Challenge_title":"Could wandb be used in the inference?",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":163.0,
        "Challenge_word_count":44,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/eneas-jr\">@eneas-jr<\/a>,<\/p>\n<p>Yes, wandb can definitely be used at inference time as well to record system metrics. <code>wandb<\/code> will record system metrics in between the calls to <code>wandb.init()<\/code> and <code>wandb.finish()<\/code>.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.8,
        "Solution_reading_time":4.05,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":34.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1351154914716,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2564.0,
        "Answerer_view_count":451.0,
        "Challenge_adjusted_solved_time":0.9837333334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I defined a training job:<\/p>\n<pre><code>job = aiplatform.AutoMLTextTrainingJob(...\n<\/code><\/pre>\n<p>then I created a model by running the job:<\/p>\n<pre><code>model = job.run(...\n<\/code><\/pre>\n<p>It worked fine but it is now the next day and the variable <code>model<\/code> was in a Jupyter notebook and no longer exists. I have tried to get it back with:<\/p>\n<pre><code>from google.cloud import aiplatform_v1beta1\n\ndef sample_get_model():\n    client = aiplatform_v1beta1.ModelServiceClient()\n\n    model_id=id_of_training_pipeline\n    name= f'projects\/{PROJECT}\/locations\/{REGION}\/models\/{model_id}'\n    \n    request = aiplatform_v1beta1.GetModelRequest(name=name)\n    response = client.get_model(request=request)\n    print(response)\n\nsample_get_model()\n<\/code><\/pre>\n<p>I have also tried the id of v1 of the model created in place of <code>id_of_training_pipeline<\/code> and I have tried <code>\/pipelines\/pipeline_id<\/code><\/p>\n<p>but I get:\n<code>E0805 15:12:36.784008212   28406 hpack_parser.cc:1234]       Error parsing metadata: error=invalid value key=content-type value=text\/html; charset=UTF-8<\/code><\/p>\n<p>(<code>PROJECT<\/code> and <code>REGION<\/code> are set correctly).<\/p>",
        "Challenge_closed_time":1659712728223,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659709186783,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73251212",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.4,
        "Challenge_reading_time":15.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.9837333334,
        "Challenge_title":"How do I retrieve a model in Vertex AI?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":45.0,
        "Challenge_word_count":119,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>Found <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/samples\/aiplatform-get-model-sample#aiplatform_get_model_sample-python\" rel=\"nofollow noreferrer\">this<\/a> Google code which works.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":31.0,
        "Solution_reading_time":2.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":9.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1542628542703,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":11.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":43.2314219444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed a Tensorflow-Model in SageMaker Studio following this tutorial:\n<a href=\"https:\/\/aws.amazon.com\/de\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/de\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/<\/a>\nThe Model needs a Multidimensional Array as input. Invoking it from the Notebook itself is working:<\/p>\n<pre><code>import numpy as np\nimport json\ndata = np.load(&quot;testValues.npy&quot;)\npred=predictor.predict(data)\n<\/code><\/pre>\n<p>But I wasnt able to invoke it from a boto 3 client using this code:<\/p>\n<pre><code>import json\nimport boto3\nimport numpy as np\nimport io\n \nclient = boto3.client('runtime.sagemaker')\ndatain = np.load(&quot;testValues.npy&quot;)\ndata=datain.tolist();\nresponse = client.invoke_endpoint(EndpointName=endpoint_name, Body=json.dumps(data))\nresponse_body = response['Body']\nprint(response_body.read())\n<\/code><\/pre>\n<p>This throws the Error:<\/p>\n<pre><code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from model with message &quot;{&quot;error&quot;: &quot;Unsupported Media Type: Unknown&quot;}&quot;.\n<\/code><\/pre>\n<p>I guess the reason is the json Media Type but i have no clue how to get it back in shape.\nI tried this:<a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/644\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/644<\/a> but it doesnt seem to change anything<\/p>",
        "Challenge_closed_time":1605773068536,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605617435417,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64875623",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":16.8,
        "Challenge_reading_time":21.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":43.2314219444,
        "Challenge_title":"Invoking an endpoint in AWS with a multidimensional array",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":550.0,
        "Challenge_word_count":147,
        "Platform":"Stack Overflow",
        "Poster_created_time":1542628542703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":11.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>This fixed it for me:\nThe Content Type was missing.<\/p>\n<pre><code>import json\nimport boto3\nimport numpy as np\nimport io\n\nclient = boto3.client('runtime.sagemaker',aws_access_key_id=..., aws_secret_access_key=...,region_name=...)\nendpoint_name = '...'\n\ndata = np.load(&quot;testValues.npy&quot;)\n\n\npayload = json.dumps(data.tolist())\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                                  ContentType='application\/json',\n                                   Body=payload)\nresult = json.loads(response['Body'].read().decode())\nres = result['predictions']\nprint(&quot;test&quot;)\n\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":18.7,
        "Solution_reading_time":7.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":38.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1285875105172,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Santa Cruz, CA, United States",
        "Answerer_reputation_count":4051.0,
        "Answerer_view_count":461.0,
        "Challenge_adjusted_solved_time":147.4295269444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Sagemaker and have a bunch of model.tar.gz files that I need to unpack and load in sklearn. I've been testing using list_objects with delimiter to get to the tar.gz files:<\/p>\n\n<pre><code>response = s3.list_objects(\nBucket = bucket,\nPrefix = 'aleks-weekly\/models\/',\nDelimiter = '.csv'\n)\n\n\nfor i in response['Contents']:\n    print(i['Key'])\n<\/code><\/pre>\n\n<p>And then I plan to extract with<\/p>\n\n<pre><code>import tarfile\ntf = tarfile.open(model.read())\ntf.extractall()\n<\/code><\/pre>\n\n<p>But how do I get to the actual tar.gz file from s3 instead of a some boto3 object? <\/p>",
        "Challenge_closed_time":1566337639600,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565806893303,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57500105",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":7.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":147.4295269444,
        "Challenge_title":"Python boto3 load model tar file from s3 and unpack it",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":4787.0,
        "Challenge_word_count":89,
        "Platform":"Stack Overflow",
        "Poster_created_time":1421343783700,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1387.0,
        "Poster_view_count":153.0,
        "Solution_body":"<p>You can download objects to files using <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.download_file\" rel=\"nofollow noreferrer\"><code>s3.download_file()<\/code><\/a>. This will make your code look like:<\/p>\n\n<pre><code>s3 = boto3.client('s3')\nbucket = 'my-bukkit'\nprefix = 'aleks-weekly\/models\/'\n\n# List objects matching your criteria\nresponse = s3.list_objects(\n    Bucket = bucket,\n    Prefix = prefix,\n    Delimiter = '.csv'\n)\n\n# Iterate over each file found and download it\nfor i in response['Contents']:\n    key = i['Key']\n    dest = os.path.join('\/tmp',key)\n    print(\"Downloading file\",key,\"from bucket\",bucket)\n    s3.download_file(\n        Bucket = bucket,\n        Key = key,\n        Filename = dest\n    )\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.7,
        "Solution_reading_time":9.49,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":171.2597222222,
        "Challenge_answer_count":2,
        "Challenge_body":"## Description\r\n\r\nI tried to load a KedroPipelineModel from mlflow, and I got a \"cannot pickle context artifacts\" error, which is due do the \r\n\r\n## Context\r\n\r\nI cannot load a previously saved KedroPipelineModel generated by pipeline_ml_factory.\r\n\r\n## Steps to Reproduce\r\n\r\nSave A KedroPipelineModel with a dataset that contains an object which cannot be deepcopied (for me, a keras tokenizer)\r\n\r\n## Expected Result\r\n\r\nThe model should be loaded\r\n\r\n## Actual Result\r\n\r\nAn error is raised\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used: 0.16.5 and 0.4.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Windows 10 & CentOS were tested\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n# Potential solution\r\n\r\nThe faulty line is:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/mlflow\/kedro_pipeline_model.py#L45",
        "Challenge_closed_time":1606599848000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605983313000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/122",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":12.1,
        "Challenge_reading_time":13.32,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":171.2597222222,
        "Challenge_title":"A KedroPipelineModel cannot be loaded from mlflow if its catalog contains non deepcopy-able DataSets",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":137,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Does removing the faulty line and using directly the initial_catalog make the model loadable again ? if Yes, we have two options :\r\n\r\n* We no longer deepcopy the initial_catalog\r\n* We copy each DataSet of the catalog with his own loader (for example, we use tf.keras.models.clone_model for keras model DataSet ...)\r\n\r\nKnowing that the `KedroPipelineModel` is intented to be used in a separated process (at inference-time), we can just remove the deepcopy part (there won't be a conflict with another function using the same catalog)\r\n After some investigation, the issues comes from the MLflowAbstractModelDataSet, and particularly the `self._mlflow_model_module` attribute which is a module and not deepcopiable by nature. I suggest to store it as a string, and have a property attribute to load the module on the fly.\r\n\r\nNote that this is a problem which occurs only when the DataSet is not deepcopiable (and not the underlying value the DataSet can load(), so we can quite safely assume that it should not occur often). If it does, we should consider a more radical solution among the ones you suggest.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.8,
        "Solution_reading_time":13.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":175.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1395737095150,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":81.0,
        "Answerer_view_count":18.0,
        "Challenge_adjusted_solved_time":154.6126952778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm using the SageMaker TensorFlow estimator for training, and specifying an output path for my model artifacts with the <code>output_path<\/code> argument, with a value of <code>s3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\/<\/code>. <\/p>\n\n<p>After model training, a directory named <code>&lt;training_job_name&gt;\/output<\/code> is created in the specified <code>output_path<\/code>. <\/p>\n\n<p>The issue I'm having is, the source code that's used for training is also uploaded to S3 by default, but instead of being placed in <code>s3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\/&lt;training_job_name&gt;\/source<\/code>, it's placed in <code>s3:\/\/&lt;bucket&gt;\/&lt;training_job_name&gt;\/source<\/code>. <\/p>\n\n<p>So how can I specify the S3 upload path for the training job's source code in order to make it use the bucket AND prefix name of <code>output_path<\/code>?<\/p>",
        "Challenge_closed_time":1558768756470,
        "Challenge_comment_count":0,
        "Challenge_created_time":1558212150767,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56202697",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":15.2,
        "Challenge_reading_time":11.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":154.6126952778,
        "Challenge_title":"SageMaker TensorFlow Estimator source code S3 upload path",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":645.0,
        "Challenge_word_count":107,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>Have you tried using the \u201ccode_location\u201d argument: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html<\/a> to specify the location for the source code?<\/p>\n\n<p>Below is a snippet code example that use code_location<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ncode-path = \"s3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\"\noutput-path = \"s3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\"\n\nabalone_estimator = TensorFlow(entry_point='abalone.py',\n                           role=role,\n                           framework_version='1.12.0',\n                           training_steps= 100, \n                           image_name=image,\n                           evaluation_steps= 100,\n                           hyperparameters={'learning_rate': 0.001},\n                           train_instance_count=1,\n                           train_instance_type='ml.c4.xlarge',\n                           code_location= code-path,\n                           output_path = output-path,\n                           base_job_name='my-job-name'\n                           )\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":22.2,
        "Solution_reading_time":11.38,
        "Solution_score_count":4.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":52.5838888889,
        "Challenge_answer_count":0,
        "Challenge_body":"## What\r\n\r\nWhen loading configs from wandb, the resulting HParams objects are not correct. This can be seen when attempting to load the model checkpoint with the given parameters (failure), or when comparing the object with the info panel for the run on wandb.\r\n\r\n## How to Reproduce\r\n\r\nLoad the configs:\r\n\r\n```python\r\nfrom wavenet import utils, model, train\r\n\r\nrun_path = 'purzelrakete\/feldberlin-wavenet\/21ei0tqc'\r\np, ptrain = utils.load_wandb_cfg(run_path)\r\np, ptrain = model.HParams(**p), train.HParams(**ptrain)\r\n```\r\n\r\nValidate against the run [on wandb](https:\/\/wandb.ai\/purzelrakete\/feldberlin-wavenet\/runs\/21ei0tqc\/overview?workspace=user-purzelrakete)\r\n\r\n## Acceptance Criteria\r\n\r\n- [x] Bug has been understood and fixed\r\n- [x] The same config given above can be loaded and is correct",
        "Challenge_closed_time":1624287268000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1624097966000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/feldberlin\/wavenet\/issues\/5",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":11.1,
        "Challenge_reading_time":10.49,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":35.0,
        "Challenge_repo_star_count":3.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":52.5838888889,
        "Challenge_title":"Loading configs from wandb yields incorrect parameters",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":99,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1365624651363,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":335.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":593.8890980556,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I have created two models in azure ml studio and i want to download those models.<\/p>\n\n<p>Is it possible to download train and score models from azure ml studio?<\/p>",
        "Challenge_closed_time":1484356723536,
        "Challenge_comment_count":1,
        "Challenge_created_time":1482218722783,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41236871",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":5.9,
        "Challenge_reading_time":2.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":593.8890980556,
        "Challenge_title":"How to download the trained models from Azure machine studio?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":7873.0,
        "Challenge_word_count":38,
        "Platform":"Stack Overflow",
        "Poster_created_time":1482218454343,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":73.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Models can be trained, scored, saved, and run in AzureML studio, but can't downloaded to your local machine. There's no way to do anything with a model outside of AzureML.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":2.19,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":30.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1569638810883,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5255.0,
        "Answerer_view_count":306.0,
        "Challenge_adjusted_solved_time":5.2755416667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am fairly new to AWS and Sagemaker and have decided to follow some of the tutorials Amazon has to familiarize myself with it. I've been following this one (<a href=\"https:\/\/aws.amazon.com\/getting-started\/hands-on\/semantic-content-recommendation-system-amazon-sagemaker\/5\/\" rel=\"nofollow noreferrer\">tutorial<\/a>) and I've realized that it's an older tutorial using Sagemaker v1. I've been able to look up and change whatever is needed for the tutorial to work in v2 but I became stuck at this part for storing the training data in a S3 bucket to deploy the model.<\/p>\n<pre><code>import io\nimport sagemaker.amazon.common as smac\n\nprint('train_features shape = ', predictions.shape)\nprint('train_labels shape = ', labels.shape)\nbuf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(buf, predictions, labels)\nbuf.seek(0)\n\nbucket = BUCKET\nprefix = PREFIX\nkey = 'knn\/train'\nfname = os.path.join(prefix, key)\nprint(fname)\nboto3.resource('s3').Bucket(bucket).Object(fname).upload_fileobj(buf)\ns3_train_data = 's3:\/\/{}\/{}\/{}'.format(bucket, prefix, key)\nprint('uploaded training data location: {}'.format(s3_train_data))\n<\/code><\/pre>\n<p>It returns this error<\/p>\n<pre><code>NameError Traceback (most recent call \nlast)\n&lt;ipython-input-20-9e52dd949332&gt; in &lt;module&gt;\n 3\n 4\n----&gt; 5 print('train_features shape = ', predictions.shape)\n 6 print('train_labels shape = ', labels.shape)\n 7 buf = io.BytesIO()\nNameError: name 'predictions' is not defined\n<\/code><\/pre>\n<p>I'm curious as to why this would have worked in Sagemaker v1 and not v2 if predictions is not defined and if anyone could point me in the right direction for correcting this.<\/p>\n<p>Thanks.<\/p>",
        "Challenge_closed_time":1623033557140,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623014565190,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1623040885867,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67863816",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":22.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":5.2755416667,
        "Challenge_title":"semantic content recommendation system with Amazon SageMaker, storing in S3",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":76.0,
        "Challenge_word_count":198,
        "Platform":"Stack Overflow",
        "Poster_created_time":1623013767550,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>It looks like they've left some of the code out, or changed the terminology and left in predictions by accident. predictions is an object that is defined on this page <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-test-model.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-test-model.html<\/a><\/p>\n<p>You'll have to work out what predictions is in your case.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.4,
        "Solution_reading_time":5.44,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":45.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":4.4848666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to training TensorFlow model on AWS Sagemaker.\nI created container with external lib for that (Use Your Own Algorithms or Models with Amazon SageMaker).<\/p>\n\n<p>we run a training job with TensorFlow API<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.tensorflow import TensorFlow\nestimator = TensorFlow(\n  entry_point=\"entry.py\",             # entry script\n  role=role,\n  framework_version=\"1.13.0\",   \n  py_version='py3',\n  hyperparameters=hyperparameters,\n  train_instance_count=1,                   # \"The number of GPUs instances to use\"\n  train_instance_type=train_instance_type,\n  image_name=my_image\n\n)\nestimator.fit({'train': train_s3, 'eval': eval_s3})\n<\/code><\/pre>\n\n<p>and got an error:<\/p>\n\n<pre><code>09:06:46\n2019-07-23 09:06:45,463 INFO - root - running container entrypoint \uf141\n09:06:46\n2019-07-23 09:06:45,463 INFO - root - starting train task \uf141\n09:06:46\n2019-07-23 09:06:45,476 INFO - container_support.training - Training starting \uf141\n09:06:46\n2019-07-23 09:06:45,479 ERROR - container_support.training - uncaught exception during training: No module named 'tf_container'\n\uf141\n09:06:46\nTraceback (most recent call last): File \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/environment.py\", line 136, in load_framework return importlib.import_module('mxnet_container') File \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File \"&lt;frozen importlib._bootstrap&gt;\", line 994, in _gcd_i \uf141\n09:06:46\nModuleNotFoundError: No module named 'mxnet_container'\n\uf141\n09:06:46\nDuring handling of the above exception, another exception occurred:\n\uf141\n09:06:46\nTraceback (most recent call last): File \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/training.py\", line 35, in start fw = TrainingEnvironment.load_framework() File \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/environment.py\", line 138, in load_framework return importlib.import_module('tf_container') File \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, \uf141\n09:06:46\nModuleNotFoundError: No module named 'tf_container'\n<\/code><\/pre>\n\n<p>What can I do to solve this issue? how can I debug this case?<\/p>",
        "Challenge_closed_time":1563901268600,
        "Challenge_comment_count":0,
        "Challenge_created_time":1563885123080,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57164290",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":29.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":4.4848666667,
        "Challenge_title":"Training with TensorFlow on Sagemaker No module named 'tf_container'",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1191.0,
        "Challenge_word_count":206,
        "Platform":"Stack Overflow",
        "Poster_created_time":1471952551663,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I'm guessing that you used your own TF container, not the SageMaker one at <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container<\/a><\/p>\n\n<p>If that's the case, your container is missing the support code needed to use the TensorFlow estimator ('tf_container' package).<\/p>\n\n<p>The solution is to start from the SageMaker container, customize it, push it back to ECR and pass the image name to the SageMaker estimator with the 'image_name' parameter.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.4,
        "Solution_reading_time":7.04,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":16.3876544444,
        "Challenge_answer_count":1,
        "Challenge_body":"I have sagemaker xgboost project template \"build, train, deploy\" working, but I'd like to modify if to use tensorflow instead of xgboost.  First up I was just trying to change the `abalone` folder to `topic` to reflect the data we are working with.\n\nI was experimenting with trying to change the `topic\/pipeline.py` file like so\n\n\n```\n    image_uri = sagemaker.image_uris.retrieve(\n        framework=\"tensorflow\",\n        region=region,\n        version=\"1.0-1\",\n        py_version=\"py3\",\n        instance_type=training_instance_type,\n    )\n```\n\ni.e. just changing the framework name from \"xgboost\" to \"tensorflow\", but then when I run the following from a notebook:\n\n\n```\nfrom pipelines.topic.pipeline import get_pipeline\n\n\npipeline = get_pipeline(\n    region=region,\n    role=role,\n    default_bucket=default_bucket,\n    model_package_group_name=model_package_group_name,\n    pipeline_name=pipeline_name,\n)\n```\n\nI get the following error\n\n```\nValueError                                Traceback (most recent call last)\n<ipython-input-5-6343f00c3471> in <module>\n      7     default_bucket=default_bucket,\n      8     model_package_group_name=model_package_group_name,\n----> 9     pipeline_name=pipeline_name,\n     10 )\n\n~\/topic-models-no-monitoring-p-rboparx6tdeg\/sagemaker-topic-models-no-monitoring-p-rboparx6tdeg-modelbuild\/pipelines\/topic\/pipeline.py in get_pipeline(region, sagemaker_project_arn, role, default_bucket, model_package_group_name, pipeline_name, base_job_prefix, processing_instance_type, training_instance_type)\n    188         version=\"1.0-1\",\n    189         py_version=\"py3\",\n--> 190         instance_type=training_instance_type,\n    191     )\n    192     tf_train = Estimator(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/utilities.py in wrapper(*args, **kwargs)\n    197                 logger.warning(warning_msg_template, arg_name, func_name, type(value))\n    198                 kwargs[arg_name] = value.default_value\n--> 199         return func(*args, **kwargs)\n    200 \n    201     return wrapper\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/image_uris.py in retrieve(framework, region, version, py_version, instance_type, accelerator_type, image_scope, container_version, distribution, base_framework_version, training_compiler_config, model_id, model_version, tolerate_vulnerable_model, tolerate_deprecated_model, sdk_version, inference_tool, serverless_inference_config)\n    152             if inference_tool == \"neuron\":\n    153                 _framework = f\"{framework}-{inference_tool}\"\n--> 154         config = _config_for_framework_and_scope(_framework, image_scope, accelerator_type)\n    155 \n    156     original_version = version\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/image_uris.py in _config_for_framework_and_scope(framework, image_scope, accelerator_type)\n    277         image_scope = available_scopes[0]\n    278 \n--> 279     _validate_arg(image_scope, available_scopes, \"image scope\")\n    280     return config if \"scope\" in config else config[image_scope]\n    281 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/image_uris.py in _validate_arg(arg, available_options, arg_name)\n    443             \"Unsupported {arg_name}: {arg}. You may need to upgrade your SDK version \"\n    444             \"(pip install -U sagemaker) for newer {arg_name}s. Supported {arg_name}(s): \"\n--> 445             \"{options}.\".format(arg_name=arg_name, arg=arg, options=\", \".join(available_options))\n    446         )\n    447 \n\nValueError: Unsupported image scope: None. You may need to upgrade your SDK version (pip install -U sagemaker) for newer image scopes. Supported image scope(s): eia, inference, training.\n```\n\nI was skeptical that the upgrade suggested by the error message would fix this, but gave it a try:\n\n```\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npipelines 0.0.1 requires sagemaker==2.93.0, but you have sagemaker 2.110.0 which is incompatible.\n```\n\nSo that seems like I can't upgrade sagemaker without changing pipelines, and it's not clear that's the right thing to do - like this project template may be all designed around those particular ealier libraries.\n\nBut so is it that the \"framework\" name should be different, e.g. \"tf\"?  Or is there some other setting that needs changing in order to allow me to get a tensorflow pipeline ...?\n\nHowever I find that if I use the existing `abalone\/pipeline.py` file I can change the framework to \"tensorflow\" and there's no problem running that particular step in the notebook.\n\nI've searched all the files in the project to try and find any dependency on the `abalone` folder name, and the closest I came was in `codebuild-buildspec.yml` but that hasn't helped.\n\nHas anyone else successfully changed the folder name from `abalone` to something else, or am I stuck with `abalone` if I want to make progress?\n\nMany thanks in advance\n\np.s. is there a slack community for sagemaker studio anywhere?\n\np.p.s. I have tried changing all instances of the term \"Abalone\" to \"Topic\" within the `topic\/pipeline.py` file (matching case as appropriate) to no avail\n\np.p.p.s. I discovered that I can get an error free run of getting the pipeline from a unit test:\n\n\n```\nimport pytest\n\nfrom pipelines.topic.pipeline import *\n\nregion = 'eu-west-1'\nrole = 'arn:aws:iam::398371982844:role\/SageMakerExecutionRole'\ndefault_bucket = 'sagemaker-eu-west-1-398371982844'\nmodel_package_group_name = 'TopicModelPackageGroup-Example'\npipeline_name = 'TopicPipeline-Example'\n\ndef test_pipeline():\n    pipeline = get_pipeline(\n        region=region,\n        role=role,\n        default_bucket=default_bucket,\n        model_package_group_name=model_package_group_name,\n        pipeline_name=pipeline_name,\n    )\n```\nand strangely if I go to a different copy of the notebook, everything runs fine, there ... so I have two seemingly identical ipynb notebooks, and in one of them when I switch to trying to get a topic pipeline I get the above error, and in the other, I get no error at all, very strange\n\np.p.p.p.s.  I also notice that `conda list` returns very different results depending on whether I run it in the notebook or the terminal ... but the conda list results are identical for the two notebooks ...",
        "Challenge_closed_time":1664450661264,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664391665708,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668489176743,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUAL9Vn9abQ6KKCs2ASwwmzg\/adjusting-sagemaker-xgboost-project-to-tensorflow-or-even-just-different-folder-name",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.6,
        "Challenge_reading_time":76.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":45,
        "Challenge_solved_time":16.3876544444,
        "Challenge_title":"adjusting sagemaker xgboost project to tensorflow (or even just different folder name)",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":167.0,
        "Challenge_word_count":666,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi! I see two parts in your question:\n1. How to use Tensorflow in a SageMaker estimator to train and deploy a model\n2. How to adapt a SageMaker MLOps template to your data and code\n\nTensorflow estimator is slightly different from XGBoost estimator, and the easiest way to work with it is not by using `sagemaker.image_uris.retrieve(framework=\"tensorflow\",...)`, but to use [`sagemaker.tensorflow.TensorFlow` estimator instead](https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html).\n\nThese are the two examples, which will be useful for you:\n* [Train an MNIST model with TensorFlow](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/frameworks\/tensorflow\/get_started_mnist_train.ipynb)\n* [Deploy a Trained TensorFlow V2 Model](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/frameworks\/tensorflow\/get_started_mnist_deploy.ipynb)\n\nAs for updating the MLOps template, I recommend you to go through the comprehensive [self-service lab on SageMaker Pipelines](https:\/\/catalog.us-east-1.prod.workshops.aws\/workshops\/63069e26-921c-4ce1-9cc7-dd882ff62575\/en-US\/lab6).\n\nIt shows you how to update the source directory from `abalone` to `customer_churn`. In your case it will be the `topic`.\n\nP. S. As for a Slack channel, to my best knowledge, this re:Post forum now is the best place to ask any questions on Amazon SageMaker, including SageMaker Studio.",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1664450661264,
        "Solution_link_count":4.0,
        "Solution_readability":14.7,
        "Solution_reading_time":18.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":155.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":24.0739286111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Training a ml model with mlflow in azure environment.<\/p>\n<pre><code>import mlflow\nfrom mlflow import MlflowClient\nfrom azureml.core import Experiment, Workspace\n\nexperiment_name = 'housing-lin-mlflow'\n\nexperiment = Experiment(ws, experiment_name)\n\nruns = mlflow.search_runs(experiment_ids=[ experiment.id ])\n\n<\/code><\/pre>\n<p>While fetching runs from search_runs getting this error :<\/p>\n<pre><code>RestException: BAD_REQUEST: For input string: &quot;5b649b3c-3b8f-497a-bb4f&quot;\n<\/code><\/pre>\n<p>MLflow version : 1.28.0\nIn Azure studio jobs have been created and successfully run.<\/p>",
        "Challenge_closed_time":1661603882123,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661517215980,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1661625379892,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73501103",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":8.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":24.0739286111,
        "Challenge_title":"Getting Bad request while searching run in mlflow",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":56.0,
        "Challenge_word_count":65,
        "Platform":"Stack Overflow",
        "Poster_created_time":1582101477803,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":171.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>The bad request in MLFlow after successful running the job is because of not giving proper API permissions for the application.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Search for <strong>MLFLOW<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Scroll down<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/s50AL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s50AL.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Click on View API Permissions<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Under API permissions, assign the permissions according to the application running region and requirements. Checkout the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-models-mlflow\" rel=\"nofollow noreferrer\">document<\/a> for further information.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":9.0,
        "Solution_readability":17.0,
        "Solution_reading_time":16.36,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":94.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":136.9105555556,
        "Challenge_answer_count":1,
        "Challenge_body":"A data scientist  is looking to host a Tensorflow model in SageMaker and process low volume streaming event data (~2-3 per second) to collect inferences about each event. Data scientist is looking at having the SageMaker inference model plugged in as a Kinesis Data Analytics Application but Kinesis Data Analytics currently only supports SQL or Flink.\n\nOne option to set up an ECS or Lambda service to consume data from Kinesis or SNS and invoke the SageMaker inference endpoint per message, but  if there is a more automated and optimal solution available for these kind of workflows.\n\nIt is not possible to pass multiple requests currently to a SageMaker endpoint, yet Tensorflow models tend to perform much better on batches of data rather than multiple single invocations so some windowing would be beneficial. Ideally the client would want to react to an inference within 10-15 seconds of the event being processed so an S3 based batch approach is probably too slow.\n\nIs there anything you can recommend for handling this sort of workload?",
        "Challenge_closed_time":1600149573000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599656695000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668524155875,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU6E1eYARES123bRYAe2B0Ag\/automated-streaming-integration-and-multiple-requests-for-sagemaker-endpoint",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":13.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":136.9105555556,
        "Challenge_title":"Automated streaming integration and multiple requests for SageMaker endpoint",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":212.0,
        "Challenge_word_count":181,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"To build integration between SageMaker endpoints and Kinesis Data Application use this blog - https:\/\/aws.amazon.com\/blogs\/architecture\/realtime-in-stream-inference-kinesis-sagemaker-flink\/. It help  to setup serverless service to invoke the SageMaker inference endpoint.\n\nTo  use batching. The Tensorflow documentation mentions the following:\n\n - [This link][1] mentions that you can include multiple instances in your predict request (or multiple examples in classify\/regress requests) to get multiple prediction results in one request to your Endpoint.\n - [This link][2] mentions that you can configure SageMaker TensorFlow Serving Container to batch multiple records together before performing an inference\n\nYou would still have to handle the logic internally in ECS\/Lambda to control how many records you consume from your stream in one batch, but at least you will be able to infer on the whole batch on the SageMaker endpoint end based on the above.\n\n  [1]: https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/deploying_tensorflow_serving.html#making-predictions-against-a-sagemaker-endpoint\n  [2]: https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container#enabling-batching",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1611341005824,
        "Solution_link_count":3.0,
        "Solution_readability":18.0,
        "Solution_reading_time":15.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":135.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.27,
        "Challenge_answer_count":1,
        "Challenge_body":"Which Amazon SageMaker built-in algorithms support checkpointing? In the [documentation][1] it says that:\n\n> SageMaker built-in algorithms and marketplace algorithms that do not checkpoint are currently limited to a `MaxWaitTimeInSeconds` of 3600 seconds (60 minutes).\n\nHowever, in the algorithms I don't find any pointer to \"checkpoint\" or \"spot\". Can you help me out?\n\n  [1]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-managed-spot-training.html",
        "Challenge_closed_time":1593611388000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593596016000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1667926133912,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QURbWeXcwDT8i4dvXKE4HZXg\/amazon-sagemaker-built-in-algorithms-and-spot-checkpointing",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":6.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":4.27,
        "Challenge_title":"Amazon SageMaker Built-in algorithms and Spot checkpointing",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":55.0,
        "Challenge_word_count":60,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This is the best resource that I've found to clarify this:\n\nhttps:\/\/aws.amazon.com\/blogs\/aws\/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs\/\n\n> Built-in algorithms: computer vision algorithms support checkpointing (Object Detection, Semantic Segmentation, and very soon Image Classification). As they tend to train on large data sets and run for longer than other algorithms, they have a higher likelihood of being interrupted. Other built-in algorithms do not support checkpointing for now.\n\nAlso:\n\n> Please note that TensorFlow uses checkpoints by default. For other frameworks, you\u2019ll find examples in our sample notebooks and in the documentation.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1614311552175,
        "Solution_link_count":1.0,
        "Solution_readability":12.3,
        "Solution_reading_time":8.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":84.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":117.2208333333,
        "Challenge_answer_count":8,
        "Challenge_body":"### System Info\r\n\r\n```shell\r\ntransformer: 4.17.0\r\ntorch: 1.10.2\r\n\r\nPlatform: Sagemaker Deep Learning Container\r\n```\r\n\r\n\r\n### Who can help?\r\n\r\n@NielsRogge\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nThe error only comes when training on Sagemaker using Huggingface.\r\n\r\nScripts to start training on Sagemaker:\r\n\r\nFolder organization:\r\n```\r\n.\/\r\n----sg_training.py\r\n----scripts\r\n-------requirements.txt\r\n-------train.py \r\n```\r\n\r\nsg_training.py:\r\n```\r\nimport boto3\r\nimport sagemaker\r\nfrom sagemaker.huggingface import HuggingFace\r\n\r\nif __name__ == \"__main__\":\r\n    iam_client = boto3.client(...)\r\n\r\n    role = iam_client.get_role(...)['Role']['Arn']\r\n    sess = sagemaker.Session()\r\n\r\n    sagemaker_session_bucket = 's3-sagemaker-session'\r\n\r\n    hyperparameters = {'epochs': 20,\r\n                       'train_batch_size': 1,\r\n                       'model_name': \"microsoft\/layoutxlm-base\",\r\n                       'output_dir': '\/opt\/ml\/model\/',\r\n                       'checkpoints': '\/opt\/ml\/checkpoints\/',\r\n                       'combine_train_val': True,\r\n                       'exp_tracker': \"all\",\r\n                       'exp_name': 'Sagemaker Training'\r\n                       }\r\n\r\n    huggingface_estimator = HuggingFace(entry_point='train.py',\r\n                                        source_dir='scripts',\r\n                                        instance_type='ml.p3.2xlarge',\r\n                                        instance_count=1,\r\n                                        role=role,\r\n                                        transformers_version='4.17.0',\r\n                                        pytorch_version='1.10.2',\r\n                                        py_version='py38',\r\n                                        hyperparameters=hyperparameters,\r\n                                        environment={'HF_TASK': 'text-classification'},\r\n                                        code_location='s3:\/\/dummy_code_location')\r\n\r\n    huggingface_estimator.fit()\r\n```\r\n\r\nEntrypoint scripts folder:\r\n\r\n\r\nrequirements.txt:\r\n```\r\ngit+https:\/\/github.com\/facebookresearch\/detectron2.git\r\n```\r\n\r\ntrain.py:\r\n```\r\nimport argparse\r\nimport logging\r\nimport os\r\nimport sys\r\n\r\nfrom transformers import LayoutLMv2ForSequenceClassification\r\n\r\n\r\ndef run():\r\n    model = LayoutLMv2ForSequenceClassification.from_pretrained('microsoft\/layoutxlm-base',\r\n                                                                num_labels=5)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--epochs\", type=int, default=3)\r\n    parser.add_argument(\"--exp_name\", type=str, default=\"Sagemaker Training\")\r\n    parser.add_argument(\"--train-batch-size\", type=int, default=2)\r\n    parser.add_argument(\"--eval-batch-size\", type=int, default=1)\r\n    parser.add_argument(\"--warmup_steps\", type=int, default=500)\r\n    parser.add_argument(\"--model_name\", type=str)\r\n    parser.add_argument(\"--learning_rate\", type=str, default=1e-5)\r\n    parser.add_argument(\"--combine_train_val\", type=bool, default=False)\r\n    # Data, model, and output directories\r\n    parser.add_argument(\"--output-data-dir\", type=str, default=os.environ[\"SM_OUTPUT_DATA_DIR\"])\r\n    parser.add_argument(\"--checkpoints\", type=str, default=\"\/opt\/ml\/checkpoints\")\r\n    parser.add_argument(\"--model-dir\", type=str, default='\/opt\/ml\/code\/model')\r\n    parser.add_argument(\"--n_gpus\", type=str, default=os.environ[\"SM_NUM_GPUS\"])\r\n    args, _ = parser.parse_known_args()\r\n\r\n    logger = logging.getLogger(__name__)\r\n    logging.basicConfig(\r\n        level=logging.getLevelName(\"INFO\"),\r\n        handlers=[logging.StreamHandler(sys.stdout)],\r\n        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\r\n    )\r\n\r\n    run()\r\n\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\n```shell\r\nHere the log on the error from AWS Cloud Watch:\r\n\r\nInvoking script with the following command:\r\n\/opt\/conda\/bin\/python3.8 train.py --checkpoints \/opt\/ml\/checkpoints\/ --combine_train_val True --epochs 20 --exp_name Sagemaker_Training_doc_cls --exp_tracker all --model_name microsoft\/layoutxlm-base --output_dir \/opt\/ml\/model\/ --train_batch_size 1\r\nTraceback (most recent call last):\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2777, in _get_module\r\nreturn importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"\/opt\/conda\/lib\/python3.8\/importlib\/__init__.py\", line 127, in import_module\r\nreturn _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\nFile \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\nFile \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\nFile \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\r\nFile \"<frozen importlib._bootstrap_external>\", line 848, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nFile \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/models\/layoutlmv2\/modeling_layoutlmv2.py\", line 48, in <module>\r\nfrom detectron2.modeling import META_ARCH_REGISTRY\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/detectron2\/modeling\/__init__.py\", line 2, in <module>\r\nfrom detectron2.layers import ShapeSpec\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/detectron2\/layers\/__init__.py\", line 2, in <module>\r\nfrom .batch_norm import FrozenBatchNorm2d, get_norm, NaiveSyncBatchNorm, CycleBatchNormList\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/detectron2\/layers\/batch_norm.py\", line 4, in <module>\r\n    from fvcore.nn.distributed import differentiable_all_reduce\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/__init__.py\", line 4, in <module>\r\n    from .focal_loss import (\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/focal_loss.py\", line 52, in <module>\r\n    sigmoid_focal_loss_jit: \"torch.jit.ScriptModule\" = torch.jit.script(sigmoid_focal_loss)\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/jit\/_script.py\", line 1310, in script\r\nfn = torch._C._jit_script_compile(\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/jit\/_recursive.py\", line 838, in try_compile_fn\r\nreturn torch.jit.script(fn, _rcb=rcb)\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/jit\/_script.py\", line 1310, in script\r\nfn = torch._C._jit_script_compile(\r\nRuntimeError: \r\nundefined value has_torch_function_variadic:\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/utils\/smdebug.py\", line 2962\r\n         >>> loss.backward()\r\n    \"\"\"\r\n    if has_torch_function_variadic(input, target, weight, pos_weight):\r\n       ~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n        return handle_torch_function(\r\n            binary_cross_entropy_with_logits,\r\n'binary_cross_entropy_with_logits' is being compiled since it was called from 'sigmoid_focal_loss'\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/focal_loss.py\", line 36\r\n    targets = targets.float()\r\n    p = torch.sigmoid(inputs)\r\n    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n    p_t = p * targets + (1 - p) * (1 - targets)\r\n    loss = ce_loss * ((1 - p_t) ** gamma)\r\nThe above exception was the direct cause of the following exception:\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 6, in <module>\r\nfrom transformers import LayoutLMv2ForSequenceClassification\r\n  File \"<frozen importlib._bootstrap>\", line 1039, in _handle_fromlist\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2768, in __getattr__\r\nvalue = getattr(module, name)\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2767, in __getattr__\r\nmodule = self._get_module(self._class_to_module[name])\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2779, in _get_module\r\nraise RuntimeError(\r\nRuntimeError: Failed to import transformers.models.layoutlmv2.modeling_layoutlmv2 because of the following error (look up to see its traceback):\r\nundefined value has_torch_function_variadic:\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/utils\/smdebug.py\", line 2962\r\n         >>> loss.backward()\r\n    \"\"\"\r\n    if has_torch_function_variadic(input, target, weight, pos_weight):\r\n       ~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n        return handle_torch_function(\r\n            binary_cross_entropy_with_logits,\r\n'binary_cross_entropy_with_logits' is being compiled since it was called from 'sigmoid_focal_loss'\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/focal_loss.py\", line 36\r\n    targets = targets.float()\r\n    p = torch.sigmoid(inputs)\r\n    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n    p_t = p * targets + (1 - p) * (1 - targets)\r\n    loss = ce_loss * ((1 - p_t) ** gamma)\r\n\r\n```\r\n```\r\n",
        "Challenge_closed_time":1656429784000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656007789000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17855",
        "Challenge_link_count":1,
        "Challenge_participation_count":8,
        "Challenge_readability":16.7,
        "Challenge_reading_time":107.62,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17219.0,
        "Challenge_repo_issue_count":20692.0,
        "Challenge_repo_star_count":76135.0,
        "Challenge_repo_watch_count":860.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":81,
        "Challenge_solved_time":117.2208333333,
        "Challenge_title":"LayoutLMv2 training on sagemaker error: undefined value has_torch_function_variadic",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":580,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"cc @philschmid (hope I am tagging correctly) @Natlem could you try adding `debugger_hook_config=False` to the `HuggingFace` estimator? \r\n\r\n```python\r\n    huggingface_estimator = HuggingFace(entry_point='train.py',\r\n                                        source_dir='scripts',\r\n                                        instance_type='ml.p3.2xlarge',\r\n                                        instance_count=1,\r\n                                        role=role,\r\n                                        transformers_version='4.17.0',\r\n                                        pytorch_version='1.10.2',\r\n                                        py_version='py38',\r\n                                        hyperparameters=hyperparameters,\r\n                                        environment={'HF_TASK': 'text-classification'},\r\n                                        code_location='s3:\/\/dummy_code_location',\r\n                                        debugger_hook_config=False,\r\n)\r\n``` Hi @philschmid ,\r\n\r\nAdded the `debugger_hook_config=False`, the error is gone now. Thanks ! Awesome, closing the issue.  Feel free to reopen if you have more issues. @Natlem i forwarded the error to the AWS team to be able use the debugger soon.  @philschmid  Thanks ! @philschmid do you have any idea why this solves the problem? Is it documented by AWS anywhere?\r\n\r\nSagemaker Debugger has cost me multiple days of time in the mysterious problems it produces. Far more than anything else on Sagemaker. I posted an issue on awslabs about this awhile back and never got a reply. I would really like to know what is going on here\r\n\r\n**For anyone encountering this while using a HyperparameterTuner**\r\nPassing `debugger_hook_config=False` in the `Estimator` will not the solve the problem. Further, passing `environment={'USE_SMDEBUG':0}` also will not solve the problem. Somehow these settings never make it to a tuner's constituent training jobs.\r\n\r\nThe only way to solve it is to set `ENV USE_SMDEBUG=\"0\"` in the docker container that will be running the constituent training jobs. > Somehow these settings never make it to a tuner's constituent training jobs.\r\n\r\nAre you using the `HuggingFace` estimator or the `HyperparameterTuner`",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.2,
        "Solution_reading_time":22.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":223.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1595479476676,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Massachusetts, USA",
        "Answerer_reputation_count":246.0,
        "Answerer_view_count":21.0,
        "Challenge_adjusted_solved_time":538.3183666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use API workflow (python code) to find a model version that has the best metric (for instance, \u201caccuracy\u201d) among several model versions. I understand we can use web UI to do so, but I would love to write python code to achieve this. Could someone help me?<\/p>",
        "Challenge_closed_time":1595480605940,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593542659820,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62664183",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":4.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":538.3183666667,
        "Challenge_title":"MLflow: find model version with best metric using python code",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":445.0,
        "Challenge_word_count":60,
        "Platform":"Stack Overflow",
        "Poster_created_time":1419619351727,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":125.0,
        "Poster_view_count":16.0,
        "Solution_body":"<pre><code>import mlflow \nclient = mlflow.tracking.MlflowClient()\nruns = client.search_runs(&quot;my_experiment_id&quot;, &quot;&quot;, order_by=[&quot;metrics.rmse DESC&quot;], max_results=1)\nbest_run = runs[0]\n<\/code><\/pre>\n<p><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.search_runs\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.search_runs<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":29.4,
        "Solution_reading_time":6.76,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":17.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1577919980176,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hamburg, Germany",
        "Answerer_reputation_count":5588.0,
        "Answerer_view_count":398.0,
        "Challenge_adjusted_solved_time":171.7806,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I developed an ANN tool by using pycharm\/tensorflow on my own computer. I uploaded the h5 and json files to Amazon Sagemaker by creating a Notebook Instance. I was finally able to successfully create an endpoint and make it work. The following code in Notebook Instance -Jupyter works:<\/p>\n<pre><code>import json\nimport boto3\nimport numpy as np\nimport io\nimport sagemaker\nfrom sagemaker.tensorflow.model import TensorFlowModel\nclient = boto3.client('runtime.sagemaker')\ndata = np.random.randn(1,6).tolist()\nendpoint_name = 'sagemaker-tensorflow-**********'\nresponse = client.invoke_endpoint(EndpointName=endpoint_name, Body=json.dumps(data))\nresponse_body = response['Body']\nprint(response_body.read())\n<\/code><\/pre>\n<p>However, the problem occurs when I created a lambda function and call the endpoint from there. The input should be a row of 6 features -that is a 1-by-6 vector. I enter the following input into lambda {&quot;data&quot;:&quot;1,1,1,1,1,1&quot;} and it gives me the following error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/var\/task\/lambda_function.py&quot;, line 20, in lambda_handler\n    Body=payload)\n  File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 316, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 635, in _make_api_call\n    raise error_class(parsed_response, operation_name)\n<\/code><\/pre>\n<p>I think the problem is that the input needs to be 1-by-6 instead of 6-by-1 and I don't know how to do that.<\/p>",
        "Challenge_closed_time":1599633575463,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599015165303,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63698011",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":20.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":171.7806,
        "Challenge_title":"AWS Notebook Instance is working but Lambda is not accepting the input",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":51.0,
        "Challenge_word_count":187,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369539919747,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":311.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>I assume the content type you specified is <code>text\/csv<\/code>, so try out:<\/p>\n<pre><code>{&quot;data&quot;: [&quot;1,1,1,1,1,1&quot;]}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.3,
        "Solution_reading_time":2.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":15.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":49.0995572222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm building a Scikit-learn model on Sagemaker.<\/p>\n\n<p>I'd like to reference the data used in training in my <code>predict_fn<\/code>. (Instead of the indices returned from NNS, I'd like to return the names and data of each neighbor.)<\/p>\n\n<p>I know this can be done by writing\/reading from S3, as in <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform\/<\/a> , but was wondering if there were more elegant solutions.<\/p>\n\n<p>Are there other ways to make the data used in the training job available to the prediction function?<\/p>\n\n<p>Edit: Using the advice from the accepted solution I was able to pass data as a dict.<\/p>\n\n<pre><code>model = nn.fit(train_data)\n\nmodel_dict = {\n   \"model\": model,\n   \"reference\": train_data\n}\n\njoblib.dump(model_dict, path)\n<\/code><\/pre>\n\n<p>predict_fn:<\/p>\n\n<pre><code>def predict_fn(input_data, model_dict):\n   model = model_dict[\"model\"]\n   reference = model_dict[\"reference\"]\n<\/code><\/pre>",
        "Challenge_closed_time":1581723595003,
        "Challenge_comment_count":0,
        "Challenge_created_time":1581546836597,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1582837925156,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60197897",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":16.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":49.0995572222,
        "Challenge_title":"Does Sagemaker pass any data other than the model itself between training and prediction steps?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":140.0,
        "Challenge_word_count":130,
        "Platform":"Stack Overflow",
        "Poster_created_time":1539701586583,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":623.0,
        "Poster_view_count":118.0,
        "Solution_body":"<p>you can bring to the endpoint instance (either in the <code>model.tar.gz<\/code> or via later download) a file storing the mapping between indexes and record names; this way you can translate from neighbor IDs to record names on the fly in the <code>predict_fn<\/code> or in the <code>output_fn<\/code>. For giant indexes this mapping (along with other metadata) can be in an external database too (eg dynamoDB, redis)<\/p>\n\n<p>the link you attach (SageMaker Batch Transform) is quite a different concept; it's for instantiating ephemeral fleet of machine(s) to run a one-time prediction task with input data in S3 and results written to s3. You question seem to refer to the alternative, permanent, real-time endpoint deployment mode.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.3,
        "Solution_reading_time":9.18,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":113.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1505653015243,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1128.0,
        "Answerer_view_count":71.0,
        "Challenge_adjusted_solved_time":0.1140905556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to export a machine learning model I created in Azure Machine Learning studio. One of the required input is \"Path to blob beginning with container\"<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/xvSDd.png\" alt=\"Here is the screenshoot from azure export\"><\/p>\n\n<p>How do I find this path? I have already created a blob storage but I have no idea how to find the path to the blob storage. <\/p>",
        "Challenge_closed_time":1538035308943,
        "Challenge_comment_count":0,
        "Challenge_created_time":1538034898217,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1538038508827,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52532078",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":6.4,
        "Challenge_reading_time":5.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.1140905556,
        "Challenge_title":"How to find the path to blob?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":15096.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>you should be able to find this from the Azure portal. Open the storage account, drill down into blobs, then your container. Use properties for the context menu, the URL should be the path ?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/r5hxi.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/r5hxi.jpg\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.3,
        "Solution_reading_time":4.7,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":44.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1333391842272,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"California, United States",
        "Answerer_reputation_count":1405.0,
        "Answerer_view_count":151.0,
        "Challenge_adjusted_solved_time":10042.3480277778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to build some <strong>neural network<\/strong> models for NLP and recommendation applications. The framework I want to use is <strong>TensorFlow<\/strong>. I plan to train these models and make predictions on Amazon web services. The application will be most likely <strong>distributed computing<\/strong>.<\/p>\n\n<p>I am wondering what are the pros and cons of SageMaker and EMR for TensorFlow applications?<\/p>\n\n<p>They both have TensorFlow integrated. <\/p>",
        "Challenge_closed_time":1573664904092,
        "Challenge_comment_count":0,
        "Challenge_created_time":1537510189837,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1537512451192,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52437599",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.3,
        "Challenge_reading_time":7.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":10042.9761819444,
        "Challenge_title":"Pros and Cons of Amazon SageMaker VS. Amazon EMR, for deploying TensorFlow-based deep learning models?",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":10776.0,
        "Challenge_word_count":78,
        "Platform":"Stack Overflow",
        "Poster_created_time":1341967360208,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2832.0,
        "Poster_view_count":368.0,
        "Solution_body":"<p>In general terms, they serve different purposes.<\/p>\n\n<p><a href=\"https:\/\/aws.amazon.com\/emr\/\" rel=\"noreferrer\"><strong>EMR<\/strong><\/a> is when you need to process massive amounts of data and heavily rely on Spark, Hadoop, and MapReduce (EMR = Elastic MapReduce). Essentially, if your data is in large enough volume to make use of the efficiencies of Spark, Hadoop, Hive, HDFS, HBase and Pig stack then go with EMR.<\/p>\n\n<p><strong>EMR Pros:<\/strong><\/p>\n\n<ul>\n<li>Generally, low cost compared to EC2 instances<\/li>\n<li>As the name suggests Elastic meaning you can provision what you need when you need it<\/li>\n<li>Hive, Pig, and HBase out of the box<\/li>\n<\/ul>\n\n<p><strong>EMR Cons:<\/strong><\/p>\n\n<ul>\n<li>You need a very specific use case to truly benefit from all the offerings in EMR. Most don't take advantage of its entire offering<\/li>\n<\/ul>\n\n<p><a href=\"https:\/\/aws.amazon.com\/sagemaker\/\" rel=\"noreferrer\"><strong>SageMaker<\/strong><\/a> is an attempt to make Machine Learning easier and distributed. The marketplace provides out of the box algos and models for quick use. It's a great service if you conform to the workflows it enforces. Meaning creating training jobs, deploying inference endpoints <\/p>\n\n<p><strong>SageMaker Pros:<\/strong><\/p>\n\n<ul>\n<li>Easy to get up and running with Notebooks<\/li>\n<li>Rich marketplace to quickly try existing models<\/li>\n<li>Many different example notebooks for popular algorithms<\/li>\n<li>Predefined kernels that minimize configuration<\/li>\n<li>Easy to deploy models<\/li>\n<li>Allows you to distribute inference compute by deploying endpoints<\/li>\n<\/ul>\n\n<p><strong>SageMaker Cons:<\/strong><\/p>\n\n<ul>\n<li>Expensive!<\/li>\n<li>Enforces a certain workflow making it hard to be fully custom<\/li>\n<li>Expensive!<\/li>\n<\/ul>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.6,
        "Solution_reading_time":22.46,
        "Solution_score_count":10.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":229.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1635045129020,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":53.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":250.6002630556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm dabbling with ML and was able to take a tutorial and get it to work for my needs.  It's a simple recommender system using TfidfVectorizer and linear_kernel.  I run into a problem with how I go about deploying it through Sagemaker with an end point.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel \nimport json\nimport csv\n\nwith open('data\/big_data.json') as json_file:\n    data = json.load(json_file)\n\nds = pd.DataFrame(data)\n\ntf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')\ntfidf_matrix = tf.fit_transform(ds['content'])\ncosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n\nresults = {}\n\nfor idx, row in ds.iterrows():\n    similar_indices = cosine_similarities[idx].argsort()[:-100:-1]\n    similar_items = [(cosine_similarities[idx][i], ds['id'][i]) for i in similar_indices]\n\n    results[row['id']] = similar_items[1:]\n\ndef item(id):\n    return ds.loc[ds['id'] == id]['id'].tolist()[0]\n\ndef recommend(item_id, num):\n    print(&quot;Recommending &quot; + str(num) + &quot; products similar to &quot; + item(item_id) + &quot;...&quot;)\n    print(&quot;-------&quot;)\n    recs = results[item_id][:num]\n    for rec in recs:\n        print(&quot;Recommended: &quot; + item(rec[1]) + &quot; (score:&quot; + str(rec[0]) + &quot;)&quot;)\n\nrecommend(item_id='129035', num=5)\n<\/code><\/pre>\n<p>As a starting point I'm not sure if the output from <code>tf.fit_transform(ds['content'])<\/code> is considered the model or the output from <code>linear_kernel(tfidf_matrix, tfidf_matrix)<\/code>.<\/p>",
        "Challenge_closed_time":1636075476147,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635046349497,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1635173315200,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69693666",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.2,
        "Challenge_reading_time":21.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":285.8685138889,
        "Challenge_title":"How to Deploy ML Recommender System on AWS",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":63.0,
        "Challenge_word_count":164,
        "Platform":"Stack Overflow",
        "Poster_created_time":1635045129020,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>I came to the conclusion that I didn't need to deploy this through SageMaker.  Since the final linear_kernel output was a Dictionary I could do quick ID lookups to find correlations.<\/p>\n<p>I have it working on AWS with API Gateway\/Lambda, DynamoDB and an EC2 server to collect, process and plug the data into DynamoDB for fast lookups.  No expensive SageMaker endpoint needed.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":4.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":17.5492611111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I attempting to deploy the universal-sentence-encoder model to a aws Sagemaker endpoint and am getting the error <code>raise ValueError('no SavedModel bundles found!')<\/code><\/p>\n\n<p>I have shown my code below, I have a feeling that one of my paths is incorrect<\/p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\nfrom sagemaker import get_execution_role\nfrom sagemaker.tensorflow.serving import Model\n\ndef tfhub_to_savedmodel(model_name,uri):\n    tfhub_uri = uri\n    model_path = 'encoder_model\/' + model_name\n\n    with tf.Session(graph=tf.Graph()) as sess:\n        module = hub.Module(tfhub_uri) \n        input_params = module.get_input_info_dict()\n        dtype = input_params['text'].dtype\n        shape = input_params['text'].get_shape()\n\n        # define the model inputs\n        inputs = {'text': tf.placeholder(dtype, shape, 'text')}\n\n        # define the model outputs\n        # we want the class ids and probabilities for the top 3 classes\n        logits = module(inputs['text'])\n        outputs = {\n            'vector': logits,\n        }\n\n        # export the model\n        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n        tf.saved_model.simple_save(\n            sess,\n            model_path,\n            inputs=inputs,\n            outputs=outputs)  \n\n    return model_path\n\n\nsagemaker_role = get_execution_role()\n\n!tar -C \"$PWD\" -czf encoder.tar.gz encoder_model\/\nmodel_data = Session().upload_data(path='encoder.tar.gz',key_prefix='model')\n\nenv = {'SAGEMAKER_TFS_DEFAULT_MODEL_NAME': 'universal-sentence-encoder-large'}\n\nmodel = Model(model_data=model_data, role=sagemaker_role, framework_version=1.12, env=env)\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.t2.medium')\n<\/code><\/pre>",
        "Challenge_closed_time":1563978858283,
        "Challenge_comment_count":0,
        "Challenge_created_time":1563915680943,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57172147",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.4,
        "Challenge_reading_time":21.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":17.5492611111,
        "Challenge_title":"'no SavedModel bundles found!' on tensorflow_hub model deployment to AWS SageMaker",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2621.0,
        "Challenge_word_count":152,
        "Platform":"Stack Overflow",
        "Poster_created_time":1531840489147,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berkeley, CA, USA",
        "Poster_reputation_count":425.0,
        "Poster_view_count":92.0,
        "Solution_body":"<p>I suppose you started from this example? <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_serving_container\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_serving_container<\/a><\/p>\n\n<p>It looks like you're not saving the TF Serving bundle properly: the model version number is missing, because of this line:<\/p>\n\n<pre><code>model_path = 'encoder_model\/' + model_name\n<\/code><\/pre>\n\n<p>Replacing it with this should fix your problem:<\/p>\n\n<pre><code>model_path = '{}\/{}\/00000001'.format('encoder_model\/', model_name)\n<\/code><\/pre>\n\n<p>Your model artefact should look like this (I used the model in the notebook above):<\/p>\n\n<pre><code>mobilenet\/\nmobilenet\/mobilenet_v2_140_224\/\nmobilenet\/mobilenet_v2_140_224\/00000001\/\nmobilenet\/mobilenet_v2_140_224\/00000001\/saved_model.pb\nmobilenet\/mobilenet_v2_140_224\/00000001\/variables\/\nmobilenet\/mobilenet_v2_140_224\/00000001\/variables\/variables.data-00000-of-00001\nmobilenet\/mobilenet_v2_140_224\/00000001\/variables\/variables.index\n<\/code><\/pre>\n\n<p>Then, upload to S3 and deploy.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":23.4,
        "Solution_reading_time":15.64,
        "Solution_score_count":6.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":76.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":24.5525,
        "Challenge_answer_count":9,
        "Challenge_body":"Hi. I just upgraded to pycaret 2.1. When I ran the compare_models function with the Titanic dataset, I got the following error:\r\n\r\nMlflowException: Unable to map 'np.object' type to MLflow DataType. np.object canbe mapped iff all values have identical data type which is one of (string, (bytes or byterray),  int, float)\r\n\r\nThe same code worked fine in pycaret 2.0.",
        "Challenge_closed_time":1598806653000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1598718264000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/566",
        "Challenge_link_count":0,
        "Challenge_participation_count":9,
        "Challenge_readability":6.9,
        "Challenge_reading_time":4.83,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":24.5525,
        "Challenge_title":"Compare models MLFlowException",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":61,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@sagarnildass Hi, Thanks for reporting. This seems like an error from `MLFlow`. I tried to reproduce this out of `pycaret` and I was successful. See below code that throws an error:\r\n\r\n```\r\nimport pandas as pd\r\ndata = pd.read_csv('titanic.csv') #train data from Kaggle\r\nfrom mlflow.models.signature import infer_signature\r\ninfer_signature(data)\r\n```\r\nThis gives the following error:\r\nMlflowException: Unable to map 'np.object' type to MLflow DataType. np.object canbe mapped iff all values have identical data type which is one of (string, (bytes or byterray),  int, float).\r\n\r\nI will log an issue on MLFlow GitHub.\r\n\r\nHere is the issue I logged on MLFlow: https:\/\/github.com\/mlflow\/mlflow\/issues\/3362 Hey\r\n\r\nThanks for the quick reply. \r\n\r\nI found out that presence of null values are a problem. If the dataset contains null values, this error was raised. When I imputed the null values, this problem was solved. Can you also mention this when you log this issue?\r\n\r\nThanks! @sagarnildass Thanks. I have added that in my issue but I don't think so it's 100% True. I have worked with few missing datasets and it worked okay. For example, the `hepatitis` dataset on our repo works fine. Example code:\r\n\r\n```\r\nfrom pycaret.datasets import get_data\r\ndata = get_data('hepatitis')\r\nfrom pycaret.classification import *\r\ns = setup(data, target = 'Class', log_experiment=True, experiment_name = 'hepatitis1')\r\n```\r\n\r\nThis dataset has missing values but it just worked fine. \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/58118658\/91642055-41991700-e9f6-11ea-9b6f-0f42a86401d9.png)\r\n\r\nCan you investigate more and add your comments on the original issue here: https:\/\/github.com\/mlflow\/mlflow\/issues\/3362\r\n\r\nThanks a lot for helping. @Yard1 I don't know how soon `MLFLow` will be able to fix this but in `2.2` we will have to create some kind of exceptions under `logging_param` chunks to not fail the process even when `infer_signature` fails, as it's not mandatory and has no impact other than the signature file that gets generated under `model` directory when `log_experiment` is set to `True`. @pycaret : I believe object datatypes are a problem. It clearly states: \"np.object canbe mapped iff all values have identical data type which is one of (string, (bytes or byterray), int, float)\". So do you think null values in a object datatype column might be the root  problem here? Because in hepatitis data, all the columns are numeric. @pycaret If we get MLFlow logging into a function then it will be easy to wrap it into a try except block.  @sagarnildass Thanks again for reporting. I am planning to do a bug fix release tomorrow `2.1.1`. For now, I have wrapped this inside `try` and `except` clause to avoid the error. I have tested it on the titanic dataset.\r\n\r\nCan you please sync the `master` and try to see if you can reproduce the error now?\r\n\r\nThanks Done...it's working as expected. @sagarnildass Thanks. I will publish the `2.1.1.` release today. @Yard1 FYI.\r\n\r\nThanks for your help @sagarnildass ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":6.6,
        "Solution_reading_time":36.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":39.0,
        "Solution_word_count":449.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1411546424280,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Edinburgh",
        "Answerer_reputation_count":58.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":4477.9070705556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed a the universal_sentence_encoder_large_3 to an aws sagemaker.  When I am attempting to predict with the deployed model I get <code>Failed precondition: Table not initialized.<\/code> as an error. I have included the part where I save my model below:<\/p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\ndef tfhub_to_savedmodel(model_name, export_path):\n\n    model_path = '{}\/{}\/00000001'.format(export_path, model_name)\n    tfhub_uri = 'http:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/3'\n\n    with tf.Session() as sess:\n        module = hub.Module(tfhub_uri)\n        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n        input_params = module.get_input_info_dict()\n        dtype = input_params['text'].dtype\n        shape = input_params['text'].get_shape()\n\n        # define the model inputs\n        inputs = {'text': tf.placeholder(dtype, shape, 'text')}\n        output = module(inputs['text'])\n        outputs = {\n            'vector': output,\n        }\n\n        # export the model\n        tf.saved_model.simple_save(\n            sess,\n            model_path,\n            inputs=inputs,\n            outputs=outputs)  \n\n    return model_path\n<\/code><\/pre>\n\n<p>I have seen other people ask this problem but no solution has been ever posted.  It seems to be a common problem with tensorflow_hub sentence encoders<\/p>",
        "Challenge_closed_time":1580113898207,
        "Challenge_comment_count":0,
        "Challenge_created_time":1563993432753,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57189292",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":17.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":4477.9070705556,
        "Challenge_title":"Failed precondition: Table not initialized. on deployed universal sentence encoder from aws sagemaker",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":365.0,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_created_time":1531840489147,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berkeley, CA, USA",
        "Poster_reputation_count":425.0,
        "Poster_view_count":92.0,
        "Solution_body":"<p>I was running into this exact issue earlier this week while trying to modify this example <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_serving_container\/tensorflow_serving_container.ipynb\" rel=\"nofollow noreferrer\">Sagemaker notebook<\/a>. Particularly the part where serving the model. That is, running <code>predictor.predict()<\/code> on the Sagemaker Tensorflow Estimator.<\/p>\n\n<p>The solution outlined in the issue worked perfectly for me- <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/issues\/773#issuecomment-509433290\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/issues\/773#issuecomment-509433290<\/a><\/p>\n\n<p>I think it's just because <code>tf.tables_initializer()<\/code> only runs for training but it needs to be specified through the <code>legacy_init_op<\/code> if you want to run it during prediction.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":18.5,
        "Solution_reading_time":12.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":78.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":40.1701361111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to share a model registry completely between Dev and Prod environment? So my idea is to create 10000 models in dev and maybe select 2000 from there to work in prod. I am planning to use AWS model registry. So if I do the training and testing and hyperparameter tuning in my AWS dev environment, is it possible to then share the registry in prod? The obvious reason is that it does not make sense to use the prod to do the training and testing again.<\/p>\n<p>Please advise!<\/p>\n<p>Thanks in advance!<\/p>",
        "Challenge_closed_time":1638378803110,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638234190620,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70163094",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":6.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":40.1701361111,
        "Challenge_title":"SageMaker Model Registry Sharing",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":229.0,
        "Challenge_word_count":97,
        "Platform":"Stack Overflow",
        "Poster_created_time":1557333597230,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":99.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>It depends how you define Dev and Prod.<\/p>\n<ul>\n<li><p>If by Dev and Prod you mean different AWS account (which is a good practice - see <a href=\"https:\/\/docs.aws.amazon.com\/whitepapers\/latest\/organizing-your-aws-environment\/benefits-of-using-multiple-aws-accounts.html\" rel=\"nofollow noreferrer\">doc<\/a> and <a href=\"https:\/\/aws.amazon.com\/blogs\/devops\/aws-building-a-secure-cross-account-continuous-delivery-pipeline\/\" rel=\"nofollow noreferrer\">blog<\/a>), you cannot share fractions of a model registry from a given account to another account, but you can create triggers to export models from one model registry to another, as documented in this blog post <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/patterns-for-multi-account-hub-and-spoke-amazon-sagemaker-model-registry\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/patterns-for-multi-account-hub-and-spoke-amazon-sagemaker-model-registry\/<\/a><\/p>\n<\/li>\n<li><p>If your Dev and Prod live in the same AWS account and you are just looking for ways to differentiate them, you can use:<\/p>\n<ul>\n<li>Model Registry Status information<\/li>\n<li>Tags<\/li>\n<\/ul>\n<\/li>\n<\/ul>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":19.7,
        "Solution_reading_time":15.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":107.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":139.6462430556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p><strong>Hello, can someone please help Me get this set up?<\/strong><br>\nI am running this in <strong>Google Colab<\/strong><\/p>\n<p>I am unable to get any data from my tacotron model.  I was able to login\/create multiple wandb runs and it tracks usage(log is connected, Utilization updates) but no data has been entered and the way it trains it runs one line forever so i dont even know how to begin.<\/p>\n<p>This is the Script that currently runs the training<\/p>\n<pre><code class=\"lang-auto\">print('FP16 Run:', hparams.fp16_run)\nprint('Dynamic Loss Scaling:', hparams.dynamic_loss_scaling)\nprint('Distributed Run:', hparams.distributed_run)\nprint('cuDNN Enabled:', hparams.cudnn_enabled)\nprint('cuDNN Benchmark:', hparams.cudnn_benchmark)\n\nfrom IPython.display import Javascript\ndisplay(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 200})'''))\n#for i in range(200):\n#  print(i)\n\ntrain(output_directory, log_directory, checkpoint_path,\n      warm_start, n_gpus, rank, group_name, hparams, log_directory2)\n\n<\/code><\/pre>\n<p><strong>I tried doing this but it didnt work either. here is my code to start the training with wandb<\/strong><\/p>\n<p>Install and login<\/p>\n<pre><code class=\"lang-auto\">#@markdown Login and start a new run\nprint('Installing wandb')\n!pip -q install wandb\nimport wandb\nprint('Login To wanb!!!\\n')\n!wandb login\n<\/code><\/pre>\n<p>Capture a dictionary of hyperparameters<\/p>\n<pre><code class=\"lang-auto\">#@markdown Capture a dictionary of hyperparameters\nwandb.config.p_attention_dropout=hparams.p_attention_dropout\nwandb.config.p_decoder_dropout=hparams.p_decoder_dropout\nwandb.config.decay_start=hparams.decay_start\nwandb.config.A_=hparams.A_\nwandb.config.B_=hparams.B_\nwandb.config.C_=hparams.C_\nwandb.config.min_learning_rate=hparams.min_learning_rate\nwandb.config.batch_size=hparams.batch_size\nwandb.config.epochs=hparams.epochs\nwandb.config.generate_mels=generate_mels\nwandb.config.show_alignments=hparams.show_alignments\nwandb.config.alignment_graph_height=alignment_graph_height\nwandb.config.alignment_graph_width=alignment_graph_width\nwandb.config.load_mel_from_disk=hparams.load_mel_from_disk\nwandb.config.ignore_layers=hparams.ignore_layers\nwandb.config.checkpoint_path=checkpoint_path\n<\/code><\/pre>\n<p>Start wanb and get runID<br>\n<code>wandb.init(project=\"tacotron\", entity=\"gmirsky2\")<\/code><\/p>\n<p><strong>start wandb run then Train<\/strong><\/p>\n<pre><code class=\"lang-auto\">#Run\napi = wandb.Api()\nrun = api.run(\"gmirsky2\/tacotron\/\" + wandb.run.id)\n\n#train\ntrain(output_directory, log_directory, checkpoint_path,\n      warm_start, n_gpus, rank, group_name, hparams, log_directory2)\n\n# save the metrics for the run to a csv file\nmetrics_dataframe = run.history()\nmetrics_dataframe.to_csv(\"metrics.csv\")\n<\/code><\/pre>\n<p>When The Training Runs it Just goes to the train line and then never finishes.<br>\nI am looking for help with how to incorporate wandb with the tacotron train script\u2026<\/p>\n<pre><code class=\"lang-auto\">train(output_directory, log_directory, checkpoint_path,\n      warm_start, n_gpus, rank, group_name, hparams, log_directory2)\n<\/code><\/pre>\n<p>I thought that was what the hyperparameters were for but i guess im wrong.<\/p>\n<p>Any help would be welcome. Thanks a bunch!<\/p>",
        "Challenge_closed_time":1641584614231,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641081887756,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/help-adding-to-tacotron-model\/1660",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":15.4,
        "Challenge_reading_time":43.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":139.6462430556,
        "Challenge_title":"Help Adding To Tacotron Model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":260.0,
        "Challenge_word_count":300,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I was able to get it working after blood\u2026 sweat\u2026 and time.<br>\nWell really it was just time.<\/p>\n<p>I ended up having to install then import it before Training and stop it after training\u2026 who would have thought. lol<\/p>\n<pre><code class=\"lang-auto\">#@markdown Install W&amp;B\n%%capture\n!pip install wandb --upgrade\n<\/code><\/pre>\n<p>wandb is super helpful and have a tf module: <em><strong>config=tf.flags.FLAGS<\/strong><\/em><\/p>\n<pre><code class=\"lang-auto\">import wandb\nimport tensorflow as tf\nwandb.init(config=tf.flags.FLAGS, sync_tensorboard=True, project=model_filename)\n<\/code><\/pre>\n<p>Then after training is done Close the wandb connection<br>\n<code>wandb_run.finish(exit_code=0)<\/code><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.1,
        "Solution_reading_time":9.14,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":80.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1513106638900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":276.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":12.3196019445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm attempting to use Tensorflows <code>tf.contrib.factorization.KMeansClustering<\/code> estimator with SageMaker but am having some trouble. The output of my SageMaker <code>predictor.predict()<\/code> call looks incorrect. The cluster values are too large as they should be integers from 0-7. (I have the number of clusters set to 8).<\/p>\n\n<p>I get a similar output on every run (where the last half of the array is <code>4L<\/code> or some other digit like <code>0L<\/code>). There are 40 values in the array because that s how many rows(users and their ratings I pass into the <code>predict()<\/code> function)<\/p>\n\n<p>Example:\n<code>{'outputs': {u'output': {'int64_val': [6L, 0L, 6L, 1L, 2L, 4L, 5L, 7L, 7L, 7L, 7L, 5L, 0L, 1L, 7L, 3L, 3L, 6L, 7L, 3L, 7L, 2L, 6L, 2L, 3L, 7L, 6L, 3L, 3L, 6L, 1L, 2L, 1L, 3L, 7L, 7L, 7L, 3L, 5L, 7L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L], 'dtype': 9, 'tensor_shape': {'dim': [{'size': 100L}]}}}, 'model_spec': {'signature_name': u'serving_default', 'version': {'value': 1534392971L}, 'name': u'generic_model'}}<\/code><\/p>\n\n<p>The data I'm working with is a sparse matrix of item ratings where <code>rows=users<\/code>, <code>cols=items<\/code>, and the cells contain floats bewteen 0.0 and 10. So my input data is a matrix instead of the typical array of features.<\/p>\n\n<p>I think the issue might be in the serving_input_fn function. Here is my SageMaker entry_point script:<\/p>\n\n<pre><code>def estimator_fn(run_config, params):\n    #feature_columns = [tf.feature_column.numeric_column('inputs', shape=list(params['input_shape']))]\n    return tf.contrib.factorization.KMeansClustering(num_clusters=NUM_CLUSTERS,\n                            distance_metric=tf.contrib.factorization.KMeansClustering.COSINE_DISTANCE,\n                            use_mini_batch=False,\n                            feature_columns=None,\n                            config=run_config)\n\ndef serving_input_fn(params):\n    tensor = tf.placeholder(tf.float32, shape=[None, None])\n    return tf.estimator.export.build_raw_serving_input_receiver_fn({'inputs': tensor})()\n\ndef train_input_fn(training_dir, params):\n    \"\"\" Returns input function that would feed the model during training \"\"\"\n    return generate_input_fn(training_dir, 'train.csv')\n\n\ndef eval_input_fn(training_dir, params):\n    \"\"\" Returns input function that would feed the model during evaluation \"\"\"\n    return generate_input_fn(training_dir, 'test.csv')\n\n\ndef generate_input_fn(training_dir, training_filename):\n    \"\"\" Generate all the input data needed to train and evaluate the model. \"\"\"\n    # Load train\/test data from s3 bucket\n    train = np.loadtxt(os.path.join(training_dir, training_filename), delimiter=\",\")\n    return tf.estimator.inputs.numpy_input_fn(\n        x={'inputs': np.array(train, dtype=np.float32)},\n        y=None,\n        num_epochs=1,\n        shuffle=False)()\n<\/code><\/pre>\n\n<p>In <code>generate_input_fn()<\/code>, <code>train<\/code> is the numpy ratings matrix.<\/p>\n\n<p>If it helps, here is my call to the <code>predict()<\/code> function, (<code>ratings_matrix<\/code> is a 40 x num_items numpy array):<\/p>\n\n<pre><code>mtx = tf.make_tensor_proto(values=ratings_matrix,\n                           shape=list(ratings_matrix.shape), dtype=tf.float32)\nresult = predictor.predict(mtx)\n<\/code><\/pre>\n\n<p>I feel like the issue is something simple I'm missing. This is the first ML algorithm I've written so any help would be appreciated.<\/p>",
        "Challenge_closed_time":1534550266343,
        "Challenge_comment_count":0,
        "Challenge_created_time":1534484294060,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1534505915776,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51888996",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.7,
        "Challenge_reading_time":44.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":18.3256341667,
        "Challenge_title":"How to write Tensorflow KMeans Estimator script for Sagemaker",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":321.0,
        "Challenge_word_count":415,
        "Platform":"Stack Overflow",
        "Poster_created_time":1516029540768,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":161.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>Thanks javadba for your answer!<\/p>\n\n<p>I am not very well adversed in Machine Learning or TensorFlow, so please correct me. However, it looks like you were able to integrate with SageMaker, but the predictions aren't what you are expecting.<\/p>\n\n<p>Ultimately, SageMaker runs your <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst#preparing-the-tensorflow-training-script\" rel=\"nofollow noreferrer\">EstimatorSpec<\/a> with <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/blob\/master\/src\/tf_container\/trainer.py#L73\" rel=\"nofollow noreferrer\">train_and_evaluate<\/a> for training and uses TensorFlow Serving for your predictions. It doesn't have any other hidden functionalities, so the results you get from your KMeans predictions using the TensorFlow estimator is going to be independent of SageMaker. It might be affected by how you define your serving_input_fn and output_fn however.<\/p>\n\n<p>When you run this same estimator outside of the SageMaker ecosystem using the same setup, do you get predictions in the format you're expecting?<\/p>\n\n<p>The SageMaker TensorFlow experience is open sourced here and shows what is possible and isn't as of now.\n<a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":15.6,
        "Solution_reading_time":17.95,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":147.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1393524211332,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":745.0,
        "Answerer_view_count":71.0,
        "Challenge_adjusted_solved_time":968.2322455556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to make transfer learning method on MXNet on Sagemaker instance. Train and serve start locally without any problem and I'm using that python code to predict:<\/p>\n\n<pre><code>def predict_mx(net, fname):\n    with open(fname, 'rb') as f:\n      img = image.imdecode(f.read())\n      plt.imshow(img.asnumpy())\n      plt.show()\n    data = transform(img, -1, test_augs)\n    plt.imshow(data.transpose((1,2,0)).asnumpy()\/255)\n    plt.show()\n    data = data.expand_dims(axis=0)\n    return net.predict(data.asnumpy().tolist())\n<\/code><\/pre>\n\n<p>I checked <code>data.asnumpy().tolist()<\/code> that is ok and pyplot draw images (firts is the original image, the second is the resized image). But <code>net.predict<\/code> raise an error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nJSONDecodeError                           Traceback (most recent call last)\n&lt;ipython-input-171-ea0f1f5bdc72&gt; in &lt;module&gt;()\n----&gt; 1 predict_mx(predictor.predict, '.\/data2\/burgers-imgnet\/00103785.jpg')\n\n&lt;ipython-input-170-150a72b14997&gt; in predict_mx(net, fname)\n     30     plt.show()\n     31     data = data.expand_dims(axis=0)\n---&gt; 32     return net(data.asnumpy().tolist())\n     33 \n\n~\/Projects\/Lab\/ML\/AWS\/v\/lib64\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data)\n     89         if self.deserializer is not None:\n     90             # It's the deserializer's responsibility to close the stream\n---&gt; 91             return self.deserializer(response_body, response['ContentType'])\n     92         data = response_body.read()\n     93         response_body.close()\n\n~\/Projects\/Lab\/ML\/AWS\/v\/lib64\/python3.6\/site-packages\/sagemaker\/predictor.py in __call__(self, stream, content_type)\n    290         \"\"\"\n    291         try:\n--&gt; 292             return json.load(codecs.getreader('utf-8')(stream))\n    293         finally:\n    294             stream.close()\n\n\/usr\/lib64\/python3.6\/json\/__init__.py in load(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n    297         cls=cls, object_hook=object_hook,\n    298         parse_float=parse_float, parse_int=parse_int,\n--&gt; 299         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n    300 \n    301 \n\n\/usr\/lib64\/python3.6\/json\/__init__.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n    352             parse_int is None and parse_float is None and\n    353             parse_constant is None and object_pairs_hook is None and not kw):\n--&gt; 354         return _default_decoder.decode(s)\n    355     if cls is None:\n    356         cls = JSONDecoder\n\n\/usr\/lib64\/python3.6\/json\/decoder.py in decode(self, s, _w)\n    337 \n    338         \"\"\"\n--&gt; 339         obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n    340         end = _w(s, end).end()\n    341         if end != len(s):\n\n\/usr\/lib64\/python3.6\/json\/decoder.py in raw_decode(self, s, idx)\n    355             obj, end = self.scan_once(s, idx)\n    356         except StopIteration as err:\n--&gt; 357             raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n    358         return obj, end\n\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\n<\/code><\/pre>\n\n<p>I tried to json.dumps my data, and there is no problem with that.<\/p>\n\n<p>Note that I didn't deployed the service on AWS yet, I want to be able to test the model and prediction locally before to make a larger train and to serve it later.<\/p>\n\n<p>Thanks for your help<\/p>",
        "Challenge_closed_time":1531584172427,
        "Challenge_comment_count":4,
        "Challenge_created_time":1528098536343,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50675708",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":10.9,
        "Challenge_reading_time":41.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":968.2322455556,
        "Challenge_title":"Sagemaker Predict on local instance, JSON Error",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1358.0,
        "Challenge_word_count":332,
        "Platform":"Stack Overflow",
        "Poster_created_time":1340280616088,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Laval, France",
        "Poster_reputation_count":2835.0,
        "Poster_view_count":281.0,
        "Solution_body":"<p>The call to <strong>net.predict<\/strong> is working fine. <\/p>\n\n<p>It seems that you are using the SageMaker Python SDK <strong>predict_fn<\/strong> for hosting. After the <strong>predict_fn<\/strong> is invoked, the MXNet container will try to serialize your prediction to JSON before sending it back to the client. You can see code that does that here: <a href=\"https:\/\/github.com\/aws\/sagemaker-mxnet-container\/blob\/master\/src\/mxnet_container\/serve\/transformer.py#L132\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-mxnet-container\/blob\/master\/src\/mxnet_container\/serve\/transformer.py#L132<\/a><\/p>\n\n<p>The container is failing to serialize because <strong>net.predict<\/strong> does not return a serializable object. You can solve this issue by returning a list instead:<\/p>\n\n<pre><code>return net.predict(data.asnumpy().tolist()).asnumpy().tolist()\n<\/code><\/pre>\n\n<p>Another alternative is to use a <strong>transform_fn<\/strong> instead of <strong>prediction_fn<\/strong> so you can handle the output serialization yourself. You can see an example of a <strong>transform_fn<\/strong> here <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/e93eff66626c0ab1f292048451c4c3ac7c39a121\/examples\/cli\/host\/script.py#L41\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/e93eff66626c0ab1f292048451c4c3ac7c39a121\/examples\/cli\/host\/script.py#L41<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":16.4,
        "Solution_reading_time":18.7,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":114.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":12.5540683334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've built models using the AutoML function and I'm trying to call the best model to deploy into production. The AutoML function ran correctly and produced the ~35 models. My goal is to pull the best model. Here is the code:  <\/p>\n<p>best_run, fitted_model = remote_run.get_output()  <br \/>\nfitted_model  <\/p>\n<p>When runnning the code, I get the following error:   <\/p>\n<p>AttributeError: 'DataTransformer' object has no attribute 'enable_dnn'  <\/p>\n<p>Any help would be much appreciated.   <\/p>",
        "Challenge_closed_time":1613128985603,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613083790957,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/270011\/remote-run-model-unable-to-be-saved",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":6.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":12.5540683334,
        "Challenge_title":"Remote run model unable to be saved",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":79,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=f0b99777-9086-42ac-b2e2-2b069641e943\">@Bernardo Jaccoud  <\/a> Did your run configure enable_dnn i.e bert <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-features#bert-integration-in-automated-ml\">settings<\/a> of automated ML? I am curious to understand what the status of your run is directly on the portal ml.azure.com?    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.9,
        "Solution_reading_time":5.3,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":745.5772222222,
        "Challenge_answer_count":2,
        "Challenge_body":"**Description**\r\nWhen using the `publish_model_to_mlflow.py` script, if the value given for the `--model_directory` argument has a trailing `\/`, the script will bomb in interesting ways.\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using? 2.19.0\r\n\r\nAre you using the Triton container or did you build it yourself? container\r\n\r\n**To Reproduce**\r\n```\r\npython publish_model_to_mlflow.py \\\r\n    --model_name abp-nvsmi-xgb \\\r\n    --model_directory \/common\/models\/abp-nvsmi-xgb\/ \\\r\n    --flavor triton\r\n```\r\n\r\nThis gives the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"publish_model_to_mlflow.py\", line 71, in <module>\r\n    publish_to_mlflow()\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1128, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1053, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1395, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 754, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"publish_model_to_mlflow.py\", line 56, in publish_to_mlflow\r\n    triton_flavor.log_model(\r\n  File \"\/mlflow\/triton-inference-server\/server\/deploy\/mlflow-triton-plugin\/scripts\/triton_flavor.py\", line 100, in log_model\r\n    Model.log(\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/mlflow\/models\/model.py\", line 282, in log\r\n    flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\r\n  File \"\/mlflow\/triton-inference-server\/server\/deploy\/mlflow-triton-plugin\/scripts\/triton_flavor.py\", line 73, in save_model\r\n    shutil.copytree(triton_model_path, model_data_path)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/shutil.py\", line 557, in copytree\r\n    return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/shutil.py\", line 458, in _copytree\r\n    os.makedirs(dst, exist_ok=dirs_exist_ok)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nFileExistsError: [Errno 17] File exists: '\/tmp\/tmpdg2r5f0_\/model\/'\r\ncommand terminated with exit code 1\r\n```\r\n\r\nThe model being used seems to have no effect on the error.\r\n\r\n**Expected behavior**\r\nThe input provided is syntactically identical to:\r\n```\r\npython publish_model_to_mlflow.py \\\r\n    --model_name abp-nvsmi-xgb \\\r\n    --model_directory \/common\/models\/abp-nvsmi-xgb \\\r\n    --flavor triton\r\n```\r\n\r\nand should provide the same outcome.",
        "Challenge_closed_time":1650643135000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647959057000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/triton-inference-server\/server\/issues\/4089",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.4,
        "Challenge_reading_time":34.23,
        "Challenge_repo_contributor_count":94.0,
        "Challenge_repo_fork_count":1046.0,
        "Challenge_repo_issue_count":5133.0,
        "Challenge_repo_star_count":4495.0,
        "Challenge_repo_watch_count":116.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":29,
        "Challenge_solved_time":745.5772222222,
        "Challenge_title":"Input to the script for publishing models to mlflow is overly particular with inputs",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":227,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"It appears that the bug has been fixed by https:\/\/github.com\/triton-inference-server\/server\/pull\/3828 and I am not able to reproduce it using the model example for the plugin. Can you try the plugin from the latest codebase?\r\n```\r\npython `pwd`\/mlflow-triton-plugin\/scripts\/publish_model_to_mlflow.py \\\r\n    --model_name onnx_float32_int32_int32 \\\r\n    --model_directory `pwd`\/mlflow-triton-plugin\/examples\/onnx_float32_int32_int32\/ \\\r\n    --flavor triton\r\n```\r\nreturns:\r\n```\r\nRegistered model 'onnx_float32_int32_int32' already exists. Creating a new version of this model...\r\n2022\/04\/07 23:03:53 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: onnx_float32_int32_int32, version 3\r\nCreated version '3' of model 'onnx_float32_int32_int32'.\r\n.\/mlruns\/0\/945d5c5d6806470d889248cfc7f10b69\/artifacts\r\n``` Closing due to in-activity.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.9,
        "Solution_reading_time":11.49,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":86.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1369156710532,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Japan",
        "Answerer_reputation_count":189.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":2.8227241667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a project in which I do several optuna studies each having around 50 trials.<\/p>\n<p>The optuna documentation suggests saving each model to a file for later use on <a href=\"https:\/\/optuna.readthedocs.io\/en\/latest\/faq.html#how-to-define-objective-functions-that-have-own-arguments\" rel=\"nofollow noreferrer\">this FAQ section<\/a><\/p>\n<p>What I want is to have all the best models of different studies in a python list. How is that possible?<\/p>\n<p>This is somewhat similar to my code:<\/p>\n<pre><code>def objective(trial, n_epochs, batch_size):\n     params = {\n              'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n              'optimizer': trial.suggest_categorical(&quot;optimizer&quot;, [&quot;Adam&quot;, &quot;RMSprop&quot;, &quot;SGD&quot;]),\n              'n_epochs': n_epochs,\n              'batch_size': batch_size\n              }\n     model = clp_network(trial)\n     accuracy, model = train_and_evaluate(params, model, trial) # THIS LINE\n     return accuracy\n<\/code><\/pre>\n<pre><code>     for i in range(50):\n           study = optuna.create_study(direction=&quot;maximize&quot;, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner())\n           study.optimize(lambda trial: objective(trial, e, b), n_trials=50, show_progress_bar=True)\n<\/code><\/pre>\n<p>I would like to either save the <code>model<\/code> variable in the line marked <strong>THIS LINE<\/strong>, or somehow get the best model as a variable from the study.<\/p>",
        "Challenge_closed_time":1661860813547,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661850651740,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73539873",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":16.6,
        "Challenge_reading_time":18.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":2.8227241667,
        "Challenge_title":"saving trained models in optuna to a variable",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":23.0,
        "Challenge_word_count":145,
        "Platform":"Stack Overflow",
        "Poster_created_time":1612517804323,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Babol, Mazandaran, Iran",
        "Poster_reputation_count":125.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>The easiest way is to define a global variable to store a model for each trial as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import optuna\nfrom collections import defaultdict\n\n\nmodels = defaultdict(dict)\n\ndef objective(t):\n    model = t.suggest_int(&quot;x&quot;, 0, 100)\n    models[t.study.study_name][t.number] = model\n    \n    return model\n\nfor _ in range(10):\n    s = optuna.create_study()\n    s.optimize(objective, n_trials=10)\n\n<\/code><\/pre>\n<p>However I reckon this approach is not scalable in terms of memory space, so I'd suggest removing non-best models after each <code>optimize<\/code> call or saving models on an external file as mentioned in Optuna's FAQ.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.8,
        "Solution_reading_time":8.49,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":82.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.2394444444,
        "Challenge_answer_count":1,
        "Challenge_body":"```\r\n0: jdbc:hive2:\/\/localhost:10001\/default> CREATE MODEL ssd1 using \"mlflow:\/model\/ssd\"\r\n. . . . . . . . . . . . . . . . . . . .> ;\r\nError: org.apache.hive.service.cli.HiveSQLException: Error running query: org.mlflow.tracking.MlflowHttpException: statusCode=404 reasonPhrase=[NOT FOUND] bodyMessage=[{\"error_code\": \"RESOURCE_DOES_NOT_EXIST\", \"message\": \"Registered Model with name=ssd1 not found\"}]\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperti\r\n```",
        "Challenge_closed_time":1642206718000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642187856000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/eto-ai\/rikai\/issues\/493",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":45.4,
        "Challenge_reading_time":14.23,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":715.0,
        "Challenge_repo_star_count":127.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":5.2394444444,
        "Challenge_title":"Can not create model in MLflowCatalog",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":41,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Duplicated to #496 ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.2,
        "Solution_reading_time":0.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":30.887275,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>After applying Principal Component Analysis PCA to my data set in order to achieve better model accuracy. The 13 features dimensions, I am reducing it to 10 features using PCA. Everything is fine till here.    <\/p>\n<p>After implementing the model in WebApp, it is building &amp; seems fine in the studio.    <\/p>\n<p>In the testing phase of model prediction, Instead of displaying 10 features as an input, the UI system is showing the original features which is 13 &amp; the output is showing 10 featu<a href=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/70351-01.pdf?platform=QnA\">70351-01.pdf<\/a>res which does not have any feature names for the newly generated features which are 10. And also prediction is not working at all after executing it.\\    <\/p>\n<p>Attached are the screenshots, Please refer.    <\/p>",
        "Challenge_closed_time":1613976829527,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613865635337,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/281600\/error-webapp-implementation-with-principal-compone",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":11.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":30.887275,
        "Challenge_title":"Error - WebApp Implementation with Principal Component Analysis(PCA)",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":126,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@RakshitSidd-7739 After updating and running the training experiment did you try to update the prediction experiment.     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/70512-image.png?platform=QnA\" alt=\"70512-image.png\" \/>    <\/p>\n<p>After the prediction experiment is updated you can update the web service and check if it shows up correctly.     <\/p>\n",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.5,
        "Solution_reading_time":4.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":41.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1428454496052,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":35.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":94.0638861111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to serialize a list of java Objects (POJO) into RecordIO format. I have seen this BeanIO (<a href=\"http:\/\/beanio.org\/\" rel=\"nofollow noreferrer\">http:\/\/beanio.org\/<\/a>) but it seems to be outdated. Is there any other Java library that could be used or a different way to do this ?<\/p>\n\n<p>Once list of objects is serialized it will be used to train a model with SageMaker.<\/p>",
        "Challenge_closed_time":1526652887110,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526314257120,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50334735",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":5.35,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":94.0638861111,
        "Challenge_title":"How to generate Recordio from Java object",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":160.0,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Poster_created_time":1428454496052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Solving my own problem. I decided to use Apache Avro instead of BeanIO. Spark allow to serialize using Avro (c.f. Spark-Avro). This seems to work however it did not fit my use case has I was trying to serialize an array of numbers.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.1,
        "Solution_reading_time":2.88,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":43.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1364896706347,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Noida, India",
        "Answerer_reputation_count":6584.0,
        "Answerer_view_count":962.0,
        "Challenge_adjusted_solved_time":5.9270777778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to train a linear learner model in Sagemaker. My training set is 422 rows split into 4 files on AWS S3. The mini-batch size that I set is 50. <\/p>\n\n<p>I keep on getting this error in Sagemaker.<\/p>\n\n<blockquote>\n  <p>Customer Error: No training data processed. Either the training\n  channel is empty or the mini-batch size is too high. Verify that\n  training data contains non-empty files and the mini-batch size is less\n  than the number of records per training host.<\/p>\n<\/blockquote>\n\n<p>I am using this InputDataConfig<\/p>\n\n<pre><code>InputDataConfig=[\n            {\n                'ChannelName': 'train',\n                'DataSource': {\n                    'S3DataSource': {\n                        'S3DataType': 'S3Prefix',\n                        'S3Uri': 's3:\/\/MY_S3_BUCKET\/REST_OF_PREFIX\/exported\/',\n                        'S3DataDistributionType': 'FullyReplicated'\n                    }\n                },\n                'ContentType': 'text\/csv',\n                'CompressionType': 'Gzip'\n            }\n        ],\n<\/code><\/pre>\n\n<p>I am not sure what I am doing wrong here. I tried increasing the number of records to 5547495 split across 6 files. The same error. That makes me think that somehow the config itself has something missing. Due to which it seems to think training channel is just not present. I tried changing 'train' to 'training' as that is what the erorr message is saying. But then I got <\/p>\n\n<blockquote>\n  <p>Customer Error: Unable to initialize the algorithm. Failed to validate\n  input data configuration. (caused by ValidationError)<\/p>\n  \n  <p>Caused by: {u'training': {u'TrainingInputMode': u'Pipe',\n  u'ContentType': u'text\/csv', u'RecordWrapperType': u'None',\n  u'S3DistributionType': u'FullyReplicated'}} is not valid under any of\n  the given schemas<\/p>\n<\/blockquote>\n\n<p>I went back to train as that seems to be what is needed. But what am I doing wrong with that? <\/p>",
        "Challenge_closed_time":1559566976623,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559544607823,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1559545639143,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56422325",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":22.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":6.2135555556,
        "Challenge_title":"AWS Sagemaker - Either the training channel is empty or the mini-batch size is too high",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":599.0,
        "Challenge_word_count":249,
        "Platform":"Stack Overflow",
        "Poster_created_time":1364896706347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Noida, India",
        "Poster_reputation_count":6584.0,
        "Poster_view_count":962.0,
        "Solution_body":"<p>Found the problem. The CompressionType was mentioned as 'Gzip' but I had changed the actual file to be not compressed when doing the exports. As soon as I changed it to be 'None' the training went smoothly.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.5,
        "Solution_reading_time":2.6,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":37.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1467237684900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":613.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":142.9037780555,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying out the sample notebooks in AWS Sagemaker, currently in the mxnet mnist example which demonstrates bringing your own code. The entry point parameter passed in when instantiating an estimator instance, only mentions the source file (mnist.py) and not a method name or any other point inside the source file.<\/p>\n\n<p>So how does aws sagemaker figure out which method to send the training data to? <\/p>",
        "Challenge_closed_time":1520316895088,
        "Challenge_comment_count":0,
        "Challenge_created_time":1519726485703,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1519802441487,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49006174",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":6.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":164.0026069444,
        "Challenge_title":"How is the entry point to the code specified in AWS sagemaker bring your own code?",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3327.0,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Poster_created_time":1450260166772,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1587.0,
        "Poster_view_count":540.0,
        "Solution_body":"<p>Your python script should implement a few methods like train, model_fn, transform_fn, input_fn etc. SagaMaker would call appropriate method when needed. <\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet-training-inference-code-template.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet-training-inference-code-template.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":25.9,
        "Solution_reading_time":5.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0349952778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi Team,    <\/p>\n<p>I am trying to deploy 2 ML models ( which is registered in Model Registry ) to Azure Container Instance using DevOps Release pipeline using AZ CLI ML extension    <\/p>\n<p>My ACI Configuration is :    <\/p>\n<p><code>containerResourceRequirements:       cpu: 1       memoryInGB: 4     computeType: ACI <\/code>    <\/p>\n<p>Inference Config :    <\/p>\n<p><code>entryScript: score.py     runtime: python     condaFile: conda_dependencies.yml     extraDockerfileSteps:     schemaFile:     sourceDirectory:     enableGpu: False     baseImage:     baseImageRegistry:<\/code>    <\/p>\n<p>All score.py, conda_dependencies.yml, aciDeploymentConfig.yml is placed in a flattened directory which is publised in to DevOps pipeline artifcat and looks like    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/10000-files.png?platform=QnA\" alt=\"10000-files.png\" \/>    <\/p>\n<p>DevOps Deploy command looks like     <\/p>\n<p><code>az ml model deploy -g $(ml.resourceGroup) -w $(ml.workspace) --name $(service.name.staging) -f .\/model.json -m &quot;GloVe:4&quot; --dc aciDeploymentConfig.yml --ic inferenceConfig.yml --overwrite --debug <\/code>    <\/p>\n<p>Also i have set the working directory as the folder where all above files are placed. something like     <\/p>\n<p>$(System.DefaultWorkingDirectory)\/_Symptom-Code-Indexing\/symptom_model\/a    <\/p>\n<p>Its getting in to an exception as     <\/p>\n<p>2020-06-08T12:50:27.9202657Z     &quot;error&quot;: {    <br \/>\n2020-06-08T12:50:27.9208361Z         &quot;message&quot;: &quot;Received bad response from Model Management Service:\\nResponse Code: 400\\nHeaders: {'Date': 'Mon, 08 Jun 2020 12:50:27 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d', 'x-ms-client-request-id': '823e8483923846b1958c08ffaba074ff', 'x-ms-client-session-id': '62e84a29-b77c-456f-9d91-5ca6be26f79c', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\\nContent: b'{&quot;code&quot;:&quot;BadRequest&quot;,&quot;statusCode&quot;:400,&quot;message&quot;:&quot;The request is invalid.&quot;,&quot;details&quot;:[{&quot;code&quot;:&quot;InvalidOverwriteRequest&quot;,&quot;message&quot;:&quot;Invalid overwrite request - cannot update container resource requirements, dns name label, or deployment type. Please delete and redeploy this service.&quot;}],&quot;correlation&quot;:{&quot;RequestId&quot;:&quot;823e8483923846b1958c08ffaba074ff&quot;}}'&quot;    <br \/>\n2020-06-08T12:50:27.9212109Z     }    <br \/>\n2020-06-08T12:50:27.9212376Z }}    <br \/>\n2020-06-08T12:50:27.9213437Z {'Azure-cli-ml Version': '1.6.0', 'Error': WebserviceException:    <br \/>\n2020-06-08T12:50:27.9214158Z  Message: Received bad response from Model Management Service:    <br \/>\n2020-06-08T12:50:27.9214688Z Response Code: 400    <br \/>\n2020-06-08T12:50:27.9217800Z Headers: {'Date': 'Mon, 08 Jun 2020 12:50:27 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d', 'x-ms-client-request-id': '823e8483923846b1958c08ffaba074ff', 'x-ms-client-session-id': '62e84a29-b77c-456f-9d91-5ca6be26f79c', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}    <br \/>\n2020-06-08T12:50:27.9222115Z Content: b'{&quot;code&quot;:&quot;BadRequest&quot;,&quot;statusCode&quot;:400,&quot;message&quot;:&quot;The request is invalid.&quot;,&quot;details&quot;:[{&quot;code&quot;:&quot;InvalidOverwriteRequest&quot;,&quot;message&quot;:&quot;Invalid overwrite request - cannot update container resource requirements, dns name label, or deployment type. Please delete and redeploy this service.&quot;}],&quot;correlation&quot;:{&quot;RequestId&quot;:&quot;823e8483923846b1958c08ffaba074ff&quot;}}'    <br \/>\n2020-06-08T12:50:27.9223705Z  InnerException None    <br \/>\n2020-06-08T12:50:27.9224049Z  ErrorResponse     <br \/>\n2020-06-08T12:50:27.9224320Z {    <br \/>\n2020-06-08T12:50:27.9224617Z     &quot;error&quot;: {    <br \/>\n2020-06-08T12:50:27.9229025Z         &quot;message&quot;: &quot;Received bad response from Model Management Service:\\nResponse Code: 400\\nHeaders: {'Date': 'Mon, 08 Jun 2020 12:50:27 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d', 'x-ms-client-request-id': '823e8483923846b1958c08ffaba074ff', 'x-ms-client-session-id': '62e84a29-b77c-456f-9d91-5ca6be26f79c', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\\nContent: b'{&quot;code&quot;:&quot;BadRequest&quot;,&quot;statusCode&quot;:400,&quot;message&quot;:&quot;The request is invalid.&quot;,&quot;details&quot;:[{&quot;code&quot;:&quot;InvalidOverwriteRequest&quot;,&quot;message&quot;:&quot;Invalid overwrite request - cannot update container resource requirements, dns name label, or deployment type. Please delete and redeploy this service.&quot;}],&quot;correlation&quot;:{&quot;RequestId&quot;:&quot;823e8483923846b1958c08ffaba074ff&quot;}}'&quot;    <br \/>\n2020-06-08T12:50:27.9230782Z     }    <br \/>\n2020-06-08T12:50:27.9230908Z }}    <br \/>\n2020-06-08T12:50:27.9231134Z Event: Cli.PostExecute [&lt;function AzCliLogging.deinit_cmd_metadata_logging at 0x7fea2ca1f730&gt;]    <br \/>\n2020-06-08T12:50:27.9231431Z az_command_data_logger : exit code: 1    <br \/>\n2020-06-08T12:50:27.9275693Z telemetry.save : Save telemetry record of length 7390 in cache    <br \/>\n2020-06-08T12:50:27.9280735Z telemetry.check : Negative: The \/home\/vsts\/work\/_temp\/.azclitask\/telemetry.txt was modified at 2020-06-08 12:47:41.161160, which in less than 600.000000 s    <br \/>\n2020-06-08T12:50:27.9290480Z command ran in 55.735 seconds.    <br \/>\n2020-06-08T12:50:28.1525434Z ##[error]Script failed with exit code: 1    <br \/>\n2020-06-08T12:50:28.1536650Z [command]\/opt\/hostedtoolcache\/Python\/3.6.10\/x64\/bin\/az account clear    <br \/>\n2020-06-08T12:50:29.9078943Z ##[section]Finishing: Deploy Model to ACI    <\/p>\n<p>But when i tried to Deploy it using Python SDK it works as well. Is there any permission issues or login to be set before using DevOps Release. I have not done any sort of login in my DevOps Build pipeline.    <\/p>\n<p>Any pointers on what is going wrong here ? It would be really helpful.    <\/p>\n<p>Thanks,    <br \/>\nSrijith    <\/p>",
        "Challenge_closed_time":1592223420340,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592223294357,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/36104\/deployment-of-multiple-models-to-container-instanc",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":87.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":60,
        "Challenge_solved_time":0.0349952778,
        "Challenge_title":"Deployment of Multiple Models to Container Instance Fails in Azure DevOps",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":509,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>They're actively answering Devops question in dedicated forums here.  <\/p>\n<p><a href=\"https:\/\/developercommunity.visualstudio.com\/spaces\/21\/index.html\">https:\/\/developercommunity.visualstudio.com\/spaces\/21\/index.html<\/a>  <\/p>\n<p>--please don't forget to <strong>Accept as answer<\/strong> if the reply is helpful--  <\/p>\n<hr \/>\n<p>Regards, Dave Patrick ....    <br \/>\nMicrosoft Certified Professional    <br \/>\nMicrosoft MVP [Windows Server] Datacenter Management    <\/p>\n<p>Disclaimer: This posting is provided &quot;AS IS&quot; with no warranties or guarantees, and confers no rights.  <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.3,
        "Solution_reading_time":7.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":59.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":64.5577547223,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>For the best run and fitted model from a previously run experiment, Looking for the python code.<\/p>",
        "Challenge_closed_time":1590393765270,
        "Challenge_comment_count":1,
        "Challenge_created_time":1590161357353,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61958473",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":1.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":64.5577547223,
        "Challenge_title":"Code for Best Run and Model from previous experiment",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":103.0,
        "Challenge_word_count":25,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Below is the code you can reuse\n<a href=\"https:\/\/github.com\/microsoft\/MLOpsPython\/blob\/master\/diabetes_regression\/evaluate\/evaluate_model.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/microsoft\/MLOpsPython\/blob\/master\/diabetes_regression\/evaluate\/evaluate_model.py<\/a><\/p>\n\n<p>Assuming in each previous experiment run, a model was registered with a tag that contains a metric of interest (test_mae for example), below is the code to retrieve the version with lowest mae.<\/p>\n\n<pre><code>from azureml.core.model import Model\n\nmodel_name = \"YOUR_MODEL_NAME\"\nmodel_path = \"LOCAL_PATH\u201d\nmodel_version_list = [(model.version,float(model.tags[\"test_mae\"])) for model in Model.list(workspace = ws,name =model_name)]\nmodel_version_list.sort(key = lambda a: a[0])\nlowest_mae_version =model_version_list[0][0]\nprint(\"best version is {} with mae at {}\".format(lowest_mae_version,model_version_list[0][1]))\nmodel = Model(name = model_name,workspace = ws, version =lowest_mae_version)\nmodel.download(model_path, exist_ok=True)\n<\/code><\/pre>\n\n<p>when the model has not been registered,models in an automl run and  would like to get all the models and compare the results depending on featurization, method used, and metrics, also with other data sets. The models are all inside the workspace with the GUI you can see them and download them by hand. <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.2,
        "Solution_reading_time":17.54,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":137.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416648155470,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":14749.0,
        "Answerer_view_count":968.0,
        "Challenge_adjusted_solved_time":7321.6952480556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The documentations of how to use SageMaker estimators are scattered around, sometimes obsolete, incorrect. Is there a one stop location which gives the comprehensive views of how to use SageMaker SDK Estimator to train and save models?<\/p>",
        "Challenge_closed_time":1630555307168,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630555307170,
        "Challenge_favorite_count":14.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69024005",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":3.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":27.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"How to use SageMaker Estimator for model training and saving",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":6655.0,
        "Challenge_word_count":46,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416648155470,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":14749.0,
        "Poster_view_count":968.0,
        "Solution_body":"<h1>Answer<\/h1>\n<p>There is no one such resource from AWS that provides the comprehensive view of how to use SageMaker SDK Estimator to train and save models.<\/p>\n<h2>Alternative Overview Diagram<\/h2>\n<p>I put a diagram and brief explanation to get the overview on how SageMaker Estimator runs a training.<\/p>\n<ol>\n<li><p>SageMaker sets up a docker container for a training job where:<\/p>\n<ul>\n<li>Environment variables are set as in <a href=\"https:\/\/github.com\/aws\/sagemaker-containers#important-environment-variables\" rel=\"noreferrer\">SageMaker Docker Container. Environment Variables<\/a>.<\/li>\n<li>Training data is setup under <code>\/opt\/ml\/input\/data<\/code>.<\/li>\n<li>Training script codes are setup under <code>\/opt\/ml\/code<\/code>.<\/li>\n<li><code>\/opt\/ml\/model<\/code> and <code>\/opt\/ml\/output<\/code> directories are setup to store training outputs.<\/li>\n<\/ul>\n<\/li>\n<\/ol>\n<pre><code>\/opt\/ml\n\u251c\u2500\u2500 input\n\u2502   \u251c\u2500\u2500 config\n\u2502   \u2502   \u251c\u2500\u2500 hyperparameters.json  &lt;--- From Estimator hyperparameter arg\n\u2502   \u2502   \u2514\u2500\u2500 resourceConfig.json\n\u2502   \u2514\u2500\u2500 data\n\u2502       \u2514\u2500\u2500 &lt;channel_name&gt;        &lt;--- From Estimator fit method inputs arg\n\u2502           \u2514\u2500\u2500 &lt;input data&gt;\n\u251c\u2500\u2500 code\n\u2502   \u2514\u2500\u2500 &lt;code files&gt;              &lt;--- From Estimator src_dir arg\n\u251c\u2500\u2500 model\n\u2502   \u2514\u2500\u2500 &lt;model files&gt;             &lt;--- Location to save the trained model artifacts\n\u2514\u2500\u2500 output\n    \u2514\u2500\u2500 failure                   &lt;--- Training job failure logs\n<\/code><\/pre>\n<ol start=\"2\">\n<li><p>SageMaker Estimator <code>fit(inputs)<\/code> method executes the training script. Estimator <code>hyperparameters<\/code> and <code>fit<\/code> method <code>inputs<\/code> are provided as its command line arguments.<\/p>\n<\/li>\n<li><p>The training script saves the model artifacts in the <code>\/opt\/ml\/model<\/code> once the training is completed.<\/p>\n<\/li>\n<li><p>SageMaker archives the artifacts under <code>\/opt\/ml\/model<\/code> into <code>model.tar.gz<\/code> and save it to the S3 location specified to <code>output_path<\/code> Estimator parameter.<\/p>\n<\/li>\n<li><p>You can set Estimator <code>metric_definitions<\/code> parameter to extract model metrics from the training logs. Then you can monitor the training progress in the SageMaker console metrics.<\/p>\n<\/li>\n<\/ol>\n<p><a href=\"https:\/\/i.stack.imgur.com\/gi8bU.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gi8bU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I believe AWS needs to stop mass-producing verbose, redundant, wordy, scattered, and obsolete documents. AWS needs to understand <strong>A picture is worth thousand words<\/strong>.<\/p>\n<p>Have diagrams and piece document parts together in a <strong>context<\/strong> with a clear objective to achieve.<\/p>\n<hr \/>\n<h1>Problem<\/h1>\n<p>AWS documentations need serious re-design and re-structuring. Just to understand <strong>how to train and save a model<\/strong> forces us going through dozens of scattered,  fragmented, verbose, redundant documentations, which are often obsolete, incomplete, and sometime incorrect.<\/p>\n<p>It is well-summarized in <a href=\"https:\/\/nandovillalba.medium.com\/why-i-think-gcp-is-better-than-aws-ea78f9975bda\" rel=\"noreferrer\">Why I think GCP is better than AWS<\/a>:<\/p>\n<blockquote>\n<p>It\u2019s not that AWS is harder to use than GCP, it\u2019s that <strong>it is needlessly hard<\/strong>; a disjointed, sprawl of infrastructure primitives with poor cohesion between them.  <br><br>\nA challenge is nice, a confusing mess is not, and <strong>the problem with AWS is that a large part of your working hours will be spent untangling their documentation and weeding through features and products to find what you want<\/strong>, rather than focusing on cool interesting challenges.<\/p>\n<\/blockquote>\n<p>Especially the SageMaker team keeps changing implementations without updating documents. Its roll-out was also inconsistent, e.g. SDK version 2 was rolled out in the SageMaker Studio making the AWS examples in Github incompatible without announcing it. Whereas SageMaker instance still had SDK 1, hence code worked in Instance but not in Studio.<\/p>\n<p>It is mind-boggling that we have to go through these many documents below to understand how to use the SageMaker SDK Estimator for training.<\/p>\n<h2>Documents for Model Training<\/h2>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\" rel=\"noreferrer\">Train a Model with Amazon SageMaker<\/a><\/li>\n<\/ul>\n<p>This document gives 20,000 feet overview of how SageMaker training but does not give any clue what to do.<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker-workshop.com\/custom\/containers.html\" rel=\"noreferrer\">Running a container for Amazon SageMaker training<\/a><\/li>\n<\/ul>\n<p>This document gives an overview of how SageMaker training looks like. However, this is not up-to-date as it is based on <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"noreferrer\">SageMaker Containers<\/a> which is obsolete.<\/p>\n<blockquote>\n<p>WARNING: This package has been deprecated. Please use the SageMaker Training Toolkit for model training and the SageMaker Inference Toolkit for model serving.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model.html\" rel=\"noreferrer\">Step 4: Train a Model<\/a><\/li>\n<\/ul>\n<p>This document layouts the steps for training.<\/p>\n<blockquote>\n<p>The Amazon SageMaker Python SDK provides framework estimators and generic estimators to train your model while orchestrating the machine learning (ML) lifecycle accessing the SageMaker features for training and the AWS infrastructures<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#train-a-model-with-the-sagemaker-python-sdk\" rel=\"noreferrer\">Train a Model with the SageMaker Python SDK<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>To train a model by using the SageMaker Python SDK, you:<\/p>\n<ul>\n<li>Prepare a training script<\/li>\n<li>Create an estimator<\/li>\n<li>Call the fit method of the estimator<\/li>\n<\/ul>\n<\/blockquote>\n<p>Finally this document gives concrete steps and ideas. However still missing comprehensiv details about Environment Variables, Directory structure in the SageMaker docker container**, S3 for uploading code, placing data, S3 where the trained model is saved, etc.<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html\" rel=\"noreferrer\">Use TensorFlow with the SageMaker Python SDK<\/a><\/li>\n<\/ul>\n<p>This documents is focused on TensorFlow Estimator implementation steps. Use <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/frameworks\/tensorflow\/get_started_mnist_train.ipynb\" rel=\"noreferrer\">Training a Tensorflow Model on MNIST<\/a> Github example to accompany with to follow the actual implementation.<\/p>\n<h2>Documents for passing parameters and data locations<\/h2>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-inputdataconfig\" rel=\"noreferrer\">How Amazon SageMaker Provides Training Information<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>This section explains how SageMaker makes training information, such as training data, hyperparameters, and other configuration information, available to your Docker container.<\/p>\n<\/blockquote>\n<p>This document finally gives the idea of how parameters and data are passed around but again, not comprehensive.<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-containers#important-environment-variables\" rel=\"noreferrer\">SageMaker Docker Container Environment Variables<\/a><\/li>\n<\/ul>\n<p>This documentation is marked as <strong>deprecated<\/strong> but the only document which explains the SageMaker Environment Variables.<\/p>\n<blockquote>\n<h3>IMPORTANT ENVIRONMENT VARIABLES<\/h3>\n<ul>\n<li>SM_MODEL_DIR<\/li>\n<li>SM_CHANNELS<\/li>\n<li>SM_CHANNEL_{channel_name}<\/li>\n<li>SM_HPS<\/li>\n<li>SM_HP_{hyperparameter_name}<\/li>\n<li>SM_CURRENT_HOST<\/li>\n<li>SM_HOSTS<\/li>\n<li>SM_NUM_GPUS<\/li>\n<\/ul>\n<h3>List of provided environment variables by SageMaker Containers<\/h3>\n<ul>\n<li>SM_NUM_CPUS<\/li>\n<li>SM_LOG_LEVEL<\/li>\n<li>SM_NETWORK_INTERFACE_NAME<\/li>\n<li>SM_USER_ARGS<\/li>\n<li>SM_INPUT_DIR<\/li>\n<li>SM_INPUT_CONFIG_DIR<\/li>\n<li>SM_OUTPUT_DATA_DIR<\/li>\n<li>SM_RESOURCE_CONFIG<\/li>\n<li>SM_INPUT_DATA_CONFIG<\/li>\n<li>SM_TRAINING_ENV<\/li>\n<\/ul>\n<\/blockquote>\n<h2>Documents for SageMaker Docker Container Directory Structure<\/h2>\n<ul>\n<li><a href=\"https:\/\/sagemaker-workshop.com\/custom\/containers.html\" rel=\"noreferrer\">Running a container for Amazon SageMaker training<\/a><\/li>\n<\/ul>\n<pre><code>\/opt\/ml\n\u251c\u2500\u2500 input\n\u2502   \u251c\u2500\u2500 config\n\u2502   \u2502   \u251c\u2500\u2500 hyperparameters.json\n\u2502   \u2502   \u2514\u2500\u2500 resourceConfig.json\n\u2502   \u2514\u2500\u2500 data\n\u2502       \u2514\u2500\u2500 &lt;channel_name&gt;\n\u2502           \u2514\u2500\u2500 &lt;input data&gt;\n\u251c\u2500\u2500 model\n\u2502   \u2514\u2500\u2500 &lt;model files&gt;\n\u2514\u2500\u2500 output\n    \u2514\u2500\u2500 failure\n<\/code><\/pre>\n<p>This document explains the directory structure and purpose of each directory.<\/p>\n<blockquote>\n<h3>The input<\/h3>\n<ul>\n<li>\/opt\/ml\/input\/config contains information to control how your program runs. hyperparameters.json is a JSON-formatted dictionary of hyperparameter names to values. These values will always be strings, so you may need to convert them. resourceConfig.json is a JSON-formatted file that describes the network layout used for distributed training. Since scikit-learn doesn\u2019t support distributed training, we\u2019ll ignore it here.<\/li>\n<li>\/opt\/ml\/input\/data\/&lt;channel_name&gt;\/ (for File mode) contains the input data for that channel. The channels are created based on the call to CreateTrainingJob but it\u2019s generally important that channels match what the algorithm expects. The files for each channel will be copied from S3 to this directory, preserving the tree structure indicated by the S3 key structure.<\/li>\n<li>\/opt\/ml\/input\/data\/&lt;channel_name&gt;_&lt;epoch_number&gt; (for Pipe mode) is the pipe for a given epoch. Epochs start at zero and go up by one each time you read them. There is no limit to the number of epochs that you can run, but you must close each pipe before reading the next epoch.<\/li>\n<\/ul>\n<h3>The output<\/h3>\n<ul>\n<li>\/opt\/ml\/model\/ is the directory where you write the model that your algorithm generates. Your model can be in any format that you want. It can be a single file or a whole directory tree. SageMaker will package any files in this directory into a compressed tar archive file. This file will be available at the S3 location returned in the DescribeTrainingJob result.<\/li>\n<li>\/opt\/ml\/output is a directory where the algorithm can write a file failure that describes why the job failed. The contents of this file will be returned in the FailureReason field of the DescribeTrainingJob result. For jobs that succeed, there is no reason to write this file as it will be ignored.<\/li>\n<\/ul>\n<\/blockquote>\n<p>However, this is not up-to-date as it is based on <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"noreferrer\">SageMaker Containers<\/a> which is obsolete.<\/p>\n<h2>Documents for Model Saving<\/h2>\n<p>The information on where the trained model is saved and in what format are fundamentally missing. The training script needs to save the model under <code>\/opt\/ml\/model<\/code> and the format and sub-directory structure depend on the frameworks e,g TensorFlow, Pytorch. This is because SageMaker deployment uses the Framework dependent model-serving, e,g. TensorFlow Serving for TensorFlow framework.<\/p>\n<p>This is not clearly documented and causing confusions. The developer needs to specify which format to use and under which sub-directory to save.<\/p>\n<p>To use TensorFlow Estimator training and deployment:<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/aws_sagemaker_studio\/frameworks\/keras_pipe_mode_horovod\/keras_pipe_mode_horovod_cifar10.html#Deploy-the-trained-model\" rel=\"noreferrer\">Deploy the trained model<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>Because <strong>we\u2019re using TensorFlow Serving for deployment<\/strong>, our training script <strong>saves the model in TensorFlow\u2019s SavedModel format<\/strong>.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/frameworks\/tensorflow\/code\/train.py#L159-L166\" rel=\"noreferrer\">amazon-sagemaker-examples\/frameworks\/tensorflow\/code\/train.py <\/a><\/li>\n<\/ul>\n<pre><code>    # Save the model\n    # A version number is needed for the serving container\n    # to load the model\n    version = &quot;00000000&quot;\n    ckpt_dir = os.path.join(args.model_dir, version)\n    if not os.path.exists(ckpt_dir):\n        os.makedirs(ckpt_dir)\n    model.save(ckpt_dir)\n<\/code><\/pre>\n<p>The code is saving the model in <code>\/opt\/ml\/model\/00000000<\/code> because this is for TensorFlow serving.<\/p>\n<ul>\n<li><a href=\"https:\/\/www.tensorflow.org\/guide\/saved_model\" rel=\"noreferrer\">Using the SavedModel format<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>The save-path follows a convention used by TensorFlow Serving where the last path component (1\/ here) is a version number for your model - it allows tools like Tensorflow Serving to reason about the relative freshness.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/www.tensorflow.org\/tfx\/tutorials\/serving\/rest_simple#save_your_model\" rel=\"noreferrer\">Train and serve a TensorFlow model with TensorFlow Serving<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>To load our trained model into TensorFlow Serving we first need to save it in SavedModel format. This will create a protobuf file in a well-defined directory hierarchy, and will include a version number. TensorFlow Serving allows us to select which version of a model, or &quot;servable&quot; we want to use when we make inference requests. Each version will be exported to a different sub-directory under the given path.<\/p>\n<\/blockquote>\n<h2>Documents for API<\/h2>\n<p>Basically the SageMaker SDK Estimator implements the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html\" rel=\"noreferrer\">CreateTrainingJob<\/a> API for training part. Hence, better to understand how it is designed and what parameters need to be defined. Otherwise working on Estimators are like walking in the dark.<\/p>\n<hr \/>\n<h1>Example<\/h1>\n<h2>Jupyter Notebook<\/h2>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\nbucket = sagemaker_session.default_bucket()\n\nmetric_definitions = [\n    {&quot;Name&quot;: &quot;train:loss&quot;, &quot;Regex&quot;: &quot;.*loss: ([0-9\\\\.]+) - accuracy: [0-9\\\\.]+.*&quot;},\n    {&quot;Name&quot;: &quot;train:accuracy&quot;, &quot;Regex&quot;: &quot;.*loss: [0-9\\\\.]+ - accuracy: ([0-9\\\\.]+).*&quot;},\n    {\n        &quot;Name&quot;: &quot;validation:accuracy&quot;,\n        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: ([0-9\\\\.]+).*&quot;,\n    },\n    {\n        &quot;Name&quot;: &quot;validation:loss&quot;,\n        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: ([0-9\\\\.]+) - val_accuracy: [0-9\\\\.]+.*&quot;,\n    },\n    {\n        &quot;Name&quot;: &quot;sec\/sample&quot;,\n        &quot;Regex&quot;: &quot;.* - \\d+s (\\d+)[mu]s\/sample - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: [0-9\\\\.]+&quot;,\n    },\n]\n\nimport uuid\n\ncheckpoint_s3_prefix = &quot;checkpoints\/{}&quot;.format(str(uuid.uuid4()))\ncheckpoint_s3_uri = &quot;s3:\/\/{}\/{}\/&quot;.format(bucket, checkpoint_s3_prefix)\n\nfrom sagemaker.tensorflow import TensorFlow\n\n# --------------------------------------------------------------------------------\n# 'trainingJobName' msut satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}\n# --------------------------------------------------------------------------------\nbase_job_name = &quot;fashion-mnist&quot;\nhyperparameters = {\n    &quot;epochs&quot;: 2, \n    &quot;batch-size&quot;: 64\n}\nestimator = TensorFlow(\n    entry_point=&quot;fashion_mnist.py&quot;,\n    source_dir=&quot;src&quot;,\n    metric_definitions=metric_definitions,\n    hyperparameters=hyperparameters,\n    role=role,\n    input_mode='File',\n    framework_version=&quot;2.3.1&quot;,\n    py_version=&quot;py37&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    base_job_name=base_job_name,\n    checkpoint_s3_uri=checkpoint_s3_uri,\n    model_dir=False\n)\nestimator.fit()\n<\/code><\/pre>\n<h2>fashion_mnist.py<\/h2>\n<pre><code>import os\nimport argparse\nimport json\nimport multiprocessing\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nfrom tensorflow.keras import backend as K\n\nprint(&quot;TensorFlow version: {}&quot;.format(tf.__version__))\nprint(&quot;Eager execution is: {}&quot;.format(tf.executing_eagerly()))\nprint(&quot;Keras version: {}&quot;.format(tf.keras.__version__))\n\n\nimage_width = 28\nimage_height = 28\n\n\ndef load_data():\n    fashion_mnist = tf.keras.datasets.fashion_mnist\n    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n\n    number_of_classes = len(set(y_train))\n    print(&quot;number_of_classes&quot;, number_of_classes)\n\n    x_train = x_train \/ 255.0\n    x_test = x_test \/ 255.0\n    x_full = np.concatenate((x_train, x_test), axis=0)\n    print(x_full.shape)\n\n    print(type(x_train))\n    print(x_train.shape)\n    print(x_train.dtype)\n    print(y_train.shape)\n    print(y_train.dtype)\n\n    # ## Train\n    # * C: Convolution layer\n    # * P: Pooling layer\n    # * B: Batch normalization layer\n    # * F: Fully connected layer\n    # * O: Output fully connected softmax layer\n\n    # Reshape data based on channels first \/ channels last strategy.\n    # This is dependent on whether you use TF, Theano or CNTK as backend.\n    # Source: https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/mnist_cnn.py\n    if K.image_data_format() == 'channels_first':\n        x = x_train.reshape(x_train.shape[0], 1, image_width, image_height)\n        x_test = x_test.reshape(x_test.shape[0], 1, image_width, image_height)\n        input_shape = (1, image_width, image_height)\n    else:\n        x_train = x_train.reshape(x_train.shape[0], image_width, image_height, 1)\n        x_test = x_test.reshape(x_test.shape[0], image_width, image_height, 1)\n        input_shape = (image_width, image_height, 1)\n\n    return x_train, y_train, x_test, y_test, input_shape, number_of_classes\n\n# tensorboard --logdir=\/full_path_to_your_logs\n\nvalidation_split = 0.2\nverbosity = 1\nuse_multiprocessing = True\nworkers = multiprocessing.cpu_count()\n\n\ndef train(model, x, y, args):\n    # SavedModel Output\n    tensorflow_saved_model_path = os.path.join(args.model_dir, &quot;tensorflow\/saved_model\/0&quot;)\n    os.makedirs(tensorflow_saved_model_path, exist_ok=True)\n\n    # Tensorboard Logs\n    tensorboard_logs_path = os.path.join(args.model_dir, &quot;tensorboard\/&quot;)\n    os.makedirs(tensorboard_logs_path, exist_ok=True)\n\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n        log_dir=tensorboard_logs_path,\n        write_graph=True,\n        write_images=True,\n        histogram_freq=1,  # How often to log histogram visualizations\n        embeddings_freq=1,  # How often to log embedding visualizations\n        update_freq=&quot;epoch&quot;,\n    )  # How often to write logs (default: once per epoch)\n\n    model.compile(\n        optimizer='adam',\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\n        metrics=['accuracy']\n    )\n    history = model.fit(\n        x,\n        y,\n        shuffle=True,\n        batch_size=args.batch_size,\n        epochs=args.epochs,\n        validation_split=validation_split,\n        use_multiprocessing=use_multiprocessing,\n        workers=workers,\n        verbose=verbosity,\n        callbacks=[\n            tensorboard_callback\n        ]\n    )\n    return history\n\n\ndef create_model(input_shape, number_of_classes):\n    model = Sequential([\n        Conv2D(\n            name=&quot;conv01&quot;,\n            filters=32,\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=&quot;same&quot;,\n            activation='relu',\n            input_shape=input_shape\n        ),\n        MaxPooling2D(\n            name=&quot;pool01&quot;,\n            pool_size=(2, 2)\n        ),\n        Flatten(),  # 3D shape to 1D.\n        BatchNormalization(\n            name=&quot;batch_before_full01&quot;\n        ),\n        Dense(\n            name=&quot;full01&quot;,\n            units=300,\n            activation=&quot;relu&quot;\n        ),  # Fully connected layer\n        Dense(\n            name=&quot;output_softmax&quot;,\n            units=number_of_classes,\n            activation=&quot;softmax&quot;\n        )\n    ])\n    return model\n\n\ndef save_model(model, args):\n    # Save the model\n    # A version number is needed for the serving container\n    # to load the model\n    version = &quot;00000000&quot;\n    model_save_dir = os.path.join(args.model_dir, version)\n    if not os.path.exists(model_save_dir):\n        os.makedirs(model_save_dir)\n    print(f&quot;saving model at {model_save_dir}&quot;)\n    model.save(model_save_dir)\n\n\ndef parse_args():\n    # --------------------------------------------------------------------------------\n    # https:\/\/docs.python.org\/dev\/library\/argparse.html#dest\n    # --------------------------------------------------------------------------------\n    parser = argparse.ArgumentParser()\n\n    # --------------------------------------------------------------------------------\n    # hyperparameters Estimator argument are passed as command-line arguments to the script.\n    # --------------------------------------------------------------------------------\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch-size', type=int, default=64)\n\n    # \/opt\/ml\/model\n    # sagemaker.tensorflow.estimator.TensorFlow override 'model_dir'.\n    # See https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/\\\n    # sagemaker.tensorflow.html#sagemaker.tensorflow.estimator.TensorFlow\n    parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])\n\n    # \/opt\/ml\/output\n    parser.add_argument(&quot;--output_dir&quot;, type=str, default=os.environ[&quot;SM_OUTPUT_DIR&quot;])\n\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == &quot;__main__&quot;:\n    args = parse_args()\n    print(&quot;---------- key\/value args&quot;)\n    for key, value in vars(args).items():\n        print(f&quot;{key}:{value}&quot;)\n\n    x_train, y_train, x_test, y_test, input_shape, number_of_classes = load_data()\n    model = create_model(input_shape, number_of_classes)\n\n    history = train(model=model, x=x_train, y=y_train, args=args)\n    print(history)\n    \n    save_model(model, args)\n    results = model.evaluate(x_test, y_test, batch_size=100)\n    print(&quot;test loss, test accuracy:&quot;, results)\n<\/code><\/pre>\n<h2>SageMaker Console<\/h2>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ctcLy.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ctcLy.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h2>Notebook output<\/h2>\n<pre><code>2021-09-03 03:02:04 Starting - Starting the training job...\n2021-09-03 03:02:16 Starting - Launching requested ML instancesProfilerReport-1630638122: InProgress\n......\n2021-09-03 03:03:17 Starting - Preparing the instances for training.........\n2021-09-03 03:04:59 Downloading - Downloading input data\n2021-09-03 03:04:59 Training - Downloading the training image...\n2021-09-03 03:05:23 Training - Training image download completed. Training in progress.2021-09-03 03:05:23.966037: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n2021-09-03 03:05:23.969704: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n2021-09-03 03:05:24.118054: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n2021-09-03 03:05:26,842 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\n2021-09-03 03:05:26,852 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:27,734 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n\/usr\/local\/bin\/python3.7 -m pip install -r requirements.txt\nWARNING: You are using pip version 21.0.1; however, version 21.2.4 is available.\nYou should consider upgrading via the '\/usr\/local\/bin\/python3.7 -m pip install --upgrade pip' command.\n\n2021-09-03 03:05:29,028 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,045 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,062 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,072 sagemaker-training-toolkit INFO     Invoking user script\n\nTraining Env:\n\n{\n    &quot;additional_framework_parameters&quot;: {},\n    &quot;channel_input_dirs&quot;: {},\n    &quot;current_host&quot;: &quot;algo-1&quot;,\n    &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,\n    &quot;hosts&quot;: [\n        &quot;algo-1&quot;\n    ],\n    &quot;hyperparameters&quot;: {\n        &quot;batch-size&quot;: 64,\n        &quot;epochs&quot;: 2\n    },\n    &quot;input_config_dir&quot;: &quot;\/opt\/ml\/input\/config&quot;,\n    &quot;input_data_config&quot;: {},\n    &quot;input_dir&quot;: &quot;\/opt\/ml\/input&quot;,\n    &quot;is_master&quot;: true,\n    &quot;job_name&quot;: &quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,\n    &quot;log_level&quot;: 20,\n    &quot;master_hostname&quot;: &quot;algo-1&quot;,\n    &quot;model_dir&quot;: &quot;\/opt\/ml\/model&quot;,\n    &quot;module_dir&quot;: &quot;s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz&quot;,\n    &quot;module_name&quot;: &quot;fashion_mnist&quot;,\n    &quot;network_interface_name&quot;: &quot;eth0&quot;,\n    &quot;num_cpus&quot;: 4,\n    &quot;num_gpus&quot;: 0,\n    &quot;output_data_dir&quot;: &quot;\/opt\/ml\/output\/data&quot;,\n    &quot;output_dir&quot;: &quot;\/opt\/ml\/output&quot;,\n    &quot;output_intermediate_dir&quot;: &quot;\/opt\/ml\/output\/intermediate&quot;,\n    &quot;resource_config&quot;: {\n        &quot;current_host&quot;: &quot;algo-1&quot;,\n        &quot;hosts&quot;: [\n            &quot;algo-1&quot;\n        ],\n        &quot;network_interface_name&quot;: &quot;eth0&quot;\n    },\n    &quot;user_entry_point&quot;: &quot;fashion_mnist.py&quot;\n}\n\nEnvironment variables:\n\nSM_HOSTS=[&quot;algo-1&quot;]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={&quot;batch-size&quot;:64,&quot;epochs&quot;:2}\nSM_USER_ENTRY_POINT=fashion_mnist.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}\nSM_INPUT_DATA_CONFIG={}\nSM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\nSM_CHANNELS=[]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=fashion_mnist\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=\/opt\/ml\/input\nSM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\nSM_OUTPUT_DIR=\/opt\/ml\/output\nSM_NUM_CPUS=4\nSM_NUM_GPUS=0\nSM_MODEL_DIR=\/opt\/ml\/model\nSM_MODULE_DIR=s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz\nSM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;batch-size&quot;:64,&quot;epochs&quot;:2},&quot;input_config_dir&quot;:&quot;\/opt\/ml\/input\/config&quot;,&quot;input_data_config&quot;:{},&quot;input_dir&quot;:&quot;\/opt\/ml\/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1&quot;,&quot;model_dir&quot;:&quot;\/opt\/ml\/model&quot;,&quot;module_dir&quot;:&quot;s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;fashion_mnist&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:4,&quot;num_gpus&quot;:0,&quot;output_data_dir&quot;:&quot;\/opt\/ml\/output\/data&quot;,&quot;output_dir&quot;:&quot;\/opt\/ml\/output&quot;,&quot;output_intermediate_dir&quot;:&quot;\/opt\/ml\/output\/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;fashion_mnist.py&quot;}\nSM_USER_ARGS=[&quot;--batch-size&quot;,&quot;64&quot;,&quot;--epochs&quot;,&quot;2&quot;]\nSM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\nSM_HP_BATCH-SIZE=64\nSM_HP_EPOCHS=2\nPYTHONPATH=\/opt\/ml\/code:\/usr\/local\/bin:\/usr\/local\/lib\/python37.zip:\/usr\/local\/lib\/python3.7:\/usr\/local\/lib\/python3.7\/lib-dynload:\/usr\/local\/lib\/python3.7\/site-packages\n\nInvoking script with the following command:\n\n\/usr\/local\/bin\/python3.7 fashion_mnist.py --batch-size 64 --epochs 2\n\n\nTensorFlow version: 2.3.1\nEager execution is: True\nKeras version: 2.4.0\n---------- key\/value args\nepochs:2\nbatch_size:64\nmodel_dir:\/opt\/ml\/model\noutput_dir:\/opt\/ml\/output\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1656913410063,
        "Solution_link_count":25.0,
        "Solution_readability":17.3,
        "Solution_reading_time":377.84,
        "Solution_score_count":65.0,
        "Solution_sentence_count":201.0,
        "Solution_word_count":2374.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1428654714763,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":596.0,
        "Answerer_view_count":80.0,
        "Challenge_adjusted_solved_time":18.9362980555,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to build an API using an MLflow model.<\/p>\n<p>the funny thing is it works from one location on my PC and not from another. So, the reason for doing I wanted to change my repo etc.<\/p>\n<p>So, the simple code of<\/p>\n<pre><code>from mlflow.pyfunc import load_model\nMODEL_ARTIFACT_PATH = &quot;.\/model\/model_name\/&quot;\nMODEL = load_model(MODEL_ARTIFACT_PATH)\n<\/code><\/pre>\n<p>now fails with<\/p>\n<pre><code>ERROR:    Traceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 540, in lifespan\n    async for item in self.lifespan_context(app):\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 481, in default_lifespan\n    await self.startup()\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 516, in startup\n    await handler()\n  File &quot;\/code\/.\/app\/main.py&quot;, line 32, in startup_load_model\n    MODEL = load_model(MODEL_ARTIFACT_PATH)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 733, in load_model\n    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/spark.py&quot;, line 737, in _load_pyfunc\n    return _PyFuncModelWrapper(spark, _load_model(model_uri=path))\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/spark.py&quot;, line 656, in _load_model\n    return PipelineModel.load(model_uri)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/util.py&quot;, line 332, in load\n    return cls.read().load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/pipeline.py&quot;, line 258, in load\n    return JavaMLReader(self.cls).load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/util.py&quot;, line 282, in load\n    java_obj = self._jread.load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/py4j\/java_gateway.py&quot;, line 1321, in __call__\n    return_value = get_return_value(\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/sql\/utils.py&quot;, line 117, in deco\n    raise converted from None\npyspark.sql.utils.AnalysisException: Unable to infer schema for Parquet. It must be specified manually.\n<\/code><\/pre>\n<p>The model artifacts are already downloaded to the folder \/model folder which has the following structure.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oqxRW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oqxRW.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>the load model call is in the main.py file\nAs I mentioned it works from another directory, but there is no reference to any absolute paths. Also, I have made sure that my package references are identical. e,g I have pinned them all down<\/p>\n<pre><code># Model\nmlflow==1.25.1\nprotobuf==3.20.1\npyspark==3.2.1\nscipy==1.6.2\nsix==1.15.0\n<\/code><\/pre>\n<p>also, the same docker file is used both places, which among other things, makes sure that the final resulting folder structure is the same<\/p>\n<pre><code>......other stuffs\n\nCOPY .\/app \/code\/app\nCOPY .\/model \/code\/model\n<\/code><\/pre>\n<p>what can explain it throwing this exception whereas in another location (on my PC), it works (same model artifacts) ?<\/p>\n<p>Since it uses load_model function, it should be able to read the parquet files ?<\/p>\n<p>Any question and I can explain.<\/p>\n<p>EDIT1: I have debugged this a little more in the docker container and it seems the parquet files in the itemFactors folder (listed in my screenshot above) are not getting copied over to my image , even though I have the copy command to copy all files under the model folder. It is copying the _started , _committed and _SUCCESS files, just not the parquet files. Anyone knows why would that be? I DO NOT have a .dockerignore file. Why are those files ignored while copying?<\/p>",
        "Challenge_closed_time":1655191745992,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655130099670,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1655134670387,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72604450",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":50.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":46,
        "Challenge_solved_time":17.1239783333,
        "Challenge_title":"MLflow load model fails Python",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":109.0,
        "Challenge_word_count":417,
        "Platform":"Stack Overflow",
        "Poster_created_time":1428654714763,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":596.0,
        "Poster_view_count":80.0,
        "Solution_body":"<p>I found the problem. Like I wrote in the EDIT1 of my post, with further observations, the parquet files were missing in the docker container. That was strange because I was copying the entire folder in my Dockerfile.<\/p>\n<p>I then realized that I was hitting this problem <a href=\"https:\/\/github.com\/moby\/buildkit\/issues\/1366\" rel=\"nofollow noreferrer\">mentioned here<\/a>. File paths exceeding 260 characters, silently fail and do not get copied over to the docker container. This was really frustrating because nothing failed during build and then during run, it gave me that cryptic error of &quot;unable to infer schema for parquet&quot;, essentially because the parquet files were not copied over during docker build.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1655202841060,
        "Solution_link_count":1.0,
        "Solution_readability":9.3,
        "Solution_reading_time":9.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":107.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":341.4130555556,
        "Challenge_answer_count":4,
        "Challenge_body":"**Describe the bug**\r\nI can't load a *mlflow* model in the beta version of BentoML 1.0\r\n\r\n**To Reproduce**\r\n1. Train & log a pyfunc model to mflow\r\n```\r\nfrom sklearn import svm, datasets\r\n\r\nimport mlflow\r\n\r\n\r\n# Load training data\r\niris = datasets.load_iris()\r\nX, y = iris.data, iris.target\r\n\r\n# Model Training\r\nclf = svm.SVC()\r\nclf.fit(X, y)\r\n\r\n# Wrap up as a custom pyfunc model\r\nclass ModelPyfunc(mlflow.pyfunc.PythonModel):\r\n    \r\n    def load_context(self, context):\r\n        self.model = clf\r\n    \r\n    def predict(self, context, model_input):\r\n        return self.model.predict(model_input)      \r\n      \r\n# Log model\r\nwith mlflow.start_run() as run:\r\n    model = ModelPyfunc()\r\n    mlflow.pyfunc.log_model(\"model\", python_model=model)\r\n    print(\"run_id: {}\".format(run.info.run_id))\r\n```\r\n\r\n2. Load it into BentoML\r\n```\r\nimport bentoml\r\n\r\nmodel_uri = f\"runs:\/{run.info.run_id}\/model\"\r\n\r\ntag = bentoml.mlflow.import_from_uri(\"model\", model_uri)\r\n\r\nmodel = bentoml.mlflow.load(tag)\r\n```\r\n3. The model gets successfully stored in the local model store (listed in `bentoml models list`), however the loading `model = bentoml.mlflow.load(tag)` is failing to **AttributeError** `module 'mlflow.pyfunc.model' has no attribute 'load_model'`\r\n\r\n**Expected behavior**\r\nPyfunc model should load without issues\r\n\r\n**Screenshots\/Logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"sandbox.py\", line 11, in <module>\r\n    bentoml.mlflow.load(tag)\r\n  File \"\/Users\/e056232\/opt\/miniconda3\/lib\/python3.8\/site-packages\/simple_di\/__init__.py\", line 124, in _\r\n    return func(*_inject_args(bind.args), **_inject_kwargs(bind.kwargs))\r\n  File \"\/Users\/e056232\/opt\/miniconda3\/lib\/python3.8\/site-packages\/bentoml\/_internal\/frameworks\/mlflow.py\", line 85, in load\r\n    return loader_module.load_model(mlflow_folder)  # noqa\r\nAttributeError: module 'mlflow.pyfunc.model' has no attribute 'load_model'\r\n```\r\n\r\n**Environment:**\r\n - OS: MacOS 11.6\r\n - Python Version Python 3.8.5\r\n - BentoML Version BentoML-1.0.0a1",
        "Challenge_closed_time":1642711286000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641482199000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/bentoml\/BentoML\/issues\/2160",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.7,
        "Challenge_reading_time":24.44,
        "Challenge_repo_contributor_count":135.0,
        "Challenge_repo_fork_count":498.0,
        "Challenge_repo_issue_count":3155.0,
        "Challenge_repo_star_count":4302.0,
        "Challenge_repo_watch_count":60.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":341.4130555556,
        "Challenge_title":"MLflow pyfunc model can't be loaded",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":187,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I think this version is not yet up-to-date with our 1.0 branch. cc @parano Line 85 is different from `main` branch @alexdivet @aarnphm the new 1.0.0a2 was just released, could you help confirm the issue has been resolved? ![Screenshot 2022-01-20 at 15 39 52](https:\/\/user-images.githubusercontent.com\/29749331\/150418600-d75d6b01-679d-4fa7-9a66-395262c13569.png)\r\nWorks just fine for me\r\nRunning on M1 Max, Python 3.9.10, with Rosetta 2. Please let me know if you still run into problems @alexdivet It works on my end too. Thanks for resolving it \ud83d\ude4f\ud83c\udffb",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":4.8,
        "Solution_reading_time":6.86,
        "Solution_score_count":3.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":79.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":176.56633,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have successfully built a Sagemaker endpoint using a Tensorflow model. The pre and post processing is done inside &quot;inference.py&quot; which calls a handler function based on this tutorial: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html#how-to-implement-the-pre-and-or-post-processing-handler-s\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html#how-to-implement-the-pre-and-or-post-processing-handler-s<\/a><\/p>\n<p>My questions are:<\/p>\n<ul>\n<li>Which method is good for validating user input data within inference.py?<\/li>\n<li>If such validation tests fail (e.g. wrong data types or data not in allowed range, etc.), how is it possible to return appropriate error messages with status codes to the user?<\/li>\n<li>How is this compatible with the API gateway placed above the endpoint?<\/li>\n<\/ul>\n<p>Here is the structure of the inference.py with the desired validation check as a comment:<\/p>\n<pre><code>import json\nimport requests\n\n\ndef handler(data, context):\n    &quot;&quot;&quot;Handle request.\n    Args:\n        data (obj): the request data\n        context (Context): an object containing request and configuration details\n    Returns:\n        (bytes, string): data to return to client, (optional) response content type\n    &quot;&quot;&quot;\n    processed_input = _process_input(data, context)\n    response = requests.post(context.rest_uri, data=processed_input)\n    return _process_output(response, context)\n\n\ndef _process_input(data, context):\n    if context.request_content_type == 'application\/json':\n        # pass through json (assumes it's correctly formed)\n        d = data.read().decode('utf-8')\n        data_dict = json.loads(data)\n\n\n        # -----&gt;   if data_dict['input_1'] &gt; 25000:\n        # -----&gt;       return some error specific message with status code 123\n\n\n        return some_preprocessing_function(data_dict)\n\n    raise ValueError('{{&quot;error&quot;: &quot;unsupported content type {}&quot;}}'.format(\n        context.request_content_type or &quot;unknown&quot;))\n\n\ndef _process_output(data, context):\n    if data.status_code != 200:\n        raise ValueError(data.content.decode('utf-8'))\n\n    response_content_type = context.accept_header\n    prediction = data.content\n    return prediction, response_content_type\n<\/code><\/pre>",
        "Challenge_closed_time":1645567161520,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644929807890,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1644931522732,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71126832",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.6,
        "Challenge_reading_time":30.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":177.042675,
        "Challenge_title":"Amazon Sagemaker: User Input data validation in Inference Endpoint",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":245.0,
        "Challenge_word_count":220,
        "Platform":"Stack Overflow",
        "Poster_created_time":1613661928947,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":160.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>I will answer your questions inline below:<\/p>\n<ol>\n<li><em>Which method is good for validating user input data within inference.py?<\/em><\/li>\n<\/ol>\n<p>Seeing that you have a <code>handler<\/code> function, <code>input_handler<\/code> and <code>output_handler<\/code> are ignored. Thus, inside your <code>handler<\/code> function (as you are correctly doing) you can have the validation logic.<\/p>\n<ol start=\"2\">\n<li><em>If such validation tests fail (e.g. wrong data types or data not in allowed range, etc.), how is it possible to return appropriate error messages with status codes to the user?<\/em><\/li>\n<\/ol>\n<p>I like to think of my SageMaker endpoint as a web server. Thus, you can return any valid HTTP response code with a response message. Please see this example <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_batch_transform\/tensorflow_cifar-10_with_inference_script\/code\/inference.py#L47\" rel=\"nofollow noreferrer\">inference.py<\/a> file that I found as a reference.<\/p>\n<pre><code>_return_error(\n            415, 'Unsupported content type &quot;{}&quot;'.format(context.request_content_type or &quot;Unknown&quot;)\n        )\n\ndef _return_error(code, message):\n    raise ValueError(&quot;Error: {}, {}&quot;.format(str(code), message))\n<\/code><\/pre>\n<ol start=\"3\">\n<li><em>How is this compatible with the API gateway placed above the endpoint?<\/em><\/li>\n<\/ol>\n<p>Please see this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">link<\/a> for details on Creating a machine learning-powered REST API with Amazon API Gateway mapping templates and Amazon SageMaker.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.5,
        "Solution_reading_time":22.68,
        "Solution_score_count":1.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":178.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.4330788889,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hello MS team,  <\/p>\n<p>I have registered an ML model in the AML workspace using an Azure Machine learning pipeline and triggered the main control script of the pipeline by linking the repo present in Azure DevOps to the AML workspace(using Service principal).   <\/p>\n<p>How do I download the latest version of the model from the AML workspace to the <em>&quot;Artifacts&quot;<\/em> folder in Azure DevOPs?  <\/p>\n<p>Any help is appreciated please.  <\/p>",
        "Challenge_closed_time":1643933985587,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643903626503,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/721792\/how-to-get-model-id-of-the-latest-version-register",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.1,
        "Challenge_reading_time":7.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":8.4330788889,
        "Challenge_title":"How to get Model ID of the Latest Version registered in Azure Machine Learning Service Model Registry using az ml cli?",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":92,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=6755dac2-30f1-48a2-9d0d-4d2c96edc5d4\">@Shivapriya Katta  <\/a>     <\/p>\n<p>I think you are mentioning how to get the latest version of model and download the model in az ml.    <\/p>\n<p>There are 2 steps, one is list the model to get the model ID you want, two is download the model.    <\/p>\n<p><strong>az ml model list<\/strong>    <br \/>\nList models in the workspace.    <\/p>\n<pre><code>az ml model list [--dataset-id]  \n                 [--latest]  \n                 [--model-name]  \n                 [--path]  \n                 [--property]  \n                 [--resource-group]  \n                 [--run-id]  \n                 [--subscription-id]  \n                 [--tag]  \n                 [--workspace-name]  \n                 [-v]  \n<\/code><\/pre>\n<p>Optional Parameters    <br \/>\n--dataset-id    <br \/>\nIf provided, will only show models with the specified dataset ID.    <\/p>\n<p>--latest -l    <br \/>\nIf provided, will only return models with the latest version.    <\/p>\n<p>--model-name -n    <br \/>\nAn optional model name to filter the list by.    <\/p>\n<p>--path    <br \/>\nPath to a project folder. Default: current directory.    <\/p>\n<p>--property    <br \/>\nKey\/value property to add (e.g. key=value ). Multiple properties can be specified with multiple --property options.    <\/p>\n<p>--resource-group -g    <br \/>\nResource group corresponding to the provided workspace.    <\/p>\n<p>--run-id    <br \/>\nIf provided, will only show models with the specified Run ID.    <\/p>\n<p>--subscription-id    <br \/>\nSpecifies the subscription Id.    <\/p>\n<p>--tag    <br \/>\nKey\/value tag to add (e.g. key=value ). Multiple tags can be specified with multiple --tag options.    <\/p>\n<p>--workspace-name -w    <br \/>\nName of the workspace containing models to list.    <\/p>\n<p>-v    <br \/>\nVerbosity flag.    <\/p>\n<p><strong>az ml model download<\/strong>    <br \/>\nDownload a model from the workspace.    <\/p>\n<pre><code>az ml model download --model-id  \n                     --target-dir  \n                     [--overwrite]  \n                     [--path]  \n                     [--resource-group]  \n                     [--subscription-id]  \n                     [--workspace-name]  \n                     [-v]  \n<\/code><\/pre>\n<p>Required Parameters    <br \/>\n--model-id -i    <br \/>\nID of model.    <\/p>\n<p>--target-dir -t    <br \/>\nTarget directory to download the model file to.    <\/p>\n<p>Optional Parameters    <br \/>\n--overwrite    <br \/>\nOverwrite if the same name file exists in target directory.    <\/p>\n<p>--path    <br \/>\nPath to a project folder. Default: current directory.    <\/p>\n<p>--resource-group -g    <br \/>\nResource group corresponding to the provided workspace.    <\/p>\n<p>--subscription-id    <br \/>\nSpecifies the subscription Id.    <\/p>\n<p>--workspace-name -w    <br \/>\nName of the workspace containing model to show.    <\/p>\n<p>-v    <br \/>\nVerbosity flag.    <\/p>\n<p>Hope this helps!     <\/p>\n<p><em>Please kindly accept the answer if you feel helpful, thank you!<\/em>    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.8,
        "Solution_reading_time":32.05,
        "Solution_score_count":0.0,
        "Solution_sentence_count":30.0,
        "Solution_word_count":344.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1623879163643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":224.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":180.3498686111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>i am trying to understand how input_fn, predict_fn and outout_fn work? I am able to understand what they are, but I am not able to understand  how they are called (invoked), can anyone help me understand the same<\/p>",
        "Challenge_closed_time":1661363649820,
        "Challenge_comment_count":1,
        "Challenge_created_time":1660714390293,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73383302",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":3.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":180.3498686111,
        "Challenge_title":"how does input_fn, predict_fn and output_fn work in aws sagemaker script mode?",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":39.0,
        "Challenge_word_count":49,
        "Platform":"Stack Overflow",
        "Poster_created_time":1573543021663,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":17.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>When your endpoint comes up, <code>model_fn<\/code> is invoked so that your model is loaded. When you invoke the endpoint, <code>input_fn<\/code> is called so that your input payload is parsed, immediately after that, <code>predict_fn<\/code> is called so that a prediction is generated, and then <code>output_fn<\/code> is called to parse the prediction before returning it to the caller.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.2,
        "Solution_reading_time":4.95,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1574151667340,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":456.0,
        "Answerer_view_count":31.0,
        "Challenge_adjusted_solved_time":13.4095258333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I got this error when I was trying to have a model registered in the model registry. Could someone help me?<\/p>\n<pre><code>RestException: INVALID_PARAMETER_VALUE: Unsupported URI '.\/mlruns' for model registry store. \nSupported schemes are: ['postgresql', 'mysql', 'sqlite', 'mssql']. \nSee https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#storage for how to setup a compatible server.\n<\/code><\/pre>",
        "Challenge_closed_time":1596626369480,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596578095187,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1598815085667,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63255631",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":6.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":10.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":13.4095258333,
        "Challenge_title":"MLflow: INVALID_PARAMETER_VALUE: Unsupported URI '.\/mlruns' for model registry store",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":12594.0,
        "Challenge_word_count":55,
        "Platform":"Stack Overflow",
        "Poster_created_time":1419619351727,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":125.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>Mlflow required DB as datastore for Model Registry\nSo you have to run tracking server with DB as backend-store and log model to this tracking server.\nThe easiest way to use DB is to use SQLite.<\/p>\n<pre><code>mlflow server \\\n    --backend-store-uri sqlite:\/\/\/mlflow.db \\\n    --default-artifact-root .\/artifacts \\\n    --host 0.0.0.0\n<\/code><\/pre>\n<p>And set MLFLOW_TRACKING_URI environment variable to <em>http:\/\/localhost:5000<\/em> or<\/p>\n<pre><code>mlflow.set_tracking_uri(&quot;http:\/\/localhost:5000&quot;)\n<\/code><\/pre>\n<p>After got to http:\/\/localhost:5000 and you can register a logged model from UI or from the code.<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":8.0,
        "Solution_reading_time":7.98,
        "Solution_score_count":27.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":72.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1352206833663,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":893.0,
        "Answerer_view_count":185.0,
        "Challenge_adjusted_solved_time":28.8670294445,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I'm trying to define a Sagemaker Training Job with an existing Python class. To my understanding, I could create my own container but would rather not deal with container management.<\/p>\n\n<p>When choosing \"Algorithm Source\" there is the option of \"Your own algorithm source\" but nothing is listed under resources. Where does this come from?<\/p>\n\n<p>I know I could do this through a notebook, but I really want this defined in a job that can be invoked through an endpoint.<\/p>",
        "Challenge_closed_time":1550361262523,
        "Challenge_comment_count":0,
        "Challenge_created_time":1550257341217,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54715601",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.4,
        "Challenge_reading_time":7.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":28.8670294445,
        "Challenge_title":"How do I create a Sagemaker training job with my own Tensorflow code without having to build a container?",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":781.0,
        "Challenge_word_count":97,
        "Platform":"Stack Overflow",
        "Poster_created_time":1366768533200,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>As Bruno has said you will have to use a container somewhere, but you can use an existing container to run your own custom tensorflow code.<\/p>\n\n<p>There is a good example <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_quickstart\/tensorflow_script_mode_quickstart.ipynb\" rel=\"nofollow noreferrer\">in the sagemaker github<\/a> for how to do this.<\/p>\n\n<p>The way this works is you modify your code to have an entry point which takes argparse command line arguments, and then you point a 'Sagemaker Tensorflow estimator' to the entry point. Then when you call fit on the sagemaker estimator it will download the tensorflow container and run your custom code in there.<\/p>\n\n<p>So you start off with your own custom code that looks something like this<\/p>\n\n<pre><code># my_custom_code.py\nimport tensorflow as tf\nimport numpy as np\n\ndef build_net():\n    # single fully connected\n    image_place = tf.placeholder(tf.float32, [None, 28*28])\n    label_place = tf.placeholder(tf.int32, [None,])\n    net = tf.layers.dense(image_place, units=1024, activation=tf.nn.relu)\n    net = tf.layers.dense(net, units=10, activation=None)\n    return image_place, label_place, net\n\n\ndef process_data():\n    # load\n    (x_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()\n\n    # center\n    x_train = x_train \/ 255.0\n    m = x_train.mean()\n    x_train = x_train - m\n\n    # convert to right types\n    x_train = x_train.astype(np.float32)\n    y_train = y_train.astype(np.int32)\n\n    # reshape so flat\n    x_train = np.reshape(x_train, [-1, 28*28])\n    return x_train, y_train\n\n\ndef train_model(init_learn, epochs):\n    image_p, label_p, logit = build_net()\n    x_train, y_train = process_data()\n\n    loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n        logits=logit,\n        labels=label_p)\n    optimiser = tf.train.AdamOptimizer(init_learn)\n    train_step = optimiser.minimize(loss)\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for _ in range(epochs):\n            sess.run(train_step, feed_dict={image_p: x_train, label_p: y_train})\n\n\nif __name__ == '__main__':\n    train_model(0.001, 10)\n<\/code><\/pre>\n\n<p>To make it work with sagemaker we need to create a command line entry point, which will allow sagemaker to run it in the container it will download for us eventually.<\/p>\n\n<pre><code># entry.py\n\nimport argparse\nfrom my_custom_code import train_model\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\n        '--model_dir',\n        type=str)\n    parser.add_argument(\n        '--init_learn',\n        type=float)\n    parser.add_argument(\n        '--epochs',\n        type=int)\n    args = parser.parse_args()\n    train_model(args.init_learn, args.epochs)\n<\/code><\/pre>\n\n<p>Apart from specifying the arguments my function needs to take, we also need to provide a <code>model_dir<\/code> argument. This is always required, and is an S3 location which is where an model artifacts will be saved when the training job completes. Note that you don't need to specify what this value is (though you can) as Sagemaker will provide a default location in S3 for you.<\/p>\n\n<p>So we have modified our code, now we need to actually run it on Sagemaker. Go to the AWS console and fire up a small instance from Sagemaker. Download your custom code to the instance, and then create a jupyter notebook as follows:<\/p>\n\n<pre><code># sagemaker_run.ipyb\nimport sagemaker\nfrom sagemaker.tensorflow import TensorFlow\n\nhyperparameters = {\n    'epochs': 10,\n    'init_learn': 0.001}\n\nrole = sagemaker.get_execution_role()\nsource_dir = '\/path\/to\/folder\/with\/my\/code\/on\/instance'\nestimator = TensorFlow(\n    entry_point='entry.py',\n    source_dir=source_dir,\n    train_instance_type='ml.t2.medium',\n    train_instance_count=1,\n    hyperparameters=hyperparameters,\n    role=role,\n    py_version='py3',\n    framework_version='1.12.0',\n    script_mode=True)\n\nestimator.fit()\n<\/code><\/pre>\n\n<p>Running the above will:<\/p>\n\n<ul>\n<li>Spin up an ml.t2.medium instance<\/li>\n<li>Download the tensorflow 1.12.0 container to the instance<\/li>\n<li>Download any data we specify in fit to the newly created instance in fit (in this case nothing)<\/li>\n<li>Run our code on the instance<\/li>\n<li>upload the model artifacts to model_dir<\/li>\n<\/ul>\n\n<p>And that is pretty much it. There is of course a lot not mentioned here but you can:<\/p>\n\n<ul>\n<li>Download training\/testing data from s3<\/li>\n<li>Save checkpoint files, and tensorboard files during training and upload them to s3<\/li>\n<\/ul>\n\n<p>The best resource I found was the example I shared but here are all the things I was looking at to get this working:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_quickstart\/tensorflow_script_mode_quickstart.ipynb\" rel=\"nofollow noreferrer\">example code again<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"nofollow noreferrer\">documentation<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-containers#list-of-provided-environment-variables-by-sagemaker-containers\" rel=\"nofollow noreferrer\">explanation of environment variables<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":12.3,
        "Solution_reading_time":66.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":50.0,
        "Solution_word_count":546.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1620154324507,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"India",
        "Answerer_reputation_count":1169.0,
        "Answerer_view_count":2077.0,
        "Challenge_adjusted_solved_time":69.4640041667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've deployed a tensorflow model in vertex AI platform using TFX Pipelines. The model have custom serving signatures but I'm strugling to specify the signature when I'm predicting.<\/p>\n<p>I've the exact same model deployed in GCP AI Platform and I'm able to specify it.<\/p>\n<p>According to the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-custom-models?authuser=5&amp;_ga=2.16305585.-680038964.1635267137#formatting-prediction-input\" rel=\"nofollow noreferrer\">vertex documentation<\/a>, we must pass a dictionary containing the Instances (List) and the Parameters (Dict) values.<\/p>\n<p>I've submitted these arguments to <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/samples\/snippets\/prediction_service\/predict_custom_trained_model_sample.py\" rel=\"nofollow noreferrer\">this function<\/a>:<\/p>\n<pre><code>instances: [{&quot;argument_n&quot;: &quot;value&quot;}]\n\nparameters: {&quot;signature_name&quot;: &quot;name_of_signature&quot;}\n<\/code><\/pre>\n<p>Doesn't work, it still get the default signature of the model.<\/p>\n<p>In GCP AI Platform, I've been able to predict directly specifying in the body of the request the signature name:<\/p>\n<pre><code>response = service.projects().predict(\n        name=name,\n        body={&quot;instances&quot;: instances,\n        &quot;signature_name&quot;: &quot;name_of_signature&quot;},\n    ).execute()\n<\/code><\/pre>\n<p>@EDIT\nI've discovered that with the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.endpoints\/rawPredict\" rel=\"nofollow noreferrer\">rawPredict method<\/a> from gcloud it works.<\/p>\n<p>Here is an example:<\/p>\n<pre><code>!gcloud ai endpoints raw-predict {endpoint} --region=us-central1 \\\n--request='{&quot;signature_name&quot;:&quot;name_of_the_signature&quot;, \\\n&quot;instances&quot;: [{&quot;instance_0&quot;: [&quot;value_0&quot;], &quot;instance_1&quot;: [&quot;value_1&quot;]}]}'\n<\/code><\/pre>\n<p>Unfortunately, looking at <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/google\/cloud\/aiplatform\/models.py\" rel=\"nofollow noreferrer\">google api models code<\/a> it only have the predict method, not the raw_predict. So I don't know if it's available through python sdk right now.<\/p>",
        "Challenge_closed_time":1636439088550,
        "Challenge_comment_count":4,
        "Challenge_created_time":1636138079960,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1636203038128,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69857932",
        "Challenge_link_count":4,
        "Challenge_participation_count":5,
        "Challenge_readability":17.4,
        "Challenge_reading_time":30.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":83.6134972223,
        "Challenge_title":"Specify signature name on Vertex AI Predict",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":508.0,
        "Challenge_word_count":192,
        "Platform":"Stack Overflow",
        "Poster_created_time":1606605180560,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brazil",
        "Poster_reputation_count":98.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>Vertex AI is a newer platform with limitations that will be improved over time. \u201csignature_name\u201d can be added to HTTP JSON Payload in <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.endpoints\/rawPredict\" rel=\"nofollow noreferrer\">RawPredictRequest<\/a> or from <a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/ai\/endpoints\/raw-predict\" rel=\"nofollow noreferrer\">gcloud<\/a> as you have done but right now this is not available in regular predict requests.<\/p>\n<p><strong>Using HTTP JSON payload :<\/strong><\/p>\n<p>Example:<\/p>\n<p>input.json :<\/p>\n<pre><code>{\n   &quot;instances&quot;: [\n     [&quot;male&quot;, 29.8811345124283, 26.0, 1, &quot;S&quot;, &quot;New York, NY&quot;, 0, 0],\n     [&quot;female&quot;, 48.0, 39.6, 1, &quot;C&quot;, &quot;London \/ Paris&quot;, 0, 1]],\n \n     &quot;signature_name&quot;: &lt;string&gt;\n}\n\n<\/code><\/pre>\n<pre><code>curl \\\n-X POST \\\n-H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\\n-H &quot;Content-Type: application\/json&quot; \\\nhttps:\/\/us-central1-aiplatform.googleapis.com\/v1\/projects\/${PROJECT_ID}\/locations\/us-central1\/endpoints\/${ENDPOINT_ID}:rawPredict \\\n-d &quot;@input.json&quot;\n\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1636453108543,
        "Solution_link_count":3.0,
        "Solution_readability":16.9,
        "Solution_reading_time":15.97,
        "Solution_score_count":3.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":96.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1477057589223,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Columbus, OH, United States",
        "Answerer_reputation_count":547.0,
        "Answerer_view_count":45.0,
        "Challenge_adjusted_solved_time":0.0,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We are currently working on making an Azure MachineLearning Studio experiment operational.<\/p>\n\n<p>Our most recent iteration has a webjob that accepts a queue message, gets some data to train the model, and consumes the ML Experiment webservice to put a trained model in a blob location.<\/p>\n\n<p>A second webjob accepts a queue message, pulls the data to be used in the predictive experiment, gets the location path of the trained .ilearner model, and then consumes THAT ML Experiment webservice.<\/p>\n\n<p>The data used to make the predictions is passed in as an input parameter, and the storage account name, key, and .ilearner path are all passed in as global parameters--Dictionary objects defined according to what the data scientist provided.<\/p>\n\n<p>Everything <em>appears<\/em> to work correctly--except in some cases, the predictive experiment fails, and the error message makes it clear the wrong .ilearner file is being used.<\/p>\n\n<p>When a non-existent blob path is passed to the experiment webservice, the error message reflects there is no such blob, so it's clear the webservice is at least validating the .ilearner's existence. <\/p>\n\n<p>The data scientist can run it locally, but has to change the name of the .ilearner file when he exports it locally through PowerShell. Ensuring each trained model has a unique file name did not resolve this issue.<\/p>\n\n<p>All files, when I view them in the Azure Storage Explorer, appear to be getting updated as expected based on last-modified dates. It's almost like there's a cached version of the .ilearner somewhere that isn't being overridden properly.<\/p>",
        "Challenge_closed_time":1535139575903,
        "Challenge_comment_count":0,
        "Challenge_created_time":1535139575903,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52010761",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":20.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":0.0,
        "Challenge_title":"Azure MachineLearning WebService Not Using Passed .ilearner Model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":65.0,
        "Challenge_word_count":260,
        "Platform":"Stack Overflow",
        "Poster_created_time":1477057589223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Columbus, OH, United States",
        "Poster_reputation_count":547.0,
        "Poster_view_count":45.0,
        "Solution_body":"<p>After ruling out all possibility of passing in the wrong file, our data scientist took a closer look at the experiment itself. He discovered that it was defaulting to one hardcoded .ilearner path he had been using in development.<\/p>\n\n<p>At one point in time, he had created webservice parameters to override this value (hence why I had them defined in my webservice call), but they had been removed during one of the redesigns of the experiment with anyone noticing, because the webservice will apparently accept superfluous arguments.<\/p>\n\n<p><strong>The webservice was accepting my global parameters<\/strong>, and apparently even validating them. But since they weren't wired to anything inside <strong>the experiment the passed .ilearner file info was never applied to anything<\/strong>--the hardcoded .ilearner was being applied no matter what.<\/p>\n\n<p>We were all very surprised there was no exception thrown about passing in parameters to the webservice that weren't actually defined. Had <em>that<\/em> happened, we would have gotten to the bottom of it much more quickly.<\/p>\n\n<p>tl\/dr: The experiment wasn't properly configured to accept an .ilearner file path (or Account Name, or Account Key) as a parameter, and the webservice was happily accepting and ignoring the parameter arguments without raising any alarm since it had the hardcoded value to run with.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.2,
        "Solution_reading_time":17.11,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":208.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":52.1266738889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there any restriction on registering an ML pickle model into Azure Machine Learning Service in terms of the size of the pickle file?  <\/p>\n<p>Does it cause latency in realtime data processing and getting the prediction results from the pickle file if we have a  model that let's say it 5MB and the other one is 500MB (The bigger file has better performance in terms of accuracy)?  <br \/>\nThanks,  <\/p>\n<p>John<\/p>",
        "Challenge_closed_time":1599800075416,
        "Challenge_comment_count":2,
        "Challenge_created_time":1599612419390,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/89630\/ml-pickle-file-size-azure-machine-learning-service",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.8,
        "Challenge_reading_time":5.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":52.1266738889,
        "Challenge_title":"ML Pickle file size Azure Machine Learning Service",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":79,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=ccc37df1-9b57-4e84-920a-f92d8316aa7c\">@JJA  <\/a> Thanks, For ACI we recommend not using a model over 1GB in size.    <br \/>\nFor AKS you are limited by the memory resources that you request for your service, minus about 500mb for the running python process in the pod.    <\/p>\n<p>There will be no difference in prediction speed once the model is successfully deployed.    <br \/>\nRegistering will take longer as we have to upload the model, and deploying will take longer as the service must download the model.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.2,
        "Solution_reading_time":6.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":83.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1579718832727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":149.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":128.2891566667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have developed keras text classification model. I have preprocessed data(tokenization). I have logged trained model successfully(mlflow.keras.log_model). I have served model using mlflow serve. Now while doing prediction on text data I need to do preprocessing using same tokenizer object used for training.\nHow to preprocess test data and get predictions from served model.<\/p>",
        "Challenge_closed_time":1584552358667,
        "Challenge_comment_count":0,
        "Challenge_created_time":1584090517703,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60667610",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.1,
        "Challenge_reading_time":5.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":128.2891566667,
        "Challenge_title":"How to deploy mlflow model with data preprocessing(text data)",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1996.0,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1498470936987,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>You can log a custom python model: \n<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#custom-python-models\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/models.html#custom-python-models<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":29.0,
        "Solution_reading_time":3.04,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":35.6138888889,
        "Challenge_answer_count":5,
        "Challenge_body":"This is the error I get  \"plot_model() got an unexpected keyword argument 'system'\"\r\n\r\n",
        "Challenge_closed_time":1650470329000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650342119000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2439",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":6.8,
        "Challenge_reading_time":1.48,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":35.6138888889,
        "Challenge_title":"plots are not saving through MLFLOW",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":18,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@akashg116414 We can't help you with this. Can you please describe a little bit more, show the code, show the complete error, etc. this error happens when i install pycaret[full]==2.3.9 and pycaret-ts-alpha both and tried basic classification example\r\nexample:\r\n\r\n```python\r\nfrom pycaret.datasets import get_data\r\ndata = get_data('juice')\r\nfrom pycaret.classification import *\r\nclf1 = setup(data, target = 'Purchase', session_id=123, log_experiment=True, experiment_name='juice1',log_plots=True)\r\n``` > this error happens when i install pycaret[full]==2.3.9 and pycaret-ts-alpha both and tried basic classification example\r\n\r\nAre you installing both of them in the same environment? That should not be done as there may be conflicts at this time (until we officially release the 3.0.0 pycaret version which will have time-series integrated with the main pycaret package).\r\n\r\nTry installing one of them in a fresh (clean) environment and see if it works. @akashg116414 By the way, I am not able to recreate the issue with the information (code) you have provided. It works fine for me (see attached notebook below).\r\n\r\nhttps:\/\/gist.github.com\/ngupta23\/f7f33a5361928cac2f36f855f7398c88\r\n\r\nPlease provide a completely reproducible example that shows the steps to get to the error you are getting. Since we are unable to recreate this and information seems to be missing, we will close this for now. Feel free to create a new issue. We add a new Bug template that will guide you through the process of submitting a reproducible example. \r\n\r\nThanks!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.6,
        "Solution_reading_time":19.02,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":219.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":571.4619775,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm running the following code to create an endpoint with a preexisting model:<\/p>\n<pre><code>from sagemaker.tensorflow import serving\nsagemaker_session = sagemaker.Session()\nclf_sm_model = serving.Model(model_data='s3:\/\/mybucket\/mytrainedmodel\/model.tar.gz',\n                                     entry_point=&quot;inference.py&quot;,\n                                     source_dir=&quot;inf_source_dir&quot;,\n                                     role=get_execution_role(),\n                                     framework_version='1.14',\n                                     sagemaker_session=sagemaker_session)\n<\/code><\/pre>\n<p>However this create a copy of the model into the default sagemaker bucket. How can I pass a custom path? I've tried model_dir, and output_path but neither are accepted as parameters<\/p>",
        "Challenge_closed_time":1645727041932,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643669778813,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70933814",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.1,
        "Challenge_reading_time":9.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":571.4619775,
        "Challenge_title":"SageMaker custom model output path for tensorflow when creating from s3 artifacts",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":216.0,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Poster_created_time":1421343783700,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1387.0,
        "Poster_view_count":153.0,
        "Solution_body":"<p>The SageMaker Python SDK repackages your model to include your <code>entry_point<\/code> and <code>source_dir<\/code> files and uploads this &quot;new&quot; tar ball to the SageMaker default bucket.<\/p>\n<p>You can change this behavior by setting the <code>default_bucket<\/code> in your <code>sagemaker_session<\/code> as follows:<\/p>\n<pre><code>sagemaker_session = sagemaker.Session(default_bucket=&quot;&lt;mybucket&gt;&quot;)\n\nclf_sm_model = serving.Model(model_data='s3:\/\/mybucket\/mytrainedmodel\/model.tar.gz',         \n                    .\n                    .\n                    sagemaker_session=sagemaker_session)\n                    .\n                    )\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":20.9,
        "Solution_reading_time":7.65,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":45.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.6372286111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've followed the documentation pretty well as outlined <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-custom-docker-image\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p>I've setup my azure machine learning environment the following way:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace\n\n# Connect to the workspace\nws = Workspace.from_config()\n\nfrom azureml.core import Environment\nfrom azureml.core import ContainerRegistry\n\nmyenv = Environment(name = &quot;myenv&quot;)\n\nmyenv.inferencing_stack_version = &quot;latest&quot;  # This will install the inference specific apt packages.\n\n# Docker\nmyenv.docker.enabled = True\nmyenv.docker.base_image_registry.address = &quot;myazureregistry.azurecr.io&quot;\nmyenv.docker.base_image_registry.username = &quot;myusername&quot;\nmyenv.docker.base_image_registry.password = &quot;mypassword&quot;\nmyenv.docker.base_image = &quot;4fb3...&quot; \nmyenv.docker.arguments = None\n\n# Environment variable (I need python to look at folders \nmyenv.environment_variables = {&quot;PYTHONPATH&quot;:&quot;\/root&quot;}\n\n# python\nmyenv.python.user_managed_dependencies = True\nmyenv.python.interpreter_path = &quot;\/opt\/miniconda\/envs\/myenv\/bin\/python&quot; \n\nfrom azureml.core.conda_dependencies import CondaDependencies\nconda_dep = CondaDependencies()\nconda_dep.add_pip_package(&quot;azureml-defaults&quot;)\nmyenv.python.conda_dependencies=conda_dep\n\nmyenv.register(workspace=ws) # works!\n<\/code><\/pre>\n<p>I have a score.py file configured for inference (not relevant to the problem I'm having)...<\/p>\n<p>I then setup inference configuration<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.model import InferenceConfig\ninference_config = InferenceConfig(entry_script=&quot;score.py&quot;, environment=myenv)\n<\/code><\/pre>\n<p>I setup my compute cluster:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.compute import ComputeTarget, AksCompute\nfrom azureml.exceptions import ComputeTargetException\n\n# Choose a name for your cluster\naks_name = &quot;theclustername&quot; \n\n# Check to see if the cluster already exists\ntry:\n    aks_target = ComputeTarget(workspace=ws, name=aks_name)\n    print('Found existing compute target')\nexcept ComputeTargetException:\n    print('Creating a new compute target...')\n    prov_config = AksCompute.provisioning_configuration(vm_size=&quot;Standard_NC6_Promo&quot;)\n\n    aks_target = ComputeTarget.create(workspace=ws, name=aks_name, provisioning_configuration=prov_config)\n\n    aks_target.wait_for_completion(show_output=True)\n\nfrom azureml.core.webservice import AksWebservice\n\n# Example\ngpu_aks_config = AksWebservice.deploy_configuration(autoscale_enabled=False,\n                                                    num_replicas=3,\n                                                    cpu_cores=4,\n                                                    memory_gb=10)\n<\/code><\/pre>\n<p>Everything succeeds; then I try and deploy the model for inference:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.model import Model\n\nmodel = Model(ws, name=&quot;thenameofmymodel&quot;)\n\n# Name of the web service that is deployed\naks_service_name = 'tryingtodeply'\n\n# Deploy the model\naks_service = Model.deploy(ws,\n                           aks_service_name,\n                           models=[model],\n                           inference_config=inference_config,\n                           deployment_config=gpu_aks_config,\n                           deployment_target=aks_target,\n                           overwrite=True)\n\naks_service.wait_for_deployment(show_output=True)\nprint(aks_service.state)\n<\/code><\/pre>\n<p>And it fails saying that it can't find the environment. More specifically, my environment version is <strong>version 11<\/strong>, but it keeps trying to find an environment with a version number that is 1 higher (i.e., <strong>version 12<\/strong>) than the current environment:<\/p>\n<pre><code>FailedERROR - Service deployment polling reached non-successful terminal state, current service state: Failed\nOperation ID: 0f03a025-3407-4dc1-9922-a53cc27267d4\nMore information can be found here: \nError:\n{\n  &quot;code&quot;: &quot;BadRequest&quot;,\n  &quot;statusCode&quot;: 400,\n  &quot;message&quot;: &quot;The request is invalid&quot;,\n  &quot;details&quot;: [\n    {\n      &quot;code&quot;: &quot;EnvironmentDetailsFetchFailedUserError&quot;,\n      &quot;message&quot;: &quot;Failed to fetch details for Environment with Name: myenv Version: 12.&quot;\n    }\n  ]\n}\n\n<\/code><\/pre>\n<p>I have tried to manually edit the environment JSON to match the version that azureml is trying to fetch, but nothing works. Can anyone see anything wrong with this code?<\/p>\n<h1>Update<\/h1>\n<p>Changing the name of the environment (e.g., <code>my_inference_env<\/code>) and passing it to <code>InferenceConfig<\/code> seems to be on the right track. However, the error now changes to the following<\/p>\n<pre><code>Running..........\nFailed\nERROR - Service deployment polling reached non-successful terminal state, current service state: Failed\nOperation ID: f0dfc13b-6fb6-494b-91a7-de42b9384692\nMore information can be found here: https:\/\/some_long_http_address_that_leads_to_nothing\nError:\n{\n  &quot;code&quot;: &quot;DeploymentFailed&quot;,\n  &quot;statusCode&quot;: 404,\n  &quot;message&quot;: &quot;Deployment not found&quot;\n}\n<\/code><\/pre>\n<h1>Solution<\/h1>\n<p>The answer from Anders below is <strong>indeed correct<\/strong> regarding the use of azure ML environments. However, the last error I was getting was because I was setting the <em>container image<\/em> using the digest value (a sha) and NOT the image name and tag (e.g., <code>imagename:tag<\/code>). Note the line of code in the first block:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>myenv.docker.base_image = &quot;4fb3...&quot; \n<\/code><\/pre>\n<p>I reference the digest value, but it should be changed to<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>myenv.docker.base_image = &quot;imagename:tag&quot;\n<\/code><\/pre>\n<p>Once I made that change, the deployment succeeded! :)<\/p>",
        "Challenge_closed_time":1597702121696,
        "Challenge_comment_count":5,
        "Challenge_created_time":1597699827673,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1599771558392,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63458904",
        "Challenge_link_count":2,
        "Challenge_participation_count":6,
        "Challenge_readability":17.3,
        "Challenge_reading_time":77.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":44,
        "Challenge_solved_time":0.6372286111,
        "Challenge_title":"Azure-ML Deployment does NOT see AzureML Environment (wrong version number)",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1768.0,
        "Challenge_word_count":509,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432406490590,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Milwaukee, WI",
        "Poster_reputation_count":381.0,
        "Poster_view_count":62.0,
        "Solution_body":"<p>One concept that took me a while to get was the bifurcation of registering and using an Azure ML <code>Environment<\/code>. If you have already registered your env, <code>myenv<\/code>, and none of the details of the your environment have changed, there is no need re-register it with <code>myenv.register()<\/code>. You can simply get the already register env using <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.environment.environment?view=azure-ml-py#get-workspace--name--version-none-\" rel=\"nofollow noreferrer\"><code>Environment.get()<\/code><\/a> like so:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>myenv = Environment.get(ws, name='myenv', version=11)\n<\/code><\/pre>\n<p>My recommendation would be to name your environment something new: like <code>&quot;model_scoring_env&quot;<\/code>. Register it once, then pass it to the <code>InferenceConfig<\/code>.<\/p>",
        "Solution_comment_count":9.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.2,
        "Solution_reading_time":11.94,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1593662684510,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":41.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":119.8228480555,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am fairly new to TensorFlow (and SageMaker) and am stuck in the process of deploying a SageMaker endpoint. I have just recently succeeded in creating a Saved Model type model, which is currently being used to service a sample endpoint (the model was created externally). However, when I checked the image I am using for the endpoint, it says '...\/tensorflow-inference', which is not the direction I want to go in because I want to use a SageMaker TensorFlow serving container (I followed tutorials from the official TensorFlow serving GitHub repo-using sample models, and they are deployed correcting using the TensorFlow serving framework).<\/p>\n<p>Am I encountering this issue because my Saved Model does not have the correct 'serving' tag? I have not checked my tag sets yet but wanted to know if this would be the core reason to the problem. Also, most importantly, <strong>what are the differences between the two container types<\/strong>-I think having a better understanding of these two concepts would show me why I am unable to produce the correct image.<\/p>\n<hr \/>\n<p>This is how I deployed the sample endpoint:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model = Model(model_data =...)\n\npredictor = model.deploy(initial_instance_count=...)\n<\/code><\/pre>\n<p>When I run the code, I get a model, an endpoint configuration, and an endpoint. I got the container type by clicking on model details within the AWS SageMaker console.<\/p>",
        "Challenge_closed_time":1595219623720,
        "Challenge_comment_count":3,
        "Challenge_created_time":1594709403670,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1594788261467,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62889537",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":11.9,
        "Challenge_reading_time":19.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":141.7277916667,
        "Challenge_title":"TensorFlow Serving vs. TensorFlow Inference (container type for SageMaker model)",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1223.0,
        "Challenge_word_count":229,
        "Platform":"Stack Overflow",
        "Poster_created_time":1593662684510,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>There are different versions for the framework containers. Since the framework version I'm using is 1.15, the image I got had to be in a tensorflow-inference container. If I used versions &lt;= 1.13, then I would get sagemaker-tensorflow-serving images. The two aren't the same, but there's no 'correct' container type.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":4.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":50.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1491898605956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sydney NSW, Australia",
        "Answerer_reputation_count":161.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":12.7567577778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following this example from aws <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-multi-model-endpoint-tensorflow-computer-vision\/blob\/main\/multi-model-endpoint-tensorflow-cv.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/sagemaker-multi-model-endpoint-tensorflow-computer-vision\/blob\/main\/multi-model-endpoint-tensorflow-cv.ipynb<\/a>\nto apply same workflow with two pre trained models (trained outside of sagemaker).<\/p>\n<p>But when I do the following, logs say that models can't be found:<\/p>\n<pre><code>import boto3\nimport datetime\nfrom datetime import datetime\nimport time\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.tensorflow.serving import TensorFlowModel\nfrom sagemaker.multidatamodel import MultiDataModel\n\nmodel_data_prefix = f's3:\/\/{BUCKET}\/{PREFIX}\/mme\/'\noutput = f's3:\/\/{BUCKET}\/{PREFIX}\/mme\/test.tar.gz'\n\nmodele = TensorFlowModel(model_data=output, \n                          role=role, \n                          image_uri=IMAGE_URI)\n\nmme = MultiDataModel(name=f'mme-tensorflow-{current_time}',\n                     model_data_prefix=model_data_prefix,\n                     model=modele,\n                     sagemaker_session=sagemaker_session)\n\npredictor = mme.deploy(initial_instance_count=1,\n                       instance_type='ml.m5.2xlarge',\n                       endpoint_name=f'mme-tensorflow-{current_time}')\n<\/code><\/pre>\n<p>When I give an image as input to predict, I have this message:<\/p>\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message &quot;&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;Internal Server Error&lt;\/title&gt;\n  &lt;\/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;&lt;p&gt;Internal Server Error&lt;\/p&gt;&lt;\/h1&gt;\n    \n  &lt;\/body&gt;\n&lt;\/html&gt;\n&quot;.\n<\/code><\/pre>\n<p>Logs give:<\/p>\n<pre><code>Could not find base path \/opt\/ml\/models\/...\/model for servable ...\n<\/code><\/pre>\n<p>What did I missed ?<\/p>",
        "Challenge_closed_time":1651629562456,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651582909370,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1651583638128,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72099790",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":21.3,
        "Challenge_reading_time":25.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":12.9591905556,
        "Challenge_title":"Error when deploying pre trained Tensorflow models to one endpoint (multimodel for one endpoint) in sagemaker?",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":72.0,
        "Challenge_word_count":155,
        "Platform":"Stack Overflow",
        "Poster_created_time":1606642099552,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":371.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>In the sample notebook, the model is trained within SageMaker. So it is created with certain environment variables like the &quot;SAGEMAKER_PROGRAM&quot;(I think, need to check the documentation) with value set to entry point script.<\/p>\n<p>But while you are creating the model with models trained outside the SageMaker you need to add those environment variables.<\/p>\n<p>Without an entry point script SageMaker is not in a position to know what to do with the request.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.4,
        "Solution_reading_time":5.93,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":73.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1556553177963,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Singapore",
        "Answerer_reputation_count":347.0,
        "Answerer_view_count":93.0,
        "Challenge_adjusted_solved_time":465.8558136111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a mismatch in shapes between inputs and the model of my reinforcement learning project.<\/p>\n\n<p>I have been closely following the AWS examples, specifically the cartpole example. However I have built my own custom environment. What I am struggling to understand is how to change my environment so that it is able to work with the prebuilt Ray RLEstimator.<\/p>\n\n<p>Here is the code for the environment:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from enum import Enum\nimport math\n\nimport gym\nfrom gym import error, spaces, utils, wrappers\nfrom gym.utils import seeding\nfrom gym.envs.registration import register\nfrom gym.spaces import Discrete, Box\n\n\nimport numpy as np\n\n# from float_space import FloatSpace\n\n\ndef sigmoid_price_fun(x, maxcust, gamma):\n    return maxcust \/ (1 + math.exp(gamma * max(0, x)))\n\n\nclass Actions(Enum):\n    DECREASE_PRICE = 0\n    INCREASE_PRICE = 1\n    HOLD = 2\n\n\nPRICE_ADJUSTMENT = {\n    Actions.DECREASE_PRICE: -0.25,\n    Actions.INCREASE_PRICE: 0.25,\n    Actions.HOLD: 0\n}\n\n\nclass ArrivalSim(gym.Env):\n    \"\"\" Simple environment for price optimising RL learner. \"\"\"\n\n\n    def __init__(self, price):\n        \"\"\"\n        Parameters\n        ----------\n        price : float\n            The initial price to use.\n        \"\"\"\n        super().__init__()\n        self.price = price\n        self.revenue = 0\n        self.action_space = Discrete(3)  # [0, 1, 2]  #increase or decrease\n        self.observation_space = Box(np.array(0.0),np.array(1000))\n#         self.observation_space = FloatSpace(price)\n\n    def step(self, action):\n        \"\"\" Enacts the specified action in the environment.\n\n        Returns the new price, reward, whether we're finished and an empty dict for compatibility with Gym's\n        interface. \"\"\"\n\n        self._take_action(Actions(action))\n        next_state = self.price\n#         next_state = self.observation_space.sample()\n        reward = self._get_reward()\n        done = False\n\n        if next_state &lt; 0 or reward == 0:\n            done = True\n\n        print(next_state, reward, done, {})\n\n        return np.array(next_state), reward, done, {}\n\n    def reset(self):\n        \"\"\" Resets the environment, selecting a random initial price. Returns the price. \"\"\"\n\n#         self.observation_space.value = np.random.rand()\n#         return self.observation_space.sample()\n        self.price = np.random.rand()\n        return self.price\n\n    def _take_action(self, action):\n#         self.observation_space.value += PRICE_ADJUSTMENT[action]\n        self.price += PRICE_ADJUSTMENT[action]\n\n    def _get_reward(self,price):\n#         price = self.observation_space.value\n#         return max(np.random.poisson(sigmoid_price_fun(price, 50, 0.5)) * price, 0)\n        self.revenue = max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)\n        return max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)\n\n\n#     def render(self, mode='human'):\n#         super().render(mode)\n\ndef testEnv():\n    register(\n        id='ArrivalSim-v0',\n        entry_point='env:ArrivalSim',\n        kwargs= {'price' : 40}\n    )\n    env = gym.make('ArrivalSim-v0')\n\n    env.reset()\n    for _ in range(20):\n        test = env.action_space.sample()\n        print(test)\n        print(env.observation_space)\n        env.step(test)  # take a random action\n    env.close()\n\n\n\nif __name__ =='__main__':\n\n    testEnv()\n\n<\/code><\/pre>\n\n<p>Here is the training script<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import json\nimport os\n\nimport gym\nimport ray\nfrom ray.tune import run_experiments\nfrom ray.tune.registry import register_env\nfrom gym.envs.registration import register\n\nfrom sagemaker_rl.ray_launcher import SageMakerRayLauncher\n\n\ndef create_environment(env_config):\n    import gym\n#     from gym.spaces import Space\n    from gym.envs.registration import register\n\n    # This import must happen inside the method so that worker processes import this code\n    register(\n        id='ArrivalSim-v0',\n        entry_point='env:ArrivalSim',\n        kwargs= {'price' : 40}\n    )\n    return gym.make('ArrivalSim-v0')\n\n\n\nclass MyLauncher(SageMakerRayLauncher):\n\n    def register_env_creator(self):\n        register_env(\"ArrivalSim-v0\", create_environment)\n\n    def get_experiment_config(self):\n        return {\n          \"training\": {\n            \"env\": \"ArrivalSim-v0\",\n            \"run\": \"PPO\",\n            \"stop\": {\n              \"episode_reward_mean\": 5000,\n            },\n            \"config\": {\n              \"gamma\": 0.995,\n              \"kl_coeff\": 1.0,\n              \"num_sgd_iter\": 10,\n              \"lr\": 0.0001,\n              \"sgd_minibatch_size\": 32768,\n              \"train_batch_size\": 320000,\n              \"monitor\": False,  # Record videos.\n              \"model\": {\n                \"free_log_std\": False\n              },\n              \"use_gae\": False,\n              \"num_workers\": (self.num_cpus-1),\n              \"num_gpus\": self.num_gpus,\n              \"batch_mode\": \"complete_episodes\"\n\n            }\n          }\n        }\n\nif __name__ == \"__main__\":\n    MyLauncher().train_main()\n<\/code><\/pre>\n\n<p>Here is the code I run in Jupyter:<\/p>\n\n<pre><code>metric_definitions = RLEstimator.default_metric_definitions(RLToolkit.RAY)\nenvironment = env = {\n    'SAGEMAKER_REQUIREMENTS': 'requirements.txt', # path relative to `source_dir` below.\n}\n\nestimator = RLEstimator(entry_point=\"train.py\",\n                        source_dir='.',\n                        toolkit=RLToolkit.RAY,\n                        toolkit_version='0.6.5',\n                        framework=RLFramework.TENSORFLOW,\n                        dependencies=[\"sagemaker_rl\"],\n#                         image_name='price-response-ray-cpu',\n                        role=role,\n#                         train_instance_type=\"ml.c5.2xlarge\",\n                        train_instance_type='local',\n                        train_instance_count=1,\n#                         output_path=s3_output_path,\n#                         base_job_name=job_name_prefix,\n                        metric_definitions=metric_definitions\n#                         hyperparameters={\n                          # Attention scientists!  You can override any Ray algorithm parameter here:\n                          #\"rl.training.config.horizon\": 5000,\n                          #\"rl.training.config.num_sgd_iter\": 10,\n                        #}\n                    )\n\nestimator.fit(wait=True)\njob_name = estimator.latest_training_job.job_name\nprint(\"Training job: %s\" % job_name)\n<\/code><\/pre>\n\n<p>The error message I have been receiving has been the following:<\/p>\n\n<pre><code>algo-1-dxwxx_1  | == Status ==\nalgo-1-dxwxx_1  | Using FIFO scheduling algorithm.\nalgo-1-dxwxx_1  | Resources requested: 0\/3 CPUs, 0\/0 GPUs\nalgo-1-dxwxx_1  | Memory usage on this node: 1.1\/4.1 GB\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | == Status ==\nalgo-1-dxwxx_1  | Using FIFO scheduling algorithm.\nalgo-1-dxwxx_1  | Resources requested: 2\/3 CPUs, 0\/0 GPUs\nalgo-1-dxwxx_1  | Memory usage on this node: 1.4\/4.1 GB\nalgo-1-dxwxx_1  | Result logdir: \/opt\/ml\/output\/intermediate\/training\nalgo-1-dxwxx_1  | Number of trials: 1 ({'RUNNING': 1})\nalgo-1-dxwxx_1  | RUNNING trials:\nalgo-1-dxwxx_1  |  - PPO_ArrivalSim-v0_0:   RUNNING\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | (pid=72) 2019-08-30 09:35:13,030  WARNING ppo.py:172 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\nalgo-1-dxwxx_1  | 2019-08-30 09:35:13,063   ERROR trial_runner.py:460 -- Error processing event.\nalgo-1-dxwxx_1  | Traceback (most recent call last):\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/tune\/trial_runner.py\", line 409, in _process_trial\nalgo-1-dxwxx_1  |     result = self.trial_executor.fetch_result(trial)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/tune\/ray_trial_executor.py\", line 314, in fetch_result\nalgo-1-dxwxx_1  |     result = ray.get(trial_future[0])\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/worker.py\", line 2316, in get\nalgo-1-dxwxx_1  |     raise value\nalgo-1-dxwxx_1  | ray.exceptions.RayTaskError: ray_worker (pid=72, host=b9b15d495b68)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/model.py\", line 83, in __init__\nalgo-1-dxwxx_1  |     restored, num_outputs, options)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/model.py\", line 135, in _build_layers_v2\nalgo-1-dxwxx_1  |     raise NotImplementedError\nalgo-1-dxwxx_1  | NotImplementedError\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | During handling of the above exception, another exception occurred:\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | ray_worker (pid=72, host=b9b15d495b68)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 276, in __init__\nalgo-1-dxwxx_1  |     Trainable.__init__(self, config, logger_creator)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/tune\/trainable.py\", line 88, in __init__\nalgo-1-dxwxx_1  |     self._setup(copy.deepcopy(self.config))\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 373, in _setup\nalgo-1-dxwxx_1  |     self._init()\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/ppo\/ppo.py\", line 77, in _init\nalgo-1-dxwxx_1  |     self.env_creator, self._policy_graph)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 506, in make_local_evaluator\nalgo-1-dxwxx_1  |     extra_config or {}))\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 714, in _make_evaluator\nalgo-1-dxwxx_1  |     async_remote_worker_envs=config[\"async_remote_worker_envs\"])\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/evaluation\/policy_evaluator.py\", line 288, in __init__\nalgo-1-dxwxx_1  |     self._build_policy_map(policy_dict, policy_config)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/evaluation\/policy_evaluator.py\", line 661, in _build_policy_map\nalgo-1-dxwxx_1  |     policy_map[name] = cls(obs_space, act_space, merged_conf)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/ppo\/ppo_policy_graph.py\", line 176, in __init__\nalgo-1-dxwxx_1  |     seq_lens=existing_seq_lens)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/catalog.py\", line 215, in get_model\nalgo-1-dxwxx_1  |     seq_lens)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/catalog.py\", line 255, in _get_model\nalgo-1-dxwxx_1  |     num_outputs, options)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/model.py\", line 86, in __init__\nalgo-1-dxwxx_1  |     input_dict[\"obs\"], num_outputs, options)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/fcnet.py\", line 37, in _build_layers\nalgo-1-dxwxx_1  |     scope=label)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/contrib\/framework\/python\/ops\/arg_scope.py\", line 182, in func_with_args\nalgo-1-dxwxx_1  |     return func(*args, **current_args)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/contrib\/layers\/python\/layers\/layers.py\", line 1854, in fully_connected\nalgo-1-dxwxx_1  |     outputs = layer.apply(inputs)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/base_layer.py\", line 817, in apply\nalgo-1-dxwxx_1  |     return self.__call__(inputs, *args, **kwargs)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/layers\/base.py\", line 374, in __call__\nalgo-1-dxwxx_1  |     outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/base_layer.py\", line 730, in __call__\nalgo-1-dxwxx_1  |     self._assert_input_compatibility(inputs)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/base_layer.py\", line 1493, in _assert_input_compatibility\nalgo-1-dxwxx_1  |     str(x.shape.as_list()))\nalgo-1-dxwxx_1  | ValueError: Input 0 of layer default\/fc1 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [None]\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | 2019-08-30 09:35:13,064   INFO ray_trial_executor.py:178 -- Destroying actor for trial PPO_ArrivalSim-v0_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\nalgo-1-dxwxx_1  | 2019-08-30 09:35:13,076   INFO trial_runner.py:497 -- Attempting to recover trial state from last checkpoint.\nalgo-1-dxwxx_1  | (pid=72) 2019-08-30 09:35:13,041  INFO policy_evaluator.py:278 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n<\/code><\/pre>\n\n<p>I am not sure how to change the input the environment gives to the model or the models setup itself. It seems the documentations are quite obscure. I have a hunch that problem lies with the observation and action spaces<\/p>\n\n<p>Here is the reference to the original aws project example:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/reinforcement_learning\/rl_roboschool_ray\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/reinforcement_learning\/rl_roboschool_ray<\/a><\/p>",
        "Challenge_closed_time":1567460466780,
        "Challenge_comment_count":0,
        "Challenge_created_time":1567158570927,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1567160893503,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57724414",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":17.2,
        "Challenge_reading_time":159.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":119,
        "Challenge_solved_time":83.8599591667,
        "Challenge_title":"How to make the inputs and model have the same shape (RLlib Ray Sagemaker reinforcement learning)",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1492.0,
        "Challenge_word_count":999,
        "Platform":"Stack Overflow",
        "Poster_created_time":1565097950652,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p><strong>Possible reason:<\/strong><\/p>\n\n<p>The error message:<\/p>\n\n<p><code>ValueError: Input 0 of layer default\/fc1 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [None]<\/code><\/p>\n\n<p>Your original environment obs space is <code>self.observation_space = Box(np.array(0.0),np.array(1000))<\/code>.<\/p>\n\n<p>Displaying the shape of your environment obs space gives:<\/p>\n\n<p><code>print(Box(np.array(0.0), np.array(1000), dtype=np.float32).shape)<\/code> = <code>()<\/code><\/p>\n\n<p>This could be indicated by <code>Full shape received: [None]<\/code> in the error message.<\/p>\n\n<p>If you pass the shape <code>(1,1)<\/code> into <code>np.zeros<\/code>, you get the expected  <code>min_ndim=2<\/code>:<\/p>\n\n<p><code>x = np.zeros((1, 1))\nprint(x)\n[[0.]]\nprint(x.ndim)\n2<\/code><\/p>\n\n<p><strong>Suggested solution:<\/strong><\/p>\n\n<p>I assume that you want your environment obs space to range from 0.0 to 1000.0 as indicated by the <code>self.price = np.random.rand()<\/code> in your <code>reset<\/code> function.<\/p>\n\n<p>Try using the following for your environment obs space:<\/p>\n\n<p><code>self.observation_space = Box(0.0, 1000.0, shape=(1,1), dtype=np.float32)<\/code><\/p>\n\n<p>I hope that by setting the <code>Box<\/code> with an explicit <code>shape<\/code> helps.<\/p>\n\n<p><strike>\n<strong>EDIT (20190903):<\/strong><\/p>\n\n<p>I have modified your training script. This modification includes new imports, custom model class, model registration &amp; addition of registered custom model to config. For readability, only sections added are shown below. The entire modified training script is available in this <a href=\"https:\/\/gist.github.com\/ChuaCheowHuan\/ddf70654bd928d70e5415c947d4d43f3\" rel=\"nofollow noreferrer\">gist<\/a>. Please run with the proposed obs space as describe above.<\/p>\n\n<p>New additional imports:<\/p>\n\n<pre><code># new imports\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom ray.rllib.models import ModelCatalog\nfrom ray.rllib.models.tf.tf_modelv2 import TFModelV2\nfrom ray.rllib.models.tf.fcnet_v2 import FullyConnectedNetwork\n\nfrom ray.rllib.utils import try_import_tf\nfrom ray.tune import grid_search\n\ntf = try_import_tf()\n# end new imports\n<\/code><\/pre>\n\n<p>Custom model class:<\/p>\n\n<pre><code># Custom model class (fcnet)\nclass CustomModel(TFModelV2):\n    \"\"\"Example of a custom model that just delegates to a fc-net.\"\"\"\n\n    def __init__(self, obs_space, action_space, num_outputs, model_config,\n                 name):\n        super(CustomModel, self).__init__(obs_space, action_space, num_outputs,\n                                          model_config, name)\n        self.model = FullyConnectedNetwork(obs_space, action_space,\n                                           num_outputs, model_config, name)\n        self.register_variables(self.model.variables())\n\n    def forward(self, input_dict, state, seq_lens):\n        return self.model.forward(input_dict, state, seq_lens)\n\n    def value_function(self):\n        return self.model.value_function()\n<\/code><\/pre>\n\n<p>Registered &amp; add custom model:<\/p>\n\n<pre><code>    def get_experiment_config(self):\n\n\n        # Register custom model\n        ModelCatalog.register_custom_model(\"my_model\", CustomModel)\n\n\n        return {\n          \"training\": {\n            \"env\": \"ArrivalSim-v0\",\n            \"run\": \"PPO\",\n            \"stop\": {\n              \"episode_reward_mean\": 5000,\n            },\n            \"config\": {\n\n\n              \"model\": {\"custom_model\": \"my_model\"}, # Add registered custom model\n\n\n              \"gamma\": 0.995,\n              \"kl_coeff\": 1.0,\n              \"num_sgd_iter\": 10,\n              \"lr\": 0.0001,\n              \"sgd_minibatch_size\": 32768,\n              \"train_batch_size\": 320000,\n              \"monitor\": False,  # Record videos.\n              \"model\": {\n                \"free_log_std\": False\n              },\n              \"use_gae\": False,\n              \"num_workers\": (self.num_cpus-1),\n              \"num_gpus\": self.num_gpus,\n              \"batch_mode\": \"complete_episodes\"\n            }\n          }\n        }\n<\/code><\/pre>\n\n<p><\/strike><\/p>\n\n<p><strong>EDIT 2 (20190910):<\/strong><\/p>\n\n<p>To show that it works, truncated output from Sagemaker (Jupyter notebook instance):<\/p>\n\n<pre><code>.\n.\n.\nalgo-1-y2ayw_1  | price b = 0.439261780930142\nalgo-1-y2ayw_1  | price a = 0.439261780930142\nalgo-1-y2ayw_1  | (self.price).shape = (1,)\nalgo-1-y2ayw_1  | [0.43926178] 10.103020961393266 False {}\nalgo-1-y2ayw_1  | price b = 0.439261780930142\nalgo-1-y2ayw_1  | price a = 0.439261780930142\nalgo-1-y2ayw_1  | (self.price).shape = (1,)\nalgo-1-y2ayw_1  | [0.43926178] 9.663759180463124 False {}\nalgo-1-y2ayw_1  | price b = 0.439261780930142\nalgo-1-y2ayw_1  | price a = 0.189261780930142\nalgo-1-y2ayw_1  | (self.price).shape = (1,)\nalgo-1-y2ayw_1  | [0.18926178] 5.67785342790426 False {}\nalgo-1-y2ayw_1  | price b = 0.189261780930142\nalgo-1-y2ayw_1  | price a = -0.06073821906985799\nalgo-1-y2ayw_1  | (self.price).shape = (1,)\nalgo-1-y2ayw_1  | [-0.06073822] 0 True {}\nalgo-1-y2ayw_1  | Result for PPO_ArrivalSim-v0_0:\nalgo-1-y2ayw_1  |   date: 2019-09-10_11-51-13\nalgo-1-y2ayw_1  |   done: true\nalgo-1-y2ayw_1  |   episode_len_mean: 126.72727272727273\nalgo-1-y2ayw_1  |   episode_reward_max: 15772.677709596366\nalgo-1-y2ayw_1  |   episode_reward_mean: 2964.4609668691965\nalgo-1-y2ayw_1  |   episode_reward_min: 0.0\nalgo-1-y2ayw_1  |   episodes: 5\nalgo-1-y2ayw_1  |   experiment_id: 5d3b9f2988854a0db164a2e5e9a7550f\nalgo-1-y2ayw_1  |   hostname: 2dae585dcc65\nalgo-1-y2ayw_1  |   info:\nalgo-1-y2ayw_1  |     cur_lr: 4.999999873689376e-05\nalgo-1-y2ayw_1  |     entropy: 1.0670874118804932\nalgo-1-y2ayw_1  |     grad_time_ms: 1195.066\nalgo-1-y2ayw_1  |     kl: 3.391784191131592\nalgo-1-y2ayw_1  |     load_time_ms: 44.725\nalgo-1-y2ayw_1  |     num_steps_sampled: 463\nalgo-1-y2ayw_1  |     num_steps_trained: 463\nalgo-1-y2ayw_1  |     policy_loss: -0.05383850634098053\nalgo-1-y2ayw_1  |     sample_time_ms: 621.282\nalgo-1-y2ayw_1  |     total_loss: 2194493.5\nalgo-1-y2ayw_1  |     update_time_ms: 145.352\nalgo-1-y2ayw_1  |     vf_explained_var: -5.519390106201172e-05\nalgo-1-y2ayw_1  |     vf_loss: 2194492.5\nalgo-1-y2ayw_1  |   iterations_since_restore: 2\nalgo-1-y2ayw_1  |   node_ip: 172.18.0.2\nalgo-1-y2ayw_1  |   pid: 77\nalgo-1-y2ayw_1  |   policy_reward_mean: {}\nalgo-1-y2ayw_1  |   time_since_restore: 4.55129861831665\nalgo-1-y2ayw_1  |   time_this_iter_s: 1.3484764099121094\nalgo-1-y2ayw_1  |   time_total_s: 4.55129861831665\nalgo-1-y2ayw_1  |   timestamp: 1568116273\nalgo-1-y2ayw_1  |   timesteps_since_restore: 463\nalgo-1-y2ayw_1  |   timesteps_this_iter: 234\nalgo-1-y2ayw_1  |   timesteps_total: 463\nalgo-1-y2ayw_1  |   training_iteration: 2\nalgo-1-y2ayw_1  |\nalgo-1-y2ayw_1  | A worker died or was killed while executing task 00000000781a7b5b94a203683f8f789e593abbb1.\nalgo-1-y2ayw_1  | A worker died or was killed while executing task 00000000d3507bc6b41ee1c9fc36292eeae69557.\nalgo-1-y2ayw_1  | == Status ==\nalgo-1-y2ayw_1  | Using FIFO scheduling algorithm.\nalgo-1-y2ayw_1  | Resources requested: 0\/3 CPUs, 0\/0 GPUs\nalgo-1-y2ayw_1  | Result logdir: \/opt\/ml\/output\/intermediate\/training\nalgo-1-y2ayw_1  | TERMINATED trials:\nalgo-1-y2ayw_1  |  - PPO_ArrivalSim-v0_0:   TERMINATED [pid=77], 4 s, 2 iter, 463 ts, 2.96e+03 rew\nalgo-1-y2ayw_1  |\nalgo-1-y2ayw_1  | Saved model configuration.\nalgo-1-y2ayw_1  | Saved the checkpoint file \/opt\/ml\/output\/intermediate\/training\/PPO_ArrivalSim-v0_0_2019-09-10_11-50-53vd32vlux\/checkpoint-2.extra_data as \/opt\/ml\/model\/checkpoint.extra_data\nalgo-1-y2ayw_1  | Saved the checkpoint file \/opt\/ml\/output\/intermediate\/training\/PPO_ArrivalSim-v0_0_2019-09-10_11-50-53vd32vlux\/checkpoint-2.tune_metadata as \/opt\/ml\/model\/checkpoint.tune_metadata\nalgo-1-y2ayw_1  | Created LogSyncer for \/root\/ray_results\/PPO_ArrivalSim-v0_2019-09-10_11-51-13xdn_5i34 -&gt; None\nalgo-1-y2ayw_1  | 2019-09-10 11:51:13.941718: I tensorflow\/core\/common_runtime\/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\nalgo-1-y2ayw_1  | reset -&gt; (self.price).shape =  (1,)\nalgo-1-y2ayw_1  | LocalMultiGPUOptimizer devices ['\/cpu:0']\nalgo-1-y2ayw_1  | reset -&gt; (self.price).shape =  (1,)\nalgo-1-y2ayw_1  | INFO:tensorflow:No assets to save.\nalgo-1-y2ayw_1  | No assets to save.\nalgo-1-y2ayw_1  | INFO:tensorflow:No assets to write.\nalgo-1-y2ayw_1  | No assets to write.\nalgo-1-y2ayw_1  | INFO:tensorflow:SavedModel written to: \/opt\/ml\/model\/1\/saved_model.pb\nalgo-1-y2ayw_1  | SavedModel written to: \/opt\/ml\/model\/1\/saved_model.pb\nalgo-1-y2ayw_1  | Saved TensorFlow serving model!\nalgo-1-y2ayw_1  | A worker died or was killed while executing task 00000000f352d985b807ca399460941fe2264899.\n\nalgo-1-y2ayw_1  | 2019-09-10 11:51:20,075 sagemaker-containers INFO\n\n Reporting training SUCCESS\n\ntmpwwb4b358_algo-1-y2ayw_1 exited with code 0\n\nAborting on container exit...\nFailed to delete: \/tmp\/tmpwwb4b358\/algo-1-y2ayw Please remove it manually.\n\n===== Job Complete =====\n<\/code><\/pre>\n\n<p>This time I make edits in all 3 files. Your environment, training script &amp; the Jupyter notebook but it turns out that there isn't a need to define custom models for your custom environment. However, that remains viable. And you're right, the main cause of the issue is still in the obs space.<\/p>\n\n<p>I set <code>self.price<\/code> to be a 1D numpy array to make it talk better with Ray RLlib. The creation of the custom environment in the training script was done in a simpler way as shown below. As for the notebook, I used version 0.5.3 instead of 0.6.5 for toolkit_version &amp; the training is done in local mode (in the docker container on the Sagemaker Jupyter notebook instance, still on AWS) with CPU only. However, it will also work with any ML instance (e.g ml.m4.xlarge) with GPU.<\/p>\n\n<p>The entire package along with all dependencies is in <a href=\"https:\/\/github.com\/ChuaCheowHuan\/sagemaker_Ray_RLlib_custom_env\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>The edited env:<\/p>\n\n<pre><code># new\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n# end new\n\n\nfrom enum import Enum\nimport math\n\nimport gym\nfrom gym import error, spaces, utils, wrappers\nfrom gym.utils import seeding\nfrom gym.envs.registration import register\nfrom gym.spaces import Discrete, Box\n\nimport numpy as np\n\n\ndef sigmoid_price_fun(x, maxcust, gamma):\n    return maxcust \/ (1 + math.exp(gamma * max(0, x)))\n\n\nclass Actions(Enum):\n    DECREASE_PRICE = 0\n    INCREASE_PRICE = 1\n    HOLD = 2\n\n\nPRICE_ADJUSTMENT = {\n    Actions.DECREASE_PRICE: -0.25,\n    Actions.INCREASE_PRICE: 0.25,\n    Actions.HOLD: 0\n}\n\n\nclass ArrivalSim(gym.Env):\n    \"\"\" Simple environment for price optimising RL learner. \"\"\"\n\n    def __init__(self, price):\n        \"\"\"\n        Parameters\n        ----------\n        price : float\n            The initial price to use.\n        \"\"\"\n        super().__init__()\n\n        self.price = price\n        self.revenue = 0\n        self.action_space = Discrete(3)  # [0, 1, 2]  #increase or decrease\n        # original obs space:\n        #self.observation_space = Box(0.0, 1000.0, shape=(1,1), dtype=np.float32)\n        # obs space initially suggested:\n        #self.observation_space = Box(0.0, 1000.0, shape=(1,1), dtype=np.float32)\n        # obs space suggested in this edit:\n        self.observation_space = spaces.Box(np.array([0.0]), np.array([1000.0]), dtype=np.float32)\n\n    def step(self, action):\n        \"\"\" Enacts the specified action in the environment.\n\n        Returns the new price, reward, whether we're finished and an empty dict for compatibility with Gym's\n        interface. \"\"\"\n\n        self._take_action(Actions(action))\n\n        next_state = self.price\n        print('(self.price).shape =', (self.price).shape)\n        #next_state = self.observation_space.sample()\n\n        reward = self._get_reward()\n        done = False\n\n        if next_state &lt; 0 or reward == 0:\n            done = True\n\n        print(next_state, reward, done, {})\n\n        return np.array(next_state), reward, done, {}\n\n    def reset(self):\n        \"\"\" Resets the environment, selecting a random initial price. Returns the price. \"\"\"\n        #self.observation_space.value = np.random.rand()\n        #return self.observation_space.sample()\n\n        self.price = np.random.rand(1)\n\n        print('reset -&gt; (self.price).shape = ', (self.price).shape)\n\n        return self.price\n\n    def _take_action(self, action):\n#         self.observation_space.value += PRICE_ADJUSTMENT[action]\n        #print('price b =', self.price)\n        print('price b =', self.price[0])\n        #print('price b =', self.price[[0]])\n        #self.price += PRICE_ADJUSTMENT[action]\n        self.price[0] += PRICE_ADJUSTMENT[action]\n        #self.price[[0]] += PRICE_ADJUSTMENT[action]\n        #print('price a =', self.price)\n        print('price a =', self.price[0])\n        #print('price a =', self.price[[0]])\n\n    #def _get_reward(self, price):\n    def _get_reward(self):\n#         price = self.observation_space.value\n#         return max(np.random.poisson(sigmoid_price_fun(price, 50, 0.5)) * price, 0)\n        #self.revenue = max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)\n        #return max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)\n        self.revenue = max(np.random.poisson(sigmoid_price_fun(self.price[0], 50, 0.5)) * self.price[0], 0)\n        return max(np.random.poisson(sigmoid_price_fun(self.price[0], 50, 0.5)) * self.price[0], 0)\n\n#     def render(self, mode='human'):\n#         super().render(mode)\n\ndef testEnv():\n    \"\"\"\n    register(\n        id='ArrivalSim-v0',\n        entry_point='env:ArrivalSim',\n        kwargs= {'price' : 40.0}\n    )\n    env = gym.make('ArrivalSim-v0')\n    \"\"\"\n    env = ArrivalSim(30.0)\n\n    val = env.reset()\n    print('val.shape = ', val.shape)\n\n    for _ in range(5):\n        print('env.observation_space =', env.observation_space)\n        act = env.action_space.sample()\n        print('\\nact =', act)\n        next_state, reward, done, _ = env.step(act)  # take a random action\n        print('next_state = ', next_state)\n    env.close()\n\n\n\nif __name__ =='__main__':\n\n    testEnv()\n<\/code><\/pre>\n\n<p>The edited training script:<\/p>\n\n<pre><code>import json\nimport os\n\nimport gym\nimport ray\nfrom ray.tune import run_experiments\nimport ray.rllib.agents.a3c as a3c\nimport ray.rllib.agents.ppo as ppo\nfrom ray.tune.registry import register_env\nfrom mod_op_env import ArrivalSim\n\nfrom sagemaker_rl.ray_launcher import SageMakerRayLauncher\n\n\"\"\"\ndef create_environment(env_config):\n    import gym\n#     from gym.spaces import Space\n    from gym.envs.registration import register\n\n    # This import must happen inside the method so that worker processes import this code\n    register(\n        id='ArrivalSim-v0',\n        entry_point='env:ArrivalSim',\n        kwargs= {'price' : 40}\n    )\n    return gym.make('ArrivalSim-v0')\n\"\"\"\ndef create_environment(env_config):\n    price = 30.0\n    # This import must happen inside the method so that worker processes import this code\n    from mod_op_env import ArrivalSim\n    return ArrivalSim(price)\n\n\nclass MyLauncher(SageMakerRayLauncher):\n    def __init__(self):        \n        super(MyLauncher, self).__init__()\n        self.num_gpus = int(os.environ.get(\"SM_NUM_GPUS\", 0))\n        self.hosts_info = json.loads(os.environ.get(\"SM_RESOURCE_CONFIG\"))[\"hosts\"]\n        self.num_total_gpus = self.num_gpus * len(self.hosts_info)\n\n    def register_env_creator(self):\n        register_env(\"ArrivalSim-v0\", create_environment)\n\n    def get_experiment_config(self):\n        return {\n          \"training\": {\n            \"env\": \"ArrivalSim-v0\",\n            \"run\": \"PPO\",\n            \"stop\": {\n              \"training_iteration\": 3,\n            },\n\n            \"local_dir\": \"\/opt\/ml\/model\/\",\n            \"checkpoint_freq\" : 3,\n\n            \"config\": {                                \n              #\"num_workers\": max(self.num_total_gpus-1, 1),\n              \"num_workers\": max(self.num_cpus-1, 1),\n              #\"use_gpu_for_workers\": False,\n              \"train_batch_size\": 128, #5,\n              \"sample_batch_size\": 32, #1,\n              \"gpu_fraction\": 0.3,\n              \"optimizer\": {\n                \"grads_per_step\": 10\n              },\n            },\n            #\"trial_resources\": {\"cpu\": 1, \"gpu\": 0, \"extra_gpu\": max(self.num_total_gpus-1, 1), \"extra_cpu\": 0},\n            #\"trial_resources\": {\"cpu\": 1, \"gpu\": 0, \"extra_gpu\": max(self.num_total_gpus-1, 0),\n            #                    \"extra_cpu\": max(self.num_cpus-1, 1)},\n            \"trial_resources\": {\"cpu\": 1,\n                                \"extra_cpu\": max(self.num_cpus-1, 1)},              \n          }\n        }\n\nif __name__ == \"__main__\":\n    os.environ[\"LC_ALL\"] = \"C.UTF-8\"\n    os.environ[\"LANG\"] = \"C.UTF-8\"\n    os.environ[\"RAY_USE_XRAY\"] = \"1\"\n    print(ppo.DEFAULT_CONFIG)\n    MyLauncher().train_main()\n\n<\/code><\/pre>\n\n<p>The notebook code:<\/p>\n\n<pre><code>!\/bin\/bash .\/setup.sh\n\nfrom time import gmtime, strftime\nimport sagemaker \nrole = sagemaker.get_execution_role()\n\nsage_session = sagemaker.session.Session()\ns3_bucket = sage_session.default_bucket()  \ns3_output_path = 's3:\/\/{}\/'.format(s3_bucket)\nprint(\"S3 bucket path: {}\".format(s3_output_path))\n\njob_name_prefix = 'ArrivalSim'\n\nfrom sagemaker.rl import RLEstimator, RLToolkit, RLFramework\n\nestimator = RLEstimator(entry_point=\"mod_op_train.py\", # Our launcher code\n                        source_dir='src', # Directory where the supporting files are at. All of this will be\n                                          # copied into the container.\n                        dependencies=[\"common\/sagemaker_rl\"], # some other utils files.\n                        toolkit=RLToolkit.RAY, # We want to run using the Ray toolkit against the ray container image.\n                        framework=RLFramework.TENSORFLOW, # The code is in tensorflow backend.\n                        toolkit_version='0.5.3', # Toolkit version. This will also choose an apporpriate tf version.                                               \n                        #toolkit_version='0.6.5', # Toolkit version. This will also choose an apporpriate tf version.                        \n                        role=role, # The IAM role that we created at the begining.\n                        #train_instance_type=\"ml.m4.xlarge\", # Since we want to run fast, lets run on GPUs.\n                        train_instance_type=\"local\", # Since we want to run fast, lets run on GPUs.\n                        train_instance_count=1, # Single instance will also work, but running distributed makes things \n                                                # fast, particularly in the case of multiple rollout training.\n                        output_path=s3_output_path, # The path where we can expect our trained model.\n                        base_job_name=job_name_prefix, # This is the name we setup above to be to track our job.\n                        hyperparameters = {      # Some hyperparameters for Ray toolkit to operate.\n                          \"s3_bucket\": s3_bucket,\n                          \"rl.training.stop.training_iteration\": 2, # Number of iterations.\n                          \"rl.training.checkpoint_freq\": 2,\n                        },\n                        #metric_definitions=metric_definitions, # This will bring all the logs out into the notebook.\n                    )\n\nestimator.fit()\n<\/code><\/pre>",
        "Solution_comment_count":19.0,
        "Solution_last_edit_time":1568837974432,
        "Solution_link_count":2.0,
        "Solution_readability":12.1,
        "Solution_reading_time":222.55,
        "Solution_score_count":1.0,
        "Solution_sentence_count":215.0,
        "Solution_word_count":1601.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":2.4873480555,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I estimated a factorization machine model in sagemaker and it saved a file <code>model.tar.gz<\/code> into an s3 folder.<\/p>\n\n<p>Is there a way I can load this file in Python and access the parameter of the model, i.e. the factors, directly?<\/p>\n\n<p>Thanks<\/p>",
        "Challenge_closed_time":1575844058683,
        "Challenge_comment_count":0,
        "Challenge_created_time":1575827808930,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1575835104230,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59238265",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":3.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":4.5138202778,
        "Challenge_title":"sagemaker - factorization machines - deserialize model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":164.0,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Poster_created_time":1456487654208,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berlin, Germany",
        "Poster_reputation_count":1464.0,
        "Poster_view_count":62.0,
        "Solution_body":"<p>As of April 2019: yes. An official AWS blog post was created to show how to open the SageMaker Factorization Machines artifact and extract its parameters: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/extending-amazon-sagemaker-factorization-machines-algorithm-to-predict-top-x-recommendations\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/extending-amazon-sagemaker-factorization-machines-algorithm-to-predict-top-x-recommendations\/<\/a><\/p>\n\n<p>That being said, be aware that Amazon SageMaker built-in algorithm are primarily built for deployment on AWS, and only <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"nofollow noreferrer\">SageMaker XGBoost<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/blazingtext.html\" rel=\"nofollow noreferrer\">SageMaker BlazingText<\/a> are designed to produce artifacts interoperable with their open-source equivalent.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":26.7,
        "Solution_reading_time":12.79,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1267440784443,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Somewhere",
        "Answerer_reputation_count":15705.0,
        "Answerer_view_count":2150.0,
        "Challenge_adjusted_solved_time":0.1769625,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I can train a XGBoost model using Sagemaker images like so:<\/p>\n<pre><code>import boto3\nimport sagemaker\nfrom sagemaker.inputs import TrainingInput\nimport os\n\nfolder = r&quot;C:\\Somewhere&quot;\nos.chdir(folder)\n\ns3_prefix = 'some_model'\ns3_bucket_name = 'the_bucket'\ntrain_file_name = 'train.csv'\nval_file_name = 'val.csv'\nrole_arn = 'arn:aws:iam::482777693429:role\/bla_instance_role'\n\nregion_name = boto3.Session().region_name\n\ns3_input_train = TrainingInput(s3_data='s3:\/\/{}\/{}\/{}'.format(s3_bucket_name, s3_prefix, train_file_name), content_type='csv')\ns3_input_val = TrainingInput(s3_data='s3:\/\/{}\/{}\/{}'.format(s3_bucket_name, s3_prefix, val_file_name), content_type='csv')\n\nprint(type(s3_input_train))\n\nhyperparameters = {\n        &quot;max_depth&quot;:&quot;13&quot;,\n        &quot;eta&quot;:&quot;0.15&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;objective&quot;:&quot;reg:squarederror&quot;,\n        &quot;num_round&quot;:&quot;50&quot;}\n\noutput_path = 's3:\/\/{}\/{}\/output'.format(s3_bucket_name, s3_prefix)\n\n# 1.5-1\n# 1.3-1\nestimator = sagemaker.estimator.Estimator(image_uri=sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region_name, &quot;1.2-2&quot;), \n                                          hyperparameters=hyperparameters,\n                                          role=role_arn,\n                                          instance_count=1, \n                                          instance_type='ml.m5.2xlarge',\n                                          #instance_type='local', \n                                          volume_size=1, # 1 GB \n                                          output_path=output_path)\n\nestimator.fit({'train': s3_input_train, 'validation': s3_input_val})\n<\/code><\/pre>\n<p>This work for all versions 1.2-2, 1.3-1 and 1.5-1. Unfortunately the following code only works for version 1.2-2:<\/p>\n<pre><code>import boto3\nimport os\nimport pickle as pkl \nimport tarfile\nimport pandas as pd\nimport xgboost as xgb\n\nfolder = r&quot;C:\\Somewhere&quot;\nos.chdir(folder)\n\ns3_prefix = 'some_model'\ns3_bucket_name = 'the_bucket'\nmodel_path = 'output\/sagemaker-xgboost-2022-04-30-10-52-29-877\/output\/model.tar.gz'\nsession = boto3.Session(profile_name='default')\nsession.resource('s3').Bucket(s3_bucket_name).download_file('{}\/{}'.format(s3_prefix, model_path), 'model.tar.gz')\nt = tarfile.open('model.tar.gz', 'r:gz')\nt.extractall()\n\nmodel_file_name = 'xgboost-model'\nwith open(model_file_name, &quot;rb&quot;) as input_file:\ne = pkl.load(input_file) \n<\/code><\/pre>\n<p>Otherwise I get a:<\/p>\n<pre><code>_pickle.UnpicklingError: unpickling stack underflow\n<\/code><\/pre>\n<p>Am I missing something? Is my &quot;pickle loading code wrong&quot;?<\/p>\n<p>The version of xgboost is 1.6.0 where I run the pickle code.<\/p>",
        "Challenge_closed_time":1651318339852,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651317702787,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72068059",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":17.4,
        "Challenge_reading_time":34.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":0.1769625,
        "Challenge_title":"cannot load pickle files for xgboost images of version > 1.2-2 in sagemaker - UnpicklingError",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":140.0,
        "Challenge_word_count":185,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>I found the solution <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/2952\" rel=\"nofollow noreferrer\">here<\/a>. I will leave it in case someone come accross the same issue.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.8,
        "Solution_reading_time":2.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":20.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":14.8261302778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am following the link below for training a TensorFlow model in Azure ML:  <\/p>\n<p><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/ml-frameworks\/tensorflow\/train-hyperparameter-tune-deploy-with-tensorflow\/train-hyperparameter-tune-deploy-with-tensorflow.ipynb\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/ml-frameworks\/tensorflow\/train-hyperparameter-tune-deploy-with-tensorflow\/train-hyperparameter-tune-deploy-with-tensorflow.ipynb<\/a>  <\/p>\n<p>However, as my training dataset is in a container named &quot;sample-datasets&quot; in ADLS Gen2, I changed the following code (in the above link) to refer to the paths in my data lake. So I replaced code A (in the link above) with code B (my code)  <\/p>\n<p>Code A:  <\/p>\n<p>urllib.request.urlretrieve('https:\/\/azureopendatastorage.blob.core.windows.net\/mnist\/train-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 'train-images-idx3-ubyte.gz'))  <br \/>\nurllib.request.urlretrieve('https:\/\/azureopendatastorage.blob.core.windows.net\/mnist\/train-labels-idx1-ubyte.gz',  <br \/>\nfilename=os.path.join(data_folder, 'train-labels-idx1-ubyte.gz'))  <br \/>\nurllib.request.urlretrieve('https:\/\/azureopendatastorage.blob.core.windows.net\/mnist\/t10k-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 't10k-images-idx3-ubyte.gz'))  <br \/>\nurllib.request.urlretrieve('https:\/\/azureopendatastorage.blob.core.windows.net\/mnist\/t10k-labels-idx1-ubyte.gz',  <br \/>\nfilename=os.path.join(data_folder, 't10k-labels-idx1-ubyte.gz'))  <\/p>\n<p>Code B:  <\/p>\n<p>from azureml.core.dataset import Dataset  <br \/>\nurllib.request.urlretrieve('https:\/\/lakehousestgenrichedzone.dfs.core.windows.net\/sample-datasets\/train-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 'train-images-idx3-ubyte.gz'))  <br \/>\nurllib.request.urlretrieve('https:\/\/lakehousestgenrichedzone.dfs.core.windows.net\/sample-datasets\/train-labels-idx1-ubyte.gz', filename=os.path.join(data_folder, 'train-labels-idx1-ubyte.gz'))  <br \/>\nurllib.request.urlretrieve('https:\/\/lakehousestgenrichedzone.dfs.core.windows.net\/sample-datasets\/t10k-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 't10k-images-idx3-ubyte.gz'))  <br \/>\nurllib.request.urlretrieve('https:\/\/lakehousestgenrichedzone.dfs.core.windows.net\/sample-datasets\/t10k-labels-idx1-ubyte.gz', filename=os.path.join(data_folder, 't10k-labels-idx1-ubyte.gz'))  <\/p>\n<p>But I receive the following error:  <\/p>\n<p>HTTPError: HTTP Error 401: Server failed to authenticate the request. Please refer to the information in the www-authenticate header.  <\/p>\n<p>Can you please let me know how I can train the model using my data which are stored in the data lake? More precisely, how my Python code can copy the training dataset from my data lake into data_folder?  <\/p>\n<p>PS: Please note that I have already granted the Blob Storage data Contributor role on my data lake storage account to my Azure ML workspace as a managed identity. <\/p>",
        "Challenge_closed_time":1649420498972,
        "Challenge_comment_count":1,
        "Challenge_created_time":1649367124903,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/804968\/training-a-tensorflow-model-in-azure-ml",
        "Challenge_link_count":9,
        "Challenge_participation_count":3,
        "Challenge_readability":23.0,
        "Challenge_reading_time":41.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":14.8261302778,
        "Challenge_title":"Training a TensorFlow model in Azure ML",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":213,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><em>anonymous user<\/em> I have not worked on ADLS scenarios with Azure ML but I have added the ADLS tag to this thread for others to chip in and add their views.     <\/p>\n<p>Based on the <a href=\"https:\/\/learn.microsoft.com\/en-us\/rest\/api\/storageservices\/data-lake-storage-gen2\">documentation<\/a> for ADLS REST API it supports Azure Active Directory (Azure AD), Shared Key, and shared access signature (SAS) authorization with the APIs that are available to download the files from its storage. So, I think a direct download might not work in this case without authentication.     <\/p>\n<p>I think the easiest way to get your files locally from ADLS is to use the python SDK to authenticate using account key or AD as listed <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/storage\/blobs\/data-lake-storage-directory-file-acl-python\">here<\/a>.    <\/p>\n<p>If you have many files that needs to be downloaded and referenced in your ML experiments then you may also consider to use the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/component-reference\/import-data\">import data<\/a> module of designer for designer experiments or register them as <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-connect-data-ui?tabs=credential\">dataset<\/a> from dataset tab of ml.azure.com which can also be referenced using the Azure ML SDK.     <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":13.2,
        "Solution_reading_time":22.45,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":195.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1321893442023,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1611.0,
        "Answerer_view_count":189.0,
        "Challenge_adjusted_solved_time":367.2256972222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running a Python script with Tensorflow in Amazon Sagemaker notebook instance.  I have no trouble writing to the storage in the notebook normally, but for some reason I am unsuccessful when trying to save Tensorflow model checkpoints.  This code previously worked before it was ported to Sagemaker.<\/p>\n\n<p>Below is a reduced version of my code:<\/p>\n\n<pre><code>bucket = 'sagemaker-complaints-data'    \nprefix = 'DeepTestV2' # place to upload training files within the bucket\ntimestamp = str(int(time()))\nout_dir = os.path.abspath(os.path.join(bucket, prefix, \"runs\", timestamp))\ncheckpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"model\")\npath = saver.save(sess, checkpoint_prefix, global_step=current_step)\nprint(\"Saved model checkpoint to {}\\n\".format(path))\n<\/code><\/pre>\n\n<p>No errors are being thrown and the print statement is outputting the correct path.  I have researched whether there are any known issues with using checkpoints in Sagemaker but have come across literally no posts describing this.<\/p>",
        "Challenge_closed_time":1518715595183,
        "Challenge_comment_count":1,
        "Challenge_created_time":1517393582673,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48539564",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":14.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":367.2256972222,
        "Challenge_title":"Tensorflow - Checkpoints not saving to Sagemaker Notebook Instance",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":690.0,
        "Challenge_word_count":136,
        "Platform":"Stack Overflow",
        "Poster_created_time":1321893442023,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1611.0,
        "Poster_view_count":189.0,
        "Solution_body":"<p>I have found out where this is - for some reason \"checkpoints\" seems to be a reserved word - changing the word to \"checks\" allowed me to write the folder.  Hope this helps someone!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.8,
        "Solution_reading_time":2.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":32.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416648155470,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":14749.0,
        "Answerer_view_count":968.0,
        "Challenge_adjusted_solved_time":6386.0465466667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy a model trained with sklearn to an endpoint and serve it as an API for predictions. All I want to use sagemaker for, is to deploy and server model I had serialised using <code>joblib<\/code>, nothing more. every blog I have read and sagemaker python documentation showed that sklearn model had to be trained on sagemaker in order to be deployed in sagemaker.<\/p>\n<p>When I was going through the SageMaker documentation I learned that sagemaker does let users <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#load-a-model\" rel=\"nofollow noreferrer\">load a serialised model<\/a> stored in S3 as shown below:<\/p>\n<pre><code>def model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))\n    return clf\n<\/code><\/pre>\n<p>And this is what documentation says about the argument <code>model_dir<\/code>:<\/p>\n<blockquote>\n<p>SageMaker will inject the directory where your model files and\nsub-directories, saved by save, have been mounted. Your model function\nshould return a model object that can be used for model serving.<\/p>\n<\/blockquote>\n<p>This again means that training has to be done on sagemaker.<\/p>\n<p>So, is there a way I can just specify the S3 location of my serialised model and have sagemaker de-serialise(or load) the model from S3 and use it for inference?<\/p>\n<h2>EDIT 1:<\/h2>\n<p>I used code in the answer to my application and I got below error when trying to deploy from notebook of SageMaker studio. I believe SageMaker is screaming that training wasn't done on SageMaker.<\/p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-4-6662bbae6010&gt; in &lt;module&gt;\n      1 predictor = model.deploy(\n      2     initial_instance_count=1,\n----&gt; 3     instance_type='ml.m4.xlarge'\n      4 )\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in deploy(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, use_compiled_model, wait, model_name, kms_key, data_capture_config, tags, **kwargs)\n    770         &quot;&quot;&quot;\n    771         removed_kwargs(&quot;update_endpoint&quot;, kwargs)\n--&gt; 772         self._ensure_latest_training_job()\n    773         self._ensure_base_job_name()\n    774         default_name = name_from_base(self.base_job_name)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in _ensure_latest_training_job(self, error_message)\n   1128         &quot;&quot;&quot;\n   1129         if self.latest_training_job is None:\n-&gt; 1130             raise ValueError(error_message)\n   1131 \n   1132     delete_endpoint = removed_function(&quot;delete_endpoint&quot;)\n\nValueError: Estimator is not associated with a training job\n<\/code><\/pre>\n<p>My code:<\/p>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\n# from sagemaker.pytorch import PyTorchModel\nfrom sagemaker.sklearn import SKLearn\nfrom sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n\nsm_role = sagemaker.get_execution_role()  # IAM role to run SageMaker, access S3 and ECR\n\nmodel_file = &quot;s3:\/\/sagemaker-manual-bucket\/sm_model_artifacts\/model.tar.gz&quot;   # Must be &quot;.tar.gz&quot; suffix\n\nclass AnalysisClass(RealTimePredictor):\n    def __init__(self, endpoint_name, sagemaker_session):\n        super().__init__(\n            endpoint_name,\n            sagemaker_session=sagemaker_session,\n            serializer=json_serializer,\n            deserializer=json_deserializer,   # To be able to use JSON serialization\n            content_type='application\/json'   # To be able to send JSON as HTTP body\n        )\n\nmodel = SKLearn(model_data=model_file,\n                entry_point='inference.py',\n                name='rf_try_1',\n                role=sm_role,\n                source_dir='code',\n                framework_version='0.20.0',\n                instance_count=1,\n                instance_type='ml.m4.xlarge',\n                predictor_cls=AnalysisClass)\npredictor = model.deploy(initial_instance_count=1,\n                         instance_type='ml.m4.xlarge')\n<\/code><\/pre>",
        "Challenge_closed_time":1607293074048,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607263332140,
        "Challenge_favorite_count":3.0,
        "Challenge_last_edit_time":1607399645212,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65168915",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":15.5,
        "Challenge_reading_time":51.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":8.2616411111,
        "Challenge_title":"AWS SageMaker - How to load trained sklearn model to serve for inference?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3221.0,
        "Challenge_word_count":393,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559910246180,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":2046.0,
        "Poster_view_count":369.0,
        "Solution_body":"<p>Yes you can. AWS documentation focuses on end-to-end from training to deployment in SageMaker which makes the impression that training has to be done on sagemaker. AWS documentation and examples should have clear separation among Training in Estimator, Saving and loading model, and Deployment model to SageMaker Endpoint.<\/p>\n<h2>SageMaker Model<\/h2>\n<p>You need to create the <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-model.html\" rel=\"nofollow noreferrer\">AWS::SageMaker::Model<\/a> resource which refers to the &quot;model&quot; you have trained <strong>and more<\/strong>. AWS::SageMaker::Model is in CloudFormation document but it is only to explain what AWS resource you need.<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\" rel=\"nofollow noreferrer\">CreateModel<\/a> API creates a SageMaker model resource. The parameters specifie the docker image to use, model location in S3, IAM role to use, etc. See <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html#your-algorithms-inference-code-load-artifacts\" rel=\"nofollow noreferrer\">How SageMaker Loads Your Model Artifacts<\/a>.<\/p>\n<h3>Docker image<\/h3>\n<p>Obviously you need the framework e.g. ScikitLearn, TensorFlow, PyTorch, etc that you used to train your model to get inferences. You need a docker image that has the framework, and HTTP front end to respond to the prediction calls. See <a href=\"https:\/\/github.com\/aws\/sagemaker-inference-toolkit\" rel=\"nofollow noreferrer\">SageMaker Inference Toolkit<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/amazon-sagemaker-toolkits.html\" rel=\"nofollow noreferrer\">Using the SageMaker Training and Inference Toolkits<\/a>.<\/p>\n<p>To build the image is not easy. Hence AWS provides pre-built images called <a href=\"https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/deep-learning-containers-images.html\" rel=\"nofollow noreferrer\">AWS Deep Learning Containers<\/a> and available images are in <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">Github<\/a>.<\/p>\n<p>If your framework and the version is listed there, you can use it as the image. Otherwise you need to build by yourself. See <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-mlops-workshop\/blob\/master\/lab\/01_CreateAlgorithmContainer\/01_Creating%20a%20Classifier%20Container.ipynb\" rel=\"nofollow noreferrer\">Building a docker container for training\/deploying our classifier<\/a>.<\/p>\n<h2>SageMaker Python SDK for Frameworks<\/h2>\n<p>Create SageMaker Model by yourself using API is hard. Hence AWS SageMaker Python SDK has provided utilities to create the SageMaker models for several frameworks. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/index.html\" rel=\"nofollow noreferrer\">Frameworks<\/a> for available frameworks. If it is not there, you may still be able to use <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html#sagemaker.model.FrameworkModel\" rel=\"nofollow noreferrer\">sagemaker.model.FrameworkModel<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html\" rel=\"nofollow noreferrer\">Model<\/a> to load your trained model. For your case, see <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html\" rel=\"nofollow noreferrer\">Using Scikit-learn with the SageMaker Python SDK<\/a>.<\/p>\n<h3>model.tar.gz<\/h3>\n<p>For instance if you used PyTorch and save the model as model.pth. To load the model and the inference code to get the prediction from the model, you need to create a model.tar.gz file. The structure inside the model.tar.gz is explained in <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#model-directory-structure\" rel=\"nofollow noreferrer\">Model Directory Structure<\/a>. If you use Windows, beware of the CRLF to LF. AWS SageMaker runs in *NIX environment. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-the-directory-structure-for-your-model-files\" rel=\"nofollow noreferrer\">Create the directory structure for your model files<\/a>.<\/p>\n<pre><code>|- model.pth        # model file is inside \/ directory.\n|- code\/            # Code artefacts must be inside \/code\n  |- inference.py   # Your inference code for the framework\n  |- requirements.txt  # only for versions 1.3.1 and higher. Name must be &quot;requirements.txt&quot;\n<\/code><\/pre>\n<p>Save the tar.gz file in S3. Make sure of the IAM role to access the S3 bucket and objects.<\/p>\n<h3>Loading model and get inference<\/h3>\n<p>See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-a-pytorchmodel-object\" rel=\"nofollow noreferrer\">Create a PyTorchModel object<\/a>. When instantiating the PyTorchModel class, SageMaker automatically selects the AWS Deep Learning Container image for PyTorch for the version specified in <strong>framework_version<\/strong>. If the image for the version does not exist, then it fails. This has not been documented in AWS but need to be aware of. SageMaker then internally calls the CreateModel API with the S3 model file location and the AWS Deep Learning Container image URL.<\/p>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.pytorch import PyTorchModel\nfrom sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n\nrole = sagemaker.get_execution_role()  # IAM role to run SageMaker, access S3 and ECR\nmodel_file = &quot;s3:\/\/YOUR_BUCKET\/YOUR_FOLDER\/model.tar.gz&quot;   # Must be &quot;.tar.gz&quot; suffix\n\n\nclass AnalysisClass(RealTimePredictor):\n    def __init__(self, endpoint_name, sagemaker_session):\n        super().__init__(\n            endpoint_name,\n            sagemaker_session=sagemaker_session,\n            serializer=json_serializer,\n            deserializer=json_deserializer,   # To be able to use JSON serialization\n            content_type='application\/json'   # To be able to send JSON as HTTP body\n        )\n\nmodel = PyTorchModel(\n    model_data=model_file,\n    name='YOUR_MODEL_NAME_WHATEVER',\n    role=role,\n    entry_point='inference.py',\n    source_dir='code',              # Location of the inference code\n    framework_version='1.5.0',      # Availble AWS Deep Learning PyTorch container version must be specified\n    predictor_cls=AnalysisClass     # To specify the HTTP request body format (application\/json)\n)\n\npredictor = model.deploy(\n    initial_instance_count=1,\n    instance_type='ml.m5.xlarge'\n)\n\ntest_data = {&quot;body&quot;: &quot;YOUR PREDICTION REQUEST&quot;}\nprediction = predictor.predict(test_data)\n<\/code><\/pre>\n<p>By default, SageMaker uses NumPy as the serialization format. To be able to use JSON, need to specify the serializer and content_type. Instead of using RealTimePredictor class, you can specify them to predictor.<\/p>\n<pre><code>predictor.serializer=json_serializer\npredictor.predict(test_data)\n<\/code><\/pre>\n<p>Or<\/p>\n<pre><code>predictor.serializer=None # As the serializer is None, predictor won't serialize the data\nserialized_test_data=json.dumps(test_data) \npredictor.predict(serialized_test_data)\n<\/code><\/pre>\n<h3>Inference code sample<\/h3>\n<p>See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#process-model-input\" rel=\"nofollow noreferrer\">Process Model Input<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#get-predictions-from-a-pytorch-model\" rel=\"nofollow noreferrer\">Get Predictions from a PyTorch Model<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#process-model-output\" rel=\"nofollow noreferrer\">Process Model Output<\/a>. The prediction request is sent as JSON in HTTP request body in this example.<\/p>\n<pre><code>import os\nimport sys\nimport datetime\nimport json\nimport torch\nimport numpy as np\n\nCONTENT_TYPE_JSON = 'application\/json'\n\ndef model_fn(model_dir):\n    # SageMaker automatically load the model.tar.gz from the S3 and \n    # mount the folders inside the docker container. The  'model_dir'\n    # points to the root of the extracted tar.gz file.\n\n    model_path = f'{model_dir}\/'\n    \n    # Load the model\n    # You can load whatever from the Internet, S3, wherever &lt;--- Answer to your Question\n    # NO Need to use the model in tar.gz. You can place a dummy model file.\n    ...\n\n    return model\n\n\ndef predict_fn(input_data, model):\n    # Do your inference\n    ...\n\ndef input_fn(serialized_input_data, content_type=CONTENT_TYPE_JSON):\n    input_data = json.loads(serialized_input_data)\n    return input_data\n\n\ndef output_fn(prediction_output, accept=CONTENT_TYPE_JSON):\n    if accept == CONTENT_TYPE_JSON:\n        return json.dumps(prediction_output), accept\n    raise Exception('Unsupported content type') \n<\/code><\/pre>\n<h2>Related<\/h2>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker\" rel=\"nofollow noreferrer\">Using Models Trained Outside of Amazon SageMaker\n<\/a><\/li>\n<\/ul>\n<h2>Note<\/h2>\n<p>SageMaker team keeps changing the implementations and the documentations are frequently obsolete. When you are sure you did follow the documents and it does not work, obsolete documentation is quite likely. In such case, need to clarify with AWS support, or open an issue in the Github.<\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1630389412780,
        "Solution_link_count":19.0,
        "Solution_readability":16.5,
        "Solution_reading_time":122.15,
        "Solution_score_count":7.0,
        "Solution_sentence_count":93.0,
        "Solution_word_count":900.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":47.2837916667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a model using sagemaker (on aws ml notebook). \nI then exported that model to s3 and a <code>.tar.gz<\/code> file was created there.<\/p>\n\n<p>Im trying to find a way to load the model object to memory in my code (without using AWS docker images and deployment) and run a prediction on it.<\/p>\n\n<p>I looked for functions to do that in the model section of the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html#\" rel=\"nofollow noreferrer\">sagemaker docs<\/a>, but everything there is tightly coupled to the AWS docker images.<\/p>\n\n<p>I then tried opening the file with <code>tarfile<\/code> and <code>shutil<\/code> packages but that was useless.<\/p>\n\n<p>Any ideas?<\/p>",
        "Challenge_closed_time":1562768843267,
        "Challenge_comment_count":1,
        "Challenge_created_time":1562675762410,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56952741",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.1,
        "Challenge_reading_time":9.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":25.8557936111,
        "Challenge_title":"Sagemaker export and load model to memory",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1838.0,
        "Challenge_word_count":106,
        "Platform":"Stack Overflow",
        "Poster_created_time":1480290282627,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2906.0,
        "Poster_view_count":266.0,
        "Solution_body":"<p>With the exception of XGBoost, built-in algorithms are implemented with Apache MXNet, so simply extract the model from the .tar.gz file and load it with MXNet: load_checkpoint() is the API to use.<\/p>\n\n<p>XGBoost models are just pickled objects. Unpickle and load in sklearn:<\/p>\n\n<pre><code>$ python3\n&gt;&gt;&gt; import sklearn, pickle\n&gt;&gt;&gt; model = pickle.load(open(\"xgboost-model\", \"rb\"))\n&gt;&gt;&gt; type(model)\n&lt;class 'xgboost.core.Booster'&gt;\n<\/code><\/pre>\n\n<p>Models trained with built-in library (Tensorflow, MXNet, Pytorch, etc.) are vanilla models that can be loaded as-is with the correct library.<\/p>\n\n<p>Hope this helps.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1562845984060,
        "Solution_link_count":0.0,
        "Solution_readability":7.3,
        "Solution_reading_time":8.33,
        "Solution_score_count":3.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":82.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1565022238448,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":370.0,
        "Answerer_view_count":18.0,
        "Challenge_adjusted_solved_time":6.0927694445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a simple RegisterModel.py script that uses the Azure ML Service SDK to register a fastText .bin model. This completes successfully and I can see the model in the Azure Portal UI (I cannot see what model files are in it). I then want to download the model (DownloadModel.py) and use it (for testing purposes), however it throws an error on the <strong>model.download<\/strong> method (<em>tarfile.ReadError: file could not be opened successfully<\/em>) and makes a 0 byte rjtestmodel8.tar.gz file.<\/p>\n\n<p>I then use the Azure Portal and Add Model and select the same bin model file and it uploads fine. Downloading it with the download.py script below works fine, so I am assuming something is not correct with the Register script.<\/p>\n\n<p>Here are the 2 scripts and the stacktrace - let me know if you can see anything wrong:<\/p>\n\n<p><strong>RegisterModel.py<\/strong><\/p>\n\n<pre><code>import azureml.core\nfrom azureml.core import Workspace, Model\nws = Workspace.from_config()\nmodel = Model.register(workspace=ws,\n                       model_name='rjSDKmodel10',\n                       model_path='riskModel.bin')\n<\/code><\/pre>\n\n<p><strong>DownloadModel.py<\/strong><\/p>\n\n<pre><code># Works when downloading the UI Uploaded .bin file, but not the SDK registered .bin file\nimport os\nimport azureml.core\nfrom azureml.core import Workspace, Model\n\nws = Workspace.from_config()\nmodel = Model(workspace=ws, name='rjSDKmodel10')\nmodel.download(target_dir=os.getcwd(), exist_ok=True)\n<\/code><\/pre>\n\n<p><strong>Stacktrace<\/strong><\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"...\\.vscode\\extensions\\ms-python.python-2019.9.34474\\pythonFiles\\ptvsd_launcher.py\", line 43, in &lt;module&gt;\n    main(ptvsdArgs)\n  File \"...\\.vscode\\extensions\\ms-python.python-2019.9.34474\\pythonFiles\\lib\\python\\ptvsd\\__main__.py\", line 432, in main\n    run()\n  File \"...\\.vscode\\extensions\\ms-python.python-2019.9.34474\\pythonFiles\\lib\\python\\ptvsd\\__main__.py\", line 316, in run_file\n    runpy.run_path(target, run_name='__main__')\n  File \"...\\.conda\\envs\\DoC\\lib\\runpy.py\", line 263, in run_path\n    pkg_name=pkg_name, script_name=fname)\n  File \"...\\.conda\\envs\\DoC\\lib\\runpy.py\", line 96, in _run_module_code\n    mod_name, mod_spec, pkg_name, script_name)\n  File \"...\\.conda\\envs\\DoC\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"...\\\\DownloadModel.py\", line 21, in &lt;module&gt;\n    model.download(target_dir=os.getcwd(), exist_ok=True)\n  File \"...\\.conda\\envs\\DoC\\lib\\site-packages\\azureml\\core\\model.py\", line 712, in download\n    file_paths = self._download_model_files(sas_to_relative_download_path, target_dir, exist_ok)\n  File \"...\\.conda\\envs\\DoC\\lib\\site-packages\\azureml\\core\\model.py\", line 658, in _download_model_files\n    file_paths = self._handle_packed_model_file(tar_path, target_dir, exist_ok)\n  File \"...\\.conda\\envs\\DoC\\lib\\site-packages\\azureml\\core\\model.py\", line 670, in _handle_packed_model_file\n    with tarfile.open(tar_path) as tar:\n  File \"...\\.conda\\envs\\DoC\\lib\\tarfile.py\", line 1578, in open\n    raise ReadError(\"file could not be opened successfully\")\ntarfile.ReadError: file could not be opened successfully\n<\/code><\/pre>\n\n<p><strong>Environment<\/strong><\/p>\n\n<ul>\n<li>riskModel.bin is 6 megs<\/li>\n<li>AMLS 1.0.60<\/li>\n<li>Python 3.7<\/li>\n<li>Working locally with Visual Code<\/li>\n<\/ul>",
        "Challenge_closed_time":1568054240467,
        "Challenge_comment_count":2,
        "Challenge_created_time":1568032306497,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1568060910630,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57854136",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":12.0,
        "Challenge_reading_time":43.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":46,
        "Challenge_solved_time":6.0927694445,
        "Challenge_title":"Registering and downloading a fastText .bin model fails with Azure Machine Learning Service",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":281.0,
        "Challenge_word_count":327,
        "Platform":"Stack Overflow",
        "Poster_created_time":1256089885500,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sydney, Australia",
        "Poster_reputation_count":4947.0,
        "Poster_view_count":531.0,
        "Solution_body":"<p>The Azure Machine Learning service SDK has a bug with how it interacts with Azure Storage, which causes it to upload corrupted files if it has to retry uploading. <\/p>\n\n<p>A couple workarounds:<\/p>\n\n<ol>\n<li>The bug was introduced in 1.0.60 release. If you downgrade to AzureML-SDK 1.0.55, the code should fail when there are issue uploading instead of silently corrupting data.<\/li>\n<li>It's possible that the retry is being triggered by the low timeout values that the AzureML-SDK defaults to. You could investigate changing the timeout in <code>site-packages\/azureml\/_restclient\/artifacts_client.py<\/code><\/li>\n<\/ol>\n\n<p>This bug should be fixed in the next release of the AzureML-SDK.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1568054711300,
        "Solution_link_count":0.0,
        "Solution_readability":8.9,
        "Solution_reading_time":8.71,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":100.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1601729162436,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bengaluru, Karnataka, India",
        "Answerer_reputation_count":887.0,
        "Answerer_view_count":130.0,
        "Challenge_adjusted_solved_time":78.7443508334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Using Azure Machine Learning CLI extension, how do we get the Model ID for the latest version of a Model (with known model name)?<\/p>\n<p>To get the entire list of Model Details with a given name the command is<\/p>\n<pre><code>az ml model list --model-name [Model_Name] --resource-group [RGP_NAME] --subscription-id [SUB_ID] --workspace-name [WS_NAME]\n<\/code><\/pre>\n<p>Running this will give a list of all the models:<\/p>\n<pre><code>[\n  {\n    &quot;createdTime&quot;: &quot;2021-03-19T07:02:03.814172+00:00&quot;,\n    &quot;framework&quot;: &quot;Custom&quot;,\n    &quot;frameworkVersion&quot;: null,\n    &quot;id&quot;: &quot;model:2&quot;\n    &quot;name&quot;: &quot;model&quot;,\n    &quot;version&quot;: 3\n  },\n  {\n    &quot;createdTime&quot;: &quot;2021-03-19T06:46:34.301054+00:00&quot;,\n    &quot;framework&quot;: &quot;Custom&quot;,\n    &quot;frameworkVersion&quot;: null,\n    &quot;id&quot;: &quot;model:2&quot;,\n    &quot;name&quot;: &quot;model&quot;,\n    &quot;version&quot;: 2\n  },\n  {\n    &quot;createdTime&quot;: &quot;2021-03-19T06:38:56.558385+00:00&quot;,\n    &quot;framework&quot;: &quot;Custom&quot;,\n    &quot;frameworkVersion&quot;: null,\n    &quot;id&quot;: &quot;model:1&quot;,\n    &quot;name&quot;: &quot;model&quot;,\n    &quot;version&quot;: 1\n  }\n]\n<\/code><\/pre>\n<p>The <a href=\"https:\/\/docs.microsoft.com\/en-us\/cli\/azure\/ext\/azure-cli-ml\/ml\/model?view=azure-cli-latest#ext_azure_cli_ml_az_ml_model_list\" rel=\"nofollow noreferrer\">Microsoft Documentation<\/a> mentions, we can use a <code>-l<\/code> parameter to get the latest version details:<\/p>\n<pre><code>az ml model list --model-name [Model_Name] --resource-group [RGP_NAME] --subscription-id [SUB_ID] --workspace-name [WS_NAME] -l\n<\/code><\/pre>\n<p>However, running this gives the following error:<\/p>\n<pre><code>ERROR: UnrecognizedArgumentError: unrecognized arguments: -l\n<\/code><\/pre>\n<p>What is the syntax to use this <code>-l<\/code> flag?<\/p>",
        "Challenge_closed_time":1616423749203,
        "Challenge_comment_count":4,
        "Challenge_created_time":1616140269540,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66704314",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":18.9,
        "Challenge_reading_time":26.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":78.7443508334,
        "Challenge_title":"How to get Model ID of the Latest Version registered in Azure Machine Learning Service Model Registry using az ml cli?",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":545.0,
        "Challenge_word_count":175,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>If we wish to obtain the model-id for the latest model, instead of using <code>az ml model<\/code> list with <code>-l<\/code> flag, using <code>az model show<\/code> will return the details for the latest model. The syntax to get a string for model-id will be:<\/p>\n<pre><code>az ml model show --model-id $(TRN_MODEL_ID) --resource-group $(AML_TRN_RG) --subscription-id $(AML_TRN_SUB_ID) --workspace-name $(AML_TRN_WS) --query name -o tsv\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.0,
        "Solution_reading_time":5.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":60.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11.8333333333,
        "Challenge_answer_count":2,
        "Challenge_body":"Hello community,\u00a0\n\nSince the migration from Google AutoML to Google Advanced I can upload a dataset like before but then the \"Train\" button does not respond: there seems there is nothing to be trained.\u00a0\n\nHas anyone faced this problem with Google Advanced when training machine translation models? I leave a screenshot below:\u00a0\n\nThanks in advance and kind regards,\u00a0\n\nClara",
        "Challenge_closed_time":1682708280000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1682665680000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/empty-train-model-Train-of-machine-translation-GOOGLE-ADVANCED\/td-p\/548351\/jump-to\/first-unread-message",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.7,
        "Challenge_reading_time":5.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":11.8333333333,
        "Challenge_title":"empty train model (Train) of machine translation GOOGLE ADVANCED",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":67.0,
        "Challenge_word_count":67,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Good day\u00a0@claraginovart,\n\nWelcome to Google Cloud Community!\n\nThere are several reasons why you are encountering this issue, please note that in order to access your upgraded resources you will be using the Cloud Translation Advanced API not the AutoML API. Listed below are some possible solutions to the problem:\u00a0\n\n1. If you are using an API to import data into datasets unlike in legacy (AutoML) where you are using CSV file to specify the location of the source file in CGS, in native (Cloud Translation) you need to specify the locations of TSV and TMX files in CGS. You can learn more about the difference of legacy and native resources using this link:\nhttps:\/\/cloud.google.com\/translate\/docs\/advanced\/automl-upgrade#differences_between_legacy_and_nativ...\n\n2. You need to update your code to use the Cloud Translation API and call the native resource IDs not the legacy resource IDs, although legacy and native are identical they don't have the same resource IDs, same goes for newly created resources and translation predictions where you must use Cloud Translation API rather than\u00a0AutoML API. You can check these documentations to learn more:\n\u00a0https:\/\/cloud.google.com\/translate\/docs\/advanced\/automl-upgrade#cloud-translation-api\nhttps:\/\/cloud.google.com\/translate\/docs\/advanced\/translating-text-v3\nhttps:\/\/cloud.google.com\/translate\/docs\/advanced\/automl-models\n\n\n3. You need to attach the necessary roles in order for the service account to work correctly. To learn more about the necessary roles, you can check this link.\u00a0https:\/\/cloud.google.com\/translate\/docs\/intro-to-v3#iam\nIf you are also using the features of GCS or AutoML models, you need to include the necessary roles to the service account.\u00a0\n\n4. Verify your dataset to ensure it is in the proper format and has sufficient data to train the model. To see if the training procedure begins, you may also try uploading a smaller dataset.\n\nHope it helps!\n\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":12.2,
        "Solution_reading_time":24.44,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":279.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":3.9360030556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created a simply model and then registered in azure. How can I make a prediction?<\/p>\n<pre><code>from sklearn import svm\nimport joblib\nimport numpy as np\n\n# customer ages\nX_train = np.array([50, 17, 35, 23, 28, 40, 31, 29, 19, 62])\nX_train = X_train.reshape(-1, 1)\n# churn y\/n\ny_train = [&quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;]\n\nclf = svm.SVC(gamma=0.001, C=100.)\nclf.fit(X_train, y_train)\n\njoblib.dump(value=clf, filename=&quot;churn-model.pkl&quot;)\n<\/code><\/pre>\n<p>Registration:<\/p>\n<pre><code>from azureml.core import Workspace\nws = Workspace.get(name=&quot;myworkspace&quot;, subscription_id='My_subscription_id', resource_group='ML_Lingaro')\n\nfrom azureml.core.model import Model\nmodel = Model.register(workspace=ws, model_path=&quot;churn-model.pkl&quot;, model_name=&quot;churn-model-test&quot;)\n<\/code><\/pre>\n<p>Prediction:<\/p>\n<pre><code>from azureml.core.model import Model\nimport os\n\nmodel = Model(workspace=ws, name=&quot;churn-model-test&quot;)\nX_test = np.array([50, 17, 35, 23, 28, 40, 31, 29, 19, 62])\nmodel.predict(X_test) ???? \n<\/code><\/pre>\n<p>Error: <code>AttributeError: 'Model' object has no attribute 'predict'<\/code><\/p>",
        "Challenge_closed_time":1609737626627,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609722575690,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1609723457016,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65556574",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":17.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":4.1808158334,
        "Challenge_title":"How to make prediction after model registration in azure?",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":540.0,
        "Challenge_word_count":120,
        "Platform":"Stack Overflow",
        "Poster_created_time":1605834001336,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":73.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>great question -- I also had the same misconception starting out. The missing piece is that there's a difference between model 'registration' and model 'deployment'. Registration is simply for tracking and for easy downloading at a later place and time. Deployment is what you're after, making a model available to be scored against.<\/p>\n<p>There's a <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python&amp;WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">whole section in the docs about deployment<\/a>. My suggestion would be to deploy it locally first for testing.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.8,
        "Solution_reading_time":8.06,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":75.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":17.6780297222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am roughly following this script <a href=\"https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/blob\/master\/keras\/05-keras-blog-post\/Fashion%20MNIST-SageMaker.ipynb\" rel=\"nofollow noreferrer\">fashion-MNIST-sagemaker<\/a>.<\/p>\n\n<p>I see that in the notebook <\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ntf_estimator = TensorFlow(entry_point='mnist_keras_tf.py', \n                          role=role,\n                          train_instance_count=1, \n                          train_instance_type='local',\n                          framework_version='1.12', \n                          py_version='py3',\n                          script_mode=True,\n                          hyperparameters={'epochs': 1}\n                         )\n<\/code><\/pre>\n\n<p>I am wondering to what extent I can and should use the <code>train_instance_count<\/code> parameter. Will it distribute training along some dimension automatically, if yes - what is the dimension?<\/p>\n\n<p>Further, does it generally make sense to distribute training horizontally in a keras (with tensorflow) based setting?<\/p>",
        "Challenge_closed_time":1577100115740,
        "Challenge_comment_count":0,
        "Challenge_created_time":1577037857400,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59446807",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":17.5,
        "Challenge_reading_time":12.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":17.2939833333,
        "Challenge_title":"sagemaker horizontally scaling tensorflow (keras) model",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":295.0,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Poster_created_time":1456487654208,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berlin, Germany",
        "Poster_reputation_count":1464.0,
        "Poster_view_count":62.0,
        "Solution_body":"<p>distributed training is model and framework specific. Not all models are easy to distribute, and from ML framework to ML framework things are not equally easy. <strong>It is rarely automatic, even less so with TensorFlow and Keras.<\/strong><\/p>\n\n<p>Neural nets are conceptually easy to distribute under the data-parallel paradigm, whereby the gradient computation of a given mini-batch is split among workers, which could be multiple devices in the same host (multi-device) or multiple hosts with each multiple devices (multi-device multi-host). The D2L.ai course provides an in-depth view of how neural nets are distributed <a href=\"http:\/\/d2l.ai\/chapter_computational-performance\/multiple-gpus.html\" rel=\"nofollow noreferrer\">here<\/a> and <a href=\"http:\/\/d2l.ai\/chapter_computational-performance\/parameterserver.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Keras used to be trivial to distribute in multi-device, single host fashion with the <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/utils\/multi_gpu_model\" rel=\"nofollow noreferrer\"><code>multi_gpu_model<\/code>, which will sadly get deprecated in 4 months<\/a>. In your case, you seem to refer to multi-host model (more than one machine), and that requires writing ad-hoc synchronization code such as the one seen in <a href=\"https:\/\/www.tensorflow.org\/tutorials\/distribute\/multi_worker_with_keras\" rel=\"nofollow noreferrer\">this official tutorial<\/a>. <\/p>\n\n<p>Now let's look at how does this relate to SageMaker.<\/p>\n\n<p>SageMaker comes with 3 options for algorithm development. Using distributed training may require a varying amount of custom work depending on the option you choose:<\/p>\n\n<ol>\n<li><p>The <strong>built-in algorithms<\/strong> is a library of 18 pre-written algorithms. Many of them are written to be distributed in single-host multi-GPU or multi-GPU multi-host. With that first option, you don't have anything to do apart from setting <code>train_instance_count<\/code> > 1 to distribute over multiple instances<\/p><\/li>\n<li><p>The <strong>Framework containers<\/strong> (the option you are using) are containers developed for popular frameworks (TensorFlow, PyTorch, Sklearn, MXNet) and provide pre-written docker environment in which you can write arbitrary code. In this options, some container will support one-click creation of ephemeral training clusters to do distributed training, <strong>however using <code>train_instance_count<\/code> greater than one is not enough to distribute the training of your model. It will just run your script on multiple machines. In order to distribute your training, you must write appropriate distribution and synchronization code in your <code>mnist_keras_tf.py<\/code> script.<\/strong> For some frameworks such code modification will be very simple, for example for TensorFlow and Keras, SageMaker comes with Horovod pre-installed. Horovod is a peer-to-peer ring-style communication mechanism that requires very little code modification and is highly scalable (<a href=\"https:\/\/eng.uber.com\/horovod\/\" rel=\"nofollow noreferrer\">initial annoucement from Uber<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html#training-with-horovod\" rel=\"nofollow noreferrer\">SageMaker doc<\/a>, <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_horovod\/tensorflow_script_mode_horovod.ipynb\" rel=\"nofollow noreferrer\">SageMaker example<\/a>, <a href=\"https:\/\/aws.amazon.com\/fr\/blogs\/machine-learning\/launching-tensorflow-distributed-training-easily-with-horovod-or-parameter-servers-in-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">SageMaker blog post<\/a>). My recommendation would be to try using Horovod to distribute your code. Similarly, in Apache MXNet you can easily create Parameter Stores to host model parameters in a distributed fashion and sync with them from multiple nodes. MXNet scalability and ease of distribution is one of the reason Amazon loves it.<\/p><\/li>\n<li><p>The <strong>Bring-Your-Own Container<\/strong> requires you to write both docker container and algorithm code. In this situation, you can of course distribute your training over multiple machines but you also have to write machine-to-machine communication code<\/p><\/li>\n<\/ol>\n\n<p>For your specific situation my recommendation would be to scale horizontally first in a single node with multiple GPUs over bigger and bigger machine types, because latency and complexity increase drastically as you switch from single-host to multi-host context. If truly necessary, use multi-node context and things maybe easier if that's done with Horovod.\nIn any case, things are still much easier to do with SageMaker since it manages creation of ephemeral, billed-per-second clusters with built-in, logging and metadata and artifacts persistence and also handles fast training data loading from s3, sharded over training nodes. <\/p>\n\n<p><strong>Note on the relevancy of distributed training<\/strong>: Keep in mind that when you distribute over N devices a model that was running fine on one device, you usually grow the batch size by N so that the per-device batch size stays constant and each device keeps being busy. This will disturb your model convergence, because bigger batches means a less noisy SGD. A common heuristic is to grow the learning rate by N (more info in <a href=\"https:\/\/arxiv.org\/abs\/1706.02677\" rel=\"nofollow noreferrer\">this great paper from Priya Goyal et al<\/a>), but this on the other hand induces instability at the first couple epochs, so it is sometimes associated with a learning rate warmup. Scaling SGD to work well with very large batches is still an active research problem, with new ideas coming up frequently. Reaching good model performance with very large batches sometimes require ad-hoc research and a fair amount of parameter tuning, occasionally to the extent where the extra money spent on finding how to distribute well overcome the benefits of the faster training you eventually manage to run. A situation where distributed training makes sense is when an individual record represent too much compute to form a big enough physical batch on a device, a situation seen on big input sizes (eg vision over HD pictures) or big parameter counts (eg BERT). That being said, for those models requiring very big logical batch you don't necessarily have to distribute things physically: you can run sequentially N batches through your single GPU and wait N per-device batches before doing the gradient averaging and parameter update to simulate having an N times bigger GPU. (a clever hack sometimes called <em>gradient accumulation<\/em>)<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1577101498307,
        "Solution_link_count":9.0,
        "Solution_readability":13.8,
        "Solution_reading_time":85.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":41.0,
        "Solution_word_count":853.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1250158552416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Challenge_adjusted_solved_time":6.8599055556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to execute Python script from Azure machine learning studio. I had a script bundle(zip file) connect to the Python script as input. There are python files, txt files and other type of files in this zip file. My question is how do I get the file path from this zip file. For example, if I have language model in this  zip file, named lm.pcl, what's the file path of this language model? \nThanks!<\/p>",
        "Challenge_closed_time":1539582490283,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539557794623,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52807787",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.7,
        "Challenge_reading_time":5.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":6.8599055556,
        "Challenge_title":"Azure machine learning studio get access to the file in upload zip file",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":679.0,
        "Challenge_word_count":88,
        "Platform":"Stack Overflow",
        "Poster_created_time":1337362023536,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":581.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>They're available under the <code>.\/Script Bundle<\/code> directory. For example, if you were to load a pickled model from the zip file, you'd write something along these lines:<\/p>\n\n<pre><code>import pandas as pd\nimport pickle\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n\n    model = pickle.load(open(\".\/Script Bundle\/model.pkl\", \"rb\"))\n    ...\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":4.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":56.2932952778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an Azure Data factory that receives data from a service bus and then I want to classify my data with an ML model.  <\/p>\n<p>Is there any solution to save and load the ML model on the Azure Data Factory pipeline?  <\/p>\n<p>For your information, I want to use cloud base solution. I don't use the PICKLE library. <\/p>",
        "Challenge_closed_time":1623860467256,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623657811393,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/434449\/how-to-save-and-load-ml-model-with-azure-data-fact",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.5,
        "Challenge_reading_time":4.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":56.2932952778,
        "Challenge_title":"How to save and load ML model with Azure Data Factory",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":71,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=b5c4f434-7ffe-0003-0000-000000000000\">@Mohsen Akhavan  <\/a>,    <br \/>\nThanks for the ask and using the Microsoft Q&amp;A platform  .    <\/p>\n<p>I think you can use the machine learning activity . Read and watch the video <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/data-factory\/transform-data-machine-learning-service\">here<\/a> .    <\/p>\n<p>The challenge in your case is the data is in EH and at this time ADF cannot read EH data . I suggest you to use a Azure stream analytics jobs and read the data from EH and write it to SQL or blob . Once the data is in any of these two sources ADF can be used to read the data .    <\/p>\n<p>Please do let me know how it goes .    <br \/>\nThanks     <br \/>\nHimanshu    <br \/>\nPlease do consider clicking on <strong>&quot;Accept Answer&quot;<\/strong> and <strong>&quot;Up-vote&quot;<\/strong> on the post that helps you, as it can be beneficial to other community members    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.9,
        "Solution_reading_time":11.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":134.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1428432018420,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, United States",
        "Answerer_reputation_count":301.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":45.3193069444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm pretty new to Tensorflow and SageMaker and I'm trying to figure out how to write my <code>serving_input_fn()<\/code>. I've tried a number of ways to do it, but to no avail. <\/p>\n\n<p>my input function has 3 feature columns: <code>amount_normalized, x_month and y_month<\/code>:<\/p>\n\n<pre><code>def construct_feature_columns():\n    amount_normalized = tf.feature_column.numeric_column(key='amount_normalized')\n    x_month = tf.feature_column.numeric_column(key='x_month')\n    y_month = tf.feature_column.numeric_column(key='y_month')\n    return set([amount_normalized, x_month, y_month])\n<\/code><\/pre>\n\n<p>I want to be able to call my deployed model using something like <code>deployed_model.predict([1.23,0.3,0.8])<\/code> <\/p>\n\n<p>Where the first element is <code>amount_normalized<\/code>, second is <code>x_month<\/code> third is <code>y_month<\/code><\/p>\n\n<p>I've tried this:<\/p>\n\n<pre><code>FEATURES = ['amount_normalized', 'x_month', 'y_month']\ndef serving_input_fn(params):\n    feature_placeholders = {\n      key : tf.placeholder(tf.float32, [None]) \\\n        for key in FEATURES\n    }\nreturn tf.estimator.export.build_raw_serving_input_receiver_fn(feature_placeholders)()\n<\/code><\/pre>\n\n<p>But all I get is:\n<code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"\".<\/code><\/p>\n\n<p>Any help would be <strong>really<\/strong> appreciated!<\/p>",
        "Challenge_closed_time":1525019634972,
        "Challenge_comment_count":0,
        "Challenge_created_time":1524856485467,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50068941",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.2,
        "Challenge_reading_time":18.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":45.3193069444,
        "Challenge_title":"SageMaker Tensorflow - how to write my serving_input_fn()",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1337.0,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_created_time":1428432018420,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"New York, United States",
        "Poster_reputation_count":301.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>Posting this here in case anyone else has this issue.<\/p>\n\n<p>After a bunch of trial and error I managed to solve my issue by writing my serving input function like this:<\/p>\n\n<pre><code>FEATURES = ['amount_normalized', 'x_month', 'y_month']\ndef serving_input_fn(hyperparameters):\n    feature_spec = {\n        key : tf.FixedLenFeature(shape=[], dtype = tf.float32) \\\n          for key in FEATURES\n    }\n    return tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)()\n<\/code><\/pre>\n\n<p>I can then call my deployed model by passing in a hash:<\/p>\n\n<pre><code>deployed_model.predict({\"amount_normalized\": 2.3, \"x_month\": 0.2, \"y_month\": -0.3})\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.8,
        "Solution_reading_time":8.4,
        "Solution_score_count":3.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":68.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1559910246180,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bengaluru, Karnataka, India",
        "Answerer_reputation_count":2046.0,
        "Answerer_view_count":369.0,
        "Challenge_adjusted_solved_time":6.6052025,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am creating an sagemaker endpoint and loading a pretrained model from an s3 bucket. the model -&gt; model.tar.gz file has directory structure as documented here, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#model-directory-structure\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#model-directory-structure<\/a><\/p>\n<pre><code>model.tar.gz\/\n|- model.pth\n|- code\/\n  |- inference.py\n  |- requirements.txt  # only for versions 1.3.1 and higher\n<\/code><\/pre>\n<p>I have put few dependencies in requirements.txt, is there a way to verify that all the dependencies were installed correctly?<\/p>",
        "Challenge_closed_time":1645694718683,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645671696107,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71246559",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":15.8,
        "Challenge_reading_time":9.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":6.39516,
        "Challenge_title":"How to verify all the dependencies are installed in sagemaker?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":107.0,
        "Challenge_word_count":71,
        "Platform":"Stack Overflow",
        "Poster_created_time":1590797441983,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":525.0,
        "Poster_view_count":98.0,
        "Solution_body":"<p>It is not possible to get access or SSH into the machine that's running your deployment. So, one way is to assert the versions of your dependencies in the <code>model_fn<\/code> inside &quot;inference.py&quot; something like below.<\/p>\n<p>if your requirements.txt looks like this:<\/p>\n<pre><code>numpy==1.20.3\npandas==1.3.4\n<\/code><\/pre>\n<p>get the versions and assert them in `model_fn like below:<\/p>\n<pre><code>import os\n\n### your other code ###\n\ndef model_fn(model_dir):\n    # assuming you have numpy and pandas\n    assert os.popen(&quot;python3 -m pip freeze | grep -E 'numpy|pandas'&quot;).read() == 'numpy==1.20.3\\npandas==1.3.4\\n'\n    ### your other code ###\n    return xxxx\n\n### your other code ###\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1645695474836,
        "Solution_link_count":0.0,
        "Solution_readability":7.4,
        "Solution_reading_time":8.98,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":85.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1491898605956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sydney NSW, Australia",
        "Answerer_reputation_count":161.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":373.8896469444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The following code snippet is inspired by <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"nofollow noreferrer\">this<\/a>.<\/p>\n<pre><code>hyperparameters = {\n        &quot;max_depth&quot;:&quot;5&quot;,\n        &quot;eta&quot;:&quot;0.2&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;objective&quot;:&quot;reg:squarederror&quot;,\n        &quot;num_round&quot;:&quot;10&quot;}\n\noutput_path = 's3:\/\/{}\/{}\/output'.format(s3_bucket_name, s3_prefix)\n\nestimator = sagemaker.estimator.Estimator(image_uri=sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region_name, &quot;1.2-2&quot;), \n                                          hyperparameters=hyperparameters,\n                                          role=role,\n                                          instance_count=1, \n                                          instance_type='ml.m5.2xlarge', \n                                          volume_size=1, # 1 GB \n                                          output_path=output_path)\n\nestimator.fit({'train': s3_input_train, 'validation': s3_input_val})\n<\/code><\/pre>\n<p>It works fine. I was trying to use:<\/p>\n<pre><code>training_image_name = image_uris.retrieve(framework='xgboost', region=region_name, version='latest')\n<\/code><\/pre>\n<p>instead of:<\/p>\n<pre><code>sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region_name, &quot;1.2-2&quot;)\n<\/code><\/pre>\n<p>to (I believe) get hold of the latest training image but reg:squarederror is not supported? Is my code to get hold of the latest image name incorrect?<\/p>",
        "Challenge_closed_time":1651279786032,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649933783303,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71870508",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":19.7,
        "Challenge_reading_time":18.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":373.8896469444,
        "Challenge_title":"latest aws xgb image does not support reg:",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":26.0,
        "Challenge_word_count":92,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>Using &quot;latest&quot; it not suggested as per documentation(see note): <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html<\/a><\/p>\n<p>Use specific versions as they are more stable.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":32.0,
        "Solution_reading_time":4.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":21.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0783333333,
        "Challenge_answer_count":1,
        "Challenge_body":"Is it possible to train and deploy ML models in Greengrass? Or is Greengrass limited to inference while training is done using SageMaker in cloud?",
        "Challenge_closed_time":1556295681000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1556295399000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668529586004,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU2FEvRboNRL2Ipn1yE7IBvg\/greengrass-for-data-processing-and-ml-model-training",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":2.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.0783333333,
        "Challenge_title":"Greengrass for data processing and ML model training",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":77.0,
        "Challenge_word_count":32,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"As a native service offering, Greengrass has support for deploying models to the edge and running *inference* code against those models. Nothing prevents you from deploying your own code to the edge that would train a model, but I suspect you wouldn't be able to store it as a Greengrass local resource for later inferences without doing a round trip to the cloud and redeploying to GG.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1606141115182,
        "Solution_link_count":0.0,
        "Solution_readability":15.2,
        "Solution_reading_time":4.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1388815483776,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":100.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":474.1775913889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm having an issue to serve a model with reference to model registry. According to help, the path should look like this: <\/p>\n\n<p>models:\/model_name\/stage<\/p>\n\n<p>When I type in terminal: <br>\n<code>mlflow models serve -m models:\/ml_test_model1\/Staging --no-conda -h 0.0.0.0 -p 5003<\/code><\/p>\n\n<p>I got the error: <br>\n<code>mlflow.exceptions.MlflowException: Not a proper models:\/ URI: models:\/ml_test_model1\/Staging\/MLmodel. Models URIs must be of the form 'models:\/&lt;model_name&gt;\/&lt;version or stage&gt;'.<\/code><\/p>\n\n<p>Model is registered and visible in db and server. <br> \nIf I put absolute path, it works (experiment_id\/run_id\/artifacts\/model_name).<\/p>\n\n<p>mlflow version: 1.4 <br>\nPython version: 3.7.3<\/p>\n\n<p>Is it matter of some environmental settings or something different?<\/p>",
        "Challenge_closed_time":1577232243980,
        "Challenge_comment_count":0,
        "Challenge_created_time":1575544859307,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59194004",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.4,
        "Challenge_reading_time":10.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":468.7179647222,
        "Challenge_title":"MLflow - Serving model by reference to model registry",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1225.0,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1575543357812,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>That style of referencing model artefacts is fixed from mlflow v1.5 (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/pull\/2067\" rel=\"nofollow noreferrer\">Bug Fix<\/a>).<\/p>\n\n<p>You'll need to run <code>mlflow db upgrade &lt;db uri&gt;<\/code> to refresh your schemas before restarting your mlflow server.<\/p>\n\n<p>You may find listing registered models helpful:<\/p>\n\n<p><code>&lt;server&gt;:&lt;port&gt;\/api\/2.0\/preview\/mlflow\/registered-models\/list<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1577251898636,
        "Solution_link_count":1.0,
        "Solution_readability":10.1,
        "Solution_reading_time":6.02,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":42.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1512770138847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":90.6846269444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a <code>.npy<\/code> file for each one of the training instances. All of these files are available on S3 in <code>train_data<\/code> folder. I want to train a tensorflow model on these training instances. To do that, I wish to spin up separate aws training instance for each training job which could access the files from s3 and train the model on it. What changes in the training script are required for doing this?<\/p>\n<p>I have following config in the training script:<\/p>\n<pre><code>parser.add_argument('--gpu-count', type=int, default=os.environ['SM_NUM_GPUS'])\nparser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\nparser.add_argument('--train_channel', type=str, default=os.environ['SM_CHANNELS'])\n<\/code><\/pre>\n<p>I have created the training estimator in jupyter instance as:<\/p>\n<pre><code>tf_estimator = TensorFlow(entry_point = 'my_model.py', \n                          role = role, \n                          train_instance_count = 1, \n                          train_instance_type = 'local_gpu', \n                          framework_version = '1.15.2', \n                          py_version = 'py3', \n                          hyperparameters = {'epochs': 1})\n<\/code><\/pre>\n<p>I am calling the fit function of the estimator as:<\/p>\n<pre><code>tf_estimator.fit({'train_channel':'s3:\/\/sagemaker-ml\/train_data\/'})\n<\/code><\/pre>\n<p>where <code>train_data<\/code> folder on S3 contains the <code>.npy<\/code> files of training instances.<\/p>\n<p>But when I call the fit function, I get an error:<\/p>\n<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '[&quot;train_channel&quot;]\/train_data_12.npy'\n<\/code><\/pre>\n<p>Not sure what am I missing here, as I can see the file mentioned above on S3.<\/p>",
        "Challenge_closed_time":1593530314240,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593203486503,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1593203849583,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62602435",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":21.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":90.7854825,
        "Challenge_title":"How to train tensorflow on sagemaker in script mode when the data resides in multiple files on s3?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":340.0,
        "Challenge_word_count":198,
        "Platform":"Stack Overflow",
        "Poster_created_time":1378268842847,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, India",
        "Poster_reputation_count":4616.0,
        "Poster_view_count":592.0,
        "Solution_body":"<p><code>SM_CHANNELS<\/code> returns a list of channel names. What you're looking for is <code>SM_CHANNEL_TRAIN_CHANNEL<\/code> (&quot;SM_CHANNEL&quot; + your channel name), which provides the filesystem location for the channel:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>parser.add_argument('--train_channel', type=str, default=os.environ['SM_CHANNEL_TRAIN_CHANNEL'])\n<\/code><\/pre>\n<p>docs: <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\/blob\/master\/ENVIRONMENT_VARIABLES.md#sm_channel_channel_name\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-training-toolkit\/blob\/master\/ENVIRONMENT_VARIABLES.md#sm_channel_channel_name<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":27.3,
        "Solution_reading_time":9.24,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":36.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1507058830472,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":1619.9066602778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I used AutoML in Databricks Notebooks for a binary classification problem and the winning model flavor was XGBoost (big surprise).<\/p>\n<p>The outputted model is of this variety:<\/p>\n<pre><code>mlflow.pyfunc.loaded_model:\n      artifact_path: model\n      flavor: mlflow.sklearn\n      run_id: 123456789\n<\/code><\/pre>\n<p>Any idea why when I use <code>model.predict_proba(X)<\/code>, I get this response?<\/p>\n<p><code>AttributeError: 'PyFuncModel' object has no attribute 'predict_proba'<\/code><\/p>\n<p>I know it is possible to get the probabilities because ROC\/AUC is a metric used for tuning the model. Any help would be amazing!<\/p>",
        "Challenge_closed_time":1646744187860,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640912523883,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70538098",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":8.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1619.9066602778,
        "Challenge_title":"Databricks MLFlow AutoML XGBoost can't predict_proba()",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":451.0,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Poster_created_time":1562828557216,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Francisco, CA, USA",
        "Poster_reputation_count":77.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>I had the same issue with catboost model.\nThe way I solved it was by saving the artifacts in a local dir<\/p>\n<pre><code>import os\nfrom mlflow.tracking import MlflowClient\nclient = MlflowClient()\nlocal_dir = &quot;\/dbfs\/FileStore\/user\/models&quot;\nlocal_path = client.download_artifacts('run_id', &quot;model&quot;, local_dir)```\n\n```model_path = '\/dbfs\/FileStore\/user\/models\/model\/model.cb'\nmodel = CatBoostClassifier()\nmodel = model.load_model(model_path)\nmodel.predict_proba(test_set)```\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.2,
        "Solution_reading_time":6.7,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":44.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":1.8609305556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create a simple linear learner in AWS SageMaker with MXNet. I have never worked with SageMaker or MXNet previously. Fitting the model gives runtime error as follows and shuts the instance:<\/p>\n\n<blockquote>\n  <p>UnexpectedStatusException: Error for Training job\n  linear-learner-2020-02-11-06-13-22-712: Failed. Reason: ClientError:\n  Unable to read data channel 'train'. Requested content-type is\n  'application\/x-recordio-protobuf'. Please verify the data matches the\n  requested content-type. (caused by MXNetError)<\/p>\n<\/blockquote>\n\n<p>I think that the data should be converted to protobuf format before passing as training data. Could someone please explain to me what is the correct format for MXNet models? What is the best way to convert a simple data frame into protobuf?<\/p>",
        "Challenge_closed_time":1581411605430,
        "Challenge_comment_count":0,
        "Challenge_created_time":1581404906080,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60163614",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":10.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":1.8609305556,
        "Challenge_title":"What is correct input for mxnet's linear learner in AWS SageMaker?",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":617.0,
        "Challenge_word_count":120,
        "Platform":"Stack Overflow",
        "Poster_created_time":1490866954400,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Ahmedabad, Gujarat, India",
        "Poster_reputation_count":834.0,
        "Poster_view_count":91.0,
        "Solution_body":"<p><a href=\"https:\/\/github.com\/awslabs\/fraud-detection-using-machine-learning\/blob\/master\/source\/notebooks\/sagemaker_fraud_detection.ipynb\" rel=\"nofollow noreferrer\">This end-to-end demo<\/a> shows usage of Linear Learner from input data pre-processed in <code>pandas<\/code> dataframes and then converted to protobuf using the SDK. But note that:<\/p>\n\n<ul>\n<li>There is no need to use protobuf, you can also pass csv data with the target variable on the first column of the files, as <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/linear-learner.html#ll-input_output\" rel=\"nofollow noreferrer\">indicated here<\/a>.<\/li>\n<li>There is no need to know MXNet in order to use the SageMaker Linear Learner, just use the SDK of your choice, bring data to S3, and orchestrate training and inference :)<\/li>\n<\/ul>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.1,
        "Solution_reading_time":10.56,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":93.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1539125320752,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":5233.5912852778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to understand how parameters servers (PS's) work for distributed training in Tensorflow on Amazon SageMaker. <\/p>\n\n<p>To make things more concrete, I am able to run the example from AWS using PS's: <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/tf-distributed-training.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/tf-distributed-training.ipynb<\/a><\/p>\n\n<p>Here is the code block that initializes the estimator for Tensorflow:<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ngit_config = {'repo': 'https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode', 'branch': 'master'}\n\nps_instance_type = 'ml.p3.2xlarge'\nps_instance_count = 2\n\nmodel_dir = \"\/opt\/ml\/model\"\n\ndistributions = {'parameter_server': {\n                    'enabled': True}\n                }\nhyperparameters = {'epochs': 60, 'batch-size' : 256}\n\nestimator_ps = TensorFlow(\n                       git_config=git_config,\n                       source_dir='tf-distribution-options\/code',\n                       entry_point='train_ps.py', \n                       base_job_name='ps-cifar10-tf',\n                       role=role,\n                       framework_version='1.13',\n                       py_version='py3',\n                       hyperparameters=hyperparameters,\n                       train_instance_count=ps_instance_count, \n                       train_instance_type=ps_instance_type,\n                       model_dir=model_dir,\n                       tags = [{'Key' : 'Project', 'Value' : 'cifar10'},{'Key' : 'TensorBoard', 'Value' : 'dist'}],\n                       distributions=distributions)\n<\/code><\/pre>\n\n<p>Going through the documentation for Tensorflow, it seems that a device scope can be used for assigning a variable to a particular worker. However, I never see this done when running training jobs on SageMaker. In the example from AWS, the model is defined by:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/code\/model_def.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/code\/model_def.py<\/a><\/p>\n\n<p>Here is a snippet:<\/p>\n\n<pre><code>def get_model(learning_rate, weight_decay, optimizer, momentum, size, mpi=False, hvd=False):\n\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), padding='same', input_shape=(HEIGHT, WIDTH, DEPTH)))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Conv2D(32, (3, 3)))\n\n    ...\n\n    model.add(Flatten())\n    model.add(Dense(512))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(NUM_CLASSES))\n    model.add(Activation('softmax'))\n\n    if mpi:\n        size = hvd.size()\n\n    if optimizer.lower() == 'sgd':\n        ...\n\n    if mpi:\n        opt = hvd.DistributedOptimizer(opt)\n\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=opt,\n                  metrics=['accuracy'])\n\n    return model\n<\/code><\/pre>\n\n<p>Here, there are no references to distribution strategies (except with MPI, but that flag is set to False for PS's). Somehow, Tensorflow or the SageMaker container is able to decide where the variables for each layer should be stored. However, I'm not seeing anything in the container code that does anything with the distribution strategy.<\/p>\n\n<p>I am able to run this code and train the model using 1 and 2 instances. When i do so, I see a decrease of almost 50% in the runtime, suggesting that a distributed training is occurring.<\/p>\n\n<p>My questions are:<\/p>\n\n<ol>\n<li>How does Tensorflow decide the distribution of variables on the PS's? In the example code, there is no explicit reference to devices. Somehow the distribution is done automatically.<\/li>\n<li>Is it possible to see which parameters have been assigned to each PS? Or to see what the communication between PS's looks like? If so, how?<\/li>\n<\/ol>",
        "Challenge_closed_time":1600204151310,
        "Challenge_comment_count":0,
        "Challenge_created_time":1581363222683,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60157184",
        "Challenge_link_count":5,
        "Challenge_participation_count":1,
        "Challenge_readability":16.1,
        "Challenge_reading_time":47.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":5233.5912852778,
        "Challenge_title":"Tensorflow Parameter Servers on SageMaker",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":307.0,
        "Challenge_word_count":350,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416337478872,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":101.0,
        "Poster_view_count":13.0,
        "Solution_body":"<blockquote>\n<p>My questions are:<\/p>\n<p>How does Tensorflow decide the distribution of variables on the PS's?\nIn the example code, there is no explicit reference to devices.\nSomehow the distribution is done automatically.<\/p>\n<\/blockquote>\n<p>The TensorFlow image provided by SageMaker has the code to setup TF_CONFIG and launching parameter server for multi work training. See the code [here][1] The setup is for each node in the cluster there is a PS and a worker thread configured.<\/p>\n<p>It's not using any DistributionStrategy so the default strategy is used. See [here][2].<\/p>\n<p>If you would like to use a different DistributionStrategy or different TF_CONFIG you will need to disable <code>parameter_server<\/code> option when launching the SageMaker training job and set everything up in your training script.<\/p>\n<blockquote>\n<p>Is it possible to see which parameters have been assigned to each PS?\nOr to see what the communication between PS's looks like? If so, how?<\/p>\n<\/blockquote>\n<p>You should be able to get some information from the output log which can be found in CloudWatch. The link is available on the Training Job console page.\n[1]: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-training-toolkit\/blob\/master\/src\/sagemaker_tensorflow_container\/training.py#L37\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-training-toolkit\/blob\/master\/src\/sagemaker_tensorflow_container\/training.py#L37<\/a>\n[2]: <a href=\"https:\/\/www.tensorflow.org\/guide\/distributed_training#default_strategy\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/guide\/distributed_training#default_strategy<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":12.6,
        "Solution_reading_time":21.32,
        "Solution_score_count":1.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":187.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1978.4166666667,
        "Challenge_answer_count":3,
        "Challenge_body":"Hi, I just got started using vertex ai with google cloud console. I am trying to deploy this mode to an endpoint. https:\/\/tfhub.dev\/tensorflow\/efficientnet\/lite0\/feature-vector\/2 I successfully imported it into a google storage bucket and uploaded it to the model registry. However, when I attempt to deploy the model to an endpoint, I receive the following error.\n\nHello Vertex AI Customer,\n\nDue to an error, Vertex AI was unable to create endpoint \"Feature Vectors\".\nAdditional Details:\nOperation State: Failed with errors\nResource Name: \n**path to project**\nError Messages: Model server terminated: model server container terminated: \nexit_code:       255\nreason: \"Error\"\nstarted_at {\n   seconds: 1669817118\n}\nfinished_at {\n   seconds: 1669817421\n}\n. Model server logs can be found at \n**some link**\n\nI have attempted to change the TensorFlow version and the folder that i import (I attempted to import the containing folder instead of the model folder) however nothing seems to help. Any suggestions would be greatly appreciated. Thank you!",
        "Challenge_closed_time":1676994480000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1669872180000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Model-Deployment-Error\/td-p\/495020\/jump-to\/first-unread-message",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":11.2,
        "Challenge_reading_time":13.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":1978.4166666667,
        "Challenge_title":"Vertex AI Model Deployment Error",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":262.0,
        "Challenge_word_count":155,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Yep, the solution was pretty simple.\u00a0\n\nThe trick was to import the model as a tensorflow GraphDef and then export the model with the serve tags included. You can then use this exported model instead of the untagged model. Hope this helps!\u00a0\n\n\n\n# import tensorflow as tf\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nfrom tensorflow.python.platform import gfile\n\nmodel_file = \".\/efficientnet_lite0_feature-vector_2\/saved_model.pb\"\n\nwith tf.Session() as sess:\n\n    graph = tf.Graph()\n    graph_def = tf.GraphDef()\n    with open(model_file, \"rb\") as f:\n        graph_def.ParseFromString(f.read())\n\n    tf.import_graph_def(graph_def)\n\n# Export the model to \/tmp\/my-model.meta.\nmeta_graph_def = tf.serve.export_meta_graph(filename='.\/efficientnet_lite0_feature-vector_2\/info.meta')\n\n\u00a0\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.4,
        "Solution_reading_time":10.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":84.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1595479476676,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Massachusetts, USA",
        "Answerer_reputation_count":246.0,
        "Answerer_view_count":21.0,
        "Challenge_adjusted_solved_time":168.9973711111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to create an MLOps Pipeline using Azure DevOps and Azure Databricks. From Azure DevOps, I am submitting a Databricks job to a cluster, which trains a Machine Learning Model and saves it into MLFlow Model Registry with a custom flavour (using PyFunc Custom Model).<\/p>\n<p>Now after the job gets over, I want to export this MLFlow Object (with all dependencies - Conda dependencies, two model files - one <code>.pkl<\/code> and one <code>.h5<\/code>, the Python Class with <code>load_context()<\/code> and <code>predict()<\/code> functions defined so that after exporting I can import it and call predict as we do with MLFlow Models).<\/p>\n<p>How do I export this entire MLFlow Model and save it as an AzureDevOps Artifact to be used in the CD phase (where I will deploy it to an AKS cluster with a custom base image)?<\/p>",
        "Challenge_closed_time":1629787454223,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629179063687,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68812238",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.4,
        "Challenge_reading_time":11.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":168.9973711111,
        "Challenge_title":"How to export a MLFlow Model from Azure Databricks as an Azure DevOps Artifacts for CD phase?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":575.0,
        "Challenge_word_count":151,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>There is no official way to export a Databricks MLflow run from one workspace to another. However, there is an &quot;unofficial&quot; tool that does most of the job with the main limitation being that notebook revisions linked to a run cannot be exported due to lack of a REST API endpoint for this.<\/p>\n<p><a href=\"https:\/\/github.com\/amesar\/mlflow-export-import\" rel=\"nofollow noreferrer\">https:\/\/github.com\/amesar\/mlflow-export-import<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.0,
        "Solution_reading_time":5.74,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":57.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1437986390372,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"M\u00fcnchen, Deutschland",
        "Answerer_reputation_count":361.0,
        "Answerer_view_count":149.0,
        "Challenge_adjusted_solved_time":0.0983611111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have an sklearn k-means model. I am training the model and saving it in a pickle file so I can deploy it later using azure ml library. The model that I am training uses a custom Feature Encoder called <strong>MultiColumnLabelEncoder<\/strong>.\nThe pipeline model is defined as follow :<\/p>\n\n<pre><code># Pipeline\nkmeans = KMeans(n_clusters=3, random_state=0)\npipe = Pipeline([\n(\"encoder\", MultiColumnLabelEncoder()),\n('k-means', kmeans),\n])\n#Training the pipeline\nmodel = pipe.fit(visitors_df)\nprediction = model.predict(visitors_df)\n#save the model in pickle\/joblib format\nfilename = 'k_means_model.pkl'\njoblib.dump(model, filename)\n<\/code><\/pre>\n\n<p>The model saving works fine. The Deployment steps are the same as the steps in this link : <\/p>\n\n<p><a href=\"https:\/\/notebooks.azure.com\/azureml\/projects\/azureml-getting-started\/html\/how-to-use-azureml\/deploy-to-cloud\/model-register-and-deploy.ipynb\" rel=\"nofollow noreferrer\">https:\/\/notebooks.azure.com\/azureml\/projects\/azureml-getting-started\/html\/how-to-use-azureml\/deploy-to-cloud\/model-register-and-deploy.ipynb<\/a><\/p>\n\n<p>However the deployment always fails with this error :<\/p>\n\n<pre><code>  File \"\/var\/azureml-server\/create_app.py\", line 3, in &lt;module&gt;\n    from app import main\n  File \"\/var\/azureml-server\/app.py\", line 27, in &lt;module&gt;\n    import main as user_main\n  File \"\/var\/azureml-app\/main.py\", line 19, in &lt;module&gt;\n    driver_module_spec.loader.exec_module(driver_module)\n  File \"\/structure\/azureml-app\/score.py\", line 22, in &lt;module&gt;\n    importlib.import_module(\"multilabelencoder\")\n  File \"\/azureml-envs\/azureml_b707e8c15a41fd316cf6c660941cf3d5\/lib\/python3.6\/importlib\/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\nModuleNotFoundError: No module named 'multilabelencoder'\n<\/code><\/pre>\n\n<p>I understand that pickle\/joblib has some problems unpickling the custom function MultiLabelEncoder. That's why I defined this class in a separate python script (which I executed also). I called this custom function in the training python script, in the deployment script and in the scoring python file (score.py). The importing in the score.py file is not successful. \nSo my question is how can I import custom python module to azure ml deployment environment ?<\/p>\n\n<p>Thank you in advance.<\/p>\n\n<p>EDIT: \nThis is my .yml file<\/p>\n\n<pre><code>name: project_environment\ndependencies:\n  # The python interpreter version.\n  # Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.2\n\n- pip:\n  - multilabelencoder==1.0.4\n  - scikit-learn\n  - azureml-defaults==1.0.74.*\n  - pandas\nchannels:\n- conda-forge\n<\/code><\/pre>",
        "Challenge_closed_time":1575635052340,
        "Challenge_comment_count":1,
        "Challenge_created_time":1575463048677,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1575634698240,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59176241",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.4,
        "Challenge_reading_time":35.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":47.7787952778,
        "Challenge_title":"import custom python module in azure ml deployment environment",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2611.0,
        "Challenge_word_count":276,
        "Platform":"Stack Overflow",
        "Poster_created_time":1437986390372,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"M\u00fcnchen, Deutschland",
        "Poster_reputation_count":361.0,
        "Poster_view_count":149.0,
        "Solution_body":"<p>In fact, the solution was to import my customized class <strong>MultiColumnLabelEncoder<\/strong> as a pip package (You can find it through pip install multilllabelencoder==1.0.5).\nThen I passed the pip package to the .yml file or in the InferenceConfig of the azure ml environment.\nIn the score.py file, I imported the class as follows :<\/p>\n\n<pre><code>from multilabelencoder import multilabelencoder\ndef init():\n    global model\n\n    # Call the custom encoder to be used dfor unpickling the model\n    encoder = multilabelencoder.MultiColumnLabelEncoder() \n    # Get the path where the deployed model can be found.\n    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'k_means_model_45.pkl')\n    model = joblib.load(model_path)\n<\/code><\/pre>\n\n<p>Then the deployment was successful. \nOne more important thing is I had to use the same pip package (multilabelencoder) in the training pipeline as here :<\/p>\n\n<pre><code>from multilabelencoder import multilabelencoder \npipe = Pipeline([\n    (\"encoder\", multilabelencoder.MultiColumnLabelEncoder(columns)),\n    ('k-means', kmeans),\n])\n#Training the pipeline\ntrainedModel = pipe.fit(df)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.2,
        "Solution_reading_time":14.38,
        "Solution_score_count":4.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":132.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":278.3016666667,
        "Challenge_answer_count":4,
        "Challenge_body":"I mistakenly put my model in pretrained folder but outside the model subfolder. In such case an exception is caught silently and then a sleep is called only to retry the exact behaviour.\r\nWhile I did not fix the issue, I added logging to make it verbose. I will try to upload a patch.",
        "Challenge_closed_time":1562359564000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1561357678000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws-deepracer-community\/deepracer-core\/issues\/21",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":6.1,
        "Challenge_reading_time":4.39,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":105.0,
        "Challenge_repo_issue_count":108.0,
        "Challenge_repo_star_count":236.0,
        "Challenge_repo_watch_count":16.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":278.3016666667,
        "Challenge_title":"When pretrained model is not found, sagemaker falls into an infinite silent loop",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":66,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"```\r\nIndex: rl_coach\/src\/markov\/s3_client.py\r\nIDEA additional info:\r\nSubsystem: com.intellij.openapi.diff.impl.patch.CharsetEP\r\n<+>UTF-8\r\n===================================================================\r\n--- rl_coach\/src\/markov\/s3_client.py\t(revision 789d7553bdfda74d10dcfbcc0c0286fdb0b5b57f)\r\n+++ rl_coach\/src\/markov\/s3_client.py\t(date 1561357411000)\r\n@@ -60,10 +60,12 @@\r\n \r\n     def download_model(self, checkpoint_dir):\r\n         s3_client = self.get_client()\r\n+        logger.info(\"Downloading pretrained model from %s\/%s %s\" % (self.bucket, self.model_checkpoints_prefix, checkpoint_dir))\r\n         filename = \"None\"\r\n         try:\r\n             filename = os.path.abspath(os.path.join(checkpoint_dir, \"checkpoint\"))\r\n             if not os.path.exists(checkpoint_dir):\r\n+                logger.info(\"Model folder %s does not exist, creating\" % filename)\r\n                 os.makedirs(checkpoint_dir)\r\n \r\n             while True:\r\n@@ -73,13 +75,17 @@\r\n                 if \"Contents\" not in response:\r\n                     # If no lock is found, try getting the checkpoint\r\n                     try:\r\n+                        key = self._get_s3_key(\"checkpoint\")\r\n+                        logger.info(\"Downloading %s\" % key)\r\n                         s3_client.download_file(Bucket=self.bucket,\r\n-                                                Key=self._get_s3_key(\"checkpoint\"),\r\n+                                                Key=key,\r\n                                                 Filename=filename)\r\n                     except Exception as e:\r\n+                        logger.info(\"Something went wrong, will retry in 2 seconds %s\" % e)\r\n                         time.sleep(2)\r\n                         continue\r\n                 else:\r\n+                    logger.info(\"Found a lock file, waiting\")\r\n                     time.sleep(2)\r\n                     continue\r\n \r\n@@ -98,6 +104,8 @@\r\n                             filename = os.path.abspath(os.path.join(checkpoint_dir,\r\n                                                                     obj[\"Key\"].replace(self.model_checkpoints_prefix,\r\n                                                                                        \"\")))\r\n+\r\n+                            logger.info(\"Downloading model file %s\" % filename)\r\n                             s3_client.download_file(Bucket=self.bucket,\r\n                                                     Key=obj[\"Key\"],\r\n                                                     Filename=filename)\r\n\r\n``` Sadly, no file upload is available in issues You can open a pull request if you like, do you need help in doing so? It's definitely an issue and I've encountered it myself. It also happens when the checkpoint file references files that don't exist. Thank you @bhannebipro , I lost track :)",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.4,
        "Solution_reading_time":24.24,
        "Solution_score_count":1.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":164.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1460437080990,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":386.0,
        "Answerer_view_count":42.0,
        "Challenge_adjusted_solved_time":0.7767452778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to reduce my development headaches for creating a ML Webservice on Azure ML Studio. One of the things that stuck me was can we just upload .rda files in the workbench and load it via an RScript (like in the figure below). <\/p>\n\n<p><img src=\"https:\/\/raw.githubusercontent.com\/pratos\/pratos.github.io\/master\/images\/stackb1model.png\" alt=\"Do\"><\/p>\n\n<p>But can't connect directly to the R Script block. There's another way to do it (works to upload packages that aren't available in Azure's R directories) -- using zip. But there isn't really any resource out there that I found to access the .rda file in .zip.<\/p>\n\n<p>I have 2 options here, make the .zip work or any other work around where I can directly use my .rda model. If someone could guide me about how to go forward it would appreciate it.<\/p>\n\n<p>Note: Currently, I'm creating models via the \"Create RModel\" block, training them and saving it, so that I can use it to make a predictive web service. But for models like Random Forest, not sure how the randomness might create models (local versions and Azure versions are different, the setting of seed also isn't very helpful). A bit tight on schedule, Azure ML seems boxed for creating iterations and automating the ML workflow (or maybe I'm doing it wrong).<\/p>",
        "Challenge_closed_time":1486104439200,
        "Challenge_comment_count":0,
        "Challenge_created_time":1486101642917,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42017727",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":16.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":0.7767452778,
        "Challenge_title":"Upload Saved ML Model in R (local) to Azure Machine Learning Studio",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":730.0,
        "Challenge_word_count":221,
        "Platform":"Stack Overflow",
        "Poster_created_time":1479363468550,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":108.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>Here is an example of uploading a .rda file for scoring:\n<a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Womens-Health-Risk-Assessment-using-the-XGBoost-classification-algorithm-1\" rel=\"nofollow noreferrer\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Womens-Health-Risk-Assessment-using-the-XGBoost-classification-algorithm-1<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":48.9,
        "Solution_reading_time":5.01,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":15.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":14.2479583334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I developed a designer to implement regression models in azure machine learning studio. I have taken the data set pill and then split the data set into train and test in prescribed manner. When I am trying to implement the evaluation metrics and run the pipeline, it was showing a warning and error in the moment I called the dataset for the operation. I am bit confused, with the same implementation, when i tried to run with linear regression and it worked as shown in the image. If the same approach is used to implement logistic regression it was showing some warning and error in building the evaluation metrics.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/DbEeq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DbEeq.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>the above success is in linear regression. When it comes to logistic regression it was showing the warning and error in pipeline.<\/p>\n<p>Any help is appreciated.<\/p>",
        "Challenge_closed_time":1664021303563,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663970010913,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73833320",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":13.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":14.2479583334,
        "Challenge_title":"parameters error in azure ML designer in evaluation metrics in regression model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":29.0,
        "Challenge_word_count":156,
        "Platform":"Stack Overflow",
        "Poster_created_time":1652172570283,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":19.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>Creating a sample pipeline with designer with mathematical format.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/kCx3A.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kCx3A.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>We need to create a compute instance.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/7bQA5.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7bQA5.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/NQWZV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NQWZV.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/MpxPY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/MpxPY.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bAfEv.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bAfEv.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Assign the compute instance and click on create<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wIS0d.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wIS0d.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Now the import data warning will be removed. In the same manner, we will be getting similar error in other pills too.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/zFK74.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zFK74.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Yk5gY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Yk5gY.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Create a mathematical format. If not needed for your case, try to remove that math operation and give the remaining.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/uhKlv.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uhKlv.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Assign the column set. Select any option according to the requirement.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/mcNZe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mcNZe.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Finally, we can find the pills which have no warning or error.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":20.0,
        "Solution_readability":14.1,
        "Solution_reading_time":30.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":189.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1627764232887,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":4.7981325,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained a SageMaker semantic segmentation model, using the built-in sagemaker semantic segmentation algorithm. This deploys ok to a SageMaker endpoint and I can run inference in the cloud  successfully from it.\nI would like to use the model on a edge device (AWS Panorama Appliance) which should just mean compiling the model with SageMaker Neo to the specifications of the target device.<\/p>\n<p>However, regardless of what my target device is (the Neo settings), I cant seem to compile the model with Neo as I get the following error:<\/p>\n<pre><code>ClientError: InputConfiguration: No valid Mxnet model file -symbol.json found\n<\/code><\/pre>\n<p>The model.tar.gz for semantic segmentation models contains hyperparams.json, model_algo-1, model_best.params. According to the docs, model_algo-1 is the serialized mxnet model. Aren't gluon models supported by Neo?<\/p>\n<p>Incidentally I encountered the exact same problem with another SageMaker built-in algorithm, the k-Nearest Neighbour (k-NN). It too seems to be compiled without a -symbol.json.<\/p>\n<p>Is there some scripts I can run to recreated a -symbol.json file or convert the compiled sagemaker model?<\/p>\n<p>After building my model with an Estimator, I got to compile it in SageMaker Neo with code:<\/p>\n<pre><code>optimized_ic = my_estimator.compile_model(\n target_instance_family=&quot;ml_c5&quot;,\n target_platform_os=&quot;LINUX&quot;,\n target_platform_arch=&quot;ARM64&quot;,\n input_shape={&quot;data&quot;: [1,3,512,512]},  \n output_path=s3_optimized_output_location,\n framework=&quot;mxnet&quot;,\n framework_version=&quot;1.8&quot;, \n)\n<\/code><\/pre>\n<p>I would expect this to compile ok, but that is where I get the error saying the model is missing the *-symbol.json file.<\/p>",
        "Challenge_closed_time":1648038217500,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647989575803,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1648020944223,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71579883",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":23.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":13.5115825,
        "Challenge_title":"Missing -symbol.json error when trying to compile a SageMaker semantic segmentation model (built-in algorithm) with SageMaker Neo",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":52.0,
        "Challenge_word_count":236,
        "Platform":"Stack Overflow",
        "Poster_created_time":1586928819952,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":25.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>For some reason, AWS has decided to not make its built-in algorithms directly compatible with Neo... However, you can re-engineer the network parameters using the model.tar.gz output file and then compile.<\/p>\n<p>Step 1: Extract model from tar file<\/p>\n<pre><code>import tarfile\n#path to local tar file\nmodel = 'ss_model.tar.gz'\n\n#extract tar file \nt = tarfile.open(model, 'r:gz')\nt.extractall()\n<\/code><\/pre>\n<p>This should output two files:\nmodel_algo-1, model_best.params<\/p>\n<ol start=\"2\">\n<li>Load weights into network from gluon model zoo for the architecture that you chose<\/li>\n<\/ol>\n<p>In this case I used DeepLabv3 with resnet50<\/p>\n<pre><code>import gluoncv\nimport mxnet as mx\nfrom gluoncv import model_zoo\nfrom gluoncv.data.transforms.presets.segmentation import test_transform\n\nmodel = model_zoo.DeepLabV3(nclass=2, backbone='resnet50', pretrained_base=False, height=800, width=1280, crop_size=240)\nmodel.load_parameters(&quot;model_algo-1&quot;)\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Check the parameters have loaded correctly by making a prediction with new model<\/li>\n<\/ol>\n<p>Use an image that was used for training.<\/p>\n<pre><code>#use cpu\nctx = mx.cpu(0)\n#decode image bytes of loaded file\nimg = image.imdecode(imbytes)\n\n#transform image\nimg = test_transform(img, ctx)\nimg = img.astype('float32')\nprint('tranformed image shape: ', img.shape)\n\n#get prediction\noutput = model.predict(img)\n<\/code><\/pre>\n<ol start=\"4\">\n<li>Hybridise model into output required by Sagemaker Neo<\/li>\n<\/ol>\n<p>Additional check for image shape compatibility<\/p>\n<pre><code>model.hybridize()\nmodel(mx.nd.ones((1,3,800,1280)))\nexport_block('deeplabv3-res50', model, data_shape=(3,800,1280), preprocess=None, layout='CHW')\n<\/code><\/pre>\n<ol start=\"5\">\n<li>Recompile model into tar.gz format<\/li>\n<\/ol>\n<p>This contains the params and json file which Neo looks for.<\/p>\n<pre><code>tar = tarfile.open(&quot;comp_model.tar.gz&quot;, &quot;w:gz&quot;)\nfor name in [&quot;deeplabv3-res50-0000.params&quot;, &quot;deeplabv3-res50-symbol.json&quot;]:\n    tar.add(name)\ntar.close()\n<\/code><\/pre>\n<ol start=\"6\">\n<li>Save tar.gz file to s3 and then compile using Neo GUI<\/li>\n<\/ol>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":28.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":231.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":37.0426969444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am very new to SageMaker. Upon my first interaction, it looks like the AWS SageMaker requires you to start from its Notebook. I have a training set which is ready. Is there a way to bypass setting the Notebook and just to start by upload the training set? Or it should be done through the Notebook. If anyone knows some example fitting my need above, that will be great. <\/p>",
        "Challenge_closed_time":1520631374252,
        "Challenge_comment_count":0,
        "Challenge_created_time":1520498020543,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49168673",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.4,
        "Challenge_reading_time":5.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":37.0426969444,
        "Challenge_title":"How to load a training set in AWS SageMaker to build a model?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":700.0,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Poster_created_time":1305269513436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":10317.0,
        "Poster_view_count":595.0,
        "Solution_body":"<p>Amazon SageMaker is a combination of multiple services that each is independent of the others. You can use the notebook instances if you want to develop your models in the familiar Jupyter environment. But if just need to train a model, you can use the training jobs without opening a notebook instance. <\/p>\n\n<p>There a few ways to launch a training job:<\/p>\n\n<ul>\n<li>Use the high-level SDK for Python that is similar to the way that you start a training step in your python code<\/li>\n<\/ul>\n\n<p><code>kmeans.fit(kmeans.record_set(train_set[0]))<\/code><\/p>\n\n<p>Here is the link to the python library: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk<\/a><\/p>\n\n<ul>\n<li>Use the low-level API to Create-Training-Job, and you can do that using various SDK (Java, Python, JavaScript, C#...) or the CLI. <\/li>\n<\/ul>\n\n<p><code>sagemaker = boto3.client('sagemaker')\n sagemaker.create_training_job(**create_training_params)<\/code><\/p>\n\n<p>Here is a link to the documentation on these options: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html<\/a> <\/p>\n\n<ul>\n<li>Use Spark interface to launch it using a similar interface to creating an MLLib training job<\/li>\n<\/ul>\n\n<p><code>val estimator = new KMeansSageMakerEstimator(\n  sagemakerRole = IAMRole(roleArn),\n  trainingInstanceType = \"ml.p2.xlarge\",\n  trainingInstanceCount = 1,\n  endpointInstanceType = \"ml.c4.xlarge\",\n  endpointInitialInstanceCount = 1)\n  .setK(10).setFeatureDim(784)<\/code><\/p>\n\n<p><code>val model = estimator.fit(trainingData)<\/code><\/p>\n\n<p>Here is a link to the spark-sagemaker library: <a href=\"https:\/\/github.com\/aws\/sagemaker-spark\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-spark<\/a><\/p>\n\n<ul>\n<li>Create a training job in the Amazon SageMaker console using the wizard there: <a href=\"https:\/\/console.aws.amazon.com\/sagemaker\/home?region=us-east-1#\/jobs\" rel=\"nofollow noreferrer\">https:\/\/console.aws.amazon.com\/sagemaker\/home?region=us-east-1#\/jobs<\/a><\/li>\n<\/ul>\n\n<p>Please note that there a few options also to train models, either using the built-in algorithms such as K-Means, Linear Learner or XGBoost (see here for the complete list: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html<\/a>). But you can also bring your own models for pre-baked Docker images such as TensorFlow (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/tf.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/tf.html<\/a>) or MXNet (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet.html<\/a>), your own Docker image (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo.html<\/a>).  <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":16.0,
        "Solution_readability":16.7,
        "Solution_reading_time":42.09,
        "Solution_score_count":1.0,
        "Solution_sentence_count":27.0,
        "Solution_word_count":275.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":6.1298294445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there a way to deploy a xgboost model trained locally using amazon sagemaker? I only saw tutorial talking about both training and deploying model with amazon sagemaker.\nThanks.<\/p>",
        "Challenge_closed_time":1531778753543,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531756686157,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51365850",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":2.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":6.1298294445,
        "Challenge_title":"how to deploy a xgboost model on amazon sagemaker?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1641.0,
        "Challenge_word_count":37,
        "Platform":"Stack Overflow",
        "Poster_created_time":1530193663836,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>This <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/d5681a07611ae29567355b60b2f22500b561218b\/advanced_functionality\/xgboost_bring_your_own_model\/xgboost_bring_your_own_model.ipynb\" rel=\"nofollow noreferrer\">example notebook<\/a> is good starting point showing how to use a pre-existing scikit-learn xgboost model with the Amazon SageMaker to create a hosted endpoint for that model.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":21.1,
        "Solution_reading_time":5.55,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1535695625688,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":19.0,
        "Challenge_adjusted_solved_time":355.6374158334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am learning Sagemaker and I have this entry point:<\/p>\n\n<pre><code>import os\nimport tensorflow as tf\nfrom tensorflow.python.estimator.model_fn import ModeKeys as Modes\n\nINPUT_TENSOR_NAME = 'inputs'\nSIGNATURE_NAME = 'predictions'\n\nLEARNING_RATE = 0.001\n\n\ndef model_fn(features, labels, mode, params):\n    # Input Layer\n    input_layer = tf.reshape(features[INPUT_TENSOR_NAME], [-1, 28, 28, 1])\n\n    # Convolutional Layer #1\n    conv1 = tf.layers.conv2d(\n        inputs=input_layer,\n        filters=32,\n        kernel_size=[5, 5],\n        padding='same',\n        activation=tf.nn.relu)\n\n    # Pooling Layer #1\n    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n\n    # Convolutional Layer #2 and Pooling Layer #2\n    conv2 = tf.layers.conv2d(\n        inputs=pool1,\n        filters=64,\n        kernel_size=[5, 5],\n        padding='same',\n        activation=tf.nn.relu)\n    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n\n    # Dense Layer\n    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n    dropout = tf.layers.dropout(\n        inputs=dense, rate=0.4, training=(mode == Modes.TRAIN))\n\n    # Logits Layer\n    logits = tf.layers.dense(inputs=dropout, units=10)\n\n    # Define operations\n    if mode in (Modes.PREDICT, Modes.EVAL):\n        predicted_indices = tf.argmax(input=logits, axis=1)\n        probabilities = tf.nn.softmax(logits, name='softmax_tensor')\n\n    if mode in (Modes.TRAIN, Modes.EVAL):\n        global_step = tf.train.get_or_create_global_step()\n        label_indices = tf.cast(labels, tf.int32)\n        loss = tf.losses.softmax_cross_entropy(\n            onehot_labels=tf.one_hot(label_indices, depth=10), logits=logits)\n        tf.summary.scalar('OptimizeLoss', loss)\n\n    if mode == Modes.PREDICT:\n        predictions = {\n            'classes': predicted_indices,\n            'probabilities': probabilities\n        }\n        export_outputs = {\n            SIGNATURE_NAME: tf.estimator.export.PredictOutput(predictions)\n        }\n        return tf.estimator.EstimatorSpec(\n            mode, predictions=predictions, export_outputs=export_outputs)\n\n    if mode == Modes.TRAIN:\n        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n        train_op = optimizer.minimize(loss, global_step=global_step)\n        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n\n    if mode == Modes.EVAL:\n        eval_metric_ops = {\n            'accuracy': tf.metrics.accuracy(label_indices, predicted_indices)\n        }\n        return tf.estimator.EstimatorSpec(\n            mode, loss=loss, eval_metric_ops=eval_metric_ops)\n\n\ndef serving_input_fn(params):\n    inputs = {INPUT_TENSOR_NAME: tf.placeholder(tf.float32, [None, 784])}\n    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n\n\ndef read_and_decode(filename_queue):\n    reader = tf.TFRecordReader()\n    _, serialized_example = reader.read(filename_queue)\n\n    features = tf.parse_single_example(\n        serialized_example,\n        features={\n            'image_raw': tf.FixedLenFeature([], tf.string),\n            'label': tf.FixedLenFeature([], tf.int64),\n        })\n\n    image = tf.decode_raw(features['image_raw'], tf.uint8)\n    image.set_shape([784])\n    image = tf.cast(image, tf.float32) * (1. \/ 255)\n    label = tf.cast(features['label'], tf.int32)\n\n    return image, label\n\n\ndef train_input_fn(training_dir, params):\n    return _input_fn(training_dir, 'train.tfrecords', batch_size=100)\n\n\ndef eval_input_fn(training_dir, params):\n    return _input_fn(training_dir, 'test.tfrecords', batch_size=100)\n\n\ndef _input_fn(training_dir, training_filename, batch_size=100):\n    test_file = os.path.join(training_dir, training_filename)\n    filename_queue = tf.train.string_input_producer([test_file])\n\n    image, label = read_and_decode(filename_queue)\n    images, labels = tf.train.batch(\n        [image, label], batch_size=batch_size,\n        capacity=1000 + 3 * batch_size)\n\n    return {INPUT_TENSOR_NAME: images}, labels\n\ndef neo_preprocess(payload, content_type):\n    import logging\n    import numpy as np\n    import io\n\n    logging.info('Invoking user-defined pre-processing function')\n\n    if content_type != 'application\/x-image' and content_type != 'application\/vnd+python.numpy+binary':\n        raise RuntimeError('Content type must be application\/x-image or application\/vnd+python.numpy+binary')\n\n    f = io.BytesIO(payload)\n    image = np.load(f)*255\n\n    return image\n\n### NOTE: this function cannot use MXNet\ndef neo_postprocess(result):\n    import logging\n    import numpy as np\n    import json\n\n    logging.info('Invoking user-defined post-processing function')\n\n    # Softmax (assumes batch size 1)\n    result = np.squeeze(result)\n    result_exp = np.exp(result - np.max(result))\n    result = result_exp \/ np.sum(result_exp)\n\n    response_body = json.dumps(result.tolist())\n    content_type = 'application\/json'\n\n    return response_body, content_type\n<\/code><\/pre>\n\n<p>And I am training it <\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='cnn_fashion_mnist.py',\n                       role=role,\n                       input_mode='Pipe',\n                       training_steps=1, \n                       evaluation_steps=1,\n                       train_instance_count=1,\n                       output_path=output_path,\n                       train_instance_type='ml.c5.2xlarge',\n                       base_job_name='mnist')\n<\/code><\/pre>\n\n<p>so far it is trying correctly and it tells me that everything when well, but when I check the output there is nothing there or if I try to deploy it I get the error saying it couldn't find the model because there is nothing in the bucker, any ideas or extra configurations? Thank you<\/p>",
        "Challenge_closed_time":1576626745670,
        "Challenge_comment_count":0,
        "Challenge_created_time":1575346450973,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59150100",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":17.6,
        "Challenge_reading_time":66.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":64,
        "Challenge_solved_time":355.6374158334,
        "Challenge_title":"Sagemaker and Tensorflow model not saved",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":658.0,
        "Challenge_word_count":407,
        "Platform":"Stack Overflow",
        "Poster_created_time":1462770189772,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":45.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>Looks like you are using one of the older Tensorflow versions.\nWe would recommend switching to a newer more straight-forward way of running Tensorflow in SageMaker (script mode) by switching to a more recent Tensorflow version.<\/p>\n\n<p>You can read more about it in our documentation:\n<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html<\/a><\/p>\n\n<p>Here is an example that might help:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/tensorflow_script_mode_training_and_serving.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/tensorflow_script_mode_training_and_serving.ipynb<\/a> <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":25.0,
        "Solution_reading_time":12.16,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":61.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1374169767267,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":548.0,
        "Answerer_view_count":70.0,
        "Challenge_adjusted_solved_time":7.0366380556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to call my SageMaker model endpoint both from Postman and the AWS CLI. The endpoint's status is &quot;in service&quot; but whenever I try to call it it gives me an error. When I try to use the predict function in the SageMaker notebook and provide it a numpy array (ex. <code>np.array([1,2,3,4])<\/code>), it successfully gives me an output. I'm unsure what I'm doing wrong.<\/p>\n<pre><code>$ aws2 sagemaker-runtime invoke-endpoint \\\n$ --endpoint-name=pytorch-model \\\n$ --body=1,2 \\\n$ --content-type=text\/csv \\\n$ --cli-binary-format=raw-in-base64-out \\\n$ output.json\n\nAn error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message &quot;tensors used as indices must be long, byte or bool tensors\nTraceback (most recent call last):\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/transformer.py&quot;, line 125, in transform\n    result = self._transform_fn(self._model, input_data, content_type, accept)\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/transformer.py&quot;, line 215, in _default_transform_fn\n    prediction = self._predict_fn(data, model)\n  File &quot;\/opt\/ml\/model\/code\/pytorch-model-reco.py&quot;, line 268, in predict_fn\n    return torch.argsort(- final_matrix[input_data, :], dim = 1)\nIndexError: tensors used as indices must be long, byte or bool tensors\n<\/code><\/pre>",
        "Challenge_closed_time":1596555829307,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596530497410,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63243154",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.5,
        "Challenge_reading_time":18.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":7.0366380556,
        "Challenge_title":"Invoking SageMaker Endpoint for PyTorch Model",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":394.0,
        "Challenge_word_count":156,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The clue is in the final few lines of your stacktrace:<\/p>\n<pre><code>  File &quot;\/opt\/ml\/model\/code\/pytorch-model-reco.py&quot;, line 268, in predict_fn\n    return torch.argsort(- final_matrix[input_data, :], dim = 1)\nIndexError: tensors used as indices must be long, byte or bool tensors\n<\/code><\/pre>\n<p>In your <code>predict_fn<\/code> in <code>pytorch-model-reco.py<\/code> on line 268, you're trying to use <code>input_data<\/code> as indices for <code>final_matrix<\/code>, but <code>input_data<\/code> is the wrong type.<\/p>\n<p>I would guess there is some type casting that your <code>predict_fn<\/code> should be doing when the input type is <code>text\/csv<\/code>. This type casting is happening outside of the <code>predict_fn<\/code> when your input type is numpy data. Taking a look at the <a href=\"https:\/\/github.com\/aws\/sagemaker-inference-toolkit\/tree\/master\/src\/sagemaker_inference\" rel=\"nofollow noreferrer\"><code>sagemaker_inference<\/code><\/a> source code might reveal more.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.3,
        "Solution_reading_time":12.9,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":109.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":30.2268313889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I know that loading a .csv file into sagemaker notebook from S3 bucket is pretty straightforward but I want to load a model.tar.gz file stored in S3 bucket. I tried to do the following<\/p>\n\n<pre><code>import botocore \nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.predictor import csv_serializer\nimport boto3\n\nsm_client = boto3.client(service_name='sagemaker')\nruntime_sm_client = boto3.client(service_name='sagemaker-runtime')\n\ns3 = boto3.resource('s3')\ns3_client = boto3.client('s3')\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n\nACCOUNT_ID  = boto3.client('sts').get_caller_identity()['Account']\nREGION      = boto3.Session().region_name\nBUCKET      = 'sagemaker.prismade.net'\ndata_key    = 'DEMO_MME_ANN\/multi_model_artifacts\/axel.tar.gz'\nloc = 's3:\/\/{}\/{}'.format(BUCKET, data_key)\nprint(loc)\nwith tarfile.open(loc) as tar:\n    tar.extractall(path='.')\n<\/code><\/pre>\n\n<p>I get the following error:<\/p>\n\n<pre><code>--------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\n&lt;ipython-input-215-bfdddac71b95&gt; in &lt;module&gt;()\n     20 loc = 's3:\/\/{}\/{}'.format(BUCKET, data_key)\n     21 print(loc)\n---&gt; 22 with tarfile.open(loc) as tar:\n     23     tar.extractall(path='.')\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/tarfile.py in open(cls, name, mode, fileobj, bufsize, **kwargs)\n   1567                     saved_pos = fileobj.tell()\n   1568                 try:\n-&gt; 1569                     return func(name, \"r\", fileobj, **kwargs)\n   1570                 except (ReadError, CompressionError):\n   1571                     if fileobj is not None:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/tarfile.py in gzopen(cls, name, mode, fileobj, compresslevel, **kwargs)\n   1632 \n   1633         try:\n-&gt; 1634             fileobj = gzip.GzipFile(name, mode + \"b\", compresslevel, fileobj)\n   1635         except OSError:\n   1636             if fileobj is not None and mode == 'r':\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/gzip.py in __init__(self, filename, mode, compresslevel, fileobj, mtime)\n    161             mode += 'b'\n    162         if fileobj is None:\n--&gt; 163             fileobj = self.myfileobj = builtins.open(filename, mode or 'rb')\n    164         if filename is None:\n    165             filename = getattr(fileobj, 'name', '')\n\nFileNotFoundError: [Errno 2] No such file or directory: 's3:\/\/sagemaker.prismade.net\/DEMO_MME_ANN\/multi_model_artifacts\/axel.tar.gz'\n<\/code><\/pre>\n\n<p>What is the mistake here and how can I accomplish this?<\/p>",
        "Challenge_closed_time":1581004771980,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580895955387,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60072981",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":31.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":30.2268313889,
        "Challenge_title":"How to open a model tarfile stored in S3 bucket in sagemaker notebook?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2984.0,
        "Challenge_word_count":234,
        "Platform":"Stack Overflow",
        "Poster_created_time":1550756471932,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":67.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>Not every python library that is designed to work with a file system (tarfile.open, in this example) knows how to read an object from S3 as a file. <\/p>\n\n<p>The simple way to solve it is to first copy the object into the local file system as a file.<\/p>\n\n<pre><code>import boto3\n\ns3 = boto3.client('s3')\ns3.download_file('BUCKET_NAME', 'OBJECT_NAME', 'FILE_NAME')\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":4.7,
        "Solution_score_count":7.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":57.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":17.0636369444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>If I have a trained model in Using pickle, or Joblib.\nLets say its Logistic regression or XGBoost.<\/p>\n<p>I would like to host that model in AWS Sagemaker as endpoint without running a training job.\nHow to achieve that.<\/p>\n<pre><code>#Lets Say myBucketName contains model.pkl\nmodel = joblib.load('filename.pkl')  \n# X_test = Numpy Array \nmodel.predict(X_test)  \n<\/code><\/pre>\n<p>I am not interested to <code>sklearn_estimator.fit('S3 Train, S3 Validate' )<\/code> , I have the trained model<\/p>",
        "Challenge_closed_time":1599722577176,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599661148083,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63813624",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":7.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":17.0636369444,
        "Challenge_title":"Load a Picked or Joblib Pre trained ML Model to Sagemaker and host as endpoint",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1622.0,
        "Challenge_word_count":80,
        "Platform":"Stack Overflow",
        "Poster_created_time":1370509408500,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1573.0,
        "Poster_view_count":194.0,
        "Solution_body":"<p>For Scikit Learn for example, you can get inspiration from this public demo <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a><\/p>\n<p>Step 1: Save your artifact (eg the joblib) compressed in S3 at <code>s3:\/\/&lt;your path&gt;\/model.tar.gz<\/code><\/p>\n<p>Step 2: Create an inference script with the deserialization function <code>model_fn<\/code>. (Note that you could also add custom inference functions <code>input_fn<\/code>, <code>predict_fn<\/code>, <code>output_fn<\/code> but for scikit the defaults function work fine)<\/p>\n<pre><code>%%writefile inference_script.py. # Jupiter command to create file in case you're in Jupiter\n\nimport joblib\nimport os\n\ndef model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))\n    return clf\n<\/code><\/pre>\n<p>Step 3: Create a model associating the artifact with the right container<\/p>\n<pre><code>from sagemaker.sklearn.model import SKLearnModel\n\nmodel = SKLearnModel(\n    model_data='s3:\/\/&lt;your path&gt;\/model.tar.gz',\n    role='&lt;your role&gt;',\n    entry_point='inference_script.py',\n    framework_version='0.23-1')\n<\/code><\/pre>\n<p>Step 4: Deploy!<\/p>\n<pre><code>model.deploy(\n    instance_type='ml.c5.large',  # choose the right instance type\n    initial_instance_count=1)\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.5,
        "Solution_reading_time":20.38,
        "Solution_score_count":5.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":122.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":24.0739286111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Training a ml model with mlflow in azure environment.<\/p>\n<pre><code>import mlflow\nfrom mlflow import MlflowClient\nfrom azureml.core import Experiment, Workspace\n\nexperiment_name = 'housing-lin-mlflow'\n\nexperiment = Experiment(ws, experiment_name)\n\nruns = mlflow.search_runs(experiment_ids=[ experiment.id ])\n\n<\/code><\/pre>\n<p>While fetching runs from search_runs getting this error :<\/p>\n<pre><code>RestException: BAD_REQUEST: For input string: &quot;5b649b3c-3b8f-497a-bb4f&quot;\n<\/code><\/pre>\n<p>MLflow version : 1.28.0\nIn Azure studio jobs have been created and successfully run.<\/p>",
        "Challenge_closed_time":1661603882123,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661517215980,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1661625379892,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73501103",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":8.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":24.0739286111,
        "Challenge_title":"Getting Bad request while searching run in mlflow",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":56.0,
        "Challenge_word_count":65,
        "Platform":"Stack Overflow",
        "Poster_created_time":1582101477803,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":171.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>The bad request in MLFlow after successful running the job is because of not giving proper API permissions for the application.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Search for <strong>MLFLOW<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Scroll down<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/s50AL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s50AL.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Click on View API Permissions<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Under API permissions, assign the permissions according to the application running region and requirements. Checkout the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-models-mlflow\" rel=\"nofollow noreferrer\">document<\/a> for further information.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":9.0,
        "Solution_readability":17.0,
        "Solution_reading_time":16.36,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":94.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1627043269590,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Richmond, VA, USA",
        "Answerer_reputation_count":67.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":6.3228980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Suppose I have two Azure ML workspaces:<\/p>\n<ol>\n<li><p>Workspace1 - This is being used by one team (Team1) who only train the model and store the model in model registry of Workspace1<\/p>\n<\/li>\n<li><p>Workspace2 - This is used by another team  (Team2) who containerise the model, push it to ACR and then deploy the containerised model in Azure ML Compute.<\/p>\n<\/li>\n<\/ol>\n<p>Is it possible for Team2 to access the model registry of Workspace1 from their Workspace2 and retrieve the model for containerisation and subsequent deployment? Alternatively, is there any concept of a shared model registry in Azure ML where both the teams can store and access a common model registry? If none of these are possible, then what is the way for Team1 and Team2 to work together on a single model with the given responsibilities as described above?<\/p>",
        "Challenge_closed_time":1638210489528,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638197253147,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70156610",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":11.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":3.6767725,
        "Challenge_title":"Accessing an Azure ML Model Registry from another Azure ML Workspace",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":421.0,
        "Challenge_word_count":148,
        "Platform":"Stack Overflow",
        "Poster_created_time":1373471094267,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United Kingdom",
        "Poster_reputation_count":395.0,
        "Poster_view_count":66.0,
        "Solution_body":"<p>As described, I think the best solution is to use one Workspace, not two.  It sounds like you have Team 1 and Team 2 sharing contributions on a single project.  What may work better is to define user roles in the Azure ML workspace, such that Team 2 has permissions to deploy models, and Team 1 has permission to create models.<\/p>\n<p>Otherwise you can always write Python code using the ML SDK to connect to any workspace given you know the subscription, resource group, workspace name etc.<\/p>\n<pre><code>from azure.core import Workspace, Model\n\n# connect to an existing workspace\nname = 'WorkspaceName'\nsub = 'subscriptionName'\nresource_group = 'resourceGroupName'\nws = Workspace.get(name=name, subscription_id=sub, resource_group=resource_group) \n\n# retrieve existing model\nmodel = Model(ws, name='your model name')\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1638220015580,
        "Solution_link_count":0.0,
        "Solution_readability":9.8,
        "Solution_reading_time":10.42,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":116.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1400869861950,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Rio de Janeiro, Brazil",
        "Answerer_reputation_count":135.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":18.1100302778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a pretrained spacy model on a local folder that I can easily read with <code>m = spacy.load(&quot;path\/model\/&quot;)<\/code><\/p>\n<p>But now I have to upload it as a .tar.gz file to use as a Sagemaker model artifact.\nHow can I read this .tar.gz file?<\/p>\n<p>Ideally I want to read the unzipped folder from memory. Without extracting all to disk and then reading it again<\/p>\n<p>My question is almost a duplicate of this one <a href=\"https:\/\/stackoverflow.com\/questions\/49274650\/directly-load-spacy-model-from-packaged-tar-gz-file\">Directly load spacy model from packaged tar.gz file<\/a>. But the answers don't explain how to untar unzip the folder into memory<\/p>",
        "Challenge_closed_time":1652803358076,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652738161967,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72266041",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.4,
        "Challenge_reading_time":9.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":18.1100302778,
        "Challenge_title":"Loading a spacy .tar.gz model artifact from s3 Sagemaker",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":184.0,
        "Challenge_word_count":102,
        "Platform":"Stack Overflow",
        "Poster_created_time":1400869861950,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Rio de Janeiro, Brazil",
        "Poster_reputation_count":135.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>Turns out Sagemaker already decompress the <code>.tar.gz<\/code> file automatically.\nSo I can just read the folder exactly like before.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":1.81,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1342709052703,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"D\u00fcsseldorf, Germany",
        "Answerer_reputation_count":1889.0,
        "Answerer_view_count":654.0,
        "Challenge_adjusted_solved_time":25.8006916667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained the following Sagemaker model: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco<\/a><\/p>\n\n<p>I've tried both the JSON and RecordIO version. In both, the algorithm is tested on ONE sample image. However, I have a dataset of 2000 pictures, which I would like to test. I have saved the 2000 jpg pictures in a folder within an S3 bucket and I also have two .mat files (pics + ground truth). How can I apply this model to all 2000 pictures at once and then save the results, rather than doing it one picture at a time?<\/p>\n\n<p>I am using the code below to load a single picture from my S3 bucket:<\/p>\n\n<pre><code>object = bucket.Object('pictures\/pic1.jpg')\nobject.download_file('pic1.jpg')\nimg=mpimg.imread('pic1.jpg')\nimg_name = 'pic1.jpg'\nimgplot = plt.imshow(img)\nplt.show(imgplot)\n\nwith open(img_name, 'rb') as image:\n    f = image.read()\n    b = bytearray(f)\n    ne = open('n.txt','wb')\n    ne.write(b)\n\nimport json\nobject_detector.content_type = 'image\/jpeg'\nresults = object_detector.predict(b)\ndetections = json.loads(results)\nprint (detections['prediction'])\n<\/code><\/pre>",
        "Challenge_closed_time":1539965456440,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539872573950,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52876202",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":18.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":25.8006916667,
        "Challenge_title":"How to bulk test the Sagemaker Object detection model with a .mat dataset or S3 folder of images?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":112.0,
        "Challenge_word_count":157,
        "Platform":"Stack Overflow",
        "Poster_created_time":1489873508190,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":509.0,
        "Poster_view_count":84.0,
        "Solution_body":"<p>I'm not sure if I understood your question correctly. However, if you want to feed multiple images to the model at once, you can create a multi-dimensional array of images (byte arrays) to feed the model.<\/p>\n\n<p>The code would look something like this.<\/p>\n\n<pre><code>import numpy as np\n...\n\n#  predict_images_list is a Python list of byte arrays\npredict_images = np.stack(predict_images_list)\n\nwith graph.as_default():\n    #  results is an list of typical results you'd get.\n    results = object_detector.predict(predict_images)\n<\/code><\/pre>\n\n<p>But, I'm not sure if it's a good idea to feed 2000 images at once. Better to batch them in 20-30 images at a time and predict. <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":8.31,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":99.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":93.1594822222,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello everyone, I was trying to execute the example mentioned in the docs - [https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_neo_compilation_jobs\/pytorch_torchvision\/pytorch_torchvision_neo.html]().\nI was able to successfully run this example but as soon as I changed the target_device  to `jetson_tx2`, after which I ran the entire script again, keeping the rest of the code as it is, the model stopped working. I was not getting any inferences from the deployed model and it always errors out with the message:\n\n```\nAn error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from <users-sagemaker-endpoint> with message \"Your invocation timed out while waiting for a response from container model. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\"                \n```\nAccording to the troubleshoot docs [https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/neo-troubleshooting-inference.html](), this seems to be an issue of **model_fn**() function.\nThe inference script used by this example is mentioned here [https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_neo_compilation_jobs\/pytorch_torchvision\/code\/resnet18.py]() , which itself doesn't contain any model_fn() definition but it still worked for target device `ml_c5`.\nSo could anyone please help me with the following questions:\n1. What changes does SageMaker Neo do to the model depending on `target_device` type? Since it seems the same model is loaded in a different way for different target device.\n2. Is there any way to determine how the model is expected to load for a certain target_device type so that I could define the **model_fn**() function myself in the same inference script mentioned above?\n3. At-last, can anyone please help with the inference script for this very same model as mentioned in docs above which works for `jetson_tx2` device as well.\n\nAny suggestions or links on how to resolve this issue would be really helpful.",
        "Challenge_closed_time":1668766092280,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668430718144,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668778694208,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUJvbkzp91TGSZO1GwW-r90w\/help-with-inference-script-for-amazon-sagemaker-neo-compiled-models",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":26.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":93.1594822222,
        "Challenge_title":"Help with Inference Script for Amazon Sagemaker Neo Compiled Models",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":58.0,
        "Challenge_word_count":282,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"As you mentioned, you changed the Neo compiling target from `ml_c5` to `jetson_tx2`, the compiled model will require runtime from `jetson_tx2`. If you kept other code unchanged, the model will be deployed to a `ml.c5.9xlarge` EC2 instance, which doesn't provide Nvida Jeston.\n\nThe model can't be loaded and will error out since Jestion is a device Nvidia GPU structure while c5 is only equipped with CPU. No CUDA environment. \n\nIf you compile the model with `jeston_tx2` as target, you should download the model and run the compiled model in a real Nvidia Jeston device.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1668766092280,
        "Solution_link_count":0.0,
        "Solution_readability":7.1,
        "Solution_reading_time":6.96,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":94.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1459917054448,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Frankfurt, Germany",
        "Answerer_reputation_count":9168.0,
        "Answerer_view_count":675.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created a time series predictor with Keras and  Dockerized the model with with Flash and Gunicorn as per AWS docs. I am loading the serialized model with this code.<\/p>\n\n<pre><code>@classmethod\ndef get_model(cls):\n    if cls.model == None:\n        cls.model = load_model('\/opt\/ml\/bitcoin_model.h5')\n    return cls.model\n<\/code><\/pre>\n\n<p>Then I used the predict method to produce the results , the dockerized container is working perfectly in the local environment , but when I try to host the model in sagemaker it produces this error.<\/p>\n\n<pre><code>ValueError: Tensor Tensor(\"dense_1\/BiasAdd:0\", shape=(?, 1), dtype=float32) is not an element of this graph.\n<\/code><\/pre>\n\n<p>So how can I resolve this issue ?<\/p>",
        "Challenge_closed_time":1541480239328,
        "Challenge_comment_count":0,
        "Challenge_created_time":1541480239330,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1541480860787,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53165953",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":10.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"ValueError: Tensor is not an element of this graph, when hosting a model in Sagemaker with Gunicorn and Flask and Keras",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":223.0,
        "Challenge_word_count":117,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459917054448,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Frankfurt, Germany",
        "Poster_reputation_count":9168.0,
        "Poster_view_count":675.0,
        "Solution_body":"<p>The issue was resolved by calling _make_predict_function() method in the model load phase.<\/p>\n\n<pre><code>@classmethod\ndef get_model(cls):\n    if cls.model == None:\n        cls.model = load_model('\/opt\/ml\/bitcoin_model.h5')\n        cls.model._make_predict_function()\n    return cls.model\n<\/code><\/pre>\n\n<p>Bug Reference : <a href=\"https:\/\/github.com\/keras-team\/keras\/issues\/6462\" rel=\"nofollow noreferrer\">https:\/\/github.com\/keras-team\/keras\/issues\/6462<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.3,
        "Solution_reading_time":5.96,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":85.3072222222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use mlflow package in databricks to save the model into Azure Storage.<\/p>\n<p>The Script:<\/p>\n<p><code>abfss_path='abfss:\/\/mlops@dlsgdpeasdev03.dfs.core.windows.net'<\/code><\/p>\n<p><code>project = 'test'<\/code><\/p>\n<p><code>model_version = 'v1.0.1'<\/code><\/p>\n<p><code>model = {model training step}<\/code>  <br \/>\n<code>prefix_model_path = os.path.join(abfss_path, project, model_version)<\/code><\/p>\n<p><code>model_path = prefix_model_path<\/code><\/p>\n<p><code>print(model_path) # <\/code>abfss:\/\/mlops@dlsgdpeasdev03.dfs.core.windows.net\/test\/v1.0.1<\/p>\n<p><code>mlflow.sklearn.save_model(model, model_path)<\/code><\/p>\n<p>The message is successfully save the model. <\/p>\n<p>When I check the container and file does not exist, but I am able to load model with the same path. That mean the model file saved in databricks somewhere.<\/p>\n<p>I want to know where is the model file in databricks, and how to save the model directly from databricks notebook to Azure Storage.<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1673889987903,
        "Challenge_comment_count":1,
        "Challenge_created_time":1673582881903,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1160457\/how-databrick-save-ml-model-into-azure-storage-con",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.1,
        "Challenge_reading_time":13.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":85.3072222222,
        "Challenge_title":"How databrick save ml model into Azure Storage Container?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":109,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=ff486681-dc42-4832-92a7-87a687f2ec26\">@Benny Lau ,Shui Hong - Group Office  <\/a>, <\/p>\n<p>Thanks for the ask and welcome to Microsoft Q&amp;A . <\/p>\n<p>As I understand the ask here is to where the model is saved and how you can save to the blob . <\/p>\n<p>As per the document here : [https:\/\/learn.microsoft.com\/en-us\/azure\/databricks\/mlflow\/models#api-commands<\/p>\n<p>You have three option and I assume that your  model file is getting stored in the DBFS on the Azure databricks cluster .<\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/4d409166-ee35-4252-9294-6e7a4140ffab?platform=QnA\" alt=\"User's image\" \/><\/p>\n<p>Databricks can save a machine learning model to an Azure Storage Container using the <strong><code>dbutils.fs<\/code><\/strong> module. This module provides a set of functions for interacting with the Databricks file system (DBFS) and Azure Blob Storage. Here is an example of how to save a model to an Azure Storage Container:<\/p>\n<ol>\n<li> First, you will need to mount the Azure Storage Container to DBFS, this can be done using the <strong><code>dbutils.fs.mount<\/code><\/strong> function.<\/li>\n<\/ol>\n<pre><code>dbutils.fs.mount(\n  source='wasbs:\/\/&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.blob.core.windows.net',\n  mount_point='\/mnt\/&lt;your-mount-point&gt;',\n  extra_configs={\n    &quot;fs.azure.account.auth.type&quot;: &quot;OAuth&quot;,\n    &quot;fs.azure.account.oauth.provider.type&quot;: &quot;org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider&quot;,\n    &quot;fs.azure.account.oauth2.client.id&quot;: &quot;&lt;your-client-id&gt;&quot;,\n    &quot;fs.azure.account.oauth2.client.secret&quot;: &quot;&lt;your-client-secret&gt;&quot;,\n    &quot;fs.azure.account.oauth2.client.endpoint&quot;: &quot;https:\/\/login.microsoftonline.com\/&lt;your-tenant-id&gt;\/oauth2\/token&quot;\n  }\n)\n\n<\/code><\/pre>\n<ol>\n<li> Once the container is mounted, you can use the <strong><code>dbutils.fs.cp<\/code><\/strong> function to copy the model from the local file system to the mount point.<\/li>\n<\/ol>\n<p>dbutils.fs.cp(&quot;path\/to\/local\/model&quot;, &quot;\/mnt\/&lt;your-mount-point&gt;\/model&quot;)<\/p>\n<ol>\n<li> You can also use <strong><code>model.save()<\/code><\/strong> method to save the model in the mounted container path<\/li>\n<\/ol>\n<p>model.save(&quot;\/mnt\/&lt;your-mount-point&gt;\/model&quot;)<\/p>\n<p>Note: Be sure to replace the placeholders in the above code with the appropriate values for your use case.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.7,
        "Solution_reading_time":33.04,
        "Solution_score_count":0.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":231.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":3520.4706275,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have pretrained model artifacts stored in S3 buckets. I want to create a service that loads this model and uses it for inference.<\/p>\n\n<p>I am working in AWS ecosystem and confused between using ECS vs Sagemaker for model deployment?\nWhat are some pros\/cons for choosing one over other?<\/p>",
        "Challenge_closed_time":1578553162123,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578049726853,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59577521",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":4.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":139.8431305556,
        "Challenge_title":"AWS Sagemaker vs ECS for model hosting",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3288.0,
        "Challenge_word_count":55,
        "Platform":"Stack Overflow",
        "Poster_created_time":1390885171883,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"College Station, TX, United States",
        "Poster_reputation_count":426.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>SageMaker has a higher price mark but it is taking a lot of the heavy lifting of deploying a machine learning model, such as wiring the pieces (load balancer, gunicorn, CloudWatch, Auto-Scaling...) and it is easier to automate the processes such as A\/B testing.<\/p>\n\n<p>If you have a strong team of DevOps that have nothing more important to do, you can build a flow that will be cheaper than the SageMaker option. ECS and EKS are doing at the same time a lot of work to make it very easy for you to automate the machine learning model deployments. However, they will always be more general purpose and SageMaker with its focus on machine learning will be easier for these use cases. <\/p>\n\n<p>The usual pattern of using the cloud is to use the managed services early on as you want to move fast and you don't really know where are your future problems. Once the system is growing and you start feeling some pains here and there, you can decide to spend the time and improve that part of the system. Therefore, if you don't know the pros\/cons, start with using the simpler options. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1590723421112,
        "Solution_link_count":0.0,
        "Solution_readability":10.5,
        "Solution_reading_time":13.09,
        "Solution_score_count":7.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":196.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":32.0736591667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to use the tensorflow hub to retrain existing models, however tensorflow supports the hub library only on their 2.2 version. And The Estimator azure presents supports tf 2.0.  <\/p>\n<p>When I list tensorflow 2.2 as a required dependency as a pip package, during docker image creation the system fails - it seems like horovod is responsible, - that it cannot find the correct libraries.   <\/p>\n<p>Is this possible to be fixed? as in either an Estimator with tf 2.2 support, or an esitmator without the horovod - as I do not need a distributed system for my solution. <\/p>",
        "Challenge_closed_time":1591454811350,
        "Challenge_comment_count":3,
        "Challenge_created_time":1591339346177,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/32334\/tensorflow-2-2-0-update-for-the-tensorflow-estimat",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":8.7,
        "Challenge_reading_time":8.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":32.0736591667,
        "Challenge_title":"TensorFlow 2.2.0 update for the tensorflow estimator for Azure ML, or disable horovod?",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":111,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Following the pointers from <a>@romungi-MSFT<\/a>, defining estimator with gpubase image; &quot;mcr.microsoft.com\/azureml\/base-gpu:openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04&quot; solves the problem, and Tensorflow 2.2 can be included. Tensorflow uses GPU by default when available.<\/p>\n<pre><code> estimator = Estimator(source_directory=experiment_folder,\n                       compute_target=compute_target,\n                       script_params=script_params,\n                       entry_script='rps_efn_b0.py',\n                       node_count=1,        \n                       conda_packages=['ipykernel'],\n                       pip_packages = ['azureml-sdk',\n                                       'pyarrow',\n                                       'pyspark',\n                                       'azureml-mlflow',\n                                       'joblib',\n                                       'matplotlib',\n                                       'Pillow',\n                                       'tensorflow==2.2',\n                                       'tensorflow-datasets',\n                                       'tensorflow-hub',\n                                       'azureml-defaults',\n                                       'azureml-dataprep[fuse,pandas]'],\n                       custom_docker_image='mcr.microsoft.com\/azureml\/base-gpu:openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04')\n<\/code><\/pre>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":24.9,
        "Solution_reading_time":11.33,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":50.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":933.8588888889,
        "Challenge_answer_count":2,
        "Challenge_body":"### System Info\n\ncc @philschmid  , cc @ydshieh  , cc @sgugger \r\n\r\nHello,\r\n\r\nThis is a follow up on a related post with the below link) with the same title:\r\nhttps:\/\/github.com\/huggingface\/transformers\/issues\/16890\r\n\r\nWe ade a bit of more progress but are still facing with some issues and are trying to fix them after trying out several fixes including matching the python, transformers, and pytorch versions according to the recommendations (3.8, 4.16.2, and 1.10.2, respectively):\r\n\r\n-ValueError: not enough values to unpack (expected 2, got 1)\r\n\r\nThe error is in the \u201cmodeling_led\u201d within the transformers module expecting a different input_ids shape. \r\n\r\nNew Update is we tried below to unsqueeze input tensors to the \"modeling_led\" to solve the above error:\r\ndef unsqueeze_col(example):\r\nreturn {\"input_ids\": torch.unsqueeze(example[\"input_ids\"], 0)}\r\npubmed_train = pubmed_train.map(unsqueeze_col)\r\n\r\n\r\nIt helped moving forward in the process, but we got another error, below, a little further down in the code:\r\n\r\nUnexpectedStatusException: Error for Training job huggingface-pytorch-training-2022-06-29-04-04-58-606: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\r\nExitCode 1\r\nErrorMessage \":RuntimeError: Tensors must have same number of dimensions: got 4 and 3\r\n :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set -------------------------------------------------------------------------- Primary job  terminated normally, but 1 process returned a non-zero exit code. Per user-direction, the job has been aborted. mpirun.real detected that one or more processes exited with non-zero status, thus causing the job to be terminated. The first process to do so was:    Process name: [[41154,1],0]   Exit code:    1\"\r\nCommand \"mpirun --host algo-1:8 \r\n\r\n\r\nI\u2019d greatly appreciate your feedback. Please let me know if you need any further information about the project.\n\n### Who can help?\n\n[SageMakerAprilTraining.zip](https:\/\/github.com\/huggingface\/transformers\/files\/9065968\/SageMakerAprilTraining.zip)\r\n\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\nRunning this attached file with the training python file\n\n### Expected behavior\n\nI have shared the notebook and the error raised in it for clarification",
        "Challenge_closed_time":1660575729000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657213837000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/18060",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":14.3,
        "Challenge_reading_time":36.15,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17219.0,
        "Challenge_repo_issue_count":20692.0,
        "Challenge_repo_star_count":76135.0,
        "Challenge_repo_watch_count":860.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":933.8588888889,
        "Challenge_title":"LED Model returns AlgorithmError when using SageMaker SMP training #16890",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":355,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@omid0001 @kanwari3, \r\n\r\nWould it be possible for you to reproduce this issue (`not enough values to unpack`) without using SageMaker, i.e. just with a Python script?\r\n```bash\r\n[1,0]: bsz, seq_len = input_ids_shape[:2]\r\n[1,0]:ValueError: not enough values to unpack (expected 2, got 1)\r\n```\r\n\r\nIt would be a good idea to verify what data is received by the model first. Usually the batches in data (`input_ids`) should be already of the format `(batch_size, sequence_length)`, and if you see the above error, it is likely the data or its processing pipeline has some issues. Using `torch.unsqueeze` is not really a good idea, as it implies you have only `batch_size` being 1.\r\n\r\nMy suggestion:\r\n- Try to run your training without SageMaker (and without the using the fix `torch.unsqueeze`)\r\n- Check what is received by the model, and check in the data pipeline if it prepares the correct input format\r\n  - If you still get the issue and can't figure it out:\r\n    - I could try to help if you could provide the training script + data processing script + a tiny portion of your data    \r\n  - If the issue only occurs when you wrap the training in SageMaker, I don't have the competence to help in this case, sorry. This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.7,
        "Solution_reading_time":18.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":243.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1637398374483,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":176.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":296.2662186111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>i esttablished a function of optuna to find out best model of gbm and xgboost for my data but i was wondering if i can take the best model and apply it directly into my notebook(extracting best model as an object to reuse it later)\nhere is my objective function:<\/p>\n<pre><code>import lightgbm as lgb \nimport optuna\nimport sklearn.metrics\nfrom xgboost import XGBRegressor\nfrom optuna.integration import XGBoostPruningCallback\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nbest_booster = None\ngbm = None\ndef objective(trial,random_state=22,n_jobs=1,early_stopping_rounds=50):\n    \n    regrosser_name = trial.suggest_categorical(&quot;regressor&quot;, [&quot;XGBoost&quot;, &quot;lightgbm&quot;])\n    train_x, valid_x, train_y, valid_y = train_test_split(X_train, y_train, test_size=0.25)\n    dtrain = lgb.Dataset(train_x, label=train_y)\n    # Step 2. Setup values for the hyperparameters:\n    if regrosser_name == 'XGBoost':\n        params = {\n        &quot;verbosity&quot;: 0,  # 0 (silent) - 3 (debug)\n        &quot;objective&quot;: &quot;reg:squarederror&quot;,\n        &quot;n_estimators&quot;: 10000,\n        &quot;max_depth&quot;: trial.suggest_int(&quot;max_depth&quot;, 4, 12),\n        &quot;learning_rate&quot;: trial.suggest_loguniform(&quot;learning_rate&quot;, 0.005, 0.05),\n        &quot;colsample_bytree&quot;: trial.suggest_loguniform(&quot;colsample_bytree&quot;, 0.2, 0.6),\n        &quot;subsample&quot;: trial.suggest_loguniform(&quot;subsample&quot;, 0.4, 0.8),\n        &quot;alpha&quot;: trial.suggest_loguniform(&quot;alpha&quot;, 0.01, 10.0),\n        &quot;lambda&quot;: trial.suggest_loguniform(&quot;lambda&quot;, 1e-8, 10.0),\n        &quot;gamma&quot;: trial.suggest_loguniform(&quot;lambda&quot;, 1e-8, 10.0),\n        &quot;min_child_weight&quot;: trial.suggest_loguniform(&quot;min_child_weight&quot;, 10, 1000),\n        &quot;seed&quot;: random_state,\n        &quot;n_jobs&quot;: n_jobs,\n        }\n        model = XGBRegressor(**params)\n        model.fit(train_x, train_y)\n        y_pred = model.predict(X_val)\n        accuracy_rf = sklearn.metrics.mean_absolute_error(valid_y, y_pred)\n        return accuracy_rf\n    \n        print(rf_max_depth)\n        print(rf_n_estimators)\n        \n    else:\n        param = {\n        &quot;objective&quot;: &quot;binary&quot;,\n        &quot;metric&quot;: &quot;binary_logloss&quot;,\n        &quot;verbosity&quot;: -1,\n        &quot;boosting_type&quot;: &quot;gbdt&quot;,\n        &quot;lambda_l1&quot;: trial.suggest_float(&quot;lambda_l1&quot;, 1e-8, 10.0, log=True),\n        &quot;lambda_l2&quot;: trial.suggest_float(&quot;lambda_l2&quot;, 1e-8, 10.0, log=True),\n        &quot;num_leaves&quot;: trial.suggest_int(&quot;num_leaves&quot;, 2, 256),\n        &quot;feature_fraction&quot;: trial.suggest_float(&quot;feature_fraction&quot;, 0.4, 1.0),\n        &quot;bagging_fraction&quot;: trial.suggest_float(&quot;bagging_fraction&quot;, 0.4, 1.0),\n        &quot;bagging_freq&quot;: trial.suggest_int(&quot;bagging_freq&quot;, 1, 7),\n        &quot;min_child_samples&quot;: trial.suggest_int(&quot;min_child_samples&quot;, 5, 100),\n        }\n        gbm = lgb.train(param, dtrain)\n        preds_gbm = gbm.predict(valid_x)\n        pred_labels_gbm = np.rint(preds_gbm)\n        accuracy_gbm = sklearn.metrics.mean_absolute_error(valid_y, pred_labels_gbm)\n        return accuracy_gbm\n<\/code><\/pre>\n<p>and here is how i tried to solve this issue:<\/p>\n<pre><code>def callback(study, trial):\n    global best_booster\n    if study.best_trial == trial:\n        best_booster = gbm\nif __name__ == &quot;__main__&quot;:\n    study = optuna.create_study(direction=&quot;maximize&quot;)\n    study.optimize(objective, n_trials=100, callbacks=[callback])\n<\/code><\/pre>\n<p>i think its about importing somthing, and if there is any tips on my optuna function please state it<\/p>",
        "Challenge_closed_time":1649608540120,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648541981733,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71658687",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.8,
        "Challenge_reading_time":50.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":296.2662186111,
        "Challenge_title":"can i take best parameters and best model of optuna function and apply this model directly in my notebook?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":585.0,
        "Challenge_word_count":295,
        "Platform":"Stack Overflow",
        "Poster_created_time":1548633918320,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":79.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>If I understood your question correctly, then yes, that's what models are for.<\/p>\n<p>Like bring your saved model to your notebook, feed it data that has the same structure  as what you used to train it, and it should serve its purpose.  Or use it in a pipeline.<\/p>\n<p>Even 1 line of the same structure as an np array can be used.  For example, my model predicts whether a loan should be approved or not.<\/p>\n<p>For example, a bank customer wants a loan and submits his information.  The bank officer inputs this info in the system.  The system transforms this information into a single np array with the same structure as the dataset used to train the model.<\/p>\n<p>The model is then used by the system to predict whether the loan should be approved or not.<\/p>\n<p>I save my optuna xgb models as json, e.g.<\/p>\n<p>my_model.get_booster().save_model(f'{savepath}my_model.json')<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":10.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":146.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1601729162436,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bengaluru, Karnataka, India",
        "Answerer_reputation_count":887.0,
        "Answerer_view_count":130.0,
        "Challenge_adjusted_solved_time":97.342265,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Azure Machine Learning Service's Model Artifact has the ability to store references to the Datasets associated with the model. We can use <code>azureml.core.model.Model.add_dataset_references([('relation-as-a-string', Dataset)])<\/code> to add these dataset references.\nHow do we retrieve a Dataset from the references stored in this Model class by using a reference to the Model Class?<\/p>",
        "Challenge_closed_time":1624008862167,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623658430013,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67966905",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.2,
        "Challenge_reading_time":5.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":97.342265,
        "Challenge_title":"How to extract a dataset from azureml.core.model.Model Class?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":225.0,
        "Challenge_word_count":58,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>Consider that a Dataset was added as a reference to a Model with the name <code>'training_dataset'<\/code><\/p>\n<p>In order to get a reference to this Dataset we use:<\/p>\n<pre><code>model = Model(workspace, name)\ndataset_id = next(dictionary['id'] for dictionary in model.serialize()['datasets'] if dictionary[&quot;name&quot;] == 'training_dataset')\ndataset_reference = Dataset.get_by_id(workspace, dataset_id )\n<\/code><\/pre>\n<p>After this step we can use <code>dataset_reference<\/code> as any other AzureML Dataset Class object.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.1,
        "Solution_reading_time":6.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":57.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":8.0327202778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Using Amazon Sagemaker, I created an Xgboost model. After unpacking the resulting tar.gz file, I end up with a file \"xgboost-model\". <\/p>\n\n<p>The next step will be to upload the model directly from my S3 bucket, without downloading it using <em>pickle<\/em>. Here is what I tried:<\/p>\n\n<pre><code>obj = client.get_object(Bucket='...',Key='xgboost-model')\n\nxgb_model = pkl.load(open((obj['Body'].read())),\"rb\")\n<\/code><\/pre>\n\n<p>But it throws me the error:<\/p>\n\n<pre><code>TypeError: embedded NUL character\n<\/code><\/pre>\n\n<p>Also tried this:<\/p>\n\n<pre><code>xgb_model = pkl.loads(open((obj['Body'].read())),\"rb\")\n<\/code><\/pre>\n\n<p>the outcome was the same.<\/p>\n\n<p>Another approach:<\/p>\n\n<pre><code>bucket='...'\nkey='xgboost-model'\n\nwith s3io.open('s3:\/\/{0}\/{1}'.format(bucket, key),mode='w') as s3_file:\n  pkl.dump(mdl, s3_file)\n<\/code><\/pre>\n\n<p>This giving the error:<\/p>\n\n<pre><code>CertificateError: hostname bucket doesn't match either of '*.s3.amazonaws.com', 's3.amazonaws.com'\n<\/code><\/pre>\n\n<p>This although the bucket is the same.<\/p>\n\n<p>How Can I upload the model in a pickle object so I can then use it it for predictions?<\/p>",
        "Challenge_closed_time":1531429065220,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531399214863,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1531400147427,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51305956",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.2,
        "Challenge_reading_time":15.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":8.2917658333,
        "Challenge_title":"S3 read Sagemaker trained model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2742.0,
        "Challenge_word_count":129,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432680790120,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":455.0,
        "Poster_view_count":58.0,
        "Solution_body":"<p>My assumption is you have trained the model using Sagemaker XGBoost built-in algorithm. You would like to use that model and do the predictions in your own hosting environment (not Sagemaker hosting).<\/p>\n\n<p><code>pickle.load(file)<\/code> reads a pickled object from the open file object file and <code>pickle.loads(bytes_object)<\/code> reads a pickled object from a bytes object and returns the deserialized object. Since you have the S3 object already downloaded (into memory) as bytes, you can use <code>pickle.loads<\/code> without using <code>open<\/code><\/p>\n\n<pre><code>xgb_model = pkl.loads(obj['Body'].read())\n<\/code><\/pre>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.8,
        "Solution_reading_time":8.11,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":2.3703016666,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed a PyTorch model on AWS with SageMaker, and I try to send a request to test the service. However, I got a very vague error message saying \"no module named 'sagemaker'\". I have tried to search online, but cannot find posts about similar message.<\/p>\n\n<p>My client code: <\/p>\n\n<pre><code>import numpy as np\nfrom sagemaker.pytorch.model import PyTorchPredictor\n\nENDPOINT = '&lt;endpoint name&gt;'\n\npredictor = PyTorchPredictor(ENDPOINT)\npredictor.predict(np.random.random_sample([1, 3, 224, 224]).tobytes())\n<\/code><\/pre>\n\n<p>Detailed error message:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"client.py\", line 7, in &lt;module&gt;\n    predictor.predict(np.random.random_sample([1, 3, 224, 224]).tobytes())\n  File \"\/Users\/jiashenc\/Env\/py3\/lib\/python3.7\/site-packages\/sagemaker\/predictor.py\", line 110, in predict\n    response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n  File \"\/Users\/jiashenc\/Env\/py3\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 276, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File \"\/Users\/jiashenc\/Env\/py3\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 586, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"No module named 'sagemaker'\". See https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/&lt;endpoint name&gt; in account xxxxxxxxxxxxxx for more information.\n<\/code><\/pre>\n\n<p>This bug is because I merge both the serving script and my deploy script together, see below<\/p>\n\n<pre><code>import os\nimport torch\nimport numpy as np\nfrom sagemaker.pytorch.model import PyTorchModel\nfrom torch import cuda\nfrom torchvision.models import resnet50\n\n\ndef model_fn(model_dir):\n    device = torch.device('cuda' if cuda.is_available() else 'cpu')\n    model = resnet50()\n    with open(os.path.join(model_dir, 'model.pth'), 'rb') as f:\n        model.load_state_dict(torch.load(f, map_location=device))\n    return model.to(device)\n\ndef predict_fn(input_data, model):\n    device = torch.device('cuda' if cuda.is_available() else 'cpu')\n    model.eval()\n    with torch.no_grad():\n        return model(input_data.to(device))\n\n\nif __name__ == '__main__':\n    pytorch_model = PyTorchModel(model_data='s3:\/\/&lt;bucket name&gt;\/resnet50\/model.tar.gz',\n                                    entry_point='serve.py', role='jiashenC-sagemaker',\n                                    py_version='py3', framework_version='1.3.1')\n    predictor = pytorch_model.deploy(instance_type='ml.t2.medium', initial_instance_count=1)\n    print(predictor.predict(np.random.random_sample([1, 3, 224, 224]).astype(np.float32)))\n<\/code><\/pre>\n\n<p>The root cause is the 4th line in my code. It tries to import sagemaker, which is an unavailable library. <\/p>",
        "Challenge_closed_time":1581189783900,
        "Challenge_comment_count":3,
        "Challenge_created_time":1581115266463,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1581260842870,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60122070",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":14.3,
        "Challenge_reading_time":38.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":37,
        "Challenge_solved_time":20.6992880556,
        "Challenge_title":"AWS SageMaker PyTorch: no module named 'sagemaker'",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":5568.0,
        "Challenge_word_count":261,
        "Platform":"Stack Overflow",
        "Poster_created_time":1489634112740,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Atlanta, GA, USA",
        "Poster_reputation_count":1653.0,
        "Poster_view_count":72.0,
        "Solution_body":"<p><em>(edit 2\/9\/2020 with extra code snippets)<\/em><\/p>\n\n<p>Your serving code tries to use the <code>sagemaker<\/code> module internally. The <code>sagemaker<\/code> module (also called <a href=\"http:\/\/sagemaker.readthedocs.io\" rel=\"nofollow noreferrer\">SageMaker Python SDK<\/a>, one of the numerous orchestration SDKs for SageMaker) is not designed to be used in model containers, but instead out of models, to orchestrate their activity (train, deploy, bayesian tuning, etc). In your specific example, you shouldn't include the deployment and model call code to server code, as those are actually actions that will be conducted from outside the server to orchestrate its lifecyle and interact with it. For model deployment with the Sagemaker Pytorch container, your entry point script just needs to contain the required <code>model_fn<\/code> function for model deserialization, and optionally an <code>input_fn<\/code>, <code>predict_fn<\/code> and <code>output_fn<\/code>, respectively for pre-processing, inference and post-processing (<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_pytorch.html#the-sagemaker-pytorch-model-server\" rel=\"nofollow noreferrer\">detailed in the documentation here<\/a>). This logic is beautiful :) : you don't need anything else to deploy a production-ready deep learning server! (MMS in the case of Pytorch and MXNet, Flask+Gunicorn in the case of sklearn).<\/p>\n\n<p>In summary, this is how your code should be split:<\/p>\n\n<p>An entry_point script <code>serve.py<\/code> that contains model serving code and looks like this:<\/p>\n\n<pre><code>import os\n\nimport numpy as np\nimport torch\nfrom torch import cuda\nfrom torchvision.models import resnet50\n\ndef model_fn(model_dir):\n    # TODO instantiate a model from its artifact stored in model_dir\n    return model\n\ndef predict_fn(input_data, model):\n    # TODO apply model to the input_data, return result of interest\n    return result\n<\/code><\/pre>\n\n<p>and some orchestration code to instantiate a SageMaker Model object, deploy it to a server and query it. This is run from the orchestration runtime of your choice, which could be a SageMaker Notebook, your laptop, an AWS Lambda function, an Apache Airflow operator, etc - and with the SDK for your choice; don't need to use python for this.<\/p>\n\n<pre><code>import numpy as np\nfrom sagemaker.pytorch.model import PyTorchModel\n\npytorch_model = PyTorchModel(\n    model_data='s3:\/\/&lt;bucket name&gt;\/resnet50\/model.tar.gz',\n    entry_point='serve.py',\n    role='jiashenC-sagemaker',\n    py_version='py3',\n    framework_version='1.3.1')\n\npredictor = pytorch_model.deploy(instance_type='ml.t2.medium', initial_instance_count=1)\n\nprint(predictor.predict(np.random.random_sample([1, 3, 224, 224]).astype(np.float32)))\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1581269375956,
        "Solution_link_count":2.0,
        "Solution_readability":13.8,
        "Solution_reading_time":35.05,
        "Solution_score_count":4.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":321.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1528790837107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":610.0,
        "Answerer_view_count":203.0,
        "Challenge_adjusted_solved_time":108.1698811111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm working with data scientists who would like to gain insight and understanding of the neural network models that they train using the visual interfaces in Azure Machine Learning Studio\/Service. Is it possible to dump out and inspect the internal representation of a neural network model? Is there a way that I could write code that accesses the nodes and weights of a trained neural network in order to visualize the network as a graph structure? Or if Azure Machine Learning Studio\/Service doesn't support this I'd appreciate advice on a different machine learning framework that might be more appropriate for this kind of analysis.<\/p>\n\n<p>Things I have tried:<\/p>\n\n<ul>\n<li>Train Model outputs an ILearnerDotNet (AML Studio) or Model (AML Service). I looked for items to drag into the workspace where I could write custom code such as Execute Python Script. They seem to accept datasets, but not ILearnerDotNet\/Model as input.<\/li>\n<li>I wasn't able to locate documentation about the ILearnerDotNet\/Model interfaces.<\/li>\n<li>Selecting the Train Model output offers the option to Save as Trained Model. This creates a trained model object and that would help me reference the trained model in other places, but I didn't find a way to use this to get at its internals.<\/li>\n<\/ul>\n\n<p>I'm new to the Azure Machine Learning landscape, and could use some help with how to get started on how to access this data.<\/p>",
        "Challenge_closed_time":1559896269912,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559506858340,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56418684",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":19.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":108.1698811111,
        "Challenge_title":"Possible to access the internal representation of a neural network trained in Azure Machine Learning Service or Azure Machine Learning Studio?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":167.0,
        "Challenge_word_count":250,
        "Platform":"Stack Overflow",
        "Poster_created_time":1427643193808,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":45.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>Quote from Azure ML Exam reference:<\/p>\n\n<blockquote>\n  <p>By default, the architecture of neural networks is limited to a single\n  hidden layer with sigmoid as the activation function and softmax in\n  the last layer. You can change this in the properties of the model,\n  opening the Hidden layer specification dropdown list, and selecting a\n  Custom definition script. A text box will appear in which you will be\n  able to insert a Net# script. This script language allows you to\n  define neural networks architectures.<\/p>\n<\/blockquote>\n\n<p>For instance, if you want to create a two layer network, you may put the following code.<\/p>\n\n<pre><code>input Picture [28, 28];\nhidden H1 [200] from Picture all;\nhidden H2 [200] from H1 all;\noutput Result [10] softmax from H2 all;\n<\/code><\/pre>\n\n<p>Nevertheless, with Net# you will face certain limitations as, it does not accept regularization (neither L2 nor dropout). Also, there is no ReLU activation that are\ncommonly used in deep learning due to their benefits in backpropagation. You cannot modify the batch size of the Stochastic Gradient Descent (SGD). Besides that, you cannot use other optimization algorithms. You can use SGD with momentum, but not others like Adam, or RMSprop. You cannot define recurrent or recursive neural networks.<\/p>\n\n<p>Another great tool is CNTK (Cognitive Toolkit) that allows you defining your computational graph and create a fully customizable model.\nQuote from documentation<\/p>\n\n<blockquote>\n  <p>It is a Microsoft open source deep learning toolkit. Like other deep\n  learning tools, CNTK is based on the construction of computational\n  graphs and their optimization using automatic differentiation. The\n  toolkit is highly optimized and scales efficiently (from CPU, to GPU,\n  to multiple machines). CNTK is also very portable and flexible; you\n  can use it with programming languages like Python, C#, or C++, but you\n  can also use a model description language called BrainScript.<\/p>\n<\/blockquote>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.8,
        "Solution_reading_time":24.34,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":301.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":147.1668483334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is AWS Sage Maker Auto Pilot suitable for NLP?<\/p>\n\n<p>We currently have a tensorflow model that does classification on input of a sequence of URLS (\nWe transform the URLs to Word vec and Char vec to feed it to the model).<\/p>\n\n<p>Looking at Sage Maker Auto Pilot documentation it says that it works on input in tabular form.\nI was wondering if we could use it to for our use case.<\/p>",
        "Challenge_closed_time":1578756147747,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578226347093,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59599721",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":5.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":147.1668483334,
        "Challenge_title":"Is AWS Sage Maker Auto Pilot suitable for NLP?",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":261.0,
        "Challenge_word_count":79,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458550179920,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":383.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>No. SageMaker AutoPilot doesn't support deep learning at the moment, only classification and regression problems on tabular data. Technically, I guess you could pass embeddings in CSV format, and pray that XGBoost figures them out, but I seriously doubt that this would deliver meaningful results :)<\/p>\n\n<p>Amazon Comprehend does support fully managed custom classification models <a href=\"https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/how-document-classification.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/how-document-classification.html<\/a>. It may be worth taking a look at it.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":19.4,
        "Solution_reading_time":8.26,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":68.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1550401247147,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":71.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":385.9981336111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am currently working on setting up a pipeline in Amazon Sagemaker. For that I set up an xgboost-estimator and trained it on my dataset. The training job runs as expected and the freshly trained model is saved to the specified output bucket. Later I want to reimport the model, which is done by getting the mode.tar.gz from the output bucket, extracting the model and serializing the binary via pickle.<\/p>\n<pre><code># download the model artifact from AWS S3\n!aws s3 cp s3:\/\/my-bucket\/output\/sagemaker-xgboost-2021-09-06-12-19-41-306\/output\/model.tar.gz .\n\n# opens the downloaded model artifcat and loads it as 'model' variable\nmodel_path = &quot;model.tar.gz&quot;\nwith tarfile.open(model_path) as tar:\n    tar.extractall(path=&quot;.&quot;)\n\nmodel = pkl.load(open(&quot;xgboost-model&quot;, &quot;rb&quot;))\n<\/code><\/pre>\n<p>Whenever I try to tun this I receive an unpickling stack underflow:<\/p>\n<pre><code>---------------------------------------------------------------------------\nUnpicklingError                           Traceback (most recent call last)\n&lt;ipython-input-9-b88a7424f790&gt; in &lt;module&gt;\n     10     tar.extractall(path=&quot;.&quot;)\n     11 \n---&gt; 12 model = pkl.load(open(&quot;xgboost-model&quot;, &quot;rb&quot;))\n     13 \n\nUnpicklingError: unpickling stack underflow\n<\/code><\/pre>\n<p>So far I retrained the model to see, if the error occurs with a different model file and it does. I also downloaded the model.tar.gz and validated it via gunzip. When extracting the binary file xgboost-model is extracted correctly, I just can't pickle it. Every occurence of the error I found on stackoverflow points at a damaged file, but this one is generated directly by SageMaker and I do note perform any transformation on it, but extracting it from the model.tar.gz. Reloading a model like this seems to be quite a common use case, referring to the documentation and different tutorials.\nLocally I receive the same error with the downloaded file. I tried to step directly into pickle for debugging it but couldn't make much sense of it. The complete error stack looks like this:<\/p>\n<pre><code>Exception has occurred: UnpicklingError       (note: full exception trace is shown but execution is paused at: _run_module_as_main)\nunpickling stack underflow\n  File &quot;\/sagemaker_model.py&quot;, line 10, in &lt;module&gt;\n    model = pkl.load(open('xgboost-model', 'rb'))\n  File &quot;\/usr\/local\/Cellar\/python@3.9\/3.9.1_5\/Frameworks\/Python.framework\/Versions\/3.9\/lib\/python3.9\/runpy.py&quot;, line 87, in _run_code\n    exec(code, run_globals)\n  File &quot;\/usr\/local\/Cellar\/python@3.9\/3.9.1_5\/Frameworks\/Python.framework\/Versions\/3.9\/lib\/python3.9\/runpy.py&quot;, line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File &quot;\/usr\/local\/Cellar\/python@3.9\/3.9.1_5\/Frameworks\/Python.framework\/Versions\/3.9\/lib\/python3.9\/runpy.py&quot;, line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File &quot;\/usr\/local\/Cellar\/python@3.9\/3.9.1_5\/Frameworks\/Python.framework\/Versions\/3.9\/lib\/python3.9\/runpy.py&quot;, line 87, in _run_code\n    exec(code, run_globals)\n  File &quot;\/usr\/local\/Cellar\/python@3.9\/3.9.1_5\/Frameworks\/Python.framework\/Versions\/3.9\/lib\/python3.9\/runpy.py&quot;, line 197, in _run_module_as_main (Current frame)\n    return _run_code(code, main_globals, None,\n<\/code><\/pre>\n<p>What could cause this issue and at which step during the process could I apply changes to fix or workaround the problem.<\/p>",
        "Challenge_closed_time":1632479560168,
        "Challenge_comment_count":4,
        "Challenge_created_time":1631089966887,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69099627",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":11.9,
        "Challenge_reading_time":45.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":385.9981336111,
        "Challenge_title":"what causes an unpickling stack underflow when trying to serialize a succesfully generated SageMaker model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1747.0,
        "Challenge_word_count":387,
        "Platform":"Stack Overflow",
        "Poster_created_time":1550401247147,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":71.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>The issue rooted in the model version used for the xgboost framework. from 1.3.0 on the default output changed from pickle to json and the sagemaker documentation does not seem to have been updated accordingly. So if you want to read the model via<\/p>\n<pre><code>    tar.extractall(path=&quot;.&quot;)\n\nmodel = pkl.load(open(&quot;xgboost-model&quot;, &quot;rb&quot;))\n<\/code><\/pre>\n<p>as described in the sagemaker docs, you need to import the XGBOOST framework with with a former version, e.g. 1.2.1.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":6.39,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":27.0735711111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have trained a semantic segmentation model using the sagemaker and the out has been saved to a s3 bucket. I want to load this model from the s3 to predict some images in sagemaker. <\/p>\n\n<p>I know how to predict if I leave the notebook instance running after the training as its just an easy deploy but doesn't really help if I want to use an older model.<\/p>\n\n<p>I have looked at these sources and been able to come up with something myself but it doesn't work hence me being here:<\/p>\n\n<p><a href=\"https:\/\/course.fast.ai\/deployment_amzn_sagemaker.html#deploy-to-sagemaker\" rel=\"noreferrer\">https:\/\/course.fast.ai\/deployment_amzn_sagemaker.html#deploy-to-sagemaker<\/a>\n<a href=\"https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"noreferrer\">https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/<\/a><\/p>\n\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/pipeline.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/pipeline.html<\/a><\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb<\/a><\/p>\n\n<p>My code is this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.pipeline import PipelineModel\nfrom sagemaker.model import Model\n\ns3_model_bucket = 'bucket'\ns3_model_key_prefix = 'prefix'\ndata = 's3:\/\/{}\/{}\/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')\nmodels = ss_model.create_model() # ss_model is my sagemaker.estimator\n\nmodel = PipelineModel(name=data, role=role, models= [models])\nss_predictor = model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')\n<\/code><\/pre>",
        "Challenge_closed_time":1558621559712,
        "Challenge_comment_count":0,
        "Challenge_created_time":1558522248223,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1558524094856,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56255154",
        "Challenge_link_count":8,
        "Challenge_participation_count":2,
        "Challenge_readability":22.6,
        "Challenge_reading_time":27.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":27.5865247222,
        "Challenge_title":"How to use a pretrained model from s3 to predict some data?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":7404.0,
        "Challenge_word_count":152,
        "Platform":"Stack Overflow",
        "Poster_created_time":1558518328152,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":79.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>You can actually instantiate a Python SDK <code>model<\/code> object from existing artifacts, and deploy it to an endpoint. This allows you to deploy a model from trained artifacts, without having to retrain in the notebook. For example, for the semantic segmentation model:<\/p>\n\n<pre><code>trainedmodel = sagemaker.model.Model(\n    model_data='s3:\/\/...model path here..\/model.tar.gz',\n    image='685385470294.dkr.ecr.eu-west-1.amazonaws.com\/semantic-segmentation:latest',  # example path for the semantic segmentation in eu-west-1\n    role=role)  # your role here; could be different name\n\ntrainedmodel.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')\n<\/code><\/pre>\n\n<p>And similarly, you can instantiate a predictor object on a deployed endpoint from any authenticated client supporting the SDK, with the following command:<\/p>\n\n<pre><code>predictor = sagemaker.predictor.RealTimePredictor(\n    endpoint='endpoint name here',\n    content_type='image\/jpeg',\n    accept='image\/png')\n<\/code><\/pre>\n\n<p>More on those abstractions:<\/p>\n\n<ul>\n<li><code>Model<\/code>: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html<\/a><\/li>\n<li><code>Predictor<\/code>:\n<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":22.9,
        "Solution_reading_time":18.55,
        "Solution_score_count":13.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":112.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":291.4263888889,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\n`TypeError: unsupported operand type(s) for \/: 'str' and 'str'` occurs when `MlflowArtifactDataSet` is used with `MlflowModelSaverDataSet`.\r\n\r\n## Context\r\n\r\nLogging locally and to MLflow in one step.\r\n\r\n## Steps to Reproduce\r\n\r\n```yaml\r\nsklearn_model:\r\n    type: kedro_mlflow.io.artifacts.MlflowArtifactDataSet\r\n    data_set:\r\n        type: kedro_mlflow.io.models.MlflowModelSaverDataSet\r\n        flavor: mlflow.sklearn\r\n        filepath: data\/06_models\/sklearn_model\r\n        versioned: true\r\n```\r\n\r\n## Expected Result\r\n\r\nThe model should be saved locally and in MLflow run at the same time.\r\n\r\n## Actual Result\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 240, in save\r\n    self._save(data)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/kedro-mlflow\/kedro_mlflow\/io\/artifacts\/mlflow_artifact_dataset.py\", line 40, in _save\r\n    if hasattr(self, \"_version\")\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 605, in _get_save_path\r\n    versioned_path = self._get_versioned_path(save_version)  # type: ignore\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 616, in _get_versioned_path\r\n    return self._filepath \/ version \/ self._filepath.name\r\nTypeError: unsupported operand type(s) for \/: 'str' and 'str'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/bin\/kedro\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/cli\/cli.py\", line 725, in main\r\n    cli_collection()\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 829, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 782, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 1259, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 1066, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 610, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/kedro_cli.py\", line 230, in run\r\n    pipeline_name=pipeline,\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/context\/context.py\", line 767, in run\r\n    raise exc\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/context\/context.py\", line 759, in run\r\n    run_result = runner.run(filtered_pipeline, catalog, run_id)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/runner.py\", line 101, in run\r\n    self._run(pipeline, catalog, run_id)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/sequential_runner.py\", line 90, in _run\r\n    run_node(node, catalog, self._is_async, run_id)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/runner.py\", line 213, in run_node\r\n    node = _run_node_sequential(node, catalog, run_id)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/runner.py\", line 249, in _run_node_sequential\r\n    catalog.save(name, data)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/data_catalog.py\", line 448, in save\r\n    func(data)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 625, in save\r\n    super().save(data)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 247, in save\r\n    raise DataSetError(message) from exc\r\nkedro.io.core.DataSetError: Failed while saving data to data set MlflowMlflowModelSaverDataSet(filepath=\/Users\/olszewk2\/dev\/pyzypad-example\/data\/06_models\/pclass_encoder, flavor=mlflow.sklearn, load_args={}, save_args={}, version=Version(load=None, save='2020-11-06T12.28.57.593Z')).\r\nunsupported operand type(s) for \/: 'str' and 'str'\r\n```\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* kedro 0.16.6\r\n* kedro-mlflow 0.4.0\r\n* Python 3.7.7\r\n* MacOS Catalina\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes.",
        "Challenge_closed_time":1605715301000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604666166000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/116",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":16.6,
        "Challenge_reading_time":65.61,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":48,
        "Challenge_solved_time":291.4263888889,
        "Challenge_title":"TypeError: unsupported operand type(s) for \/: 'str' and 'str' when using MlflowArtifactDataSet with MlflowModelSaverDataSet",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":337,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1520413126203,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":37123.0,
        "Answerer_view_count":4058.0,
        "Challenge_adjusted_solved_time":47.4339141667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I trained a model in AZURE ML. Now i want to use that model in my ios app to predict the output\u00a0.<\/p>\n\n<p>How to download the model from AZURE and use it my swift code.<\/p>",
        "Challenge_closed_time":1525849618928,
        "Challenge_comment_count":0,
        "Challenge_created_time":1525678856837,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1558224843256,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50209284",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":3.0,
        "Challenge_reading_time":2.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":47.4339141667,
        "Challenge_title":"How to use the trained model developed in AZURE ML",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":516.0,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Poster_created_time":1510206999776,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":81.0,
        "Poster_view_count":41.0,
        "Solution_body":"<p>As far as I know, the model could run in <strong>Azure Machine Learning Studio<\/strong>.It seems that you are unable to download it, the model could do nothing outside of Azure ML. <\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/41236871\/how-to-download-the-trained-models-from-azure-machine-studio\">Here<\/a> is a similar post for you to refer, I have also tried @Ahmet's \nmethod, but result is like @mrjrdnthms says.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1525850503192,
        "Solution_link_count":1.0,
        "Solution_readability":8.6,
        "Solution_reading_time":5.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1572457215103,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"United Kingdom",
        "Answerer_reputation_count":42593.0,
        "Answerer_view_count":5761.0,
        "Challenge_adjusted_solved_time":0.3024208333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In python I would like to check that a given function is called within a <code>with<\/code> statement of a given type<\/p>\n<pre><code>class Bar:\n def __init__(self, x):\n  self.x = x\n def __enter__(self):\n  return self\n def __exit__(self, *a, **k):\n  pass\n\ndef foo(x):\n # assert that the enclosing context is an instance of bar\n # assert isinstance('enclosed context', Bar)\n print(x*2)\n\nwith Bar(1) as bar:\n foo(bar.x)\n<\/code><\/pre>\n<p>I could do something like enforcing an arg passed into <code>foo<\/code> and wrapping functions in a decorator i.e.<\/p>\n<pre><code>class Bar:\n def __init__(self, x):\n  self.x = x\n def __enter__(self):\n  return self\n def __exit__(self, *a, **k):\n  pass\n\ndef assert_bar(func):\n def inner(bar, *a, **k):\n  assert isinstance(bar, Bar)\n  return func(*a, **k)\n return inner\n\n\n@assert_bar\ndef foo(x):\n print(x*2)\n\nwith Bar(1) as bar:\n foo(bar, bar.x)\n\n<\/code><\/pre>\n<p>but then I would have to pass around <code>bar<\/code> everywhere.<\/p>\n<p>As a result I'm trying to see if there's a way to access the <code>with<\/code> context<\/p>\n<p>Note: The real world application of this is ensuring that <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.pyfunc.html#mlflow.pyfunc.log_model\" rel=\"nofollow noreferrer\"><code>mlflow.pyfunc.log_model<\/code><\/a> is called within an <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.ActiveRun\" rel=\"nofollow noreferrer\"><code>mlflow.ActiveRun<\/code><\/a> context, or it leaves an <code>ActiveRun<\/code> open, causing problems later on<\/p>",
        "Challenge_closed_time":1644974017812,
        "Challenge_comment_count":6,
        "Challenge_created_time":1644973389773,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71135260",
        "Challenge_link_count":2,
        "Challenge_participation_count":7,
        "Challenge_readability":10.6,
        "Challenge_reading_time":20.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.1744552778,
        "Challenge_title":"Python assert a function is called within a certain `with` statement's context",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":77.0,
        "Challenge_word_count":189,
        "Platform":"Stack Overflow",
        "Poster_created_time":1473193743196,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, United Kingdom",
        "Poster_reputation_count":617.0,
        "Poster_view_count":75.0,
        "Solution_body":"<p>Here's an ugly way to do it: global state.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class Bar:\n    active = 0\n    def __init__(self, x):\n        self.x = x\n    def __enter__(self):\n        Bar.active += 1\n        return self\n    def __exit__(self, *a, **k):\n        Bar.active -= 1\n\nfrom functools import wraps\n\ndef assert_bar(func):\n    @wraps(func)\n    def wrapped(*vargs, **kwargs):\n        if Bar.active &lt;= 0:\n            # raises even if asserts are disabled\n            raise AssertionError()\n        return func(*vargs, **kwargs)\n    return wrapped\n<\/code><\/pre>\n<p>Unfortunately I don't think there is any non-ugly way to do it. If you aren't going to pass around a <code>Bar<\/code> instance yourself then you must rely on some state existing somewhere else to tell you that a <code>Bar<\/code> instance exists and is currently being used as a context manager.<\/p>\n<p>The only way you can avoid that global state is to store the state in the instance, which means the decorator needs to be an instance method and the instance needs to exist before the function is declared:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from functools import wraps\n\nclass Bar:\n    def __init__(self, x):\n        self.x = x\n        self.active = 0\n    def __enter__(self):\n        self.active += 1\n        return self\n    def __exit__(self, *a, **k):\n        self.active -= 1\n    def assert_this(self, func):\n        @wraps(func)\n        def wrapped(*vargs, **kwargs):\n            if self.active &lt;= 0:\n                raise AssertionError()\n            return func(*vargs, **kwargs)\n        return wrapped\n\nbar = Bar(1)\n\n@bar.assert_this\ndef foo(x):\n    print(x + 1)\n\nwith bar:\n    foo(1)\n<\/code><\/pre>\n<p>This is still &quot;global state&quot; in the sense that the function <code>foo<\/code> now holds a reference to the <code>Bar<\/code> instance that holds the state. But it may be more palatable if <code>foo<\/code> is only ever going to be a local function.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1644974478488,
        "Solution_link_count":0.0,
        "Solution_readability":8.5,
        "Solution_reading_time":21.9,
        "Solution_score_count":2.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":245.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1546969667040,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":1689.0,
        "Answerer_view_count":170.0,
        "Challenge_adjusted_solved_time":0.4279386111,
        "Challenge_answer_count":1,
        "Challenge_body":"<h1>Difficulty in understanding<\/h1>\n<p>Q2) How to download a file from S3?<\/p>\n<p><strong>From<\/strong>  <a href=\"https:\/\/medium.com\/akeneo-labs\/machine-learning-workflow-with-sagemaker-b83b293337ff\" rel=\"nofollow noreferrer\">The Machine Learning Workflow with SageMaker<\/a><\/p>\n<p>And also why are we using this piece of code?<\/p>\n<p><code>estimator.fit(train_data_location)<\/code><\/p>",
        "Challenge_closed_time":1568928522076,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568926981497,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58018893",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":13.7,
        "Challenge_reading_time":6.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.4279386111,
        "Challenge_title":"How do S3 file download and estimator.fit() work in this blog post?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":220.0,
        "Challenge_word_count":43,
        "Platform":"Stack Overflow",
        "Poster_created_time":1556451987416,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":1309.0,
        "Poster_view_count":288.0,
        "Solution_body":"<h2>Downloading a file from S3:<\/h2>\n\n<p>This code block in the Q2 section defines the function that downloads a file from S3. The user instantiates an S3 client, and then passes the S3 URL along to the <code>s3.Bucket.download_file()<\/code> method.<\/p>\n\n<pre><code>def download_from_s3(url):\n    \"\"\"ex: url = s3:\/\/sagemakerbucketname\/data\/validation.tfrecords\"\"\"\n    url_parts = url.split(\"\/\")  # =&gt; ['s3:', '', 'sagemakerbucketname', 'data', ...\n    bucket_name = url_parts[2]\n    key = os.path.join(*url_parts[3:])\n    filename = url_parts[-1]\n    if not os.path.exists(filename):\n        try:\n            # Create an S3 client\n            s3 = boto3.resource('s3')\n            print('Downloading {} to {}'.format(url, filename))\n            s3.Bucket(bucket_name).download_file(key, filename)\n        except botocore.exceptions.ClientError as e:\n            if e.response['Error']['Code'] == \"404\":\n                print('The object {} does not exist in bucket {}'.format(\n                    key, bucket_name))\n            else:\n                raise\n<\/code><\/pre>\n\n<h2>Estimator.fit() explanation:<\/h2>\n\n<p>The <code>estimator.fit(train_data_location)<\/code> line is what initiates the training process with SageMaker. When run, SageMaker will provision the necessary infrastructure, fetch the data from the location the user designated (here, <code>train_data_location<\/code> which is a path to Amazon S3) and distribute it amongst the training cluster, carry out the training process, return the resulting model, and tear down the training infrastructure. <\/p>\n\n<p>You can find the result of this training job in the SageMaker console.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.4,
        "Solution_reading_time":19.13,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":166.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1383535151876,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":502.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":70.0674963889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a mlflow question regarding to serve R models, below is my code to train and serialize the R model.<\/p>\n<pre><code>library(mlflow)\nlibrary(caret)\nlibrary(carrier)\n\nset.seed(101)\nsample &lt;- createDataPartition(iris$Species, p=0.80, list=FALSE)\niris_train &lt;- iris[sample,]\niris_test &lt;- iris[-sample,]\n\n\nwith(mlflow_start_run()  , {\n  control &lt;- trainControl(method='cv', number=10)\n  metric &lt;- 'Accuracy'\n\n  # Linear Discriminant Analysis (LDA)\n  model &lt;- train(Species~., data=iris_train, method='lda', trControl=control, metric=metric,\n                preProcess=c(&quot;center&quot;, &quot;scale&quot;))\n  fn &lt;- crate(~ caret::predict.train(model, .x), model = model)\n  # fn &lt;- crate(~ stats::predict(model, .x), model = model)\n  iris_prediction &lt;- predict(model, iris_test)\n\n  mlflow_log_model(model = fn, artifact_path=&quot;model&quot;)\n})\n\n<\/code><\/pre>\n<p>However, when I serve it with the API request,<\/p>\n<pre><code>curl --location --request POST 'localhost:5000\/invocations' \\\n--header 'Content-Type: application\/json' \\\n--data-raw '{&quot;columns&quot;: [&quot;Sepal.Length&quot;, &quot;Sepal.With&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;], &quot;data&quot;: [[5.2, 4.1, 1.5, 0.1], [6.4, 2.8, 5.6, 2.1], [7.9, 3.8, 6.4, 2.0], [6.7, 3.1, 5.6, 2.4], [6.3, 3.4, 5.6, 2.4]]}'\n<\/code><\/pre>\n<p>I have the following error:\n<code>Invalid Request.  object 'Sepal.Width' not found<\/code><\/p>\n<p>Note, if I change the training to use linear regression model,<\/p>\n<pre><code>with(mlflow_start_run(), {\n  model &lt;- lm(Sepal.Width ~ Sepal.Length, iris)\n  mlflow_log_model(\n    crate(~ stats::predict(model, .x), model=model), &quot;model&quot;)\n})\n<\/code><\/pre>\n<p>The same API request, everything works fine.<\/p>\n<p>Can anyone help me find any clue? I am not that familiar with R.<\/p>\n<p>Thanks a lot!<\/p>",
        "Challenge_closed_time":1641228071910,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640975828923,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70544873",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":24.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":70.0674963889,
        "Challenge_title":"mlflow serving r models failed if use LDA instead of linear regression",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":57.0,
        "Challenge_word_count":194,
        "Platform":"Stack Overflow",
        "Poster_created_time":1383535151876,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":502.0,
        "Poster_view_count":40.0,
        "Solution_body":"<p>it turns out the issue is a typo in curl script. it's Sepal.Width not Sepal.With.<\/p>\n<pre><code>curl --location --request POST 'localhost:5000\/invocations' \\\n--header 'Content-Type: application\/json' \\\n--data-raw '{&quot;columns&quot;: [&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;], &quot;data&quot;: [[5.2, 4.1, 1.5, 0.1], [6.4, 2.8, 5.6, 2.1], [7.9, 3.8, 6.4, 2.0], [6.7, 3.1, 5.6, 2.4], [6.3, 3.4, 5.6, 2.4]]}'\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.6,
        "Solution_reading_time":6.4,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":51.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1402755934688,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Porto, Portugal",
        "Answerer_reputation_count":5998.0,
        "Answerer_view_count":426.0,
        "Challenge_adjusted_solved_time":5.7713575,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm deploying a <code>tensorflow.serving<\/code> endpoint with a custom <code>inference.py<\/code> script via the <code>entry point<\/code> parameter<\/p>\n<pre><code>model = Model(role='xxx',\n              framework_version='2.2.0',\n              entry_point='inference.py',\n              model_data='xxx')\n\npredictor = model.deploy(instance_type='xxx',\n                         initial_instance_count=1,\n                         endpoint_name='xxx')\n<\/code><\/pre>\n<p>inference.py constains an <code>input_handler<\/code> and an <code>output_handler<\/code> functions, but when i call predict with:<\/p>\n<pre><code>model = Predictor(endpoint_name='xxx')\nurl = 'xxx'\n\ninput = {\n    'instances': [url]\n}\n\npredictions = model.predict(input)\n<\/code><\/pre>\n<p>I'm getting the following <code>error<\/code>:<\/p>\n<p><em>botocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message &quot;{&quot;error&quot;: &quot;Failed to process element: 0 of 'instances' list. Error: Invalid argument: JSON Value: &quot;xxx&quot; Type: String is not of expected type: float&quot; }&quot;<\/em><\/p>\n<p>It seems the function is never calling the <code>input_handler<\/code> function in inference.py script. Do you know why this might be happening?<\/p>",
        "Challenge_closed_time":1595952661830,
        "Challenge_comment_count":2,
        "Challenge_created_time":1595851293460,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1595931884943,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63114905",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":13.8,
        "Challenge_reading_time":17.35,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":28.1578805556,
        "Challenge_title":"Sagemaker tensorflow endpoint not calling the input_handler when being invoked for a prediction",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":474.0,
        "Challenge_word_count":129,
        "Platform":"Stack Overflow",
        "Poster_created_time":1402755934688,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Porto, Portugal",
        "Poster_reputation_count":5998.0,
        "Poster_view_count":426.0,
        "Solution_body":"<p>Found the problem thanks to AWS support:<\/p>\n<p>I was creating an endpoint that already had an endpoint configuration with the same name and the new configuration wasn't being utilized.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.6,
        "Solution_reading_time":2.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":29.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1440024011336,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":335.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":4.1729647222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy(local) using this line:<\/p>\n\n<pre><code>local_service = Model.deploy(ws, \"test\", [model], inference_config, deployment_config)\n<\/code><\/pre>\n\n<p>Then I get this output in the terminal:<\/p>\n\n<pre><code>tarfile.ReadError: file could not be opened successfully\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/WXbPe.png\" rel=\"nofollow noreferrer\">Screenshot of the output<\/a><\/p>",
        "Challenge_closed_time":1570725963836,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570710941163,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58323029",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":6.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":4.1729647222,
        "Challenge_title":"Azure ML deploy locally: tarfile.ReadError: file could not be opened successfully",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":77.0,
        "Challenge_word_count":48,
        "Platform":"Stack Overflow",
        "Poster_created_time":1570704481987,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>There was a bug with the retry logic when files were being uploaded. That bug has since been fixed, so updating your SDK should fix the issue.<\/p>\n\n<p>Similar post: <a href=\"https:\/\/stackoverflow.com\/questions\/57854136\/registering-and-downloading-a-fasttext-bin-model-fails-with-azure-machine-learn\">Registering and downloading a fastText .bin model fails with Azure Machine Learning Service<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.2,
        "Solution_reading_time":5.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":4.3897661111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained and deployed a model in Pytorch with Sagemaker. I am able to call the endpoint and get a prediction. I am using the default input_fn() function (i.e. not defined in my serve.py).<\/p>\n\n<pre><code>model = PyTorchModel(model_data=trained_model_location,\n                     role=role,\n                     framework_version='1.0.0',\n                     entry_point='serve.py',\n                     source_dir='source')\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>A prediction can be made as follows:<\/p>\n\n<pre><code>input =\"0.12787057,  1.0612601,  -1.1081504\"\npredictor.predict(np.genfromtxt(StringIO(input), delimiter=\",\").reshape(1,3) )\n<\/code><\/pre>\n\n<p>I want to be able to serve the model with REST API and am HTTP POST using lambda and API gateway. I was able to use invoke_endpoint() for this with an XGBOOST model in Sagemaker this way. I am not sure what to send into the body for Pytorch.<\/p>\n\n<pre><code>client = boto3.client('sagemaker-runtime')\nresponse = client.invoke_endpoint(EndpointName=ENDPOINT  ,\nContentType='text\/csv',\nBody=???)\n<\/code><\/pre>\n\n<p>I believe I need to understand how to write the customer input_fn to accept and process the type of data I am able to send through invoke_client. Am I on the right track and if so, how could the input_fn be written to accept a csv from invoke_endpoint?<\/p>",
        "Challenge_closed_time":1559781766688,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559765963530,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56467434",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.5,
        "Challenge_reading_time":17.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":4.3897661111,
        "Challenge_title":"Making a Prediction Sagemaker Pytorch",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2346.0,
        "Challenge_word_count":167,
        "Platform":"Stack Overflow",
        "Poster_created_time":1294628108596,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1748.0,
        "Poster_view_count":393.0,
        "Solution_body":"<p>Yes you are on the right track. You can send csv-serialized input to the endpoint without using the <code>predictor<\/code> from the SageMaker SDK, and using other SDKs such as <code>boto3<\/code> which is installed in lambda:<\/p>\n\n<pre><code>import boto3\nruntime = boto3.client('sagemaker-runtime')\n\npayload = '0.12787057,  1.0612601,  -1.1081504'\n\nresponse = runtime.invoke_endpoint(\n    EndpointName=ENDPOINT_NAME,\n    ContentType='text\/csv',\n    Body=payload.encode('utf-8'))\n\nresult = json.loads(response['Body'].read().decode()) \n<\/code><\/pre>\n\n<p>This will pass to the endpoint a csv-formatted input, that you may need to reshape back in the <code>input_fn<\/code> to put in the appropriate dimension expected by the model.<\/p>\n\n<p>for example:<\/p>\n\n<pre><code>def input_fn(request_body, request_content_type):\n    if request_content_type == 'text\/csv':\n        return torch.from_numpy(\n            np.genfromtxt(StringIO(request_body), delimiter=',').reshape(1,3))\n<\/code><\/pre>\n\n<p><strong>Note<\/strong>: I wasn't able to test the specific <code>input_fn<\/code> above with your input content and shape but I used the approach on Sklearn RandomForest couple times, and looking at the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_pytorch.html#model-serving\" rel=\"nofollow noreferrer\">Pytorch SageMaker serving doc<\/a> the above rationale should work.<\/p>\n\n<p>Don't hesitate to use endpoint logs in Cloudwatch to diagnose any inference error (available from the endpoint UI in the console), those logs are usually <strong>much more verbose<\/strong> that the high-level logs returned by the inference SDKs<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.9,
        "Solution_reading_time":20.61,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":172.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":22.7411330556,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I have a notebook based on <a href=\"http:\/\/wandb.me\/lit-colab\" rel=\"noopener nofollow ugc\"> Supercharge your Training with PyTorch Lightning + Weights &amp; Biases<\/a> and I\u2019m wondering what the easiest approach to load a model with the best checkpoint after training finishes.<\/p>\n<p>I\u2019m assuming that after training the \u201cmodel\u201d instance will just have the weights of the most recent epoch, which might not be the most accurate model (in case it started overfitting etc).<\/p>\n<p>Specifically I was looking for an easy way to get the directory where the checkpoints artifacts are stored, which in my case look like this: <code>.\/MnistKaggle\/1vzsgin6\/checkpoints<\/code>, where <code>1vzsgin6<\/code> is the run id auto-generated by wandb.<\/p>\n<p>One (clunky) way to do it would be:<\/p>\n<pre><code class=\"lang-auto\">wandb_logger = WandbLogger(project=\"MnistKaggle\")\ncheckpoint_dir_path = None\n\ndef my_after_save_checkpoint(checkpoint):\n  checkpoint_dir_path = checkpoint.dirpath\n\nwandb_logger.after_save_checkpoint = my_after_save_checkpoint\n\n# Now find the checkpoint file in the checkpoint_dir_path directory and load the model from that.\n<\/code><\/pre>\n<p>Is there an easier way?  I was sorta expecting the <code>WandbLogger<\/code> object to have an easy method like <code>get_save_checkpoint_dirpath()<\/code>, but I\u2019m not seeing anything.<\/p>\n<p>Thanks in advance for any help!<\/p>",
        "Challenge_closed_time":1667430011968,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667348143889,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/easiest-way-to-load-the-best-model-checkpoint-after-training-w-pytorch-lightning\/3365",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":10.9,
        "Challenge_reading_time":18.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":22.7411330556,
        "Challenge_title":"Easiest way to load the best model checkpoint after training w\/ pytorch lightning",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1385.0,
        "Challenge_word_count":182,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/tleyden\">@tleyden<\/a> , happy to help. Please review the following <a href=\"https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/extensions\/generated\/pytorch_lightning.loggers.WandbLogger.html#:~:text=(model)-,Log%20model%20checkpoints,-Log%20model%20checkpoints\" rel=\"noopener nofollow ugc\">resource<\/a> on model checkpointing and retrieval.<\/p>\n<p>A common flow would be to log a model checkpoint as in the example then to also log a \u201cbest model\u201d artifact. Since artifacts are versioned you don\u2019t have to worry about renaming the new \u201cbest model\u201d artifact. Then at the end of your run you not only have an artifact history of your model at each of the checkpoints but also a versioned history of all the best models.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.9,
        "Solution_reading_time":9.74,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":91.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1446631384107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Helsinki, Finland",
        "Answerer_reputation_count":4255.0,
        "Answerer_view_count":877.0,
        "Challenge_adjusted_solved_time":0.5821208334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am running the <code>pipeline.submit()<\/code> in AzureML, which has a <code>PythonScriptStep<\/code>.\nInside this step, I download a model from tensorflow-hub, retrain it and save it as a <code>.zip<\/code>, and finally, I would like to register it in the Azure ML.\nBut as inside the script I do not have a workspace, <code>Model.register()<\/code> is not the case.\nSo I am trying to use <code>Run.register_model()<\/code> method as below:<\/p>\n\n<pre><code>os.replace(os.path.join('.', archive_name + '.zip'), \n           os.path.join('.', 'outputs', archive_name + '.zip'))\n\nprint(os.listdir('.\/outputs'))\nprint('========================')\n\nrun_context = Run.get_context()\nfinetuning_model = run_context.register_model(model_name='finetuning_similarity_model',\n                                              model_path=os.path.join(archive_name+'.zip'),\n                                              tags={},\n                                              description=\"Finetuning Similarity model\")\n<\/code><\/pre>\n\n<p>But then I have got an error:<\/p>\n\n<blockquote>\n  <p>ErrorResponse \n  {\n      \"error\": {\n          \"message\": \"Could not locate the provided model_path retrained.zip in the set of files uploaded to the run:<\/p>\n<\/blockquote>\n\n<p>despite I have the retrained <code>.zip<\/code> in the <code>.\/outputs<\/code> dir as we can see from the log:<\/p>\n\n<pre><code>['retrained.zip']\n========================\n<\/code><\/pre>\n\n<p>I guess that I am doing something wrong?<\/p>",
        "Challenge_closed_time":1578745983587,
        "Challenge_comment_count":0,
        "Challenge_created_time":1574164584153,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1578744224352,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58933565",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":17.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":1272.6109538889,
        "Challenge_title":"How to register model from the Azure ML Pipeline Script step",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3429.0,
        "Challenge_word_count":150,
        "Platform":"Stack Overflow",
        "Poster_created_time":1574162655727,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":75.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>I was able to fix the same issue (<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.exceptions.modelpathnotfoundexception?view=azure-ml-py\" rel=\"noreferrer\"><code>ModelPathNotFoundException<\/code><\/a>) by explicitly uploading the model into the run history record before trying to register the model:<\/p>\n\n<pre><code>run.upload_file(\"outputs\/my_model.pickle\", \"outputs\/my_model.pickle\")\n<\/code><\/pre>\n\n<p>Which I found surprising because this wasn't mentioned in many of the official examples and according to the <code>upload_file()<\/code> <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#upload-file-name--path-or-stream-\" rel=\"noreferrer\">documentation<\/a>:<\/p>\n\n<blockquote>\n  <p>Runs automatically capture file in the specified output directory, which defaults to \".\/outputs\" for most run types. Use upload_file only when additional files need to be uploaded or an output directory is not specified.<\/p>\n<\/blockquote>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1578746319987,
        "Solution_link_count":2.0,
        "Solution_readability":19.6,
        "Solution_reading_time":13.4,
        "Solution_score_count":14.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":88.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.4033333333,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi, I am trying to fit an MXNet model locally. I am adapting this https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/ and doing the following:\n\n    bucket = 'XXXXXXXXXXX'\n    prefix = 'sagemaker\/cifar-bench\/data'\n    \n    inputs = sagemaker_session.upload_data(\n        path='data',\n        bucket=bucket, \n        key_prefix=prefix)\n    \n    print('data sent to ' + inputs)\n    \n    \n    Inception = MXNet('gluon_cifar_net.py', \n              role=role, \n              train_instance_count=1, \n              train_instance_type='local_gpu',\n              framework_version='1.2.1',\n              base_job_name='cifar10-inception-',\n              hyperparameters={'batch_size': 256, \n                               'optimizer': 'sgd',\n                               'epochs': 100, \n                               'learning_rate': 0.1, \n                               'momentum': 0.9})\n    \n    \n    Inception.fit(inputs)\n\nwhich returns an `OSError: [Errno 2] No such file or directory`\n\nIn the error log I can see that there seems to be error at `self.latest_training_job = _TrainingJob.start_new(self, inputs)` and `self.sagemaker_client.create_training_job(**train_request)`\n\nHow can I make the local mode work?\n",
        "Challenge_closed_time":1537550695000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1537534843000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668119355544,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUQu1fDak6RL2wmivZ5UJwUw\/sagemaker-mxnet-local-mode-not-working",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":16.0,
        "Challenge_reading_time":13.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":4.4033333333,
        "Challenge_title":"SageMaker MXNet local mode not working",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":255.0,
        "Challenge_word_count":93,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"It is very likely that you don't have docker-compose (or docker) installed in the box, that is why you are getting a No such file or directory.\n\nIf you want to use the GPU setup I would recommend running on a sagemaker notebook instance. Navigate to one of the example notebooks such as: https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_cifar10\/mxnet_cifar10_local_mode.ipynb\n\nAnd run the setup.sh cell. This will install and configure all the docker dependencies correctly and then you should be able to use MXNet locally on GPU without any issue.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925587140,
        "Solution_link_count":1.0,
        "Solution_readability":12.2,
        "Solution_reading_time":7.52,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":84.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":13.4696980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been following along with this really helpful XGBoost tutorial on Medium (code used towards bottom of article): <a href=\"https:\/\/medium.com\/analytics-vidhya\/random-forest-and-xgboost-on-amazon-sagemaker-and-aws-lambda-29abd9467795\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/analytics-vidhya\/random-forest-and-xgboost-on-amazon-sagemaker-and-aws-lambda-29abd9467795<\/a>.<\/p>\n<p>To-date, I've been able to get data appropriately formatted for ML purposes, a model created based on training data, and then test data fed through the model to give useful results.<\/p>\n<p>Whenever I leave and come back to work more on the model or feed in new test data however, I find I need to re-run all model creation steps in order to make any further predictions. Instead I would like to just call my already created model endpoint based on the Image_URI and feed in new data.<\/p>\n<p>Current steps performed:<\/p>\n<p>Model Training<\/p>\n<pre><code>xgb = sagemaker.estimator.Estimator(containers[my_region],\n                                    role, \n                                    train_instance_count=1, \n                                    train_instance_type='ml.m4.xlarge',\n                                    output_path='s3:\/\/{}\/{}\/output'.format(bucket_name, prefix),\n                                    sagemaker_session=sess)\nxgb.set_hyperparameters(eta=0.06,\n                        alpha=0.8,\n                        lambda_bias=0.8,\n                        gamma=50,\n                        min_child_weight=6,\n                        subsample=0.5,\n                        silent=0,\n                        early_stopping_rounds=5,\n                        objective='reg:linear',\n                        num_round=1000)\n\nxgb.fit({'train': s3_input_train})\n\nxgb_predictor = xgb.deploy(initial_instance_count=1,instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n<p>Evaluation<\/p>\n<pre><code>test_data_array = test_data.drop([ 'price','id','sqft_above','date'], axis=1).values #load the data into an array\n\nxgb_predictor.serializer = csv_serializer # set the serializer type\n\npredictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\npredictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array\nprint(predictions_array.shape)\n\nfrom sklearn.metrics import r2_score\nprint(&quot;R2 score : %.2f&quot; % r2_score(test_data['price'],predictions_array))\n<\/code><\/pre>\n<p>It seems that this particular line:<\/p>\n<pre><code>predictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\n<\/code><\/pre>\n<p>needs to be re-written in order to not reference xgb.predictor but instead reference the model location.<\/p>\n<p>I have tried the following<\/p>\n<pre><code>trained_model = sagemaker.model.Model(\n    model_data='s3:\/\/{}\/{}\/output\/xgboost-2020-11-10-00-00\/output\/model.tar.gz'.format(bucket_name, prefix),\n    image_uri='XXXXXXXXXX.dkr.ecr.us-east-1.amazonaws.com\/xgboost:latest',\n    role=role)  # your role here; could be different name\n\ntrained_model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n<p>and then replaced<\/p>\n<pre><code>xgb_predictor.serializer = csv_serializer # set the serializer type\npredictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\n<\/code><\/pre>\n<p>with<\/p>\n<pre><code>trained_model.serializer = csv_serializer # set the serializer type\npredictions = trained_model.predict(test_data_array).decode('utf-8') # predict!\n<\/code><\/pre>\n<p>but I get the following error:<\/p>\n<pre><code>AttributeError: 'Model' object has no attribute 'predict'\n<\/code><\/pre>",
        "Challenge_closed_time":1605108362620,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605059871707,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64779388",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":17.5,
        "Challenge_reading_time":43.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":13.4696980556,
        "Challenge_title":"How to invoke Sagemaker XGBoost endpoint post model creation?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1242.0,
        "Challenge_word_count":272,
        "Platform":"Stack Overflow",
        "Poster_created_time":1532706046196,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":191.0,
        "Poster_view_count":29.0,
        "Solution_body":"<p>that's a good question :) I agree, many of the official tutorials tend to show the full train-to-invoke pipeline and don't emphasize enough that each step can be done separately. In your specific case, when you want to invoke an already-deployed endpoint, you can either: (A) use the invoke API call in one of the numerous SDKs (example in <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker-runtime\/invoke-endpoint.html\" rel=\"nofollow noreferrer\">CLI<\/a>, <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker-runtime.html#SageMakerRuntime.Client.invoke_endpoint\" rel=\"nofollow noreferrer\">boto3<\/a>) or (B) or instantiate a <code>predictor<\/code> with the high-level Python SDK, either the generic <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html\" rel=\"nofollow noreferrer\"><code>sagemaker.model.Model<\/code><\/a> class or its XGBoost-specific child: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/xgboost\/xgboost.html#sagemaker.xgboost.model.XGBoostPredictor\" rel=\"nofollow noreferrer\"><code>sagemaker.xgboost.model.XGBoostPredictor<\/code><\/a> as illustrated below:<\/p>\n<pre><code>from sagemaker.xgboost.model import XGBoostPredictor\n    \npredictor = XGBoostPredictor(endpoint_name='your-endpoint')\npredictor.predict('&lt;payload&gt;')\n<\/code><\/pre>\n<p>similar question <a href=\"https:\/\/stackoverflow.com\/questions\/56255154\/how-to-use-a-pretrained-model-from-s3-to-predict-some-data\/56277411#56277411\">How to use a pretrained model from s3 to predict some data?<\/a><\/p>\n<p>Note:<\/p>\n<ul>\n<li>If you want the <code>model.deploy()<\/code> call to return a predictor, your model must be instantiated with a <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html\" rel=\"nofollow noreferrer\"><code>predictor_cls<\/code><\/a>. This is optional, you can also first deploy a model, and then invoke it as a separate step with the above technique<\/li>\n<li>Endpoints create charges even if you don't invoke them; they are charged per uptime. So if you don't need an always-on endpoint, don't hesitate to shut it down to minimize costs.<\/li>\n<\/ul>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":17.5,
        "Solution_reading_time":28.73,
        "Solution_score_count":2.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":196.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1515548304623,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vietnam",
        "Answerer_reputation_count":549.0,
        "Answerer_view_count":68.0,
        "Challenge_adjusted_solved_time":2.1902408334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to do a neural network for regression analysis using optuna based on <a href=\"https:\/\/dreamer-uma.com\/pytorch-optuna-hyperparameter-tuning\/\" rel=\"nofollow noreferrer\">this site<\/a>.\nI would like to create a model with two 1D data as input and one 1D data as output in batch learning.<\/p>\n<p><code>x<\/code> is the training data and <code>y<\/code> is the teacher data.<\/p>\n<pre><code>class Model(nn.Module):\n    # \u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf(\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u751f\u6210\u6642\u306e\u521d\u671f\u5316)\n    def __init__(self,trial, mid_units1, mid_units2):\n        super(Model, self).__init__()\n        self.linear1 = nn.Linear(2, mid_units1)\n        self.bn1 = nn.BatchNorm1d(mid_units1)\n        self.linear2 = nn.Linear(mid_units1, mid_units2)\n        self.bn2 = nn.BatchNorm1d(mid_units2)\n        self.linear3 = nn.Linear(mid_units2, 1)\n        self.activation = trial_activation(trial)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.bn1(x)\n        x = self.activation(x)\n        x = self.linear2(x)\n\ndevice = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;\n\nEPOCH = 100\nx = torch.from_numpy(a[0].astype(np.float32)).to(device)\ny = torch.from_numpy(a[1].astype(np.float32)).to(device)\n\ndef train_epoch(model, optimizer, criterion):\n    model.train()\n    optimizer.zero_grad()    # \u52fe\u914d\u60c5\u5831\u30920\u306b\u521d\u671f\u5316\n    y_pred = model(x)                                               # \u4e88\u6e2c\n    loss = criterion(y_pred.reshape(y.shape), y)          # \u640d\u5931\u3092\u8a08\u7b97(shape\u3092\u63c3\u3048\u308b)\n    loss.backward()                                                       # \u52fe\u914d\u306e\u8a08\u7b97\n    optimizer.step()                                                      # \u52fe\u914d\u306e\u66f4\u65b0\n    return loss.item()\n\ndef trial_activation(trial):\n    activation_names = ['ReLU','logsigmoid']\n    activation_name = trial.suggest_categorical('activation', activation_names)\n    if activation_name == activation_names[0]:\n        activation = F.relu\n    else:\n        activation = F.logsigmoid\n    return activation\n\ndef objective(trial):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    # \u4e2d\u9593\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\u306e\u8a66\u884c\n    mid_units1 = int(trial.suggest_discrete_uniform(&quot;mid_units1&quot;, 1024*2,1024*4, 64*2))\n    mid_units2 = int(trial.suggest_discrete_uniform(&quot;mid_units2&quot;, 1024, 1024*2, 64*2))\n\n    net = Model(trial, mid_units1, mid_units2).to(device)\n\n    criterion = nn.MSELoss() \n    # \u6700\u9069\u5316\u624b\u6cd5\u306e\u8a66\u884c\n    optimizer = trial_optimizer(trial, net)\n    train_loss = 0\n    for epoch in range(EPOCH):\n        train_loss = train_epoch(net, optimizer, criterion, device)\n    torch.save(net.state_dict(), str(trial.number) + &quot;new1.pth&quot;)\n    return train_loss\n\nstrage_name = &quot;a.sql&quot;\nstudy_name = 'a'\n\nstudy = optuna.create_study(\n    study_name = study_name,\n    storage='sqlite:\/\/\/'  + strage_name, \n    load_if_exists=True,\n    direction='minimize')\nTRIAL_SIZE = 100\n\nstudy.optimize(objective, n_trials=TRIAL_SIZE)\n<\/code><\/pre>\n<p>error message<\/p>\n<pre><code>---&gt; 28     loss = criterion(y_pred.reshape(y.shape), y)          # \u640d\u5931\u3092\u8a08\u7b97(shape\u3092\u63c3\u3048\u308b)\n     29     loss.backward()                                                       # \u52fe\u914d\u306e\u8a08\u7b97\n     30     optimizer.step()                                                      # \u52fe\u914d\u306e\u66f4\u65b0\n\nAttributeError: 'NoneType' object has no attribute 'reshape'\n<\/code><\/pre>\n<p>Because of the above error, I checked the value of <code>y_pred<\/code> and found it to be <code>None<\/code>.<\/p>\n<pre><code>    model.train()\n    optimizer.zero_grad()\n<\/code><\/pre>\n<p>I am thinking that these two lines may be wrong, but I don't know how to solve this problem.<\/p>",
        "Challenge_closed_time":1646568057230,
        "Challenge_comment_count":1,
        "Challenge_created_time":1646559560287,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1646560172363,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71369132",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":14.2,
        "Challenge_reading_time":39.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":2.3602619445,
        "Challenge_title":"The pytorch training model cannot be created successfully",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":56.0,
        "Challenge_word_count":267,
        "Platform":"Stack Overflow",
        "Poster_created_time":1643702319420,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":17.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>With PyTorch, when you call <code>y_pred = model(x)<\/code> that will call the <code>forward<\/code> function which is defined in the <code>Model<\/code> class.<\/p>\n<p>So, <code>y_pred <\/code> will get the result of the <code>forward<\/code> function, in your case, it returns nothing, that's why you get a <code>None<\/code> value. You can change the <code>forward<\/code> function as below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>    def forward(self, x):\n        x = self.linear1(x)\n        x = self.bn1(x)\n        x = self.activation(x)\n        x = self.linear2(x)\n        return x\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":7.24,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":69.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":20.4471097222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I've been working recently on deploying a machine learning model as a web service. I used Azure Machine Learning Studio for creating my own Workspace ID and Authorization Token. Then, I trained LogisticRegressionCV model from <strong>sklearn.linear_model<\/strong> locally on my machine (using python 2.7.13) and with the usage of below code snippet I wanted to publish my model as web service:<\/p>\n\n<pre><code>from azureml import services\n\n@services.publish('workspaceID','authorization_token')\n@services.types(var_1= float, var_2= float)\n@services.returns(int)\n\ndef predicting(var_1, var_2):\n    input = np.array([var_1, var_2].reshape(1,-1)\nreturn model.predict_proba(input)[0][1]\n<\/code><\/pre>\n\n<p>where <em>input<\/em> variable is a list with data to be scored and <em>model<\/em> variable contains trained classifier. Then after defining above function I want to make a prediction on sample input vector:<\/p>\n\n<pre><code>predicting.service(1.21, 1.34)\n<\/code><\/pre>\n\n<p>However following error occurs:<\/p>\n\n<pre><code>RuntimeError: Error 0085: The following error occurred during script \nevaluation, please view the output log for more information:\n<\/code><\/pre>\n\n<p>And the most important message in log is: <\/p>\n\n<pre><code>AttributeError: 'module' object has no attribute 'LogisticRegressionCV'\n<\/code><\/pre>\n\n<p>The error is strange to me because when I was using normal <em>sklearn.linear_model.LogisticRegression<\/em> everything was fine. I was able to make predictions sending POST requests to created endpoint, so I guess <strong>sklearn<\/strong> worked correctly. \nAfter changing to <em>LogisticRegressionCV<\/em> it does not. <\/p>\n\n<p>Therefore I wanted to update sklearn on my workspace.<\/p>\n\n<p>Do you have any ideas how to do it? Or even more general question: how to install any python module on azure machine learning studio in a way to use predict functions of any model I develpoed locally?<\/p>\n\n<p>Thanks<\/p>",
        "Challenge_closed_time":1507014239332,
        "Challenge_comment_count":0,
        "Challenge_created_time":1506940629737,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46523924",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.0,
        "Challenge_reading_time":25.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":20.4471097222,
        "Challenge_title":"Adding python modules to AzureML workspace",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2578.0,
        "Challenge_word_count":249,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458750704640,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Warszawa, Polska",
        "Poster_reputation_count":186.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>For installing python module on Azure ML Studio, there is a section <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/cdb56f95-7f4c-404d-bde7-5bb972e6f232\/#Anchor_3\" rel=\"nofollow noreferrer\"><code>Technical Notes<\/code><\/a> of the offical document <code>Execute Python Script<\/code> which introduces it.<\/p>\n\n<p>The general steps as below.<\/p>\n\n<ol>\n<li>Create a Python project via <code>virtualenv<\/code> and active it.<\/li>\n<li>Install all packages you want via <code>pip<\/code> on the virtual Python environment, and then<\/li>\n<li>Package all files and directorys under the path <code>Lib\\site-packages<\/code> of your project as a zip file.<\/li>\n<li>Upload the zip package into your Azure ML WorkSpace as a dataSet.<\/li>\n<li>Follow the offical <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/execute-python-scripts#importing-existing-python-script-modules\" rel=\"nofollow noreferrer\">document<\/a> to import Python Module for your <code>Execute Python Script<\/code>.<\/li>\n<\/ol>\n\n<p>For more details, you can refer to the other similar SO thread <a href=\"https:\/\/stackoverflow.com\/questions\/46222606\/updating-pandas-to-version-0-19-in-azure-ml-studio\/46232963#46232963\">Updating pandas to version 0.19 in Azure ML Studio<\/a>, it even introduced how to update the version of Python packages installed by Azure.<\/p>\n\n<p>Hope it helps.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.6,
        "Solution_reading_time":17.94,
        "Solution_score_count":2.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":140.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":75.75,
        "Challenge_answer_count":3,
        "Challenge_body":"Hi,\n\u00a0\nIs there a way to download my Google AutoML Transation model and use it offline once it's trained?\n\u00a0\nAnd in what format can the model be exported?\u00a0\n\u00a0\nThank you",
        "Challenge_closed_time":1664553420000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664280720000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/exporting-a-google-autoML-translate-model\/td-p\/471646\/jump-to\/first-unread-message",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":6.7,
        "Challenge_reading_time":2.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":75.75,
        "Challenge_title":"exporting a google autoML translate model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":135.0,
        "Challenge_word_count":35,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"1.- No.\n\n2.- You can create a Feature Request at\u00a0Issue Tracker\u00a0and\u00a0add a description about the feature you want(Export Translation Models), and the engineer team will look at it. You can see here how it is more likely that the team prioritize the work of the Feature Request\/Issues.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.2,
        "Solution_reading_time":3.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":55.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1421401313787,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":326.0,
        "Answerer_view_count":21.0,
        "Challenge_adjusted_solved_time":149.0252658333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm stuck with the MLFlow model registry. Does anyone know how to load a model using the object &quot;mlflow.tracking.client.MlflowClient&quot;?<\/p>\n<p>I would like to do a predict after with that. I'm sure I'm wrong somewhere because I've already done that in the past. I'm not able to find it in the doc, in the web.<\/p>",
        "Challenge_closed_time":1609791959680,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609255468723,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1611139575950,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65494496",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.9,
        "Challenge_reading_time":4.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":149.0252658333,
        "Challenge_title":"How to load a model using the object \"mlflow.tracking.client.MlflowClient\"?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":608.0,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423640080283,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lyon, France",
        "Poster_reputation_count":457.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>You'll have to make use of <code>mlflow.&lt;model_flavor&gt;.load_model()<\/code> to load a given model from the Model Registry. For example:<\/p>\n<pre><code>import mlflow.pyfunc\n\nmodel = mlflow.pyfunc.load_model(\n          model_uri=&quot;models:\/&lt;model_name&gt;\/&lt;model_version&gt;&quot;\n          )\n\nmodel.predict(...)\n<\/code><\/pre>\n<p>With <code>mlflow.tracking.client.MlflowClient<\/code> you can retrieve metadata about a model from the model registry, but for retrieving the actual model you will need to use <code>mlflow.&lt;model_flavor&gt;.load_model<\/code>. For example, you could use the MlflowClient to get the download URI for a given model, and then use <code>mlflow.&lt;flavor&gt;.load_model<\/code> to retrieve that model.<\/p>\n<pre><code>model_uri = client.get_model_version_download_uri(&quot;&lt;model_name&gt;&quot;, &lt;version&gt;)\nmodel = mlflow.pyfunc.load_model(model_uri)\n\nmodel.predict(...)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.4,
        "Solution_reading_time":12.16,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":81.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":10.5155730556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>After I've trained and deployed the model with AWS SageMaker, I want to evaluate it on several csv files:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>- category-1-eval.csv (~700000 records)\n- category-2-eval.csv (~500000 records)\n- category-3-eval.csv (~800000 records)\n...\n<\/code><\/pre>\n\n<p>The right way to do this is with using <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/estimator\/Estimator#evaluate\" rel=\"nofollow noreferrer\">Estimator.evaluate()<\/a> method, as it is fast.<\/p>\n\n<p>The problem is - I cannot find the way to restore SageMaker model into Tensorflow Estimator, is it possible?<\/p>\n\n<p>I've tried to restore a model like this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>tf.estimator.DNNClassifier(\n    feature_columns=...,\n    hidden_units=[...],\n    model_dir=\"s3:\/\/&lt;bucket_name&gt;\/checkpoints\",\n)\n<\/code><\/pre>\n\n<p>In AWS SageMaker documentation a different approach is described - <a href=\"https:\/\/docs.aws.amazon.com\/en_us\/sagemaker\/latest\/dg\/how-it-works-model-validation.html\" rel=\"nofollow noreferrer\">to test the actual endpoint from the Notebook<\/a> - but it takes to much time and requires a lot of API calls to the endpoint.<\/p>",
        "Challenge_closed_time":1562591149540,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562553970617,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56927813",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":16.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":10.3274786111,
        "Challenge_title":"Using of Estamator.evaluate() on trained sagemaker tensorflow model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":155.0,
        "Challenge_word_count":126,
        "Platform":"Stack Overflow",
        "Poster_created_time":1530642335903,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Vancouver, BC, Canada",
        "Poster_reputation_count":53.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>if you used the built-in Tensorflow container, your model has been saved in Tensorflow Serving format, e.g.:<\/p>\n\n<pre><code>$ tar tfz model.tar.gz\nmodel\/\nmodel\/1\/\nmodel\/1\/saved_model.pb\nmodel\/1\/variables\/\nmodel\/1\/variables\/variables.index\nmodel\/1\/variables\/variables.data-00000-of-00001\n<\/code><\/pre>\n\n<p>You can easily load it with Tensorflow Serving on your local machine, and send it samples to predict. More info at <a href=\"https:\/\/www.tensorflow.org\/tfx\/guide\/serving\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/tfx\/guide\/serving<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1562591826680,
        "Solution_link_count":2.0,
        "Solution_readability":12.2,
        "Solution_reading_time":7.39,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":53.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1329.0686111111,
        "Challenge_answer_count":2,
        "Challenge_body":"I have a custom model built-in TensorFlow. I am trying to deploy this model on amazon sagemaker for inference. The model takes three inputs and gives five outputs.\r\nThe name of the inputs are:\r\n1. `input_image` \r\n2. `input_image_meta` \r\n3. `input_anchors` \r\n\r\n\r\nand the name of outputs are:\r\n1.  `output_detections`\r\n2.  `output_mrcnn_class`\r\n3.  `output_mrcnn_bbox`\r\n4.  `output_mrcnn_mask`\r\n5.  `output_rois`\r\n\r\nI have successfully created the model endpoint on sagemaker and when I am trying to hit the request for the results, I am getting `{'error': \"Missing 'inputs' or 'instances' key\"}` in return.\r\n \r\nI have made a model.tar.gz file which has the following structure:\r\n\r\n    mymodel\r\n        |__1\r\n            |__variables\r\n            |__saved_model.pb\r\n\r\n    code\r\n        |__inference.py\r\n        |__requirements.txt\r\n\r\nAs specified in the documentation, inference.py has input_handler and output handler functions. From the client-side, I pass the S3 link of the image which then transforms to the three inputs for the model. \r\n\r\nThe structure of input_handler is as follows:\r\n\r\n```\r\ndef input_handler(data, context):\r\n     input_data = json.loads(data.read().decode('utf-8'))\r\n\r\n    obj = bucket.Object(input_data['img_link'])\r\n    tmp = tempfile.NamedTemporaryFile()\r\n    \r\n    # download image from AWS S3\r\n    with open(tmp.name, 'wb') as f:\r\n        obj.download_fileobj(f)\r\n        image=mpimg.imread(tmp.name)\r\n    \r\n    # make preprocessing\r\n    image = Image.fromarray(image)\r\n     \r\n     ...... # some more transformations \r\n     return = {\"input_image\": Python list for image,\r\n                    \"input_image_meta: Python list for input image meta,\r\n                    \"input_anchors\": Python list for input anchors}\r\n\r\n```\r\nThe deifinition of output_handler is as follows:\r\n\r\n```\r\ndef output_handler(data, context):\r\n      output_string = data.content.decode('unicode-escape')\r\n      return output_string, context.accept_header\r\n```\r\n\r\nThe sagemaker endpoint gets created and the tensorflow server also starts(as shown in CloudWatch logs).\r\nOn the client side, I call the predictor using follwoing code:\r\n\r\n```\r\nrequest = {}\r\nrequest[\"img_link\"] = \"image.jpg\"\r\nresult = predictor.predict(request)\r\n```\r\n\r\nBut when I print the result the following gets printed out, `{'error': \"Missing 'inputs' or 'instances' key\"}`\r\nAll the bucket connections for loading the image are in inference.py\r\n      ",
        "Challenge_closed_time":1571957138000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1567172491000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/73",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.1,
        "Challenge_reading_time":28.7,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":104.0,
        "Challenge_repo_issue_count":229.0,
        "Challenge_repo_star_count":160.0,
        "Challenge_repo_watch_count":37.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":1329.0686111111,
        "Challenge_title":"Error in giving inputs to the tensorflow serving model on sagemaker. {'error': \"Missing 'inputs' or 'instances' key\"}",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":283,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello @janismdhanbad,\r\n\r\nI believe your inference requests will have to follow the TensorFlow serving REST API specifications defined here: https:\/\/www.tensorflow.org\/tfx\/serving\/api_rest#request_format_2\r\n\r\n```\r\nThe request body for predict API must be JSON object formatted as follows:\r\n\r\n{\r\n  \/\/ (Optional) Serving signature to use.\r\n  \/\/ If unspecifed default serving signature is used.\r\n  \"signature_name\": <string>,\r\n\r\n  \/\/ Input Tensors in row (\"instances\") or columnar (\"inputs\") format.\r\n  \/\/ A request can have either of them but NOT both.\r\n  \"instances\": <value>|<(nested)list>|<list-of-objects>\r\n  \"inputs\": <value>|<(nested)list>|<object>\r\n}\r\n``` closing due to inactivity. feel free to reopen if necessary.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.3,
        "Solution_reading_time":8.78,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.4459375,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have tried importing a simple logistic regression model into Azure ML to use in a Execute Python Script-module It's attached the correct way, but I keep getting the same error. From what I understand this has to do with the version of scikit-learn and scipy, but I created venv for almost every possible version of Python.<\/p>\n<p>Error 0085: The following error occurred during script evaluation, please view the output log for more information:  <br \/>\n---------- Start of error message from Python interpreter ----------  <br \/>\nCaught exception while executing function: Traceback (most recent call last):  <br \/>\nFile &quot;C:\\server\\invokepy.py&quot;, line 199, in batch  <br \/>\nodfs = mod.azureml_main(*idfs)  <br \/>\nFile &quot;C:\\temp\\8c190f6b5c65446f8824ed5a578a75d5.py&quot;, line 22, in azureml_main  <br \/>\nmodel = pickle.load( open( &quot;.\/Script Bundle\/logreg_model_36.pkl&quot;, &quot;rb&quot; ) )  <br \/>\nImportError: No module named 'sklearn.linear_model._logistic'  <br \/>\nProcess returned with non-zero exit code 1<\/p>\n<p>Can someone please explain which version of scikit-learn I need to execute in Azure ML studio (classic) with the environment set to 'Anconda 4.0\/Python3.5'?<\/p>\n<p>Kind regards<\/p>",
        "Challenge_closed_time":1617261351652,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617234546277,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/339957\/importerror-no-module-named-sklearn-linear-model-l",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.0,
        "Challenge_reading_time":16.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":7.4459375,
        "Challenge_title":"ImportError: No module named 'sklearn.linear_model._logistic': failing to import a locally train and serialized model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":170,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=7cd6c1b8-29d2-45e1-9211-3963bc7ae8f3\">@David Eeckhout  <\/a> The packages installed in the designer version of the studio are listed <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/execute-python-script#preinstalled-python-packages\">here<\/a> which uses the following versions:    <\/p>\n<pre><code>scikit-learn==0.22.2  \nscipy==1.4.1  \n<\/code><\/pre>\n<p>The classic version of the studio is still available but it is preferable to use the designer studio which has better support for some of the pre-installed packages.     <br \/>\nIf you prefer to use classic version then if the package is not available you can try installing it in the execute python script. For example,    <\/p>\n<pre><code>import importlib.util  \npackage_name = 'scikit-misc'  \nspec = importlib.util.find_spec(package_name)  \nif spec is None:  \n    import os  \n    os.system(f&quot;pip install scikit-misc&quot;)  \n<\/code><\/pre>\n<p>Hope this helps.    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.9,
        "Solution_reading_time":12.55,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":103.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1626973229036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":199.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":30.4097575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an already trained a TensorFlow model outside of SageMaker.<\/p>\n<p>I am trying to focus on deployment\/inference but I am facing issues with inference.<\/p>\n<p>For deployment I did this:<\/p>\n<pre><code>from sagemaker.tensorflow.serving import TensorFlowModel\ninstance_type = 'ml.c5.xlarge' \n\nmodel = TensorFlowModel(\n    model_data=model_data,\n    name= 'tfmodel1',\n    framework_version=&quot;2.2&quot;,\n    role=role, \n    source_dir='code',\n)\n\npredictor = model.deploy(endpoint_name='test', \n                                       initial_instance_count=1, \n                                       tags=tags,\n                                       instance_type=instance_type)\n<\/code><\/pre>\n<p>When I tried to infer the model I did this:<\/p>\n<pre><code>import PIL\nfrom PIL import Image\nimport numpy as np\nimport json\nimport boto3\n\nimage = PIL.Image.open('img_test.jpg')\nclient = boto3.client('sagemaker-runtime')\nbatch_size = 1\nimage = np.asarray(image.resize((512, 512)))\nimage = np.concatenate([image[np.newaxis, :, :]] * batch_size)\nbody = json.dumps({&quot;instances&quot;: image.tolist()})\n\nioc_predictor_endpoint_name = &quot;test&quot;\ncontent_type = 'application\/x-image'   \nioc_response = client.invoke_endpoint(\n    EndpointName=ioc_predictor_endpoint_name,\n    Body=body,\n    ContentType=content_type\n )\n<\/code><\/pre>\n<p>But I have this error:<\/p>\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from primary with message &quot;{&quot;error&quot;: &quot;Unsupported Media Type: application\/x-image&quot;}&quot;.\n<\/code><\/pre>\n<p>I also tried:<\/p>\n<pre><code>from sagemaker.predictor import Predictor\n\npredictor = Predictor(ioc_predictor_endpoint_name)\ninference_response = predictor.predict(data=body)\nprint(inference_response)\n<\/code><\/pre>\n<p>And have this error:<\/p>\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from primary with message &quot;{&quot;error&quot;: &quot;Unsupported Media Type: application\/octet-stream&quot;}&quot;.\n<\/code><\/pre>\n<p>What can I do ? I don't know if I missed something<\/p>",
        "Challenge_closed_time":1651191185627,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651077733827,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1651081710500,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72032469",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.5,
        "Challenge_reading_time":27.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":31.5143888889,
        "Challenge_title":"How to infer a tensorflow pre trained deployed model with an image?",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":145.0,
        "Challenge_word_count":191,
        "Platform":"Stack Overflow",
        "Poster_created_time":1606642099552,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":371.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>Have you tested this model locally? How does inference work with your TF model locally? This should show you how the input needs to be formatted for inference with that model in specific. Application\/x-image data format should be fine. Do you have a custom inference script? Check out this link here for adding an inference script with will let you control pre\/post processing and you can log each line to capture the error: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.9,
        "Solution_reading_time":7.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":77.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":3.4073972222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I try to use azuremlsdk to deploy a locally trained model (a perfectly valid use case AFIK). I follow <a href=\"https:\/\/cran.r-project.org\/web\/packages\/azuremlsdk\/vignettes\/train-and-deploy-first-model.html\" rel=\"nofollow noreferrer\">this<\/a> and managed to create a ML workspace and register a &quot;model&quot; like so:<\/p>\n<pre><code>library(azuremlsdk)\n\ninteractive_auth &lt;- interactive_login_authentication(tenant_id=&quot;xxx&quot;)\nws &lt;- get_workspace(\n        name = &quot;xxx&quot;, \n        subscription_id = &quot;xxx&quot;, \n        resource_group =&quot;xxx&quot;, \n        auth = interactive_auth\n)\n\nadd &lt;- function(a, b) {\n    return(a + b)\n}\n\nadd(1,2)\n\nsaveRDS(add, file = &quot;D:\/add.rds&quot;)\n\nmodel &lt;- register_model(\n    ws, \n    model_path = &quot;D:\/add.rds&quot;, \n    model_name = &quot;add_model&quot;,\n    description = &quot;An amazing model&quot;\n)\n<\/code><\/pre>\n<p>This seemed to work fine, as I get some nice log messages telling me that the model was registered. For my sanity, I wonder where can I find this registered (&quot;materialised&quot;) model\/object\/function in the Azure UI please?<\/p>",
        "Challenge_closed_time":1621001633740,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620989367110,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67533091",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":14.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3.4073972222,
        "Challenge_title":"where are registered models in azure machine learning",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":41.0,
        "Challenge_word_count":118,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>On ml.azure.com, there is a &quot;Models&quot; option on the left-hand blade.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Y7cZe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Y7cZe.png\" alt=\"UI Sidebar\" \/><\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.1,
        "Solution_reading_time":3.17,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1393021961043,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":838.0,
        "Answerer_view_count":193.0,
        "Challenge_adjusted_solved_time":4512.6045455556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm following this example notebook to learn SageMaker's processing jobs API: <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.ipynb<\/a><\/p>\n<p>I'm trying to modify their code to avoid using the default S3 bucket, namely: <code>s3:\/\/sagemaker-&lt;region&gt;-&lt;account_id&gt;\/<\/code><\/p>\n<p>For their data processing step with the <code>.run<\/code> method:<\/p>\n<pre><code>from sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor.run(\n    code=&quot;preprocessing.py&quot;,\n    inputs=[ProcessingInput(source=input_data, destination=&quot;\/opt\/ml\/processing\/input&quot;)],\n    outputs=[\n        ProcessingOutput(output_name=&quot;train_data&quot;, source=&quot;\/opt\/ml\/processing\/train&quot;),\n        ProcessingOutput(output_name=&quot;test_data&quot;, source=&quot;\/opt\/ml\/processing\/test&quot;),\n    ],\n    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;],\n)\n<\/code><\/pre>\n<p>I was able to modify it to use my own S3 bucket by using the <code>destination<\/code> parameter like this:<\/p>\n<pre><code>sklearn_processor.run( \n    code=output_bucket_uri + &quot;preprocessing.py&quot;, \n    inputs=[ProcessingInput( \n        source=input_bucket_uri + &quot;census-income.csv&quot;, \n        destination=path+&quot;input\/&quot;, \n    )], \n    outputs=[ \n        ProcessingOutput( \n            output_name=&quot;train_data&quot;, \n            source=path+&quot;train\/&quot;, \n            destination=output_bucket_uri + &quot;train\/&quot;, \n        ), \n        ProcessingOutput( \n            output_name=&quot;test_data&quot;, \n            source=path+&quot;test\/&quot;, \n            destination=output_bucket_uri + &quot;test\/&quot;, \n        ), \n    ], \n    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;], \n)\n<\/code><\/pre>\n<p>But for the <code>.fit<\/code> method:<\/p>\n<pre><code>sklearn.fit({&quot;train&quot;: preprocessed_training_data})\n<\/code><\/pre>\n<p>I have not been able to find a parameter to pass it so that the output artifacts are saved to a S3 bucket that I specify instead of the default s3 bucket <code>s3:\/\/sagemaker-&lt;region&gt;-&lt;account_id&gt;\/<\/code>.<\/p>",
        "Challenge_closed_time":1648557908696,
        "Challenge_comment_count":0,
        "Challenge_created_time":1632276763933,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1632312532332,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69277390",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":26.2,
        "Challenge_reading_time":32.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":4522.5402119444,
        "Challenge_title":"Can I specify S3 bucket for sagemaker.sklearn.estimator's SKLearn?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":575.0,
        "Challenge_word_count":144,
        "Platform":"Stack Overflow",
        "Poster_created_time":1329161697310,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":4154.0,
        "Poster_view_count":314.0,
        "Solution_body":"<p>For SKLearnProcessor, the ideal way to specify default bucket is by creating a sagemaker session with that bucket, and sending that as sagemaker_session parameter. Example:<\/p>\n<pre><code>from sagemaker.session import Session    \nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role='&lt;arn-role&gt;',\n                                     instance_type='ml.m5.xlarge',\n                                     instance_count=1,\n                                     sagemaker_session=Session(default_bucket='&lt;s3-bucket-name&gt;'))\n<\/code><\/pre>\n<p>I know this is not your exact question but you have added an alternative to this in your question details. So I am adding it here as a cleaner approach.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.3,
        "Solution_reading_time":7.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1522076554448,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"S\u00e3o Paulo, State of S\u00e3o Paulo, Brazil",
        "Answerer_reputation_count":96.0,
        "Answerer_view_count":31.0,
        "Challenge_adjusted_solved_time":4030.6928727778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to retrieve the pickle off my trained model, which I know is in the run file inside my experiments in Databricks.<\/p>\n<p>It seems that the <code>mlflow.pyfunc.load_model<\/code> can only do the <code>predict<\/code> method.<\/p>\n<p>There is an option to directly access the pickle?<\/p>\n<p>I also tried to use the path in the run using the <code>pickle.load(path)<\/code> (example of path: dbfs:\/databricks\/mlflow-tracking\/20526156406\/92f3ec23bf614c9d934dd0195\/artifacts\/model\/model.pkl).<\/p>",
        "Challenge_closed_time":1629748421287,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628544411170,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68718719",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.9,
        "Challenge_reading_time":7.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":334.4472547222,
        "Challenge_title":"How can I retrive the model.pkl in the experiment in Databricks",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3081.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1522076554448,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"S\u00e3o Paulo, State of S\u00e3o Paulo, Brazil",
        "Poster_reputation_count":96.0,
        "Poster_view_count":31.0,
        "Solution_body":"<p>I recently found the solution which can be done by the following two approaches:<\/p>\n<ol>\n<li>Use the customized predict function at the moment of saving the model (check <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">databricks<\/a> documentation for more details).<\/li>\n<\/ol>\n<p>example give by Databricks<\/p>\n<pre><code>class AddN(mlflow.pyfunc.PythonModel):\n\n    def __init__(self, n):\n        self.n = n\n\n    def predict(self, context, model_input):\n        return model_input.apply(lambda column: column + self.n)\n# Construct and save the model\nmodel_path = &quot;add_n_model&quot;\nadd5_model = AddN(n=5)\nmlflow.pyfunc.save_model(path=model_path, python_model=add5_model)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(model_path)\n<\/code><\/pre>\n<ol start=\"2\">\n<li>Load the model artefacts as we are downloading the artefact:<\/li>\n<\/ol>\n<pre><code>from mlflow.tracking import MlflowClient\n\nclient = MlflowClient()\n\ntmp_path = client.download_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path='model\/model.pkl')\n\nf = open(tmp_path,'rb')\n\nmodel = pickle.load(f)\n\nf.close()\n\n \n\nclient.list_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path=&quot;&quot;)\n\nclient.list_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path=&quot;model&quot;)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1643054905512,
        "Solution_link_count":1.0,
        "Solution_readability":16.0,
        "Solution_reading_time":18.3,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":109.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1435766573232,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":10645.0,
        "Answerer_view_count":1173.0,
        "Challenge_adjusted_solved_time":6.5547283333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is that possible to use machine learning methods from Microsoft Azure Machine Learning  as an API from my own code (without ML Studio) with possibility to calculate everything on their side?<\/p>",
        "Challenge_closed_time":1450317613532,
        "Challenge_comment_count":0,
        "Challenge_created_time":1450294016510,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34320449",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":2.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":6.5547283333,
        "Challenge_title":"Use Azure ML methods like an API",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":180.0,
        "Challenge_word_count":37,
        "Platform":"Stack Overflow",
        "Poster_created_time":1336227824220,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":834.0,
        "Poster_view_count":122.0,
        "Solution_body":"<p>You can <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-publish-a-machine-learning-web-service\/\" rel=\"nofollow\">publish<\/a> an experiment (machine learning functions you hooked together in Azure ML Studio) as an API. When you call that API in your custom code you give it your data and all the computation runs in the cloud in Azure ML. <\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.8,
        "Solution_reading_time":4.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":46.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1401922736652,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Brisbane",
        "Answerer_reputation_count":748.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":510.2445766667,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I'm looking to make some hyper parameters available to the serving endpoint in SageMaker. The training instances is given access to input parameters using hyperparameters in:<\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='autocat.py',\n                       role=role,\n                       output_path=params['output_path'],\n                       code_location=params['code_location'],\n                       train_instance_count=1,\n                       train_instance_type='ml.c4.xlarge',\n                       training_steps=10000,\n                       evaluation_steps=None,\n                       hyperparameters=params)\n<\/code><\/pre>\n\n<p>However, when the endpoint is deployed, there is no way to pass in parameters that are used to control the data processing in the <code>input_fn(serialized_input, content_type)<\/code> function.<\/p>\n\n<p>What would be the best way to pass parameters to the serving instance?? Is the <code>source_dir<\/code> parameter defined in the <code>sagemaker.tensorflow.TensorFlow<\/code> class copied to the serving instance? If so, I could use a config.yml or similar.<\/p>",
        "Challenge_closed_time":1523591814356,
        "Challenge_comment_count":0,
        "Challenge_created_time":1521754623920,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1521754933880,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49438903",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":13.4,
        "Challenge_reading_time":13.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":510.3306766667,
        "Challenge_title":"How to make parameters available to SageMaker Tensorflow Endpoint",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1426.0,
        "Challenge_word_count":108,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443201378360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":749.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>Ah i have had a similar problem to you where I needed to download something off S3 to use in the input_fn for inference. In my case it was a dictionary.<\/p>\n\n<p>Three options:<\/p>\n\n<ol>\n<li>use your config.yml approach, and download and import the s3 file from within your entrypoint file before any function declarations. This would make it available to the input_fn <\/li>\n<li>Keep using the hyperparameter approach, download and import the vectorizer in <code>serving_input_fn<\/code> and make it available via a global variable so that <code>input_fn<\/code> has access to it.<\/li>\n<li>Download the file from s3 before training and include it in the source_dir directly.<\/li>\n<\/ol>\n\n<p>Option 3 would only work if you didnt need to make changes to the vectorizer seperately after initial training.<\/p>\n\n<p>Whatever you do, don't download the file directly in input_fn. I made that mistake and the performance is terrible as each invoking of the endpoint would result in the s3 file being downloaded.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.1,
        "Solution_reading_time":12.44,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":157.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1640956373383,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":309.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":168.3167494445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have an XGBoost model currently in production using AWS sagemaker and making real time inferences. After a while, I would like to update the model with a newer one trained on more data and keep everything as is (e.g. same endpoint, same inference procedure, so really no changes aside from the model itself)<\/p>\n<p>The current deployment procedure is the following :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.xgboost.model import XGBoostModel\nfrom sagemaker.xgboost.model import XGBoostPredictor\n\nxgboost_model = XGBoostModel(\n    model_data = &lt;S3 url&gt;,\n    role = &lt;sagemaker role&gt;,\n    entry_point = 'inference.py',\n    source_dir = 'src',\n    code_location = &lt;S3 url of other dependencies&gt;\n    framework_version='1.5-1',\n    name = model_name)\n\nxgboost_model.deploy(\n    instance_type='ml.c5.large',\n    initial_instance_count=1,\n    endpoint_name = model_name)\n<\/code><\/pre>\n<p>Now that I updated the model a few weeks later, I would like to re-deploy it. I am aware that the <code>.deploy()<\/code> method creates an endpoint and an endpoint configuration so it does it all. I cannot simply re-run my script again since I would encounter an error.<\/p>\n<p>In previous versions of sagemaker I could have updated the model with an extra argument passed to the <code>.deploy()<\/code> method called <code>update_endpoint = True<\/code>. In sagemaker &gt;=2.0 this is a no-op. Now, in sagemaker &gt;= 2.0, I need to use the predictor object as stated in the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html\" rel=\"nofollow noreferrer\">documentation<\/a>. So I try the following :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>predictor = XGBoostPredictor(model_name)\npredictor.update_endpoint(model_name= model_name)\n<\/code><\/pre>\n<p>Which actually updates the endpoint according to a new endpoint configuration. However, I do not know what it is updating... I do not specify in the above 2 lines of code that we need to considering the new <code>xgboost_model<\/code> trained on more data...  so where do I tell the update to take a more recent model?<\/p>\n<p>Thank you!<\/p>\n<p><strong>Update<\/strong><\/p>\n<p>I believe that I need to be looking at production variants as stated in their documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-ab-testing.html\" rel=\"nofollow noreferrer\">here<\/a>. However, their whole tutorial is based on the amazon sdk for python (boto3) which has artifacts that are hard to manage when I have difference entry points for each model variant (e.g. different <code>inference.py<\/code> scripts).<\/p>",
        "Challenge_closed_time":1663924957328,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663233254363,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1663319367852,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73728499",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":11.5,
        "Challenge_reading_time":33.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":192.1397125,
        "Challenge_title":"How to update an existing model in AWS sagemaker >= 2.0",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":41.0,
        "Challenge_word_count":333,
        "Platform":"Stack Overflow",
        "Poster_created_time":1640956373383,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":309.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>Since I found an answer to my own question I will post it here for those who encounter the same problem.<\/p>\n<p>I ended up re-coding all my deployment script using the boto3 SDK rather than the sagemaker SDK (or a mix of both as some documentation suggest).<\/p>\n<p>Here's the whole script that shows how to create a sagemaker model object, an endpoint configuration and an endpoint to deploy the model on for the first time. In addition, it shows what to do how to update the endpoint with a newer model (which was my main question)<\/p>\n<p>Here's the code to do all 3 in case you want to bring your own model and update it safely in production using sagemaker :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\nimport time\nfrom datetime import datetime\nfrom sagemaker import image_uris\nfrom fileManager import *  # this is a local script for helper functions\n\n# name of zipped model and zipped inference code\nCODE_TAR = 'your_inference_code_and_other_artifacts.tar.gz'\nMODEL_TAR = 'your_saved_xgboost_model.tar.gz'\n\n# sagemaker params\nsmClient = boto3.client('sagemaker')\nsmRole = &lt;your_sagemaker_role&gt;\nbucket = sagemaker.Session().default_bucket()\n\n# deploy algorithm\nclass Deployer:\n\n    def __init__(self, modelName, deployRetrained=False):\n        self.modelName=modelName\n        self.deployRetrained = deployRetrained\n        self.prefix = &lt;S3_model_path_prefix&gt;\n    \n    def deploy(self):\n        '''\n        Main method to create a sagemaker model, create an endpoint configuration and deploy the model. If deployRetrained\n        param is set to True, this method will update an already existing endpoint.\n        '''\n        # define model name and endpoint name to be used for model deployment\/update\n        model_name = self.modelName + &lt;any_suffix&gt;\n        endpoint_config_name = self.modelName + '-%s' %datetime.now().strftime('%Y-%m-%d-%HH%M')\n        endpoint_name = self.modelName\n        \n        # deploy model for the first time\n        if not self.deployRetrained:\n            print('Deploying for the first time')\n\n            # here you should copy and zip the model dependencies that you may have (such as preprocessors, inference code, config code...)\n            # mine were zipped into the file called CODE_TAR\n\n            # upload model and model artifacts needed for inference to S3\n            uploadFile(list_files=[MODEL_TAR, CODE_TAR], prefix = self.prefix)\n\n            # create sagemaker model and endpoint configuration\n            self.createSagemakerModel(model_name)\n            self.createEndpointConfig(endpoint_config_name, model_name)\n\n            # deploy model and wait while endpoint is being created\n            self.createEndpoint(endpoint_name, endpoint_config_name)\n            self.waitWhileCreating(endpoint_name)\n        \n        # update model\n        else:\n            print('Updating existing model')\n\n            # upload model and model artifacts needed for inference (here the old ones are replaced)\n            # make sure to make a backup in S3 if you would like to keep the older models\n            # we replace the old ones and keep the same names to avoid having to recreate a sagemaker model with a different name for the update!\n            uploadFile(list_files=[MODEL_TAR, CODE_TAR], prefix = self.prefix)\n\n            # create a new endpoint config that takes the new model\n            self.createEndpointConfig(endpoint_config_name, model_name)\n\n            # update endpoint\n            self.updateEndpoint(endpoint_name, endpoint_config_name)\n\n            # wait while endpoint updates then delete outdated endpoint config once it is InService\n            self.waitWhileCreating(endpoint_name)\n            self.deleteOutdatedEndpointConfig(model_name, endpoint_config_name)\n\n    def createSagemakerModel(self, model_name):\n        ''' \n        Create a new sagemaker Model object with an xgboost container and an entry point for inference using boto3 API\n        '''\n        # Retrieve that inference image (container)\n        docker_container = image_uris.retrieve(region=region, framework='xgboost', version='1.5-1')\n\n        # Relative S3 path to pre-trained model to create S3 model URI\n        model_s3_key = f'{self.prefix}\/'+ MODEL_TAR\n\n        # Combine bucket name, model file name, and relate S3 path to create S3 model URI\n        model_url = f's3:\/\/{bucket}\/{model_s3_key}'\n\n        # S3 path to the necessary inference code\n        code_url = f's3:\/\/{bucket}\/{self.prefix}\/{CODE_TAR}'\n        \n        # Create a sagemaker Model object with all its artifacts\n        smClient.create_model(\n            ModelName = model_name,\n            ExecutionRoleArn = smRole,\n            PrimaryContainer = {\n                'Image': docker_container,\n                'ModelDataUrl': model_url,\n                'Environment': {\n                    'SAGEMAKER_PROGRAM': 'inference.py', #inference.py is at the root of my zipped CODE_TAR\n                    'SAGEMAKER_SUBMIT_DIRECTORY': code_url,\n                }\n            }\n        )\n    \n    def createEndpointConfig(self, endpoint_config_name, model_name):\n        ''' \n        Create an endpoint configuration (only for boto3 sdk procedure) and set production variants parameters.\n        Each retraining procedure will induce a new variant name based on the endpoint configuration name.\n        '''\n        smClient.create_endpoint_config(\n            EndpointConfigName=endpoint_config_name,\n            ProductionVariants=[\n                {\n                    'VariantName': endpoint_config_name,\n                    'ModelName': model_name,\n                    'InstanceType': INSTANCE_TYPE,\n                    'InitialInstanceCount': 1\n                }\n            ]\n        )\n\n    def createEndpoint(self, endpoint_name, endpoint_config_name):\n        '''\n        Deploy the model to an endpoint\n        '''\n        smClient.create_endpoint(\n            EndpointName=endpoint_name,\n            EndpointConfigName=endpoint_config_name)\n    \n    def deleteOutdatedEndpointConfig(self, name_check, current_endpoint_config):\n        '''\n        Automatically detect and delete endpoint configurations that contain a string 'name_check'. This method can be used\n        after a retrain procedure to delete all previous endpoint configurations but keep the current one named 'current_endpoint_config'.\n        '''\n        # get a list of all available endpoint configurations\n        all_configs = smClient.list_endpoint_configs()['EndpointConfigs']\n\n        # loop over the names of endpoint configs\n        names_list = []\n        for config_dict in all_configs:\n            endpoint_config_name = config_dict['EndpointConfigName']\n\n            # get only endpoint configs that contain name_check in them and save names to a list\n            if name_check in endpoint_config_name:\n                names_list.append(endpoint_config_name)\n        \n        # remove the current endpoint configuration from the list (we do not want to detele this one since it is live)\n        names_list.remove(current_endpoint_config)\n\n        for name in names_list:\n            try:\n                smClient.delete_endpoint_config(EndpointConfigName=name)\n                print('Deleted endpoint configuration for %s' %name)\n            except:\n                print('INFO : No endpoint configuration was found for %s' %endpoint_config_name)\n\n    def updateEndpoint(self, endpoint_name, endpoint_config_name):\n        ''' \n        Update existing endpoint with a new retrained model\n        '''\n        smClient.update_endpoint(\n            EndpointName=endpoint_name,\n            EndpointConfigName=endpoint_config_name,\n            RetainAllVariantProperties=True)\n    \n    def waitWhileCreating(self, endpoint_name):\n        ''' \n        While the endpoint is being created or updated sleep for 60 seconds.\n        '''\n        # wait while creating or updating endpoint\n        status = smClient.describe_endpoint(EndpointName=endpoint_name)['EndpointStatus']\n        print('Status: %s' %status)\n        while status != 'InService' and status !='Failed':\n            time.sleep(60)\n            status = smClient.describe_endpoint(EndpointName=endpoint_name)['EndpointStatus']\n            print('Status: %s' %status)\n        \n        # in case of a deployment failure raise an error\n        if status == 'Failed':\n            raise ValueError('Endpoint failed to deploy')\n\nif __name__==&quot;__main__&quot;:\n    deployer = Deployer('churnmodel', deployRetrained=True)\n    deployer.deploy()\n<\/code><\/pre>\n<p>Final comments :<\/p>\n<ul>\n<li><p>The sagemaker <a href=\"https:\/\/docs.amazonaws.cn\/en_us\/sagemaker\/latest\/dg\/realtime-endpoints-deployment.html\" rel=\"nofollow noreferrer\">documentation<\/a> mentions all this but fails to state that you can provide an 'entry_point' to the <code>create_model<\/code> method as well as a 'source_dir' for inference dependencies (e.g. normalization artifacts). It can be done as seen in <code>PrimaryContainer<\/code> argument.<\/p>\n<\/li>\n<li><p>my <code>fileManager.py<\/code> script just contains basic functions to make tar files, upload and download to and from my S3 paths. To simplify the class, I have not included them in.<\/p>\n<\/li>\n<li><p>The method deleteOutdatedEndpointConfig may seem like an overkill with an unnecessary loop and checks, I do so because I have multiple endpoint configurations to handle and wanted to remove the ones that weren't live AND contain the string <code>name_check<\/code> (I do not know the exact name of the configuration since there is a datetime suffix). Feel free to simplify it or remove it all together if you feel like it.<\/p>\n<\/li>\n<\/ul>\n<p>Hope it helps.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1663925308150,
        "Solution_link_count":1.0,
        "Solution_readability":14.3,
        "Solution_reading_time":106.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":57.0,
        "Solution_word_count":924.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":19.2317730556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Sagemaker is a great tool to train your models, and we save some money by using AWS spot instances. However, training jobs sometimes get stopped in the middle. We are using some mechanisms to continue from the latest checkpoint after a restart. See also the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-checkpoints.html\" rel=\"nofollow noreferrer\">docs<\/a>.<\/p>\n<p>Still, how do you efficiently test such a mechanism? Can you trigger it yourself? Otherwise you have to wait until the spot instance actually \u00eds restarted.<\/p>\n<p>Also, are you expected to use the linked <code>checkpoint_s3_uri<\/code> argument or the <code>model_dir<\/code> for this? E.g. the <code>TensorFlow<\/code> estimator <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/sagemaker.tensorflow.html#tensorflow-estimator\" rel=\"nofollow noreferrer\">docs<\/a> seem to suggest something <code>model_dir<\/code>for checkpoints.<\/p>",
        "Challenge_closed_time":1616662041463,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616592352363,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1616592807080,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66782040",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":12.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":19.3580833334,
        "Challenge_title":"Reloading from checkpoing during AWS Sagemaker Training",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":286.0,
        "Challenge_word_count":110,
        "Platform":"Stack Overflow",
        "Poster_created_time":1484838464572,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Amsterdam, Nederland",
        "Poster_reputation_count":3937.0,
        "Poster_view_count":387.0,
        "Solution_body":"<p>Since you can't manually terminate a sagemaker instance, run an Amazon SageMaker Managed Spot training for a small number of epochs, Amazon SageMaker would have backed up your checkpoint files to S3. Check that checkpoints are there. Now run a second training run, but this time provide the first jobs\u2019 checkpoint location to <code>checkpoint_s3_uri<\/code>. Reference is <a href=\"https:\/\/towardsdatascience.com\/a-quick-guide-to-using-spot-instances-with-amazon-sagemaker-b9cfb3a44a68\" rel=\"nofollow noreferrer\">here<\/a>, this also answer your second question.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.8,
        "Solution_reading_time":7.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":42.8373255556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy a classification TensorFlow model on AZURE from GitHub. It is getting deployed correctly which can be seen from the below logs.  <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/111656-image.png?platform=QnA\" alt=\"111656-image.png\" \/><\/p>\n<p>But I'm getting an OSError on log Stream saying saved model doesn't exist as shown below. The error msg is highlighted in red.  <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/111560-image.png?platform=QnA\" alt=\"111560-image.png\" \/><\/p>\n<p>But this is working correctly on local. This model has been checked locally.  <br \/>\nThe repository for this can be checked at <a href=\"https:\/\/github.com\/Vikeshkr-DSP\/cassava-leaf-disease-prediction\">https:\/\/github.com\/Vikeshkr-DSP\/cassava-leaf-disease-prediction<\/a>.  <br \/>\nThanks in advance for your help.<\/p>",
        "Challenge_closed_time":1625582635012,
        "Challenge_comment_count":3,
        "Challenge_created_time":1625428420640,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/462250\/facing-problem-in-deplying-pyhton-application-from",
        "Challenge_link_count":3,
        "Challenge_participation_count":4,
        "Challenge_readability":11.7,
        "Challenge_reading_time":12.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":42.8373255556,
        "Challenge_title":"Facing problem in deplying pyhton application from github- unable to load tensorflow saved model in AZURE.",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":105,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The above issue on <code>OSError: SavedModel file does not exist at: cassava_leaf.h5\/{saved_model.pbtxt|saved_model.pb}<\/code> was due to a bit large size of model and we did not provide an absolute path to the model location, the application could not find the same during startup.  <\/p>\n<p>The issue resolved by manually transferring the h5-model file to a location like \/home on the App Service and updated the app.py file to use an absolute path in order to refer the file.   <\/p>\n<p>Similar to: <code>model=load_model('\/home\/cassava_leaf.h5')<\/code>  <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.9,
        "Solution_reading_time":7.02,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":82.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1254829817772,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":2595.0,
        "Answerer_view_count":357.0,
        "Challenge_adjusted_solved_time":197.5463266667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to implement a input_handler() in inference.py for a sagemaker inference container.<\/p>\n<p>The images\/arrays are very big (3D). So I want to pass in a S3 URI, then the input_handler() function should load the image\/array from s3 and return the actual numpy array for the model (which expects a tensor):<\/p>\n<pre><code>def input_handler(data, context):\n\n    d = data.read().decode('utf-8')\n\n    body = json.loads(d)\n    s3path = body['s3_path']\n\n    s3 = S3FileSystem()\n    df = np.load(s3.open(s3path))\n\n    return df\n<\/code><\/pre>\n<p>Returning a numpy array worked with the Sagemaker python api version &lt; 1.0 and input_fn(), but does not work with the new container used by sagemaker python api &gt; 2.0 that expects input_handler().<\/p>\n<p>The actual container image is &quot;763104351884.dkr.ecr.eu-central-1.amazonaws.com\/tensorflow-inference:1.15-gpu&quot;.<\/p>\n<p>During inference, I get the following error in CloudWatch thrown by the container:<\/p>\n<pre><code>ERROR:python_service:exception handling request: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(\n\nTraceback (most recent call last):\n  File &quot;\/sagemaker\/python_service.py&quot;, line 289, in _handle_invocation_post\n    res.body, res.content_type = self._handlers(data, context)\n  File &quot;\/sagemaker\/python_service.py&quot;, line 322, in handler\n    response = requests.post(context.rest_uri, data=processed_input)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/requests\/api.py&quot;, line 116, in post\n    return request('post', url, data=data, json=json, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/requests\/api.py&quot;, line 60, in request\n    return session.request(method=method, url=url, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/requests\/sessions.py&quot;, line 512, in request\n    data=data or \n{}\n,\n<\/code><\/pre>\n<p>What is the correct return type? All examples I found were for json &amp; text...<\/p>",
        "Challenge_closed_time":1600259722303,
        "Challenge_comment_count":3,
        "Challenge_created_time":1599496754993,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1599548555527,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63781356",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.6,
        "Challenge_reading_time":26.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":211.9353638889,
        "Challenge_title":"How to correctly write a sagemaker tensorflow input_handler() that returns a numpy array?",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":636.0,
        "Challenge_word_count":222,
        "Platform":"Stack Overflow",
        "Poster_created_time":1254829817772,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":2595.0,
        "Poster_view_count":357.0,
        "Solution_body":"<p>This seems to work:<\/p>\n<p><code>return json.dumps({&quot;inputs&quot;: df.tolist() }).<\/code><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":1.38,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":8.0,
        "Tool":"Amazon SageMaker"
    }
]