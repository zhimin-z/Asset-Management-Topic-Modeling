[
    {
        "Answerer_created_time":1416648155470,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":14749.0,
        "Answerer_view_count":968.0,
        "Challenge_adjusted_solved_time":7321.6952480556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The documentations of how to use SageMaker estimators are scattered around, sometimes obsolete, incorrect. Is there a one stop location which gives the comprehensive views of how to use SageMaker SDK Estimator to train and save models?<\/p>",
        "Challenge_closed_time":1630555307168,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630555307170,
        "Challenge_favorite_count":14.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in finding comprehensive and accurate documentation on how to use SageMaker SDK Estimator for model training and saving.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69024005",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":3.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":27.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"How to use SageMaker Estimator for model training and saving",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":6655.0,
        "Challenge_word_count":46,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416648155470,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":14749.0,
        "Poster_view_count":968.0,
        "Solution_body":"<h1>Answer<\/h1>\n<p>There is no one such resource from AWS that provides the comprehensive view of how to use SageMaker SDK Estimator to train and save models.<\/p>\n<h2>Alternative Overview Diagram<\/h2>\n<p>I put a diagram and brief explanation to get the overview on how SageMaker Estimator runs a training.<\/p>\n<ol>\n<li><p>SageMaker sets up a docker container for a training job where:<\/p>\n<ul>\n<li>Environment variables are set as in <a href=\"https:\/\/github.com\/aws\/sagemaker-containers#important-environment-variables\" rel=\"noreferrer\">SageMaker Docker Container. Environment Variables<\/a>.<\/li>\n<li>Training data is setup under <code>\/opt\/ml\/input\/data<\/code>.<\/li>\n<li>Training script codes are setup under <code>\/opt\/ml\/code<\/code>.<\/li>\n<li><code>\/opt\/ml\/model<\/code> and <code>\/opt\/ml\/output<\/code> directories are setup to store training outputs.<\/li>\n<\/ul>\n<\/li>\n<\/ol>\n<pre><code>\/opt\/ml\n\u251c\u2500\u2500 input\n\u2502   \u251c\u2500\u2500 config\n\u2502   \u2502   \u251c\u2500\u2500 hyperparameters.json  &lt;--- From Estimator hyperparameter arg\n\u2502   \u2502   \u2514\u2500\u2500 resourceConfig.json\n\u2502   \u2514\u2500\u2500 data\n\u2502       \u2514\u2500\u2500 &lt;channel_name&gt;        &lt;--- From Estimator fit method inputs arg\n\u2502           \u2514\u2500\u2500 &lt;input data&gt;\n\u251c\u2500\u2500 code\n\u2502   \u2514\u2500\u2500 &lt;code files&gt;              &lt;--- From Estimator src_dir arg\n\u251c\u2500\u2500 model\n\u2502   \u2514\u2500\u2500 &lt;model files&gt;             &lt;--- Location to save the trained model artifacts\n\u2514\u2500\u2500 output\n    \u2514\u2500\u2500 failure                   &lt;--- Training job failure logs\n<\/code><\/pre>\n<ol start=\"2\">\n<li><p>SageMaker Estimator <code>fit(inputs)<\/code> method executes the training script. Estimator <code>hyperparameters<\/code> and <code>fit<\/code> method <code>inputs<\/code> are provided as its command line arguments.<\/p>\n<\/li>\n<li><p>The training script saves the model artifacts in the <code>\/opt\/ml\/model<\/code> once the training is completed.<\/p>\n<\/li>\n<li><p>SageMaker archives the artifacts under <code>\/opt\/ml\/model<\/code> into <code>model.tar.gz<\/code> and save it to the S3 location specified to <code>output_path<\/code> Estimator parameter.<\/p>\n<\/li>\n<li><p>You can set Estimator <code>metric_definitions<\/code> parameter to extract model metrics from the training logs. Then you can monitor the training progress in the SageMaker console metrics.<\/p>\n<\/li>\n<\/ol>\n<p><a href=\"https:\/\/i.stack.imgur.com\/gi8bU.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gi8bU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I believe AWS needs to stop mass-producing verbose, redundant, wordy, scattered, and obsolete documents. AWS needs to understand <strong>A picture is worth thousand words<\/strong>.<\/p>\n<p>Have diagrams and piece document parts together in a <strong>context<\/strong> with a clear objective to achieve.<\/p>\n<hr \/>\n<h1>Problem<\/h1>\n<p>AWS documentations need serious re-design and re-structuring. Just to understand <strong>how to train and save a model<\/strong> forces us going through dozens of scattered,  fragmented, verbose, redundant documentations, which are often obsolete, incomplete, and sometime incorrect.<\/p>\n<p>It is well-summarized in <a href=\"https:\/\/nandovillalba.medium.com\/why-i-think-gcp-is-better-than-aws-ea78f9975bda\" rel=\"noreferrer\">Why I think GCP is better than AWS<\/a>:<\/p>\n<blockquote>\n<p>It\u2019s not that AWS is harder to use than GCP, it\u2019s that <strong>it is needlessly hard<\/strong>; a disjointed, sprawl of infrastructure primitives with poor cohesion between them.  <br><br>\nA challenge is nice, a confusing mess is not, and <strong>the problem with AWS is that a large part of your working hours will be spent untangling their documentation and weeding through features and products to find what you want<\/strong>, rather than focusing on cool interesting challenges.<\/p>\n<\/blockquote>\n<p>Especially the SageMaker team keeps changing implementations without updating documents. Its roll-out was also inconsistent, e.g. SDK version 2 was rolled out in the SageMaker Studio making the AWS examples in Github incompatible without announcing it. Whereas SageMaker instance still had SDK 1, hence code worked in Instance but not in Studio.<\/p>\n<p>It is mind-boggling that we have to go through these many documents below to understand how to use the SageMaker SDK Estimator for training.<\/p>\n<h2>Documents for Model Training<\/h2>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\" rel=\"noreferrer\">Train a Model with Amazon SageMaker<\/a><\/li>\n<\/ul>\n<p>This document gives 20,000 feet overview of how SageMaker training but does not give any clue what to do.<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker-workshop.com\/custom\/containers.html\" rel=\"noreferrer\">Running a container for Amazon SageMaker training<\/a><\/li>\n<\/ul>\n<p>This document gives an overview of how SageMaker training looks like. However, this is not up-to-date as it is based on <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"noreferrer\">SageMaker Containers<\/a> which is obsolete.<\/p>\n<blockquote>\n<p>WARNING: This package has been deprecated. Please use the SageMaker Training Toolkit for model training and the SageMaker Inference Toolkit for model serving.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model.html\" rel=\"noreferrer\">Step 4: Train a Model<\/a><\/li>\n<\/ul>\n<p>This document layouts the steps for training.<\/p>\n<blockquote>\n<p>The Amazon SageMaker Python SDK provides framework estimators and generic estimators to train your model while orchestrating the machine learning (ML) lifecycle accessing the SageMaker features for training and the AWS infrastructures<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#train-a-model-with-the-sagemaker-python-sdk\" rel=\"noreferrer\">Train a Model with the SageMaker Python SDK<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>To train a model by using the SageMaker Python SDK, you:<\/p>\n<ul>\n<li>Prepare a training script<\/li>\n<li>Create an estimator<\/li>\n<li>Call the fit method of the estimator<\/li>\n<\/ul>\n<\/blockquote>\n<p>Finally this document gives concrete steps and ideas. However still missing comprehensiv details about Environment Variables, Directory structure in the SageMaker docker container**, S3 for uploading code, placing data, S3 where the trained model is saved, etc.<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html\" rel=\"noreferrer\">Use TensorFlow with the SageMaker Python SDK<\/a><\/li>\n<\/ul>\n<p>This documents is focused on TensorFlow Estimator implementation steps. Use <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/frameworks\/tensorflow\/get_started_mnist_train.ipynb\" rel=\"noreferrer\">Training a Tensorflow Model on MNIST<\/a> Github example to accompany with to follow the actual implementation.<\/p>\n<h2>Documents for passing parameters and data locations<\/h2>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-inputdataconfig\" rel=\"noreferrer\">How Amazon SageMaker Provides Training Information<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>This section explains how SageMaker makes training information, such as training data, hyperparameters, and other configuration information, available to your Docker container.<\/p>\n<\/blockquote>\n<p>This document finally gives the idea of how parameters and data are passed around but again, not comprehensive.<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-containers#important-environment-variables\" rel=\"noreferrer\">SageMaker Docker Container Environment Variables<\/a><\/li>\n<\/ul>\n<p>This documentation is marked as <strong>deprecated<\/strong> but the only document which explains the SageMaker Environment Variables.<\/p>\n<blockquote>\n<h3>IMPORTANT ENVIRONMENT VARIABLES<\/h3>\n<ul>\n<li>SM_MODEL_DIR<\/li>\n<li>SM_CHANNELS<\/li>\n<li>SM_CHANNEL_{channel_name}<\/li>\n<li>SM_HPS<\/li>\n<li>SM_HP_{hyperparameter_name}<\/li>\n<li>SM_CURRENT_HOST<\/li>\n<li>SM_HOSTS<\/li>\n<li>SM_NUM_GPUS<\/li>\n<\/ul>\n<h3>List of provided environment variables by SageMaker Containers<\/h3>\n<ul>\n<li>SM_NUM_CPUS<\/li>\n<li>SM_LOG_LEVEL<\/li>\n<li>SM_NETWORK_INTERFACE_NAME<\/li>\n<li>SM_USER_ARGS<\/li>\n<li>SM_INPUT_DIR<\/li>\n<li>SM_INPUT_CONFIG_DIR<\/li>\n<li>SM_OUTPUT_DATA_DIR<\/li>\n<li>SM_RESOURCE_CONFIG<\/li>\n<li>SM_INPUT_DATA_CONFIG<\/li>\n<li>SM_TRAINING_ENV<\/li>\n<\/ul>\n<\/blockquote>\n<h2>Documents for SageMaker Docker Container Directory Structure<\/h2>\n<ul>\n<li><a href=\"https:\/\/sagemaker-workshop.com\/custom\/containers.html\" rel=\"noreferrer\">Running a container for Amazon SageMaker training<\/a><\/li>\n<\/ul>\n<pre><code>\/opt\/ml\n\u251c\u2500\u2500 input\n\u2502   \u251c\u2500\u2500 config\n\u2502   \u2502   \u251c\u2500\u2500 hyperparameters.json\n\u2502   \u2502   \u2514\u2500\u2500 resourceConfig.json\n\u2502   \u2514\u2500\u2500 data\n\u2502       \u2514\u2500\u2500 &lt;channel_name&gt;\n\u2502           \u2514\u2500\u2500 &lt;input data&gt;\n\u251c\u2500\u2500 model\n\u2502   \u2514\u2500\u2500 &lt;model files&gt;\n\u2514\u2500\u2500 output\n    \u2514\u2500\u2500 failure\n<\/code><\/pre>\n<p>This document explains the directory structure and purpose of each directory.<\/p>\n<blockquote>\n<h3>The input<\/h3>\n<ul>\n<li>\/opt\/ml\/input\/config contains information to control how your program runs. hyperparameters.json is a JSON-formatted dictionary of hyperparameter names to values. These values will always be strings, so you may need to convert them. resourceConfig.json is a JSON-formatted file that describes the network layout used for distributed training. Since scikit-learn doesn\u2019t support distributed training, we\u2019ll ignore it here.<\/li>\n<li>\/opt\/ml\/input\/data\/&lt;channel_name&gt;\/ (for File mode) contains the input data for that channel. The channels are created based on the call to CreateTrainingJob but it\u2019s generally important that channels match what the algorithm expects. The files for each channel will be copied from S3 to this directory, preserving the tree structure indicated by the S3 key structure.<\/li>\n<li>\/opt\/ml\/input\/data\/&lt;channel_name&gt;_&lt;epoch_number&gt; (for Pipe mode) is the pipe for a given epoch. Epochs start at zero and go up by one each time you read them. There is no limit to the number of epochs that you can run, but you must close each pipe before reading the next epoch.<\/li>\n<\/ul>\n<h3>The output<\/h3>\n<ul>\n<li>\/opt\/ml\/model\/ is the directory where you write the model that your algorithm generates. Your model can be in any format that you want. It can be a single file or a whole directory tree. SageMaker will package any files in this directory into a compressed tar archive file. This file will be available at the S3 location returned in the DescribeTrainingJob result.<\/li>\n<li>\/opt\/ml\/output is a directory where the algorithm can write a file failure that describes why the job failed. The contents of this file will be returned in the FailureReason field of the DescribeTrainingJob result. For jobs that succeed, there is no reason to write this file as it will be ignored.<\/li>\n<\/ul>\n<\/blockquote>\n<p>However, this is not up-to-date as it is based on <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"noreferrer\">SageMaker Containers<\/a> which is obsolete.<\/p>\n<h2>Documents for Model Saving<\/h2>\n<p>The information on where the trained model is saved and in what format are fundamentally missing. The training script needs to save the model under <code>\/opt\/ml\/model<\/code> and the format and sub-directory structure depend on the frameworks e,g TensorFlow, Pytorch. This is because SageMaker deployment uses the Framework dependent model-serving, e,g. TensorFlow Serving for TensorFlow framework.<\/p>\n<p>This is not clearly documented and causing confusions. The developer needs to specify which format to use and under which sub-directory to save.<\/p>\n<p>To use TensorFlow Estimator training and deployment:<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/aws_sagemaker_studio\/frameworks\/keras_pipe_mode_horovod\/keras_pipe_mode_horovod_cifar10.html#Deploy-the-trained-model\" rel=\"noreferrer\">Deploy the trained model<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>Because <strong>we\u2019re using TensorFlow Serving for deployment<\/strong>, our training script <strong>saves the model in TensorFlow\u2019s SavedModel format<\/strong>.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/frameworks\/tensorflow\/code\/train.py#L159-L166\" rel=\"noreferrer\">amazon-sagemaker-examples\/frameworks\/tensorflow\/code\/train.py <\/a><\/li>\n<\/ul>\n<pre><code>    # Save the model\n    # A version number is needed for the serving container\n    # to load the model\n    version = &quot;00000000&quot;\n    ckpt_dir = os.path.join(args.model_dir, version)\n    if not os.path.exists(ckpt_dir):\n        os.makedirs(ckpt_dir)\n    model.save(ckpt_dir)\n<\/code><\/pre>\n<p>The code is saving the model in <code>\/opt\/ml\/model\/00000000<\/code> because this is for TensorFlow serving.<\/p>\n<ul>\n<li><a href=\"https:\/\/www.tensorflow.org\/guide\/saved_model\" rel=\"noreferrer\">Using the SavedModel format<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>The save-path follows a convention used by TensorFlow Serving where the last path component (1\/ here) is a version number for your model - it allows tools like Tensorflow Serving to reason about the relative freshness.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/www.tensorflow.org\/tfx\/tutorials\/serving\/rest_simple#save_your_model\" rel=\"noreferrer\">Train and serve a TensorFlow model with TensorFlow Serving<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>To load our trained model into TensorFlow Serving we first need to save it in SavedModel format. This will create a protobuf file in a well-defined directory hierarchy, and will include a version number. TensorFlow Serving allows us to select which version of a model, or &quot;servable&quot; we want to use when we make inference requests. Each version will be exported to a different sub-directory under the given path.<\/p>\n<\/blockquote>\n<h2>Documents for API<\/h2>\n<p>Basically the SageMaker SDK Estimator implements the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html\" rel=\"noreferrer\">CreateTrainingJob<\/a> API for training part. Hence, better to understand how it is designed and what parameters need to be defined. Otherwise working on Estimators are like walking in the dark.<\/p>\n<hr \/>\n<h1>Example<\/h1>\n<h2>Jupyter Notebook<\/h2>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\nbucket = sagemaker_session.default_bucket()\n\nmetric_definitions = [\n    {&quot;Name&quot;: &quot;train:loss&quot;, &quot;Regex&quot;: &quot;.*loss: ([0-9\\\\.]+) - accuracy: [0-9\\\\.]+.*&quot;},\n    {&quot;Name&quot;: &quot;train:accuracy&quot;, &quot;Regex&quot;: &quot;.*loss: [0-9\\\\.]+ - accuracy: ([0-9\\\\.]+).*&quot;},\n    {\n        &quot;Name&quot;: &quot;validation:accuracy&quot;,\n        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: ([0-9\\\\.]+).*&quot;,\n    },\n    {\n        &quot;Name&quot;: &quot;validation:loss&quot;,\n        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: ([0-9\\\\.]+) - val_accuracy: [0-9\\\\.]+.*&quot;,\n    },\n    {\n        &quot;Name&quot;: &quot;sec\/sample&quot;,\n        &quot;Regex&quot;: &quot;.* - \\d+s (\\d+)[mu]s\/sample - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: [0-9\\\\.]+&quot;,\n    },\n]\n\nimport uuid\n\ncheckpoint_s3_prefix = &quot;checkpoints\/{}&quot;.format(str(uuid.uuid4()))\ncheckpoint_s3_uri = &quot;s3:\/\/{}\/{}\/&quot;.format(bucket, checkpoint_s3_prefix)\n\nfrom sagemaker.tensorflow import TensorFlow\n\n# --------------------------------------------------------------------------------\n# 'trainingJobName' msut satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}\n# --------------------------------------------------------------------------------\nbase_job_name = &quot;fashion-mnist&quot;\nhyperparameters = {\n    &quot;epochs&quot;: 2, \n    &quot;batch-size&quot;: 64\n}\nestimator = TensorFlow(\n    entry_point=&quot;fashion_mnist.py&quot;,\n    source_dir=&quot;src&quot;,\n    metric_definitions=metric_definitions,\n    hyperparameters=hyperparameters,\n    role=role,\n    input_mode='File',\n    framework_version=&quot;2.3.1&quot;,\n    py_version=&quot;py37&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    base_job_name=base_job_name,\n    checkpoint_s3_uri=checkpoint_s3_uri,\n    model_dir=False\n)\nestimator.fit()\n<\/code><\/pre>\n<h2>fashion_mnist.py<\/h2>\n<pre><code>import os\nimport argparse\nimport json\nimport multiprocessing\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nfrom tensorflow.keras import backend as K\n\nprint(&quot;TensorFlow version: {}&quot;.format(tf.__version__))\nprint(&quot;Eager execution is: {}&quot;.format(tf.executing_eagerly()))\nprint(&quot;Keras version: {}&quot;.format(tf.keras.__version__))\n\n\nimage_width = 28\nimage_height = 28\n\n\ndef load_data():\n    fashion_mnist = tf.keras.datasets.fashion_mnist\n    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n\n    number_of_classes = len(set(y_train))\n    print(&quot;number_of_classes&quot;, number_of_classes)\n\n    x_train = x_train \/ 255.0\n    x_test = x_test \/ 255.0\n    x_full = np.concatenate((x_train, x_test), axis=0)\n    print(x_full.shape)\n\n    print(type(x_train))\n    print(x_train.shape)\n    print(x_train.dtype)\n    print(y_train.shape)\n    print(y_train.dtype)\n\n    # ## Train\n    # * C: Convolution layer\n    # * P: Pooling layer\n    # * B: Batch normalization layer\n    # * F: Fully connected layer\n    # * O: Output fully connected softmax layer\n\n    # Reshape data based on channels first \/ channels last strategy.\n    # This is dependent on whether you use TF, Theano or CNTK as backend.\n    # Source: https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/mnist_cnn.py\n    if K.image_data_format() == 'channels_first':\n        x = x_train.reshape(x_train.shape[0], 1, image_width, image_height)\n        x_test = x_test.reshape(x_test.shape[0], 1, image_width, image_height)\n        input_shape = (1, image_width, image_height)\n    else:\n        x_train = x_train.reshape(x_train.shape[0], image_width, image_height, 1)\n        x_test = x_test.reshape(x_test.shape[0], image_width, image_height, 1)\n        input_shape = (image_width, image_height, 1)\n\n    return x_train, y_train, x_test, y_test, input_shape, number_of_classes\n\n# tensorboard --logdir=\/full_path_to_your_logs\n\nvalidation_split = 0.2\nverbosity = 1\nuse_multiprocessing = True\nworkers = multiprocessing.cpu_count()\n\n\ndef train(model, x, y, args):\n    # SavedModel Output\n    tensorflow_saved_model_path = os.path.join(args.model_dir, &quot;tensorflow\/saved_model\/0&quot;)\n    os.makedirs(tensorflow_saved_model_path, exist_ok=True)\n\n    # Tensorboard Logs\n    tensorboard_logs_path = os.path.join(args.model_dir, &quot;tensorboard\/&quot;)\n    os.makedirs(tensorboard_logs_path, exist_ok=True)\n\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n        log_dir=tensorboard_logs_path,\n        write_graph=True,\n        write_images=True,\n        histogram_freq=1,  # How often to log histogram visualizations\n        embeddings_freq=1,  # How often to log embedding visualizations\n        update_freq=&quot;epoch&quot;,\n    )  # How often to write logs (default: once per epoch)\n\n    model.compile(\n        optimizer='adam',\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\n        metrics=['accuracy']\n    )\n    history = model.fit(\n        x,\n        y,\n        shuffle=True,\n        batch_size=args.batch_size,\n        epochs=args.epochs,\n        validation_split=validation_split,\n        use_multiprocessing=use_multiprocessing,\n        workers=workers,\n        verbose=verbosity,\n        callbacks=[\n            tensorboard_callback\n        ]\n    )\n    return history\n\n\ndef create_model(input_shape, number_of_classes):\n    model = Sequential([\n        Conv2D(\n            name=&quot;conv01&quot;,\n            filters=32,\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=&quot;same&quot;,\n            activation='relu',\n            input_shape=input_shape\n        ),\n        MaxPooling2D(\n            name=&quot;pool01&quot;,\n            pool_size=(2, 2)\n        ),\n        Flatten(),  # 3D shape to 1D.\n        BatchNormalization(\n            name=&quot;batch_before_full01&quot;\n        ),\n        Dense(\n            name=&quot;full01&quot;,\n            units=300,\n            activation=&quot;relu&quot;\n        ),  # Fully connected layer\n        Dense(\n            name=&quot;output_softmax&quot;,\n            units=number_of_classes,\n            activation=&quot;softmax&quot;\n        )\n    ])\n    return model\n\n\ndef save_model(model, args):\n    # Save the model\n    # A version number is needed for the serving container\n    # to load the model\n    version = &quot;00000000&quot;\n    model_save_dir = os.path.join(args.model_dir, version)\n    if not os.path.exists(model_save_dir):\n        os.makedirs(model_save_dir)\n    print(f&quot;saving model at {model_save_dir}&quot;)\n    model.save(model_save_dir)\n\n\ndef parse_args():\n    # --------------------------------------------------------------------------------\n    # https:\/\/docs.python.org\/dev\/library\/argparse.html#dest\n    # --------------------------------------------------------------------------------\n    parser = argparse.ArgumentParser()\n\n    # --------------------------------------------------------------------------------\n    # hyperparameters Estimator argument are passed as command-line arguments to the script.\n    # --------------------------------------------------------------------------------\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch-size', type=int, default=64)\n\n    # \/opt\/ml\/model\n    # sagemaker.tensorflow.estimator.TensorFlow override 'model_dir'.\n    # See https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/\\\n    # sagemaker.tensorflow.html#sagemaker.tensorflow.estimator.TensorFlow\n    parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])\n\n    # \/opt\/ml\/output\n    parser.add_argument(&quot;--output_dir&quot;, type=str, default=os.environ[&quot;SM_OUTPUT_DIR&quot;])\n\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == &quot;__main__&quot;:\n    args = parse_args()\n    print(&quot;---------- key\/value args&quot;)\n    for key, value in vars(args).items():\n        print(f&quot;{key}:{value}&quot;)\n\n    x_train, y_train, x_test, y_test, input_shape, number_of_classes = load_data()\n    model = create_model(input_shape, number_of_classes)\n\n    history = train(model=model, x=x_train, y=y_train, args=args)\n    print(history)\n    \n    save_model(model, args)\n    results = model.evaluate(x_test, y_test, batch_size=100)\n    print(&quot;test loss, test accuracy:&quot;, results)\n<\/code><\/pre>\n<h2>SageMaker Console<\/h2>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ctcLy.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ctcLy.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h2>Notebook output<\/h2>\n<pre><code>2021-09-03 03:02:04 Starting - Starting the training job...\n2021-09-03 03:02:16 Starting - Launching requested ML instancesProfilerReport-1630638122: InProgress\n......\n2021-09-03 03:03:17 Starting - Preparing the instances for training.........\n2021-09-03 03:04:59 Downloading - Downloading input data\n2021-09-03 03:04:59 Training - Downloading the training image...\n2021-09-03 03:05:23 Training - Training image download completed. Training in progress.2021-09-03 03:05:23.966037: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n2021-09-03 03:05:23.969704: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n2021-09-03 03:05:24.118054: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n2021-09-03 03:05:26,842 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\n2021-09-03 03:05:26,852 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:27,734 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n\/usr\/local\/bin\/python3.7 -m pip install -r requirements.txt\nWARNING: You are using pip version 21.0.1; however, version 21.2.4 is available.\nYou should consider upgrading via the '\/usr\/local\/bin\/python3.7 -m pip install --upgrade pip' command.\n\n2021-09-03 03:05:29,028 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,045 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,062 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,072 sagemaker-training-toolkit INFO     Invoking user script\n\nTraining Env:\n\n{\n    &quot;additional_framework_parameters&quot;: {},\n    &quot;channel_input_dirs&quot;: {},\n    &quot;current_host&quot;: &quot;algo-1&quot;,\n    &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,\n    &quot;hosts&quot;: [\n        &quot;algo-1&quot;\n    ],\n    &quot;hyperparameters&quot;: {\n        &quot;batch-size&quot;: 64,\n        &quot;epochs&quot;: 2\n    },\n    &quot;input_config_dir&quot;: &quot;\/opt\/ml\/input\/config&quot;,\n    &quot;input_data_config&quot;: {},\n    &quot;input_dir&quot;: &quot;\/opt\/ml\/input&quot;,\n    &quot;is_master&quot;: true,\n    &quot;job_name&quot;: &quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,\n    &quot;log_level&quot;: 20,\n    &quot;master_hostname&quot;: &quot;algo-1&quot;,\n    &quot;model_dir&quot;: &quot;\/opt\/ml\/model&quot;,\n    &quot;module_dir&quot;: &quot;s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz&quot;,\n    &quot;module_name&quot;: &quot;fashion_mnist&quot;,\n    &quot;network_interface_name&quot;: &quot;eth0&quot;,\n    &quot;num_cpus&quot;: 4,\n    &quot;num_gpus&quot;: 0,\n    &quot;output_data_dir&quot;: &quot;\/opt\/ml\/output\/data&quot;,\n    &quot;output_dir&quot;: &quot;\/opt\/ml\/output&quot;,\n    &quot;output_intermediate_dir&quot;: &quot;\/opt\/ml\/output\/intermediate&quot;,\n    &quot;resource_config&quot;: {\n        &quot;current_host&quot;: &quot;algo-1&quot;,\n        &quot;hosts&quot;: [\n            &quot;algo-1&quot;\n        ],\n        &quot;network_interface_name&quot;: &quot;eth0&quot;\n    },\n    &quot;user_entry_point&quot;: &quot;fashion_mnist.py&quot;\n}\n\nEnvironment variables:\n\nSM_HOSTS=[&quot;algo-1&quot;]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={&quot;batch-size&quot;:64,&quot;epochs&quot;:2}\nSM_USER_ENTRY_POINT=fashion_mnist.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}\nSM_INPUT_DATA_CONFIG={}\nSM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\nSM_CHANNELS=[]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=fashion_mnist\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=\/opt\/ml\/input\nSM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\nSM_OUTPUT_DIR=\/opt\/ml\/output\nSM_NUM_CPUS=4\nSM_NUM_GPUS=0\nSM_MODEL_DIR=\/opt\/ml\/model\nSM_MODULE_DIR=s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz\nSM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;batch-size&quot;:64,&quot;epochs&quot;:2},&quot;input_config_dir&quot;:&quot;\/opt\/ml\/input\/config&quot;,&quot;input_data_config&quot;:{},&quot;input_dir&quot;:&quot;\/opt\/ml\/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1&quot;,&quot;model_dir&quot;:&quot;\/opt\/ml\/model&quot;,&quot;module_dir&quot;:&quot;s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;fashion_mnist&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:4,&quot;num_gpus&quot;:0,&quot;output_data_dir&quot;:&quot;\/opt\/ml\/output\/data&quot;,&quot;output_dir&quot;:&quot;\/opt\/ml\/output&quot;,&quot;output_intermediate_dir&quot;:&quot;\/opt\/ml\/output\/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;fashion_mnist.py&quot;}\nSM_USER_ARGS=[&quot;--batch-size&quot;,&quot;64&quot;,&quot;--epochs&quot;,&quot;2&quot;]\nSM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\nSM_HP_BATCH-SIZE=64\nSM_HP_EPOCHS=2\nPYTHONPATH=\/opt\/ml\/code:\/usr\/local\/bin:\/usr\/local\/lib\/python37.zip:\/usr\/local\/lib\/python3.7:\/usr\/local\/lib\/python3.7\/lib-dynload:\/usr\/local\/lib\/python3.7\/site-packages\n\nInvoking script with the following command:\n\n\/usr\/local\/bin\/python3.7 fashion_mnist.py --batch-size 64 --epochs 2\n\n\nTensorFlow version: 2.3.1\nEager execution is: True\nKeras version: 2.4.0\n---------- key\/value args\nepochs:2\nbatch_size:64\nmodel_dir:\/opt\/ml\/model\noutput_dir:\/opt\/ml\/output\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1656913410063,
        "Solution_link_count":25.0,
        "Solution_readability":17.3,
        "Solution_reading_time":377.84,
        "Solution_score_count":65.0,
        "Solution_sentence_count":201.0,
        "Solution_word_count":2374.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":18.8037608333,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have built an XGBoost model using Amazon Sagemaker, but I was unable to find anything which will help me interpret the model and validate if it has learned the right dependencies.<\/p>\n\n<p>Generally, we can see Feature Importance for XGBoost by get_fscore() function in the python API (<a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html\" rel=\"nofollow noreferrer\">https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html<\/a>) I see nothing of that sort in the sagemaker api(<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html<\/a>).<\/p>\n\n<p>I know I can build my own model and then deploy that using sagemaker but I am curious if anyone has faced this problem and how they overcame it.<\/p>\n\n<p>Thanks.<\/p>",
        "Challenge_closed_time":1555001695856,
        "Challenge_comment_count":0,
        "Challenge_created_time":1554934002317,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has built an XGBoost model using Amazon Sagemaker but is unable to find a way to interpret the model and validate if it has learned the right dependencies. They are specifically looking for a way to see the feature importance for XGBoost in Sagemaker, but have not found any function in the Sagemaker API to do so. The user is seeking advice on how to overcome this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55621967",
        "Challenge_link_count":4,
        "Challenge_participation_count":3,
        "Challenge_readability":14.7,
        "Challenge_reading_time":11.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":18.8037608333,
        "Challenge_title":"Feature Importance for XGBoost in Sagemaker",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3637.0,
        "Challenge_word_count":99,
        "Platform":"Stack Overflow",
        "Poster_created_time":1419327925267,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Mountain View, CA, USA",
        "Poster_reputation_count":123.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>SageMaker XGBoost currently does not provide interface to retrieve feature importance from the model. You can write some code to get the feature importance from the XGBoost model. You have to get the booster object artifacts from the model in S3 and then use the following snippet <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pickle as pkl\nimport xgboost\nbooster = pkl.load(open(model_file, 'rb'))\nbooster.get_score()\nbooster.get_fscore()\n<\/code><\/pre>\n\n<p>Refer <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html\" rel=\"nofollow noreferrer\">XGBoost doc<\/a> for methods to get feature importance from the Booster object such as <code>get_score()<\/code> or <code>get_fscore()<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.4,
        "Solution_reading_time":9.42,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":83.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":176.56633,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have successfully built a Sagemaker endpoint using a Tensorflow model. The pre and post processing is done inside &quot;inference.py&quot; which calls a handler function based on this tutorial: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html#how-to-implement-the-pre-and-or-post-processing-handler-s\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html#how-to-implement-the-pre-and-or-post-processing-handler-s<\/a><\/p>\n<p>My questions are:<\/p>\n<ul>\n<li>Which method is good for validating user input data within inference.py?<\/li>\n<li>If such validation tests fail (e.g. wrong data types or data not in allowed range, etc.), how is it possible to return appropriate error messages with status codes to the user?<\/li>\n<li>How is this compatible with the API gateway placed above the endpoint?<\/li>\n<\/ul>\n<p>Here is the structure of the inference.py with the desired validation check as a comment:<\/p>\n<pre><code>import json\nimport requests\n\n\ndef handler(data, context):\n    &quot;&quot;&quot;Handle request.\n    Args:\n        data (obj): the request data\n        context (Context): an object containing request and configuration details\n    Returns:\n        (bytes, string): data to return to client, (optional) response content type\n    &quot;&quot;&quot;\n    processed_input = _process_input(data, context)\n    response = requests.post(context.rest_uri, data=processed_input)\n    return _process_output(response, context)\n\n\ndef _process_input(data, context):\n    if context.request_content_type == 'application\/json':\n        # pass through json (assumes it's correctly formed)\n        d = data.read().decode('utf-8')\n        data_dict = json.loads(data)\n\n\n        # -----&gt;   if data_dict['input_1'] &gt; 25000:\n        # -----&gt;       return some error specific message with status code 123\n\n\n        return some_preprocessing_function(data_dict)\n\n    raise ValueError('{{&quot;error&quot;: &quot;unsupported content type {}&quot;}}'.format(\n        context.request_content_type or &quot;unknown&quot;))\n\n\ndef _process_output(data, context):\n    if data.status_code != 200:\n        raise ValueError(data.content.decode('utf-8'))\n\n    response_content_type = context.accept_header\n    prediction = data.content\n    return prediction, response_content_type\n<\/code><\/pre>",
        "Challenge_closed_time":1645567161520,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644929807890,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user has successfully built a Sagemaker endpoint using a Tensorflow model and is now seeking guidance on how to validate user input data within inference.py, return appropriate error messages with status codes to the user if validation tests fail, and how to make it compatible with the API gateway placed above the endpoint. The user has provided the structure of the inference.py with the desired validation check as a comment.",
        "Challenge_last_edit_time":1644931522732,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71126832",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.6,
        "Challenge_reading_time":30.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":177.042675,
        "Challenge_title":"Amazon Sagemaker: User Input data validation in Inference Endpoint",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":245.0,
        "Challenge_word_count":220,
        "Platform":"Stack Overflow",
        "Poster_created_time":1613661928947,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":160.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>I will answer your questions inline below:<\/p>\n<ol>\n<li><em>Which method is good for validating user input data within inference.py?<\/em><\/li>\n<\/ol>\n<p>Seeing that you have a <code>handler<\/code> function, <code>input_handler<\/code> and <code>output_handler<\/code> are ignored. Thus, inside your <code>handler<\/code> function (as you are correctly doing) you can have the validation logic.<\/p>\n<ol start=\"2\">\n<li><em>If such validation tests fail (e.g. wrong data types or data not in allowed range, etc.), how is it possible to return appropriate error messages with status codes to the user?<\/em><\/li>\n<\/ol>\n<p>I like to think of my SageMaker endpoint as a web server. Thus, you can return any valid HTTP response code with a response message. Please see this example <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_batch_transform\/tensorflow_cifar-10_with_inference_script\/code\/inference.py#L47\" rel=\"nofollow noreferrer\">inference.py<\/a> file that I found as a reference.<\/p>\n<pre><code>_return_error(\n            415, 'Unsupported content type &quot;{}&quot;'.format(context.request_content_type or &quot;Unknown&quot;)\n        )\n\ndef _return_error(code, message):\n    raise ValueError(&quot;Error: {}, {}&quot;.format(str(code), message))\n<\/code><\/pre>\n<ol start=\"3\">\n<li><em>How is this compatible with the API gateway placed above the endpoint?<\/em><\/li>\n<\/ol>\n<p>Please see this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">link<\/a> for details on Creating a machine learning-powered REST API with Amazon API Gateway mapping templates and Amazon SageMaker.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.5,
        "Solution_reading_time":22.68,
        "Solution_score_count":1.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":178.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1431724059856,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":66.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":412.5112844445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Im trying to deploy a web app that takes 1 web input, then \"Set Column In Dataset\" a few times for each model , and then sends out a web output for each model.  <\/p>\n\n<p>Right now the way I have it setup is I have a few web inputs, then a model that runs for each, and then a web output for each.  It works for now, but it's a hassle because every time I want to add a new model to be predicted I have to add a bunch of stuff in both azure and my web application.  Just wondering if there is an easier way I'm missing.  <\/p>",
        "Challenge_closed_time":1467051292427,
        "Challenge_comment_count":0,
        "Challenge_created_time":1465566251803,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while deploying a web app on Azure Machine Learning that takes one web input and generates multiple web outputs after running multiple models using \"Set Column In Dataset\" for each model. The current setup involves multiple web inputs, models, and web outputs, which becomes a hassle when adding new models. The user is seeking an easier way to accomplish this task.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37749862",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":6.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":412.5112844445,
        "Challenge_title":"Azure Machine Learning, 1 web input with multiple outputs?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":492.0,
        "Challenge_word_count":118,
        "Platform":"Stack Overflow",
        "Poster_created_time":1365606882047,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":810.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>I am not quite sure I understand the workflow you described. Can you provide more details on what are you trying to accomplish with your web app and your experiment? For example, what do you mean when you say \"I have to add a bunch of stuff\"?<\/p>\n\n<p>Azure ML does support multiple web service inputs and outputs. Adding a new model to the experiment requires you to re-deploy your web service.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":4.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":71.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":2.4873480555,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I estimated a factorization machine model in sagemaker and it saved a file <code>model.tar.gz<\/code> into an s3 folder.<\/p>\n\n<p>Is there a way I can load this file in Python and access the parameter of the model, i.e. the factors, directly?<\/p>\n\n<p>Thanks<\/p>",
        "Challenge_closed_time":1575844058683,
        "Challenge_comment_count":0,
        "Challenge_created_time":1575827808930,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a factorization machine model in sagemaker and saved it as a file in an s3 folder. They are seeking guidance on how to load the file in Python and access the model's parameters.",
        "Challenge_last_edit_time":1575835104230,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59238265",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":3.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":4.5138202778,
        "Challenge_title":"sagemaker - factorization machines - deserialize model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":164.0,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Poster_created_time":1456487654208,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berlin, Germany",
        "Poster_reputation_count":1464.0,
        "Poster_view_count":62.0,
        "Solution_body":"<p>As of April 2019: yes. An official AWS blog post was created to show how to open the SageMaker Factorization Machines artifact and extract its parameters: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/extending-amazon-sagemaker-factorization-machines-algorithm-to-predict-top-x-recommendations\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/extending-amazon-sagemaker-factorization-machines-algorithm-to-predict-top-x-recommendations\/<\/a><\/p>\n\n<p>That being said, be aware that Amazon SageMaker built-in algorithm are primarily built for deployment on AWS, and only <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"nofollow noreferrer\">SageMaker XGBoost<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/blazingtext.html\" rel=\"nofollow noreferrer\">SageMaker BlazingText<\/a> are designed to produce artifacts interoperable with their open-source equivalent.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":26.7,
        "Solution_reading_time":12.79,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":199.5710802778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi.  <\/p>\n<p>I am working on an ML model in Designer.  <\/p>\n<p>I have a dataset of c. 55,000 rows.   <\/p>\n<p>When I add an &quot;ID&quot; column (unique per row - so 55,000 IDs) to my dataset for training \/ scoring, I receive the error message:  <\/p>\n<blockquote>\n<p>ModuleExceptionMessage:ColumnUniqueValuesExceeded: Number of unique values in column: &quot;ID&quot; is greater than allowed.  <\/p>\n<\/blockquote>\n<p><strong>Question:<\/strong> is this error based on a physical cap on number of rows - or capacity based on e.g. Compute power associated with the instance?  <\/p>\n<p>I can run 20k rows through the model <em>without<\/em> the ID column - so it seems the unique rows is the challenge.  <\/p>\n<p>But then - how do I keep an identifying column in the scored dataset, if there is a cap on unique values?   <\/p>\n<p>Because I need the ID column to join with other data that is not able to be used in modelling as features etc.   <\/p>\n<p>Any guidance welcome! <\/p>",
        "Challenge_closed_time":1605654456912,
        "Challenge_comment_count":6,
        "Challenge_created_time":1604936001023,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error message in Azure ML Designer when adding an \"ID\" column to a dataset for training\/scoring. The error message states that the number of unique values in the column exceeds the allowed limit. The user is unsure if this error is due to a physical cap on the number of rows or capacity based on compute power. The user needs the ID column to join with other data that cannot be used in modeling as features.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/156534\/azure-ml-id-column-for-joining-data-returns-no-of",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":6.5,
        "Challenge_reading_time":12.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":199.5710802778,
        "Challenge_title":"Azure ML: ID column for joining data returns \"No. Of unique values ... is greater than allowed\"",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":172,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>User can use Edit Metadata module to mark the ID column as &quot;ClearFeature&quot;, and thus this will not be used in Train Model. This should prevent the error. Please have a try and let me know if there is any questions. <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/edit-metadata\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/edit-metadata<\/a>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/40390-microsoftteams-image-7.png?platform=QnA\" alt=\"40390-microsoftteams-image-7.png\" \/>    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.8,
        "Solution_reading_time":8.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":52.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.3237788889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to register a data set via the Azure Machine Learning Studio designer but keep getting an error. Here is my code, used in a &quot;Execute Python Script&quot; module:<\/p>\n<pre><code>import pandas as pd\nfrom azureml.core.dataset import Dataset\nfrom azureml.core import Workspace\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    ws = Workspace.get(name = &lt;my_workspace_name&gt;, subscription_id = &lt;my_id&gt;, resource_group = &lt;my_RG&gt;)\n    ds = Dataset.from_pandas_dataframe(dataframe1)\n    ds.register(workspace = ws,\n                name = &quot;data set name&quot;,\n                description = &quot;example description&quot;,\n                create_new_version = True)\n    return dataframe1, \n<\/code><\/pre>\n<p>But I get the following error in the Workspace.get line:<\/p>\n<pre><code>Authentication Exception: Unknown error occurred during authentication. Error detail: Unexpected polling state code_expired.\n<\/code><\/pre>\n<p>Since I am inside the workspace and in the designer, I do not usually need to do any kind of authentication (or even reference the workspace). Can anybody offer some direction? Thanks!<\/p>",
        "Challenge_closed_time":1628038438487,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628037272883,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to register a data set via the Azure Machine Learning Studio designer. The error occurs in the Workspace.get line and states \"Authentication Exception: Unknown error occurred during authentication. Error detail: Unexpected polling state code_expired.\" The user is unsure why authentication is needed since they are already inside the workspace and in the designer.",
        "Challenge_last_edit_time":1628038626927,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68644137",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":14.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.3237788889,
        "Challenge_title":"Azure Machine Learning Studio Designer Error: code_expired",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":321.0,
        "Challenge_word_count":133,
        "Platform":"Stack Overflow",
        "Poster_created_time":1371499229816,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1111.0,
        "Poster_view_count":191.0,
        "Solution_body":"<p>when you're inside a &quot;Execute Python Script&quot; module or <code>PythonScriptStep<\/code>, the authentication for fetching the workspace is already done for you (unless you're trying to authenticate to different Azure ML workspace.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Run\nrun = Run.get_context()\n\nws = run.experiment.workspace\n<\/code><\/pre>\n<p>You should be able to use that <code>ws<\/code> object to register a Dataset.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":6.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":55.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1362025964372,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":78.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":9.3757052778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm following the <a href=\"https:\/\/aws.amazon.com\/getting-started\/hands-on\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"nofollow noreferrer\">AWS Sagemaker tutorial<\/a>, but I think there's an error in the step 4a. Particularly, at line 3 I'm instructed to type:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>     s3_input_train = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/train'.format(bucket_name, prefix), content_type='csv')\n<\/code><\/pre>\n<p>and I get the error<\/p>\n<pre><code>----&gt; 3 s3_input_train = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/train'.format(bucket_name, prefix), content_type='csv')\n\nAttributeError: module 'sagemaker' has no attribute 's3_input'\n<\/code><\/pre>\n<p>Indeed, using <code>dir<\/code> shows that sagemaker has no attribute called s3_input. How can fix this so that I can keep advancing in the tutorial? I tried using <code>session.inputs<\/code>, but this redirects me to a page saying that <code>session<\/code> is deprecated and suggesting that I use <code>sagemaker.inputs.TrainingInput<\/code> instead of <code>sagemaker.s3_inputs<\/code>. Is this a good way of going forward?<\/p>\n<p>Thanks everyone for the help and patience!<\/p>",
        "Challenge_closed_time":1609450565407,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609421425757,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is following the AWS Sagemaker tutorial and encountering an AttributeError in step 4a where the sagemaker.s3_input() function is called. The error message indicates that the sagemaker module has no attribute called s3_input. The user is seeking help to fix this issue and is considering using sagemaker.inputs.TrainingInput instead of sagemaker.s3_inputs.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65521556",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":16.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":8.0943472222,
        "Challenge_title":"Does the sagemaker official tutorial generate an AttributeError, and how to solve it?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":625.0,
        "Challenge_word_count":130,
        "Platform":"Stack Overflow",
        "Poster_created_time":1349183356160,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Oxford, UK",
        "Poster_reputation_count":3803.0,
        "Poster_view_count":240.0,
        "Solution_body":"<p>Using <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/utility\/inputs.html#inputs\" rel=\"noreferrer\"><code>sagemaker.inputs.TrainingInput<\/code><\/a> instead of <code>sagemaker.s3_inputs<\/code> worked to get that code cell functioning. It is an appropriate solution, though there may be another approach.<\/p>\n<p>Step 4.b also had code which needed updating<\/p>\n<pre><code>sess = sagemaker.Session()\nxgb = sagemaker.estimator.Estimator(containers[my_region],role, train_instance_count=1, train_instance_type='ml.m4.xlarge',output_path='s3:\/\/{}\/{}\/output'.format(bucket_name, prefix),sagemaker_session=sess)\nxgb.set_hyperparameters(max_depth=5,eta=0.2,gamma=4,min_child_weight=6,subsample=0.8,silent=0,objective='binary:logistic',num_round=100)\n<\/code><\/pre>\n<p>uses parameters <code>train_instance_count<\/code> and <code>train_instance_type<\/code> which have been changed in a later version (<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html#parameter-and-class-name-changes\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html#parameter-and-class-name-changes<\/a>).<\/p>\n<p>Making these changes resolved the errors for the tutorial using <code>conda_python3<\/code> kernel.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1609455178296,
        "Solution_link_count":3.0,
        "Solution_readability":28.2,
        "Solution_reading_time":16.72,
        "Solution_score_count":6.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":220.4830555556,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nI try to reproduce the minimal example from the Docs: a Kedro project using the starter `pandas-iris` using the `kedro-mlflow` functinality. I do not arrive at initializing the kedro-mlflow project, since the cli commands are not available.\r\n\r\n## Context\r\n\r\nIt is unclear to me if this is connected to #157 \r\nI wanted to start looking into kedro-mlflow, but got immediatle blocked by the initialization of the project. Therefore any advice on where to look to fix this would also be appreciated. \r\n\r\n## Steps to Reproduce\r\n\r\n```\r\nconda create -n kedro_mlflow python=3.8\r\nconda activate kedro_mlflow\r\npip install kedro-mlflow\r\nkedro mlflow -h\r\nkedro new --starter=pandas-iris\r\ncd mlflow_test\/\r\nkedro mlflow -h\r\n> ERROR \"No such command 'mlflow'\"\r\n```\r\n\r\n## Expected Result\r\n\r\n`kedro mlflow` is available in a project directory, i.e. `kedro mlflow -h` gives the same output inside the folder as before\r\n\r\n## Actual Result\r\n\r\ninside the project folder the `mlflow` command is unknown to Kedro\r\n\r\n```\r\n...\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/pkg_resources\/__init__.py:1130: DeprecationWarning: Use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\r\n  return get_provider(package_or_requirement).get_resource_filename(\r\n....\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/mlflow\/types\/schema.py:49: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \r\nDeprecated in NumPy 1.20; for more details and guidance: https:\/\/numpy.org\/devdocs\/release\/1.20.0-notes.html#deprecations\r\n  binary = (7, np.dtype(\"bytes\"), \"BinaryType\", np.object)\r\n2021-04-23 17:49:52,197 - root - INFO - Registered hooks from 2 installed plugin(s): kedro-mlflow-0.7.1\r\nUsage: kedro [OPTIONS] COMMAND [ARGS]...\r\nTry 'kedro -h' for help.\r\n\r\nError: No such command 'mlflow'.\r\n\r\n```\r\n\r\n## Your Environment\r\n\r\nUbuntu 18.04.5\r\n\r\n- Kedro 0.17.3\r\n- kedro-mlflow 0.7.1\r\n- python 3.8.8.\r\n- mlflow 1.15.0\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nyes",
        "Challenge_closed_time":1619987466000,
        "Challenge_comment_count":6,
        "Challenge_created_time":1619193727000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to load a previously saved KedroPipelineModel from mlflow due to a \"cannot pickle context artifacts\" error caused by a non-deepcopyable dataset (in this case, a keras tokenizer). The issue occurs with kedro and kedro-mlflow versions 0.16.5 and 0.4.0, and the bug also happens with the latest version on develop. A potential solution involves modifying a line of code in the kedro_mlflow package.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/193",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":8.4,
        "Challenge_reading_time":27.15,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":220.4830555556,
        "Challenge_title":"kedro-mlflow CLI is unavailable inside a Kedro project",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":273,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi, \r\n\r\nI wil try to check it out this weekend, but the `kedro==0.17.3` version is brand new (it was released yesterday), and given my experience with past kedro versions update 2 things might have happened on kedro's side: \r\n- They have broken the auto-discovery mechanism (I've seen in the release note that they change the CLI command discovery to enale overriding project commands by plugins)\r\n- They have not updated their `pandas-iris` starter yet which does not match the new version and is only compliant with `kedro==0.17.2`. \r\n\r\nWhile I am investigating, would you please confirm that :\r\n- `kedro-mlflow` works fine with kedro==0.17.2 with your setup\r\n- `kedro-mlflow` works fine if you don't use the `pandas-iris` starter: try `kedro new` with `kedro==0.17.3` and then add one ode to test the plugin\r\n- I'd be glad to see if another plugin (e.g. `kedro-viz`) is facing the same problem that kedro-mlflow. Would you mind checking?\r\n\r\nOf course there is the possibility that the problem comes from `kedro-mlflow` itself, but I hardly believe it. I'll tell you within 2 days. I am sorry, I am quite busy for now and I will not debug this before next week. Once again, it is very likely kedro's plugin discovery mechanism has been broken in the new release, I strongly suggest you go back to `kedro==0.17.2`.\r\n\r\nNext actions: \r\n- [X] reproduce the bug -> Done, thanks for the very good reproducible example\r\n- [X] Check if it happens with other plugins (say kedro-viz) -> `kedro viz` global command is properly discovered\r\n- [X] Check if hooks are properly loaded -> everything works fine if I add a `mlflow.yml` manually in the `conf\/local` folder (or any folder in `conf\/` actually). -> **This is a short term solution for you**,e ven if it is not very convenient. You can find allowed keys [in the documentation](https:\/\/kedro-mlflow.readthedocs.io\/en\/latest\/source\/04_experimentation_tracking\/01_configuration.html#the-mlflow-yml-file) or irectly [copy paste it from the code](https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/master\/kedro_mlflow\/template\/project\/mlflow.yml)\r\n- [X] Check if the tests pass with kedro==0.17.3 -> *Some tests are failing, but not the one related to the CLI commands which seems discovered. I need to investigate further*.\r\n- [x] Check if other plugins with *local* commands are discovered\r\n- [x] Check if it also happens it an empty project (i.e. *not* a starter)\r\n First of all, thank you for looking so quickly into it!\r\n\r\nFrom how I read your second message you already know that, but to answer your questions:\r\n- detecting `kedro mlflow` works fine with `kedro==0.17.2`\r\n- the problem is consistent with kedro==0.17.3 independent if I use the pandas-iris starter or not\r\n- `kedro viz` is found also with `kedro=0.17.3`\r\n\r\nAgain, thank you for providing workarounds directly on Monday morning, I can nicely work with those! A question for my understanding of the plugin: As long as the hooks are loaded, the mlflow functionality depends only on a `mlflow.yml` to be present, and all that `kedro mlflow init` does is copy this file from the template into `conf\/local`, is this correct? TL;DR: \r\n\r\nInstall this version for now, it should make the command available again:\r\n\r\n```console\r\npip uninstall kedro-mlflow\r\npip install git+https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow.git@bug\/no-cli\r\n```\r\n**Beware:** it is very important to uninstall your existing version of kedro-mlflow before reinstalling because the patch has the same version number that the current release.\r\n\r\nIf you confirm this works for you, I will deploy the patch to PyPI before kedro provides a patch on their side.\r\n_____________________________\r\n\r\nHi, some follow-up about this bug:\r\n\r\n- I've figured out *what* is going on but not *why* it happens. The `mlflow` group of command exists both at global (`new`) and project (`init`, `ui`) levels and for an unknown reason, `kedro` takes into account only one group of command in its `0.17.3` version. This is a bug I will report to the core team. However, it does not affect their other plugins (kedro-viz, kedro-docker, kedro-airflow) because none of them has both global and project commands.\r\n- The quickest (hacky) fix is to remove the global group of command to the make the project ones available. I've done this in the branch `bug\/no-cli` of the repo.\r\n\r\nTo answer your question: \r\n\r\n> A question for my understanding of the plugin: As long as the hooks are loaded, the mlflow functionality depends only on a mlflow.yml to be present, and all that kedro mlflow init does is copy this file from the template into conf\/local, is this correct?\r\n\r\nExactly: the `init` command renders the template (i.e. copy paste it + replace the jinja tags with dynamic values like the name of your project) to a folder in your `conf\/` folder (by default `local`, but you can specify an environment like this: `kedro mlflow init --env=<your-env-folder>`). The hooks contain all the code logic  and this mlflow.yml file is just here to pass parameters to them. \r\n\r\nThe other project command is `kedro mlflow ui` which is just a wrapper of \"mlflow ui\" with the parameters (mlflow_tracking_uri, port, host) defined in your `mlflow.yml` file.\r\n thanks, form a quick test I would say: the patch works like a charm! Hi @dmb23, I've just deployed the patch to PyPI. You can use `pip install kedro_mlflow==0.7.2`` and it should be ok for now. I close the issue, but feel free to reopen if you still encounter any issue in this new version.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":7.8,
        "Solution_reading_time":66.71,
        "Solution_score_count":null,
        "Solution_sentence_count":48.0,
        "Solution_word_count":849.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1546969667040,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":1689.0,
        "Answerer_view_count":170.0,
        "Challenge_adjusted_solved_time":3.5946425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am aware that it is better to use aws Rekognition for this. However, it does not seem to work well when I tried it out with the images I have (which are sort of like small containers with labels on them). The text comes out misspelled and fragmented.<\/p>\n\n<p>I am new to ML and sagemaker. From what I have seen, the use cases seem to be for prediction and image classification. I could not find one on training a model for detecting text in an image. Is it possible to to do it with Sagemaker? I would appreciate it if someone pointed me in the right direction.<\/p>",
        "Challenge_closed_time":1549313202943,
        "Challenge_comment_count":0,
        "Challenge_created_time":1549300262230,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges with using AWS Rekognition to detect text in images, as the text comes out misspelled and fragmented. They are new to ML and Sagemaker and are looking for guidance on whether it is possible to train a model for detecting text in an image using Sagemaker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54521080",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.9,
        "Challenge_reading_time":7.35,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":3.5946425,
        "Challenge_title":"aws sagemaker for detecting text in an image",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2255.0,
        "Challenge_word_count":115,
        "Platform":"Stack Overflow",
        "Poster_created_time":1412300825550,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"West Lafayette, IN, United States",
        "Poster_reputation_count":463.0,
        "Poster_view_count":111.0,
        "Solution_body":"<p>The different services will all provide different levels of abstraction for Optical Character Recognition (OCR) depending on what parts of the pipeline you are most comfortable with working with, and what you prefer to have abstracted.<\/p>\n\n<p>Here are a few options:<\/p>\n\n<ul>\n<li><p><strong>Rekognition<\/strong> will provide out of the box OCR with the <a href=\"https:\/\/docs.aws.amazon.com\/rekognition\/latest\/dg\/text-detecting-text-procedure.html\" rel=\"nofollow noreferrer\">DetectText<\/a> feature. However, it seems you will need to perform some sort of pre-processing on your images in your current case in order to get better results. This can be done through any method of your choice (Lambda, EC2, etc).<\/p><\/li>\n<li><p><strong>SageMaker<\/strong> is a tool that will enable you to easily train and deploy your own models (of any type). You have two primary options with SageMaker:<\/p>\n\n<ol>\n<li><p><em>Do-it-yourself option<\/em>: If you're looking to go the route of labeling your own data, gathering a sizable training set, and training your own OCR model, this is possible by training and deploying your own model via SageMaker.<\/p><\/li>\n<li><p><em>Existing OCR algorithm<\/em>: There are many algorithms out there that all have different potential tradeoffs for OCR. One example would be <a href=\"https:\/\/github.com\/tesseract-ocr\/tesseract\" rel=\"nofollow noreferrer\">Tesseract<\/a>. Using this, you can more closely couple your pre-processing step to the text detection.<\/p><\/li>\n<\/ol><\/li>\n<li><p><a href=\"https:\/\/aws.amazon.com\/textract\/\" rel=\"nofollow noreferrer\"><strong>Amazon Textract<\/strong><\/a> (In preview) is a purpose-built dedicated OCR service that may offer better performance depending on what your images look like and the settings you choose. <\/p><\/li>\n<\/ul>\n\n<p>I would personally recommend looking into <a href=\"https:\/\/docparser.com\/blog\/improve-ocr-accuracy\/\" rel=\"nofollow noreferrer\">pre-processing for OCR<\/a> to see if it improves Rekognition accuracy before moving onto the other options. Even if it doesn't improve Rekognition's accuracy, it will still be valuable for most of the other options!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":12.3,
        "Solution_reading_time":27.32,
        "Solution_score_count":4.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":274.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1452222077950,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"China",
        "Answerer_reputation_count":2187.0,
        "Answerer_view_count":894.0,
        "Challenge_adjusted_solved_time":12.8401833334,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I use next command to save output results:<\/p>\n\n<pre><code>ws.datasets.add_from_dataframe(data, 'GenericCSV', 'output.csv', 'Uotput results')\n<\/code><\/pre>\n\n<p>where <code>ws<\/code> is <code>azureml.Workspace<\/code> object and <code>data<\/code> is <code>pandas.DataFrame<\/code>.<\/p>\n\n<p>It works fine if my dataset size less than 4 mb. Otherwise I got a error:<\/p>\n\n<pre><code>AzureMLHttpError: Maximum request length exceeded.\n<\/code><\/pre>\n\n<p>As I understood this is the error raised by Azure environment limits and the maximum size of the dataset could not be changed. <\/p>\n\n<p>I could split my dataset to 4 mb parts and download them from Azure ML studio, but it is very inconvinient if size of my output dataset is more than 400 mb.<\/p>",
        "Challenge_closed_time":1457935312407,
        "Challenge_comment_count":3,
        "Challenge_created_time":1457889087747,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to save a dataset from an ipython notebook in Azure ML Studio using a command that works fine for datasets less than 4 mb, but encounters an error \"Maximum request length exceeded\" for larger datasets. The user is unable to change the maximum size limit and splitting the dataset into smaller parts is inconvenient for larger datasets.",
        "Challenge_last_edit_time":1457963855452,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35973168",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":6.9,
        "Challenge_reading_time":10.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":12.8401833334,
        "Challenge_title":"How could I save dataset from ipython notebook in Azure ML Studio?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3128.0,
        "Challenge_word_count":112,
        "Platform":"Stack Overflow",
        "Poster_created_time":1452426675696,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":77.0,
        "Poster_view_count":21.0,
        "Solution_body":"<p>I have read the source code in the python package <strong>azureml<\/strong>, and found out that they are using a simple request post when uploading a dataset, which has a limited content length 4194304 bytes.<\/p>\n\n<p>I tried to modify the code inside \"http.py\" within the python package <strong>azureml<\/strong>. I posted the request with a chunked data, and I got the following error:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \".\\azuremltest.py\", line 10, in &lt;module&gt;\n    ws.datasets.add_from_dataframe(frame, 'GenericCSV', 'output2.csv', 'Uotput results')\n  File \"C:\\Python34\\lib\\site-packages\\azureml\\__init__.py\", line 507, in add_from_dataframe\n    return self._upload(raw_data, data_type_id, name, description)\n  File \"C:\\Python34\\lib\\site-packages\\azureml\\__init__.py\", line 550, in _upload\nraw_data, None)\n  File \"C:\\Python34\\lib\\site-packages\\azureml\\http.py\", line 135, in upload_dataset\n    upload_result = self._send_post_req(api_path, raw_data)\n  File \"C:\\Python34\\lib\\site-packages\\azureml\\http.py\", line 197, in _send_post_req\n    raise AzureMLHttpError(response.text, response.status_code)\nazureml.errors.AzureMLHttpError: Chunked transfer encoding is not permitted. Upload size must be indicated in the Content-Length header.\nRequest ID: 7b692d82-845c-4106-b8ec-896a91ecdf2d 2016-03-14 04:32:55Z\n<\/code><\/pre>\n\n<p>The REST API in <strong>azureml<\/strong> package does not support chunked transfer encoding. Hence, I took a look at how the Azure ML studio implements this, and I found out this:<\/p>\n\n<ol>\n<li><p>It post a request with content-length=0 to <code>https:\/\/studioapi.azureml.net\/api\/resourceuploads\/workspaces\/&lt;workspace_id&gt;\/?userStorage=true&amp;dataTypeId=GenericCSV<\/code>, which will return an id in the response body.<\/p><\/li>\n<li><p>Break the .csv file into chunks less than 4194304 bytes, and post them to <code>https:\/\/studioapi.azureml.net\/api\/blobuploads\/workspaces\/&lt;workspace_id&gt;\/?numberOfBlocks=&lt;the number of chunks&gt;&amp;blockId=&lt;index of chunk&gt;&amp;uploadId=&lt;the id you get from previous request&gt;&amp;dataTypeId=GenericCSV<\/code><\/p><\/li>\n<\/ol>\n\n<p>If you really want this functionality, you can implement it with python and the above REST API.<\/p>\n\n<p>If you think it's too complicated, report the issue to <a href=\"https:\/\/github.com\/Azure\/Azure-MachineLearning-ClientLibrary-Python\/issues\" rel=\"nofollow\">this<\/a>. The <strong>azureml<\/strong> python package is still under development, so your suggestion would be very helpful for them.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1457939067983,
        "Solution_link_count":3.0,
        "Solution_readability":11.2,
        "Solution_reading_time":33.05,
        "Solution_score_count":4.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":258.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":19.3071963889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm training a model in Azure ML Studio and the Net# specification I'm using doesn't match the NET# specification in the training output.<\/p>\n\n<p>Here's my experiment - <a href=\"https:\/\/i.stack.imgur.com\/WFMGs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WFMGs.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>and here are my NN params - <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/sVzcA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sVzcA.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>and finally here is the NET# specification in the Hyperparams output -<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ehImq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ehImq.png\" alt=\"enter image description here\"><\/a>\nIt's not using two hidden layers and it's also using sigmoid instead of ReLu. Is this expected behavior?<\/p>\n\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1508336707887,
        "Challenge_comment_count":1,
        "Challenge_created_time":1508267201980,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with the neural network schema in the training output of their model in Azure ML Studio. The Net# specification used in the experiment does not match the NET# specification in the Hyperparams output, which is not using two hidden layers and is using sigmoid instead of ReLu. The user is seeking clarification if this is expected behavior.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46797468",
        "Challenge_link_count":6,
        "Challenge_participation_count":2,
        "Challenge_readability":13.0,
        "Challenge_reading_time":12.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":19.3071963889,
        "Challenge_title":"Incorrect neural network schema in training output",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":36.0,
        "Challenge_word_count":97,
        "Platform":"Stack Overflow",
        "Poster_created_time":1274124294967,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":655.0,
        "Poster_view_count":47.0,
        "Solution_body":"<p>There is an issue with using custom NET# and parameter sweeps together: it switches over to using the default fully connected topology. <\/p>\n\n<p>Unfortunately, the workaround is to train the model for each parameter value separately.  <\/p>\n\n<p>-Roope  <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.5,
        "Solution_reading_time":3.19,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":133.7906688889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am attempting to follow this <a href=\"http:\/\/www.toptal.com\/machine-learning\/predicting-gas-prices-using-azure-machine-learning-studio\" rel=\"nofollow\">tutorial<\/a> however I was attempting to predict MPG for a set of cars rather than oil prices and have the following set up:<\/p>\n\n<ol>\n<li>MPG Sample dataset<\/li>\n<li>Remove missing values, project everything (weight, displacement, cylinders, etc) except model name<\/li>\n<li>Split 75 to train model, 25 to score model<\/li>\n<li>Train model on MPG column with neural network<\/li>\n<li>Score model which is fed by Train Model and Split<\/li>\n<li>Score model is fed to Evaluate model<\/li>\n<\/ol>\n\n<p>This all seems to run fine and without issue, so I create a scoring experiment and then publish it as a web service, however when I attempt to input values it is asking for an MPG input. My understanding is that this would be the predicted value, so it seems somewhat opposite to have to enter this as a value, or am I just understanding a basic tenet of machine learning? <\/p>\n\n<p>In short: Ideally I would like to be able to enter everything but the MPG and get a prediction on what the MPG is for a given set of value.<\/p>",
        "Challenge_closed_time":1432778316968,
        "Challenge_comment_count":0,
        "Challenge_created_time":1432296670560,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is attempting to predict MPG for a set of cars using Azure Machine Learning Prediction. They have followed a tutorial and set up a model using a neural network, which runs without issue. However, when attempting to input values, the system is asking for an MPG input, which seems opposite to the user's understanding of machine learning. The user would like to be able to enter everything but the MPG and get a prediction on what the MPG is for a given set of values.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/30396392",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":18.1,
        "Challenge_reading_time":15.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":133.7906688889,
        "Challenge_title":"Azure Machine Learning Prediction - Input and Outputs",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1527.0,
        "Challenge_word_count":190,
        "Platform":"Stack Overflow",
        "Poster_created_time":1352816892227,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United States",
        "Poster_reputation_count":2054.0,
        "Poster_view_count":260.0,
        "Solution_body":"<p>You could also add project columns to exclude label as part of scoring experiment and connect web service output port to the output of project columns<\/p>",
        "Solution_comment_count":8.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.3,
        "Solution_reading_time":1.94,
        "Solution_score_count":2.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":26.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":75.75,
        "Challenge_answer_count":3,
        "Challenge_body":"Hi,\n\u00a0\nIs there a way to download my Google AutoML Transation model and use it offline once it's trained?\n\u00a0\nAnd in what format can the model be exported?\u00a0\n\u00a0\nThank you",
        "Challenge_closed_time":1664553420000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664280720000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is looking for a way to download and use their Google AutoML Translation model offline after it has been trained. They are also inquiring about the format in which the model can be exported.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/exporting-a-google-autoML-translate-model\/m-p\/471646#M607",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":6.7,
        "Challenge_reading_time":2.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":75.75,
        "Challenge_title":"exporting a google autoML translate model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":142.0,
        "Challenge_word_count":35,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"1.- No.\n\n2.- You can create a Feature Request at\u00a0Issue Tracker\u00a0and\u00a0add a description about the feature you want(Export Translation Models), and the engineer team will look at it. You can see here how it is more likely that the team prioritize the work of the Feature Request\/Issues.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.2,
        "Solution_reading_time":3.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":55.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.7290330556,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Is there a way in the AML designer to set a pipeline and\/or specific step to now allow reuse between runs?  I've seen quite a few posts on how to do this in code, but I can't seem to find a way to set that property in the designer.<\/p>",
        "Challenge_closed_time":1617671557012,
        "Challenge_comment_count":1,
        "Challenge_created_time":1617640132493,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is looking for a way to specify \"do not allow reuse\" in Azure Machine Learning designer, but is unable to find a way to set that property in the designer. They have seen posts on how to do this in code, but are specifically looking for a solution within the designer.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/344626\/how-to-specify-do-not-allow-reuse-in-azure-machine",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":8.7,
        "Challenge_reading_time":3.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":8.7290330556,
        "Challenge_title":"How to specify do not allow reuse in Azure Machine Learning designer",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":60,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello,    <\/p>\n<p>Use the following steps to update a module pipeline parameter:    <\/p>\n<p>At the top of the canvas, select the gear icon.    <br \/>\nIn the Pipeline parameters section, you can view and update the name and default value for all of your pipeline parameter.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/84664-image.png?platform=QnA\" alt=\"84664-image.png\" \/>    <\/p>\n<p>Hope this helps. Thanks.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.1,
        "Solution_reading_time":5.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":59.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":37.0426969444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am very new to SageMaker. Upon my first interaction, it looks like the AWS SageMaker requires you to start from its Notebook. I have a training set which is ready. Is there a way to bypass setting the Notebook and just to start by upload the training set? Or it should be done through the Notebook. If anyone knows some example fitting my need above, that will be great. <\/p>",
        "Challenge_closed_time":1520631374252,
        "Challenge_comment_count":0,
        "Challenge_created_time":1520498020543,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is new to AWS SageMaker and wants to know if there is a way to upload a training set without starting from the Notebook. They are seeking examples that fit their needs.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49168673",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.4,
        "Challenge_reading_time":5.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":37.0426969444,
        "Challenge_title":"How to load a training set in AWS SageMaker to build a model?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":700.0,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Poster_created_time":1305269513436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":10317.0,
        "Poster_view_count":595.0,
        "Solution_body":"<p>Amazon SageMaker is a combination of multiple services that each is independent of the others. You can use the notebook instances if you want to develop your models in the familiar Jupyter environment. But if just need to train a model, you can use the training jobs without opening a notebook instance. <\/p>\n\n<p>There a few ways to launch a training job:<\/p>\n\n<ul>\n<li>Use the high-level SDK for Python that is similar to the way that you start a training step in your python code<\/li>\n<\/ul>\n\n<p><code>kmeans.fit(kmeans.record_set(train_set[0]))<\/code><\/p>\n\n<p>Here is the link to the python library: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk<\/a><\/p>\n\n<ul>\n<li>Use the low-level API to Create-Training-Job, and you can do that using various SDK (Java, Python, JavaScript, C#...) or the CLI. <\/li>\n<\/ul>\n\n<p><code>sagemaker = boto3.client('sagemaker')\n sagemaker.create_training_job(**create_training_params)<\/code><\/p>\n\n<p>Here is a link to the documentation on these options: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html<\/a> <\/p>\n\n<ul>\n<li>Use Spark interface to launch it using a similar interface to creating an MLLib training job<\/li>\n<\/ul>\n\n<p><code>val estimator = new KMeansSageMakerEstimator(\n  sagemakerRole = IAMRole(roleArn),\n  trainingInstanceType = \"ml.p2.xlarge\",\n  trainingInstanceCount = 1,\n  endpointInstanceType = \"ml.c4.xlarge\",\n  endpointInitialInstanceCount = 1)\n  .setK(10).setFeatureDim(784)<\/code><\/p>\n\n<p><code>val model = estimator.fit(trainingData)<\/code><\/p>\n\n<p>Here is a link to the spark-sagemaker library: <a href=\"https:\/\/github.com\/aws\/sagemaker-spark\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-spark<\/a><\/p>\n\n<ul>\n<li>Create a training job in the Amazon SageMaker console using the wizard there: <a href=\"https:\/\/console.aws.amazon.com\/sagemaker\/home?region=us-east-1#\/jobs\" rel=\"nofollow noreferrer\">https:\/\/console.aws.amazon.com\/sagemaker\/home?region=us-east-1#\/jobs<\/a><\/li>\n<\/ul>\n\n<p>Please note that there a few options also to train models, either using the built-in algorithms such as K-Means, Linear Learner or XGBoost (see here for the complete list: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html<\/a>). But you can also bring your own models for pre-baked Docker images such as TensorFlow (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/tf.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/tf.html<\/a>) or MXNet (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet.html<\/a>), your own Docker image (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo.html<\/a>).  <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":16.0,
        "Solution_readability":16.7,
        "Solution_reading_time":42.09,
        "Solution_score_count":1.0,
        "Solution_sentence_count":27.0,
        "Solution_word_count":275.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1421803794992,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"St. Louis, MO, USA",
        "Answerer_reputation_count":373.0,
        "Answerer_view_count":56.0,
        "Challenge_adjusted_solved_time":16.7299083334,
        "Challenge_answer_count":4,
        "Challenge_body":"<p><strong>Problem:<\/strong>\nI am trying to setup a model in Sagemaker, however it fails when it comes to downloading the data.\nDoes anyone know what I am doing wrong?<\/p>\n\n<p><strong>What I did so far<\/strong>:\nIn order to avoid any mistakes on my side I decided to use the AWS tutorial:\ntensorflow_iris_dnn_classifier_using_estimators<\/p>\n\n<p>And I made only two changes:<\/p>\n\n<ol>\n<li>I copied the dataset to my own S3 instance. --> I tested if I could access \/ show the data and it worked.<\/li>\n<li>I edited the path to point to the new folder.<\/li>\n<\/ol>\n\n<p>This is the AWS source code:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators<\/a><\/p>\n\n<pre><code>%%time\nimport boto3\n\n# use the region-specific sample data bucket\nregion = boto3.Session().region_name\n#train_data_location = 's3:\/\/sagemaker-sample-data-{}\/tensorflow\/iris'.format(region)\ntrain_data_location = 's3:\/\/my-s3-bucket'\n\niris_estimator.fit(train_data_location)\n<\/code><\/pre>\n\n<p>And this is the error I get:<\/p>\n\n<pre><code>\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/IPython\/core\/interactiveshell.pyc in run_cell_magic(self, magic_name, line, cell)\n   2115             magic_arg_s = self.var_expand(line, stack_depth)\n   2116             with self.builtin_trap:\n-&gt; 2117                 result = fn(magic_arg_s, cell)\n   2118             return result\n   2119 \n\n&lt;decorator-gen-60&gt; in time(self, line, cell, local_ns)\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/IPython\/core\/magic.pyc in &lt;lambda&gt;(f, *a, **k)\n    186     # but it's overkill for just that one bit of state.\n    187     def magic_deco(arg):\n--&gt; 188         call = lambda f, *a, **k: f(*a, **k)\n    189 \n    190         if callable(arg):\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/IPython\/core\/magics\/execution.pyc in time(self, line, cell, local_ns)\n   1191         else:\n   1192             st = clock2()\n-&gt; 1193             exec(code, glob, local_ns)\n   1194             end = clock2()\n   1195             out = None\n\n&lt;timed exec&gt; in &lt;module&gt;()\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/tensorflow\/estimator.pyc in fit(self, inputs, wait, logs, job_name, run_tensorboard_locally)\n    314                 tensorboard.join()\n    315         else:\n--&gt; 316             fit_super()\n    317 \n    318     @classmethod\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/tensorflow\/estimator.pyc in fit_super()\n    293 \n    294         def fit_super():\n--&gt; 295             super(TensorFlow, self).fit(inputs, wait, logs, job_name)\n    296 \n    297         if run_tensorboard_locally and wait is False:\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/estimator.pyc in fit(self, inputs, wait, logs, job_name)\n    232         self.latest_training_job = _TrainingJob.start_new(self, inputs)\n    233         if wait:\n--&gt; 234             self.latest_training_job.wait(logs=logs)\n    235 \n    236     def _compilation_job_name(self):\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/estimator.pyc in wait(self, logs)\n    571     def wait(self, logs=True):\n    572         if logs:\n--&gt; 573             self.sagemaker_session.logs_for_job(self.job_name, wait=True)\n    574         else:\n    575             self.sagemaker_session.wait_for_job(self.job_name)\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/session.pyc in logs_for_job(self, job_name, wait, poll)\n   1126 \n   1127         if wait:\n-&gt; 1128             self._check_job_status(job_name, description, 'TrainingJobStatus')\n   1129             if dot:\n   1130                 print()\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/session.pyc in _check_job_status(self, job, desc, status_key_name)\n    826             reason = desc.get('FailureReason', '(No reason provided)')\n    827             job_type = status_key_name.replace('JobStatus', ' job')\n--&gt; 828             raise ValueError('Error for {} {}: {} Reason: {}'.format(job_type, job, status, reason))\n    829 \n    830     def wait_for_endpoint(self, endpoint, poll=5):\n\nValueError: Error for Training job sagemaker-tensorflow-2019-01-03-16-32-16-435: Failed Reason: ClientError: Data download failed:S3 key: s3:\/\/my-s3-bucket\/\/sagemaker-tensorflow-2019-01-03-14-02-39-959\/source\/sourcedir.tar.gz has an illegal char sub-sequence '\/\/' in it\n<\/code><\/pre>",
        "Challenge_closed_time":1546594981603,
        "Challenge_comment_count":2,
        "Challenge_created_time":1546534753933,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while setting up a model in AWS Sagemaker. The error occurs when the data is being downloaded and the user has copied the dataset to their own S3 instance and edited the path to point to the new folder. The error message indicates that the data download failed due to an illegal character sub-sequence in the S3 key.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54026623",
        "Challenge_link_count":2,
        "Challenge_participation_count":6,
        "Challenge_readability":15.9,
        "Challenge_reading_time":57.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":9.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":16.7299083334,
        "Challenge_title":"AWS Sagemaker - ClientError: Data download failed",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":5145.0,
        "Challenge_word_count":367,
        "Platform":"Stack Overflow",
        "Poster_created_time":1546261116430,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>The script is expecting 'bucket' to be bucket = Session().default_bucket() or your own. Have you tried setting bucket equal to your personal bucket?<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":1.95,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":22.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":795.3458333333,
        "Challenge_answer_count":0,
        "Challenge_body":"I got this error with Azure Machine Learning. \r\n\r\nConfigException: ConfigException:\r\n\tMessage: blacklisted and whitelisted models are exactly the same. Found: {'XGBoostClassifier'}.Please remove models from the blacklist or add models to the whitelist.\r\n\r\nThe settings are as follow. 'XGBoostClassifier' is in the whitelist; and backlist is None. Would you please help with the error?\r\n\r\nautoml_settings = {\r\n    \"iteration_timeout_minutes\": 2,\r\n    \"experiment_timeout_minutes\": 20,\r\n    \"enable_early_stopping\": True,\r\n    \"primary_metric\": 'accuracy',\r\n    \"featurization\": 'auto',\r\n    \"verbosity\": logging.INFO,\r\n    \"n_cross_validations\": 5\r\n}\r\n\r\nfrom azureml.train.automl import AutoMLConfig\r\n\r\nautoml_config = AutoMLConfig(task='classification',\r\n                             enable_tf = True,\r\n                             debug_log='automated_ml_errors.log',\r\n                             X=x_train.values,\r\n                             y=y_train.values.flatten(),\r\n                             blacklist_models = None,\r\n                             whitelist_models = ['XGBoostClassifier'],\r\n                             **automl_settings)\r\n\r\n(Note: XGBoostClassifier was installed in the notebook)",
        "Challenge_closed_time":1583863460000,
        "Challenge_comment_count":8,
        "Challenge_created_time":1581000215000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an ImportError while trying to import 'AutoMLStep' from 'azureml.train.automl'.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/767",
        "Challenge_link_count":0,
        "Challenge_participation_count":8,
        "Challenge_readability":16.2,
        "Challenge_reading_time":13.22,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":795.3458333333,
        "Challenge_title":"Azure Machine Learning error: Can not use 'XGBoostClassifier'",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":98,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello,\r\n\r\nWe're so sorry you've encountered this issue. I've gone ahead and filed a work item to investigate and fix the issue around whitelisting XGBoost. We will reach out again here once a fix is in.\r\n\r\nThank you,\r\nSabina It could be possible that XGBoostClassifier was blacklisted by the system. We can double check if you can share your runId. In the meanwhile, we will improve the error msg for this scenario. Thanks! @waltz2u Can you please run the following line of code in your jupyter notebook and let me know what it says? \r\n\r\n`import xgboost`\r\n\r\nThanks,\r\nSabina Hi @waltz2u, I was able to reproduce and overcome this issue by double checking that import xgboost was installed correctly by trying `import xgboost`.\r\n\r\n\r\n`pip install \"py-xgboost<=0.80\"` fixed it on my end. Can you please try that and let us know if it solved the issue?  Hi @cartacioS and @jialiu103, sorry for the late reply. Yes it works now for me. Thank you very much.\r\n\r\nCD\r\n Will now proceed to close this thread. Thanks. @cartacioS I'm facing the same error, except that I'm kicking off the AutoML run from my local machine, using a remote compute as my aml compute target. Using this issue above, and this [one](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/313), it seems that I would still need to add xgboost to my env (locally) although technically I won't be using that package in my AutoML exercise? @jadhosn If you do not require XGBoost for your training, you can simply ignore this warning. But if you want XGBoost to be a potential recommended model, then yes you will need to add XGBoost to your local environment regardless of local\/remote compute.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":6.3,
        "Solution_reading_time":19.93,
        "Solution_score_count":null,
        "Solution_sentence_count":20.0,
        "Solution_word_count":275.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":27.2427933334,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I think it would be beneficial to select and delete several experiments at the same time.<br>\nNow I have to delete one by one and it is very time consuming.<\/p>\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1652874850582,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652776776526,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is requesting a feature to be able to select and delete multiple experiments at once, as deleting them one by one is time-consuming.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/remove-multiple-runs-at-the-same-time\/2435",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":4.8,
        "Challenge_reading_time":2.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":27.2427933334,
        "Challenge_title":"Remove multiple runs at the same time",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":249.0,
        "Challenge_word_count":39,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey <a class=\"mention\" href=\"\/u\/lucasventura\">@lucasventura<\/a>, you can do it like <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/runs-table#filter-and-delete-unwanted-runs\">this<\/a>.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":30.0,
        "Solution_reading_time":2.67,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":23.4807313889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>my dataset is 3 folders (train, validation and test) of images. each folder has two subfolders (cat1 and cat2). I am using AWS sage maker to preprocess my data and train my model. we all know that we have to upload the training data to S3 bucket before starting the &quot;.fit&quot; process.\nI want to know how to upload my data set to S3<\/p>\n<pre><code># general prefix\nprefix='chest-xray'\n#unique train\/test prefixes\ntrain_prefix   = '{}\/{}'.format(prefix, 'train')\nval_prefix   = '{}\/{}'.format(prefix, 'validation')\ntest_prefix    = '{}\/{}'.format(prefix, 'test')\n\n# uploading data to S3, and saving locations\ntrain_path  = sagemaker_session.upload_data(train_data, bucket=bucket, key_prefix=train_prefix)\n<\/code><\/pre>\n<p>what the train_data parameters should look like<\/p>",
        "Challenge_closed_time":1611678605710,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611594075077,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to know how to upload their image dataset, which consists of three folders (train, validation, and test) with two subfolders each (cat1 and cat2), to S3 in AWS SageMaker for preprocessing and model training. They have provided a code snippet for uploading data to S3 but are unsure about the format of the \"train_data\" parameter.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65889143",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":10.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":23.4807313889,
        "Challenge_title":"upload image dataset to S3 sagemaker",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":444.0,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1477757915556,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":79.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>According to the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/utility\/session.html#sagemaker.session.Session.upload_data\" rel=\"nofollow noreferrer\">documentation<\/a> <code>train_data<\/code> is the local path of the file to upload to S3, so you need this file locally where you are launching the training job. If you are using a notebook this is not the way to do. You have instead to manually upload your dataset in a S3 bucket. I suggest to preprocess your dataset in a single file (tfrecord for example if you are using TF) and upload that file to S3. You can do it using the AWS web console or using the AWS-CLI with the <code>aws s3 cp yourfile s3:\/\/your-bucket <\/code>command.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":8.78,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":102.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1589293508567,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":833.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":0.0,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I was trying to register a model using the <code>Run<\/code> Class like this:<\/p>\n<pre><code>model = run.register_model(\n    model_name=model_name,\n    model_path=model_path)\n<\/code><\/pre>\n<p>Errors with message: <code>Could not locate the provided model_path ... in the set of files uploaded to the run...<\/code><\/p>",
        "Challenge_closed_time":1643643837027,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643643837027,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error while trying to register a model using the Run Class in AzureML. The error message stated that the provided model_path could not be located in the uploaded files.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70928761",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.9,
        "Challenge_reading_time":4.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.0,
        "Challenge_title":"AzureML Model Register",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":319.0,
        "Challenge_word_count":38,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589293508567,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":833.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>The only way I found to fix the issue was to use the <code>Model<\/code> Class instead:<\/p>\n<pre><code>        model = Model.register(\n            workspace=ws,\n            model_name=model_name,\n            model_path=model_path,\n            model_framework=Model.Framework.SCIKITLEARN,\n            model_framework_version=sklearn.__version__,\n            description='Model Deescription',\n            tags={'Name' : 'ModelName', 'Type' : 'Production'},\n            model_framework=Model.Framework.SCIKITLEARN,\n            model_framework_version='1.0'\n            )\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":20.6,
        "Solution_reading_time":6.17,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":33.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1491898605956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sydney NSW, Australia",
        "Answerer_reputation_count":161.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":12.7567577778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following this example from aws <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-multi-model-endpoint-tensorflow-computer-vision\/blob\/main\/multi-model-endpoint-tensorflow-cv.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/sagemaker-multi-model-endpoint-tensorflow-computer-vision\/blob\/main\/multi-model-endpoint-tensorflow-cv.ipynb<\/a>\nto apply same workflow with two pre trained models (trained outside of sagemaker).<\/p>\n<p>But when I do the following, logs say that models can't be found:<\/p>\n<pre><code>import boto3\nimport datetime\nfrom datetime import datetime\nimport time\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.tensorflow.serving import TensorFlowModel\nfrom sagemaker.multidatamodel import MultiDataModel\n\nmodel_data_prefix = f's3:\/\/{BUCKET}\/{PREFIX}\/mme\/'\noutput = f's3:\/\/{BUCKET}\/{PREFIX}\/mme\/test.tar.gz'\n\nmodele = TensorFlowModel(model_data=output, \n                          role=role, \n                          image_uri=IMAGE_URI)\n\nmme = MultiDataModel(name=f'mme-tensorflow-{current_time}',\n                     model_data_prefix=model_data_prefix,\n                     model=modele,\n                     sagemaker_session=sagemaker_session)\n\npredictor = mme.deploy(initial_instance_count=1,\n                       instance_type='ml.m5.2xlarge',\n                       endpoint_name=f'mme-tensorflow-{current_time}')\n<\/code><\/pre>\n<p>When I give an image as input to predict, I have this message:<\/p>\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message &quot;&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;Internal Server Error&lt;\/title&gt;\n  &lt;\/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;&lt;p&gt;Internal Server Error&lt;\/p&gt;&lt;\/h1&gt;\n    \n  &lt;\/body&gt;\n&lt;\/html&gt;\n&quot;.\n<\/code><\/pre>\n<p>Logs give:<\/p>\n<pre><code>Could not find base path \/opt\/ml\/models\/...\/model for servable ...\n<\/code><\/pre>\n<p>What did I missed ?<\/p>",
        "Challenge_closed_time":1651629562456,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651582909370,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while deploying pre-trained TensorFlow models to one endpoint in SageMaker. The logs indicate that the models cannot be found and the user receives a ModelError when attempting to predict with an image input. The error message suggests that the base path for the servable model cannot be found.",
        "Challenge_last_edit_time":1651583638128,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72099790",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":21.3,
        "Challenge_reading_time":25.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":12.9591905556,
        "Challenge_title":"Error when deploying pre trained Tensorflow models to one endpoint (multimodel for one endpoint) in sagemaker?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":72.0,
        "Challenge_word_count":155,
        "Platform":"Stack Overflow",
        "Poster_created_time":1606642099552,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":371.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>In the sample notebook, the model is trained within SageMaker. So it is created with certain environment variables like the &quot;SAGEMAKER_PROGRAM&quot;(I think, need to check the documentation) with value set to entry point script.<\/p>\n<p>But while you are creating the model with models trained outside the SageMaker you need to add those environment variables.<\/p>\n<p>Without an entry point script SageMaker is not in a position to know what to do with the request.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.4,
        "Solution_reading_time":5.93,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":73.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":20.8108333333,
        "Challenge_answer_count":0,
        "Challenge_body":"When using wandb, it shows step as X and not episode.\r\n\r\nHence, longer runs have more steps and it makes the comparaison between runs difficult.\r\n\r\n\r\n![photo_2020-11-17_13-47-41](https:\/\/user-images.githubusercontent.com\/13030198\/99403033-5052e400-28ea-11eb-92c0-a3efd14b654a.jpg)\r\n\r\n",
        "Challenge_closed_time":1605698611000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605623692000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where loading configs from wandb results in incorrect HParams objects, which can be seen when attempting to load the model checkpoint or comparing the object with the info panel for the run on wandb. The user has provided a code snippet to reproduce the issue and has set acceptance criteria for the bug to be fixed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/MathisFederico\/LearnRL\/issues\/96",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":7.1,
        "Challenge_reading_time":3.89,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":4.0,
        "Challenge_repo_issue_count":137.0,
        "Challenge_repo_star_count":17.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":20.8108333333,
        "Challenge_title":"Add episode to wandb",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":29,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1528790837107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":610.0,
        "Answerer_view_count":203.0,
        "Challenge_adjusted_solved_time":208.4337880556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I could really use some help!<\/p>\n\n<p>The company I work for is made up of 52 very different businesses so I can't predict at the company level but instead need to predict business by business then roll up the result to give company wide prediction.<\/p>\n\n<p>I have written an ML model in studio.azureml.net\nIt works great with a 0.947 Coefficient of Determination, but this is for 1 of the businesses.\nI now need to train the model for the other 51.<\/p>\n\n<p>Is there a way to do this in a single ML model rather than having to create 52 very similar models?<\/p>\n\n<p>Any help would be much appreciated !!!<\/p>\n\n<p>Kind Regards\nMartin<\/p>",
        "Challenge_closed_time":1559897214220,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559146852583,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created an ML model in Azure ML for one of the 52 businesses in their company, but now needs to train the model for the other 51 businesses. They are seeking help to determine if there is a way to do this in a single ML model instead of creating 52 similar models.",
        "Challenge_last_edit_time":1560930522380,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56364828",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":8.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":208.4337880556,
        "Challenge_title":"Azure ML - Train a model on segments of the data-set",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":123.0,
        "Challenge_word_count":121,
        "Platform":"Stack Overflow",
        "Poster_created_time":1300461675980,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":189.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>You can use Ensembles, combining several models to improve predictions. The most direct is stacking when the outputs of all the models are trained on the entire dataset. \nThe method that, I think, corresponds the best to your problem is bagging (bootstrap aggregation). You need to divide the training set into different subsets (each corresponding to a certain business), then train a different model on each subset and combine the result of each classifier. \nAnother way is boosting but it is difficult to implement in Azure ML. \nYou can see an example in <a href=\"https:\/\/gallery.azure.ai\/Experiment\/b6b09fc0c26047e6b4c733ab78a86498\" rel=\"nofollow noreferrer\">Azure ML Gallery<\/a>. <\/p>\n\n<p>Quote from book:<\/p>\n\n<blockquote>\n  <p>Stacking and bagging can be easily implemented in Azure Machine\n  Learning, but other ensemble methods are more difficult. Also, it\n  turns out to be very tedious to implement in Azure Machine Learning an\n  ensemble of, say, more than five models. The experiment is filled with\n  modules and is quite difficult to maintain. Sometimes it is worthwhile\n  to use any ensemble method available in R or Python. Adding more\n  models to an ensemble written in a script can be as trivial as\n  changing a number in the code, instead of copying and pasting modules\n  into the experiment.<\/p>\n<\/blockquote>\n\n<p>You may also have a look at <a href=\"http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html\" rel=\"nofollow noreferrer\">sklearn (Python)<\/a> and caret (R) documentation for further details.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.4,
        "Solution_reading_time":18.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":220.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":32.0736591667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to use the tensorflow hub to retrain existing models, however tensorflow supports the hub library only on their 2.2 version. And The Estimator azure presents supports tf 2.0.  <\/p>\n<p>When I list tensorflow 2.2 as a required dependency as a pip package, during docker image creation the system fails - it seems like horovod is responsible, - that it cannot find the correct libraries.   <\/p>\n<p>Is this possible to be fixed? as in either an Estimator with tf 2.2 support, or an esitmator without the horovod - as I do not need a distributed system for my solution. <\/p>",
        "Challenge_closed_time":1591454811350,
        "Challenge_comment_count":3,
        "Challenge_created_time":1591339346177,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to use TensorFlow hub to retrain existing models, but Azure ML's Estimator only supports TensorFlow 2.0. When the user tries to list TensorFlow 2.2 as a required dependency, the system fails due to horovod not finding the correct libraries. The user is looking for a solution to either have an Estimator with TensorFlow 2.2 support or an Estimator without horovod as they do not need a distributed system for their solution.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/32334\/tensorflow-2-2-0-update-for-the-tensorflow-estimat",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":8.7,
        "Challenge_reading_time":8.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":32.0736591667,
        "Challenge_title":"TensorFlow 2.2.0 update for the tensorflow estimator for Azure ML, or disable horovod?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":111,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Following the pointers from <a>@romungi-MSFT<\/a>, defining estimator with gpubase image; &quot;mcr.microsoft.com\/azureml\/base-gpu:openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04&quot; solves the problem, and Tensorflow 2.2 can be included. Tensorflow uses GPU by default when available.<\/p>\n<pre><code> estimator = Estimator(source_directory=experiment_folder,\n                       compute_target=compute_target,\n                       script_params=script_params,\n                       entry_script='rps_efn_b0.py',\n                       node_count=1,        \n                       conda_packages=['ipykernel'],\n                       pip_packages = ['azureml-sdk',\n                                       'pyarrow',\n                                       'pyspark',\n                                       'azureml-mlflow',\n                                       'joblib',\n                                       'matplotlib',\n                                       'Pillow',\n                                       'tensorflow==2.2',\n                                       'tensorflow-datasets',\n                                       'tensorflow-hub',\n                                       'azureml-defaults',\n                                       'azureml-dataprep[fuse,pandas]'],\n                       custom_docker_image='mcr.microsoft.com\/azureml\/base-gpu:openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04')\n<\/code><\/pre>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":24.9,
        "Solution_reading_time":11.33,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":50.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":45.3307377778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have 3 main process to perform using Amazon SageMaker.<\/p>\n\n<ol>\n<li>Using own training python script, (not using sagemaker container, inbuilt algorithm) [Train.py]<\/li>\n<\/ol>\n\n<p>-> For this, I have referred to this link:<br>\n<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/train-and-host-scikit-learn-models-in-amazon-sagemaker-by-building-a-scikit-docker-container\/\" rel=\"nofollow noreferrer\">Bring own algorithm to AWS sagemaker<\/a>\nand it seems that we can bring our own training script to sagemaker managed training setup, and model artifacts can be uploaded to s3 etc.\nNote: I am using Light GBM model for training.<\/p>\n\n<ol start=\"2\">\n<li>Writing forecast to AWS RDS DB:<\/li>\n<\/ol>\n\n<p>-> There is no need to deploy model and create endpoint, because training will happen everyday, and will create forecast as soon as training completes. (Need to generate forecast in train.py itself)<\/p>\n\n<p>-> <strong>Challenge is how can I write forecast in AWS RDS DB from train.py script. (Given that script is running in Private VPC)<\/strong><\/p>\n\n<ol start=\"3\">\n<li>Scheduling this process as daily job:<\/li>\n<\/ol>\n\n<p>--> I have gone through AWS step functions and seems to be the way to trigger daily training and write forecast to RDS.<\/p>\n\n<p>--> <strong>Challenge is how to use step function for time based trigger and not event based.<\/strong><\/p>\n\n<p>Any suggestions on how to do this? Any best practices to follow? Thank you in advance.<\/p>",
        "Challenge_closed_time":1566713256896,
        "Challenge_comment_count":0,
        "Challenge_created_time":1566547602673,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in performing three main processes using Amazon SageMaker. The first process involves using their own training python script, not using the sagemaker container, and uploading model artifacts to S3. The second process involves writing forecast to AWS RDS DB from the train.py script running in a private VPC. The third process involves scheduling this process as a daily job using AWS step functions for time-based trigger instead of event-based. The user is seeking suggestions and best practices to overcome these challenges.",
        "Challenge_last_edit_time":1566550066240,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57622122",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":19.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":46.0150619444,
        "Challenge_title":"Custom sagemaker container for training, write forecast to AWS RDS, on a daily basis",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":466.0,
        "Challenge_word_count":217,
        "Platform":"Stack Overflow",
        "Poster_created_time":1469183608840,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Gurugram, Haryana, India",
        "Poster_reputation_count":624.0,
        "Poster_view_count":67.0,
        "Solution_body":"<p>The way to trigger Step Functions on schedule is by using CloudWatch Events (sort of cron). Check out this tutorial: <a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/tutorial-cloudwatch-events-target.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/tutorial-cloudwatch-events-target.html<\/a><\/p>\n\n<p>Don't write to the RDS from your Python code! It is better to write the output to S3 and then \"copy\" the files from S3 into the RDS. Decoupling these batches will make a more reliable and scalable process. You can trigger the bulk copy into the RDS when the files are written to S3 or to a later time when your DB is not too busy. <\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.6,
        "Solution_reading_time":8.74,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":92.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.4033333333,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi, I am trying to fit an MXNet model locally. I am adapting this https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/ and doing the following:\n\n    bucket = 'XXXXXXXXXXX'\n    prefix = 'sagemaker\/cifar-bench\/data'\n    \n    inputs = sagemaker_session.upload_data(\n        path='data',\n        bucket=bucket, \n        key_prefix=prefix)\n    \n    print('data sent to ' + inputs)\n    \n    \n    Inception = MXNet('gluon_cifar_net.py', \n              role=role, \n              train_instance_count=1, \n              train_instance_type='local_gpu',\n              framework_version='1.2.1',\n              base_job_name='cifar10-inception-',\n              hyperparameters={'batch_size': 256, \n                               'optimizer': 'sgd',\n                               'epochs': 100, \n                               'learning_rate': 0.1, \n                               'momentum': 0.9})\n    \n    \n    Inception.fit(inputs)\n\nwhich returns an `OSError: [Errno 2] No such file or directory`\n\nIn the error log I can see that there seems to be error at `self.latest_training_job = _TrainingJob.start_new(self, inputs)` and `self.sagemaker_client.create_training_job(**train_request)`\n\nHow can I make the local mode work?\n",
        "Challenge_closed_time":1537550695000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1537534843000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to fit an MXNet model locally using SageMaker but is encountering an \"OSError: [Errno 2] No such file or directory\" error. The error log indicates that there is an issue with \"self.latest_training_job = _TrainingJob.start_new(self, inputs)\" and \"self.sagemaker_client.create_training_job(**train_request)\". The user is seeking help to resolve the issue and make the local mode work.",
        "Challenge_last_edit_time":1668119355544,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUQu1fDak6RL2wmivZ5UJwUw\/sagemaker-mxnet-local-mode-not-working",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":16.0,
        "Challenge_reading_time":13.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":4.4033333333,
        "Challenge_title":"SageMaker MXNet local mode not working",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":259.0,
        "Challenge_word_count":93,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"It is very likely that you don't have docker-compose (or docker) installed in the box, that is why you are getting a No such file or directory.\n\nIf you want to use the GPU setup I would recommend running on a sagemaker notebook instance. Navigate to one of the example notebooks such as: https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_cifar10\/mxnet_cifar10_local_mode.ipynb\n\nAnd run the setup.sh cell. This will install and configure all the docker dependencies correctly and then you should be able to use MXNet locally on GPU without any issue.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925587140,
        "Solution_link_count":1.0,
        "Solution_readability":12.2,
        "Solution_reading_time":7.52,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":84.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":6.2307972222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to train an automl model on gcp's vertex ai using multiple datasets.  I would like to keep the datasets separate, since they come from different sources, want to train on them separately, etc.  Is that possible?  Or will I need to create a dataset containing both datasets? It looks like I can only select one dataset in the web UI.<\/p>",
        "Challenge_closed_time":1628828205167,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628805774297,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to train a Google-Cloud-Automl model on multiple datasets but is unsure if it is possible to keep the datasets separate or if they need to be combined into one dataset. The user has noticed that the web UI only allows for the selection of one dataset.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68764644",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.3,
        "Challenge_reading_time":4.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":6.2307972222,
        "Challenge_title":"Training Google-Cloud-Automl Model on multiple datasets",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":223.0,
        "Challenge_word_count":68,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540925643620,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Utah, USA",
        "Poster_reputation_count":1212.0,
        "Poster_view_count":110.0,
        "Solution_body":"<p>It is possible via the Vertex AI API as long as your sources are in Google Cloud Storage, just provide a list of training data which are in JSON or CSV format that qualifies with the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/prepare-image\" rel=\"nofollow noreferrer\">best practices for formatting of training data<\/a>.<\/p>\n<p>See code for creating and importing datasets. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/create-dataset-api#create-dataset\" rel=\"nofollow noreferrer\">documentation<\/a> for code reference and further details.<\/p>\n<pre><code>from typing import List, Union\nfrom google.cloud import aiplatform\n\n    def create_and_import_dataset_image_sample(\n        project: str,\n        location: str,\n        display_name: str,\n        src_uris: Union[str, List[str]], \/\/ example: [&quot;gs:\/\/bucket\/file1.csv&quot;, &quot;gs:\/\/bucket\/file2.csv&quot;]\n        sync: bool = True,\n    ):\n        aiplatform.init(project=project, location=location)\n    \n        ds = aiplatform.ImageDataset.create(\n            display_name=display_name,\n            gcs_source=src_uris,\n            import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification,\n            sync=sync,\n        )\n    \n        ds.wait()\n    \n        print(ds.display_name)\n        print(ds.resource_name)\n        return ds\n<\/code><\/pre>\n<p>NOTE: The links provided are for Vertex AI AutoML Image. If you access the links there are options for other AutoML products like Text, Tabular and Video.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.1,
        "Solution_reading_time":18.04,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":131.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":40.1701361111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to share a model registry completely between Dev and Prod environment? So my idea is to create 10000 models in dev and maybe select 2000 from there to work in prod. I am planning to use AWS model registry. So if I do the training and testing and hyperparameter tuning in my AWS dev environment, is it possible to then share the registry in prod? The obvious reason is that it does not make sense to use the prod to do the training and testing again.<\/p>\n<p>Please advise!<\/p>\n<p>Thanks in advance!<\/p>",
        "Challenge_closed_time":1638378803110,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638234190620,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of sharing a model registry between their development and production environments using AWS model registry. They want to create 10,000 models in dev and select 2,000 for use in prod without having to repeat the training and testing process in prod. They are seeking advice on how to accomplish this.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70163094",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":6.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":40.1701361111,
        "Challenge_title":"SageMaker Model Registry Sharing",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":229.0,
        "Challenge_word_count":97,
        "Platform":"Stack Overflow",
        "Poster_created_time":1557333597230,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":99.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>It depends how you define Dev and Prod.<\/p>\n<ul>\n<li><p>If by Dev and Prod you mean different AWS account (which is a good practice - see <a href=\"https:\/\/docs.aws.amazon.com\/whitepapers\/latest\/organizing-your-aws-environment\/benefits-of-using-multiple-aws-accounts.html\" rel=\"nofollow noreferrer\">doc<\/a> and <a href=\"https:\/\/aws.amazon.com\/blogs\/devops\/aws-building-a-secure-cross-account-continuous-delivery-pipeline\/\" rel=\"nofollow noreferrer\">blog<\/a>), you cannot share fractions of a model registry from a given account to another account, but you can create triggers to export models from one model registry to another, as documented in this blog post <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/patterns-for-multi-account-hub-and-spoke-amazon-sagemaker-model-registry\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/patterns-for-multi-account-hub-and-spoke-amazon-sagemaker-model-registry\/<\/a><\/p>\n<\/li>\n<li><p>If your Dev and Prod live in the same AWS account and you are just looking for ways to differentiate them, you can use:<\/p>\n<ul>\n<li>Model Registry Status information<\/li>\n<li>Tags<\/li>\n<\/ul>\n<\/li>\n<\/ul>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":19.7,
        "Solution_reading_time":15.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":107.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1114.6180555556,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\ndoing\r\n\r\n`$ aim convert mlflow --tracking_uri 'file:\/\/\/Users\/aim_user\/mlruns' --experiment 61`\r\n\r\nas described here https:\/\/aimstack.readthedocs.io\/en\/latest\/quick_start\/convert_data.html#show-mlflow-logs-in-aim\r\n\r\nfails with the following error\r\n\r\n![Screenshot from 2022-02-27 02-33-17](https:\/\/user-images.githubusercontent.com\/26168435\/155864827-dc7f3acb-0c79-4fab-9c79-a599f1a954ab.png)\r\n\r\nusing the experiment name instead of the experiment id\r\n\r\n![Screenshot from 2022-02-27 02-33-55](https:\/\/user-images.githubusercontent.com\/26168435\/155864887-63c19423-865e-4540-bfb7-c034e123af80.png)\r\n\r\ni.e.\r\n\r\n`$ aim convert mlflow --tracking_uri 'file:\/\/\/Users\/aim_user\/mlruns' --experiment 'ai-vengers-collab'` \r\n\r\nworks:\r\n\r\n![Screenshot from 2022-02-27 02-31-46](https:\/\/user-images.githubusercontent.com\/26168435\/155864881-03434a11-68f8-47e3-90e3-13465cbe86b4.png)\r\n\r\n### To reproduce\r\n\r\nsee above\r\n\r\n### Expected behavior\r\n\r\nconvert the experiment by ID\r\n\r\n### Environment\r\n\r\n- Aim Version 3.6\r\n- Python 3.8.1\r\n- pip3\r\n- Ubuntu 20.04.3 LTS\r\n",
        "Challenge_closed_time":1649939186000,
        "Challenge_comment_count":6,
        "Challenge_created_time":1645926561000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while running mlflow.projects.run consistently with etag error. The error is related to Etag conflict on the environment definition.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aimhubio\/aim\/issues\/1415",
        "Challenge_link_count":4,
        "Challenge_participation_count":6,
        "Challenge_readability":13.3,
        "Challenge_reading_time":14.53,
        "Challenge_repo_contributor_count":47.0,
        "Challenge_repo_fork_count":181.0,
        "Challenge_repo_issue_count":2399.0,
        "Challenge_repo_star_count":2909.0,
        "Challenge_repo_watch_count":35.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1114.6180555556,
        "Challenge_title":"aim convert mlflow --experiment fails for experiment id, works for experiment name",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":81,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hey @luisoala, thanks for reporting the issue!\r\n@devfox-se could you please take a look at this? Thanks for reporting this @luisoala, will take a look soon! Hey @luisoala! We've released `v3.6.2` containing the fix for mlflow converter. Please check it out and let me know if there are any issues. thanks @alberttorosyan working through a few other deadlines atm, aiming for a test ~ next tuesday, will share result here @luisoala Hi, have you had a chance to test this?:) Closing due to inactivity, feel free to reopen in case this still persists.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.2,
        "Solution_reading_time":6.68,
        "Solution_score_count":null,
        "Solution_sentence_count":8.0,
        "Solution_word_count":92.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.8001691667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I can\u2019t find some of the basic modules from this week. Any significant change about Designer? <\/p>",
        "Challenge_closed_time":1660841903856,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660839023247,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to locate some basic modules in the Machine Learning Designer and is questioning if there have been any significant changes made to the Designer.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/972775\/change-in-machine-learning-designer",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":1.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.8001691667,
        "Challenge_title":"Change in Machine Learning Designer",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":21,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=2ce87912-8dda-483a-8ead-0e2912a6e6ef\">@Mofoch  <\/a>     <\/p>\n<p>Thanks for using Microsoft Q&amp;A platform, there is no surprising change in Azure Machine Learning Designer.    <\/p>\n<p>Based on my experience, you may use the filter so you can not see some of the modules as below screenshot.     <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/232490-image.png?platform=QnA\" alt=\"232490-image.png\" \/>    <\/p>\n<p>If this is not your case, could you please share which module you have lost? Thanks.     <\/p>\n<p>I hope this helps.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.4,
        "Solution_reading_time":9.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":91.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.4330788889,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hello MS team,  <\/p>\n<p>I have registered an ML model in the AML workspace using an Azure Machine learning pipeline and triggered the main control script of the pipeline by linking the repo present in Azure DevOps to the AML workspace(using Service principal).   <\/p>\n<p>How do I download the latest version of the model from the AML workspace to the <em>&quot;Artifacts&quot;<\/em> folder in Azure DevOPs?  <\/p>\n<p>Any help is appreciated please.  <\/p>",
        "Challenge_closed_time":1643933985587,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643903626503,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking assistance in downloading the latest version of an ML model registered in the Azure Machine Learning Service Model Registry to the \"Artifacts\" folder in Azure DevOps. They have already registered the model using an Azure Machine Learning pipeline and linked the repo present in Azure DevOps to the AML workspace using Service principal.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/721792\/how-to-get-model-id-of-the-latest-version-register",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.1,
        "Challenge_reading_time":7.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":8.4330788889,
        "Challenge_title":"How to get Model ID of the Latest Version registered in Azure Machine Learning Service Model Registry using az ml cli?",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":92,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=6755dac2-30f1-48a2-9d0d-4d2c96edc5d4\">@Shivapriya Katta  <\/a>     <\/p>\n<p>I think you are mentioning how to get the latest version of model and download the model in az ml.    <\/p>\n<p>There are 2 steps, one is list the model to get the model ID you want, two is download the model.    <\/p>\n<p><strong>az ml model list<\/strong>    <br \/>\nList models in the workspace.    <\/p>\n<pre><code>az ml model list [--dataset-id]  \n                 [--latest]  \n                 [--model-name]  \n                 [--path]  \n                 [--property]  \n                 [--resource-group]  \n                 [--run-id]  \n                 [--subscription-id]  \n                 [--tag]  \n                 [--workspace-name]  \n                 [-v]  \n<\/code><\/pre>\n<p>Optional Parameters    <br \/>\n--dataset-id    <br \/>\nIf provided, will only show models with the specified dataset ID.    <\/p>\n<p>--latest -l    <br \/>\nIf provided, will only return models with the latest version.    <\/p>\n<p>--model-name -n    <br \/>\nAn optional model name to filter the list by.    <\/p>\n<p>--path    <br \/>\nPath to a project folder. Default: current directory.    <\/p>\n<p>--property    <br \/>\nKey\/value property to add (e.g. key=value ). Multiple properties can be specified with multiple --property options.    <\/p>\n<p>--resource-group -g    <br \/>\nResource group corresponding to the provided workspace.    <\/p>\n<p>--run-id    <br \/>\nIf provided, will only show models with the specified Run ID.    <\/p>\n<p>--subscription-id    <br \/>\nSpecifies the subscription Id.    <\/p>\n<p>--tag    <br \/>\nKey\/value tag to add (e.g. key=value ). Multiple tags can be specified with multiple --tag options.    <\/p>\n<p>--workspace-name -w    <br \/>\nName of the workspace containing models to list.    <\/p>\n<p>-v    <br \/>\nVerbosity flag.    <\/p>\n<p><strong>az ml model download<\/strong>    <br \/>\nDownload a model from the workspace.    <\/p>\n<pre><code>az ml model download --model-id  \n                     --target-dir  \n                     [--overwrite]  \n                     [--path]  \n                     [--resource-group]  \n                     [--subscription-id]  \n                     [--workspace-name]  \n                     [-v]  \n<\/code><\/pre>\n<p>Required Parameters    <br \/>\n--model-id -i    <br \/>\nID of model.    <\/p>\n<p>--target-dir -t    <br \/>\nTarget directory to download the model file to.    <\/p>\n<p>Optional Parameters    <br \/>\n--overwrite    <br \/>\nOverwrite if the same name file exists in target directory.    <\/p>\n<p>--path    <br \/>\nPath to a project folder. Default: current directory.    <\/p>\n<p>--resource-group -g    <br \/>\nResource group corresponding to the provided workspace.    <\/p>\n<p>--subscription-id    <br \/>\nSpecifies the subscription Id.    <\/p>\n<p>--workspace-name -w    <br \/>\nName of the workspace containing model to show.    <\/p>\n<p>-v    <br \/>\nVerbosity flag.    <\/p>\n<p>Hope this helps!     <\/p>\n<p><em>Please kindly accept the answer if you feel helpful, thank you!<\/em>    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.8,
        "Solution_reading_time":32.05,
        "Solution_score_count":0.0,
        "Solution_sentence_count":30.0,
        "Solution_word_count":344.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":19.2317730556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Sagemaker is a great tool to train your models, and we save some money by using AWS spot instances. However, training jobs sometimes get stopped in the middle. We are using some mechanisms to continue from the latest checkpoint after a restart. See also the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-checkpoints.html\" rel=\"nofollow noreferrer\">docs<\/a>.<\/p>\n<p>Still, how do you efficiently test such a mechanism? Can you trigger it yourself? Otherwise you have to wait until the spot instance actually \u00eds restarted.<\/p>\n<p>Also, are you expected to use the linked <code>checkpoint_s3_uri<\/code> argument or the <code>model_dir<\/code> for this? E.g. the <code>TensorFlow<\/code> estimator <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/sagemaker.tensorflow.html#tensorflow-estimator\" rel=\"nofollow noreferrer\">docs<\/a> seem to suggest something <code>model_dir<\/code>for checkpoints.<\/p>",
        "Challenge_closed_time":1616662041463,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616592352363,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while using AWS Sagemaker for training models with spot instances. The training jobs sometimes get stopped in the middle, and the user is using mechanisms to continue from the latest checkpoint after a restart. However, the user is unsure how to efficiently test this mechanism and which argument to use for checkpoints - the `checkpoint_s3_uri` or the `model_dir`.",
        "Challenge_last_edit_time":1616592807080,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66782040",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":12.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":19.3580833334,
        "Challenge_title":"Reloading from checkpoing during AWS Sagemaker Training",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":286.0,
        "Challenge_word_count":110,
        "Platform":"Stack Overflow",
        "Poster_created_time":1484838464572,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Amsterdam, Nederland",
        "Poster_reputation_count":3937.0,
        "Poster_view_count":387.0,
        "Solution_body":"<p>Since you can't manually terminate a sagemaker instance, run an Amazon SageMaker Managed Spot training for a small number of epochs, Amazon SageMaker would have backed up your checkpoint files to S3. Check that checkpoints are there. Now run a second training run, but this time provide the first jobs\u2019 checkpoint location to <code>checkpoint_s3_uri<\/code>. Reference is <a href=\"https:\/\/towardsdatascience.com\/a-quick-guide-to-using-spot-instances-with-amazon-sagemaker-b9cfb3a44a68\" rel=\"nofollow noreferrer\">here<\/a>, this also answer your second question.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.8,
        "Solution_reading_time":7.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1231266594036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, United States",
        "Answerer_reputation_count":2800.0,
        "Answerer_view_count":262.0,
        "Challenge_adjusted_solved_time":35.1478936111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I just started using aws sagemaker for running and maintaining models, experiments. just wanted to know is there any persistent layer for the sagemaker from where i can get data of my experiments\/models instead of looking into the sagemaker studio. Does sagemaker saves the experiments or its data like s3 location in any table  something like modelsdb? <\/p>",
        "Challenge_closed_time":1584546176820,
        "Challenge_comment_count":0,
        "Challenge_created_time":1584418712033,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is new to using AWS SageMaker for running and maintaining models and experiments. They are looking for a persistent layer where they can access data related to their experiments and models instead of having to look into SageMaker Studio. They are wondering if SageMaker saves experiment data, such as S3 location, in a table like ModelsDB.",
        "Challenge_last_edit_time":1584419644403,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60716202",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":4.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":35.4068852778,
        "Challenge_title":"SageMaker experiments store",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":229.0,
        "Challenge_word_count":60,
        "Platform":"Stack Overflow",
        "Poster_created_time":1427130497092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":187.0,
        "Poster_view_count":151.0,
        "Solution_body":"<p>SageMaker Studio is using the SageMaker API to pull all of the data its displaying.  Essentially there's no secret API here getting invoked.<\/p>\n\n<p>Quite a bit of what's being displayed with respect to experiments is from the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_Search.html\" rel=\"nofollow noreferrer\">search results<\/a>, the rest coming from either List* or Describe* calls.  Studio is taking the results from the search request and displaying it in the table format that you're seeing.  Search results when searching over resource ExperimentTrialComponent that have a source (such as a training job) will be enhanced with the original sources data ([result]::SourceDetail::TrainingJob) if supported (work is ongoing to add additional source detail resource types).<\/p>\n\n<p>All of the metadata that is related to resources in SageMaker is available via the APIs; there is no other location (in the cloud) like s3 for that data.<\/p>\n\n<p>As of this time there is no effort to determine if it's possible to add support into <a href=\"https:\/\/github.com\/VertaAI\/modeldb\" rel=\"nofollow noreferrer\">modeldb<\/a> for SageMaker that I'm aware of.  Given that modeldb appears to make some assumptions about it's talking to a relational database it would appear unlikely to be something that would be doable. (I only read the overview very quickly so this might be inaccurate.)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.6,
        "Solution_reading_time":17.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":201.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1428654714763,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":596.0,
        "Answerer_view_count":80.0,
        "Challenge_adjusted_solved_time":18.9362980555,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to build an API using an MLflow model.<\/p>\n<p>the funny thing is it works from one location on my PC and not from another. So, the reason for doing I wanted to change my repo etc.<\/p>\n<p>So, the simple code of<\/p>\n<pre><code>from mlflow.pyfunc import load_model\nMODEL_ARTIFACT_PATH = &quot;.\/model\/model_name\/&quot;\nMODEL = load_model(MODEL_ARTIFACT_PATH)\n<\/code><\/pre>\n<p>now fails with<\/p>\n<pre><code>ERROR:    Traceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 540, in lifespan\n    async for item in self.lifespan_context(app):\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 481, in default_lifespan\n    await self.startup()\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 516, in startup\n    await handler()\n  File &quot;\/code\/.\/app\/main.py&quot;, line 32, in startup_load_model\n    MODEL = load_model(MODEL_ARTIFACT_PATH)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 733, in load_model\n    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/spark.py&quot;, line 737, in _load_pyfunc\n    return _PyFuncModelWrapper(spark, _load_model(model_uri=path))\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/spark.py&quot;, line 656, in _load_model\n    return PipelineModel.load(model_uri)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/util.py&quot;, line 332, in load\n    return cls.read().load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/pipeline.py&quot;, line 258, in load\n    return JavaMLReader(self.cls).load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/util.py&quot;, line 282, in load\n    java_obj = self._jread.load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/py4j\/java_gateway.py&quot;, line 1321, in __call__\n    return_value = get_return_value(\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/sql\/utils.py&quot;, line 117, in deco\n    raise converted from None\npyspark.sql.utils.AnalysisException: Unable to infer schema for Parquet. It must be specified manually.\n<\/code><\/pre>\n<p>The model artifacts are already downloaded to the folder \/model folder which has the following structure.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oqxRW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oqxRW.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>the load model call is in the main.py file\nAs I mentioned it works from another directory, but there is no reference to any absolute paths. Also, I have made sure that my package references are identical. e,g I have pinned them all down<\/p>\n<pre><code># Model\nmlflow==1.25.1\nprotobuf==3.20.1\npyspark==3.2.1\nscipy==1.6.2\nsix==1.15.0\n<\/code><\/pre>\n<p>also, the same docker file is used both places, which among other things, makes sure that the final resulting folder structure is the same<\/p>\n<pre><code>......other stuffs\n\nCOPY .\/app \/code\/app\nCOPY .\/model \/code\/model\n<\/code><\/pre>\n<p>what can explain it throwing this exception whereas in another location (on my PC), it works (same model artifacts) ?<\/p>\n<p>Since it uses load_model function, it should be able to read the parquet files ?<\/p>\n<p>Any question and I can explain.<\/p>\n<p>EDIT1: I have debugged this a little more in the docker container and it seems the parquet files in the itemFactors folder (listed in my screenshot above) are not getting copied over to my image , even though I have the copy command to copy all files under the model folder. It is copying the _started , _committed and _SUCCESS files, just not the parquet files. Anyone knows why would that be? I DO NOT have a .dockerignore file. Why are those files ignored while copying?<\/p>",
        "Challenge_closed_time":1655191745992,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655130099670,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to build an API using an MLflow model, but the load_model function fails with an error message stating that it is unable to infer schema for Parquet. The model artifacts are already downloaded to the folder \/model, and the load model call is in the main.py file. The user has ensured that the package references are identical and that the final resulting folder structure is the same. However, the parquet files in the itemFactors folder are not getting copied over to the image, even though the copy command is present. The user is seeking help to understand why the load_model function is throwing an exception and why the parquet files are not getting copied over to the image.",
        "Challenge_last_edit_time":1655134670387,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72604450",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":50.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":46,
        "Challenge_solved_time":17.1239783333,
        "Challenge_title":"MLflow load model fails Python",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":109.0,
        "Challenge_word_count":417,
        "Platform":"Stack Overflow",
        "Poster_created_time":1428654714763,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":596.0,
        "Poster_view_count":80.0,
        "Solution_body":"<p>I found the problem. Like I wrote in the EDIT1 of my post, with further observations, the parquet files were missing in the docker container. That was strange because I was copying the entire folder in my Dockerfile.<\/p>\n<p>I then realized that I was hitting this problem <a href=\"https:\/\/github.com\/moby\/buildkit\/issues\/1366\" rel=\"nofollow noreferrer\">mentioned here<\/a>. File paths exceeding 260 characters, silently fail and do not get copied over to the docker container. This was really frustrating because nothing failed during build and then during run, it gave me that cryptic error of &quot;unable to infer schema for parquet&quot;, essentially because the parquet files were not copied over during docker build.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1655202841060,
        "Solution_link_count":1.0,
        "Solution_readability":9.3,
        "Solution_reading_time":9.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":107.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1598606826112,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":41.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":222.6413147222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm using sagemaker 2.5.1 and tensorflow 2.3.0\nThe weird part is that the same code worked before, the only change that I could think of is the new release of the two libraries<\/p>",
        "Challenge_closed_time":1599662565023,
        "Challenge_comment_count":1,
        "Challenge_created_time":1598861056290,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to save a TensorFlow model in an S3 bucket using SageMaker 2.5.1. The error message 'KeyError: 'callable_inputs'' is being displayed. The user suspects that the issue might be related to the recent release of the two libraries.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63667022",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":5.3,
        "Challenge_reading_time":3.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":222.6413147222,
        "Challenge_title":"Getting KeyError : 'callable_inputs' when trying to save a TF model in S3 bucket",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":174.0,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Poster_created_time":1598606826112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>The problem is actually coming from smdebug version 0.9.1\nDowngrading to 0.8.1 solves the issue<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":1.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":15.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1599815370823,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Western Australia, Australia",
        "Answerer_reputation_count":185.0,
        "Answerer_view_count":33.0,
        "Challenge_adjusted_solved_time":0.3898341667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying mlflow sklearn auto logging, in colab, mlflow prints a lot of info messages and at times it crashes the browser. Attaching the pic of info logs<a href=\"https:\/\/i.stack.imgur.com\/RqvNM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RqvNM.png\" alt=\"mlflow info logs\" \/><\/a><\/p>\n<p>codes are in <a href=\"https:\/\/colab.research.google.com\/drive\/1wvHSgYk6boKW0AMPqIt-AByyFHSO26wm?usp=sharing\" rel=\"nofollow noreferrer\">this colab file<\/a><\/p>\n<p>Am not sure what am missing here, but the same code works fine without producing these info logs on my local computer.<\/p>",
        "Challenge_closed_time":1611053713910,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611052310507,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with MLFlow sklearn autologging in Colab, where it prints a lot of info messages and sometimes crashes the browser. The same code works fine without producing these info logs on the user's local computer.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65789715",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":10.6,
        "Challenge_reading_time":8.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.3898341667,
        "Challenge_title":"MLFlow sklearn autologging prints too many info messages in colab",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":194.0,
        "Challenge_word_count":77,
        "Platform":"Stack Overflow",
        "Poster_created_time":1608633925527,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Dubai - United Arab Emirates",
        "Poster_reputation_count":25.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>This is a known issue with MLFlow package, in which a hotfix has been raised.<\/p>\n<p>See here: <a href=\"https:\/\/github.com\/mlflow\/mlflow\/pull\/3978\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/pull\/3978<\/a><\/p>\n<p><strong>Description of fault<\/strong><\/p>\n<p>In MLflow 1.13.0 and 1.13.1, the following Python event logging message is emitted when a patched ML training function begins execution within a preexisting MLflow run.<\/p>\n<p>Unfortunately, for patched ML training routines that make child calls to other patched ML training routines (e.g. sklearn random forests that call fit() on a collection of sklearn DecisionTree instances), this event log is printed to stdout every time a child is called.<\/p>\n<p>This can produce hundreds of redundant event logging calls that don't provide value to the user.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.0,
        "Solution_reading_time":10.61,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":109.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.935,
        "Challenge_answer_count":0,
        "Challenge_body":"`TypeError: object of type 'NoneType' has no len()` happens when suggested [VSCode configuration for kedro](https:\/\/kedro.readthedocs.io\/en\/stable\/09_development\/01_set_up_vscode.html) is used for debugging. The error is due to commandline arguments being `None` when running pipeline directly through `run.py`.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/__main__.py\", line 45, in <module>\r\n    cli.main()\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/..\/debugpy\/server\/cli.py\", line 430, in main\r\n    run()\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/..\/debugpy\/server\/cli.py\", line 267, in run_file\r\n    runpy.run_path(options.target, run_name=compat.force_str(\"__main__\"))\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/pyzypad_example\/run.py\", line 75, in <module>\r\n    run_package()\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/pyzypad_example\/run.py\", line 71, in run_package\r\n    project_context.run()\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/context\/context.py\", line 725, in run\r\n    run_params=record_data, pipeline=filtered_pipeline, catalog=catalog\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/hooks.py\", line 286, in __call__\r\n    return self._hookexec(self, self.get_hookimpls(), kwargs)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/manager.py\", line 93, in _hookexec\r\n    return self._inner_hookexec(hook, methods, kwargs)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/manager.py\", line 87, in <lambda>\r\n    firstresult=hook.spec.opts.get(\"firstresult\") if hook.spec else False,\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 208, in _multicall\r\n    return outcome.get_result()\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 80, in get_result\r\n    raise ex[1].with_traceback(ex[2])\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 187, in _multicall\r\n    res = hook_impl.function(*args)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/kedro-mlflow\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py\", line 85, in before_pipeline_run\r\n    pipeline_name=run_params[\"pipeline_name\"],\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/kedro-mlflow\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py\", line 136, in _generate_kedro_command\r\n    if len(from_inputs) > 0:\r\nTypeError: object of type 'NoneType' has no len()\r\n```",
        "Challenge_closed_time":1601893558000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1601890192000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue where the plugin they are using is only compatible with kedro-mlflow version less than 0.8.0, and the `context` package has been moved or refactored, causing an exception to be thrown.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/78",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":22.2,
        "Challenge_reading_time":48.48,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":34,
        "Challenge_solved_time":0.935,
        "Challenge_title":"TypeError in _generate_kedro_command when debugging run in VSCode",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":212,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I see its fixed now so I'm closing this issue.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.5,
        "Solution_reading_time":0.54,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":10.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1525393797416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Philadelphia, USA",
        "Answerer_reputation_count":130.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":0.4764786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created an mlflow model with custom pyfunc. It shows the results when I send input to the loaded model in Jupyter notebook.\nHowever if I am trying to serve it to a local port<\/p>\n<pre><code>!mlflow models serve -m Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model -p 8001\n<\/code><\/pre>\n<p>I am getting this error<\/p>\n<pre><code> Traceback (most recent call last):\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/bin\/mlflow&quot;, line 10, in &lt;module&gt;\n    sys.exit(cli())\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 829, in __call__\n    return self.main(*args, **kwargs)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 782, in main\n    rv = self.invoke(ctx)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 610, in invoke\n    return callback(*args, **kwargs)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/models\/cli.py&quot;, line 56, in serve\n    install_mlflow=install_mlflow).serve(model_uri=model_uri, port=port,\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/models\/cli.py&quot;, line 163, in _get_flavor_backend\n    append_to_uri_path(underlying_model_uri, &quot;MLmodel&quot;), output_path=tmp.path())\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/tracking\/artifact_utils.py&quot;, line 76, in _download_artifact_from_uri\n    artifact_path=artifact_path, dst_path=output_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/local_artifact_repo.py&quot;, line 67, in download_artifacts\n    return super(LocalArtifactRepository, self).download_artifacts(artifact_path, dst_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py&quot;, line 140, in download_artifacts\n    return download_file(artifact_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py&quot;, line 105, in download_file\n    self._download_file(remote_file_path=fullpath, local_path=local_file_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/local_artifact_repo.py&quot;, line 95, in _download_file\n    shutil.copyfile(remote_file_path, local_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/shutil.py&quot;, line 120, in copyfile\n    with open(src, 'rb') as fsrc:\nFileNotFoundError: [Errno 2] No such file or directory: 'Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model\/MLmodel'\n<\/code><\/pre>",
        "Challenge_closed_time":1611840603100,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611838887777,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to serve an mlflow model locally and is encountering a \"FileNotFoundError\" when trying to run the model on a local port. The error message suggests that the file or directory specified in the model URI does not exist.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65937623",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":23.6,
        "Challenge_reading_time":47.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":0.4764786111,
        "Challenge_title":"Unable to serve an mlflow model locally",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1277.0,
        "Challenge_word_count":199,
        "Platform":"Stack Overflow",
        "Poster_created_time":1573739890560,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>From your error traceback, the model artifact can't be located. In your code, you are executing the 'mlflow' command from within a Jupyter Notebook. I would suggest trying the following:<\/p>\n<ol>\n<li>Check if your models artifacts are on the path you are using Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model<\/li>\n<li>Try opening a terminal, then <code>cd \/Home\/miniconda3\/envs<\/code> and  execute <code>mlflow models serve -m .\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model -p 8001<\/code><\/li>\n<li>MLFlow offers different solutions to serve a model, you can try to register your model and refer to it as &quot;models:\/{model_name}\/{stage}&quot; as mentioned in the Model Registry <a href=\"https:\/\/mlflow.org\/docs\/latest\/model-registry.html#serving-an-mlflow-model-from-model-registry\" rel=\"nofollow noreferrer\">docs<\/a><\/li>\n<\/ol>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.2,
        "Solution_reading_time":11.53,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":92.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1626973229036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":199.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":30.4097575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an already trained a TensorFlow model outside of SageMaker.<\/p>\n<p>I am trying to focus on deployment\/inference but I am facing issues with inference.<\/p>\n<p>For deployment I did this:<\/p>\n<pre><code>from sagemaker.tensorflow.serving import TensorFlowModel\ninstance_type = 'ml.c5.xlarge' \n\nmodel = TensorFlowModel(\n    model_data=model_data,\n    name= 'tfmodel1',\n    framework_version=&quot;2.2&quot;,\n    role=role, \n    source_dir='code',\n)\n\npredictor = model.deploy(endpoint_name='test', \n                                       initial_instance_count=1, \n                                       tags=tags,\n                                       instance_type=instance_type)\n<\/code><\/pre>\n<p>When I tried to infer the model I did this:<\/p>\n<pre><code>import PIL\nfrom PIL import Image\nimport numpy as np\nimport json\nimport boto3\n\nimage = PIL.Image.open('img_test.jpg')\nclient = boto3.client('sagemaker-runtime')\nbatch_size = 1\nimage = np.asarray(image.resize((512, 512)))\nimage = np.concatenate([image[np.newaxis, :, :]] * batch_size)\nbody = json.dumps({&quot;instances&quot;: image.tolist()})\n\nioc_predictor_endpoint_name = &quot;test&quot;\ncontent_type = 'application\/x-image'   \nioc_response = client.invoke_endpoint(\n    EndpointName=ioc_predictor_endpoint_name,\n    Body=body,\n    ContentType=content_type\n )\n<\/code><\/pre>\n<p>But I have this error:<\/p>\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from primary with message &quot;{&quot;error&quot;: &quot;Unsupported Media Type: application\/x-image&quot;}&quot;.\n<\/code><\/pre>\n<p>I also tried:<\/p>\n<pre><code>from sagemaker.predictor import Predictor\n\npredictor = Predictor(ioc_predictor_endpoint_name)\ninference_response = predictor.predict(data=body)\nprint(inference_response)\n<\/code><\/pre>\n<p>And have this error:<\/p>\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from primary with message &quot;{&quot;error&quot;: &quot;Unsupported Media Type: application\/octet-stream&quot;}&quot;.\n<\/code><\/pre>\n<p>What can I do ? I don't know if I missed something<\/p>",
        "Challenge_closed_time":1651191185627,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651077733827,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has encountered an error while trying to infer a pre-trained TensorFlow model deployed outside of SageMaker using an image. The user has deployed the model successfully but is facing issues with inference. The error message indicates an unsupported media type for the image format. The user has tried different methods for inference but still faces the same error. The user is seeking guidance on how to resolve the issue.",
        "Challenge_last_edit_time":1651081710500,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72032469",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.5,
        "Challenge_reading_time":27.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":31.5143888889,
        "Challenge_title":"How to infer a tensorflow pre trained deployed model with an image?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":145.0,
        "Challenge_word_count":191,
        "Platform":"Stack Overflow",
        "Poster_created_time":1606642099552,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":371.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>Have you tested this model locally? How does inference work with your TF model locally? This should show you how the input needs to be formatted for inference with that model in specific. Application\/x-image data format should be fine. Do you have a custom inference script? Check out this link here for adding an inference script with will let you control pre\/post processing and you can log each line to capture the error: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.9,
        "Solution_reading_time":7.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":77.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1437986390372,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"M\u00fcnchen, Deutschland",
        "Answerer_reputation_count":361.0,
        "Answerer_view_count":149.0,
        "Challenge_adjusted_solved_time":0.0983611111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have an sklearn k-means model. I am training the model and saving it in a pickle file so I can deploy it later using azure ml library. The model that I am training uses a custom Feature Encoder called <strong>MultiColumnLabelEncoder<\/strong>.\nThe pipeline model is defined as follow :<\/p>\n\n<pre><code># Pipeline\nkmeans = KMeans(n_clusters=3, random_state=0)\npipe = Pipeline([\n(\"encoder\", MultiColumnLabelEncoder()),\n('k-means', kmeans),\n])\n#Training the pipeline\nmodel = pipe.fit(visitors_df)\nprediction = model.predict(visitors_df)\n#save the model in pickle\/joblib format\nfilename = 'k_means_model.pkl'\njoblib.dump(model, filename)\n<\/code><\/pre>\n\n<p>The model saving works fine. The Deployment steps are the same as the steps in this link : <\/p>\n\n<p><a href=\"https:\/\/notebooks.azure.com\/azureml\/projects\/azureml-getting-started\/html\/how-to-use-azureml\/deploy-to-cloud\/model-register-and-deploy.ipynb\" rel=\"nofollow noreferrer\">https:\/\/notebooks.azure.com\/azureml\/projects\/azureml-getting-started\/html\/how-to-use-azureml\/deploy-to-cloud\/model-register-and-deploy.ipynb<\/a><\/p>\n\n<p>However the deployment always fails with this error :<\/p>\n\n<pre><code>  File \"\/var\/azureml-server\/create_app.py\", line 3, in &lt;module&gt;\n    from app import main\n  File \"\/var\/azureml-server\/app.py\", line 27, in &lt;module&gt;\n    import main as user_main\n  File \"\/var\/azureml-app\/main.py\", line 19, in &lt;module&gt;\n    driver_module_spec.loader.exec_module(driver_module)\n  File \"\/structure\/azureml-app\/score.py\", line 22, in &lt;module&gt;\n    importlib.import_module(\"multilabelencoder\")\n  File \"\/azureml-envs\/azureml_b707e8c15a41fd316cf6c660941cf3d5\/lib\/python3.6\/importlib\/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\nModuleNotFoundError: No module named 'multilabelencoder'\n<\/code><\/pre>\n\n<p>I understand that pickle\/joblib has some problems unpickling the custom function MultiLabelEncoder. That's why I defined this class in a separate python script (which I executed also). I called this custom function in the training python script, in the deployment script and in the scoring python file (score.py). The importing in the score.py file is not successful. \nSo my question is how can I import custom python module to azure ml deployment environment ?<\/p>\n\n<p>Thank you in advance.<\/p>\n\n<p>EDIT: \nThis is my .yml file<\/p>\n\n<pre><code>name: project_environment\ndependencies:\n  # The python interpreter version.\n  # Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.2\n\n- pip:\n  - multilabelencoder==1.0.4\n  - scikit-learn\n  - azureml-defaults==1.0.74.*\n  - pandas\nchannels:\n- conda-forge\n<\/code><\/pre>",
        "Challenge_closed_time":1575635052340,
        "Challenge_comment_count":1,
        "Challenge_created_time":1575463048677,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has trained an sklearn k-means model using a custom Feature Encoder called MultiColumnLabelEncoder and saved it in a pickle file for deployment using Azure ML library. However, the deployment fails with an error due to the inability to import the custom module MultiLabelEncoder. The user has defined the class in a separate python script and called it in the training, deployment, and scoring python files. The user is seeking a solution to import the custom python module to the Azure ML deployment environment.",
        "Challenge_last_edit_time":1575634698240,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59176241",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.4,
        "Challenge_reading_time":35.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":47.7787952778,
        "Challenge_title":"import custom python module in azure ml deployment environment",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2611.0,
        "Challenge_word_count":276,
        "Platform":"Stack Overflow",
        "Poster_created_time":1437986390372,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"M\u00fcnchen, Deutschland",
        "Poster_reputation_count":361.0,
        "Poster_view_count":149.0,
        "Solution_body":"<p>In fact, the solution was to import my customized class <strong>MultiColumnLabelEncoder<\/strong> as a pip package (You can find it through pip install multilllabelencoder==1.0.5).\nThen I passed the pip package to the .yml file or in the InferenceConfig of the azure ml environment.\nIn the score.py file, I imported the class as follows :<\/p>\n\n<pre><code>from multilabelencoder import multilabelencoder\ndef init():\n    global model\n\n    # Call the custom encoder to be used dfor unpickling the model\n    encoder = multilabelencoder.MultiColumnLabelEncoder() \n    # Get the path where the deployed model can be found.\n    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'k_means_model_45.pkl')\n    model = joblib.load(model_path)\n<\/code><\/pre>\n\n<p>Then the deployment was successful. \nOne more important thing is I had to use the same pip package (multilabelencoder) in the training pipeline as here :<\/p>\n\n<pre><code>from multilabelencoder import multilabelencoder \npipe = Pipeline([\n    (\"encoder\", multilabelencoder.MultiColumnLabelEncoder(columns)),\n    ('k-means', kmeans),\n])\n#Training the pipeline\ntrainedModel = pipe.fit(df)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.2,
        "Solution_reading_time":14.38,
        "Solution_score_count":4.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":132.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":28.6481963889,
        "Challenge_answer_count":2,
        "Challenge_body":"I make pytorch model with sagemaker, MMS.\nThis is my mms code.\n```python\n%%time\ninstance_type = 'c5.large'\n# accelerator_type = 'eia2.medium'\npredictor = mme.deploy(\n    initial_instance_count=1,\n    instance_type=f\"ml.{instance_type}\"\n)\n\nmme.add_model(model_data_source=model_path, model_data_path=\"model.tar.gz\")\nlist(mme.list_models())\n#> [ 'model.tar.gz']\n```\n\n\nI try to predict with this code.\n\n```python\nstart_time = time.time()\npredicted_value = predictor.predict(requests, target_model=\"LV1\")\nduration = time.time() - start_time\nprint(\"${:,.2f}, took {:,d} ms\\n\".format(predicted_value[0], int(duration * 1000)))\n```\n\nAnd, return error message.\n\n```\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"{\n  \"code\": 500,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Failed to start workers\"\n}\n```\n\nMMS with pytorch is 'little' difficult. X)\n\nhelp me, please.",
        "Challenge_closed_time":1660312923532,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660209790025,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error code 500 with type InternalServerException while trying to predict with a pytorch model using MMS in Sagemaker. The error message states that the workers failed to start, and the user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":1668077202772,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUBCxtcfyrTymZ7isHG3X5Qg\/problem-at-mms-predict-at-mms-sagemaker-error-code-500-type-internalserverexception",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.7,
        "Challenge_reading_time":13.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":28.6481963889,
        "Challenge_title":"[problem at MMS predict] At MMS(sagemaker), error code(500), type(InternalServerException)",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":119.0,
        "Challenge_word_count":97,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi ,\nI think your target model on the prediction needs to have the name of the model you have deployed - \nfor example , when you are adding the model with \nmme.add_model(model_data_source=model_path, model_data_path=\"model.tar.gz\")\nthe model_data_path contains the name of the model . From the sagemaker-examples:\n(https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb)\n**model_data_path is the relative path to the S3 prefix we specified above (i.e. model_data_prefix) where our endpoint will source models for inference requests.Since this is a relative path, we can simply pass the name of what we wish to call the model artifact at inference time (i.e. Chicago_IL.tar.gz). In your case \"model.tar.gz\".\nHowever, when predicting you call the model ,target_model=\"LV1\"?",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1660312923532,
        "Solution_link_count":1.0,
        "Solution_readability":13.0,
        "Solution_reading_time":11.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":103.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1342709052703,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"D\u00fcsseldorf, Germany",
        "Answerer_reputation_count":1889.0,
        "Answerer_view_count":654.0,
        "Challenge_adjusted_solved_time":25.8006916667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained the following Sagemaker model: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco<\/a><\/p>\n\n<p>I've tried both the JSON and RecordIO version. In both, the algorithm is tested on ONE sample image. However, I have a dataset of 2000 pictures, which I would like to test. I have saved the 2000 jpg pictures in a folder within an S3 bucket and I also have two .mat files (pics + ground truth). How can I apply this model to all 2000 pictures at once and then save the results, rather than doing it one picture at a time?<\/p>\n\n<p>I am using the code below to load a single picture from my S3 bucket:<\/p>\n\n<pre><code>object = bucket.Object('pictures\/pic1.jpg')\nobject.download_file('pic1.jpg')\nimg=mpimg.imread('pic1.jpg')\nimg_name = 'pic1.jpg'\nimgplot = plt.imshow(img)\nplt.show(imgplot)\n\nwith open(img_name, 'rb') as image:\n    f = image.read()\n    b = bytearray(f)\n    ne = open('n.txt','wb')\n    ne.write(b)\n\nimport json\nobject_detector.content_type = 'image\/jpeg'\nresults = object_detector.predict(b)\ndetections = json.loads(results)\nprint (detections['prediction'])\n<\/code><\/pre>",
        "Challenge_closed_time":1539965456440,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539872573950,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a Sagemaker object detection model and wants to test it on a dataset of 2000 pictures saved in an S3 bucket. They are currently using code to load a single picture from the bucket and test the model on it, but they want to know how to apply the model to all 2000 pictures at once and save the results.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52876202",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":18.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":25.8006916667,
        "Challenge_title":"How to bulk test the Sagemaker Object detection model with a .mat dataset or S3 folder of images?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":112.0,
        "Challenge_word_count":157,
        "Platform":"Stack Overflow",
        "Poster_created_time":1489873508190,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":509.0,
        "Poster_view_count":84.0,
        "Solution_body":"<p>I'm not sure if I understood your question correctly. However, if you want to feed multiple images to the model at once, you can create a multi-dimensional array of images (byte arrays) to feed the model.<\/p>\n\n<p>The code would look something like this.<\/p>\n\n<pre><code>import numpy as np\n...\n\n#  predict_images_list is a Python list of byte arrays\npredict_images = np.stack(predict_images_list)\n\nwith graph.as_default():\n    #  results is an list of typical results you'd get.\n    results = object_detector.predict(predict_images)\n<\/code><\/pre>\n\n<p>But, I'm not sure if it's a good idea to feed 2000 images at once. Better to batch them in 20-30 images at a time and predict. <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":8.31,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":99.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.6225,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nIs there a way to regularly checkpoint model artifact in a SageMaker training job for BYO training container?",
        "Challenge_closed_time":1586359356000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586331915000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to regularly checkpoint the model artifact during a SageMaker training job for a BYO training container.",
        "Challenge_last_edit_time":1668623326404,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUrXX2MIygS5igas27GrAhHw\/how-to-checkpoint-sagemaker-model-artifact-during-a-training-job",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":2.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":7.6225,
        "Challenge_title":"How to checkpoint SageMaker model artifact during a training job?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":403.0,
        "Challenge_word_count":28,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"If you specify a checkpoint configuration (***regardless of managed spot training***) when starting a training job, checkpointing will work. You can provide a local path and S3 path as follows (API reference):\n\n    \"CheckpointConfig\": { \n      \"LocalPath\": \"string\",\n      \"S3Uri\": \"string\"\n    }\n\nThe local path defaults to `\/opt\/ml\/checkpoints\/`, and then you specify the target path in S3 with `S3Uri`.\n\n***Given this configuration, SageMaker will configure an output channel with Continuous upload mode to Amazon S3***. At the time being, this results in running an agent on the hosts that watches the file system and continuously uploads data to Amazon S3. Similar behavior is applied when debugging is enabled, for delivering tensor data to Amazon S3.\n\nAs commented, `sagemaker-containers` implements its own code to save intermediate outputs and watching files on the file system, but I would rather rely on the functionality offered by the service to avoid dependencies on specific libraries where possible.\n\n**Note**: when using SageMaker Processing, which in my view can be considered an abstraction over training or, from another perspective, the foundation for training, you can configure an output channel to use continuous upload mode; further info [here][1].\n\n\n  [1]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ProcessingS3Output.html",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925569444,
        "Solution_link_count":1.0,
        "Solution_readability":16.1,
        "Solution_reading_time":16.88,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":188.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1619177157943,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":835.0,
        "Answerer_view_count":516.0,
        "Challenge_adjusted_solved_time":396.3547583334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Why <code> kfp.v2.dsl.Output<\/code> as function argument works without being provided?<\/p>\n<p>I am following <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/training-data-analyst\/blob\/master\/courses\/machine_learning\/deepdive2\/machine_learning_in_the_enterprise\/solutions\/create_run_vertex_pipeline.ipynb\" rel=\"nofollow noreferrer\">Create and run ML pipelines with Vertex Pipelines!<\/a> Jupyter notebook example from GCP.<\/p>\n<p>The function <code>classif_model_eval_metrics<\/code> takes <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> which have no default values.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],                # No default value set, hence must be mandatory\n    metricsc: Output[ClassificationMetrics], # No default value set, hence must be mandatory\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]): \n    # Full code at the bottom.\n<\/code><\/pre>\n<p>Hence those arguments should be mandatory, but the function is called without those arguments.<\/p>\n<pre><code>    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p>The entire pipeline code is below.<\/p>\n<pre><code>@kfp.dsl.pipeline(name=&quot;automl-tab-beans-training-v2&quot;,\n                  pipeline_root=PIPELINE_ROOT)\ndef pipeline(\n    bq_source: str = &quot;bq:\/\/aju-dev-demos.beans.beans1&quot;,\n    display_name: str = DISPLAY_NAME,\n    project: str = PROJECT_ID,\n    gcp_region: str = &quot;us-central1&quot;,\n    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str = '{&quot;auRoc&quot;: 0.95}',\n):\n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project, display_name=display_name, bq_source=bq_source\n    )\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        column_transformations=COLUMNS,\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],\n        target_column=&quot;Class&quot;,\n    )\n    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Why does it work and what are <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> of type <code>kfp.v2.dsl.Output<\/code>?<\/p>\n<hr \/>\n<h2>classif_model_eval_metrics function code<\/h2>\n<pre><code>from kfp.v2.dsl import (\n    Dataset, Model, Output, Input, \n    OutputPath, ClassificationMetrics, Metrics, component\n)\n\n@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]):  # Return parameter.\n    &quot;&quot;&quot;Renders evaluation metrics for an AutoML Tabular classification model.\n    Retrieves the classification model evaluation and render the ROC and confusion matrix\n    for the model. Determine whether the model is sufficiently accurate to deploy.\n    &quot;&quot;&quot;\n    import json\n    import logging\n    from google.cloud import aiplatform\n\n    # Fetch model eval info\n    def get_eval_info(client, model_name):\n        from google.protobuf.json_format import MessageToDict\n        response = client.list_model_evaluations(parent=model_name)\n        metrics_list = []\n        metrics_string_list = []\n        for evaluation in response:\n            metrics = MessageToDict(evaluation._pb.metrics)\n            metrics_str = json.dumps(metrics)\n            metrics_list.append(metrics)\n            metrics_string_list.append(metrics_str)\n        return (\n            evaluation.name,\n            metrics_list,\n            metrics_string_list,\n        )\n\n    def classification_thresholds_check(metrics_dict, thresholds_dict):\n        for k, v in thresholds_dict.items():\n            if k in [&quot;auRoc&quot;, &quot;auPrc&quot;]:  # higher is better\n                if metrics_dict[k] &lt; v:  # if under threshold, don't deploy\n                    return False\n        return True\n\n    def log_metrics(metrics_list, metricsc):\n        test_confusion_matrix = metrics_list[0][&quot;confusionMatrix&quot;]\n        logging.info(&quot;rows: %s&quot;, test_confusion_matrix[&quot;rows&quot;])\n        # log the ROC curve\n        fpr = [], tpr = [], thresholds = []\n        for item in metrics_list[0][&quot;confidenceMetrics&quot;]:\n            fpr.append(item.get(&quot;falsePositiveRate&quot;, 0.0))\n            tpr.append(item.get(&quot;recall&quot;, 0.0))\n            thresholds.append(item.get(&quot;confidenceThreshold&quot;, 0.0))\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        # log the confusion matrix\n        annotations = []\n        for item in test_confusion_matrix[&quot;annotationSpecs&quot;]:\n            annotations.append(item[&quot;displayName&quot;])\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n        # log textual metrics info as well\n        for metric in metrics_list[0].keys():\n            if metric != &quot;confidenceMetrics&quot;:\n                val_string = json.dumps(metrics_list[0][metric])\n                metrics.log_metric(metric, val_string)\n        # metrics.metadata[&quot;model_type&quot;] = &quot;AutoML Tabular classification&quot;\n\n    aiplatform.init(project=project)\n\n    client = aiplatform.gapic.ModelServiceClient(client_options={&quot;api_endpoint&quot;: api_endpoint})\n    eval_name, metrics_list, metrics_str_list = get_eval_info(\n        client, model.uri.replace(&quot;aiplatform:\/\/v1\/&quot;, &quot;&quot;)\n    )\n    log_metrics(metrics_list, metricsc)\n    thresholds_dict = json.loads(thresholds_dict_str)\n\n    return (&quot;true&quot;,) if classification_thresholds_check(metrics_list[0], thresholds_dict) else (&quot;false&quot;, )\n<\/code><\/pre>",
        "Challenge_closed_time":1652802212843,
        "Challenge_comment_count":5,
        "Challenge_created_time":1651375335713,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is following a GCP Jupyter notebook example and has encountered an issue where the function 'classif_model_eval_metrics' takes 'metrics: Output[Metrics]' and 'metricsc: Output[ClassificationMetrics]' as mandatory arguments, but the function is called without those arguments. The user is confused about why it works and what 'metrics: Output[Metrics]' and 'metricsc: Output[ClassificationMetrics]' of type 'kfp.v2.dsl.Output' are.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72073763",
        "Challenge_link_count":3,
        "Challenge_participation_count":6,
        "Challenge_readability":23.4,
        "Challenge_reading_time":91.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":50,
        "Challenge_solved_time":396.3547583334,
        "Challenge_title":"GCP Vertex Pipeline - Why kfp.v2.dsl.Output as function arguments work without being provided?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":358.0,
        "Challenge_word_count":467,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416648155470,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":14749.0,
        "Poster_view_count":968.0,
        "Solution_body":"<p>The custom component is defined as a Python function with a <code>@kfp.v2.dsl.component<\/code> decorator.<\/p>\n<p>The <code>@component<\/code> decorator specifies three optional arguments: the base container image to use; any packages to install; and the yaml file to which to write the component specification.<\/p>\n<p>The component function, <code>classif_model_eval_metrics<\/code>, has some input parameters.  The model parameter is an input <code>kfp.v2.dsl.Model artifact<\/code>.<\/p>\n<p>The two function args, <code>metrics<\/code> and <code>metricsc<\/code>, are component Outputs, in this case of types Metrics and ClassificationMetrics. They\u2019re not explicitly passed as inputs to the component step, but rather are automatically instantiated and can be used in the component.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-3:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;,\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,  # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n)\n<\/code><\/pre>\n<p>For example, in the function below, we\u2019re calling <code>metricsc.log_roc_curve()<\/code> and <code>metricsc.log_confusion_matrix()<\/code> to render these visualizations in the Pipelines UI. These Output params become component outputs when the component is compiled, and can be consumed by other pipeline steps.<\/p>\n<pre><code>def log_metrics(metrics_list, metricsc):\n        ...\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        ...\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n<\/code><\/pre>\n<p>For more information you can refer to this <a href=\"https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/use-vertex-pipelines-build-automl-classification-end-end-workflow\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":19.5,
        "Solution_reading_time":27.41,
        "Solution_score_count":2.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":180.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.9119444444,
        "Challenge_answer_count":0,
        "Challenge_body":"Logging using Weights and Biases does not differentiate between training and testing modes in `logbook.write_metric_log({  'mode': 'train' ... })`",
        "Challenge_closed_time":1583456108000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1583449225000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the writing of checkpoints to wandb during training. Despite starting a non-dry run via a notebook, no checkpoints are being uploaded to wandb. The expected behavior is for checkpoints to be uploaded whenever there is a better one available during training. The user is struggling to understand the symlinking model of wandb, which requires checkpoints to be under the project root, making it difficult to run multiple experiments simultaneously.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/shagunsodhani\/ml-logger\/issues\/25",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":2.73,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":88.0,
        "Challenge_repo_star_count":17.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1.9119444444,
        "Challenge_title":"[BUG] Weights & Biases logging does not differentiate between modes",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":25,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@koustuvsinha Thanks for bringing this up. Could you try the new version?\r\n\r\nWhen constructing the logbook, pass an additional parameter:\r\n\r\n```\r\nfrom ml_logger import logbook as ml_logbook\r\nlogbook_config = ml_logbook.make_config(\r\n    logger_file_path = <path to write logs>,\r\n    wandb_config = <wandb config or None>,\r\n    wandb_prefix_key = \"mode\",\r\n)\r\n```",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":4.26,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":40.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1544390307847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Palo Alto, CA, USA",
        "Answerer_reputation_count":151.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":270.3311944445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've seen examples of labeling data using SageMaker Ground Truth and then using that data to train off-the-shelf SageMaker models. However, am I able to use this same annotation format with TensorFlow Script Mode? <\/p>\n\n<p>More specifically, I have a tensorflow.keras model I'm training using TF Script Mode, and I'd like to take data labeled with Ground Truth and convert my script from File mode to Pipe mode.<\/p>",
        "Challenge_closed_time":1549915976767,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548942784467,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to determine if they can use the annotation format from SageMaker Ground Truth with TensorFlow Script Mode to train a tensorflow.keras model and convert their script from File mode to Pipe mode.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54462105",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":5.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":270.3311944445,
        "Challenge_title":"SageMaker Ground Truth with TensorFlow",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":454.0,
        "Challenge_word_count":72,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>I am from Amazon SageMaker Ground Truth team and happy to assist you in your experiment. Just to be clear our understanding, are you running TF model in SageMaker using TF estimator in your own container (<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst<\/a>)? <\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":19.3,
        "Solution_reading_time":5.85,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":41.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1562055808543,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":895.0,
        "Answerer_view_count":53.0,
        "Challenge_adjusted_solved_time":3027.8199811111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to deploy a TF2.0 model to SageMaker. So far, I managed to train the model and save it into an S3 bucket but when I'm calling the <code>.deploy()<\/code> method, I get the following error from cloud Watch <\/p>\n\n<p><code>ValueError: no SavedModel bundles found!<\/code><\/p>\n\n<p>Here is my training script: <\/p>\n\n<pre><code>### Code to add in a tensorflow_estimator.py file\n\nimport argparse\nimport os\nimport pathlib\nimport tensorflow as tf\n\n\nif __name__ == '__main__':\n\n\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch_size', type=int, default=100)\n    parser.add_argument('--learning_rate', type=float, default=0.1)\n\n    # Data, model, and output directories\n    parser.add_argument('--output-data-dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n    args, _ = parser.parse_known_args()\n\n    print(\"##### ARGS ##### \\n{}\".format(args))\n\n    # Get files \n    path = pathlib.Path(args.train)\n\n    # Print out folder content \n    for item in path.iterdir():\n        print(\"##### DIRECTORIES ##### \\n{}\".format(item))\n\n    # Get all images \n    all_images = list(path.glob(\"*\/*\"))\n    all_image_paths = [str(path) for path in list(path.glob(\"*\/*\"))]\n\n    # Transform images into tensors\n    def preprocess_and_load_images(path):\n        image = tf.io.read_file(path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.resize(image, [192, 192])\n        return image\n\n    # Apply preprocessing function\n    ds_paths = tf.data.Dataset.from_tensor_slices(all_image_paths)\n    ds_images = ds_paths.map(preprocess_and_load_images)\n\n    # Map Labels\n    labels = []\n    for data in path.iterdir():  \n        if data.is_dir():               \n            labels += [data.name]    \n\n    labels_index = {}\n    for i,label in enumerate(labels):\n        labels_index[label]=i\n\n    print(\"##### Label Index ##### \\n{}\".format(labels_index))\n\n    all_image_labels = [labels_index[path.parent.name] for path in list(path.glob(\"*\/*\"))]\n\n    # Create a tf Dataset\n    labels_ds = tf.data.Dataset.from_tensor_slices(all_image_labels)\n\n    # Zip train and labeled dataset\n    full_ds = tf.data.Dataset.zip((ds_images, labels_ds))\n\n    # Shuffle Dataset and batch it \n    full_ds = full_ds.shuffle(len(all_images)).batch(args.batch_size)\n\n    # Create a pre-trained model \n    base_model = tf.keras.applications.InceptionV3(input_shape=(192,192,3), \n                                               include_top=False,\n                                               weights = \"imagenet\"\n                                               )\n\n    base_model.trainable = False\n    model = tf.keras.Sequential([\n                base_model,\n                tf.keras.layers.GlobalAveragePooling2D(),\n                tf.keras.layers.Dense(len(labels), activation=\"softmax\")\n            ])\n\n    initial_learning_rate = args.learning_rate\n\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate,\n        decay_steps=1000,\n        decay_rate=0.96,\n        staircase=True) \n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr_schedule),\n              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\n\n    # Train the model\n    model.fit(full_ds, epochs=args.epochs)\n\n    # Save the model \n    model.save(os.path.join(args.model_dir, \"tf_model\"), save_format=\"tf\")\n\ndef model_fn(model_dir):\n    classifier = tf.keras.models.load_model(os.path.join(model_dir, \"tf_model\"))\n    return classifier\n<\/code><\/pre>\n\n<p>And here is the code that I wrote into Colab <\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ntf_estimator = TensorFlow(entry_point='tensorflow_estimator.py', \n                          role=role,\n                          train_instance_count=1, \n                          train_instance_type='ml.m5.large',\n                          framework_version='2.0.0', \n                          sagemaker_session=sagemaker_session,\n                          output_path=s3_output_location,\n                          hyperparameters={'epochs': 1,\n                                           'batch_size': 30,\n                                           'learning_rate': 0.001},\n                          py_version='py3')\n\n\ntf_estimator.fit({\"train\":train_data})\n\nfrom sagemaker.tensorflow.serving import Model\n\nmodel = Model(model_data='s3:\/\/path\/to\/model.tar.gz', \n              role=role,\n              framework_version=\"2.0.0\",\n              sagemaker_session=sagemaker_session)\n\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.m5.large')\n<\/code><\/pre>\n\n<p>I already tried to look at <a href=\"https:\/\/stackoverflow.com\/questions\/57172147\/no-savedmodel-bundles-found-on-tensorflow-hub-model-deployment-to-aws-sagemak\">this thread<\/a> but I actually don't have the problem of versions in my tar.gz file as the structure is the following : <\/p>\n\n<pre><code>\u251c\u2500\u2500 assets\n\u251c\u2500\u2500 saved_model.pb\n\u2514\u2500\u2500 variables\n    \u251c\u2500\u2500 variables.data-00000-of-00001\n    \u2514\u2500\u2500 variables.index\n<\/code><\/pre>\n\n<p>I feel I might be wrong when defining <code>model_fn()<\/code> in my training script but definitely don't what to replace that with. Would you guys have an idea? <\/p>\n\n<p>Thanks a lot for your help!  <\/p>",
        "Challenge_closed_time":1579799847967,
        "Challenge_comment_count":1,
        "Challenge_created_time":1579796394240,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering a \"ValueError: no SavedModel bundles found!\" error when trying to deploy a TensorFlow 2.0 model to SageMaker. They have successfully trained and saved the model to an S3 bucket, but are encountering issues when calling the .deploy() method. The user has provided their training script and deployment code, and has already checked for version issues in their tar.gz file. They suspect that the issue may be with their model_fn() function but are unsure of how to proceed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59882941",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":16.2,
        "Challenge_reading_time":64.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":68,
        "Challenge_solved_time":0.9593686111,
        "Challenge_title":"ValueError: no SavedModel bundles found! when trying to deploy a TF2.0 model to SageMaker",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":943.0,
        "Challenge_word_count":395,
        "Platform":"Stack Overflow",
        "Poster_created_time":1562055808543,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":895.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>I actually tried to modify my training script to the following : <\/p>\n\n<pre><code>### Code to add in a tensorflow_estimator.py file\n\nimport argparse\nimport os\nimport pathlib\nimport tensorflow as tf\n\n\nif __name__ == '__main__':\n\n\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch_size', type=int, default=100)\n    parser.add_argument('--learning_rate', type=float, default=0.1)\n\n    # Data, model, and output directories\n    parser.add_argument('--output-data-dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n    args, _ = parser.parse_known_args()\n\n    print(\"##### ARGS ##### \\n{}\".format(args))\n\n    # Get files \n    path = pathlib.Path(args.train)\n\n    # Print out folder content \n    for item in path.iterdir():\n        print(\"##### DIRECTORIES ##### \\n{}\".format(item))\n\n    # Get all images \n    all_images = list(path.glob(\"*\/*\"))\n    all_image_paths = [str(path) for path in list(path.glob(\"*\/*\"))]\n\n    # Transform images into tensors\n    def preprocess_and_load_images(path):\n        image = tf.io.read_file(path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.resize(image, [192, 192])\n        return image\n\n    # Apply preprocessing function\n    ds_paths = tf.data.Dataset.from_tensor_slices(all_image_paths)\n    ds_images = ds_paths.map(preprocess_and_load_images)\n\n    # Map Labels\n    labels = []\n    for data in path.iterdir():  \n        if data.is_dir():               \n            labels += [data.name]    \n\n    labels_index = {}\n    for i,label in enumerate(labels):\n        labels_index[label]=i\n\n    print(\"##### Label Index ##### \\n{}\".format(labels_index))\n\n    all_image_labels = [labels_index[path.parent.name] for path in list(path.glob(\"*\/*\"))]\n\n    # Create a tf Dataset\n    labels_ds = tf.data.Dataset.from_tensor_slices(all_image_labels)\n\n    # Zip train and labeled dataset\n    full_ds = tf.data.Dataset.zip((ds_images, labels_ds))\n\n    # Shuffle Dataset and batch it \n    full_ds = full_ds.shuffle(len(all_images)).batch(args.batch_size)\n\n    # Create a pre-trained model \n    base_model = tf.keras.applications.InceptionV3(input_shape=(192,192,3), \n                                               include_top=False,\n                                               weights = \"imagenet\"\n                                               )\n\n    base_model.trainable = False\n    model = tf.keras.Sequential([\n                base_model,\n                tf.keras.layers.GlobalAveragePooling2D(),\n                tf.keras.layers.Dense(len(labels), activation=\"softmax\")\n            ])\n\n    initial_learning_rate = args.learning_rate\n\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate,\n        decay_steps=1000,\n        decay_rate=0.96,\n        staircase=True) \n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr_schedule),\n              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\n\n    # Train the model\n    model.fit(full_ds, epochs=args.epochs)\n\n    # Save the model \n    model.save(os.path.join(args.model_dir, \"tensorflow_model\/1\"), save_format=\"tf\")\n\n<\/code><\/pre>\n\n<p>It seems that it's important to have a numerical name for your folder:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Save the model\nmodel.save(os.path.join(args.model_dir, \"tensorflow_model\/1\"), save_format=\"tf\")\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1590696546172,
        "Solution_link_count":0.0,
        "Solution_readability":18.3,
        "Solution_reading_time":44.06,
        "Solution_score_count":4.0,
        "Solution_sentence_count":46.0,
        "Solution_word_count":244.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1540226093150,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":119.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":5.7906316667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using Amazon's SageMaker Studio Lab to train a model using a certain dataset.<br>\nThe code is as follow (which saves the History object in history variable):<\/p>\n<pre><code>model = tf.keras.models.load_model('best_model.hdf5')  # Every run after runtime end, use the last saved model\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\ncheckpointer = ModelCheckpoint(filepath='best_model.hdf5', verbose=1, save_best_only=True)\ncsv_logger = CSVLogger('history.log')\n\nhistory = model.fit_generator(train_generator,\n                    steps_per_epoch = nb_train_samples \/\/ batch_size,\n                    validation_data=validation_generator,\n                    validation_steps=nb_validation_samples \/\/ batch_size,\n                    epochs=30,\n                    verbose=1,\n                    callbacks=[csv_logger, checkpointer])\n<\/code><\/pre>\n<p>I had to make several pauses due to ending runtime, and with each pause I saved the .log file. Now after appending those .log files, I'm trying to access them using the standard accuracy and loss plotting methods:<\/p>\n<pre><code>def plot_accuracy(history,title):\n    plt.title(title)\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train_accuracy', 'validation_accuracy'], loc='best')\n    plt.show()\ndef plot_loss(history,title):\n    plt.title(title)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train_loss', 'validation_loss'], loc='best')\n    plt.show()\n<\/code><\/pre>\n<p>But the issue is I can't seem to manage to receive a working History object file.<br>\nAmong the things I tried:<br>\nI read about this possible method, of re-loading the model and trying to get it's history, but it didn't work, &quot;'NoneType' object has no attribute 'history'&quot;<\/p>\n<pre><code>model = tf.keras.models.load_model('best_model.hdf5')\nhistory = model.history\n<\/code><\/pre>\n<p>Another try was using pandas package and loading the file, which generated an error &quot;'DataFrame' object has no attribute 'history'&quot;:<\/p>\n<pre><code>history = pd.read_csv('history.log', sep=',', engine='python')\n<\/code><\/pre>\n<p>And this try generated a CSVLogger object, &quot;'CSVLogger' object has no attribute 'history'&quot;:<\/p>\n<pre><code>history = CSVLogger('history.log')\n<\/code><\/pre>\n<p>Appreciate any help on how to recover the History object, so I can plot those results (if it's even possible?)...<br>\nThanks.<\/p>",
        "Challenge_closed_time":1662234715187,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662213868913,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to access the .log files after training a model using .fit_generator with multiple pauses. The user has tried different methods to access the History object file, including re-loading the model and trying to get its history, using pandas package to load the file, and generating a CSVLogger object, but none of them worked. The user is seeking help to recover the History object to plot the results.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73592834",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":17.6,
        "Challenge_reading_time":33.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":5.7906316667,
        "Challenge_title":"Access .log files acc\\loss after training model (using .fit_generator) with multiple pauses",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":54.0,
        "Challenge_word_count":248,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540226093150,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":119.0,
        "Poster_view_count":32.0,
        "Solution_body":"<p>Instead of recreating the History object, what I did was read the .log file using pandas package, <code>read_csv<\/code> method, and create a DataFrame data structure with the wanted columns and plot it. Code below:<\/p>\n<pre><code>history = pd.read_csv('history.log')\nhistory_acc = pd.DataFrame(history, columns=[&quot;accuracy&quot;, &quot;val_accuracy&quot;])\nhistory_loss = pd.DataFrame(history, columns=[&quot;loss&quot;, &quot;val_loss&quot;])\nplot_accuracy(history_acc,'plot title...')\nplot_loss(history_loss,'plot title...')\n<\/code><\/pre>\n<br>\n<pre><code>def plot_accuracy(history,title):\n    plt.title(title)\n    plt.plot(history)\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train_accuracy', 'validation_accuracy'], loc='best')\n    plt.show()\ndef plot_loss(history,title):\n    plt.title(title)\n    plt.plot(history)\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train_loss', 'validation_loss'], loc='best')\n    plt.show()\n<\/code><\/pre>\n<p>Hope this helps someone having the same issue as I did in the future.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.9,
        "Solution_reading_time":13.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":85.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1408529239847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Dublin, Ireland",
        "Answerer_reputation_count":1652.0,
        "Answerer_view_count":293.0,
        "Challenge_adjusted_solved_time":17.9931061111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using Weights and Biases to track my deep learning models. To monitor everything I use the <code>WandbCallback<\/code> in <code>.fit<\/code>.\nIn the <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/integrations\/keras\/wandbcallback\" rel=\"nofollow noreferrer\">WandbCallback documentation<\/a> there is the keyword <code>save_graph<\/code> which defaults to True. The description is very brief and I wondered what that saved graph is and what it's for? Is saving a graph a costly operation? For what is it needed? (like does it complement something else, like saving the best model?)<\/p>",
        "Challenge_closed_time":1636642289972,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636577514790,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using Weights and Biases to track their deep learning models and is using the WandbCallback in .fit to monitor everything. They are curious about the \"save_graph\" keyword in the WandbCallback documentation, which defaults to True, and want to know what the saved graph is for and if it is a costly operation. They also want to know if saving the graph complements something else, like saving the best model.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69920078",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":8.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":17.9931061111,
        "Challenge_title":"What does \"save_graph\" keyword in WandbCallback mean?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":60.0,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Poster_created_time":1600299419448,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":62.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>That is used to create log a <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/data-types\/graph\" rel=\"nofollow noreferrer\">wandb.Graph<\/a> of the model. This class is typically used for saving and diplaying neural net models. It represents the graph as an array of nodes and edges. The nodes can have labels that can be visualized by wandb. Here's an example of the graph that it produces: <a href=\"https:\/\/wandb.ai\/l2k2\/keras-examples\/runs\/ieqy2e9h\/model\" rel=\"nofollow noreferrer\">https:\/\/wandb.ai\/l2k2\/keras-examples\/runs\/ieqy2e9h\/model<\/a><\/p>\n<p>Here's the code that does that within the callback. <a href=\"https:\/\/github.com\/wandb\/client\/blob\/1609f82c84e2244ed8fe62c746099d2094bd746a\/wandb\/integration\/keras\/keras.py#L552\" rel=\"nofollow noreferrer\">https:\/\/github.com\/wandb\/client\/blob\/1609f82c84e2244ed8fe62c746099d2094bd746a\/wandb\/integration\/keras\/keras.py#L552<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":12.1,
        "Solution_reading_time":11.75,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":74.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1656670919183,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":76.5733497222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm new with ruby and I want to use GCP AIPlatform but I'm struggeling with the payload.<\/p>\n<p>So far, I have :<\/p>\n<pre class=\"lang-rb prettyprint-override\"><code>client = ::Google::Cloud::AIPlatform::V1::PredictionService::Client.new do |config|\n  config.endpoint = &quot;#{location}-aiplatform.googleapis.com&quot;\nend\n\nimg = File.open(imgPath, 'rb') do |img|\n  'data:image\/png;base64,' + Base64.strict_encode64(img.read)\nend\n\ninstance = Instance.new(:content =&gt; img)\n\nrequest = Google::Cloud::AIPlatform::V1::PredictRequest.new(\n  endpoint: &quot;projects\/#{project}\/locations\/#{location}\/endpoints\/#{endpoint}&quot;,\n  instances: [instance]\n)\n\nresult = client.predict request\np result\n<\/code><\/pre>\n<p>Here is my proto<\/p>\n<pre><code>message Instance {\n  required bytes content = 1;\n};\n<\/code><\/pre>\n<p>But I have the following error : <code>Invalid type Instance to assign to submessage field 'instances'<\/code><\/p>\n<p>I read the documentation but for ruby SDK it's a bit light.\nThe parameters are OK, the JS example here : <a href=\"https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-image-object-detection.js\" rel=\"nofollow noreferrer\">https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-image-object-detection.js<\/a> is working with those parameters<\/p>\n<p>What am I doing wrong ?<\/p>",
        "Challenge_closed_time":1656947266916,
        "Challenge_comment_count":1,
        "Challenge_created_time":1656671602857,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is new to Ruby and is trying to use GCP AIPlatform for image classification prediction. They are struggling with the payload and are encountering an error message \"Invalid type Instance to assign to submessage field 'instances'\". The user has provided their code and proto and is seeking help to identify the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72827960",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":15.3,
        "Challenge_reading_time":18.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":76.5733497222,
        "Challenge_title":"How to get image classification prediction from GCP AIPlatform in ruby?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":54.0,
        "Challenge_word_count":126,
        "Platform":"Stack Overflow",
        "Poster_created_time":1656670919183,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I managed it<\/p>\n<pre class=\"lang-rb prettyprint-override\"><code>client = Google::Cloud::AIPlatform::V1::PredictionService::Client.new do |config|\n  config.endpoint = &quot;#{location}-aiplatform.googleapis.com&quot;\nend\n\nimg = File.open(imgPath, 'rb') do |img|\n  Base64.strict_encode64(img.read)\nend\n\ninstance = Google::Protobuf::Value.new(:struct_value =&gt; {:fields =&gt; {\n  :content =&gt; {:string_value =&gt; img}\n}})\nendpoint = &quot;projects\/#{project}\/locations\/#{location}\/endpoints\/#{endpoint}&quot;\n\n\nrequest = Google::Cloud::AIPlatform::V1::PredictRequest.new(\n  endpoint: endpoint,\n  instances: [instance]\n)\n\nresult = client.predict request\np result\n<\/code><\/pre>\n<p>The use of the Google::Protobuf::Value looks ugly to me but it works<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.6,
        "Solution_reading_time":9.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":55.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1392296244356,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Chicago, IL, USA",
        "Answerer_reputation_count":1020.0,
        "Answerer_view_count":206.0,
        "Challenge_adjusted_solved_time":17.7042919445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am finetuning multiple models using for loop as follows.<\/p>\n<pre><code>for file in os.listdir(args.data_dir):\n    finetune(args, file)\n<\/code><\/pre>\n<p>BUT <code>wandb<\/code> website shows plots and logs only for the first file i.e., <code>file1<\/code> in <code>data_dir<\/code> although it is training and saving models for other files. It feels very strange behavior.<\/p>\n<pre><code>wandb: Synced bertweet-base-finetuned-file1: https:\/\/wandb.ai\/***\/huggingface\/runs\/***\n<\/code><\/pre>\n<p>This is a small snippet of <strong>finetuning<\/strong> code with Huggingface:<\/p>\n<pre><code>def finetune(args, file):\n    training_args = TrainingArguments(\n        output_dir=f'{model_name}-finetuned-{file}',\n        overwrite_output_dir=True,\n        evaluation_strategy='no',\n        num_train_epochs=args.epochs,\n        learning_rate=args.lr,\n        weight_decay=args.decay,\n        per_device_train_batch_size=args.batch_size,\n        per_device_eval_batch_size=args.batch_size,\n        fp16=True, # mixed-precision training to boost speed\n        save_strategy='no',\n        seed=args.seed,\n        dataloader_num_workers=4,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset['train'],\n        eval_dataset=None,\n        data_collator=data_collator,\n    )\n    trainer.train()\n    trainer.save_model()\n<\/code><\/pre>",
        "Challenge_closed_time":1650469853183,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650379721377,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with the Wandb website for Huggingface Trainer, where the website is only showing plots and logs for the first model in the data directory, even though the user is training and saving models for other files. The user is using a for loop to fine-tune multiple models and has shared a code snippet for the same.",
        "Challenge_last_edit_time":1650406117732,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71926953",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":15.8,
        "Challenge_reading_time":17.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":25.0366127778,
        "Challenge_title":"Wandb website for Huggingface Trainer shows plots and logs only for the first model",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":178.0,
        "Challenge_word_count":108,
        "Platform":"Stack Overflow",
        "Poster_created_time":1392296244356,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Chicago, IL, USA",
        "Poster_reputation_count":1020.0,
        "Poster_view_count":206.0,
        "Solution_body":"<p><code>wandb.init(reinit=True)<\/code> and <code>run.finish()<\/code> helped me to log the models <strong>separately<\/strong> on wandb website.<\/p>\n<p>The working code looks like below:<\/p>\n<pre><code>\nfor file in os.listdir(args.data_dir):\n    finetune(args, file)\n\nimport wandb\ndef finetune(args, file):\n    run = wandb.init(reinit=True)\n    ...\n    run.finish()\n<\/code><\/pre>\n<p>Reference: <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/launch#how-do-i-launch-multiple-runs-from-one-script\" rel=\"nofollow noreferrer\">https:\/\/docs.wandb.ai\/guides\/track\/launch#how-do-i-launch-multiple-runs-from-one-script<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.5,
        "Solution_reading_time":8.14,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":40.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":31.5461152778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to save images that I configure during training to the output bucket in sagemaker.  I've read that all the information that needs to be saved during training goes into the model.tar.gz file.  I've tried saving plots using the model_dir and the output_data_dir to no avail.  The model itself is saved properly, but the additional information is not being stored with it.  I want to reload this additional information (the saved images) during inference but have heard that storing all the information in the model.tar.gz can cause slow inference.  I would love some help.<\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>from sagemaker.pytorch import PyTorch\nestimator = PyTorch(entry_point='XXXXXXXX\/AWS\/mnist.py',\n                    role=role,\n                    py_version='py3',\n                    framework_version='1.8.0',\n                    instance_count=1,\n                    instance_type='ml.c5.xlarge',\n                    output_path='s3:\/\/XXXXX-bucket\/',\n                    )<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code># mnist.py\nimport os\nimport pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nimport argparse\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import ToTensor\nfrom torchvision.io import read_image\nfrom torch import nn\nimport matplotlib.pyplot as plt\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\n\ndef train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X.to(device))\n        loss = loss_fn(pred, y.to(device))\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}\/{size:&gt;5d}]\")\n\n\ndef test_loop(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X.to(device))\n            test_loss += loss_fn(pred, y.to(device)).item()\n            correct += (pred.argmax(1) == y.to(device)).type(torch.float).sum().item()\n\n    test_loss \/= num_batches\n    correct \/= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")\n\n# Initialize the loss function\nif __name__=='__main__':\n    # default to the value in environment variable `SM_MODEL_DIR`. Using args makes the script more portable.\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n\n    args, _ = parser.parse_known_args()\n\n    training_data = datasets.FashionMNIST(\n        root=\"data\",\n        train=True,\n        download=True,\n        transform=ToTensor()\n    )\n\n    test_data = datasets.FashionMNIST(\n        root=\"data\",\n        train=False,\n        download=True,\n        transform=ToTensor()\n    )\n\n    labels_map = {\n        0: \"T-Shirt\",\n        1: \"Trouser\",\n        2: \"Pullover\",\n        3: \"Dress\",\n        4: \"Coat\",\n        5: \"Sandal\",\n        6: \"Shirt\",\n        7: \"Sneaker\",\n        8: \"Bag\",\n        9: \"Ankle Boot\",\n    }\n\n    figure = plt.figure(figsize=(8, 8))\n    cols, rows = 3, 3\n    for i in range(1, cols * rows + 1):\n        sample_idx = torch.randint(len(training_data), size=(1,)).item()\n        img, label = training_data[sample_idx]\n        figure.add_subplot(rows, cols, i)\n        plt.title(labels_map[label])\n        plt.axis(\"off\")\n        plt.imsave(args.output_data_dir+'plot'+str(i)+'.jpg', img.squeeze(), cmap=\"gray\")\n\n    train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n    test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n\n    # Display image and label.\n    train_features, train_labels = next(iter(train_dataloader))\n    print(f\"Feature batch shape: {train_features.size()}\")\n    print(f\"Labels batch shape: {train_labels.size()}\")\n    img = train_features[0].squeeze()\n    label = train_labels[0]\n    plt.imsave(args.output_data_dir+'sample.jpg', img, cmap=\"gray\")\n    print(\"Saved img.\")\n    print(f\"Label: {label}\")\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using {device} device\")\n\n    model = NeuralNetwork().to(device)\n    print(model)\n\n    learning_rate = 1e-3\n    batch_size = 64\n    epochs = 5\n    # ... train `model`, then save it to `model_dir`\n    \n\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n    epochs = 1\n    for t in range(epochs):\n        print(f\"Epoch {t+1}\\n-------------------------------\")\n        train_loop(train_dataloader, model, loss_fn, optimizer)\n        test_loop(test_dataloader, model, loss_fn)\n    print(\"Done!\")\n\n    \n\n\n\n    with open(os.path.join(args.model_dir, 'model.pth'), 'wb') as f:\n        torch.save(model.state_dict(), f)\n        plt.plot([1,2,3,4])\n        plt.ylabel('some numbers')\n        plt.show()\n        plt.savefig('test.jpeg')<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>",
        "Challenge_closed_time":1661165052743,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661051216633,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to save images during training in Sagemaker, but is unable to do so using the model_dir and output_data_dir. They want to reload this additional information during inference, but storing all the information in the model.tar.gz can cause slow inference. The user has provided a code snippet using PyTorch to train a neural network and save the model.",
        "Challenge_last_edit_time":1661051486728,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73431378",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":68.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":63,
        "Challenge_solved_time":31.6211416667,
        "Challenge_title":"Save images from sagemaker training",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":43.0,
        "Challenge_word_count":478,
        "Platform":"Stack Overflow",
        "Poster_created_time":1510598465376,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Francisco, CA, United States",
        "Poster_reputation_count":1.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I suspect there is an issue with string concatenation in <code>plt.imsave<\/code> because the environment variable <code>SM_OUTPUT_DATA_DIR<\/code> by default points to <code>\/opt\/ml\/output\/data<\/code> (that's the actual value of <code>args.output_data_dir<\/code>, since you don't pass this parameter) so the outcome is something like <code>\/opt\/ml\/output\/dataplot1.jpg<\/code>. The same happen if you use the <code>model_dir<\/code> in the same way. I'd rather use something like <code>os.path.join<\/code> like you're already doing for the model. <a href=\"https:\/\/nono.ma\/sagemaker-model-dir-output-dir-and-output-data-dir-parameters\" rel=\"nofollow noreferrer\">here<\/a> a nice exaplaination about these folders and environment variables in sagemaker.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.3,
        "Solution_reading_time":9.92,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":30.8515688889,
        "Challenge_answer_count":1,
        "Challenge_body":"I want to train and build the model in Sagemaker studio and then be able to export the model as a container image to ECR, so I can use the model in external platform by sharing the ECR image to another account where I Can create container with the image  from ECR",
        "Challenge_closed_time":1663369533112,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663258467464,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to export a trained model from Sagemaker studio as a container image to ECR, so that it can be used in an external platform by sharing the ECR image to another account.",
        "Challenge_last_edit_time":1667926271895,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUZHWz5-hpSc-80dEIkuxwQw\/how-to-export-tresained-models-to-ecr-as-container-image",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.0,
        "Challenge_reading_time":3.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":30.8515688889,
        "Challenge_title":"How to export tresained models to ECR as container image",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":72.0,
        "Challenge_word_count":61,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The models you train in SageMaker are stored in S3 as .tar.gz files that you can use to deploy to an endpoint, or even test locally (extracting the model file from the tar file). \nIf you are using a built-in algorithm, you can share the .tar.gz file to the second account and deploy the model in the second account, since built-in algorithm containers can be accessed from any AWS account. \n\nIf you are using a custom training image ([docs here](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/adapt-training-container.html)), you can push this image to ECR and allow a [second account to pull the image](https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/secondary-account-access-ecr\/) and then use the image with the model that you have trained. However, note that Studio at this time does not support building Docker images out of the box. You can use [SageMaker Notebook Instances](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi.html) instead.\n\nI would recommend keeping the model (.tar.gz) and the image (Docker) separate, since you can easily retrain and deploy the newer versions of models without updating the image every single time.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1663369533112,
        "Solution_link_count":3.0,
        "Solution_readability":10.2,
        "Solution_reading_time":14.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":163.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":0.1789469444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In my setup, I run a script that <strong>trains<\/strong> a model and starts generating checkpoints. Another script watches for new checkpoints and <strong>evaluates<\/strong> them. The scripts run in parallel, so evaluation is just a step behind training.<\/p>\n\n<p>What's the right Tracks configuration to support this scenario?<\/p>",
        "Challenge_closed_time":1591906575912,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591905931703,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for the appropriate Trains configuration to track separate train\/test processes where a script trains a model and generates checkpoints while another script evaluates them in parallel.",
        "Challenge_last_edit_time":1609771005756,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62332672",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.4,
        "Challenge_reading_time":4.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.1789469444,
        "Challenge_title":"Tracking separate train\/test processes with Trains",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":99.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_created_time":1311330349880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tel Aviv",
        "Poster_reputation_count":3784.0,
        "Poster_view_count":342.0,
        "Solution_body":"<p>disclaimer: I'm part of the <a href=\"https:\/\/github.com\/allegroai\/trains\/\" rel=\"nofollow noreferrer\">allegro.ai Trains<\/a> team<\/p>\n<p>Do you have two experiments? one for testing one for training ?<\/p>\n<p>If you do have two experiments, then I would make sure the models are logged in both of them (which if they are stored on the same shared-folder\/s3\/etc will be automatic)\nThen you can quickly see the performance of each-one.<\/p>\n<p>Another option is sharing the same experiment, then the second process adds reports to the original experiment, that means that somehow you have to pass to it the experiment id.\nThen you can do:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>task = Task.get_task(task_id='training_task_id`)\ntask.get_logger().report_scalar('title', 'loss', value=0.4, iteration=1)\n<\/code><\/pre>\n<p>EDIT:\nAre the two processes always launched together, or is the checkpoint test a general purpose code ?<\/p>\n<p>EDIT2:<\/p>\n<p>Let's assume you have main script training a model. This experiment has a unique task ID:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>my_uid = Task.current_task().id\n<\/code><\/pre>\n<p>Let's also assume you have a way to pass it to your second process (If this is an actual sub-process, it inherits the os environment variables so you could do <code>os.environ['MY_TASK_ID']=my_uid<\/code>)<\/p>\n<p>Then in the evaluation script you could report directly into the main training Task like so:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>train_task = Task.get_task(task_id=os.environ['MY_TASK_ID'])\ntrain_task.get_logger().report_scalar('title', 'loss', value=0.4, iteration=1)\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1592833417200,
        "Solution_link_count":1.0,
        "Solution_readability":10.9,
        "Solution_reading_time":21.3,
        "Solution_score_count":1.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":202.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":76.3483333333,
        "Challenge_answer_count":0,
        "Challenge_body":"### System Info\r\n\r\n- `transformers` version: 4.21.0.dev0\r\n- Platform: Linux-5.8.0-43-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- Huggingface_hub version: 0.7.0\r\n- PyTorch version (GPU?): 1.11.0+cu113 (True)\r\n- Tensorflow version (GPU?): 2.9.1 (False)\r\n- Flax version (CPU?\/GPU?\/TPU?): 0.5.0 (cpu)\r\n- Jax version: 0.3.6\r\n- JaxLib version: 0.3.5\r\n- Using GPU in script?: <fill in>\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n\r\n\r\n### Who can help?\r\n\r\n@sgugger \r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n1.enable sigopt HPO in example and run.\r\n2. work log like\"UserWarning: You're currently using the old SigOpt Experience. Try out the new and improved SigOpt experience by getting started with the docs today. You have until July 2022 to migrate over without experiencing breaking changes.\"\r\n\r\n### Expected behavior\r\n\r\nHPO with sigopt backend could work correctly without warning",
        "Challenge_closed_time":1658150380000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657875526000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the ModelUploadOp method from \"Vertex AI Pipelines: model upload using google-cloud-pipeline-components\" while attempting to upload a custom model to Vertex AI models. The logs show that the job is succeeding, but the model never actually gets uploaded. The user has provided code examples that show the method that doesn't work and the one that does work. The user is using Vertex AI Pipelines with Pipeline SDK (Kubeflow Pipelines\/TFX) Version: kfp and Pipelines Version: kfp==1.8.11 on the Google Cloud Vertex AI platform.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/18145",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":6.1,
        "Challenge_reading_time":14.47,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17206.0,
        "Challenge_repo_issue_count":20680.0,
        "Challenge_repo_star_count":76088.0,
        "Challenge_repo_watch_count":860.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":76.3483333333,
        "Challenge_title":"the Sigopt api is outdated in transformers trainer.py, the old api could not work",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":153,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"SigOpt"
    },
    {
        "Answerer_created_time":1221810788500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paderborn, North-Rhine-Westphalia, Germany",
        "Answerer_reputation_count":68522.0,
        "Answerer_view_count":7896.0,
        "Challenge_adjusted_solved_time":380.8509777778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>It is not clear to me if one could use mlflow to serve a model that is evolving continuously based on its previous predictions.<\/p>\n<p>I need to be able to query a model in order to make a prediction on a sample of data which is the basic use of mlflow serve. However I also want the model to be updated internaly now that it has seen new data.<\/p>\n<p>Is it possible or does it need a FR ?<\/p>",
        "Challenge_closed_time":1618124640630,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616753577110,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unsure if mlflow can be used to continuously update a model based on its previous predictions while also being able to query the model for predictions on new data. They are seeking clarification on whether this is possible or if it requires a feature request.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66814885",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":5.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":380.8509777778,
        "Challenge_title":"Serve online learning models with mlflow",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":360.0,
        "Challenge_word_count":84,
        "Platform":"Stack Overflow",
        "Poster_created_time":1515156959092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Fairbanks, AK, United States",
        "Poster_reputation_count":76.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>I think that you should be able to do that by implementing the custom python model or custom flavor, as it's described in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">documentation<\/a>.  In this case you need to create a class that is inherited from <code>mlflow.pyfunc.PythonModel<\/code>, and implement the <code>predict<\/code> method, and inside that method you're free to do anything.  Here is just simple example from documentation:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class AddN(mlflow.pyfunc.PythonModel):\n\n    def __init__(self, n):\n        self.n = n\n\n    def predict(self, context, model_input):\n        return model_input.apply(lambda column: column + self.n)\n<\/code><\/pre>\n<p>and this model is then could be saved &amp; loaded again just as normal models:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># Construct and save the model\nmodel_path = &quot;add_n_model&quot;\nadd5_model = AddN(n=5)\nmlflow.pyfunc.save_model(path=model_path, python_model=add5_model)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(model_path)\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.9,
        "Solution_reading_time":14.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":120.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.340015,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi! I wanted to see if VBA and Azure Machine Learning Excel Add In can be connected to each other. Are there any way to code VBA (use VBA) for controlling or altering Azure Machine Learning Excel Add In? I have used Azure Machine Learning to rate candidate feedback as negative or positive, but it has like a 75 -80% success rate - there are still a good chunk of comments that are rated wrong. However, it is still an amazing tool that I want to use v- I was just wondering if I can increase the accuracy of it somehow by creating a VBA code that connects it to Azure Machine Learning where I can add words related to negative responses or vice versa for positive response to increase the accuracy. <\/p>",
        "Challenge_closed_time":1610161769647,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610149745593,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking to connect VBA with Azure Machine Learning Excel Add In to improve the accuracy of candidate feedback ratings. They have used Azure Machine Learning with a 75-80% success rate but want to increase accuracy by adding words related to negative or positive responses through VBA coding.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/224491\/vba-and-azure-machine-learning-excel-add-in",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":8.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":3.340015,
        "Challenge_title":"VBA  And Azure Machine Learning Excel Add In",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":139,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, we currently don't support VBA and Azure ML Excel add-in integration. You'll need to apply <a href=\"https:\/\/stackoverflow.com\/questions\/41447104\/how-to-improve-classification-accuracy-for-machine-learning\">ML techniques for improving your model<\/a> and re-deploy your model.  <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.2,
        "Solution_reading_time":3.8,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":28.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1571417311507,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Redmond, WA, USA",
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":99.8513777778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When trying to make predictions for forecasting models using Azure ML Service, the swagger.json includes the following schema for input:<\/p>\n\n<pre><code>\"example\": {\"data\": [{\"date\": \"2019-08-30T00:00:00.000Z\", \"y_query\": 1.0}]}\n<\/code><\/pre>\n\n<p>However, when I feed this as an input to generate predictions, I receive the following error:<\/p>\n\n<pre><code>data= {\"data\": [{\"date\": \"2019-08-30T00:00:00.000Z\", \"y_query\": 1 }]}\n# Convert to JSON string\ninput_data = json.dumps(data)\n\n# Set the content type\nheaders = {'Content-Type': 'application\/json'}\n# If authentication is enabled, set the authorization header\n#headers['Authorization'] = f'Bearer {key}'\n\n# Make the request and display the response\nresp = requests.post(scoring_uri, input_data, headers=headers)\nprint(resp.text)\n\n<\/code><\/pre>\n\n<pre><code>\"{\\\"error\\\": \\\"DataException:\\\\n\\\\tMessage: y values are present for each date. Nothing to forecast.\\\\n\\\\tInnerException None\\\\n\\\\tErrorResponse \\\\n{\\\\n    \\\\\\\"error\\\\\\\": {\\\\n        \\\\\\\"code\\\\\\\": \\\\\\\"UserError\\\\\\\",\\\\n        \\\\\\\"inner_error\\\\\\\": {\\\\n            \\\\\\\"code\\\\\\\": \\\\\\\"InvalidData\\\\\\\"\\\\n        },\\\\n        \\\\\\\"message\\\\\\\": \\\\\\\"y values are present for each date. Nothing to forecast.\\\\\\\"\\\\n    }\\\\n}\\\"}\"\n<\/code><\/pre>\n\n<p>I have tried not passing a y value, which causes an 'expected two axis got one' and passing 0 as the y_query. Any guidance on how to make predictions using this approach would be greatly appreciated. <\/p>\n\n<p>The documentation for web services is here: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-consume-web-service\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-consume-web-service<\/a><\/p>",
        "Challenge_closed_time":1571417536483,
        "Challenge_comment_count":6,
        "Challenge_created_time":1571058071523,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while making predictions for forecasting models using Azure ML Service. The swagger.json example json for the forecast model is not returning predictions and is resulting in an error message stating that y values are present for each date and nothing can be forecasted. The user has tried different approaches but has not been successful. The user is seeking guidance on how to make predictions using this approach.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58377444",
        "Challenge_link_count":2,
        "Challenge_participation_count":8,
        "Challenge_readability":12.4,
        "Challenge_reading_time":22.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":99.8513777778,
        "Challenge_title":"swagger.json example json for forecast model doesn't seem to return predictions",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":530.0,
        "Challenge_word_count":180,
        "Platform":"Stack Overflow",
        "Poster_created_time":1464949169832,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":25.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>Try using nan as the value for y_query. and make sure the date is the next time unit after the one that was used in the training set.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.0,
        "Solution_reading_time":1.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":28.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.6086344444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hey experts, I am looking for a document of how features in SDK V1 mapping to SDK v2 to show my team and plan how we should move to SDK v2. I cannot find a summary for that. Can you please help with this <\/p>",
        "Challenge_closed_time":1682725999300,
        "Challenge_comment_count":0,
        "Challenge_created_time":1682716608216,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is looking for a document that maps the features of SDK V1 to SDK V2 to help their team plan the transition to the newer version, but they are unable to find a summary for it. They are seeking assistance in locating such a document.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1266094\/sdk-features-mapping",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.4,
        "Challenge_reading_time":2.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":2.6086344444,
        "Challenge_title":"SDK features mapping",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":47,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"https:\/\/learn.microsoft.com\/users\/na\/?userid=9603a4b0-3119-4f80-93b6-9637337c7a94\">@otto atler<\/a> <\/p>\n<p>Thanks for reaching out to us again, please see below list: <\/p>\n<p>For workspace - <\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.workspace.workspace\">Method\/API in SDK v1 (use links to ref docs)<\/a>    <\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.entities.workspace\">Method\/API in SDK v2 (use links to ref docs)<\/a><\/p>\n<p>For compute - <\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.compute.amlcompute(class)\">Method\/API in SDK v1 (use links to ref docs)<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.entities.amlcompute\">Method\/API in SDK v2 (use links to ref docs)<\/a><\/p>\n<p>For datastore -<\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_storage_datastore.azureblobdatastore?view=azure-ml-py&amp;preserve-view=true\">azureml_blob_datastore<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.entities.azuredatalakegen1datastore\">azureml_blob_datastore<\/a><\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_data_lake_datastore.azuredatalakedatastore?view=azure-ml-py&amp;preserve-view=true\">azureml_data_lake_gen1_datastore<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.entities.azuredatalakegen1datastore\">azureml_data_lake_gen1_datastore<\/a><\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_data_lake_datastore.azuredatalakegen2datastore?view=azure-ml-py&amp;preserve-view=true\">azureml_data_lake_gen2_datastore<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.entities.azuredatalakegen2datastore\">azureml_data_lake_gen2_datastore<\/a><\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_sql_database_datastore.azuresqldatabasedatastore?view=azure-ml-py&amp;preserve-view=true\">azuremlml_sql_database_datastore<\/a><\/p>\n<p>V2 Will be supported via import &amp; export functionalities|<\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_my_sql_datastore.azuremysqldatastore?view=azure-ml-py&amp;preserve-view=true\">azuremlml_my_sql_datastore<\/a><\/p>\n<p>V2 Will be supported via import &amp; export functionalities|<\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_postgre_sql_datastore.azurepostgresqldatastore?view=azure-ml-py&amp;preserve-view=true\">azuremlml_postgre_sql_datastore<\/a><\/p>\n<p>V2 Will be supported via import &amp; export functionalities|<\/p>\n<p>For data assets - <\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data\">Method\/API in SDK v1<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.entities\">Method\/API in SDK v2<\/a><\/p>\n<p>For model assets - <\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model(class)#azureml-core-model-register\">Model.register<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.mlclient#azure-ai-ml-mlclient-create-or-update\">ml_client.models.create_or_update<\/a><\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run#azureml-core-run-run-register-model\">run.register_model<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.mlclient#azure-ai-ml-mlclient-create-or-update\">ml_client.models.create_or_update<\/a><\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model(class)#azureml-core-model-deploy\">Model.deploy<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.mlclient#azure-ai-ml-mlclient-begin-create-or-update\">ml_client.begin_create_or_update(blue_deployment)<\/a><\/p>\n<p>I hope this helps, please let me know if you have any questions.<\/p>\n<p>Regards,<\/p>\n<p>Yutong<\/p>\n<p>-Please kindly accept the answer and vote 'Yes' if you feel helpful to support he community, thanks a lot.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":22.0,
        "Solution_readability":30.1,
        "Solution_reading_time":61.37,
        "Solution_score_count":0.0,
        "Solution_sentence_count":24.0,
        "Solution_word_count":197.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":17.5492611111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I attempting to deploy the universal-sentence-encoder model to a aws Sagemaker endpoint and am getting the error <code>raise ValueError('no SavedModel bundles found!')<\/code><\/p>\n\n<p>I have shown my code below, I have a feeling that one of my paths is incorrect<\/p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\nfrom sagemaker import get_execution_role\nfrom sagemaker.tensorflow.serving import Model\n\ndef tfhub_to_savedmodel(model_name,uri):\n    tfhub_uri = uri\n    model_path = 'encoder_model\/' + model_name\n\n    with tf.Session(graph=tf.Graph()) as sess:\n        module = hub.Module(tfhub_uri) \n        input_params = module.get_input_info_dict()\n        dtype = input_params['text'].dtype\n        shape = input_params['text'].get_shape()\n\n        # define the model inputs\n        inputs = {'text': tf.placeholder(dtype, shape, 'text')}\n\n        # define the model outputs\n        # we want the class ids and probabilities for the top 3 classes\n        logits = module(inputs['text'])\n        outputs = {\n            'vector': logits,\n        }\n\n        # export the model\n        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n        tf.saved_model.simple_save(\n            sess,\n            model_path,\n            inputs=inputs,\n            outputs=outputs)  \n\n    return model_path\n\n\nsagemaker_role = get_execution_role()\n\n!tar -C \"$PWD\" -czf encoder.tar.gz encoder_model\/\nmodel_data = Session().upload_data(path='encoder.tar.gz',key_prefix='model')\n\nenv = {'SAGEMAKER_TFS_DEFAULT_MODEL_NAME': 'universal-sentence-encoder-large'}\n\nmodel = Model(model_data=model_data, role=sagemaker_role, framework_version=1.12, env=env)\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.t2.medium')\n<\/code><\/pre>",
        "Challenge_closed_time":1563978858283,
        "Challenge_comment_count":0,
        "Challenge_created_time":1563915680943,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error \"no SavedModel bundles found!\" while attempting to deploy the universal-sentence-encoder model to an AWS SageMaker endpoint. The user suspects that one of the paths in the code might be incorrect. The code involves converting a TensorFlow Hub model to a SavedModel and deploying it to SageMaker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57172147",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.4,
        "Challenge_reading_time":21.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":17.5492611111,
        "Challenge_title":"'no SavedModel bundles found!' on tensorflow_hub model deployment to AWS SageMaker",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2621.0,
        "Challenge_word_count":152,
        "Platform":"Stack Overflow",
        "Poster_created_time":1531840489147,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berkeley, CA, USA",
        "Poster_reputation_count":425.0,
        "Poster_view_count":92.0,
        "Solution_body":"<p>I suppose you started from this example? <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_serving_container\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_serving_container<\/a><\/p>\n\n<p>It looks like you're not saving the TF Serving bundle properly: the model version number is missing, because of this line:<\/p>\n\n<pre><code>model_path = 'encoder_model\/' + model_name\n<\/code><\/pre>\n\n<p>Replacing it with this should fix your problem:<\/p>\n\n<pre><code>model_path = '{}\/{}\/00000001'.format('encoder_model\/', model_name)\n<\/code><\/pre>\n\n<p>Your model artefact should look like this (I used the model in the notebook above):<\/p>\n\n<pre><code>mobilenet\/\nmobilenet\/mobilenet_v2_140_224\/\nmobilenet\/mobilenet_v2_140_224\/00000001\/\nmobilenet\/mobilenet_v2_140_224\/00000001\/saved_model.pb\nmobilenet\/mobilenet_v2_140_224\/00000001\/variables\/\nmobilenet\/mobilenet_v2_140_224\/00000001\/variables\/variables.data-00000-of-00001\nmobilenet\/mobilenet_v2_140_224\/00000001\/variables\/variables.index\n<\/code><\/pre>\n\n<p>Then, upload to S3 and deploy.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":23.4,
        "Solution_reading_time":15.64,
        "Solution_score_count":6.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":76.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1523264005616,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Yokohama, Kanagawa, Japan",
        "Answerer_reputation_count":1635.0,
        "Answerer_view_count":143.0,
        "Challenge_adjusted_solved_time":1.0014597222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm pretty new to SageMaker, so I'm sorry if I miss something obvious.<\/p>\n\n<p>I've trained a DL model which uses frames from a video to make a prediction. The current script, that runs in the SageMaker jupyter-notebook, takes a video URL as an input and uses an FFMPEG subprocess pipe to extract the frames and predict them afterwards. This works fine, but now I want to start that script from Lambda.<\/p>\n\n<p>As far as I understood, I could deploy my model with sagemaker and make predictions for every single frame from Lambda, unfortunately this is not an option, as ffprobe, ffmpeg and numpy are too large to fit into the limited lambda space.<\/p>\n\n<p>tl;dr: Is it possible to run my custom script (ffmpeg frame extraction + tensorflow model prediction) as an endpoint in SageMaker?<\/p>",
        "Challenge_closed_time":1580863000328,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580859395073,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a deep learning model that uses frames from a video to make predictions. They want to run their custom script (ffmpeg frame extraction + tensorflow model prediction) as an endpoint in SageMaker, but are unable to do so due to the limited space in Lambda.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60067075",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":10.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.0014597222,
        "Challenge_title":"SageMaker deploy custom script",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":386.0,
        "Challenge_word_count":136,
        "Platform":"Stack Overflow",
        "Poster_created_time":1460444230316,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":404.0,
        "Poster_view_count":127.0,
        "Solution_body":"<p>Sagemaker allows you to use a custom Docker image (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms.html\" rel=\"nofollow noreferrer\">AWS document<\/a>)<\/p>\n\n<blockquote>\n  <p>Build your own custom container image: If there is no pre-built Amazon\n  SageMaker container image that you can use or modify for an advanced\n  scenario, you can package your own script or algorithm to use with\n  Amazon SageMaker.You can use any programming language or framework to\n  develop your container<\/p>\n<\/blockquote>\n\n<ul>\n<li>Create a docker image with your code (FFmpeg, TensorFlow)<\/li>\n<li>Testing the docker container locally<\/li>\n<li>Deploying the image on Amazon ECR (Elastic Container Repository)<\/li>\n<li>Create a SageMaker model and point to the image<\/li>\n<\/ul>\n\n<p>For details, you can learn more from <a href=\"https:\/\/towardsdatascience.com\/brewing-up-custom-ml-models-on-aws-sagemaker-e09b64627722\" rel=\"nofollow noreferrer\">this tutorial<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.9,
        "Solution_reading_time":12.43,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":111.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1623779095963,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany, Hesse",
        "Answerer_reputation_count":1341.0,
        "Answerer_view_count":128.0,
        "Challenge_adjusted_solved_time":4.1247302778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to understand what an RFormula is in MLflow or spark.<\/p>\n<p>I have found these:<\/p>\n<p><a href=\"https:\/\/george-jen.gitbook.io\/data-science-and-apache-spark\/rformula\" rel=\"nofollow noreferrer\">https:\/\/george-jen.gitbook.io\/data-science-and-apache-spark\/rformula<\/a>\n<a href=\"https:\/\/spark.apache.org\/docs\/latest\/api\/python\/reference\/api\/pyspark.ml.feature.RFormula.html\" rel=\"nofollow noreferrer\">https:\/\/spark.apache.org\/docs\/latest\/api\/python\/reference\/api\/pyspark.ml.feature.RFormula.html<\/a><\/p>\n<p>but still cannot understand how to interpret an RFormula fully. I am not sure how to interpret the below table\n<a href=\"https:\/\/i.stack.imgur.com\/guLyU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/guLyU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>based on the formula &quot;y ~ x+ s&quot;, y is related to x and s, but in the table when y=0 and x=0 and s =a (i.e. third row), then the features is [0,1] and label is 0, so how shall I interpret this.<\/p>\n<p>I have found <a href=\"https:\/\/stackoverflow.com\/questions\/61290042\/spark-rformula-interpretation\">this<\/a> but still cannot understand my way through this problem.<\/p>",
        "Challenge_closed_time":1627334164712,
        "Challenge_comment_count":2,
        "Challenge_created_time":1627319315683,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to understand what an RFormula is in MLflow or Spark and has found some resources but is still struggling to interpret it fully. They are specifically confused about how to interpret a table based on the formula \"y ~ x+ s\" and are seeking help to understand it.",
        "Challenge_last_edit_time":1627371880352,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68533916",
        "Challenge_link_count":7,
        "Challenge_participation_count":3,
        "Challenge_readability":14.0,
        "Challenge_reading_time":16.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":4.1247302778,
        "Challenge_title":"what is features and how to interpret in RFormula",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":78.0,
        "Challenge_word_count":120,
        "Platform":"Stack Overflow",
        "Poster_created_time":1375186444008,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Auckland, New Zealand",
        "Poster_reputation_count":912.0,
        "Poster_view_count":288.0,
        "Solution_body":"<p>So your label is y. You parse x and s in rformula.<\/p>\n<p>x stays the same:<\/p>\n<pre><code>+-----------+---+\n|      x    | x |\n+-----------+---+\n|     1.0   |1.0|\n|     2.0   |2.0|\n|     0.0   |0.0|\n+-----------+---+\n<\/code><\/pre>\n<p>s:<\/p>\n<pre><code>+-----------+---+\n|       s   | s |\n+-----------+---+\n|       a   |1.0|\n|       b   |0.0|\n|       a   |1.0|\n+-----------+---+\n<\/code><\/pre>\n<p>I hope I could answer you question.\nRformula just converts the strings, standarize them and parse them into a vector.<\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1627334932660,
        "Solution_link_count":0.0,
        "Solution_readability":2.5,
        "Solution_reading_time":5.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":57.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1432655047272,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":463.0,
        "Answerer_view_count":76.0,
        "Challenge_adjusted_solved_time":264.5982422222,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>What is a is the curl command to make a POST request to sage-maker and receive a ML inference?<\/p>",
        "Challenge_closed_time":1514326523572,
        "Challenge_comment_count":1,
        "Challenge_created_time":1513373969900,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to use the curl command to make a POST request to Amazon Sagemaker and receive a machine learning inference.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47840209",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":8.2,
        "Challenge_reading_time":1.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":11.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":264.5982422222,
        "Challenge_title":"How to Curl an Amazon Sagemaker Endpoint",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":6889.0,
        "Challenge_word_count":25,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443201378360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":749.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>Rather than using curl, it's recommended that you use the SageMaker Runtime client to send data and get back inferences from a SageMaker Endpoint:<\/p>\n\n<p><a href=\"http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_runtime_InvokeEndpoint.html\" rel=\"nofollow noreferrer\">http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_runtime_InvokeEndpoint.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.7,
        "Solution_reading_time":4.86,
        "Solution_score_count":4.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1393524211332,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":745.0,
        "Answerer_view_count":71.0,
        "Challenge_adjusted_solved_time":609.6229966667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Have exhausted myself on this one so any help would be appreciated.<\/p>\n\n<p>I am trying to set up hosting my tensorflow model with Amazon Sagemaker and following the example found <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators\/iris_dnn_classifier.py\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>This example uses hard coded feature columns with known dimensionality. <\/p>\n\n<pre><code>feature_columns = [tf.feature_column.numeric_column(INPUT_TENSOR_NAME, shape=[4])]\n<\/code><\/pre>\n\n<p>I need to avoid this as my dataset changes often.<\/p>\n\n<h1>Local Machine Set Up<\/h1>\n\n<p>Now on my local machine, I define a list of columns <\/p>\n\n<pre><code>my_feature_columns = []\n<\/code><\/pre>\n\n<p>With the following strategy<\/p>\n\n<pre><code>#Define placeholder nodes based on datatype being inserted\n\nfor key in train_x.keys():\n<\/code><\/pre>\n\n<p>Where train_x is the dataset without labels.<\/p>\n\n<p>'OBJECTS' become hashed buckets as there are many possible categories<\/p>\n\n<pre><code>    if train_x[key].dtypes == 'object':\n\n        categorical_column = tf.feature_column.categorical_column_with_hash_bucket(\n                key = key,\n                hash_bucket_size = len(train_x[key].unique()))\n\n        my_feature_columns.append(tf.feature_column.embedding_column(\n                categorical_column=categorical_column,\n                dimension=5))\n<\/code><\/pre>\n\n<p>'INT64' become categorical columns as there are only two possible categories (I have recoded booleans to 0\/1)<\/p>\n\n<pre><code>    elif train_x[key].dtypes == 'int64':\n\n        categorical_column = tf.feature_column.categorical_column_with_identity(\n                key=key,\n                num_buckets=2)\n\n        my_feature_columns.append(tf.feature_column.indicator_column(categorical_column))\n<\/code><\/pre>\n\n<p>'FLOATS' become continuous columns<\/p>\n\n<pre><code>    elif train_x[key].dtypes == 'float':\n        my_feature_columns.append(\n        tf.feature_column.numeric_column(\n        key=key))\n<\/code><\/pre>\n\n<p>On the local machine this yields a nice list of all of my features that can be given as an argument when instantiating a tf.estimator.DNNClassifier. As more categories are added to each OBJECT column, this is handled by<\/p>\n\n<pre><code>hash_bucket_size = len(train_x[key].unique())\n<\/code><\/pre>\n\n<h1>Sagemaker<\/h1>\n\n<p>From the <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst#preparing-the-tensorflow-training-script\" rel=\"nofollow noreferrer\">Docs<\/a><\/p>\n\n<p><em>Preparing the TensorFlow training script\nYour TensorFlow training script must be a Python 2.7 source file. The SageMaker TensorFlow docker image uses this script by calling specifically-named functions from this script.<\/em><\/p>\n\n<p><em>The training script must contain the following:<\/em><\/p>\n\n<p><em>Exactly one of the following:\nmodel_fn: defines the model that will be trained.\nkeras_model_fn: defines the tf.keras model that will be trained.\nestimator_fn: defines the tf.estimator.Estimator that will train the model.<\/em><\/p>\n\n<p><em>train_input_fn: preprocess and load training data.<\/em><\/p>\n\n<p><em>eval_input_fn: preprocess and load evaluation data.<\/em><\/p>\n\n<p>Again, from the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators\/iris_dnn_classifier.py\" rel=\"nofollow noreferrer\">example<\/a><\/p>\n\n<pre><code>def train_input_fn(training_dir, params):\n\"\"\"Returns input function that would feed the model during training\"\"\"\nreturn _generate_input_fn(training_dir, 'iris_training.csv')\n<\/code><\/pre>\n\n<p>This function is called by the sagemaker docker image, which adds its own argument for <strong>training_dir<\/strong>, it is not a global parameter.<\/p>\n\n<p>When trying to access my training data from the estimator_fn to build a my_feature_columns list<\/p>\n\n<pre><code>NameError: global name 'training_dir' is not defined\n<\/code><\/pre>\n\n<h1>I would love to be able to do something like this.<\/h1>\n\n<pre><code>def estimator_fn(run_config, params):\n\nmy_feature_columns = []\n\ntrain_x , _ , _ , _ = datasplitter(os.path.join(training_dir, 'leads_test_frame.csv'))\n\nfor key in train_x.keys():\n    if train_x[key].dtypes == 'object':\n\n        categorical_column = tf.feature_column.categorical_column_with_hash_bucket(\n                key = key,\n                hash_bucket_size = len(train_x[key].unique()))\n\n        my_feature_columns.append(tf.feature_column.embedding_column(\n                categorical_column=categorical_column,\n                dimension=5))\n\n    elif train_x[key].dtypes == 'int64':\n\n        categorical_column = tf.feature_column.categorical_column_with_identity(\n                key=key,\n                num_buckets=2)\n\n        my_feature_columns.append(tf.feature_column.indicator_column(categorical_column))\n\n    elif train_x[key].dtypes == 'float':\n        my_feature_columns.append(\n        tf.feature_column.numeric_column(\n        key=key))\n\nreturn tf.estimator.DNNClassifier(feature_columns=my_feature_columns,\n                                  hidden_units=[10, 20, 10],\n                                  n_classes=2,\n                                  config=run_config)\n<\/code><\/pre>\n\n<p>Thanks to anyone who can help in any way. Will happily give more info if needed but feel like 4 pages is probably enough :-S<\/p>\n\n<p>Cheers!\nClem<\/p>",
        "Challenge_closed_time":1537837258768,
        "Challenge_comment_count":0,
        "Challenge_created_time":1535642615980,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to set up hosting their TensorFlow model with Amazon Sagemaker, but is facing challenges with generating feature columns. They are using a strategy on their local machine to define a list of columns, but are unable to access their training data from the estimator_fn to build a my_feature_columns list. They would like to be able to define the my_feature_columns list in the estimator_fn function.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52100549",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":16.5,
        "Challenge_reading_time":67.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":47,
        "Challenge_solved_time":609.6229966667,
        "Challenge_title":"Procedural Generation of Feature Columns for use with Sagemaker Tensorflow Instance",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":165.0,
        "Challenge_word_count":458,
        "Platform":"Stack Overflow",
        "Poster_created_time":1491420193248,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Windsor, UK",
        "Poster_reputation_count":163.0,
        "Poster_view_count":71.0,
        "Solution_body":"<p><strong>training_dir<\/strong> points to your training channel, i.e. <em>\/opt\/ml\/input\/data\/training<\/em>. You can hardcode this location inside your <strong>estimation_fn<\/strong>.<\/p>\n\n<p>When training starts, SageMaker makes the data for the channel available in the <em>\/opt\/ml\/input\/data\/<strong>channel_name<\/em><\/strong> directory in the Docker container.<\/p>\n\n<p>You can find more information here <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo.html#your-algorithms-training-algo-running-container\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo.html#your-algorithms-training-algo-running-container<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":26.9,
        "Solution_reading_time":9.81,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":45.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1627043269590,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Richmond, VA, USA",
        "Answerer_reputation_count":67.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":6.3228980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Suppose I have two Azure ML workspaces:<\/p>\n<ol>\n<li><p>Workspace1 - This is being used by one team (Team1) who only train the model and store the model in model registry of Workspace1<\/p>\n<\/li>\n<li><p>Workspace2 - This is used by another team  (Team2) who containerise the model, push it to ACR and then deploy the containerised model in Azure ML Compute.<\/p>\n<\/li>\n<\/ol>\n<p>Is it possible for Team2 to access the model registry of Workspace1 from their Workspace2 and retrieve the model for containerisation and subsequent deployment? Alternatively, is there any concept of a shared model registry in Azure ML where both the teams can store and access a common model registry? If none of these are possible, then what is the way for Team1 and Team2 to work together on a single model with the given responsibilities as described above?<\/p>",
        "Challenge_closed_time":1638210489528,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638197253147,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge of accessing an Azure ML Model Registry from another Azure ML Workspace. They have two workspaces, one used by Team1 to train and store the model in the registry, and the other used by Team2 to containerize and deploy the model. The user is seeking a way for Team2 to access the model registry of Workspace1 from their Workspace2 or a shared model registry where both teams can store and access a common model registry.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70156610",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":11.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":3.6767725,
        "Challenge_title":"Accessing an Azure ML Model Registry from another Azure ML Workspace",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":421.0,
        "Challenge_word_count":148,
        "Platform":"Stack Overflow",
        "Poster_created_time":1373471094267,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United Kingdom",
        "Poster_reputation_count":395.0,
        "Poster_view_count":66.0,
        "Solution_body":"<p>As described, I think the best solution is to use one Workspace, not two.  It sounds like you have Team 1 and Team 2 sharing contributions on a single project.  What may work better is to define user roles in the Azure ML workspace, such that Team 2 has permissions to deploy models, and Team 1 has permission to create models.<\/p>\n<p>Otherwise you can always write Python code using the ML SDK to connect to any workspace given you know the subscription, resource group, workspace name etc.<\/p>\n<pre><code>from azure.core import Workspace, Model\n\n# connect to an existing workspace\nname = 'WorkspaceName'\nsub = 'subscriptionName'\nresource_group = 'resourceGroupName'\nws = Workspace.get(name=name, subscription_id=sub, resource_group=resource_group) \n\n# retrieve existing model\nmodel = Model(ws, name='your model name')\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1638220015580,
        "Solution_link_count":0.0,
        "Solution_readability":9.8,
        "Solution_reading_time":10.42,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":116.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":40.3672083333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Using API Management - can I import from a Swagger JSON file definition AND where that endpoint requires the following injection to a header so that it can call\/retrieve that Swagger JSON?<\/p>\n<p>The API in question is actually an Azure Machine Learning Online Endpoint - and that contains a Swagger JSON definition. In order to see the definition you have to inject the following header records into the HTTP call...<\/p>\n<pre><code>Authorization: Bearer longRandomtokengeneratedbyAML12345\nazureml-model-deployment: nameofmyamlmodeldendpointdeployment\n<\/code><\/pre>\n<p>Using API Management - can I somehow import this Swagger JSON ? I can't find a way to reach the endpoint and specify the x2 header records it requires<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1675215836676,
        "Challenge_comment_count":1,
        "Challenge_created_time":1675070514726,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a challenge in importing a Swagger JSON file definition into API Management, which requires HTTP header records to call\/retrieve the Swagger JSON. The API in question is an Azure Machine Learning Online Endpoint, and the user needs to inject two header records into the HTTP call to see the definition. The user is unable to find a way to reach the endpoint and specify the required header records.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1165334\/api-management-can-you-import-from-swagger-json-th",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.9,
        "Challenge_reading_time":10.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":40.3672083333,
        "Challenge_title":"API Management - Can you import from Swagger JSON that requires HTTP Header records?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":114,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@<a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=0ec06fb6-513e-4f5c-9aff-281bc5e44e22\">Neil McAlister<\/a> Thanks for posting it in Microsoft Q&amp;A. Based on the statement above, it looks like you are importing Swagger JSON to APIM via portal, CLI or PowerShell with specification URL, correct?<\/p>\n<p>If so, unfortunately that's not possible right now. There is no way to specify headers with <code>--specification-url<\/code> in <a href=\"https:\/\/learn.microsoft.com\/en-us\/cli\/azure\/apim\/api?view=azure-cli-latest#az-apim-api-import\">az apim api import<\/a> (or <a href=\"https:\/\/learn.microsoft.com\/en-us\/powershell\/module\/az.apimanagement\/import-azapimanagementapi\">Import-AzApiManagementApi<\/a> - PowerShell) and you would need to download swagger JSON in your pipeline and then use it with <code>--specification-path<\/code>(check size limitation when using this parameter <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/api-management\/api-management-api-import-restrictions#openapi-specifications\">here<\/a>) or place it in another location where APIM can directly access it. <\/p>\n<p>If you are interested in this feature and like to submit feedback to our product team, please submit it via <a href=\"https:\/\/aka.ms\/apimwish\">https:\/\/aka.ms\/apimwish<\/a> and would help our product team to prioritize the features. Also, others with similar interests can upvote it too.<\/p>\n<p>Feel free to reach out if you have any other questions.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":14.5,
        "Solution_reading_time":19.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":151.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1374162232292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":523.0,
        "Answerer_view_count":48.0,
        "Challenge_adjusted_solved_time":40.9724888889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I tried to create my own experiment using the module, but failed to make it work.\nhere is the exception i got:  <\/p>\n\n<blockquote>\n  <p>Error 0018: Training dataset of user-item-rating triples contains invalid data.\n  [Critical]     {\"InputParameters\":{\"DataTable\":[{\"Rows\":14,\"Columns\":3,\"estimatedSize\":12668928,\"ColumnTypes\":{\"System.String\":1,\"System.Int32\":1,\"System.Double\":1},\"IsComplete\":true,\"Statistics\":{\"0\":[10,0],\"1\":[5422.0,5999.0,873.0,6616.0,1758.0582820478173,7.0,0.0],\"2\":[1.0,1.0,1.0,1.0,0.0,1.0,0.0]}},{\"Rows\":2338,\"Columns\":3,\"estimatedSize\":1404928,\"ColumnTypes\":{\"System.String\":1,\"System.Int32\":1,\"System.Double\":1},\"IsComplete\":true,\"Statistics\":{\"0\":[2338,0],\"1\":[7.5367835757057318,3.0,0.0,704.0,17.738259318519511,64.0,0.0],\"2\":[3.3737234816082085,1.5,0.0,352.0,8.3956874404883841,122.0,0.0]}},{\"Rows\":2532,\"Columns\":22,\"estimatedSize\":4648960,\"ColumnTypes\":{\"System.Int32\":10,\"System.String\":5,\"System.Double\":6,\"System.Boolean\":1},\"IsComplete\":true,\"Statistics\":{\"0\":[4575.7263033175359,5326.5,539.0,6871.0,1987.9561375024909,2532.0,0.0],\"1\":[4575.7263033175359,5326.5,539.0,6871.0,1987.9561375024909,2532.0,0.0],\"2\":[613.0,613.0,613.0,613.0,0.0,1.0,0.0],\"3\":[0,2532],\"4\":[0,2532],\"5\":[4575.7263033175359,5326.5,539.0,6871.0,1987.9561375024909,2532.0,0.0],\"6\":[23.647231437598673,19.99,1.99,149.99,17.237723488320938,90.0,0.0],\"7\":[0.043827014218009476,0.0,0.0,45.99,1.3460680431173562,3.0,0.0],\"8\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"9\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"10\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"11\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"12\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"13\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"14\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"15\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"16\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"17\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"18\":[2524,0],\"19\":[242,18],\"20\":[1,0],\"21\":[2524,0]}}],\"Generic\":{\"traitCount\":10,\"iterationCount\":5,\"batchCount\":4}},\"OutputParameters\":[],\"ModuleType\":\"Microsoft.Analytics.Modules.MatchboxRecommender.Dll\",\"ModuleVersion\":\" Version=6.0.0.0\",\"AdditionalModuleInfo\":\"Microsoft.Analytics.Modules.MatchboxRecommender.Dll, Version=6.0.0.0, Culture=neutral, PublicKeyToken=69c3241e6f0468ca;Microsoft.Analytics.Modules.MatchboxRecommender.Dll.MatchboxRecommender;Train\",\"Errors\":\"Microsoft.Analytics.Exceptions.ErrorMapping+ModuleException: Error 0018: Training dataset of user-item-rating triples contains invalid data.\\r\\n   at Microsoft.Analytics.Modules.MatchboxRecommender.Dll.Utilities.UpdateRatingMetadata(DataTable dataset, String datasetName) in d:\\_Bld\\8833\\7669\\Sources\\Product\\Source\\Modules\\MatchboxRecommender.Dll\\Utilities.cs:line 179\\r\\n   at Microsoft.Analytics.Modules.MatchboxRecommender.Dll.MatchboxRecommender.TrainImpl(DataTable userItemRatingTriples, DataTable userFeatures, DataTable itemFeatures, Int32 traitCount, Int32 iterationCount, Int32 batchCount) in d:\\_Bld\\8833\\7669\\Sources\\Product\\Source\\Modules\\MatchboxRecommender.Dll\\MatchboxRecommender.cs:line 62\",\"Warnings\":[],\"Duration\":\"00:00:00.6722068\"}\n  Module finished after a runtime of 00:00:01.1250071 with exit code -2\n  Module failed due to negative exit code of -2<\/p>\n<\/blockquote>\n\n<p>i've check the input data i'm setting as input user-place-rating table, record by record (no worries it's only 14 records) here it is: <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/LjyD6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LjyD6.png\" alt=\"the input data\"><\/a><\/p>\n\n<p>Here is a screenshot of the experiment:\n<a href=\"https:\/\/i.stack.imgur.com\/I43tG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/I43tG.png\" alt=\"the experiment\"><\/a><\/p>\n\n<p>since the error message is not very informative, I don't know where to start, so, if anybody has an idea, I would be happy to hear about it.<\/p>\n\n<p>Update:\nA friend of mine suggested to add \"Edit Metadata\" module to change the \"rating\" feature into \"int\" or \"float\" types, and the two other(placeID and userID) into string features. that didn't help as well.<\/p>",
        "Challenge_closed_time":1464083199416,
        "Challenge_comment_count":0,
        "Challenge_created_time":1463926726100,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error while trying to create their own experiment using the \"Train Matchbox Recommender\" module in AzureML. The error message received was not very informative, and the user checked the input data they set as input user-place-rating table, record by record. The user also added an \"Edit Metadata\" module to change the \"rating\" feature into \"int\" or \"float\" types, and the two other (placeID and userID) into string features, but it did not help.",
        "Challenge_last_edit_time":1463935698456,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37375506",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":22.4,
        "Challenge_reading_time":56.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":43.46481,
        "Challenge_title":"AzureML: \"Train Matchbox Recommender\" is not working and does not descibe the error",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1187.0,
        "Challenge_word_count":216,
        "Platform":"Stack Overflow",
        "Poster_created_time":1320061998252,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":778.0,
        "Poster_view_count":89.0,
        "Solution_body":"<p>The matchbox recommender requires that ratings be numerical or categorical. Also when training, your ratings cannot all be the same.<\/p>\n\n<p>You need to use a metadata editor <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn905986.aspx\" rel=\"nofollow\">https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn905986.aspx<\/a> to convert the ratings into numerical features and you need to make sure you are using a range of ratings.<\/p>\n\n<p>Then this should work!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.0,
        "Solution_reading_time":6.1,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.1856155556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I created a notebook in the workspace and when I sent the experiment for training I received error message <strong>undefined symbol: XGDMatrixSetDenseInfo<\/strong> for algorithm <strong>Xgboost<\/strong>. Do you know how to fix the problem?<\/p>\n<p><strong>Azure ML Version:<\/strong> 1.22.0  <br \/>\n<strong>Compute Instance:<\/strong> Standard_DS3_v2<\/p>\n<ul>\n<li> Code:  import logging  <br \/>\n  from azureml.train.automl import AutoMLConfig  <br \/>\n  from azureml.core.experiment import Experiment  automl_settings = {  <br \/>\n  &quot;iteration_timeout_minutes&quot;: 10,  <br \/>\n  &quot;experiment_timeout_hours&quot;: 0.3,  <br \/>\n  &quot;enable_early_stopping&quot;: True,  <br \/>\n  &quot;primary_metric&quot;: 'normalized_root_mean_squared_error',  <br \/>\n  &quot;featurization&quot;: 'auto',  <br \/>\n  &quot;verbosity&quot;: logging.INFO,  <br \/>\n  &quot;n_cross_validations&quot;: 5  <br \/>\n  }  automl_config = AutoMLConfig(task='regression',  <br \/>\n  debug_log='automated_ml_errors.log',  <br \/>\n  training_data=x_train,  <br \/>\n  label_column_name=&quot;production_time&quot;,  <br \/>\n  **automl_settings)  experiment = Experiment(ws, &quot;train-model&quot;)  <br \/>\n  local_run = experiment.submit(automl_config, show_output=True)<\/li>\n<li> Full Error Message:  ERROR: FitException:  <br \/>\n  Message: \/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo  <br \/>\n  InnerException: AttributeError: \/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo  <br \/>\n  ErrorResponse  <br \/>\n  {  <br \/>\n  &quot;error&quot;: {  <br \/>\n  &quot;code&quot;: &quot;SystemError&quot;,  <br \/>\n  &quot;message&quot;: &quot;Encountered an internal AutoML error. Error Message\/Code: FitException. Additional Info: FitException:\\n\\tMessage: \/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo\\n\\tInnerException: None\\n\\tErrorResponse \\n{\\n \\&quot;error\\&quot;: {\\n \\&quot;message\\&quot;: \\&quot;\/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo\\&quot;,\\n \\&quot;target\\&quot;: \\&quot;Xgboost\\&quot;,\\n \\&quot;reference_code\\&quot;: \\&quot;Xgboost\\&quot;\\n }\\n}&quot;,  <br \/>\n  &quot;details_uri&quot;: &quot;https:\/\/learn.microsoft.com\/azure\/machine-learning\/resource-known-issues#automated-machine-learning&quot;,  <br \/>\n  &quot;target&quot;: &quot;Xgboost&quot;,  <br \/>\n  &quot;inner_error&quot;: {  <br \/>\n  &quot;code&quot;: &quot;ClientError&quot;,  <br \/>\n  &quot;inner_error&quot;: {  <br \/>\n  &quot;code&quot;: &quot;AutoMLInternal&quot;  <br \/>\n  }  <br \/>\n  },  <br \/>\n  &quot;reference_code&quot;: &quot;Xgboost&quot;  <br \/>\n  }  <br \/>\n  }<\/li>\n<\/ul>\n<p>Best regards,  <br \/>\nCristina<\/p>\n<p><a href=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/79658-packages.txt?platform=QnA\">79658-packages.txt<\/a><\/p>",
        "Challenge_closed_time":1616196599996,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616170731780,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while running an experiment in Azure Machine Learning Services AutoML. The error message stated \"undefined symbol: XGDMatrixSetDenseInfo\" for the Xgboost algorithm. The user is seeking help to fix the problem. The Azure ML version used was 1.22.0 and the compute instance was Standard_DS3_v2. The error message and code used in the experiment are provided in the post.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/322886\/azure-machine-learning-services-automl-error-runni",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":23.9,
        "Challenge_reading_time":39.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":7.1856155556,
        "Challenge_title":"Azure Machine Learning Services - AutoMl - Error running experiment.submit: \"\/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo\"",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":197,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, can you try uninstalling and reinstalling Xgboost (try versions &lt;= 0.90 if you continue to get errors).<\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":1.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.8427261111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>According the following doc, I should be able to to turn on FeaturizationConfig in the settings:    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train<\/a>    <\/p>\n<p>However I'm getting the following error when I try to change the switch to 'FeaturizationConfig' when setting up the AutoML experiment:    <\/p>\n<blockquote>\n<p>ConfigException: ConfigException: Message: Invalid argument(s) 'featurizationconfig' specified. Supported value(s): 'off, auto'    <\/p>\n<\/blockquote>\n<p>The following is my settings:    <\/p>\n<blockquote>\n<p>import logging    <\/p>\n<p>automl_settings = {    <br \/>\n    &quot;iteration_timeout_minutes&quot;: 15,    <br \/>\n    &quot;experiment_timeout_hours&quot;: 0.3,    <br \/>\n    &quot;enable_early_stopping&quot;: True,    <br \/>\n    &quot;primary_metric&quot;: 'spearman_correlation',    <br \/>\n    &quot;featurization&quot;: 'FeaturizationConfig',    <br \/>\n    &quot;verbosity&quot;: logging.INFO,    <br \/>\n    &quot;n_cross_validations&quot;: 5    <br \/>\n}    <\/p>\n<\/blockquote>",
        "Challenge_closed_time":1638189983847,
        "Challenge_comment_count":1,
        "Challenge_created_time":1638154550033,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to turn on FeaturizationConfig in the settings of Azure AutoML experiment. The error message indicates that the argument 'featurizationconfig' is invalid and only 'off' and 'auto' are supported values. The user has shared their settings which include the featurization parameter set to 'FeaturizationConfig'.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/643670\/azure-automl-featurisation-error",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":20.0,
        "Challenge_reading_time":14.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":9.8427261111,
        "Challenge_title":"Azure AutoML Featurisation Error",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":94,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=6760e7be-77ee-4c41-9c63-8bbcdab1aaae\">@SoonJoo@Genting  <\/a> Thanks, Previously, it was a black-box preprocessing, with user\u2019s preprocess=True\/False setting.    <br \/>\nNew change includes deprecation of <code>preprocess<\/code> and introduction of new field <code>featurization<\/code>, where featurization = \u2018auto\u2019 (for automatic featurization, comparable to preprocess=True) \/ \u2018off\u2019 (to turn off featurization, comparable to preprocess=False) \/ FeaturizationConfig (object to pass in customized configuration on featurization setting).    <\/p>\n<p>For more information on custom featurization as well as how to construct FeaturizationConfig is in this <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-features#customize-featurization\">documentation<\/a>.    <br \/>\nWe also have a notebook available with example in our git <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/regression-explanation-featurization\/auto-ml-regression-explanation-featurization.ipynb\">repo<\/a>.    <br \/>\nUsage example:    <\/p>\n<pre><code>from azureml.automl.core.featurization import FeaturizationConfig  \n  \nfeaturization_config = FeaturizationConfig()  \nfeaturization_config.add_column_purpose('Column2', 'Categorical')  \nfeaturization_config.add_column_purpose('Column5', 'Categorical')  \n  \nautoml_config = AutoMLConfig(task = 'classification', compute_target=compute_target, featurization=featurization_config, **automl_settings )  \nremote_run = experiment.submit(automl_config, show_output = False)  \n<\/code><\/pre>\n<p>For classification &amp; regression you do have the option to turn off automatic featurization.     <\/p>\n<p>featurization    <br \/>\nstr or FeaturizationConfig    <br \/>\n'auto' \/ 'off' \/ FeaturizationConfig Indicator for whether featurization step should be done automatically or not, or whether customized featurization should be used.    <br \/>\n\u2026    <br \/>\nNote: Timeseries features are handled separately when the task type is set to forecasting independent of this parameter.    <\/p>\n<p>\u2022\t<a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-train-automl-client\/azureml.train.automl.automlconfig.automlconfig?view=azure-ml-py\">AutoMLConfig Class<\/a>    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":24.0,
        "Solution_reading_time":30.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":180.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1540534565632,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":161.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":0.4110722223,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using AWS Sagemaker for model training and deployment, this is sample example for model training <\/p>\n\n<pre><code>from sagemaker.estimator import Estimator\nhyperparameters = {'train-steps': 10}\ninstance_type = 'ml.m4.xlarge'\n\nestimator = Estimator(role=role,\n                      train_instance_count=1,\n                      train_instance_type=instance_type,\n                      image_name=ecr_image,\n                      hyperparameters=hyperparameters)\n\nestimator.fit(data_location)\n<\/code><\/pre>\n\n<p>The docker image mentioned here is a tensorflow system. <\/p>\n\n<p>Suppose it will take 1000 seconds to train the model, now I will increase the instance count to 5 then the training time will increase 5 times i.e. 5000 seconds. As per my understanding the training job will be distributed to 5 machines so ideally it will take 200 seconds per machine but seems its doing separate training on each machine. Can someone please let me know its working over distributed system in general or with Tensorflow.<\/p>\n\n<p>I tried to find out the answer on this documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-dg.pdf\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-dg.pdf<\/a> but seems the way of working on distributed machines is not mentioned here.<\/p>",
        "Challenge_closed_time":1546585143583,
        "Challenge_comment_count":0,
        "Challenge_created_time":1546584216850,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing challenges with AWS Sagemaker for model training and deployment. They are using a tensorflow system and have increased the instance count to 5, but the training time has increased 5 times instead of being distributed among the machines. The user is seeking clarification on whether this is how distributed systems work in general or with Tensorflow. They have tried to find answers in the documentation but have not found any relevant information.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54034172",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.7,
        "Challenge_reading_time":17.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.2574258334,
        "Challenge_title":"AWS Sagemaker | Why multiple instances training taking time multiplied to instance number",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1386.0,
        "Challenge_word_count":160,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501403168107,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":1370.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>Are you using <a href=\"https:\/\/www.tensorflow.org\/guide\/estimators\" rel=\"nofollow noreferrer\">TensorFlow estimator APIs<\/a> in your script? If yes, I think you should run the script by wrapping it in <code>sagemaker.tensorflow.TensorFlow<\/code> class as described <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst#training-with-tensorflow-estimator\" rel=\"nofollow noreferrer\">in the documentation here<\/a>. If you run training that way, parallelization and communication between instances should work out-of-the-box.<\/p>\n\n<p>But note that scaling will not be linear when you increase the number of instances. Communicating between instances takes time and there could be non-parallelizable bottlenecks in your script like loading data to memory.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1546585696710,
        "Solution_link_count":2.0,
        "Solution_readability":14.5,
        "Solution_reading_time":10.56,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":84.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1978.4166666667,
        "Challenge_answer_count":3,
        "Challenge_body":"Hi, I just got started using vertex ai with google cloud console. I am trying to deploy this mode to an endpoint. https:\/\/tfhub.dev\/tensorflow\/efficientnet\/lite0\/feature-vector\/2 I successfully imported it into a google storage bucket and uploaded it to the model registry. However, when I attempt to deploy the model to an endpoint, I receive the following error.\n\nHello Vertex AI Customer,\n\nDue to an error, Vertex AI was unable to create endpoint \"Feature Vectors\".\nAdditional Details:\nOperation State: Failed with errors\nResource Name: \n**path to project**\nError Messages: Model server terminated: model server container terminated: \nexit_code:       255\nreason: \"Error\"\nstarted_at {\n   seconds: 1669817118\n}\nfinished_at {\n   seconds: 1669817421\n}\n. Model server logs can be found at \n**some link**\n\nI have attempted to change the TensorFlow version and the folder that i import (I attempted to import the containing folder instead of the model folder) however nothing seems to help. Any suggestions would be greatly appreciated. Thank you!",
        "Challenge_closed_time":1676994480000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1669872180000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an error while trying to deploy a model to an endpoint using Vertex AI with Google Cloud Console. The error message states that the model server container terminated with an exit code of 255 and provides a link to the model server logs. The user has tried changing the TensorFlow version and the folder that they import, but the issue persists. They are seeking suggestions to resolve the problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Model-Deployment-Error\/m-p\/495020#M885",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":11.2,
        "Challenge_reading_time":13.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":1978.4166666667,
        "Challenge_title":"Vertex AI Model Deployment Error",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":283.0,
        "Challenge_word_count":155,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Yep, the solution was pretty simple.\u00a0\n\nThe trick was to import the model as a tensorflow GraphDef and then export the model with the serve tags included. You can then use this exported model instead of the untagged model. Hope this helps!\u00a0\n\n\n\n# import tensorflow as tf\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nfrom tensorflow.python.platform import gfile\n\nmodel_file = \".\/efficientnet_lite0_feature-vector_2\/saved_model.pb\"\n\nwith tf.Session() as sess:\n\n    graph = tf.Graph()\n    graph_def = tf.GraphDef()\n    with open(model_file, \"rb\") as f:\n        graph_def.ParseFromString(f.read())\n\n    tf.import_graph_def(graph_def)\n\n# Export the model to \/tmp\/my-model.meta.\nmeta_graph_def = tf.serve.export_meta_graph(filename='.\/efficientnet_lite0_feature-vector_2\/info.meta')\n\n\u00a0\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.4,
        "Solution_reading_time":10.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":84.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1638953828923,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":607.0,
        "Answerer_view_count":770.0,
        "Challenge_adjusted_solved_time":41.5038786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a serialized XGBClassifier object, trained and generated using xgboost=1.5.2.<\/p>\n<pre><code>XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n              colsample_bynode=1, colsample_bytree=0.30140958911801474,\n              eval_metric='logloss', gamma=0.1203484640861413, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.1, max_bin=368, max_delta_step=0, max_depth=6,\n              min_child_weight=1, missing=nan, monotone_constraints='()',\n              n_estimators=100, n_jobs=6, num_parallel_tree=1, random_state=42,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n              single_precision_histogram=True, subsample=0.976171515775659,\n              tree_method='gpu_hist', use_label_encoder=False,\n              validate_parameters=1, verbosity=None)\n<\/code><\/pre>\n<p>I load the object using:<\/p>\n<pre><code>clf_model = joblib.load(model_path)\n<\/code><\/pre>\n<p>I want to use the object to predict on some data I am using Azure environment which also has xgboost=1.5.2. but it gives error:<\/p>\n<pre><code>File &quot;score.py&quot;, line 78, in score_execution\n[stderr]    clf_preds = clf_model.predict(clf_data_transformed)\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 1284, in predict\n[stderr]    class_probs = super().predict(\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 879, in predict\n[stderr]    if self._can_use_inplace_predict():\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 811, in _can_use_inplace_predict\n[stderr]    predictor = self.get_params().get(&quot;predictor&quot;, None)\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 505, in get_params\n[stderr]    params.update(cp.__class__.get_params(cp, deep))\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 502, in get_params\n[stderr]    params = super().get_params(deep)\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/sklearn\/base.py&quot;, line 210, in get_params\n[stderr]    value = getattr(self, key)\n[stderr]AttributeError: 'XGBModel' object has no attribute 'enable_categorical'\n<\/code><\/pre>\n<p>We have same version in pipelines that produce\/serialize the model and in the pipeline that deserialize the model to predict on new data.<\/p>",
        "Challenge_closed_time":1645427212643,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645277798680,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has encountered an error while trying to use a serialized XGBClassifier object, trained and generated using xgboost=1.5.2, to predict on some data in an Azure environment. The error message states that the 'XGBModel' object has no attribute 'enable_categorical'. The user has confirmed that the same version of xgboost is being used in both the pipelines that produce\/serialize the model and in the pipeline that deserializes the model to predict on new data.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71185505",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.9,
        "Challenge_reading_time":32.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":41.5038786111,
        "Challenge_title":"XGBClassifer, when de-serialized, gives 'XGBModel' object has no attribute 'enable_categorical'",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":358.0,
        "Challenge_word_count":185,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443017464707,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sweden",
        "Poster_reputation_count":644.0,
        "Poster_view_count":126.0,
        "Solution_body":"<p>Here are some possible solutions :<\/p>\n<ul>\n<li>Save the model in some other way, e.g. the JSON specified here <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html\" rel=\"nofollow noreferrer\">https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html<\/a><\/li>\n<li>Limit the allowed range of xgboost versions to those that are known to work with our model. This could lead to issues in the future, for example if the aging version of xgboost we require is no longer supported by newer versions of Python.<\/li>\n<li>Using <code>save_model<\/code> to save in JSON is worth a shot to try.<\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.8,
        "Solution_reading_time":8.09,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":80.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.0104277778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'd like to use GPT-3 for my application.  I understand MS has licensed GPT-3 from OpenAI, and that there is pricing too.  So how do I get to use GPT-3?  <\/p>\n<p>Chris Powell<\/p>",
        "Challenge_closed_time":1605182707940,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605179070400,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information on how to access GPT-3 for their application, including details on Microsoft's licensing and pricing.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/160489\/gpt-3-access",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.2,
        "Challenge_reading_time":2.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.0104277778,
        "Challenge_title":"GPT-3 access",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":34,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=912f72cd-88e9-416f-bbe1-5a7356385053\">@Crispy  <\/a> Thanks for the question, Innovations from our GPT-3 workstreams will be incorporated in later versions of Azure. In the meantime, If you are interested in participation in the OpenAI GPT-3 and Azure Service partnership please fill out this <a href=\"https:\/\/forms.office.com\/Pages\/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRyj5DlT4gqZKgEsfbkRQK5xUQVlSVlJITkxDQkRaOVdESjJGN0dONkQzNy4u\">form<\/a> to submit a request.    <\/p>\n<p>Ignite blog announcement: <a href=\"https:\/\/blogs.microsoft.com\/ai-for-business\/ai-at-scale-ignite\/\">https:\/\/blogs.microsoft.com\/ai-for-business\/ai-at-scale-ignite\/<\/a>    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.0,
        "Solution_reading_time":9.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":3520.4706275,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have pretrained model artifacts stored in S3 buckets. I want to create a service that loads this model and uses it for inference.<\/p>\n\n<p>I am working in AWS ecosystem and confused between using ECS vs Sagemaker for model deployment?\nWhat are some pros\/cons for choosing one over other?<\/p>",
        "Challenge_closed_time":1578553162123,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578049726853,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has pre-trained model artifacts stored in S3 buckets and wants to create a service that loads this model and uses it for inference. They are confused between using AWS ECS and Sagemaker for model deployment and are seeking advice on the pros and cons of each option.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59577521",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":4.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":139.8431305556,
        "Challenge_title":"AWS Sagemaker vs ECS for model hosting",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3288.0,
        "Challenge_word_count":55,
        "Platform":"Stack Overflow",
        "Poster_created_time":1390885171883,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"College Station, TX, United States",
        "Poster_reputation_count":426.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>SageMaker has a higher price mark but it is taking a lot of the heavy lifting of deploying a machine learning model, such as wiring the pieces (load balancer, gunicorn, CloudWatch, Auto-Scaling...) and it is easier to automate the processes such as A\/B testing.<\/p>\n\n<p>If you have a strong team of DevOps that have nothing more important to do, you can build a flow that will be cheaper than the SageMaker option. ECS and EKS are doing at the same time a lot of work to make it very easy for you to automate the machine learning model deployments. However, they will always be more general purpose and SageMaker with its focus on machine learning will be easier for these use cases. <\/p>\n\n<p>The usual pattern of using the cloud is to use the managed services early on as you want to move fast and you don't really know where are your future problems. Once the system is growing and you start feeling some pains here and there, you can decide to spend the time and improve that part of the system. Therefore, if you don't know the pros\/cons, start with using the simpler options. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1590723421112,
        "Solution_link_count":0.0,
        "Solution_readability":10.5,
        "Solution_reading_time":13.09,
        "Solution_score_count":7.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":196.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1467237684900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":613.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":3.7905394444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to do linear regression with aws sagemaker. Where i have trained my model with some values and it's predicting values as per inputs. but sometimes it predicts value out of range as in i am predicting percentage which can't go less than 0 and more than 100. how can i restrict it here:<\/p>\n\n<pre><code>sess = sagemaker.Session()\n\nlinear = \nsagemaker.estimator.Estimator(containers[boto3.Session().region_name],\nrole, \ntrain_instance_count=1, \ntrain_instance_type='ml.c4.xlarge',\noutput_path='s3:\/\/{}\/{}\/output'.format(bucket, prefix),\n sagemaker_session=sess)\nlinear.set_hyperparameters(feature_dim=5,\nmini_batch_size=100,\npredictor_type='regressor',\nepochs=10,\nnum_models=32,\nloss='absolute_loss')\n\nlinear.fit({'train': s3_train_data, 'validation': s3_validation_data})\n<\/code><\/pre>\n\n<p>how can i make my model not to predict values out of range : [0,100].<\/p>",
        "Challenge_closed_time":1522946824052,
        "Challenge_comment_count":3,
        "Challenge_created_time":1522933178110,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to perform linear regression with AWS SageMaker, but sometimes the predicted values are out of range. The user wants to restrict the predicted values within the range of 0 to 100. The user has provided the code used for training the model and is seeking advice on how to modify it to achieve the desired outcome.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49673042",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":13.0,
        "Challenge_reading_time":11.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":3.7905394444,
        "Challenge_title":"How to restrict model predicted value within range?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":817.0,
        "Challenge_word_count":97,
        "Platform":"Stack Overflow",
        "Poster_created_time":1462951158703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":4928.0,
        "Poster_view_count":304.0,
        "Solution_body":"<p>Yes you can. You can implement the output_fn to \"brick wall\" your output. SageMaker would call the output_fn after the model returns the value to do any post-processing of the result. \nThis can be done by creating a separate python file, specify the output_fn method there. \nProvide this python file when instantiating your Estimator. \nsomething like <\/p>\n\n<pre><code>sess = sagemaker.Session()\n\nlinear = \nsagemaker.estimator.Estimator(containers[boto3.Session().region_name],\nrole, \ntrain_instance_count=1, \ntrain_instance_type='ml.c4.xlarge',\noutput_path='s3:\/\/{}\/{}\/output'.format(bucket, prefix),\n sagemaker_session=sess)\nlinear.set_hyperparameters(feature_dim=5,\nmini_batch_size=100,\npredictor_type='regressor',\nepochs=10,\nnum_models=32,\nloss='absolute_loss', \n<\/code><\/pre>\n\n<blockquote>\n  <p>entry_point = 'entry.py'<\/p>\n<\/blockquote>\n\n<p>)<\/p>\n\n<pre><code>linear.fit({'train': s3_train_data, 'validation': s3_validation_data})\n<\/code><\/pre>\n\n<p>Your entry.py could look something like <\/p>\n\n<pre><code>def output_fn(data, accepts):\n    \"\"\"\n    Args:\n        data: A result from TensorFlow Serving\n        accepts: The Amazon SageMaker InvokeEndpoint Accept value. The content type the response object should be\n            serialized to.\n    Returns:\n        object: The serialized object that will be send to back to the client.\n\n    \"\"\"    \n<\/code><\/pre>\n\n<blockquote>\n  <p>Implement the logic to \"brick wall\" here.<\/p>\n<\/blockquote>\n\n<pre><code>    return data.outputs['outputs'].string_val\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.7,
        "Solution_reading_time":18.91,
        "Solution_score_count":2.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":146.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1480786532470,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Nagpur, Sitabuldi, Nagpur, Maharashtra, India",
        "Answerer_reputation_count":111.0,
        "Answerer_view_count":44.0,
        "Challenge_adjusted_solved_time":2013.1323233333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>This is the first time I am using amazon web services to deploy my machine learning pre-trained model. I want to deploy my pre-trained TensorFlow model to Aws-Sagemaker. I am somehow able to deploy the endpoints successfully But whenever I call the <code>predictor.predict(some_data)<\/code> method to make prediction to invoking the endpoints it's throwing an error.<\/p>\n\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"\". See https:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/sagemaker-tensorflow-2020-04-07-04-25-27-055 in account 453101909370 for more information.\n<\/code><\/pre>\n\n<p>After going through the cloud watch logs I found this error.<\/p>\n\n<pre><code>#011details = \"NodeDef mentions attr 'explicit_paddings' not in Op&lt;name=Conv2D; signature=input:T, filter:T -&gt; output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]; attr=dilations:list(int),default=[1, 1, 1, 1]&gt;; NodeDef: {{node conv1_conv\/convolution}} = Conv2D[T=DT_FLOAT, _output_shapes=[[?,112,112,64]], data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"\/job:localhost\/replica:0\/task:0\/device:CPU:0\"](conv1_pad\/Pad, conv1_conv\/kernel\/read). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\n<\/code><\/pre>\n\n<p>I don't know where I am wrong and I have wasted 2 days already to solve this error and couldn't find out the information regarding this. The detailed logs I have shared <a href=\"https:\/\/docs.google.com\/document\/d\/1NXsLRd6cfbNE55xSVq5d63ETt-cBmwZsOaic8IyF1Qw\/edit?usp=sharing\" rel=\"nofollow noreferrer\">here<\/a>. <\/p>\n\n<p>Tensorflow version of my notebook instance is 1.15<\/p>",
        "Challenge_closed_time":1593503184160,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586244384423,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to deploy a pre-trained TensorFlow model on AWS Sagemaker. The endpoints are deployed successfully, but when the user tries to make predictions by calling the predictor.predict() method, it throws a ModelError with a server error message. The user has checked the cloud watch logs and found an error related to the Conv2D operation. The user is unsure about the cause of the error and has been unable to find a solution after spending two days on it. The TensorFlow version used by the user's notebook instance is 1.15.",
        "Challenge_last_edit_time":1586255907796,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61074798",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":15.9,
        "Challenge_reading_time":29.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":2016.3332602778,
        "Challenge_title":"Deploy pre-trained tensorflow model on the aws sagemaker - ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":740.0,
        "Challenge_word_count":214,
        "Platform":"Stack Overflow",
        "Poster_created_time":1480786532470,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Nagpur, Sitabuldi, Nagpur, Maharashtra, India",
        "Poster_reputation_count":111.0,
        "Poster_view_count":44.0,
        "Solution_body":"<p>After a lot of searching and try &amp; error, I was able to solve this problem. In many cases, the problem arises because of the TensorFlow and Python versions.<\/p>\n<p><strong>Cause of the problem:<\/strong>\nTo deploy the endpoints, I was using the <code>TensorflowModel<\/code> on TF 1.12 and python 3 and which exactly caused the problem.<\/p>\n<blockquote>\n<pre><code>sagemaker_model = TensorFlowModel(model_data = model_data,\n                                  role = role,\n                                  framework_version = '1.12',\n                                  entry_point = 'train.py')\n<\/code><\/pre>\n<\/blockquote>\n<p>Apparently, <code>TensorFlowModel<\/code> only allows python 2 on TF version 1.11, 1.12. 2.1.0.<\/p>\n<p><strong>How I fixed it:<\/strong> There are two TensorFlow solutions that handle serving in the Python SDK. They have different class representations and documentation as shown here.<\/p>\n<ol>\n<li><strong>TensorFlowModel<\/strong> - <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/model.py#L47\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/model.py#L47<\/a><\/li>\n<\/ol>\n<ul>\n<li>Doc:\n<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/tree\/v1.12.0\/src\/sagemaker\/tensorflow#deploying-directly-from-model-artifacts\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/tree\/v1.12.0\/src\/sagemaker\/tensorflow#deploying-directly-from-model-artifacts<\/a><\/li>\n<li>Key difference: Uses a proxy GRPC client to send requests<\/li>\n<li>Container impl:\n<a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/blob\/master\/src\/tf_container\/serve.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/blob\/master\/src\/tf_container\/serve.py<\/a><\/li>\n<\/ul>\n<ol start=\"2\">\n<li><strong>Model<\/strong> - <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/serving.py#L96\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/serving.py#L96<\/a><\/li>\n<\/ol>\n<ul>\n<li>Doc: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst<\/a><\/li>\n<li>Key difference: Utilizes the TensorFlow serving rest API<\/li>\n<li>Container impl: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/blob\/master\/container\/sagemaker\/serve.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/blob\/master\/container\/sagemaker\/serve.py<\/a><\/li>\n<\/ul>\n<p>Python 3 isn't supported using the <code>TensorFlowModel<\/code> object, as the container uses the TensorFlow serving API library in conjunction with the GRPC client to handle making inferences, however, the TensorFlow serving API isn't supported in Python 3 officially, so there are only Python 2 versions of the containers when using the <code>TensorFlowModel<\/code> object.\nIf you need Python 3 then you will need to use the <code>Model<\/code> object defined in #2 above.<\/p>\n<p>Finally, I used the <code>Model<\/code> with the TensorFlow version 1.15.1.<\/p>\n<blockquote>\n<pre><code>sagemaker_model = Model(model_data = model_data,\n                        role = role,\n                        framework_version='1.15.2',\n                        entry_point = 'train.py')\n<\/code><\/pre>\n<\/blockquote>\n<p>Also, here are the successful results.\n<a href=\"https:\/\/i.stack.imgur.com\/OMsEf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/OMsEf.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":14.0,
        "Solution_readability":21.1,
        "Solution_reading_time":48.42,
        "Solution_score_count":4.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":271.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1508924024027,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":118.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":504.0913175,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have registered a scikit learn model on my MLflow Tracking server, and I am loading it with <code>sklearn.load_model(model_uri)<\/code>.<\/p>\n<p>Now, I would like to access the signature of the model so I can get a list of the model's required inputs\/features so I can retrieve them from my feature store by name. I can't seem to find any utility or method in the <code>mlflow<\/code> API or the <code>MLFlowClient<\/code> API that will let me access a signature or inputs\/outputs attribute, even though I can see a list of inputs and outputs under each version of the model in the UI.<\/p>\n<p>I know that I can find the input sample and the model configuration in the model's artifacts, but that would require me actually downloading the artifacts and loading them manually in my script. I don't need to avoid that, but I am surprised that I can't just return the signature as a dictionary the same way I can return a run's parameters or metrics.<\/p>",
        "Challenge_closed_time":1645469817663,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643655088920,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has registered a scikit learn model on their MLflow Tracking server and is trying to access the model signature to retrieve a list of required inputs\/features. However, they are unable to find any utility or method in the mlflow API or the MLFlowClient API that will let them access the signature or inputs\/outputs attribute. They can see a list of inputs and outputs under each version of the model in the UI, but they don't want to download the artifacts and load them manually in their script.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70931309",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":12.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":504.0913175,
        "Challenge_title":"How to retrieve the model signature from the MLflow Model Registry",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":904.0,
        "Challenge_word_count":173,
        "Platform":"Stack Overflow",
        "Poster_created_time":1466188731112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Michigan",
        "Poster_reputation_count":414.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>The way to access the model's signature without downloading the MLModel file is under the loaded model. And then you'll access the model's attributes, such as its signature or even other Pyfunc-defined methods.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\n\nmodel = mlflow.pyfunc.load_model(&quot;runs:\/&lt;run_id&gt;\/model&quot;)\nprint(model._model_meta._signature)\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.5,
        "Solution_reading_time":5.3,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":41.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1352206833663,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":893.0,
        "Answerer_view_count":185.0,
        "Challenge_adjusted_solved_time":28.8670294445,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I'm trying to define a Sagemaker Training Job with an existing Python class. To my understanding, I could create my own container but would rather not deal with container management.<\/p>\n\n<p>When choosing \"Algorithm Source\" there is the option of \"Your own algorithm source\" but nothing is listed under resources. Where does this come from?<\/p>\n\n<p>I know I could do this through a notebook, but I really want this defined in a job that can be invoked through an endpoint.<\/p>",
        "Challenge_closed_time":1550361262523,
        "Challenge_comment_count":0,
        "Challenge_created_time":1550257341217,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to create a Sagemaker training job with their own Tensorflow code without building a container. They are trying to define a training job with an existing Python class but do not want to deal with container management. They are looking for guidance on how to use the \"Your own algorithm source\" option under \"Algorithm Source\" as nothing is listed under resources. The user wants to define the job in a way that can be invoked through an endpoint.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54715601",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.4,
        "Challenge_reading_time":7.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":28.8670294445,
        "Challenge_title":"How do I create a Sagemaker training job with my own Tensorflow code without having to build a container?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":781.0,
        "Challenge_word_count":97,
        "Platform":"Stack Overflow",
        "Poster_created_time":1366768533200,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>As Bruno has said you will have to use a container somewhere, but you can use an existing container to run your own custom tensorflow code.<\/p>\n\n<p>There is a good example <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_quickstart\/tensorflow_script_mode_quickstart.ipynb\" rel=\"nofollow noreferrer\">in the sagemaker github<\/a> for how to do this.<\/p>\n\n<p>The way this works is you modify your code to have an entry point which takes argparse command line arguments, and then you point a 'Sagemaker Tensorflow estimator' to the entry point. Then when you call fit on the sagemaker estimator it will download the tensorflow container and run your custom code in there.<\/p>\n\n<p>So you start off with your own custom code that looks something like this<\/p>\n\n<pre><code># my_custom_code.py\nimport tensorflow as tf\nimport numpy as np\n\ndef build_net():\n    # single fully connected\n    image_place = tf.placeholder(tf.float32, [None, 28*28])\n    label_place = tf.placeholder(tf.int32, [None,])\n    net = tf.layers.dense(image_place, units=1024, activation=tf.nn.relu)\n    net = tf.layers.dense(net, units=10, activation=None)\n    return image_place, label_place, net\n\n\ndef process_data():\n    # load\n    (x_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()\n\n    # center\n    x_train = x_train \/ 255.0\n    m = x_train.mean()\n    x_train = x_train - m\n\n    # convert to right types\n    x_train = x_train.astype(np.float32)\n    y_train = y_train.astype(np.int32)\n\n    # reshape so flat\n    x_train = np.reshape(x_train, [-1, 28*28])\n    return x_train, y_train\n\n\ndef train_model(init_learn, epochs):\n    image_p, label_p, logit = build_net()\n    x_train, y_train = process_data()\n\n    loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n        logits=logit,\n        labels=label_p)\n    optimiser = tf.train.AdamOptimizer(init_learn)\n    train_step = optimiser.minimize(loss)\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for _ in range(epochs):\n            sess.run(train_step, feed_dict={image_p: x_train, label_p: y_train})\n\n\nif __name__ == '__main__':\n    train_model(0.001, 10)\n<\/code><\/pre>\n\n<p>To make it work with sagemaker we need to create a command line entry point, which will allow sagemaker to run it in the container it will download for us eventually.<\/p>\n\n<pre><code># entry.py\n\nimport argparse\nfrom my_custom_code import train_model\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\n        '--model_dir',\n        type=str)\n    parser.add_argument(\n        '--init_learn',\n        type=float)\n    parser.add_argument(\n        '--epochs',\n        type=int)\n    args = parser.parse_args()\n    train_model(args.init_learn, args.epochs)\n<\/code><\/pre>\n\n<p>Apart from specifying the arguments my function needs to take, we also need to provide a <code>model_dir<\/code> argument. This is always required, and is an S3 location which is where an model artifacts will be saved when the training job completes. Note that you don't need to specify what this value is (though you can) as Sagemaker will provide a default location in S3 for you.<\/p>\n\n<p>So we have modified our code, now we need to actually run it on Sagemaker. Go to the AWS console and fire up a small instance from Sagemaker. Download your custom code to the instance, and then create a jupyter notebook as follows:<\/p>\n\n<pre><code># sagemaker_run.ipyb\nimport sagemaker\nfrom sagemaker.tensorflow import TensorFlow\n\nhyperparameters = {\n    'epochs': 10,\n    'init_learn': 0.001}\n\nrole = sagemaker.get_execution_role()\nsource_dir = '\/path\/to\/folder\/with\/my\/code\/on\/instance'\nestimator = TensorFlow(\n    entry_point='entry.py',\n    source_dir=source_dir,\n    train_instance_type='ml.t2.medium',\n    train_instance_count=1,\n    hyperparameters=hyperparameters,\n    role=role,\n    py_version='py3',\n    framework_version='1.12.0',\n    script_mode=True)\n\nestimator.fit()\n<\/code><\/pre>\n\n<p>Running the above will:<\/p>\n\n<ul>\n<li>Spin up an ml.t2.medium instance<\/li>\n<li>Download the tensorflow 1.12.0 container to the instance<\/li>\n<li>Download any data we specify in fit to the newly created instance in fit (in this case nothing)<\/li>\n<li>Run our code on the instance<\/li>\n<li>upload the model artifacts to model_dir<\/li>\n<\/ul>\n\n<p>And that is pretty much it. There is of course a lot not mentioned here but you can:<\/p>\n\n<ul>\n<li>Download training\/testing data from s3<\/li>\n<li>Save checkpoint files, and tensorboard files during training and upload them to s3<\/li>\n<\/ul>\n\n<p>The best resource I found was the example I shared but here are all the things I was looking at to get this working:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_quickstart\/tensorflow_script_mode_quickstart.ipynb\" rel=\"nofollow noreferrer\">example code again<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"nofollow noreferrer\">documentation<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-containers#list-of-provided-environment-variables-by-sagemaker-containers\" rel=\"nofollow noreferrer\">explanation of environment variables<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":12.3,
        "Solution_reading_time":66.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":50.0,
        "Solution_word_count":546.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":18.5603936111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>It was written in the Microsoft AzureML documentation, &quot;A run represents a single trial of an experiment. Runs are used to monitor the asynchronous execution of a trial&quot; and A Run object is also created when you submit or start_logging with the Experiment class.&quot;<\/p>\n<p>Related to <code>start_logging<\/code>, as far as I know, when we have simply started the run by executing this <code>start logging<\/code> method. We have to stop, or complete by <code>complete<\/code> method when the run is completed. This is because  <code>start_logging<\/code> is a synchronized way of creating an experiment. However, Run object created from <code>start_logging<\/code> is to monitor the asynchronous execution of a trial.<\/p>\n<p>Can anyone clarify whether <code>start_logging<\/code> will start asynchronous execution or synchronous execution?<\/p>",
        "Challenge_closed_time":1660048147607,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659981330190,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on whether the \"start_logging\" method in AzureML starts asynchronous or synchronous execution, as the documentation states that the Run object created from it is used to monitor asynchronous execution, but the user believes it to be a synchronized way of creating an experiment.",
        "Challenge_last_edit_time":1660267366787,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73282180",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":11.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":18.5603936111,
        "Challenge_title":"In AzureML, start_logging will start asynchronous execution or synchronous execution?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":61.0,
        "Challenge_word_count":127,
        "Platform":"Stack Overflow",
        "Poster_created_time":1525287643000,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Ottawa, ON, Canada",
        "Poster_reputation_count":27.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p><strong>start_logging<\/strong> will be considered as <strong>asynchronous<\/strong> execution as this generates the multiple interactive run sessions. In a specific experiment, there is a chance of multiple interactive sessions, that work parallelly and there will be no scenario to be followed in sequential.<\/p>\n<p>The individual operation can be performed and recognized based on the parameters like <strong>args<\/strong>  and <strong>kwargs<\/strong>.<\/p>\n<p>When the start_logging is called, then an interactive run like <strong>jupyter notebook<\/strong> was created. The complete metrics and components which are created when the start_logging was called will be utilized. When the output directory was mentioned for each interactive run, based on the args value, the output folder will be called seamlessly.<\/p>\n<p>The following code block will help to define the operation of start_logging<\/p>\n<pre><code>experiment = Experiment(your_workspace, &quot;your_experiment_name&quot;)\n   run = experiment.start_logging(outputs=None, snapshot_directory=&quot;.&quot;, display_name=&quot;test&quot;)\n   ...\n   run.log_metric(&quot;Accuracy_Value&quot;, accuracy)\n   run.complete()\n<\/code><\/pre>\n<p>the below code block will be defining the basic syntax of start_logging<\/p>\n<pre><code>start_logging(*args, **kwargs)\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.4,
        "Solution_reading_time":17.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":147.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":992.0369444444,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\n\r\nCreate a notebook instance in one of the configured region.\r\nRan the below query and got that error\r\n\r\n```\r\nselect * from aws_sagemaker_notebook_instance;\r\nError: hydrate call listAwsSageMakerNotebookInstanceTags failed with panic interface conversion: interface {} is *sagemaker.NotebookInstanceSummary, not *sagemaker.DescribeNotebookInstanceOutput\r\n\r\n```\r\n\r\n\r\n\r\n**Steampipe version (`steampipe -v`)**\r\n: v0.4.1\r\n\r\n**Plugin version (`steampipe plugin list`)**\r\naws: v0.15.0\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1623682749000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620111416000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"the user encountered a challenge where zenml was failing to create a s3 bucket due to an incorrect regex in its name.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/turbot\/steampipe-plugin-aws\/issues\/364",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.4,
        "Challenge_reading_time":7.39,
        "Challenge_repo_contributor_count":49.0,
        "Challenge_repo_fork_count":43.0,
        "Challenge_repo_issue_count":1491.0,
        "Challenge_repo_star_count":115.0,
        "Challenge_repo_watch_count":13.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":992.0369444444,
        "Challenge_title":"Getting an error from `aws_sagemaker_notebook_instance` table. Please see the detail below.",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":60,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":6.7927280555,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm thinking of deploying a TensorFlow model using Vertex AI in GCP. I am almost sure that the cost will be directly related to the number of queries per second (QPS) because I am going to use automatic scaling. I also know that the type of machine (with GPU, TPU, etc.) will have an impact on the cost.<\/p>\n<ul>\n<li>Do you have any estimation about the cost versus the number of queries per second?<\/li>\n<li>How does the type of virtual machine changes this cost?<\/li>\n<\/ul>\n<p>The type of model is for object detection.<\/p>",
        "Challenge_closed_time":1657283230408,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657258379287,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is planning to deploy a TensorFlow model using Vertex AI in GCP and is concerned about the cost, which they believe will be directly related to the number of queries per second and the type of machine used. They are seeking an estimation of the cost based on the number of queries per second and how the cost is affected by the type of virtual machine used. The model is for object detection.",
        "Challenge_last_edit_time":1657258776587,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72907038",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":6.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":6.9030891667,
        "Challenge_title":"Cost of deploying a TensorFlow model in GCP?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":74.0,
        "Challenge_word_count":100,
        "Platform":"Stack Overflow",
        "Poster_created_time":1569457921527,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Luis Potos\u00ed, S.L.P., M\u00e9xico",
        "Poster_reputation_count":41.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>Autoscaling depends on the CPU and GPU utilization which directly correlates to the QPS, as you have said. To estimate the cost based on the QPS, you can deploy a custom prediction container to a Compute Engine instance directly, then benchmark the instance by making prediction calls until the VM hits 90+ percent CPU utilization (consider GPU utilization if configured). Do this multiple times for different machine types, and determine the &quot;QPS per cost per hour&quot; of different machine types. You can re-run these experiments while benchmarking latency to find the <strong>ideal cost per QPS per your latency targets<\/strong> for your specific custom prediction container. For more information about choosing the ideal machine for your workload, refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#finding_the_ideal_machine_type\" rel=\"nofollow noreferrer\">documentation<\/a>.<\/p>\n<p>For your second question, as per the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/pricing#custom-trained_models:%7E:text=a%20specific%20job.-,Prediction%20and%20explanation,-This%20table%20provides\" rel=\"nofollow noreferrer\">Vertex AI pricing documentation<\/a> (for model deployment), cost estimation is done based on the node hours. A node hour represents the time a virtual machine spends running your prediction job or waiting in a ready state to handle prediction or explanation requests. Each type of VM offered has a specific pricing per node hour depending on the number of cores and the amount of memory. Using a VM with more resources will cost more per node hour and vice versa. To choose an ideal VM for your deployment, please follow the steps given in the first paragraph which will help you find a good trade off between cost and performance.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.4,
        "Solution_reading_time":22.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":243.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1513074641863,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Antarctica",
        "Answerer_reputation_count":155.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":14.8578630556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>My question is somehow related to <a href=\"https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/217305\/data-input-format-call-the-service-for-azure-ml-ti.html\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/217305\/data-input-format-call-the-service-for-azure-ml-ti.html<\/a> - however, the provided solution does not seem to work.<\/p>\n<p>I am constructing a simple model with heart-disease dataset but I wrap it into Pipeline as I use some featurization steps (scaling, encoding etc.) The full script below:<\/p>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport pickle\n\n# data input\ndf = pd.read_csv('heart.csv')\n\n# numerical variables\nnum_cols = ['age',\n            'trestbps',\n            'chol',\n            'thalach',\n            'oldpeak'\n]\n\n# categorical variables\ncat_cols = ['sex',\n            'cp',\n            'fbs',\n            'restecg',\n            'exang',\n            'slope',\n            'ca',\n            'thal']\n\n# changing format of the categorical variables\ndf[cat_cols] = df[cat_cols].apply(lambda x: x.astype('object'))\n\n# target variable\ny = df['target']\n\n# features\nX = df.drop(['target'], axis=1)\n\n# data split:\n\n# random seed\nnp.random.seed(42)\n\n# splitting the data\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.2,\n                                                    stratify=y)\n\n# double check\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n# pipeline for numerical data\nnum_preprocessing = Pipeline([('num_imputer', SimpleImputer(strategy='mean')), # imputing with mean\n                                                   ('minmaxscaler', MinMaxScaler())]) # scaling\n\n# pipeline for categorical data\ncat_preprocessing = Pipeline([('cat_imputer', SimpleImputer(strategy='constant', fill_value='missing')), # filling missing values\n                                                ('onehot', OneHotEncoder(drop='first', handle_unknown='error'))]) # One Hot Encoding\n\n# preprocessor - combining pipelines\npreprocessor = ColumnTransformer([\n                                  ('categorical', cat_preprocessing, cat_cols),\n                                  ('numerical', num_preprocessing, num_cols)\n                                                           ])\n\n# initial model parameters\nlog_ini_params = {'penalty': 'l2', \n                  'tol': 0.0073559740277086005, \n                  'C': 1.1592424247511928, \n                  'fit_intercept': True, \n                  'solver': 'liblinear'}\n\n# model - Pipeline\nlog_clf = Pipeline([('preprocessor', preprocessor),\n                  ('clf', LogisticRegression(**log_ini_params))])\n\nlog_clf.fit(X_train, y_train)\n\n# dumping the model\nf = 'model\/log.pkl'\nwith open(f, 'wb') as file:\n    pickle.dump(log_clf, file)\n\n# loading it\nloaded_model = joblib.load(f)\n\n# double check on a single datapoint\nnew_data = pd.DataFrame({'age': 71,\n                         'sex': 0,\n                         'cp': 0,\n                         'trestbps': 112,\n                         'chol': 203,\n                         'fbs': 0,\n                         'restecg': 1,\n                         'thalach': 185,\n                         'exang': 0,\n                         'oldpeak': 0.1,\n                         'slope': 2,\n                         'ca': 0,\n                          'thal': 2}, index=[0])\n\nloaded_model.predict(new_data)\n\n<\/code><\/pre>\n<p>...and it works just fine.  Then I deploy the model to the Azure Web Service using these steps:<\/p>\n<ol>\n<li>I create the score.py file<\/li>\n<\/ol>\n<pre><code>import joblib\nfrom azureml.core.model import Model\nimport json\n\ndef init():\n    global model\n    model_path = Model.get_model_path('log') # logistic\n    print('Model Path is  ', model_path)\n    model = joblib.load(model_path)\n\n\ndef run(data):\n    try:\n        data = json.loads(data)\n        result = model.predict(data['data'])\n        # any data type, as long as it is JSON serializable.\n        return {'data' : result.tolist() , 'message' : 'Successfully classified heart diseases'}\n    except Exception as e:\n        error = str(e)\n        return {'data' : error , 'message' : 'Failed to classify heart diseases'}\n<\/code><\/pre>\n<ol>\n<li>I deploy the model:<\/li>\n<\/ol>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core.webservice import AciWebservice\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.environment import Environment\nfrom azureml.core import Workspace\nfrom azureml.core.model import Model\nfrom azureml.core.conda_dependencies import CondaDependencies\n\nws = Workspace.from_config()\n\nmodel = Model.register(workspace = ws,\n              model_path ='model\/log.pkl',\n              model_name = 'log',\n              tags = {'version': '1'},\n              description = 'Heart disease classification',\n              )\n\n# to install required packages\nenv = Environment('env')\ncd = CondaDependencies.create(pip_packages=['pandas==1.1.5', 'azureml-defaults','joblib==0.17.0'], conda_packages = ['scikit-learn==0.23.2'])\nenv.python.conda_dependencies = cd\n\n# Register environment to re-use later\nenv.register(workspace = ws)\nprint('Registered Environment')\n\nmyenv = Environment.get(workspace=ws, name='env')\n\nmyenv.save_to_directory('.\/environ', overwrite=True)\n\naciconfig = AciWebservice.deploy_configuration(\n            cpu_cores=1,\n            memory_gb=1,\n            tags={'data':'heart disease classifier'},\n            description='Classification of heart diseases',\n            )\n\ninference_config = InferenceConfig(entry_script='score.py', environment=myenv)\n\nservice = Model.deploy(workspace=ws,\n                name='hd-model-log',\n                models=[model],\n                inference_config=inference_config,\n                deployment_config=aciconfig, \n                overwrite = True)\n\nservice.wait_for_deployment(show_output=True)\nurl = service.scoring_uri\nprint(url)\n<\/code><\/pre>\n<p>The deployment is fine:<\/p>\n<blockquote>\n<p>Succeeded\nACI service creation operation finished, operation &quot;Succeeded&quot;<\/p>\n<\/blockquote>\n<p>But I can not make any predictions with the new data. I try to use:<\/p>\n<pre><code>import pandas as pd\n\nnew_data = pd.DataFrame([[71, 0, 0, 112, 203, 0, 1, 185, 0, 0.1, 2, 0, 2],\n                         [80, 0, 0, 115, 203, 0, 1, 185, 0, 0.1, 2, 0, 0]],\n                         columns=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal'])\n<\/code><\/pre>\n<p>Following the answer from this topic (<a href=\"https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/217305\/data-input-format-call-the-service-for-azure-ml-ti.html\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/217305\/data-input-format-call-the-service-for-azure-ml-ti.html<\/a>) I transform the data:<\/p>\n<pre><code>test_sample = json.dumps({'data': new_data.to_dict(orient='records')})\n<\/code><\/pre>\n<p>And try to make some predictions:<\/p>\n<pre><code>import json\nimport requests\ndata = test_sample\nheaders = {'Content-Type':'application\/json'}\nr = requests.post(url, data=data, headers = headers)\nprint(r.status_code)\nprint(r.json())\n<\/code><\/pre>\n<p>However, I encounter an error:<\/p>\n<blockquote>\n<p>200\n{'data': &quot;Expected 2D array, got 1D array instead:\\narray=[{'age': 71, 'sex': 0, 'cp': 0, 'trestbps': 112, 'chol': 203, 'fbs': 0, 'restecg': 1, 'thalach': 185, 'exang': 0, 'oldpeak': 0.1, 'slope': 2, 'ca': 0, 'thal': &gt; 2}\\n {'age': 80, 'sex': 0, 'cp': 0, 'trestbps': 115, 'chol': 203, 'fbs': 0, 'restecg': 1, 'thalach': 185, 'exang': 0, 'oldpeak': 0.1, 'slope': 2, 'ca': 0, 'thal': 0}].\\nReshape your data either using array.reshape(-1, &gt; 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.&quot;, 'message': 'Failed to classify heart diseases'}<\/p>\n<\/blockquote>\n<p>How is it possible to adjust the input data to this form of predictions and add other output like predict_proba so I could store them in a separate output dataset?<\/p>\n<p>I know this error is somehow related either with the &quot;run&quot; part of the score.py file or the last code cell that calls the webservice, but I'm unable to find it.<\/p>\n<p>Would really appreciate some help.<\/p>",
        "Challenge_closed_time":1653558488392,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653476427160,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in making predictions with Azure Machine Learning using new data that contains headers. The user has constructed a simple model with heart-disease dataset and wrapped it into Pipeline as they use some featurization steps. The model is deployed to the Azure Web Service, but the user is unable to make any predictions with the new data. The user is encountering an error related to the input data format and is seeking help to adjust the input data to this form of predictions and add other output like predict_proba so they could store them in a separate output dataset.",
        "Challenge_last_edit_time":1653505948283,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72376401",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":12.9,
        "Challenge_reading_time":99.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":79,
        "Challenge_solved_time":22.7947866667,
        "Challenge_title":"Making predictions with Azure Machine learning with new data that contains headers (like pd.Dataframe)",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":280.0,
        "Challenge_word_count":761,
        "Platform":"Stack Overflow",
        "Poster_created_time":1513074641863,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Antarctica",
        "Poster_reputation_count":155.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>I believe I managed to solve the problem - even though I encountered some serious issues. :)<\/p>\n<ol>\n<li>As described here <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script\" rel=\"nofollow noreferrer\">here<\/a> - I edited the <code>score.py<\/code> script:<\/li>\n<\/ol>\n<pre><code>import joblib\nfrom azureml.core.model import Model\nimport numpy as np\nimport json\nimport pandas as pd\nimport numpy as np\n\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\nfrom inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\nfrom inference_schema.parameter_types.standard_py_parameter_type import StandardPythonParameterType\n    \ndata_sample = PandasParameterType(pd.DataFrame({'age': pd.Series([0], dtype='int64'),\n                                                'sex': pd.Series(['example_value'], dtype='object'),\n                                                'cp': pd.Series(['example_value'], dtype='object'),\n                                                'trestbps': pd.Series([0], dtype='int64'),\n                                                'chol': pd.Series([0], dtype='int64'),\n                                                'fbs': pd.Series(['example_value'], dtype='object'),\n                                                'restecg': pd.Series(['example_value'], dtype='object'),\n                                                'thalach': pd.Series([0], dtype='int64'),\n                                                'exang': pd.Series(['example_value'], dtype='object'),\n                                                'oldpeak': pd.Series([0.0], dtype='float64'),\n                                                'slope': pd.Series(['example_value'], dtype='object'),\n                                                'ca': pd.Series(['example_value'], dtype='object'),\n                                                'thal': pd.Series(['example_value'], dtype='object')}))\n\ninput_sample = StandardPythonParameterType({'data': data_sample})\nresult_sample = NumpyParameterType(np.array([0]))\noutput_sample = StandardPythonParameterType({'Results':result_sample})\n\ndef init():\n    global model\n    # Example when the model is a file\n    model_path = Model.get_model_path('log') # logistic\n    print('Model Path is  ', model_path)\n    model = joblib.load(model_path)\n\n@input_schema('Inputs', input_sample)\n@output_schema(output_sample)\ndef run(Inputs):\n    try:\n        data = Inputs['data']\n        result = model.predict_proba(data)\n        return result.tolist()\n    except Exception as e:\n        error = str(e)\n        return error\n<\/code><\/pre>\n<ol start=\"2\">\n<li>In the deployment step I adjusted the <code>CondaDependencies<\/code>:<\/li>\n<\/ol>\n<pre><code># to install required packages\nenv = Environment('env')\ncd = CondaDependencies.create(pip_packages=['pandas==1.1.5', 'azureml-defaults','joblib==0.17.0', 'inference-schema==1.3.0'], conda_packages = ['scikit-learn==0.22.2.post1'])\nenv.python.conda_dependencies = cd\n# Register environment to re-use later\nenv.register(workspace = ws)\nprint('Registered Environment')\n<\/code><\/pre>\n<p>as<\/p>\n<p>a) It is necessary to include <code>inference-schema<\/code> in the <code>Dependencies<\/code> file\nb) I downgraded <code>scikit-learn<\/code> to <code>scikit-learn==0.22.2.post1<\/code> version because of <a href=\"https:\/\/github.com\/hyperopt\/hyperopt\/issues\/668\" rel=\"nofollow noreferrer\">this issue<\/a><\/p>\n<p>Now, when I feed the model with new data:<\/p>\n<pre><code>new_data = {\n  &quot;Inputs&quot;: {\n    &quot;data&quot;: [\n      {\n        &quot;age&quot;: 71,\n        &quot;sex&quot;: &quot;0&quot;,\n        &quot;cp&quot;: &quot;0&quot;,\n        &quot;trestbps&quot;: 112,\n        &quot;chol&quot;: 203,\n        &quot;fbs&quot;: &quot;0&quot;,\n        &quot;restecg&quot;: &quot;1&quot;,\n        &quot;thalach&quot;: 185,\n        &quot;exang&quot;: &quot;0&quot;,\n        &quot;oldpeak&quot;: 0.1,\n        &quot;slope&quot;: &quot;2&quot;,\n        &quot;ca&quot;: &quot;0&quot;,\n        &quot;thal&quot;: &quot;2&quot;\n      }\n    ]\n  }\n}\n<\/code><\/pre>\n<p>And use it for prediction:<\/p>\n<pre><code>import json\nimport requests\ndata = new_data\nheaders = {'Content-Type':'application\/json'}\nr = requests.post(url, str.encode(json.dumps(data)), headers = headers)\nprint(r.status_code)\nprint(r.json())\n<\/code><\/pre>\n<p>I get:<\/p>\n<p><code>200 [[0.02325369841858338, 0.9767463015814166]]<\/code><\/p>\n<p>Uff! Maybe someone will benefit from my painful learning path! :)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1653559436590,
        "Solution_link_count":2.0,
        "Solution_readability":17.1,
        "Solution_reading_time":51.56,
        "Solution_score_count":0.0,
        "Solution_sentence_count":36.0,
        "Solution_word_count":293.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":110.8842475,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to open a pickled XGBoost model I created in AWS Sagemaker to look at feature importances in the model. I'm trying to follow the answers in <a href=\"https:\/\/stackoverflow.com\/questions\/55621967\/feature-importance-for-xgboost-in-sagemaker\">this post<\/a>. However, I get an the error shown below. When I try to call <code>Booster.save_model<\/code>, I get an error saying <code>'Estimator' object has no attribute 'save_model'<\/code>. How can I resolve this? <\/p>\n\n<pre><code># Build initial model\nsess = sagemaker.Session()\ns3_input_train = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/train\/'.format(bucket, prefix), content_type='csv')\nxgb_cont = get_image_uri(region, 'xgboost', repo_version='0.90-1')\nxgb = sagemaker.estimator.Estimator(xgb_cont, role, train_instance_count=1, train_instance_type='ml.m4.4xlarge',\n                                    output_path='s3:\/\/{}\/{}'.format(bucket, prefix), sagemaker_session=sess)\nxgb.set_hyperparameters(eval_metric='rmse', objective='reg:squarederror', num_round=100)\nts = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nxgb_name = 'xgb-initial-' + ts\nxgb.set_hyperparameters(eta=0.1, alpha=0.5, max_depth=10)\nxgb.fit({'train': s3_input_train}, job_name=xgb_name)\n\n# Load model to get feature importances\nmodel_path = 's3:\/\/{}\/{}\/\/output\/model.tar.gz'.format(bucket, prefix, xgb_name)\nfs = s3fs.S3FileSystem()\nwith fs.open(model_path, 'rb') as f:\n    with tarfile.open(fileobj=f, mode='r') as tar_f:\n        with tar_f.extractfile('xgboost-model') as extracted_f:\n            model = pickle.load(extracted_f)\n\nXGBoostError: [19:16:42] \/workspace\/src\/learner.cc:682: Check failed: header == serialisation_header_: \n\n  If you are loading a serialized model (like pickle in Python) generated by older\n  XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version.  There's a simple script for helping\n  the process. See:\n\n    https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html\n\n  for reference to the script, and more details about differences between saving model and\n  serializing.\n<\/code><\/pre>",
        "Challenge_closed_time":1584357657183,
        "Challenge_comment_count":2,
        "Challenge_created_time":1583954281893,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to open a pickled XGBoost model created in AWS Sagemaker to look at feature importances in the model. The user is following the answers in a post but is getting an error saying \"'Estimator' object has no attribute 'save_model'\" when trying to call Booster.save_model. The error message suggests that the user should export the model by calling Booster.save_model from the older version first and then load it back in the current version.",
        "Challenge_last_edit_time":1583958473892,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60643094",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":14.6,
        "Challenge_reading_time":27.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":112.0486916667,
        "Challenge_title":"Unable to open pickled Sagemaker XGBoost model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":4586.0,
        "Challenge_word_count":199,
        "Platform":"Stack Overflow",
        "Poster_created_time":1431970105067,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":4631.0,
        "Poster_view_count":333.0,
        "Solution_body":"<p>Which version of XGBoost are you using in the notebook? The model format has changed in XGBoost 1.0. See <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html\" rel=\"nofollow noreferrer\">https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html<\/a>. Short version: if you're using 1.0 in the notebook, you can't load a pickled model. <\/p>\n\n<p>Here's a working example using XGBoost in script mode (which is much more flexible than the built in algo):<\/p>\n\n<ul>\n<li><a href=\"https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/-\/blob\/master\/sagemaker\/09-XGBoost-script-mode.ipynb\" rel=\"nofollow noreferrer\">https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/-\/blob\/master\/sagemaker\/09-XGBoost-script-mode.ipynb<\/a><\/li>\n<li><a href=\"https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/-\/blob\/master\/sagemaker\/xgb.py\" rel=\"nofollow noreferrer\">https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/-\/blob\/master\/sagemaker\/xgb.py<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":21.3,
        "Solution_reading_time":12.81,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":68.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1363541433296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Twin Cities, MN, USA",
        "Answerer_reputation_count":348.0,
        "Answerer_view_count":24.0,
        "Challenge_adjusted_solved_time":7.8782044444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying in Amazon Sagemaker to deploy an existing Scikit-Learn model. So a model that wasn't trained on SageMaker, but locally on my machine.<\/p>\n<p>On my local (windows) machine I've saved my model as model.joblib and tarred the model to model.tar.gz.<\/p>\n<p>Next, I've uploaded this model to my S3 bucket ('my_bucket') in the following path s3:\/\/my_bucket\/models\/model.tar.gz. I can see the tar file in S3.<\/p>\n<p>But when I'm trying to deploy the model, it keeps giving the error message &quot;Failed to extract model data archive&quot;.<\/p>\n<p>The .tar.gz is generated on my local machine by running 'tar -czf model.tar.gz model.joblib' in a powershell command window.<\/p>\n<p>The code for uploading to S3<\/p>\n<pre><code>import boto3\ns3 = boto3.client(&quot;s3&quot;, \n              region_name='eu-central-1', \n              aws_access_key_id=AWS_KEY_ID, \n              aws_secret_access_key=AWS_SECRET)\ns3.upload_file(Filename='model.tar.gz', Bucket=my_bucket, Key='models\/model.tar.gz')\n<\/code><\/pre>\n<p>The code for creating the estimator and deploying:<\/p>\n<pre><code>import boto3\nfrom sagemaker.sklearn.estimator import SKLearnModel\n\n...\n\nmodel_data = 's3:\/\/my_bucket\/models\/model.tar.gz'\nsklearn_model = SKLearnModel(model_data=model_data,\n                             role=role,\n                             entry_point=&quot;my-script.py&quot;,\n                             framework_version=&quot;0.23-1&quot;)\npredictor = sklearn_model.deploy(instance_type=&quot;ml.t2.medium&quot;, initial_instance_count=1)                             \n<\/code><\/pre>\n<p>The error message:<\/p>\n<blockquote>\n<p>error message: UnexpectedStatusException: Error hosting endpoint\nsagemaker-scikit-learn-2021-01-24-17-24-42-204: Failed. Reason: Failed\nto extract model data archive for container &quot;container_1&quot; from URL\n&quot;s3:\/\/my_bucket\/models\/model.tar.gz&quot;. Please ensure that the object\nlocated at the URL is a valid tar.gz archive<\/p>\n<\/blockquote>\n<p>Is there a way to see why the archive is invalid?<\/p>",
        "Challenge_closed_time":1611852069143,
        "Challenge_comment_count":5,
        "Challenge_created_time":1611565411923,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy an existing Scikit-Learn model on Amazon SageMaker that was not trained on SageMaker but locally on their machine. They saved the model as model.joblib and tarred it to model.tar.gz. The user uploaded the model to their S3 bucket but encountered an error message \"Failed to extract model data archive\" when trying to deploy the model. The user is seeking a way to see why the archive is invalid.",
        "Challenge_last_edit_time":1611823707607,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65881699",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":11.7,
        "Challenge_reading_time":25.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":79.6270055556,
        "Challenge_title":"SageMaker failed to extract model data archive tar.gz for container when deploying",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1859.0,
        "Challenge_word_count":208,
        "Platform":"Stack Overflow",
        "Poster_created_time":1472970520888,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Amersfoort, Nederland",
        "Poster_reputation_count":424.0,
        "Poster_view_count":21.0,
        "Solution_body":"<p>I had a similar issue as well, along with a similar fix to Bas (per comment above).<\/p>\n<p>I was finding I wasn't necessarily having issues with the .tar.gz step, this command does work fine:<\/p>\n<p><code>tar -czf &lt;filename&gt; .\/&lt;directory-with-files&gt;<\/code><\/p>\n<p>but rather with the uploading step.<\/p>\n<p>Manually uploading to S3 should take care of this, however, if you're doing this step programmatically, you might need to double check the steps taken. Bas appears to have had filename issues, mine were around using boto properly. Here's some code that works (Python only here, but watch for similar issues with other libraries):<\/p>\n<pre><code>bucket = 'bucket-name'\nkey = 'directory-inside-bucket'\nfile = 'the file name of the .tar.gz'\n\ns3_client = boto3.client('s3')\ns3_client.upload_file(file, bucket, key)\n<\/code><\/pre>\n<p>Docs: <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.9,
        "Solution_reading_time":14.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":120.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":4.4848666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to training TensorFlow model on AWS Sagemaker.\nI created container with external lib for that (Use Your Own Algorithms or Models with Amazon SageMaker).<\/p>\n\n<p>we run a training job with TensorFlow API<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.tensorflow import TensorFlow\nestimator = TensorFlow(\n  entry_point=\"entry.py\",             # entry script\n  role=role,\n  framework_version=\"1.13.0\",   \n  py_version='py3',\n  hyperparameters=hyperparameters,\n  train_instance_count=1,                   # \"The number of GPUs instances to use\"\n  train_instance_type=train_instance_type,\n  image_name=my_image\n\n)\nestimator.fit({'train': train_s3, 'eval': eval_s3})\n<\/code><\/pre>\n\n<p>and got an error:<\/p>\n\n<pre><code>09:06:46\n2019-07-23 09:06:45,463 INFO - root - running container entrypoint \uf141\n09:06:46\n2019-07-23 09:06:45,463 INFO - root - starting train task \uf141\n09:06:46\n2019-07-23 09:06:45,476 INFO - container_support.training - Training starting \uf141\n09:06:46\n2019-07-23 09:06:45,479 ERROR - container_support.training - uncaught exception during training: No module named 'tf_container'\n\uf141\n09:06:46\nTraceback (most recent call last): File \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/environment.py\", line 136, in load_framework return importlib.import_module('mxnet_container') File \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File \"&lt;frozen importlib._bootstrap&gt;\", line 994, in _gcd_i \uf141\n09:06:46\nModuleNotFoundError: No module named 'mxnet_container'\n\uf141\n09:06:46\nDuring handling of the above exception, another exception occurred:\n\uf141\n09:06:46\nTraceback (most recent call last): File \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/training.py\", line 35, in start fw = TrainingEnvironment.load_framework() File \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/environment.py\", line 138, in load_framework return importlib.import_module('tf_container') File \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, \uf141\n09:06:46\nModuleNotFoundError: No module named 'tf_container'\n<\/code><\/pre>\n\n<p>What can I do to solve this issue? how can I debug this case?<\/p>",
        "Challenge_closed_time":1563901268600,
        "Challenge_comment_count":0,
        "Challenge_created_time":1563885123080,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to train a TensorFlow model on AWS Sagemaker. The error message indicates that there is no module named 'tf_container'. The user is seeking advice on how to solve this issue and how to debug the case.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57164290",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":29.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":4.4848666667,
        "Challenge_title":"Training with TensorFlow on Sagemaker No module named 'tf_container'",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1191.0,
        "Challenge_word_count":206,
        "Platform":"Stack Overflow",
        "Poster_created_time":1471952551663,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I'm guessing that you used your own TF container, not the SageMaker one at <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container<\/a><\/p>\n\n<p>If that's the case, your container is missing the support code needed to use the TensorFlow estimator ('tf_container' package).<\/p>\n\n<p>The solution is to start from the SageMaker container, customize it, push it back to ECR and pass the image name to the SageMaker estimator with the 'image_name' parameter.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.4,
        "Solution_reading_time":7.04,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.1106713889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,  <br \/>\nI have been using object detection from Custom Vision to train images. Classification do not suit my goal so I'm looking at alternative methods to training images. Can I get some suggestions with using Azure for images segmentation?<\/p>",
        "Challenge_closed_time":1610955618900,
        "Challenge_comment_count":1,
        "Challenge_created_time":1610951620483,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking suggestions for using Azure to train images for segmentation, as classification does not meet their requirements.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/234232\/images-segmentation-using-azure",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.7,
        "Challenge_reading_time":3.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.1106713889,
        "Challenge_title":"Images Segmentation using Azure",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":43,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=d94e82d7-8d83-4f86-8055-f9af74b9130d\">@Nam Ly  <\/a>    <\/p>\n<p>Suggestions and refer below url for Azure for images segmentation.    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/samples\/browse\/?products=azure&amp;term=vision&amp;terms=%22Custom%20Vision%22\">Custom Vision integration sample skill for cognitive search<\/a>    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/learn\/modules\/classify-images-custom-vision\/\">Classify images with the Custom Vision service<\/a>    <\/p>\n<p>Please don\u2019t forget to <code>Accept the answer<\/code> and <code>up-vote<\/code> wherever the information provided helps you, this can be beneficial to other community members.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.2,
        "Solution_reading_time":9.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":60.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1463778034387,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":2.7701311111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I\u2019m using the following script to execute an AutoML run, also passing the test dataset<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>automl_settings = {\n    &quot;n_cross_validations&quot;: 10,\n    &quot;primary_metric&quot;: 'spearman_correlation',\n    &quot;enable_early_stopping&quot;: True,\n    &quot;max_concurrent_iterations&quot;: 10, \n    &quot;max_cores_per_iteration&quot;: -1,   \n    &quot;experiment_timeout_hours&quot;: 1,\n    &quot;featurization&quot;: 'auto',\n    &quot;verbosity&quot;: logging.INFO}\nautoml_config = AutoMLConfig(task = 'regression',\n                             debug_log = 'automl_errors.log',\n                             compute_target = compute_target,\n                             training_data = training_data,\n                             test_data = test_data,\n                             label_column_name = label_column_name,\n                             model_explainability = True,\n                             **automl_settings                            )\n<\/code><\/pre>",
        "Challenge_closed_time":1635964127852,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635954155380,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to execute an AutoML run and pass the test dataset to get metrics out of it. They have provided a script with various settings for the AutoML run, including the primary metric, featurization, and verbosity.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69827748",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":26.6,
        "Challenge_reading_time":10.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":2.7701311111,
        "Challenge_title":"get metrics out of AutoMLRun based on test_data",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":85.0,
        "Challenge_word_count":58,
        "Platform":"Stack Overflow",
        "Poster_created_time":1405457120427,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seattle, WA, USA",
        "Poster_reputation_count":3359.0,
        "Poster_view_count":555.0,
        "Solution_body":"<p>Note that the TEST DATASET SUPPORT is a feature still in PRIVATE PREVIEW. It'll probably be released as PUBLIC PREVIEW later in NOVEMBER, but until then, you need to be enrolled in the PRIVATE PREVIEW in order to see the &quot;Test runs and metrics&quot; in the UI. You can send me an email to cesardl at microsoft dot com and send me your AZURE SUBSCRIPTION ID to be enabled so you see it in the UI.<\/p>\n<p>You can see further info on how to get started here:\n<a href=\"https:\/\/github.com\/Azure\/automl-testdataset-preview\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/automl-testdataset-preview<\/a><\/p>\n<p>About how to use it, you need to either provide the test_Data (specific Test AML Tabular Dataset that for instance you loaded from a file os split manually previously)\nor you can provide a test_size which is the % (i.e. 0.2 is 20%) to be split from the single\/original dataset.<\/p>\n<p>About the TEST metrics, since you can make multiple TEST runs against a single model, you need to go to the specific TEST run available under the link &quot;Test results&quot;<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3pPPS.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":11.0,
        "Solution_reading_time":14.9,
        "Solution_score_count":3.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":177.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1588516515763,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"UK",
        "Answerer_reputation_count":29087.0,
        "Answerer_view_count":3080.0,
        "Challenge_adjusted_solved_time":0.2190208333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following <a href=\"https:\/\/github.com\/mtm12\/SageMakerDemo\" rel=\"noreferrer\">this example<\/a> on how to train a machine learning model in Amazon-sagemaker.<\/p>\n<pre><code>data_location = 's3:\/\/{}\/kmeans_highlevel_example\/data'.format(bucket)\noutput_location = 's3:\/\/{}\/kmeans_highlevel_example\/output'.format(bucket)\n\nprint('training data will be uploaded to: {}'.format(data_location))\nprint('training artifacts will be uploaded to: {}'.format(output_location))\n\nkmeans = KMeans(role=role,\n                train_instance_count=2,\n                train_instance_type='ml.c4.8xlarge',\n                output_path=output_location,\n                k=10,\n                epochs=100,\n                data_location=data_location)\n<\/code><\/pre>\n<p>So after calling the fit function the model should be saved in the S3 bucket?? How can you load this model next time?<\/p>",
        "Challenge_closed_time":1595162985808,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595162197333,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is trying to train a machine learning model in Amazon Sagemaker by following a specific example. After calling the fit function, the model should be saved in the S3 bucket, but the user is unsure how to load the model for future use.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62980380",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.4,
        "Challenge_reading_time":10.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.2190208333,
        "Challenge_title":"How to load trained model in amazon sagemaker?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":4776.0,
        "Challenge_word_count":74,
        "Platform":"Stack Overflow",
        "Poster_created_time":1310821356880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Slovenia",
        "Poster_reputation_count":14913.0,
        "Poster_view_count":1093.0,
        "Solution_body":"<p>This can be done by using the sagemaker library combined with the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html\" rel=\"noreferrer\">Inference Model<\/a>.<\/p>\n<pre><code>model = sagemaker.model.Model(\n    image=image\n    model_data='s3:\/\/bucket\/model.tar.gz',\n    role=role_arn)\n<\/code><\/pre>\n<p>The options you're passing in are:<\/p>\n<ul>\n<li><code>image<\/code> - This is the ECR image you're using for inference (which should be for the algorithm you're trying to use). Paths are available <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html\" rel=\"noreferrer\">here<\/a>.<\/li>\n<li><code>model_data<\/code> - This is the path of where your model is stored (in a <code>tar.gz<\/code> compressed archive).<\/li>\n<li><code>role<\/code> - This is the arn of a role that is capable of both pulling the image from ECR and getting the s3 archive.<\/li>\n<\/ul>\n<p>Once you've successfully done this you will need to setup an endpoint, this can be done by performing the following in your notebook through the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html#sagemaker.model.Model.deploy\" rel=\"noreferrer\">deploy function<\/a>.<\/p>\n<pre><code>model.deploy(\n   initial_instance_count=1,\n   instance_type='ml.p2.xlarge'\n)\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.4,
        "Solution_reading_time":17.2,
        "Solution_score_count":8.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":128.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1530092504712,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":915.0,
        "Answerer_view_count":288.0,
        "Challenge_adjusted_solved_time":0.5786791667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to figure out how to store intermediate Kedro pipeline objects both locally AND on S3. In particular, say I have a dataset on S3:<\/p>\n<pre><code>my_big_dataset.hdf5:\n  type: kedro.extras.datasets.pandas.HDFDataSet\n  filepath: &quot;s3:\/\/my_bucket\/data\/04_feature\/my_big_dataset.hdf5&quot;\n<\/code><\/pre>\n<p>I want to refer to these objects in the catalog by their S3 URI so that my team can use them. HOWEVER, I want to avoid re-downloading the datasets, model weights, etc. every time I run a pipeline by keeping a local copy in addition to the S3 copy. How do I mirror files with Kedro?<\/p>",
        "Challenge_closed_time":1597010598252,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597008515007,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to find a way to store Kedro pipeline objects both locally and on S3, while avoiding re-downloading datasets and model weights every time they run a pipeline. They want to refer to objects in the catalog by their S3 URI, but also keep a local copy. The user is seeking guidance on how to mirror files with Kedro.",
        "Challenge_last_edit_time":1597058318400,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63331505",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":8.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.5786791667,
        "Challenge_title":"How to catalog datasets & models by S3 URI, but keep a local copy?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":595.0,
        "Challenge_word_count":100,
        "Platform":"Stack Overflow",
        "Poster_created_time":1415053264667,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"USA",
        "Poster_reputation_count":11166.0,
        "Poster_view_count":653.0,
        "Solution_body":"<p>This is a good question, Kedro has <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.CachedDataSet.html\" rel=\"nofollow noreferrer\"><code>CachedDataSet<\/code><\/a> for caching datasets within the same run, which handles caching the dataset in memory when it's used\/loaded multiple times in the same run. There isn't really the same thing that persists across runs, in general Kedro doesn't do much persistent stuff.<\/p>\n<p>That said, off the top of my head, I can think of two options that (mostly) replicates or gives this functionality:<\/p>\n<ol>\n<li>Use the same <code>catalog<\/code> in the same config environment but with the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/04_kedro_project_setup\/02_configuration.html?#templating-configuration\" rel=\"nofollow noreferrer\"><code>TemplatedConfigLoader<\/code><\/a> where your catalog datasets have their filepaths looking something like:<\/li>\n<\/ol>\n<pre><code>my_dataset:\n  filepath: ${base_data}\/01_raw\/blah.csv\n<\/code><\/pre>\n<p>and you set <code>base_data<\/code> to <code>s3:\/\/bucket\/blah<\/code> when running in &quot;production&quot; mode and with <code>local_filepath\/data<\/code> locally. You can decide how exactly you do this in your overriden <code>context<\/code> method (whether it's using <code>local\/globals.yml<\/code> (see the linked documentation above) or environment variables or what not.<\/p>\n<ol start=\"2\">\n<li>Use separate environments, likely <code>local<\/code> (it's kind of what it was made for!) where you keep a separate copy of your catalog where the filepaths are replaced with local ones.<\/li>\n<\/ol>\n<p>Otherwise, your next best bet is to write a <code>PersistentCachedDataSet<\/code> similar to <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.CachedDataSet.html\" rel=\"nofollow noreferrer\"><code>CachedDataSet<\/code><\/a> which intercepts the loading\/saving for the wrapped dataset and makes a local copy when loading for the first time in a deterministic location that you look up on subsequent loads.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":14.2,
        "Solution_reading_time":25.97,
        "Solution_score_count":4.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":227.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1384343462316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pune, Maharashtra, India",
        "Answerer_reputation_count":478.0,
        "Answerer_view_count":118.0,
        "Challenge_adjusted_solved_time":48.2131977778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I tried to create MLproject with zero parameters as:<\/p>\n\n<pre><code>name: test\n\nconda_env: conda.yaml\n\nentry_points:\n  main:\n    parameters:\n    command: \"python test.py\"\n<\/code><\/pre>\n\n<p>when I get an error:<\/p>\n\n<pre><code>  Traceback (most recent call last):\n File \"\/home\/ubuntu\/.local\/bin\/mlflow\", line 11, in &lt;module&gt;\n    sys.exit(cli())\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/cli.py\", line 137, in run\n    run_id=run_id,\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/__init__.py\", line 230, in run\n    use_conda=use_conda, storage_dir=storage_dir, synchronous=synchronous, run_id=run_id)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/__init__.py\", line 85, in _run\n    project = _project_spec.load_project(work_dir)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/_project_spec.py\", line 40, in load_project\n    entry_points[name] = EntryPoint(name, parameters, command)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/_project_spec.py\", line 87, in __init__\n    self.parameters = {k: Parameter(k, v) for (k, v) in parameters.items()}\nAttributeError: 'NoneType' object has no attribute 'items'\n<\/code><\/pre>\n\n<p>Am I missing something or mlflow does not allow project with  zero parameters?<\/p>\n\n<p>I have also posted this at my public repo of: <a href=\"https:\/\/github.com\/sameermahajan\/mlflow-try\" rel=\"nofollow noreferrer\">https:\/\/github.com\/sameermahajan\/mlflow-try<\/a> if someone would like to try out:<\/p>\n\n<pre><code>mlflow run https:\/\/github.com\/sameermahajan\/mlflow-try.git\n<\/code><\/pre>",
        "Challenge_closed_time":1562240543612,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562066976100,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create an MLproject with zero parameters using mlflow, but is encountering an error message indicating that 'NoneType' object has no attribute 'items'. The user is unsure if they are missing something or if mlflow does not allow projects with zero parameters.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56851463",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":30.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":48.2131977778,
        "Challenge_title":"How do I specify mlflow MLproject with zero parameters?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":353.0,
        "Challenge_word_count":185,
        "Platform":"Stack Overflow",
        "Poster_created_time":1384343462316,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":478.0,
        "Poster_view_count":118.0,
        "Solution_body":"<p>For this, you completely drop the 'parameters' section as below:<\/p>\n\n<pre><code>name: test\n\nconda_env: conda.yaml\n\nentry_points:\n  main:\n    command: \"python test.py\"\n<\/code><\/pre>\n\n<p>(I thought I had tried it earlier but I was trying too many different ways to may be miss out on this one)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.7,
        "Solution_reading_time":3.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":43.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1535695625688,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":19.0,
        "Challenge_adjusted_solved_time":355.6374158334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am learning Sagemaker and I have this entry point:<\/p>\n\n<pre><code>import os\nimport tensorflow as tf\nfrom tensorflow.python.estimator.model_fn import ModeKeys as Modes\n\nINPUT_TENSOR_NAME = 'inputs'\nSIGNATURE_NAME = 'predictions'\n\nLEARNING_RATE = 0.001\n\n\ndef model_fn(features, labels, mode, params):\n    # Input Layer\n    input_layer = tf.reshape(features[INPUT_TENSOR_NAME], [-1, 28, 28, 1])\n\n    # Convolutional Layer #1\n    conv1 = tf.layers.conv2d(\n        inputs=input_layer,\n        filters=32,\n        kernel_size=[5, 5],\n        padding='same',\n        activation=tf.nn.relu)\n\n    # Pooling Layer #1\n    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n\n    # Convolutional Layer #2 and Pooling Layer #2\n    conv2 = tf.layers.conv2d(\n        inputs=pool1,\n        filters=64,\n        kernel_size=[5, 5],\n        padding='same',\n        activation=tf.nn.relu)\n    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n\n    # Dense Layer\n    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n    dropout = tf.layers.dropout(\n        inputs=dense, rate=0.4, training=(mode == Modes.TRAIN))\n\n    # Logits Layer\n    logits = tf.layers.dense(inputs=dropout, units=10)\n\n    # Define operations\n    if mode in (Modes.PREDICT, Modes.EVAL):\n        predicted_indices = tf.argmax(input=logits, axis=1)\n        probabilities = tf.nn.softmax(logits, name='softmax_tensor')\n\n    if mode in (Modes.TRAIN, Modes.EVAL):\n        global_step = tf.train.get_or_create_global_step()\n        label_indices = tf.cast(labels, tf.int32)\n        loss = tf.losses.softmax_cross_entropy(\n            onehot_labels=tf.one_hot(label_indices, depth=10), logits=logits)\n        tf.summary.scalar('OptimizeLoss', loss)\n\n    if mode == Modes.PREDICT:\n        predictions = {\n            'classes': predicted_indices,\n            'probabilities': probabilities\n        }\n        export_outputs = {\n            SIGNATURE_NAME: tf.estimator.export.PredictOutput(predictions)\n        }\n        return tf.estimator.EstimatorSpec(\n            mode, predictions=predictions, export_outputs=export_outputs)\n\n    if mode == Modes.TRAIN:\n        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n        train_op = optimizer.minimize(loss, global_step=global_step)\n        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n\n    if mode == Modes.EVAL:\n        eval_metric_ops = {\n            'accuracy': tf.metrics.accuracy(label_indices, predicted_indices)\n        }\n        return tf.estimator.EstimatorSpec(\n            mode, loss=loss, eval_metric_ops=eval_metric_ops)\n\n\ndef serving_input_fn(params):\n    inputs = {INPUT_TENSOR_NAME: tf.placeholder(tf.float32, [None, 784])}\n    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n\n\ndef read_and_decode(filename_queue):\n    reader = tf.TFRecordReader()\n    _, serialized_example = reader.read(filename_queue)\n\n    features = tf.parse_single_example(\n        serialized_example,\n        features={\n            'image_raw': tf.FixedLenFeature([], tf.string),\n            'label': tf.FixedLenFeature([], tf.int64),\n        })\n\n    image = tf.decode_raw(features['image_raw'], tf.uint8)\n    image.set_shape([784])\n    image = tf.cast(image, tf.float32) * (1. \/ 255)\n    label = tf.cast(features['label'], tf.int32)\n\n    return image, label\n\n\ndef train_input_fn(training_dir, params):\n    return _input_fn(training_dir, 'train.tfrecords', batch_size=100)\n\n\ndef eval_input_fn(training_dir, params):\n    return _input_fn(training_dir, 'test.tfrecords', batch_size=100)\n\n\ndef _input_fn(training_dir, training_filename, batch_size=100):\n    test_file = os.path.join(training_dir, training_filename)\n    filename_queue = tf.train.string_input_producer([test_file])\n\n    image, label = read_and_decode(filename_queue)\n    images, labels = tf.train.batch(\n        [image, label], batch_size=batch_size,\n        capacity=1000 + 3 * batch_size)\n\n    return {INPUT_TENSOR_NAME: images}, labels\n\ndef neo_preprocess(payload, content_type):\n    import logging\n    import numpy as np\n    import io\n\n    logging.info('Invoking user-defined pre-processing function')\n\n    if content_type != 'application\/x-image' and content_type != 'application\/vnd+python.numpy+binary':\n        raise RuntimeError('Content type must be application\/x-image or application\/vnd+python.numpy+binary')\n\n    f = io.BytesIO(payload)\n    image = np.load(f)*255\n\n    return image\n\n### NOTE: this function cannot use MXNet\ndef neo_postprocess(result):\n    import logging\n    import numpy as np\n    import json\n\n    logging.info('Invoking user-defined post-processing function')\n\n    # Softmax (assumes batch size 1)\n    result = np.squeeze(result)\n    result_exp = np.exp(result - np.max(result))\n    result = result_exp \/ np.sum(result_exp)\n\n    response_body = json.dumps(result.tolist())\n    content_type = 'application\/json'\n\n    return response_body, content_type\n<\/code><\/pre>\n\n<p>And I am training it <\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='cnn_fashion_mnist.py',\n                       role=role,\n                       input_mode='Pipe',\n                       training_steps=1, \n                       evaluation_steps=1,\n                       train_instance_count=1,\n                       output_path=output_path,\n                       train_instance_type='ml.c5.2xlarge',\n                       base_job_name='mnist')\n<\/code><\/pre>\n\n<p>so far it is trying correctly and it tells me that everything when well, but when I check the output there is nothing there or if I try to deploy it I get the error saying it couldn't find the model because there is nothing in the bucker, any ideas or extra configurations? Thank you<\/p>",
        "Challenge_closed_time":1576626745670,
        "Challenge_comment_count":0,
        "Challenge_created_time":1575346450973,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is learning Sagemaker and has created an entry point for a Tensorflow model. They are training the model using Sagemaker, but when they check the output, there is nothing there, and they cannot deploy the model due to the error of not finding the model in the bucket. The user is seeking help to understand if there are any extra configurations required.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59150100",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":17.6,
        "Challenge_reading_time":66.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":64,
        "Challenge_solved_time":355.6374158334,
        "Challenge_title":"Sagemaker and Tensorflow model not saved",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":658.0,
        "Challenge_word_count":407,
        "Platform":"Stack Overflow",
        "Poster_created_time":1462770189772,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":45.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>Looks like you are using one of the older Tensorflow versions.\nWe would recommend switching to a newer more straight-forward way of running Tensorflow in SageMaker (script mode) by switching to a more recent Tensorflow version.<\/p>\n\n<p>You can read more about it in our documentation:\n<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html<\/a><\/p>\n\n<p>Here is an example that might help:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/tensorflow_script_mode_training_and_serving.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/tensorflow_script_mode_training_and_serving.ipynb<\/a> <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":25.0,
        "Solution_reading_time":12.16,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":61.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":42.8373255556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy a classification TensorFlow model on AZURE from GitHub. It is getting deployed correctly which can be seen from the below logs.  <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/111656-image.png?platform=QnA\" alt=\"111656-image.png\" \/><\/p>\n<p>But I'm getting an OSError on log Stream saying saved model doesn't exist as shown below. The error msg is highlighted in red.  <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/111560-image.png?platform=QnA\" alt=\"111560-image.png\" \/><\/p>\n<p>But this is working correctly on local. This model has been checked locally.  <br \/>\nThe repository for this can be checked at <a href=\"https:\/\/github.com\/Vikeshkr-DSP\/cassava-leaf-disease-prediction\">https:\/\/github.com\/Vikeshkr-DSP\/cassava-leaf-disease-prediction<\/a>.  <br \/>\nThanks in advance for your help.<\/p>",
        "Challenge_closed_time":1625582635012,
        "Challenge_comment_count":3,
        "Challenge_created_time":1625428420640,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue while deploying a classification TensorFlow model on AZURE from GitHub. The model is getting deployed correctly, but the user is getting an OSError on log Stream saying saved model doesn't exist. The model is working correctly on local, and the repository for this can be checked at https:\/\/github.com\/Vikeshkr-DSP\/cassava-leaf-disease-prediction.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/462250\/facing-problem-in-deplying-pyhton-application-from",
        "Challenge_link_count":3,
        "Challenge_participation_count":4,
        "Challenge_readability":11.7,
        "Challenge_reading_time":12.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":42.8373255556,
        "Challenge_title":"Facing problem in deplying pyhton application from github- unable to load tensorflow saved model in AZURE.",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":105,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The above issue on <code>OSError: SavedModel file does not exist at: cassava_leaf.h5\/{saved_model.pbtxt|saved_model.pb}<\/code> was due to a bit large size of model and we did not provide an absolute path to the model location, the application could not find the same during startup.  <\/p>\n<p>The issue resolved by manually transferring the h5-model file to a location like \/home on the App Service and updated the app.py file to use an absolute path in order to refer the file.   <\/p>\n<p>Similar to: <code>model=load_model('\/home\/cassava_leaf.h5')<\/code>  <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.9,
        "Solution_reading_time":7.02,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":82.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.3669444444,
        "Challenge_answer_count":0,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\n\r\nSteps:\r\n1. Create a new AzureML workspace.\r\n    - Name: `azureml-test-workspace`\r\n    - Resource group: `recommenders_project_resources`\r\n    - Location: *Make sure you have enough quota in the location you choose*\r\n2. Create two new clusters: `cpu-cluster` and `gpu-cluster`. Go to compute, then compute cluster, then new.\r\n    - Select the CPU VM base. Anything above 32GB of RAM, and 8 cores should be fine.\r\n    - Select the GPU VM base. Anything above 56GB of RAM, and 6 cores, and an NVIDIA K80 should be fine.\r\n3. Add the subscription ID to GitHub action secrets [here](https:\/\/github.com\/microsoft\/recommenders\/settings\/secrets\/actions). Create a new repository secret called `AZUREML_TEST_SUBID` and add the subscription ID as the value.\r\n4. Make sure you have installed [Azure CLI](https:\/\/learn.microsoft.com\/en-us\/cli\/azure\/install-azure-cli), and that you are logged in: `az login`.\r\n5. Select your subscription: `az account set -s $AZURE_SUBSCRIPTION_ID`.\r\n5. Create a Service Principal: `az ad sp create-for-rbac --name \"CICD\" --role contributor --scopes \/subscriptions\/$AZURE_SUBSCRIPTION_ID --sdk-auth`.\r\n6. Add the output from the Service Principal (should be a JSON blob) as an action secret `AZUREML_TEST_CREDENTIALS`.\r\n\r\n\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":1669646142000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1669630421000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a bug in AzureML where the test process does not fail even if there is an error in the tests, making it difficult to identify errors. The user wants to receive a signal in GitHub when the tests fail so that the badge turns red and they are notified.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1862",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":8.1,
        "Challenge_reading_time":25.09,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":4.3669444444,
        "Challenge_title":"[BUG] Update test documentation to connect AzureML with GitHub actions",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":254,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Error:\r\n```\r\n$ az ad sp create-for-rbac --name \"CICD\" --role contributor --scopes \/subscriptions\/$AZURE_SUBSCRIPTION_ID. --sdk-auth\r\nThis command or command group has been migrated to Microsoft Graph API. Please carefully review all breaking changes introduced during this migration: https:\/\/docs.microsoft.com\/cli\/azure\/microsoft-graph-migration\r\nOption '--sdk-auth' has been deprecated and will be removed in a future release.\r\nAADSTS530003: Your device is required to be managed to access this resource.\r\nTrace ID: XXXXXXXXXXXXXXXXXXXXXXX\r\nCorrelation ID: XXXXXXXXXXXXXXXXXXXXXXX\r\nTimestamp: 2022-11-28 10:02:57Z\r\nTo re-authenticate, please run:\r\naz login --scope https:\/\/graph.microsoft.com\/\/.default\r\n```\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.8,
        "Solution_reading_time":9.09,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":77.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1221810788500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paderborn, North-Rhine-Westphalia, Germany",
        "Answerer_reputation_count":68522.0,
        "Answerer_view_count":7896.0,
        "Challenge_adjusted_solved_time":2.6447536111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I would like to create a databricks multi-task with following sequence:<\/p>\n<ul>\n<li>notebook task 1: train model with results logged to MLflow tracking server<\/li>\n<li>notebook task 2: use mlflow run_id from task 1 to register model in model registry<\/li>\n<\/ul>\n<p>Is it possible to pass run_id from task 1 to task 2 and if so is there any documentation on how this could be done?<\/p>",
        "Challenge_closed_time":1637341239876,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637331718763,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a multi-task job in Databricks where the first task trains a model and logs the results to MLflow tracking server, and the second task registers the model in the model registry using the MLflow run_id from the first task. The user is seeking information on how to pass the run_id from task 1 to task 2.",
        "Challenge_last_edit_time":1654760709848,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70036318",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":14.6,
        "Challenge_reading_time":5.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":2.6447536111,
        "Challenge_title":"Databricks multi-task jobs - pass MLflow run_id from one task to next task",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":198.0,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436447644056,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":818.0,
        "Poster_view_count":94.0,
        "Solution_body":"<p>As of <em>right now<\/em> (it may change), it's impossible to pass results between jobs if you use multi-task job.<\/p>\n<p>But you can call another notebook as a child job if you use <a href=\"https:\/\/docs.databricks.com\/notebooks\/notebook-workflows.html\" rel=\"nofollow noreferrer\">notebook workflows<\/a>  and function <code>dbutils.notebooks.run<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># notebook 1\n... training code ...\ndbutils.notebooks.run(&quot;notebook2&quot;, 300, {&quot;run_id&quot;: run_id})\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.1,
        "Solution_reading_time":7.07,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":52.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":162.8144655556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run a autoML model as follows:<\/p>\n<p>automl_settings = {  <br \/>\n&quot;n_cross_validations&quot;: 5  <br \/>\n}<\/p>\n<p>automl_config = AutoMLConfig(task = 'regression',  <br \/>\ncompute_target = compute_target,  <br \/>\ntraining_data = train_data.filter(train_data['location']==l),  <br \/>\nlabel_column_name = label,  <br \/>\n**automl_settings)<\/p>\n<p>remote_run = experiment.submit(automl_config, show_output=True)<\/p>\n<p>And I get: ValidationException: The data points should have at least 50 rows for a valid regression or classification task with cv 5.<\/p>\n<p>The data has more than 250 rows. Moreover, when I changed the cv number, nothing changed. Does anybody have any idea what might be happening?<\/p>",
        "Challenge_closed_time":1630572887363,
        "Challenge_comment_count":1,
        "Challenge_created_time":1629986755287,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a ValidationException while trying to run an autoML model for regression. The error message states that the data points should have at least 50 rows for a valid regression or classification task with cv 5. The user has more than 250 rows of data and changing the cv number did not resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/529189\/validationexception-the-data-points-should-have-at",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.4,
        "Challenge_reading_time":10.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":162.8144655556,
        "Challenge_title":"ValidationException: The data points should have at least 50 rows for a valid regression or classification task with cv 5.",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":100,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello,  <\/p>\n<p>Hope you have solved this issue. If you are still blocked by this, please feel free to let us know. We can either investigate deeper if we can have more details, or we can help you to enable a support ticket if you do not have a support plan. Thanks.  <\/p>\n<p>Regards,  <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.2,
        "Solution_reading_time":3.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":56.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2939025,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,  <\/p>\n<p>I'm aware that running <strong>compute Instance<\/strong> costs us for the number of hours we used. So, we stop it when ever we don't need it.  <\/p>\n<p>Similarly, does <strong>compute cluster<\/strong> &amp; <strong>Endpoints<\/strong> also costs us if we do not delete them after use?  <\/p>\n<p>Thanks  <br \/>\nBhaskar  <\/p>",
        "Challenge_closed_time":1617896011616,
        "Challenge_comment_count":1,
        "Challenge_created_time":1617894953567,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is inquiring whether compute clusters and endpoints will continue to incur costs if they are not deleted after use, similar to how compute instances do.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/349617\/does-compute-cluster-endpoints-costs-if-we-dont-de",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.7,
        "Challenge_reading_time":4.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.2939025,
        "Challenge_title":"Does compute cluster & Endpoints costs if we dont delete after use ?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":61,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=7c0d4f65-2f38-4fdd-91df-9f513e0321c1\">@Bhaskar11  <\/a>     <\/p>\n<p>When a <strong>compute cluster is idle<\/strong>, it autoscales to 0 nodes, so you don't pay when it's not in use. A compute instance is always on and doesn't autoscale. You should stop the compute instance when you aren't using it to avoid extra cost.    <br \/>\n<strong>refer -<\/strong> <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-target\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-target<\/a>    <\/p>\n<p>If the Answer is helpful, please click <code>Accept Answer<\/code> and <strong>up-vote<\/strong>, this can be beneficial to other community members.    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.7,
        "Solution_reading_time":9.36,
        "Solution_score_count":9.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":75.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1512770138847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":22.3412477778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to use a local training job in SageMaker.<\/p>\n<p>Following this AWS notebook (<a href=\"http:\/\/mxnet_mnist_with_gluon_local_mode.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mxnet_mnist_with_gluon_local_mode.ipynb<\/a>) I was able to train and predict locally.<\/p>\n<p>There is any way to train locally and save the trained model in the Amazon SageMaker Training Job section?\nOtherwise, how can I properly save trained models I trained using local mode?<\/p>",
        "Challenge_closed_time":1596039571220,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595954217667,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to use a local training job in Amazon SageMaker and has successfully trained and predicted locally using an AWS notebook. However, the user is facing challenges in saving the trained model in the Amazon SageMaker Training Job section and is seeking guidance on how to properly save trained models trained using local mode.",
        "Challenge_last_edit_time":1595959142728,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63138835",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":12.9,
        "Challenge_reading_time":8.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":23.7093202778,
        "Challenge_title":"How to save models trained locally in Amazon SageMaker?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3195.0,
        "Challenge_word_count":66,
        "Platform":"Stack Overflow",
        "Poster_created_time":1464391892936,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Rio de Janeiro, State of Rio de Janeiro, Brazil",
        "Poster_reputation_count":2243.0,
        "Poster_view_count":148.0,
        "Solution_body":"<p>There is no way to have your local mode training jobs appear in the AWS console. The intent of local mode is to allow for faster iteration\/debugging before using SageMaker for training your model.<\/p>\n<p>You can create SageMaker Models from local model artifacts. Compress your model artifacts into a <code>.tar.gz<\/code> file, upload that file to S3, and then create the Model (with the SDK or in the console).<\/p>\n<p>Documentation:<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":18.5,
        "Solution_reading_time":12.35,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":79.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":1.8544463889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I had to stop training in the middle, which set the Trains status to <code>Aborted<\/code>.\nLater I continued it from the last checkpoint, but the status remained <code>Aborted<\/code>.\nFurthermore, automatic training metrics stopped appearing in the dashboard (though custom metrics still do).<\/p>\n<p>Can I reset the status back to <code>Running<\/code> and make Trains log training stats again?<\/p>\n<p><strong>Edit:<\/strong> When continuing training, I retrieved the task using <code>Task.get_task()<\/code> and not <code>Task.init()<\/code>. Maybe that's why training stats are not updated anymore?<\/p>\n<p><strong>Edit2:<\/strong> I also tried <code>Task.init(reuse_last_task_id=original_task_id_string)<\/code>, but it just creates a new task, and doesn't reuse the given task ID.<\/p>",
        "Challenge_closed_time":1593515579540,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593508903533,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user had to stop training in the middle, which set the Trains status to 'Aborted'. Later, when the user continued training from the last checkpoint, the status remained 'Aborted'. Additionally, automatic training metrics stopped appearing in the dashboard, though custom metrics still do. The user is looking for a way to reset the status back to 'Running' and make Trains log training stats again. The user tried using Task.get_task() and Task.init() but was unsuccessful.",
        "Challenge_last_edit_time":1609467668432,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62654203",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":11.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1.8544463889,
        "Challenge_title":"Trains: Can I reset the status of a task? (from 'Aborted' back to 'Running')",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":228.0,
        "Challenge_word_count":111,
        "Platform":"Stack Overflow",
        "Poster_created_time":1311330349880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tel Aviv",
        "Poster_reputation_count":3784.0,
        "Poster_view_count":342.0,
        "Solution_body":"<p>Disclaimer: I'm a member of Allegro Trains team<\/p>\n<blockquote>\n<p>When continuing training, I retrieved the task using Task.get_task() and not Task.init(). Maybe that's why training stats are not updated anymore?<\/p>\n<\/blockquote>\n<p>Yes that's the only way to continue the same exact Task.\nYou can also mark it as started with <code>task.mark_started()<\/code> , that said the automatic logging will not kick in, as <code>Task.get_task<\/code> is usually used for accessing previously executed tasks and not continuing it (if you think the continue use case is important please feel free to open a GitHub issue, I can definitely see the value there)<\/p>\n<p>You can also do something a bit different, and justcreate a new Task continuing from the last iteration the previous run ended. Notice that if you load the weights file (PyTorch\/TF\/Keras\/JobLib) it will automatically connect it with the model that was created in the previous run (assuming the model was stored is the same location, or if you have the model on https\/S3\/Gs\/Azure and you are using <code>trains.StorageManager.get_local_copy()<\/code>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>previous_run = Task.get_task()\ntask = Task.init('examples', 'continue training')\ntask.set_initial_iteration(previous_run.get_last_iteration())\ntorch.load('\/tmp\/my_previous_weights')\n<\/code><\/pre>\n<p>BTW:<\/p>\n<blockquote>\n<p>I also tried Task.init(reuse_last_task_id=original_task_id_string), but it just creates a new task, and doesn't reuse the given task ID.<\/p>\n<\/blockquote>\n<p>This is a great idea for an interface to continue a previous run, feel free to add it as GitHub issue.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1593557456892,
        "Solution_link_count":0.0,
        "Solution_readability":10.6,
        "Solution_reading_time":21.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":219.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":8.0327202778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Using Amazon Sagemaker, I created an Xgboost model. After unpacking the resulting tar.gz file, I end up with a file \"xgboost-model\". <\/p>\n\n<p>The next step will be to upload the model directly from my S3 bucket, without downloading it using <em>pickle<\/em>. Here is what I tried:<\/p>\n\n<pre><code>obj = client.get_object(Bucket='...',Key='xgboost-model')\n\nxgb_model = pkl.load(open((obj['Body'].read())),\"rb\")\n<\/code><\/pre>\n\n<p>But it throws me the error:<\/p>\n\n<pre><code>TypeError: embedded NUL character\n<\/code><\/pre>\n\n<p>Also tried this:<\/p>\n\n<pre><code>xgb_model = pkl.loads(open((obj['Body'].read())),\"rb\")\n<\/code><\/pre>\n\n<p>the outcome was the same.<\/p>\n\n<p>Another approach:<\/p>\n\n<pre><code>bucket='...'\nkey='xgboost-model'\n\nwith s3io.open('s3:\/\/{0}\/{1}'.format(bucket, key),mode='w') as s3_file:\n  pkl.dump(mdl, s3_file)\n<\/code><\/pre>\n\n<p>This giving the error:<\/p>\n\n<pre><code>CertificateError: hostname bucket doesn't match either of '*.s3.amazonaws.com', 's3.amazonaws.com'\n<\/code><\/pre>\n\n<p>This although the bucket is the same.<\/p>\n\n<p>How Can I upload the model in a pickle object so I can then use it it for predictions?<\/p>",
        "Challenge_closed_time":1531429065220,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531399214863,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user created an Xgboost model using Amazon Sagemaker and wants to upload it directly from their S3 bucket without using pickle. However, they encountered errors when trying to load the model using pkl.load and pkl.loads, and also encountered a CertificateError when trying to dump the model using pkl.dump. The user is seeking a solution to upload the model in a pickle object for predictions.",
        "Challenge_last_edit_time":1531400147427,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51305956",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.2,
        "Challenge_reading_time":15.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":8.2917658333,
        "Challenge_title":"S3 read Sagemaker trained model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2742.0,
        "Challenge_word_count":129,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432680790120,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":455.0,
        "Poster_view_count":58.0,
        "Solution_body":"<p>My assumption is you have trained the model using Sagemaker XGBoost built-in algorithm. You would like to use that model and do the predictions in your own hosting environment (not Sagemaker hosting).<\/p>\n\n<p><code>pickle.load(file)<\/code> reads a pickled object from the open file object file and <code>pickle.loads(bytes_object)<\/code> reads a pickled object from a bytes object and returns the deserialized object. Since you have the S3 object already downloaded (into memory) as bytes, you can use <code>pickle.loads<\/code> without using <code>open<\/code><\/p>\n\n<pre><code>xgb_model = pkl.loads(obj['Body'].read())\n<\/code><\/pre>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.8,
        "Solution_reading_time":8.11,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1613062428296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":141.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":1.09315,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>We are struggling to model our data correctly for use in Kedro - we are using the recommended Raw\\Int\\Prm\\Ft\\Mst model but are struggling with some of the concepts....e.g.<\/p>\n<ul>\n<li>When is a dataset a feature rather than a primary dataset? The distinction seems vague...<\/li>\n<li>Is it OK for a primary dataset to consume data from another primary dataset?<\/li>\n<li>Is it good practice to build a feature dataset from the INT layer? or should it always pass through Primary?<\/li>\n<\/ul>\n<p>I appreciate there are no hard &amp; fast rules with data modelling but these are big modelling decisions &amp; any guidance or best practice on Kedro modelling would be really helpful, I can find just one table defining the layers in the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/12_faq\/01_faq.html#what-is-data-engineering-convention\" rel=\"nofollow noreferrer\">Kedro docs<\/a><\/p>\n<p>If anyone can offer any further advice or blogs\\docs talking about Kedro Data Modelling that would be awesome!<\/p>",
        "Challenge_closed_time":1623349806340,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623345871000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is struggling with data modeling for use in Kedro and has questions about the Raw\\Int\\Prm\\Ft\\Mst model, including when a dataset is a feature rather than a primary dataset, whether it's okay for a primary dataset to consume data from another primary dataset, and whether it's good practice to build a feature dataset from the INT layer. The user is seeking guidance and best practices for Kedro modeling.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67925860",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.7,
        "Challenge_reading_time":12.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":1.09315,
        "Challenge_title":"Kedro Data Modelling",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":180.0,
        "Challenge_word_count":145,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369054667740,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United Kingdom",
        "Poster_reputation_count":1445.0,
        "Poster_view_count":104.0,
        "Solution_body":"<p>Great question. As you say, there are no hard and fast rules here and opinions do vary, but let me share my perspective as a QB data scientist and kedro maintainer who has used the layering convention you referred to several times.<\/p>\n<p>For a start, let me emphasise that there's absolutely no reason to stick to the data engineering convention suggested by kedro if it's not suitable for your needs. 99% of users don't change the folder structure in <code>data<\/code>. This is not because the kedro default is the right structure for them but because they just don't think of changing it. You should absolutely add\/remove\/rename layers to suit yourself. The most important thing is to choose a set of layers (or even a non-layered structure) that works for your project rather than trying to shoehorn your datasets to fit the kedro default suggestion.<\/p>\n<p>Now, assuming you are following kedro's suggested structure - onto your questions:<\/p>\n<blockquote>\n<p>When is a dataset a feature rather than a primary dataset? The distinction seems vague...<\/p>\n<\/blockquote>\n<p>In the case of simple features, a feature dataset can be very similar to a primary one. The distinction is maybe clearest if you think about more complex features, e.g. formed by aggregating over time windows. A primary dataset would have a column that gives a cleaned version of the original data, but without doing any complex calculations on it, just simple transformations. Say the raw data is the colour of all cars driving past your house over a week. By the time the data is in primary, it will be clean (e.g. correcting &quot;rde&quot; to &quot;red&quot;, maybe mapping &quot;crimson&quot; and &quot;red&quot; to the same colour). Between primary and the feature layer, we will have done some less trivial calculations on it, e.g. to find one-hot encoded most common car colour each day.<\/p>\n<blockquote>\n<p>Is it OK for a primary dataset to consume data from another primary dataset?<\/p>\n<\/blockquote>\n<p>In my opinion, yes. This might be necessary if you want to join multiple primary tables together. In general if you are building complex pipelines it will become very difficult if you don't allow this. e.g. in the feature layer I might want to form a dataset containing <code>composite_feature = feature_1 * feature_2<\/code> from the two inputs <code>feature_1<\/code> and <code>feature_2<\/code>. There's no way of doing this without having multiple sub-layers within the feature layer.<\/p>\n<p>However, something that is generally worth avoiding is a node that consumes data from many different layers. e.g. a node that takes in one dataset from the feature layer and one from the intermediate layer. This seems a bit strange (why has the latter dataset not passed through the feature layer?).<\/p>\n<blockquote>\n<p>Is it good practice to build a feature dataset from the INT layer? or should it always pass through Primary?<\/p>\n<\/blockquote>\n<p>Building features from the intermediate layer isn't unheard of, but it seems a bit weird. The primary layer is typically an important one which forms the basis for all feature engineering. If your data is in a shape that you can build features then that means it's probably primary layer already. In this case, maybe you don't need an intermediate layer.<\/p>\n<p>The above points might be summarised by the following rules (which should no doubt be broken when required):<\/p>\n<ol>\n<li>The input datasets for a node in layer <code>L<\/code> should all be in the same layer, which can be either <code>L<\/code> or <code>L-1<\/code><\/li>\n<li>The output datasets for a node in layer <code>L<\/code> should all be in the same layer <code>L<\/code>, which can be either <code>L<\/code> or <code>L+1<\/code><\/li>\n<\/ol>\n<blockquote>\n<p>If anyone can offer any further advice or blogs\\docs talking about Kedro Data Modelling that would be awesome!<\/p>\n<\/blockquote>\n<p>I'm also interested in seeing what others think here! One possibly useful thing to note is that kedro was inspired by cookiecutter data science, and the kedro layer structure is an extended version of <a href=\"http:\/\/drivendata.github.io\/cookiecutter-data-science\/#directory-structure\" rel=\"nofollow noreferrer\">what's suggested there<\/a>. Maybe other projects have taken this directory structure and adapted it in different ways.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.2,
        "Solution_reading_time":53.65,
        "Solution_score_count":4.0,
        "Solution_sentence_count":37.0,
        "Solution_word_count":668.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4538275,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>What are the learning paths in MS Learn for Data Science, Machine Learning, and Deep Learning with Python as the base programming language? How to get the Microsoft Certification.<\/p>",
        "Challenge_closed_time":1684304707872,
        "Challenge_comment_count":0,
        "Challenge_created_time":1684303074093,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information on the learning paths available in MS Learn for Data Science, Machine Learning, and Deep Learning with Python as the base programming language. They also want to know how to obtain Microsoft Certification.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1286478\/what-is-the-learning-path-to-became-a-data-scienti",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":2.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.4538275,
        "Challenge_title":"What is the Learning path to Became a data scientist?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":38,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The certification path for Data Scientist includes 6 exams. <a href=\"https:\/\/learn.microsoft.com\/en-us\/certifications\/browse\/?roles=data-scientist\">https:\/\/learn.microsoft.com\/en-us\/certifications\/browse\/?roles=data-scientist<\/a><\/p>\n<p>Of these 6, the core exam is DP-100, passing it will earn you <strong>Microsoft Certified: Azure Data Scientist Associate<\/strong>.<\/p>\n<p>The DP-100 exam page features the Learning path collection with all of the modules to prepare for the exam; <a href=\"https:\/\/learn.microsoft.com\/en-us\/certifications\/azure-data-scientist\/\">https:\/\/learn.microsoft.com\/en-us\/certifications\/azure-data-scientist\/<\/a><\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.7,
        "Solution_reading_time":8.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":51.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.5907397222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I\u2019m performing a series of experiments with AutoML and I need to see the featurized data. I mean, not just the new features names retrieved by method get_engineered_feature_names() or the featurization details retrieved by get_featurization_summary(), I refer to the whole transformed dataset, the one obtained after scaling\/normalization\/featurization that is then used to train the models.   <\/p>\n<p>Is it possible to access to this dataset or download it as a file?  <\/p>\n<p>Thanks.  <\/p>",
        "Challenge_closed_time":1618340955136,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618306428473,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is conducting experiments with AutoML and needs to access the featurized dataset, including the transformed data obtained after scaling, normalization, and featurization, which is used to train the models. They are seeking information on whether it is possible to access or download this dataset as a file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/355323\/how-to-access-to-the-featurized-dataset-in-automat",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.3,
        "Challenge_reading_time":6.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":9.5907397222,
        "Challenge_title":"How to access to the featurized dataset in Automated ML",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":82,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, currently, we don't store the dataset from scaling\/normalization\/featurization after the run is complete. This feature isn't supported at this time. Sorry for the inconvenience.<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.1,
        "Solution_reading_time":2.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1478712810887,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Belgium",
        "Answerer_reputation_count":11153.0,
        "Answerer_view_count":888.0,
        "Challenge_adjusted_solved_time":5.928925,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've run a simple two-class neural network where I got this result in the end (eval):<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/vw2jQ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vw2jQ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I think I am going to be happy with the <code>True Positive<\/code> and <code>False Negative<\/code> results. But what does <code>False Positive<\/code> mean? <code>False Positive<\/code> means it did not correctly classify 2002 elements and missed them?<\/p>\n\n<p>The <code>Accuracy<\/code> is 66%, that's really bad right? Whats the difference between that and <code>AUC<\/code>?<\/p>\n\n<p><code>Precision<\/code> suffers because Accuracy also is bad (I hoping for a 80%+)?<\/p>\n\n<p>And how do I flip <code>Positive Label<\/code> and <code>Negative Label<\/code>? I really want to predict the classification where the target is to find <code>CANDIDATE<\/code><\/p>",
        "Challenge_closed_time":1488062187216,
        "Challenge_comment_count":0,
        "Challenge_created_time":1488042320197,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has run a two-class neural network and is trying to understand the classification results. They are unsure about the meaning of False Positive and the difference between Accuracy and AUC. They are also concerned about the low accuracy and how it affects Precision. The user wants to know how to flip Positive and Negative Labels to predict the classification for finding CANDIDATE.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42458982",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":12.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":5.5186163889,
        "Challenge_title":"Understanding classification results",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":81.0,
        "Challenge_word_count":113,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267709938320,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Norway",
        "Poster_reputation_count":13032.0,
        "Poster_view_count":1004.0,
        "Solution_body":"<p>Basically, for the false\/true positives and false\/true negatives :\nYou have detected almost all the CANDIDATE samples in your dataset, 3420 of them were correctly predicted as TRUE and 31 of them were predicted as FALSE. This information is captured in the Recall ratio : 3420\/(3420+31) = 99.1%. It is very high, so very good. <\/p>\n\n<p>However, you have predicted <strong>too many<\/strong> CANDIDATE. Indeed, in all the TRUE values predicted by the model, 3420 were actually TRUE and 2002 were actually FALSE. This makes the Precision ratio bad : 3420\/(3420+2002)=63.1%. Which is not that good. <\/p>\n\n<p>F1 is a combinaison between Precision and Recall, it summarizes them into one value, some kind of weighted average. The formula is 2*(P*R)\/(P+R). So if one of Precision or Recall is bad : the F1score will capture it. <\/p>\n\n<p>You can see that you have a total of 5999 examples in your data set. Out of those, 3451 are really TRUE and 2548 are really FALSE. So you have 57% of your data that is TRUE. If you make a really stupid classifier that classifies everything as TRUE whatever the features are, then you will get 57% accuracy. Given that, 66.1% accuracy is not really good. \nIf you look at the second column of that table, you only predict 577 FALSE out of the 5999 samples. Your classifier is heavily biased towards TRUE predictions. <\/p>\n\n<p>For the AUC, it stands for Area Under the Curve. You can read <a href=\"http:\/\/fastml.com\/what-you-wanted-to-know-about-auc\/\" rel=\"nofollow noreferrer\">more detailed info about it here<\/a>. To summarize : when you predic a value, you don't really get True or False directly. You get a real number between 0 (False) and 1 (True). The way to classify a predicted value, say 0.2, is to use a Threshold. The threshold is by default set to 0.5. So if you predict 0.2, your model will predict to classify it as a False because 0.2&lt;0.5. But you could make that treshold move between 0 and 1. If the classifier is really good, if it discriminates really well the Falses and Trues predictions, then the AUC will be close to 1. If it's really bad, it will be close to 0.5. Refer to the link if you need more information. <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1488063664327,
        "Solution_link_count":1.0,
        "Solution_readability":6.6,
        "Solution_reading_time":26.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":32.0,
        "Solution_word_count":368.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1597.1266666667,
        "Challenge_answer_count":0,
        "Challenge_body":"## \u2753 Questions and Help\r\n\r\n#### What is your question?\r\nTrying to log model into mlflow using `mlflow.pytorch.log_model` in train end. Getting the above error only in multi gpu scenario. \r\n\r\n#### Code\r\n\r\n\r\nmnist script file - \r\n\r\n```\r\nimport pytorch_lightning as pl\r\nimport torch\r\nfrom argparse import ArgumentParser\r\n#from mlflow.pytorch.pytorch_autolog import __MLflowPLCallback\r\nfrom pytorch_lightning.logging import MLFlowLogger\r\nfrom sklearn.metrics import accuracy_score\r\nfrom torch.nn import functional as F\r\nfrom torch.utils.data import DataLoader, random_split\r\nfrom torchvision import datasets, transforms\r\n\r\n\r\nclass LightningMNISTClassifier(pl.LightningModule):\r\n    def __init__(self):\r\n        \"\"\"\r\n        Initializes the network\r\n        \"\"\"\r\n        super(LightningMNISTClassifier, self).__init__()\r\n\r\n        # mnist images are (1, 28, 28) (channels, width, height)\r\n        self.layer_1 = torch.nn.Linear(28 * 28, 128)\r\n        self.layer_2 = torch.nn.Linear(128, 256)\r\n        self.layer_3 = torch.nn.Linear(256, 10)\r\n\r\n        # transforms for images\r\n        self.transform = transforms.Compose(\r\n            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\r\n        )\r\n\r\n    @staticmethod\r\n    def add_model_specific_args(parent_parser):\r\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\r\n        parser.add_argument(\r\n            \"--batch-size\",\r\n            type=int,\r\n            default=64,\r\n            metavar=\"N\",\r\n            help=\"input batch size for training (default: 64)\",\r\n        )\r\n        parser.add_argument(\r\n            \"--num-workers\",\r\n            type=int,\r\n            default=0,\r\n            metavar=\"N\",\r\n            help=\"number of workers (default: 0)\",\r\n        )\r\n        parser.add_argument(\r\n            \"--lr\",\r\n            type=float,\r\n            default=1e-3,\r\n            metavar=\"LR\",\r\n            help=\"learning rate (default: 1e-3)\",\r\n        )\r\n        return parser\r\n\r\n    def forward(self, x):\r\n        \"\"\"\r\n        Forward Function\r\n        \"\"\"\r\n        batch_size, channels, width, height = x.size()\r\n\r\n        # (b, 1, 28, 28) -> (b, 1*28*28)\r\n        x = x.view(batch_size, -1)\r\n\r\n        # layer 1 (b, 1*28*28) -> (b, 128)\r\n        x = self.layer_1(x)\r\n        x = torch.relu(x)\r\n\r\n        # layer 2 (b, 128) -> (b, 256)\r\n        x = self.layer_2(x)\r\n        x = torch.relu(x)\r\n\r\n        # layer 3 (b, 256) -> (b, 10)\r\n        x = self.layer_3(x)\r\n\r\n        # probability distribution over labels\r\n        x = torch.log_softmax(x, dim=1)\r\n\r\n        return x\r\n\r\n    def cross_entropy_loss(self, logits, labels):\r\n        \"\"\"\r\n        Loss Fn to compute loss\r\n        \"\"\"\r\n        return F.nll_loss(logits, labels)\r\n\r\n    def training_step(self, train_batch, batch_idx):\r\n        \"\"\"\r\n        training the data as batches and returns training loss on each batch\r\n        \"\"\"\r\n        x, y = train_batch\r\n        logits = self.forward(x)\r\n        loss = self.cross_entropy_loss(logits, y)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, val_batch, batch_idx):\r\n        \"\"\"\r\n        Performs validation of data in batches\r\n        \"\"\"\r\n        x, y = val_batch\r\n        logits = self.forward(x)\r\n        loss = self.cross_entropy_loss(logits, y)\r\n        return {\"val_loss\": loss}\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        \"\"\"\r\n        Computes average validation accuracy\r\n        \"\"\"\r\n        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\r\n        tensorboard_logs = {\"val_loss\": avg_loss}\r\n        return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs}\r\n\r\n    def test_step(self, test_batch, batch_idx):\r\n        \"\"\"\r\n        Performs test and computes test accuracy\r\n        \"\"\"\r\n        x, y = test_batch\r\n        output = self.forward(x)\r\n        a, y_hat = torch.max(output, dim=1)\r\n        test_acc = accuracy_score(y_hat.cpu(), y.cpu())\r\n        return {\"test_acc\": torch.tensor(test_acc)}\r\n\r\n    def test_epoch_end(self, outputs):\r\n        \"\"\"\r\n        Computes average test accuracy score\r\n        \"\"\"\r\n        avg_test_acc = torch.stack([x[\"test_acc\"] for x in outputs]).mean()\r\n        return {\"avg_test_acc\": avg_test_acc}\r\n\r\n    def prepare_data(self):\r\n        \"\"\"\r\n        Preprocess the input data.\r\n        \"\"\"\r\n        return {}\r\n\r\n    def train_dataloader(self):\r\n        \"\"\"\r\n        Loading training data as batches\r\n        \"\"\"\r\n        mnist_train = datasets.MNIST(\r\n            \"dataset\", download=True, train=True, transform=self.transform\r\n        )\r\n        return DataLoader(\r\n            mnist_train,\r\n            batch_size=64,\r\n            num_workers=1\r\n        )\r\n\r\n    def val_dataloader(self):\r\n        \"\"\"\r\n        Loading validation data as batches\r\n        \"\"\"\r\n        mnist_train = datasets.MNIST(\r\n            \"dataset\", download=True, train=True, transform=self.transform\r\n        )\r\n        mnist_train, mnist_val = random_split(mnist_train, [55000, 5000])\r\n\r\n        return DataLoader(\r\n            mnist_val,\r\n            batch_size=64,\r\n            num_workers=1\r\n        )\r\n\r\n    def test_dataloader(self):\r\n        \"\"\"\r\n        Loading test data as batches\r\n        \"\"\"\r\n        mnist_test = datasets.MNIST(\r\n            \"dataset\", download=True, train=False, transform=self.transform\r\n        )\r\n        return DataLoader(\r\n            mnist_test,\r\n            batch_size=64,\r\n            num_workers=1\r\n        )\r\n\r\n    def configure_optimizers(self):\r\n        \"\"\"\r\n        Creates and returns Optimizer\r\n        \"\"\"\r\n        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n        self.scheduler = {\r\n            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\r\n                self.optimizer,\r\n                mode=\"min\",\r\n                factor=0.2,\r\n                patience=2,\r\n                min_lr=1e-6,\r\n                verbose=True,\r\n            )\r\n        }\r\n        return [self.optimizer], [self.scheduler]\r\n\r\n    def optimizer_step(\r\n        self,\r\n        epoch,\r\n        batch_idx,\r\n        optimizer,\r\n        optimizer_idx,\r\n        second_order_closure=None,\r\n        on_tpu=False,\r\n        using_lbfgs=False,\r\n        using_native_amp=False,\r\n    ):\r\n        self.optimizer.step()\r\n        self.optimizer.zero_grad()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    from pytorch_autolog import autolog\r\n    autolog()\r\n    model = LightningMNISTClassifier()\r\n    mlflow_logger = MLFlowLogger(\r\n        experiment_name=\"Default\", tracking_uri=\"http:\/\/localhost:5000\/\"\r\n    )\r\n    trainer = pl.Trainer(\r\n        logger=mlflow_logger,\r\n        gpus=2,\r\n        distributed_backend=\"ddp\",\r\n        max_epochs=1\r\n    )\r\n    trainer.fit(model)\r\n    trainer.test()\r\n\r\n```\r\n\r\nSample code from autolog - Callback class. \r\n\r\n```\r\n    class __MLflowPLCallback(pl.Callback):\r\n\r\n        def __init__(self):\r\n            super().__init__()\r\n\r\n        def on_train_end(self, trainer, pl_module):\r\n            \"\"\"\r\n            Logs the model checkpoint into mlflow - models folder on the training end\r\n            \"\"\"\r\n\r\n            mlflow.set_tracking_uri(trainer.logger._tracking_uri )\r\n            mlflow.set_experiment(trainer.logger._experiment_name)\r\n            mlflow.start_run(trainer.logger.run_id)\r\n            mlflow.pytorch.log_model(trainer.model, \"models\")\r\n            mlflow.end_run()\r\n\r\n\r\n```\r\n\r\n\r\n\r\n\r\nStack Trace\r\n\r\n```\r\nTraceback (most recent call last):                                                                                                                                                                          \r\n  File \"mnist.py\", line 231, in <module>\r\n    trainer.fit(model)\r\n  File \"\/home\/ubuntu\/mnist\/pytorch_autolog.py\", line 218, in fit\r\n    return _run_and_log_function(self, original, args, kwargs)\r\n  File \"\/home\/ubuntu\/mnist\/pytorch_autolog.py\", line 209, in _run_and_log_function\r\n    result = original(self, *args, **kwargs)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 992, in fit\r\n    results = self.spawn_ddp_children(model)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/distrib_data_parallel.py\", line 462, in spawn_ddp_children\r\n    results = self.ddp_train(local_rank, q=None, model=model, is_master=True)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/distrib_data_parallel.py\", line 560, in ddp_train\r\n    results = self.run_pretrain_routine(model)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1213, in run_pretrain_routine\r\n    self.train()\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py\", line 392, in train\r\n    self.run_training_teardown()\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py\", line 872, in run_training_teardown\r\n    self.on_train_end()\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/callback_hook.py\", line 72, in on_train_end\r\n    callback.on_train_end(self, self.get_model())\r\n  File \"\/home\/ubuntu\/mnist\/pytorch_autolog.py\", line 120, in on_train_end\r\n    mlflow.pytorch.log_model(trainer.model, \"models\")\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/mlflow\/pytorch\/__init__.py\", line 179, in log_model\r\n    signature=signature, input_example=input_example, **kwargs)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/mlflow\/models\/model.py\", line 154, in log\r\n    **kwargs)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/mlflow\/pytorch\/__init__.py\", line 300, in save_model\r\n    torch.save(pytorch_model, model_path, pickle_module=pickle_module, **kwargs)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/torch\/serialization.py\", line 370, in save\r\n    _legacy_save(obj, opened_file, pickle_module, pickle_protocol)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/torch\/serialization.py\", line 443, in _legacy_save\r\n    pickler.dump(obj)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/cloudpickle\/cloudpickle.py\", line 491, in dump\r\n    return Pickler.dump(self, obj)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 437, in dump\r\n    self.save(obj)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 659, in save_reduce\r\n    self._batch_setitems(dictitems)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 890, in _batch_setitems\r\n    save(v)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 846, in _batch_appends\r\n    save(tmp[0])\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 524, in save\r\n    rv = reduce(self.proto)\r\nTypeError: can't pickle _thread.lock objects\r\n\r\n\r\n```\r\n\r\n\r\n\r\n#### What have you tried?\r\nTried out the possibilities mentioned in the similar thread - https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/2186\r\n\r\nTried  wrapping the code inside, `trainer.is_global_zero`  . And also tried `trainer.global_rank == 0`. Also tried decorating the method as `@rank_zero_only`. But no luck. Getting the same error. \r\n\r\n#### What's your environment?\r\n\r\n - OS: Ubuntu\r\n - Packaging - torch, pytorch-lightning, torchvision, mlflow",
        "Challenge_closed_time":1605489870000,
        "Challenge_comment_count":15,
        "Challenge_created_time":1599740214000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is using mlflow instead of tensorboard as a logger and is facing an issue where the checkpoints are being saved in the wrong location. The checkpoints should be in the `\\mlflow` folder, but they are being saved in the `\\1\\\\{guid}\\checkpoints` folder. The user is unsure if this is an mlflow or pytorch-lightning issue and is using pytorch-lightning 0.8.5 on macos running in python 3.7.6.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3444",
        "Challenge_link_count":2,
        "Challenge_participation_count":15,
        "Challenge_readability":14.2,
        "Challenge_reading_time":152.56,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":140,
        "Challenge_solved_time":1597.1266666667,
        "Challenge_title":"TypeError: can't pickle _thread.lock objects - Error while logging model into mlflow in multi gpu scenario",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":935,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi! thanks for your contribution!, great first issue! What happens if you don't use the scheduler? Please try commenting out the scheduler definition and return only the optimizer. > What happens if you don't use the scheduler? Please try commenting out the scheduler definition and return only the optimizer.\r\n\r\nThanks for the reply. Sure i will try and update here. > What happens if you don't use the scheduler? Please try commenting out the scheduler definition and return only the optimizer.\r\n\r\nI removed the scheduler part and re-ran the script. Still experiencing the same error.  @lucadiliello @williamFalcon Any suggestions here ? One more observation from my end. In multi gpu scenario when i save the model using `torch.save(trainer.model, PATH)` i get the above mentioned error . However, when i try to save the save dict `torch.save(trainer.model.state_dict(), PATH)` the state dict is successfully getting saved. \r\n\r\nTested with 2 gpus and 0.9 version of pytorch lightning.  This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n I have the same error. Training with a single gpu works fine but with multiple the error is raised > I have the same error. Training with a single gpu works fine but with multiple the error is raised\r\n\r\nTraining with single GPU is fine because there is no need to create multiple processes, so you model is not pickled. What happens if you simply load your model in a shell and try to pickle it?\r\nSomething like:\r\n```python\r\n>>> model = MyModel(...)\r\n>>>\r\n>>> import pickle\r\n>>> pickle.dump(model, \"tmp_file.pk\")\r\n```\r\n\r\nIn my little experience, most of the pickle errors are caused by lambda functions or non-global functions. See [here](https:\/\/docs.python.org\/3\/library\/pickle.html#what-can-be-pickled-and-unpickled) the list of what can be pickled. > One more observation from my end. In multi gpu scenario when i save the model using `torch.save(trainer.model, PATH)` i get the above mentioned error . However, when i try to save the save dict `torch.save(trainer.model.state_dict(), PATH)` the state dict is successfully getting saved.\r\n> \r\n> Tested with 2 gpus and 0.9 version of pytorch lightning.\r\n\r\nPlease update to the latest version and let us know whether the error persist. > > I have the same error. Training with a single gpu works fine but with multiple the error is raised\r\n> \r\n> Training with single GPU is fine because there is no need to create multiple processes, so you model is not pickled. What happens if you simply load your model in a shell and try to pickle it?\r\n> Something like:\r\n> \r\n> ```python\r\n> >>> model = MyModel(...)\r\n> >>>\r\n> >>> import pickle\r\n> >>> pickle.dump(model, \"tmp_file.pk\")\r\n> ```\r\n> \r\n> In my little experience, most of the pickle errors are caused by lambda functions or non-global functions. See [here](https:\/\/docs.python.org\/3\/library\/pickle.html#what-can-be-pickled-and-unpickled) the list of what can be pickled.\r\n\r\nI tried adding as suggested:\r\n```\r\n>>> import pickle\r\n>>> pickle.dump(model, \"tmp_file.pk\")\r\n```\r\nHowever, this raise the error: TypeError: file must have a 'write' attribute\r\nI then tried with:\r\n```\r\n>>> import pickle\r\n>>> with open(\"tmp_file.pk\",\"wb\") as f:\r\n>>>         pickle.dump(model, f)\r\n```\r\nThis then again raised TypeError: can't pickle _thread.lock objects\r\nI am on the latest version of pytorch lightning This suggests that the problem is in your model. Can you post it here?\n\nA complete log of the error would also be useful. Thanks I ran into this as well. I wrote a little function to iterate through the model.__dict__.items() and check which caused pickle errors. It looks like there's a model attribute pointing to the trainer, which has that _thread.lock on it. Maybe there's a step that's supposed to clean this up that's being missed somehow? My quick work-around was to `delattr(model, \"trainer\")` before pickling the model, but I haven't actually tried loading the model again, so I this could cause other problems. same problem same problem on 1.2.4, works fine with 1.1.0. any ideas what might be causing the difference between the versions?",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.2,
        "Solution_reading_time":51.46,
        "Solution_score_count":null,
        "Solution_sentence_count":63.0,
        "Solution_word_count":630.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1357263005087,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Granada Hills, Los Angeles, CA, United States",
        "Answerer_reputation_count":6262.0,
        "Answerer_view_count":428.0,
        "Challenge_adjusted_solved_time":5.9137519445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm on a Jupyter notebook using Python3 and trying to plot a tree with code like this:<\/p>\n\n<pre><code>import xgboost as xgb\nfrom xgboost import plot_tree\n\nplot_tree(model, num_trees=4)\n<\/code><\/pre>\n\n<p>On the last line I get:<\/p>\n\n<pre><code>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/xgboost\/plotting.py in to_graphviz(booster, fmap, num_trees, rankdir, yes_color, no_color, **kwargs)\n196         from graphviz import Digraph\n197     except ImportError:\n--&gt; 198         raise ImportError('You must install graphviz to plot tree')\n199 \n200     if not isinstance(booster, (Booster, XGBModel)):\n\nImportError: You must install graphviz to plot tree\n<\/code><\/pre>\n\n<p>How do I install graphviz so I can see the plot_tree?<\/p>",
        "Challenge_closed_time":1552366926347,
        "Challenge_comment_count":0,
        "Challenge_created_time":1552345636840,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to plot a tree using xgboost in Python3 on a Jupyter notebook on AWS Sagemaker. However, they are encountering an error message that says they need to install graphviz to plot the tree. The user is seeking guidance on how to install graphviz to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55112494",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.3,
        "Challenge_reading_time":9.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":5.9137519445,
        "Challenge_title":"Install graphiz on AWS Sagemaker",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":572.0,
        "Challenge_word_count":92,
        "Platform":"Stack Overflow",
        "Poster_created_time":1357263005087,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Granada Hills, Los Angeles, CA, United States",
        "Poster_reputation_count":6262.0,
        "Poster_view_count":428.0,
        "Solution_body":"<p>I was finally able to learn that Conda has a package which can install it for you. I was able to get it installed by running the command:<\/p>\n\n<pre><code>!conda install python-graphviz --yes\n<\/code><\/pre>\n\n<p>Note the <code>--yes<\/code> is only needed if the installation needs to verify adding\/changing other packages since the Jupyter notebook is not interactive once it is running.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.8,
        "Solution_reading_time":4.86,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":59.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1631803441500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"mexico",
        "Answerer_reputation_count":1258.0,
        "Answerer_view_count":685.0,
        "Challenge_adjusted_solved_time":23.1295969444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to set up mlops for Vertex AI, following <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/mlops-with-vertex-ai\/blob\/main\/01-dataset-management.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a>. It works until, near the end, I try:<\/p>\n<pre><code>vertex_ai.init(\nproject=PROJECT,\n    location=REGION)\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code> module 'google.cloud.aiplatform.constants' has no attribute 'SUPPORTED_REGIONS\n<\/code><\/pre>\n<p>I am using <code>us-central1<\/code> which is supported. I wondered if maybe <code>from google.cloud import aiplatform as vertex_ai<\/code> has been changed but don't know how to find out. Any help is much appreciated.<\/p>",
        "Challenge_closed_time":1652193787552,
        "Challenge_comment_count":2,
        "Challenge_created_time":1652110521003,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while setting up mlops for Vertex AI. The error message 'google.cloud.aiplatform.constants' has no attribute 'SUPPORTED_REGIONS' is displayed when trying to initialize Vertex AI with the project and location. The user is using a supported region, but is unsure if there have been any changes to the import statement.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72174602",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":11.2,
        "Challenge_reading_time":10.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":23.1295969444,
        "Challenge_title":"Why do I get 'google.cloud.aiplatform.constants' has no attribute 'SUPPORTED_REGIONS error for Vertex AI init?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":298.0,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>I followed the same Notebook as you, even though I didn't have any issue. What could be happening to you is that you are using an <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/releases\" rel=\"nofollow noreferrer\">older version of the library<\/a>.<\/p>\n<p>You can use the command to upgrade the library that is the following one: <code>pip3 install google-cloud-aiplatform --upgrade<\/code>.<\/p>\n<p>Sometimes this happens with the basic installation of the library; the problems could be in the <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform#installation\" rel=\"nofollow noreferrer\">dependencies, versions and indirectly permissions<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.6,
        "Solution_reading_time":8.59,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":76.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1476711295896,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Malabon, Metro Manila, Philippines",
        "Answerer_reputation_count":3520.0,
        "Answerer_view_count":962.0,
        "Challenge_adjusted_solved_time":21.1495677778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am unclear abut Vertex AI pricing for model predictions. In the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/pricing\" rel=\"nofollow noreferrer\">documentation<\/a>, under the heading <strong>More about automatic scaling of prediction nodes<\/strong> one of the points mentioned is:<\/p>\n<blockquote>\n<p>&quot;If you choose automatic scaling, the number of nodes scales\nautomatically, and can scale down to zero for no-traffic durations&quot;<\/p>\n<\/blockquote>\n<p>The example provided in the documentation later also seems to suggest that during a period with no traffic, zero nodes are in use. However, when I create an Endpoint in Vertex AI, under the <strong>Autoscaling<\/strong> heading it says:<\/p>\n<blockquote>\n<p><em>&quot;Autoscaling: If you set a minimum and maximum, compute nodes will scale to meet traffic demand within those boundaries&quot;<\/em><\/p>\n<\/blockquote>\n<p>The  value of 0 under <em>&quot;Minimum number of compute nodes&quot;<\/em> is not allowed so you have to enter 1 or greater, and it is mentioned that:<\/p>\n<blockquote>\n<p>Default is 1. If set to 1 or more, then compute resources will\ncontinuously run even without traffic demand. This can increase cost\nbut avoid dropped requests due to node initialization.<\/p>\n<\/blockquote>\n<p>My question is, what happens when I select autoscaling by setting Minimum to 1 and Maximum to, say, 10. Does 1 node always run continuously? Or does it scale down to 0 nodes in no traffic condition as the documentation suggests.<\/p>\n<p>To test I deployed an Endpoint with Autoscaling (min and max set to 1) and then when I sent a prediction request the response was almost immediate, suggesting the node was already up. I did that again after about an hour and again the response was immediate suggesting that the node never shut down probably. Also, for high latency requirements, is having autoscale to 0 nodes, if that is indeed possible, even practical, i.e., what latency can we expect for starting up from 0 nodes?<\/p>",
        "Challenge_closed_time":1636603298292,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636487376953,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is confused about the pricing and functionality of Vertex AI's autoscaling feature for model predictions. The documentation suggests that the number of nodes can scale down to zero during no-traffic durations, but the Autoscaling feature in the Endpoint creation process requires a minimum of one node to be continuously running, which can increase costs. The user is unsure if selecting a minimum of one node will always keep a node running or if it will scale down to zero during no-traffic periods. The user also questions the practicality of scaling down to zero nodes for high latency requirements.",
        "Challenge_last_edit_time":1636603578603,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69904211",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":11.8,
        "Challenge_reading_time":25.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":32.2003719444,
        "Challenge_title":"Vertex AI prediction - Autoscaling cannot set minimum node to 0",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":894.0,
        "Challenge_word_count":307,
        "Platform":"Stack Overflow",
        "Poster_created_time":1471292986790,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":700.0,
        "Poster_view_count":90.0,
        "Solution_body":"<p>Are you using an N1 or a non-N1 machine type? If you want to autoscale to zero, you must use non-N1 machines. See <a href=\"https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/overview#node_allocation_for_online_prediction\" rel=\"nofollow noreferrer\">second note<\/a> from node allocation:<\/p>\n<blockquote>\n<p>Note: Versions that use a Compute Engine (N1) machine type cannot scale down to zero nodes. They can scale down to 1 node, at minimum.<\/p>\n<\/blockquote>\n<p><em>Update<\/em>: AI Platform supports scaling to zero, while Vertex AI currently does not. From the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#scaling\" rel=\"nofollow noreferrer\">scaling<\/a> documentation, nodes can scale but there is no mention that it can scale down to zero. Here's a public <a href=\"https:\/\/issuetracker.google.com\/206042974\" rel=\"nofollow noreferrer\">feature request<\/a> for people who wants to track this issue.<\/p>\n<p>With regards to latency requirements, the actual output will vary. However, one thing to note according to the documentation is that the service may not be able to bring nodes online fast enough to keep up with large spikes of request traffic. If your traffic regularly has steep spikes, and if reliably low latency is important to your application, you may want to consider manual scaling.<\/p>\n<p>Additional Reference: <a href=\"https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/machine-types-online-prediction#automatic_scaling\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/machine-types-online-prediction#automatic_scaling<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1636679717047,
        "Solution_link_count":5.0,
        "Solution_readability":12.7,
        "Solution_reading_time":21.04,
        "Solution_score_count":2.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":181.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1450242937020,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Syria",
        "Answerer_reputation_count":235.0,
        "Answerer_view_count":36.0,
        "Challenge_adjusted_solved_time":0.2286694444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run a python program using Pycharm IDE but unable to do so without stumbling into \"Your system has run out of application memory\". After some research I came across a suggestion of using Microsoft Azure ML. Can anyone point me to some helpful links that can get me started or any other suggestions at all?<\/p>\n\n<p>Edit: I am working with a data that has 400,000 samples and ~5000 samples and I want to use chi2 feature selection but I am unable to run the program.<\/p>",
        "Challenge_closed_time":1454732632400,
        "Challenge_comment_count":1,
        "Challenge_created_time":1454731809190,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to run a large Python program on Pycharm IDE due to the error \"Your system has run out of application memory\". They are seeking suggestions for alternatives, and specifically mention working with a dataset of 400,000 samples and ~5000 features, and wanting to use chi2 feature selection.",
        "Challenge_last_edit_time":1483522935080,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35237226",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":6.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.2286694444,
        "Challenge_title":"Cannot run huge Python program",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":118.0,
        "Challenge_word_count":93,
        "Platform":"Stack Overflow",
        "Poster_created_time":1454200887396,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>You can use: PyPy to run your program with less memory usage and more speed. see this <a href=\"http:\/\/pypy.org\/\" rel=\"nofollow\">pypy site<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":4.8,
        "Solution_reading_time":1.88,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":21.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1460437080990,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":386.0,
        "Answerer_view_count":42.0,
        "Challenge_adjusted_solved_time":0.7767452778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to reduce my development headaches for creating a ML Webservice on Azure ML Studio. One of the things that stuck me was can we just upload .rda files in the workbench and load it via an RScript (like in the figure below). <\/p>\n\n<p><img src=\"https:\/\/raw.githubusercontent.com\/pratos\/pratos.github.io\/master\/images\/stackb1model.png\" alt=\"Do\"><\/p>\n\n<p>But can't connect directly to the R Script block. There's another way to do it (works to upload packages that aren't available in Azure's R directories) -- using zip. But there isn't really any resource out there that I found to access the .rda file in .zip.<\/p>\n\n<p>I have 2 options here, make the .zip work or any other work around where I can directly use my .rda model. If someone could guide me about how to go forward it would appreciate it.<\/p>\n\n<p>Note: Currently, I'm creating models via the \"Create RModel\" block, training them and saving it, so that I can use it to make a predictive web service. But for models like Random Forest, not sure how the randomness might create models (local versions and Azure versions are different, the setting of seed also isn't very helpful). A bit tight on schedule, Azure ML seems boxed for creating iterations and automating the ML workflow (or maybe I'm doing it wrong).<\/p>",
        "Challenge_closed_time":1486104439200,
        "Challenge_comment_count":0,
        "Challenge_created_time":1486101642917,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to upload a saved ML model in R (local) to Azure Machine Learning Studio but is facing challenges. They are unable to connect directly to the R Script block and are looking for a workaround to use their .rda model. The user is also unsure about how the randomness might create models and is looking for guidance on how to proceed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42017727",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":16.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":0.7767452778,
        "Challenge_title":"Upload Saved ML Model in R (local) to Azure Machine Learning Studio",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":730.0,
        "Challenge_word_count":221,
        "Platform":"Stack Overflow",
        "Poster_created_time":1479363468550,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":108.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>Here is an example of uploading a .rda file for scoring:\n<a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Womens-Health-Risk-Assessment-using-the-XGBoost-classification-algorithm-1\" rel=\"nofollow noreferrer\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Womens-Health-Risk-Assessment-using-the-XGBoost-classification-algorithm-1<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":48.9,
        "Solution_reading_time":5.01,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":15.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1374169767267,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":548.0,
        "Answerer_view_count":70.0,
        "Challenge_adjusted_solved_time":7.0366380556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to call my SageMaker model endpoint both from Postman and the AWS CLI. The endpoint's status is &quot;in service&quot; but whenever I try to call it it gives me an error. When I try to use the predict function in the SageMaker notebook and provide it a numpy array (ex. <code>np.array([1,2,3,4])<\/code>), it successfully gives me an output. I'm unsure what I'm doing wrong.<\/p>\n<pre><code>$ aws2 sagemaker-runtime invoke-endpoint \\\n$ --endpoint-name=pytorch-model \\\n$ --body=1,2 \\\n$ --content-type=text\/csv \\\n$ --cli-binary-format=raw-in-base64-out \\\n$ output.json\n\nAn error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message &quot;tensors used as indices must be long, byte or bool tensors\nTraceback (most recent call last):\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/transformer.py&quot;, line 125, in transform\n    result = self._transform_fn(self._model, input_data, content_type, accept)\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/transformer.py&quot;, line 215, in _default_transform_fn\n    prediction = self._predict_fn(data, model)\n  File &quot;\/opt\/ml\/model\/code\/pytorch-model-reco.py&quot;, line 268, in predict_fn\n    return torch.argsort(- final_matrix[input_data, :], dim = 1)\nIndexError: tensors used as indices must be long, byte or bool tensors\n<\/code><\/pre>",
        "Challenge_closed_time":1596555829307,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596530497410,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to call their SageMaker model endpoint using Postman and AWS CLI. The endpoint's status is \"in service,\" but the error message indicates that tensors used as indices must be long, byte, or bool tensors. However, when the user tries to use the predict function in the SageMaker notebook and provides a numpy array, it successfully gives an output.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63243154",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.5,
        "Challenge_reading_time":18.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":7.0366380556,
        "Challenge_title":"Invoking SageMaker Endpoint for PyTorch Model",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":394.0,
        "Challenge_word_count":156,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The clue is in the final few lines of your stacktrace:<\/p>\n<pre><code>  File &quot;\/opt\/ml\/model\/code\/pytorch-model-reco.py&quot;, line 268, in predict_fn\n    return torch.argsort(- final_matrix[input_data, :], dim = 1)\nIndexError: tensors used as indices must be long, byte or bool tensors\n<\/code><\/pre>\n<p>In your <code>predict_fn<\/code> in <code>pytorch-model-reco.py<\/code> on line 268, you're trying to use <code>input_data<\/code> as indices for <code>final_matrix<\/code>, but <code>input_data<\/code> is the wrong type.<\/p>\n<p>I would guess there is some type casting that your <code>predict_fn<\/code> should be doing when the input type is <code>text\/csv<\/code>. This type casting is happening outside of the <code>predict_fn<\/code> when your input type is numpy data. Taking a look at the <a href=\"https:\/\/github.com\/aws\/sagemaker-inference-toolkit\/tree\/master\/src\/sagemaker_inference\" rel=\"nofollow noreferrer\"><code>sagemaker_inference<\/code><\/a> source code might reveal more.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.3,
        "Solution_reading_time":12.9,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":109.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1569638810883,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5255.0,
        "Answerer_view_count":306.0,
        "Challenge_adjusted_solved_time":5.2755416667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am fairly new to AWS and Sagemaker and have decided to follow some of the tutorials Amazon has to familiarize myself with it. I've been following this one (<a href=\"https:\/\/aws.amazon.com\/getting-started\/hands-on\/semantic-content-recommendation-system-amazon-sagemaker\/5\/\" rel=\"nofollow noreferrer\">tutorial<\/a>) and I've realized that it's an older tutorial using Sagemaker v1. I've been able to look up and change whatever is needed for the tutorial to work in v2 but I became stuck at this part for storing the training data in a S3 bucket to deploy the model.<\/p>\n<pre><code>import io\nimport sagemaker.amazon.common as smac\n\nprint('train_features shape = ', predictions.shape)\nprint('train_labels shape = ', labels.shape)\nbuf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(buf, predictions, labels)\nbuf.seek(0)\n\nbucket = BUCKET\nprefix = PREFIX\nkey = 'knn\/train'\nfname = os.path.join(prefix, key)\nprint(fname)\nboto3.resource('s3').Bucket(bucket).Object(fname).upload_fileobj(buf)\ns3_train_data = 's3:\/\/{}\/{}\/{}'.format(bucket, prefix, key)\nprint('uploaded training data location: {}'.format(s3_train_data))\n<\/code><\/pre>\n<p>It returns this error<\/p>\n<pre><code>NameError Traceback (most recent call \nlast)\n&lt;ipython-input-20-9e52dd949332&gt; in &lt;module&gt;\n 3\n 4\n----&gt; 5 print('train_features shape = ', predictions.shape)\n 6 print('train_labels shape = ', labels.shape)\n 7 buf = io.BytesIO()\nNameError: name 'predictions' is not defined\n<\/code><\/pre>\n<p>I'm curious as to why this would have worked in Sagemaker v1 and not v2 if predictions is not defined and if anyone could point me in the right direction for correcting this.<\/p>\n<p>Thanks.<\/p>",
        "Challenge_closed_time":1623033557140,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623014565190,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is following an older tutorial on semantic content recommendation system with Amazon SageMaker and encountered an error while storing the training data in an S3 bucket to deploy the model. The error is related to the undefined variable 'predictions' and the user is seeking guidance on how to correct it.",
        "Challenge_last_edit_time":1623040885867,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67863816",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":22.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":5.2755416667,
        "Challenge_title":"semantic content recommendation system with Amazon SageMaker, storing in S3",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":76.0,
        "Challenge_word_count":198,
        "Platform":"Stack Overflow",
        "Poster_created_time":1623013767550,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>It looks like they've left some of the code out, or changed the terminology and left in predictions by accident. predictions is an object that is defined on this page <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-test-model.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-test-model.html<\/a><\/p>\n<p>You'll have to work out what predictions is in your case.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.4,
        "Solution_reading_time":5.44,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":45.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1254829817772,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":2595.0,
        "Answerer_view_count":357.0,
        "Challenge_adjusted_solved_time":197.5463266667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to implement a input_handler() in inference.py for a sagemaker inference container.<\/p>\n<p>The images\/arrays are very big (3D). So I want to pass in a S3 URI, then the input_handler() function should load the image\/array from s3 and return the actual numpy array for the model (which expects a tensor):<\/p>\n<pre><code>def input_handler(data, context):\n\n    d = data.read().decode('utf-8')\n\n    body = json.loads(d)\n    s3path = body['s3_path']\n\n    s3 = S3FileSystem()\n    df = np.load(s3.open(s3path))\n\n    return df\n<\/code><\/pre>\n<p>Returning a numpy array worked with the Sagemaker python api version &lt; 1.0 and input_fn(), but does not work with the new container used by sagemaker python api &gt; 2.0 that expects input_handler().<\/p>\n<p>The actual container image is &quot;763104351884.dkr.ecr.eu-central-1.amazonaws.com\/tensorflow-inference:1.15-gpu&quot;.<\/p>\n<p>During inference, I get the following error in CloudWatch thrown by the container:<\/p>\n<pre><code>ERROR:python_service:exception handling request: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(\n\nTraceback (most recent call last):\n  File &quot;\/sagemaker\/python_service.py&quot;, line 289, in _handle_invocation_post\n    res.body, res.content_type = self._handlers(data, context)\n  File &quot;\/sagemaker\/python_service.py&quot;, line 322, in handler\n    response = requests.post(context.rest_uri, data=processed_input)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/requests\/api.py&quot;, line 116, in post\n    return request('post', url, data=data, json=json, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/requests\/api.py&quot;, line 60, in request\n    return session.request(method=method, url=url, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/requests\/sessions.py&quot;, line 512, in request\n    data=data or \n{}\n,\n<\/code><\/pre>\n<p>What is the correct return type? All examples I found were for json &amp; text...<\/p>",
        "Challenge_closed_time":1600259722303,
        "Challenge_comment_count":3,
        "Challenge_created_time":1599496754993,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is trying to implement an input_handler() function in inference.py for a Sagemaker inference container that loads a large 3D image\/array from S3 and returns a numpy array for the model. However, the function is not working with the new container used by Sagemaker python API > 2.0 that expects input_handler(). The user is getting an error during inference, which suggests that the return type is incorrect. The user is seeking guidance on the correct return type for the input_handler() function.",
        "Challenge_last_edit_time":1599548555527,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63781356",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.6,
        "Challenge_reading_time":26.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":211.9353638889,
        "Challenge_title":"How to correctly write a sagemaker tensorflow input_handler() that returns a numpy array?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":636.0,
        "Challenge_word_count":222,
        "Platform":"Stack Overflow",
        "Poster_created_time":1254829817772,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":2595.0,
        "Poster_view_count":357.0,
        "Solution_body":"<p>This seems to work:<\/p>\n<p><code>return json.dumps({&quot;inputs&quot;: df.tolist() }).<\/code><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":1.38,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":8.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646512935796,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":217.0,
        "Answerer_view_count":46.0,
        "Challenge_adjusted_solved_time":7.9767927778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a Vertex AI AutoML image classification model. How can I assess it for overfitting? I assume I should be able to compare training vs validation accuracy but these do not seem to be <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/evaluating-automl-models\" rel=\"nofollow noreferrer\">available<\/a>.<\/p>\n<p>And if it is overfitting,can I tweak regularization parameters? Is it already doing cross validation? Anything else that can be done? (More data,early stopping, dropouts ie how can these be done?)<\/p>",
        "Challenge_closed_time":1650850146067,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650821429613,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a Vertex AI AutoML image classification model and is looking for ways to assess it for overfitting. They are unable to find training vs validation accuracy comparison and are seeking advice on tweaking regularization parameters, cross-validation, and other methods to prevent overfitting.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71990757",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":7.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":7.9767927778,
        "Challenge_title":"How can GCP Automl handle overfitting?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":27.0,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>Deploy it to endpoint and test result with sample images by uploading to endpoint. If it's overfitting you can see the stats in analysis. You can increase the training sample and retrain your model again to get better result.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":2.85,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":39.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":3.3754333334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run custom python\/sklearn sagemaker script on AWS, basically learning from these examples: <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a><\/p>\n<p>All works fine, if define the arguments, train the model and output the file:<\/p>\n<pre><code>parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\nparser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\nparser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n# train the model...\n\njoblib.dump(model, os.path.join(args.model_dir, &quot;model.joblib&quot;))\n<\/code><\/pre>\n<p>And call the job with:<\/p>\n<pre><code>aws_sklearn.fit({'train': 's3:\/\/path\/to\/train', 'test': 's3:\/\/path\/to\/test'}, wait=False)\n<\/code><\/pre>\n<p>In this case model gets stored on different auto-generated bucket, which I do not want. I want to get the output (.joblib file) in the same s3 bucket I took data from. So I add the parameter <code>model-dir<\/code>:<\/p>\n<pre><code>aws_sklearn.fit({'train': 's3:\/\/path\/to\/train', 'test': 's3:\/\/path\/to\/test', `model-dir`: 's3:\/\/path\/to\/model'}, wait=False)\n<\/code><\/pre>\n<p>But it results in error:\n<code>FileNotFoundError: [Errno 2] No such file or directory: 's3:\/\/path\/to\/model\/model.joblib'<\/code><\/p>\n<p>Same happens if I hardcode the output path inside the training script.<\/p>\n<p>So the main question, how can I get the output file in the bucket of my choice?<\/p>",
        "Challenge_closed_time":1610545645387,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610533493827,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to run a custom python\/sklearn sagemaker script on AWS and wants to save the output (.joblib file) in the same S3 bucket from where the data was taken. However, when the user adds the parameter \"model-dir\" to specify the output location, it results in an error \"FileNotFoundError: [Errno 2] No such file or directory\". The user is seeking a solution to save the output file in the bucket of their choice.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65699980",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":23.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":3.3754333334,
        "Challenge_title":"Change model file save location on AWS SageMaker Training Job",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1244.0,
        "Challenge_word_count":159,
        "Platform":"Stack Overflow",
        "Poster_created_time":1572957474856,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":123.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>You can use parameter <code>output_path<\/code> when you define the estimator. If you use the\n<code>model_dir<\/code> I guess you have to create that bucket beforehand, but you have the advantage that artifacts can be saved in real time during the training (if the instance has rights on S3). You can take a look at my <a href=\"https:\/\/github.com\/roccopietrini\/TFSagemakerDetection\" rel=\"nofollow noreferrer\">repo<\/a> for this specific case.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.5,
        "Solution_reading_time":5.66,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1444147981107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":126.5392991667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have the following simple setup in Azure ML. <a href=\"https:\/\/i.stack.imgur.com\/kWi0S.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kWi0S.jpg\" alt=\"ML setup\"><\/a> \nBasically the Reader is a SQL query to a DB which returns a vector called Pdelta, which is then passed to the R script for further processing  and the results are then returned back to the web service. The DB query is simple (<code>SELECT Pdelta FROM ...<\/code>) and it works fine. I have set the DB query as a web service paramater as well. <\/p>\n\n<p>Everything seems to work fine, but at the end when i publish it as a web service and test it, it somehow asks for an additional input parameter. The additional parameter gets called <code>PDELTA<\/code>.\n<a href=\"https:\/\/i.stack.imgur.com\/mnzPZ.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mnzPZ.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I am wondering why is this happening, what is it that I am overlooking? I would like to make this web service ask for only one parameter - the SQL query (Delta Query) which would then deliver the Pdeltas. <\/p>\n\n<p>Any ideas or suggestions would be grealty appreciated! <\/p>",
        "Challenge_closed_time":1444147981107,
        "Challenge_comment_count":0,
        "Challenge_created_time":1443692439630,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has set up a simple Azure ML configuration where the Reader is a SQL query to a database that returns a vector called Pdelta, which is then passed to the R script for further processing. However, when the user publishes it as a web service and tests it, an additional input parameter called PDELTA is requested, which the user wants to eliminate to make the web service ask for only one parameter - the SQL query (Delta Query) that would deliver the Pdeltas.",
        "Challenge_last_edit_time":1456850075240,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/32884296",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":15.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":126.5392991667,
        "Challenge_title":"Web service input into SQL query into R in Azure ML",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":347.0,
        "Challenge_word_count":182,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432829415467,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":501.0,
        "Poster_view_count":76.0,
        "Solution_body":"<p>You can remove the web service input block and publish the web service without it. That way the Pdelta input will be passed in only from the Reader module.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":1.97,
        "Solution_score_count":3.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":29.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1415722650716,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Verona, VR, Italy",
        "Answerer_reputation_count":4811.0,
        "Answerer_view_count":713.0,
        "Challenge_adjusted_solved_time":8.4415813889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>On the limited Azure Machine Learning Studio, one can import data from an On-Premises SQL Server Database.\nWhat about the ability to do the exact same thing on a python jupyter notebook on a virtual machine from the Azure Machine Learning Services workspace ?<\/p>\n\n<p>It does not seem possible from what I've found in the documentation.\nData sources would be limited in Azure ML Services : \"Currently, the list of supported Azure storage services that can be registered as datastores are Azure Blob Container, Azure File Share, Azure Data Lake, Azure Data Lake Gen2, Azure SQL Database, Azure PostgreSQL, and Databricks File System\"<\/p>\n\n<p>Thank you in advance for your assistance<\/p>",
        "Challenge_closed_time":1558479194140,
        "Challenge_comment_count":0,
        "Challenge_created_time":1558448804447,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to import data from an On-Premises SQL Server Database to a python jupyter notebook on a virtual machine from the Azure Machine Learning Services workspace. However, it seems that this is not possible as the data sources are limited to specific Azure storage services.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56240481",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.7,
        "Challenge_reading_time":9.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":8.4415813889,
        "Challenge_title":"Can I import data from On-Premises SQL Server Database to Azure Machine Learning virtual machine?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1147.0,
        "Challenge_word_count":123,
        "Platform":"Stack Overflow",
        "Poster_created_time":1558447987352,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>As of today, <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-load-data#load-sql-data\" rel=\"nofollow noreferrer\">you can load SQL data, but only a MS SQL Server source (also on-premise) is supported<\/a>.<\/p>\n\n<p>Using <code>azureml.dataprep<\/code>, code would read along the lines of<\/p>\n\n<pre><code>import azureml.dataprep as dprep\n\nsecret = dprep.register_secret(value=\"[SECRET-PASSWORD]\", id=\"[SECRET-ID]\")\n\nds = dprep.MSSQLDataSource(server_name=\"[SERVER-NAME]\",\n                           database_name=\"[DATABASE-NAME]\",\n                           user_name=\"[DATABASE-USERNAME]\",\n                           password=secret)\n\ndflow = dprep.read_sql(ds, \"SELECT top 100 * FROM [YourDB].[ATable]\")\n# print first records\ndflow.head(5)\n<\/code><\/pre>\n\n<p>As far as I understand the APIs are under heavy development and <code>azureml.dataprep<\/code> may be soon superseded by functionality provided by the <a href=\"https:\/\/aka.ms\/azureml\/concepts\/datasets\" rel=\"nofollow noreferrer\">Dataset class<\/a>.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.0,
        "Solution_reading_time":12.72,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":82.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1388815483776,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":100.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":474.1775913889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm having an issue to serve a model with reference to model registry. According to help, the path should look like this: <\/p>\n\n<p>models:\/model_name\/stage<\/p>\n\n<p>When I type in terminal: <br>\n<code>mlflow models serve -m models:\/ml_test_model1\/Staging --no-conda -h 0.0.0.0 -p 5003<\/code><\/p>\n\n<p>I got the error: <br>\n<code>mlflow.exceptions.MlflowException: Not a proper models:\/ URI: models:\/ml_test_model1\/Staging\/MLmodel. Models URIs must be of the form 'models:\/&lt;model_name&gt;\/&lt;version or stage&gt;'.<\/code><\/p>\n\n<p>Model is registered and visible in db and server. <br> \nIf I put absolute path, it works (experiment_id\/run_id\/artifacts\/model_name).<\/p>\n\n<p>mlflow version: 1.4 <br>\nPython version: 3.7.3<\/p>\n\n<p>Is it matter of some environmental settings or something different?<\/p>",
        "Challenge_closed_time":1577232243980,
        "Challenge_comment_count":0,
        "Challenge_created_time":1575544859307,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while serving a model with reference to the model registry using MLflow. The error message suggests that the URI format is incorrect and should be in the form of 'models:\/<model_name>\/<version or stage>'. The model is registered and visible in the database and server, and the user is able to serve the model using an absolute path. The user is using MLflow version 1.4 and Python version 3.7.3 and is unsure if this is an environmental setting issue or something else.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59194004",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.4,
        "Challenge_reading_time":10.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":468.7179647222,
        "Challenge_title":"MLflow - Serving model by reference to model registry",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1225.0,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1575543357812,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>That style of referencing model artefacts is fixed from mlflow v1.5 (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/pull\/2067\" rel=\"nofollow noreferrer\">Bug Fix<\/a>).<\/p>\n\n<p>You'll need to run <code>mlflow db upgrade &lt;db uri&gt;<\/code> to refresh your schemas before restarting your mlflow server.<\/p>\n\n<p>You may find listing registered models helpful:<\/p>\n\n<p><code>&lt;server&gt;:&lt;port&gt;\/api\/2.0\/preview\/mlflow\/registered-models\/list<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1577251898636,
        "Solution_link_count":1.0,
        "Solution_readability":10.1,
        "Solution_reading_time":6.02,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":42.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":4.8427816667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I registered a model in my AML workspace, and I can see it in the Model List:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rtL5Q.png\" rel=\"nofollow noreferrer\">Model List view<\/a><\/p>\n<p>But I cannot see it in Designer (preview), which prevents me from using the new model there.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/WpvIb.png\" rel=\"nofollow noreferrer\">Designer view<\/a><\/p>\n<p>Looks like a bug to me. Datasets work fine.<\/p>",
        "Challenge_closed_time":1596073257467,
        "Challenge_comment_count":3,
        "Challenge_created_time":1596056234913,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has registered a model in their AML workspace, but it is not showing up in Designer (preview), which is preventing them from using the new model. The user suspects it is a bug as datasets are working fine.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63162310",
        "Challenge_link_count":2,
        "Challenge_participation_count":5,
        "Challenge_readability":7.5,
        "Challenge_reading_time":6.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":4.7284872222,
        "Challenge_title":"Models registered in workspace do not show up in Designer (preview)",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":81.0,
        "Challenge_word_count":64,
        "Platform":"Stack Overflow",
        "Poster_created_time":1531852372996,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>This is known issue as the models registered in workspace cannot be consumed in Designer without the new custom module capability (in private preview) available.<\/p>\n<p>The models showing up in Designer today are these generated from Designer training -&gt; inference pipeline conversion and can only be used in Designer (not registered in the workspace).\nWe have an effort ongoing to reduce the confusion.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1596073668927,
        "Solution_link_count":0.0,
        "Solution_readability":12.7,
        "Solution_reading_time":5.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1646242327867,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":27.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":98.4473672222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a data that looks like this<\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Date<\/th>\n<th>Name<\/th>\n<th>SurveyID<\/th>\n<th>Score<\/th>\n<th>Error<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>2022-02-17<\/td>\n<td>Jack<\/td>\n<td>10<\/td>\n<td>95<\/td>\n<td>Name<\/td>\n<\/tr>\n<tr>\n<td>2022-02-17<\/td>\n<td>Jack<\/td>\n<td>10<\/td>\n<td>95<\/td>\n<td>Address<\/td>\n<\/tr>\n<tr>\n<td>2022-02-16<\/td>\n<td>Tom<\/td>\n<td>9<\/td>\n<td>100<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>2022-02-16<\/td>\n<td>Carl<\/td>\n<td>8<\/td>\n<td>93<\/td>\n<td>Zip<\/td>\n<\/tr>\n<tr>\n<td>2022-02-16<\/td>\n<td>Carl<\/td>\n<td>8<\/td>\n<td>93<\/td>\n<td>Email<\/td>\n<\/tr>\n<tr>\n<td>2022-02-15<\/td>\n<td>Dan<\/td>\n<td>7<\/td>\n<td>72<\/td>\n<td>Zip<\/td>\n<\/tr>\n<tr>\n<td>2022-02-15<\/td>\n<td>Dan<\/td>\n<td>7<\/td>\n<td>72<\/td>\n<td>Email<\/td>\n<\/tr>\n<tr>\n<td>2022-02-15<\/td>\n<td>Dan<\/td>\n<td>7<\/td>\n<td>72<\/td>\n<td>Name<\/td>\n<\/tr>\n<tr>\n<td>2022-02-15<\/td>\n<td>Dan<\/td>\n<td>6<\/td>\n<td>90<\/td>\n<td>Phone<\/td>\n<\/tr>\n<tr>\n<td>2022-02-14<\/td>\n<td>Tom<\/td>\n<td>5<\/td>\n<td>98<\/td>\n<td>Gender<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div>\n<p>I wanted to have a segmentation data using the avg. score per individual.<\/p>\n<pre><code>Segment\nA:  98%-100%\nB:  95%-97%\nC:  90%-94%\nD:  80%-89%\nE:  0% -79%\n<\/code><\/pre>\n<p>I did an if else formula which is this:<\/p>\n<pre><code>ifelse(Score} &gt;= 98,'A',ifelse({Score} &gt;= 95,'B',ifelse({Score} &gt;= 90,'C',ifelse({Score} &gt;= 80,'D','E'))))\n<\/code><\/pre>\n<p>This is now the output of what I did:<\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Date<\/th>\n<th>Name<\/th>\n<th>SurveyID<\/th>\n<th>Score<\/th>\n<th>Error<\/th>\n<th>Segement<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>2022-02-17<\/td>\n<td>Jack<\/td>\n<td>10<\/td>\n<td>95<\/td>\n<td>Name<\/td>\n<td>B<\/td>\n<\/tr>\n<tr>\n<td>2022-02-17<\/td>\n<td>Jack<\/td>\n<td>10<\/td>\n<td>95<\/td>\n<td>Address<\/td>\n<td>B<\/td>\n<\/tr>\n<tr>\n<td>2022-02-16<\/td>\n<td>Tom<\/td>\n<td>9<\/td>\n<td>100<\/td>\n<td><\/td>\n<td>A<\/td>\n<\/tr>\n<tr>\n<td>2022-02-16<\/td>\n<td>Carl<\/td>\n<td>8<\/td>\n<td>93<\/td>\n<td>Zip<\/td>\n<td>C<\/td>\n<\/tr>\n<tr>\n<td>2022-02-16<\/td>\n<td>Carl<\/td>\n<td>8<\/td>\n<td>93<\/td>\n<td>Email<\/td>\n<td>C<\/td>\n<\/tr>\n<tr>\n<td>2022-02-15<\/td>\n<td>Dan<\/td>\n<td>7<\/td>\n<td>72<\/td>\n<td>Zip<\/td>\n<td>E<\/td>\n<\/tr>\n<tr>\n<td>2022-02-15<\/td>\n<td>Dan<\/td>\n<td>7<\/td>\n<td>72<\/td>\n<td>Email<\/td>\n<td>E<\/td>\n<\/tr>\n<tr>\n<td>2022-02-15<\/td>\n<td>Dan<\/td>\n<td>7<\/td>\n<td>72<\/td>\n<td>Name<\/td>\n<td>E<\/td>\n<\/tr>\n<tr>\n<td>2022-02-15<\/td>\n<td>Dan<\/td>\n<td>6<\/td>\n<td>90<\/td>\n<td>Phone<\/td>\n<td>C<\/td>\n<\/tr>\n<tr>\n<td>2022-02-14<\/td>\n<td>Tom<\/td>\n<td>5<\/td>\n<td>98<\/td>\n<td>Gender<\/td>\n<td>A<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div>\n<p>I realized that the calculation I did only applies for the score. I was expecting an output like this:<\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Name<\/th>\n<th>Average Score<\/th>\n<th>Total Survey<\/th>\n<th>Segement<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>Jack<\/td>\n<td>95<\/td>\n<td>1<\/td>\n<td>B<\/td>\n<\/tr>\n<tr>\n<td>Tom<\/td>\n<td>99<\/td>\n<td>2<\/td>\n<td>A<\/td>\n<\/tr>\n<tr>\n<td>Carl<\/td>\n<td>93<\/td>\n<td>1<\/td>\n<td>C<\/td>\n<\/tr>\n<tr>\n<td>Dan<\/td>\n<td>81<\/td>\n<td>2<\/td>\n<td>D<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div>\n<p>I have tried to create another calculated field for Average Score which is:<\/p>\n<pre><code>avgOver({Score}, [Name], PRE_AGG)\n<\/code><\/pre>\n<p>I believe I am missing a distinct count of survey IDs in that formula, that I do not know where to place. As for segmentation calculation, I cannot on my life figure that part out without getting aggregation errors on Quicksight. Please help, thank you.<\/p>",
        "Challenge_closed_time":1655843015852,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655488605330,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a segmentation data using the average score per individual in AWS Quicksight. They have created an if-else formula for segmentation, but it only applies to the score and not the average score. They have also tried to create a calculated field for average score but are unsure where to place the distinct count of survey IDs. They are also encountering aggregation errors while trying to calculate segmentation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72663159",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":22.3,
        "Challenge_reading_time":47.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":98.4473672222,
        "Challenge_title":"AWS Quicksight - question about creating a calculated field using if else and custom aggregation",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":105.0,
        "Challenge_word_count":383,
        "Platform":"Stack Overflow",
        "Poster_created_time":1646242327867,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":27.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Got the answer from Quicksight Community. Pasting it here.<\/p>\n<p>For segmentation, you can use the calculated field which you created for average score .<\/p>\n<pre><code>avg_score = avgOver(Score,[Name],PRE_AGG)\n<\/code><\/pre>\n<p>Segment<\/p>\n<pre><code>ifelse\n(\n    {avg_score}&gt;= 98,'A',\n    {avg_score}&gt;= 95,'B',\n    {avg_score}&gt;= 90,'C',\n    {avg_score}&gt;= 80,'D',\n    'E'\n)\n<\/code><\/pre>\n<p>The survey id can be used to get the distinct count per individual.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.5,
        "Solution_reading_time":5.93,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":52.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1553088438367,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tehran, Tehran Province, Iran",
        "Answerer_reputation_count":415.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":24.0755444445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Similar to <a href=\"https:\/\/stackoverflow.com\/questions\/57199472\/is-it-possible-to-set-change-mlflow-run-name-after-run-initial-creation#:%7E:text=It%20is%20possible%20to%20edit,you%27d%20like%20to%20edit.&amp;text=There%27s%20currently%20no%20stable%20public,the%20tag%20with%20key%20mlflow.\">this question<\/a>, I'd like to edit\/set the description of a run via code, instead of editing it via UI.<\/p>\n<p>To clarify, I don't want to set the description of my entire experiment, only of a single run.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ogUgu.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ogUgu.png\" alt=\"Image showing what I want to edit\" \/><\/a><\/p>",
        "Challenge_closed_time":1660231264752,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660221143727,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to know how to set or edit the description of a single run in MLflow programmatically instead of doing it through the UI.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73320708",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":16.0,
        "Challenge_reading_time":9.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":2.8113958333,
        "Challenge_title":"Set run description programmatically in mlflow",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":89.0,
        "Challenge_word_count":58,
        "Platform":"Stack Overflow",
        "Poster_created_time":1527525798183,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sarajevo, Bosnia and Herzegovina",
        "Poster_reputation_count":736.0,
        "Poster_view_count":57.0,
        "Solution_body":"<p>There are two ways to set the description.<\/p>\n<h3>1. <code>description<\/code> parameter<\/h3>\n<p>You can set a description using a markdown string for your run in <code>mlflow.start_run()<\/code> using <code>description<\/code> parameter. Here is an example.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>if __name__ == &quot;__main__&quot;:\n    # load dataset and other stuff\n\n    run_description = &quot;&quot;&quot;\n### Header\nThis is a test **Bold**, *italic*, ~~strikethrough~~ text.\n[And this is an example hayperlink](http:\/\/example.com\/).\n    &quot;&quot;&quot;\n\n    with mlflow.start_run(description=run_description) as run:\n        # train model and other stuff\n<\/code><\/pre>\n<h3>2. <code>mlflow.note.content<\/code> tag<\/h3>\n<p>You can set\/edit run names by setting the tag with the key <code>mlflow.note.content<\/code>, which is what the UI (currently) does under the hood.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>if __name__ == &quot;__main__&quot;:\n    # load dataset and other stuff\n\n    run_description = &quot;&quot;&quot;\n### Header\nThis is a test **Bold**, *italic*, ~~strikethrough~~ text.\n[And this is an example hayperlink](http:\/\/example.com\/).\n    &quot;&quot;&quot;\n\n    tags = {\n        'mlflow.note.content': run_description\n    }\n\n    with mlflow.start_run(tags=tags) as run:\n        # train model and other stuff\n<\/code><\/pre>\n<h3>Result<\/h3>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4zZa9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4zZa9.png\" alt=\"output of the given example\" \/><\/a><\/p>\n<hr \/>\n<p>If you set <code>description<\/code> parameter and <code>mlflow.note.content<\/code> tag in <code>mlflow.start_run()<\/code>, you'll get this error.<\/p>\n<pre><code>Description is already set via the tag mlflow.note.content in tags.\nRemove the key mlflow.note.content from the tags or omit the description.\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1660307815687,
        "Solution_link_count":4.0,
        "Solution_readability":8.8,
        "Solution_reading_time":23.72,
        "Solution_score_count":1.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":186.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.7084425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi MSFT Community,     <\/p>\n<p>I followed this guide to set up a GPU: <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/data-science-virtual-machine\/dsvm-ubuntu-intro\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/data-science-virtual-machine\/dsvm-ubuntu-intro<\/a>    <\/p>\n<p>VM: Standard NC12_Promo, 12 vCPUs, 112 Gib RAM    <br \/>\nOperating System: Linux    <br \/>\nOffer: Ubuntu-1804    <\/p>\n<p>I am ready to start deep learning training but I am confused about what to do next. I am doing a medical image classification project. I have 1 millions images store in Azure blob now. Do I need to download them to my VM in order to train? Or is it a better way to access image efficiently?    <\/p>\n<p>What are some good tutorials to set up the experiments? I've read a lot of documentation but still confused.     <\/p>\n<p>Thank you very much!    <br \/>\nBest Regards,    <br \/>\nClaire<\/p>",
        "Challenge_closed_time":1605057040680,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605040090287,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to proceed with deep learning training on Azure for a medical image classification project. They are unsure whether they need to download the 1 million images stored in Azure blob to their VM for training or if there is a more efficient way to access them. They are also looking for tutorials to help set up the experiments.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/158168\/deep-learning-training-on-azure-steps-and-tutorial",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":11.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":4.7084425,
        "Challenge_title":"Deep learning training on Azure steps and tutorial",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":127,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=d324c18d-0c6a-4b3e-878e-30b46421559e\">@gecheng  <\/a>     <br \/>\ncheck on the below AI training modules.    <br \/>\n<a href=\"https:\/\/aischool.microsoft.com\/en-us\/services\/learning-paths\">https:\/\/aischool.microsoft.com\/en-us\/services\/learning-paths<\/a>    <\/p>\n<p>AI Lab    <br \/>\n<a href=\"https:\/\/www.microsoft.com\/en-us\/ai\/ai-lab-projects\">https:\/\/www.microsoft.com\/en-us\/ai\/ai-lab-projects<\/a>    <\/p>\n<p>AI module gallery    <br \/>\n<a href=\"https:\/\/gallery.azure.ai\/browse\">https:\/\/gallery.azure.ai\/browse<\/a>    <\/p>\n<p>----------    <\/p>\n<p>Please don\u2019t forget to &quot;Accept the answer&quot; and \u201cup-vote\u201d wherever the information provided helps you, this can be beneficial to other community members.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.4,
        "Solution_reading_time":9.67,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1411546424280,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Edinburgh",
        "Answerer_reputation_count":58.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":4477.9070705556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed a the universal_sentence_encoder_large_3 to an aws sagemaker.  When I am attempting to predict with the deployed model I get <code>Failed precondition: Table not initialized.<\/code> as an error. I have included the part where I save my model below:<\/p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\ndef tfhub_to_savedmodel(model_name, export_path):\n\n    model_path = '{}\/{}\/00000001'.format(export_path, model_name)\n    tfhub_uri = 'http:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/3'\n\n    with tf.Session() as sess:\n        module = hub.Module(tfhub_uri)\n        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n        input_params = module.get_input_info_dict()\n        dtype = input_params['text'].dtype\n        shape = input_params['text'].get_shape()\n\n        # define the model inputs\n        inputs = {'text': tf.placeholder(dtype, shape, 'text')}\n        output = module(inputs['text'])\n        outputs = {\n            'vector': output,\n        }\n\n        # export the model\n        tf.saved_model.simple_save(\n            sess,\n            model_path,\n            inputs=inputs,\n            outputs=outputs)  \n\n    return model_path\n<\/code><\/pre>\n\n<p>I have seen other people ask this problem but no solution has been ever posted.  It seems to be a common problem with tensorflow_hub sentence encoders<\/p>",
        "Challenge_closed_time":1580113898207,
        "Challenge_comment_count":0,
        "Challenge_created_time":1563993432753,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has deployed a universal sentence encoder to an AWS Sagemaker and is encountering an error \"Failed precondition: Table not initialized\" while attempting to predict with the deployed model. The user has shared the code used to save the model and has mentioned that this seems to be a common problem with TensorFlow Hub sentence encoders.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57189292",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":17.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":4477.9070705556,
        "Challenge_title":"Failed precondition: Table not initialized. on deployed universal sentence encoder from aws sagemaker",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":365.0,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_created_time":1531840489147,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berkeley, CA, USA",
        "Poster_reputation_count":425.0,
        "Poster_view_count":92.0,
        "Solution_body":"<p>I was running into this exact issue earlier this week while trying to modify this example <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_serving_container\/tensorflow_serving_container.ipynb\" rel=\"nofollow noreferrer\">Sagemaker notebook<\/a>. Particularly the part where serving the model. That is, running <code>predictor.predict()<\/code> on the Sagemaker Tensorflow Estimator.<\/p>\n\n<p>The solution outlined in the issue worked perfectly for me- <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/issues\/773#issuecomment-509433290\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/issues\/773#issuecomment-509433290<\/a><\/p>\n\n<p>I think it's just because <code>tf.tables_initializer()<\/code> only runs for training but it needs to be specified through the <code>legacy_init_op<\/code> if you want to run it during prediction.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":18.5,
        "Solution_reading_time":12.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":78.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1259808393296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, Canada",
        "Answerer_reputation_count":44706.0,
        "Answerer_view_count":4356.0,
        "Challenge_adjusted_solved_time":4.5641338889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>After AWS Autopilot creates a model, How to use that model to the training data set offline?<\/p>\n<p>How to use that .tar.gz model file?<\/p>",
        "Challenge_closed_time":1637338122312,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637321691430,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to use a model generated by AWS Autopilot for offline training data sets and how to utilize the .tar.gz model file.",
        "Challenge_last_edit_time":1658782461116,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70034212",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":2.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":4.5641338889,
        "Challenge_title":"Model generated by AWS autopilot",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":125.0,
        "Challenge_word_count":28,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The .tar.gz file is a model artifact<\/p>\n<p>To create a model, you combine the algorithm container and the model artifact<\/p>\n<p>You can do so in the Console, under Inference &gt; Models &gt; Create Model<\/p>\n<p>What do you mean by &quot;How to use that model to the training data set offline?&quot;<\/p>\n<p>If you mean, &quot;run a batch transformation&quot;, then once you create a model, you can select the model in the console, then click 'Create batch transform job'<\/p>\n<p>If you want to do it locally, then you can use the SageMaker Python SDK in <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">Local Mode<\/a> and run the transform on your local computer (requires Docker)<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1637865568940,
        "Solution_link_count":1.0,
        "Solution_readability":17.3,
        "Solution_reading_time":9.31,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":108.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":217.8333333333,
        "Challenge_answer_count":2,
        "Challenge_body":"Even with auto Ml, should carefully custom split my data to my satisfaction or just leave it to AutoML?\n\n\u00a0\n\nAnd what difference does it make?",
        "Challenge_closed_time":1626243360000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625459160000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is questioning whether they should custom split their image data or rely on AutoML to do it for them, and is seeking to understand the potential impact of their decision.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Should-I-custom-split-my-image-data\/m-p\/163031#M10",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":4.9,
        "Challenge_reading_time":2.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":217.8333333333,
        "Challenge_title":"Should I custom split my image data?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":489.0,
        "Challenge_word_count":31,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi Ayoola\n\nIf your data is large enough and have wide representation of each category, you may go with the automated split in AutoML. That would save time and perform well.\n\nIf you have some specific needs, such as the representation of certain observations in a specific category is important and limited within the data, you may want to make sure that it is well distributed for validation and test. And custom split would help for that. Another reason of using custom split could be for comparison of your model performance with external models so you use exactly the same training\/test datasets and make an apples to apples comparison.\n\nHere are some tips I find useful in this doc:\n\nhttps:\/\/cloud.google.com\/vision\/automl\/docs\/beginners-guide#distribute_examples_equally_across_categ...\n\nCheers\n\nTuba.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.5,
        "Solution_reading_time":10.36,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":127.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":18.9520191667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I wonder if it's possible to run training Amazon SageMaker object detection model on a local PC?<\/p>",
        "Challenge_closed_time":1654073043172,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654004815903,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of training an Amazon SageMaker object detection model on a local PC.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72448994",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":2.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":18.9520191667,
        "Challenge_title":"Train Amazon SageMaker object detection model on local PC",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":70.0,
        "Challenge_word_count":25,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442786553536,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kyiv",
        "Poster_reputation_count":71.0,
        "Poster_view_count":30.0,
        "Solution_body":"<p>You're probably referring to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object-detection.html\" rel=\"nofollow noreferrer\">this<\/a> object detection algorithm which is part of of <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">Amazon SageMaker built-in algorithms<\/a>. <strong>Built-in algorithms must be trained on the cloud<\/strong>.<br \/>\nIf you're bringing your own Tensorflow or PyTorch model, you could use SageMaker training jobs to train either on the cloud or locally as @kirit noted.<\/p>\n<p>I would also look at <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-jumpstart.html\" rel=\"nofollow noreferrer\">SageMaker JumpStart<\/a> for a wide variety of object detection algorithm which are TF\/PT based.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":16.0,
        "Solution_reading_time":10.43,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":79.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1542628542703,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":11.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":43.2314219444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed a Tensorflow-Model in SageMaker Studio following this tutorial:\n<a href=\"https:\/\/aws.amazon.com\/de\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/de\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/<\/a>\nThe Model needs a Multidimensional Array as input. Invoking it from the Notebook itself is working:<\/p>\n<pre><code>import numpy as np\nimport json\ndata = np.load(&quot;testValues.npy&quot;)\npred=predictor.predict(data)\n<\/code><\/pre>\n<p>But I wasnt able to invoke it from a boto 3 client using this code:<\/p>\n<pre><code>import json\nimport boto3\nimport numpy as np\nimport io\n \nclient = boto3.client('runtime.sagemaker')\ndatain = np.load(&quot;testValues.npy&quot;)\ndata=datain.tolist();\nresponse = client.invoke_endpoint(EndpointName=endpoint_name, Body=json.dumps(data))\nresponse_body = response['Body']\nprint(response_body.read())\n<\/code><\/pre>\n<p>This throws the Error:<\/p>\n<pre><code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from model with message &quot;{&quot;error&quot;: &quot;Unsupported Media Type: Unknown&quot;}&quot;.\n<\/code><\/pre>\n<p>I guess the reason is the json Media Type but i have no clue how to get it back in shape.\nI tried this:<a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/644\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/644<\/a> but it doesnt seem to change anything<\/p>",
        "Challenge_closed_time":1605773068536,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605617435417,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has deployed a Tensorflow-Model in SageMaker Studio that requires a multidimensional array as input. While invoking it from the Notebook works, the user encountered an error when trying to invoke it from a boto3 client. The error message suggests an unsupported media type, and the user suspects it is due to the JSON media type but is unsure how to fix it. The user tried a solution from a GitHub issue but it did not work.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64875623",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":16.8,
        "Challenge_reading_time":21.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":43.2314219444,
        "Challenge_title":"Invoking an endpoint in AWS with a multidimensional array",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":550.0,
        "Challenge_word_count":147,
        "Platform":"Stack Overflow",
        "Poster_created_time":1542628542703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":11.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>This fixed it for me:\nThe Content Type was missing.<\/p>\n<pre><code>import json\nimport boto3\nimport numpy as np\nimport io\n\nclient = boto3.client('runtime.sagemaker',aws_access_key_id=..., aws_secret_access_key=...,region_name=...)\nendpoint_name = '...'\n\ndata = np.load(&quot;testValues.npy&quot;)\n\n\npayload = json.dumps(data.tolist())\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                                  ContentType='application\/json',\n                                   Body=payload)\nresult = json.loads(response['Body'].read().decode())\nres = result['predictions']\nprint(&quot;test&quot;)\n\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":18.7,
        "Solution_reading_time":7.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":38.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":17.0636369444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>If I have a trained model in Using pickle, or Joblib.\nLets say its Logistic regression or XGBoost.<\/p>\n<p>I would like to host that model in AWS Sagemaker as endpoint without running a training job.\nHow to achieve that.<\/p>\n<pre><code>#Lets Say myBucketName contains model.pkl\nmodel = joblib.load('filename.pkl')  \n# X_test = Numpy Array \nmodel.predict(X_test)  \n<\/code><\/pre>\n<p>I am not interested to <code>sklearn_estimator.fit('S3 Train, S3 Validate' )<\/code> , I have the trained model<\/p>",
        "Challenge_closed_time":1599722577176,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599661148083,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user wants to know how to host a pre-trained machine learning model, specifically a Logistic Regression or XGBoost model, on AWS Sagemaker as an endpoint without running a training job. They have the model saved using pickle or Joblib and do not want to use the <code>sklearn_estimator.fit('S3 Train, S3 Validate')<\/code> method.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63813624",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":7.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":17.0636369444,
        "Challenge_title":"Load a Picked or Joblib Pre trained ML Model to Sagemaker and host as endpoint",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1622.0,
        "Challenge_word_count":80,
        "Platform":"Stack Overflow",
        "Poster_created_time":1370509408500,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1573.0,
        "Poster_view_count":194.0,
        "Solution_body":"<p>For Scikit Learn for example, you can get inspiration from this public demo <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a><\/p>\n<p>Step 1: Save your artifact (eg the joblib) compressed in S3 at <code>s3:\/\/&lt;your path&gt;\/model.tar.gz<\/code><\/p>\n<p>Step 2: Create an inference script with the deserialization function <code>model_fn<\/code>. (Note that you could also add custom inference functions <code>input_fn<\/code>, <code>predict_fn<\/code>, <code>output_fn<\/code> but for scikit the defaults function work fine)<\/p>\n<pre><code>%%writefile inference_script.py. # Jupiter command to create file in case you're in Jupiter\n\nimport joblib\nimport os\n\ndef model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))\n    return clf\n<\/code><\/pre>\n<p>Step 3: Create a model associating the artifact with the right container<\/p>\n<pre><code>from sagemaker.sklearn.model import SKLearnModel\n\nmodel = SKLearnModel(\n    model_data='s3:\/\/&lt;your path&gt;\/model.tar.gz',\n    role='&lt;your role&gt;',\n    entry_point='inference_script.py',\n    framework_version='0.23-1')\n<\/code><\/pre>\n<p>Step 4: Deploy!<\/p>\n<pre><code>model.deploy(\n    instance_type='ml.c5.large',  # choose the right instance type\n    initial_instance_count=1)\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.5,
        "Solution_reading_time":20.38,
        "Solution_score_count":5.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":122.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1342709052703,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"D\u00fcsseldorf, Germany",
        "Answerer_reputation_count":1889.0,
        "Answerer_view_count":654.0,
        "Challenge_adjusted_solved_time":16.1329663889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I was playing with the AWS instances and trying to deploy some locally trained Keras models, but I find no documentation on that. Has anyone already been able to do it? <\/p>\n\n<p>I tried to use a similar approach to <a href=\"https:\/\/aws.amazon.com\/pt\/blogs\/machine-learning\/bring-your-own-pre-trained-mxnet-or-tensorflow-models-into-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/pt\/blogs\/machine-learning\/bring-your-own-pre-trained-mxnet-or-tensorflow-models-into-amazon-sagemaker\/<\/a>, but I had no success. I also found some examples for training keras models in the cloud, but I was not able to get the entry_point + artifacts right. <\/p>\n\n<p>Thanks for your time!<\/p>",
        "Challenge_closed_time":1538644957792,
        "Challenge_comment_count":0,
        "Challenge_created_time":1538586879113,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy locally trained Keras models to AWS Sagemaker but is unable to find any documentation on it. They have tried a similar approach to deploying MXNet or TensorFlow models but have had no success. They are seeking advice on how to get the entry_point and artifacts right.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52632388",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":12.3,
        "Challenge_reading_time":9.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":16.1329663889,
        "Challenge_title":"Is it possible to deploy a already trained Keras to Sagemaker?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1291.0,
        "Challenge_word_count":87,
        "Platform":"Stack Overflow",
        "Poster_created_time":1448029868996,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Portugal",
        "Poster_reputation_count":260.0,
        "Poster_view_count":52.0,
        "Solution_body":"<p>Yes, it is possible, and yes, the official documentation is not much of help.\nHowever, I wrote an <a href=\"https:\/\/gnomezgrave.com\/2018\/07\/05\/using-a-custom-model-for-ml-inference-with-amazon-sagemaker\" rel=\"nofollow noreferrer\">article on that<\/a>, and I hope it will help you.<\/p>\n\n<p>Let me know if you need more details. Cheers!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.7,
        "Solution_reading_time":4.39,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":40.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":52.1266738889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there any restriction on registering an ML pickle model into Azure Machine Learning Service in terms of the size of the pickle file?  <\/p>\n<p>Does it cause latency in realtime data processing and getting the prediction results from the pickle file if we have a  model that let's say it 5MB and the other one is 500MB (The bigger file has better performance in terms of accuracy)?  <br \/>\nThanks,  <\/p>\n<p>John<\/p>",
        "Challenge_closed_time":1599800075416,
        "Challenge_comment_count":2,
        "Challenge_created_time":1599612419390,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is inquiring about any restrictions on registering an ML pickle model into Azure Machine Learning Service in terms of the size of the pickle file. They are also concerned about the potential latency in real-time data processing and getting prediction results from a larger pickle file, even if it has better performance in terms of accuracy.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/89630\/ml-pickle-file-size-azure-machine-learning-service",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.8,
        "Challenge_reading_time":5.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":52.1266738889,
        "Challenge_title":"ML Pickle file size Azure Machine Learning Service",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":79,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=ccc37df1-9b57-4e84-920a-f92d8316aa7c\">@JJA  <\/a> Thanks, For ACI we recommend not using a model over 1GB in size.    <br \/>\nFor AKS you are limited by the memory resources that you request for your service, minus about 500mb for the running python process in the pod.    <\/p>\n<p>There will be no difference in prediction speed once the model is successfully deployed.    <br \/>\nRegistering will take longer as we have to upload the model, and deploying will take longer as the service must download the model.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.2,
        "Solution_reading_time":6.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":83.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1369156710532,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Japan",
        "Answerer_reputation_count":189.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":2.8227241667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a project in which I do several optuna studies each having around 50 trials.<\/p>\n<p>The optuna documentation suggests saving each model to a file for later use on <a href=\"https:\/\/optuna.readthedocs.io\/en\/latest\/faq.html#how-to-define-objective-functions-that-have-own-arguments\" rel=\"nofollow noreferrer\">this FAQ section<\/a><\/p>\n<p>What I want is to have all the best models of different studies in a python list. How is that possible?<\/p>\n<p>This is somewhat similar to my code:<\/p>\n<pre><code>def objective(trial, n_epochs, batch_size):\n     params = {\n              'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n              'optimizer': trial.suggest_categorical(&quot;optimizer&quot;, [&quot;Adam&quot;, &quot;RMSprop&quot;, &quot;SGD&quot;]),\n              'n_epochs': n_epochs,\n              'batch_size': batch_size\n              }\n     model = clp_network(trial)\n     accuracy, model = train_and_evaluate(params, model, trial) # THIS LINE\n     return accuracy\n<\/code><\/pre>\n<pre><code>     for i in range(50):\n           study = optuna.create_study(direction=&quot;maximize&quot;, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner())\n           study.optimize(lambda trial: objective(trial, e, b), n_trials=50, show_progress_bar=True)\n<\/code><\/pre>\n<p>I would like to either save the <code>model<\/code> variable in the line marked <strong>THIS LINE<\/strong>, or somehow get the best model as a variable from the study.<\/p>",
        "Challenge_closed_time":1661860813547,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661850651740,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is conducting several optuna studies, each with around 50 trials, and wants to save all the best models of different studies in a Python list. The user is following the optuna documentation to save each model to a file for later use but wants to save the model variable in the code or get the best model as a variable from the study.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73539873",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":16.6,
        "Challenge_reading_time":18.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":2.8227241667,
        "Challenge_title":"saving trained models in optuna to a variable",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":23.0,
        "Challenge_word_count":145,
        "Platform":"Stack Overflow",
        "Poster_created_time":1612517804323,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Babol, Mazandaran, Iran",
        "Poster_reputation_count":125.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>The easiest way is to define a global variable to store a model for each trial as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import optuna\nfrom collections import defaultdict\n\n\nmodels = defaultdict(dict)\n\ndef objective(t):\n    model = t.suggest_int(&quot;x&quot;, 0, 100)\n    models[t.study.study_name][t.number] = model\n    \n    return model\n\nfor _ in range(10):\n    s = optuna.create_study()\n    s.optimize(objective, n_trials=10)\n\n<\/code><\/pre>\n<p>However I reckon this approach is not scalable in terms of memory space, so I'd suggest removing non-best models after each <code>optimize<\/code> call or saving models on an external file as mentioned in Optuna's FAQ.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.8,
        "Solution_reading_time":8.49,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":82.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4770.0555555556,
        "Challenge_answer_count":0,
        "Challenge_body":"error: no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Challenge_closed_time":1632465008000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1615292808000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The SageMaker Operator Types fail the KubeBuilder V2 custom CRD definition validation check due to unescaped regex patterns. The user expected KubeBuilder to generate a CRD specification that includes AWS SageMaker Operator Types. The issue can be resolved by escaping the regex pattern with quotes.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/174",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.3,
        "Challenge_reading_time":3.66,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":49.0,
        "Challenge_repo_issue_count":205.0,
        "Challenge_repo_star_count":144.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":4770.0555555556,
        "Challenge_title":"error: no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":23,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for using amazon-sagemaker-operator-for-k8s. Please help us with the steps to replicate the issue, especially the installation\r\n\r\nOfficial documentation for reference: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/amazon-sagemaker-operators-for-kubernetes.html I ran into this issue while myself and was resolved by making sure the SageMaker operator was applied and running by verifying with kubectl -n sagemaker-k8s-operator-system get pods Closing since there has been no activity in 90+ days",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":18.2,
        "Solution_reading_time":6.49,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":60.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":64.0951252778,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm using SageMaker Studio, and I have my data files as well as a requirements.txt organized under my home directory. All works fine when I run notebook kernels interactively: they can access my files just fine. However, when I create a \"notebook job\", it doesn't seem to have access to any of my files. Is there a way to give my notebook job access to the same file system as my interactive notebooks?\n\nAfter I run a job, I see that a folder for the job was created within the input S3 bucket, and within that folder there's a \"input\/\" subfolder. But I don't know how to predict the name of the temp folder created for the job, so it doesn't seem like I could myself drop additional inputs in there, even if I wanted to. And if I could, how would I find them, at run-time?\n\nCould sure use guidance on how my notebook jobs can access input files.\n\nThanks,\n\nChris",
        "Challenge_closed_time":1671466765571,
        "Challenge_comment_count":1,
        "Challenge_created_time":1671236023120,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in accessing data files and requirements.txt organized under their home directory in SageMaker Studio when running a \"notebook job\". While interactive notebook kernels can access the files, the notebook job does not seem to have access to them. The user is seeking guidance on how to give the notebook job access to the same file system as the interactive notebooks.",
        "Challenge_last_edit_time":1671582880756,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUqh6oq1d6QbKzNSszEwPm0g\/can-sagemaker-notebook-jobs-access-studio-storage",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.5,
        "Challenge_reading_time":10.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":64.0951252778,
        "Challenge_title":"Can SageMaker notebook jobs access studio storage?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":68.0,
        "Challenge_word_count":168,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi Chris, the option to use input files is to directly use the S3 URIs in the notebook itself, i.e., instead of reading from an `inputs` folder in your local EFS storage (which doesn't get copied over to `inputs` folder for the training job), read the inputs directly from the S3 URI. If the inputs will be dynamic for your notebook jobs, use parameterized executions (reference - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-auto-run-troubleshoot-override.html)",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1671466784964,
        "Solution_link_count":1.0,
        "Solution_readability":13.2,
        "Solution_reading_time":5.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":11.9719969444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a new experiment in Azure Machine Learning and added two datasets by manually uploading csv's.<\/p>\n\n<ul>\n<li>One is from a customer of which I'd like to predict which products he will order next. <\/li>\n<li>The second dataset has the same type of data, only then from all other customers as reference for learning.<\/li>\n<\/ul>\n\n<p>I have <code>productid<\/code>, <code>amount<\/code>, and <code>orderdate<\/code> and <code>orderid<\/code> for grouping and putting it on a timeframe.\nThe customer (dataset one) is always several months behind with ordering the latest products. therefor I added the dataset two with all other customers as reference.<\/p>\n\n<p>Also because the reference can tell which products are more popular (ordered more and by several customers) so perhaps I should add a customerid column to the dataset.<\/p>\n\n<p>I know how to start and get the data in, and I do know that it is common to split the data for training, feed it to the train model with a <code>Ilearnerdotnet<\/code> type and give the output to the score model and evaluate the model.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/qH7lb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qH7lb.png\" alt=\"current experiment\"><\/a>\nI do not know how to choose a classification type and how this can give an output for the next three months of order. I have read some tutorials, but I just need someone who can give me some pointers.<\/p>\n\n<p><strong>edit<\/strong> I have added the customerid to the dataset so that I have just one set now which I should split to focus on a specific customer.\n<strong>edit2<\/strong> found these templates. will look into it <a href=\"https:\/\/stackoverflow.com\/a\/36552849\/169714\">https:\/\/stackoverflow.com\/a\/36552849\/169714<\/a><\/p>",
        "Challenge_closed_time":1463090186116,
        "Challenge_comment_count":0,
        "Challenge_created_time":1463047086927,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has created an experiment in Azure Machine Learning to predict which products a customer will order next. They have added two datasets, one from the customer and the other from all other customers as reference for learning. The user has the necessary data, but they do not know how to choose a classification type and how to get an output for the next three months of order. They have added a customerid column to the dataset and are looking for guidance.",
        "Challenge_last_edit_time":1495541203567,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37183494",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":8.5,
        "Challenge_reading_time":22.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":11.9719969444,
        "Challenge_title":"Azure machine learning predict order for customer",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":845.0,
        "Challenge_word_count":263,
        "Platform":"Stack Overflow",
        "Poster_created_time":1252330528847,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Netherlands",
        "Poster_reputation_count":5867.0,
        "Poster_view_count":1692.0,
        "Solution_body":"<p>Go over this <a href=\"http:\/\/download.microsoft.com\/download\/0\/5\/A\/05AE6B94-E688-403E-90A5-6035DBE9EEC5\/machine-learning-basics-infographic-with-algorithm-examples.pdf\" rel=\"nofollow\">http:\/\/download.microsoft.com\/download\/0\/5\/A\/05AE6B94-E688-403E-90A5-6035DBE9EEC5\/machine-learning-basics-infographic-with-algorithm-examples.pdf<\/a><\/p>\n\n<p>If above infographic doesn't help, then you can try all of the learners by going over this experiment and use the one with best results - <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Algo-Evaluater-Compare-Performance-of-Multiple-Algos-against-Your-Data-1\" rel=\"nofollow\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Algo-Evaluater-Compare-Performance-of-Multiple-Algos-against-Your-Data-1<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":46.5,
        "Solution_reading_time":10.69,
        "Solution_score_count":3.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":58.8322222222,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm working on time-series modeling. I'm comparing battle-of-algorithms against the autopilot machine learning approach to identify the model that best fits my use case.\nI understand that Amazon SageMaker Autopilot doesn't work with time series.\nIs there an alternative library or algorithm in the AWS ecosystem that implements battle-of-algorithms for time series?",
        "Challenge_closed_time":1601883088000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601671292000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is working on time-series modeling and is comparing the battle-of-algorithms against the autopilot machine learning approach. However, they have found that Amazon SageMaker Autopilot does not work with time series. The user is looking for an alternative library or algorithm in the AWS ecosystem that implements battle-of-algorithms for time series.",
        "Challenge_last_edit_time":1668529103319,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUxaXaoPpqRyGdkh6aKK9uew\/can-i-use-sagemaker-autopilot-for-my-time-series-modeling",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.3,
        "Challenge_reading_time":5.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":58.8322222222,
        "Challenge_title":"Can I use SageMaker Autopilot for my time-series modeling?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":319.0,
        "Challenge_word_count":60,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Amazon SageMaker Autopilot currently supports regression, binary classification, and multi-class classification. SageMaker supports only tabular data formatted in files with comma-separated values. For more information, see [Automate model development with Amazon SageMaker Autopilot][1].\n\n[1]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/autopilot-automate-model-development.html\n\nFor your use case, you can use [Amazon Forecast][2]. Amazon Forecast includes the [AutoML][3] feature that trains different models with your target time series, related time series, and item metadata. Amazon Forecast then uses the model with the best accuracy metrics.\n\n[2]: https:\/\/docs.aws.amazon.com\/forecast\/latest\/dg\/what-is-forecast.html\n[3]: https:\/\/docs.aws.amazon.com\/forecast\/latest\/dg\/automl.html",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925587776,
        "Solution_link_count":3.0,
        "Solution_readability":18.4,
        "Solution_reading_time":10.44,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1321893442023,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1611.0,
        "Answerer_view_count":189.0,
        "Challenge_adjusted_solved_time":367.2256972222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running a Python script with Tensorflow in Amazon Sagemaker notebook instance.  I have no trouble writing to the storage in the notebook normally, but for some reason I am unsuccessful when trying to save Tensorflow model checkpoints.  This code previously worked before it was ported to Sagemaker.<\/p>\n\n<p>Below is a reduced version of my code:<\/p>\n\n<pre><code>bucket = 'sagemaker-complaints-data'    \nprefix = 'DeepTestV2' # place to upload training files within the bucket\ntimestamp = str(int(time()))\nout_dir = os.path.abspath(os.path.join(bucket, prefix, \"runs\", timestamp))\ncheckpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"model\")\npath = saver.save(sess, checkpoint_prefix, global_step=current_step)\nprint(\"Saved model checkpoint to {}\\n\".format(path))\n<\/code><\/pre>\n\n<p>No errors are being thrown and the print statement is outputting the correct path.  I have researched whether there are any known issues with using checkpoints in Sagemaker but have come across literally no posts describing this.<\/p>",
        "Challenge_closed_time":1518715595183,
        "Challenge_comment_count":1,
        "Challenge_created_time":1517393582673,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with saving Tensorflow model checkpoints to an Amazon Sagemaker notebook instance. The code previously worked before it was ported to Sagemaker, and the user is not receiving any error messages. The print statement is outputting the correct path, and the user has researched whether there are any known issues with using checkpoints in Sagemaker but has found no relevant posts.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48539564",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":14.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":367.2256972222,
        "Challenge_title":"Tensorflow - Checkpoints not saving to Sagemaker Notebook Instance",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":690.0,
        "Challenge_word_count":136,
        "Platform":"Stack Overflow",
        "Poster_created_time":1321893442023,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1611.0,
        "Poster_view_count":189.0,
        "Solution_body":"<p>I have found out where this is - for some reason \"checkpoints\" seems to be a reserved word - changing the word to \"checks\" allowed me to write the folder.  Hope this helps someone!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.8,
        "Solution_reading_time":2.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":32.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":22.8624508333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am attempting to develop an Azure ML experiment that uses R to perform predictions of a continuous response variable. The initial experiment is relatively simple, incorporating only a few experiment items, including \"Create R Model\", \"Train Model\" and \"Score Model\", along with some data input.<\/p>\n\n<p>I have written a training script and a scoring script, both of which appear to execute without errors when I run the experiment within ML Studio. However, when I examine the scored dataset, the score values are all missing values. So I am concerned that my scoring script could be returning scores incorrectly. Can anyone advise what type I should be returning? Is it meant to be a single column data.frame, or something else?<\/p>\n\n<p>It is also possible that my scores are not being properly calculated within the scoring script, although I have run the training and scoring scripts within R Studio, which shows the expected results. It would also be helpful if someone could suggest how to perform debugging of my scoring script in some way, so that I could determine whereabouts the code is failing to behave as expected.<\/p>\n\n<p>Thanks, Paul<\/p>",
        "Challenge_closed_time":1464675335140,
        "Challenge_comment_count":0,
        "Challenge_created_time":1464593030317,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is developing an Azure ML experiment that uses R to predict a continuous response variable. The training and scoring scripts appear to execute without errors, but the score values are all missing when examining the scored dataset. The user is unsure of what type should be returned from the scoring script and is seeking advice on how to debug the script.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37519858",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.8,
        "Challenge_reading_time":14.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":22.8624508333,
        "Challenge_title":"What type should the returned scores from an R scoring script?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":238.0,
        "Challenge_word_count":199,
        "Platform":"Stack Overflow",
        "Poster_created_time":1464571689107,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Try using this sample and compare with yours - <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Compare-Sample-5-in-R-vs-Azure-ML-1\" rel=\"nofollow\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Compare-Sample-5-in-R-vs-Azure-ML-1<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":41.8,
        "Solution_reading_time":3.6,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1500629225150,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5939.0,
        "Answerer_view_count":886.0,
        "Challenge_adjusted_solved_time":2.1350308333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am taking my first steps into the Terraform world so please be gentle with me. I have a user with AmazonSageMakerFullAccess, which I stored via AWS CLI in a profile called terraform. I can create an S3 bucket as follows no problem referring this user in Windows in VSC:<\/p>\n<pre><code>provider &quot;aws&quot; {\n    region = &quot;eu-west-2&quot;\n    shared_credentials_files = [&quot;C:\\\\Users\\\\amazinguser\\\\.aws\\\\credentials&quot;]\n    profile = &quot;terraform&quot;\n}\n\nresource &quot;aws_s3_bucket&quot; &quot;b&quot; {\n  bucket = &quot;blabla-test-bucket&quot;\n\n  tags = {\n    Name        = &quot;amazing_tag&quot;\n    Environment = &quot;dev&quot;\n  }\n}\n<\/code><\/pre>\n<p>I try to implement <a href=\"https:\/\/github.com\/dkhundley\/terraform-sagemaker-tutorial\/tree\/main\/Part%202a%20-%20Creating%20a%20SageMaker%20Notebook\" rel=\"nofollow noreferrer\">this<\/a> documented <a href=\"https:\/\/towardsdatascience.com\/terraform-sagemaker-part-2a-creating-a-custom-sagemaker-notebook-instance-1d68c90b192b\" rel=\"nofollow noreferrer\">here<\/a> and try to this:<\/p>\n<pre><code>resource &quot;aws_sagemaker_notebook_instance&quot; &quot;notebook_instance&quot; {\n  name = &quot;titanic-sagemaker-byoc-notebook&quot;\n  role_arn = aws_iam_role.notebook_iam_role.arn\n  instance_type = &quot;ml.t2.medium&quot;\n  #lifecycle_config_name = aws_sagemaker_notebook_instance_lifecycle_configuration.notebook_config.name\n  #default_code_repository = aws_sagemaker_code_repository.git_repo.code_repository_name\n}\n<\/code><\/pre>\n<p>I am a bit confused about the role_arn which is defined here:<\/p>\n<p><a href=\"https:\/\/github.com\/dkhundley\/terraform-sagemaker-tutorial\/blob\/main\/Part%202a%20-%20Creating%20a%20SageMaker%20Notebook\/terraform\/iam.tf\" rel=\"nofollow noreferrer\">https:\/\/github.com\/dkhundley\/terraform-sagemaker-tutorial\/blob\/main\/Part%202a%20-%20Creating%20a%20SageMaker%20Notebook\/terraform\/iam.tf<\/a><\/p>\n<p>Can I not use the above user? Thanks!<\/p>",
        "Challenge_closed_time":1649687933128,
        "Challenge_comment_count":9,
        "Challenge_created_time":1649680247017,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a SageMaker notebook instance via Terraform using a user with AmazonSageMakerFullAccess. They were able to create an S3 bucket but are confused about the role_arn required for the notebook instance and whether they can use the same user.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71827884",
        "Challenge_link_count":4,
        "Challenge_participation_count":10,
        "Challenge_readability":22.5,
        "Challenge_reading_time":26.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":2.1350308333,
        "Challenge_title":"create sagemaker notebook instance via Terraform",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":338.0,
        "Challenge_word_count":129,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>AWS services trying to call other AWS services and perform actions are not allowed to do so by default. For example, SageMaker Notebooks are basically EC2 instances. In order for SageMaker to create EC2 instances, it has to have a policy which allows e.g., injecting ENIs to a VPC. Since you probably do not want to do all that by yourself (it is a managed Notebook service after all), you have to give SageMaker permissions to perform actions on your behalf. Enter <strong>execution roles<\/strong>. For SageMaker, you can read more in [1]. Other services that you will commonly find using execution roles are Lambda, ECS and many others. An IAM role usually consists of two parts:<\/p>\n<ol>\n<li>Trust relationship (I like to call it trust policy)<\/li>\n<li>Permissions policy<\/li>\n<\/ol>\n<p>The first one decides which principal (AWS identifier, Service etc. [2]) will be able to assume the role. In your example, that is:<\/p>\n<pre><code>data &quot;aws_iam_policy_document&quot; &quot;sm_assume_role_policy&quot; {\n  statement {\n    actions = [&quot;sts:AssumeRole&quot;]\n    \n    principals {\n      type = &quot;Service&quot;\n      identifiers = [&quot;sagemaker.amazonaws.com&quot;]\n    }\n  }\n}\n<\/code><\/pre>\n<p>What this policy says is &quot;I am going to allow SageMaker (which is of type <code>Service<\/code>) to assume any role to which this policy is attached and perform actions that are defined in the permissions policy&quot;. The permissions policy is:<\/p>\n<pre><code># Attaching the AWS default policy, &quot;AmazonSageMakerFullAccess&quot;\nresource &quot;aws_iam_policy_attachment&quot; &quot;sm_full_access_attach&quot; {\n  name = &quot;sm-full-access-attachment&quot;\n  roles = [aws_iam_role.notebook_iam_role.name]\n  policy_arn = &quot;arn:aws:iam::aws:policy\/AmazonSageMakerFullAccess&quot;\n}\n<\/code><\/pre>\n<p>Without going into too much details about what the AWS managed policy for SageMaker does, it is enough to see the <code>FullAccess<\/code> part for it to be clear. What you could do if you want to be extra careful is to define a customer managed policy [3] for SageMaker notebooks. This permissions policy will be attached to the IAM role(s) defined in the <code>roles<\/code> argument. Note that it is a list, so multiple roles can have the same permissions policy attached.<\/p>\n<p>Last, but not the least, the glue between the trust and permissions policy is the role itself:<\/p>\n<pre><code>resource &quot;aws_iam_role&quot; &quot;notebook_iam_role&quot; {\n  name = &quot;sm_notebook_role&quot;\n  assume_role_policy = data.aws_iam_policy_document.sm_assume_role_policy.json\n}\n<\/code><\/pre>\n<p>As you can see, the <code>assume_role_policy<\/code> is the policy which will allow SageMaker to perform actions in the AWS account based on the permissions defined in the permissions policy.<\/p>\n<p>This topic is much more complex than in this answer, but it should give you a fair amount of information.<\/p>\n<p>NOTE: In theory, the same role accessing information in AWS and running the AWS API actions when using Terraform could be used for SageMaker, but I would strongly advise against it. Always keep in mind separation of concerns and principle of least privilege.<\/p>\n<hr \/>\n<p>[1] <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html#sagemaker-roles-create-execution-role\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html#sagemaker-roles-create-execution-role<\/a><\/p>\n<p>[2] <a href=\"https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/reference_policies_elements_principal.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/reference_policies_elements_principal.html<\/a><\/p>\n<p>[3] <a href=\"https:\/\/docs.aws.amazon.com\/acm\/latest\/userguide\/authen-custmanagedpolicies.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/acm\/latest\/userguide\/authen-custmanagedpolicies.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":14.1,
        "Solution_reading_time":50.21,
        "Solution_score_count":2.0,
        "Solution_sentence_count":28.0,
        "Solution_word_count":435.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1505653015243,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1128.0,
        "Answerer_view_count":71.0,
        "Challenge_adjusted_solved_time":0.1140905556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to export a machine learning model I created in Azure Machine Learning studio. One of the required input is \"Path to blob beginning with container\"<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/xvSDd.png\" alt=\"Here is the screenshoot from azure export\"><\/p>\n\n<p>How do I find this path? I have already created a blob storage but I have no idea how to find the path to the blob storage. <\/p>",
        "Challenge_closed_time":1538035308943,
        "Challenge_comment_count":0,
        "Challenge_created_time":1538034898217,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to export a machine learning model from Azure Machine Learning studio and needs to provide the \"Path to blob beginning with container\" as input. However, the user is unsure how to find this path even though they have already created a blob storage.",
        "Challenge_last_edit_time":1538038508827,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52532078",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":6.4,
        "Challenge_reading_time":5.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.1140905556,
        "Challenge_title":"How to find the path to blob?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":15096.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>you should be able to find this from the Azure portal. Open the storage account, drill down into blobs, then your container. Use properties for the context menu, the URL should be the path ?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/r5hxi.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/r5hxi.jpg\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.3,
        "Solution_reading_time":4.7,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":44.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1280527017200,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3035.0,
        "Answerer_view_count":129.0,
        "Challenge_adjusted_solved_time":98.7025555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have started exploring AWS SageMaker starting with these <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/blazingtext_word2vec_subwords_text8\" rel=\"nofollow noreferrer\">examples provided by AWS<\/a>. I then made some modifications to this particular setup so that it uses the data from my use case for training.<\/p>\n\n<p>Now, as I continue to work on this model and tuning, after I delete the inference endpoint once, I would like to be able to recreate the same endpoint -- even after stopping and restarting the notebook instance (so the notebook \/ kernel session is no longer valid) -- using the already trained model artifacts that gets uploaded to S3 under \/output folder.<\/p>\n\n<p>Now I cannot simply jump directly to this line of code:<\/p>\n\n<pre><code>bt_endpoint = bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>I did some searching -- including <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/blazingtext_hosting_pretrained_fasttext\" rel=\"nofollow noreferrer\">amazon's own example of hosting pre-trained models<\/a>, but I am a little lost. I would appreciate any guidance, examples, or documentation that I could emulate and adapt to my case.<\/p>",
        "Challenge_closed_time":1539622672603,
        "Challenge_comment_count":2,
        "Challenge_created_time":1539267343403,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a model on AWS SageMaker using their own data and made modifications to the setup. They want to recreate the inference endpoint using the already trained model artifacts that were uploaded to S3 under the \/output folder, even after stopping and restarting the notebook instance. The user is seeking guidance, examples, or documentation to help them achieve this.",
        "Challenge_last_edit_time":1539637545790,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52762367",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":14.1,
        "Challenge_reading_time":17.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":98.7025555556,
        "Challenge_title":"Re-hosting a trained model on AWS SageMaker",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1916.0,
        "Challenge_word_count":157,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432179256928,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":17.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Your comment is correct - you can re-create an Endpoint given an existing EndpointConfiguration. This can be done via the console, the AWS CLI, or the SageMaker boto client.<\/p>\n\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-endpoint.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-endpoint.html<\/a><\/li>\n<li><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_endpoint\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_endpoint<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":32.9,
        "Solution_reading_time":9.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":38.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1575516017430,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Mexico City, CDMX, M\u00e9xico",
        "Answerer_reputation_count":4882.0,
        "Answerer_view_count":260.0,
        "Challenge_adjusted_solved_time":48.8901825,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am very new to AWS and the cloud environment. I am a machine learning engineer, I am planning to build a custom CNN into the AWS environment to predict a given image has an iPhone present or not.<\/p>\n<p><strong>What I have done:<\/strong><\/p>\n<p><em><strong>Step 1:<\/strong><\/em><\/p>\n<p>I have created a S3 bucket for iPhone classifier with the below folder structure :<\/p>\n<pre><code> Iphone_Classifier &gt; Train &gt; Yes_iphone_images &gt; 1000 images\n                           &gt; No_iphone_images  &gt; 1000 images\n\n                   &gt; Dev   &gt; Yes_iphone_images &gt; 100 images\n                           &gt; No_iphone_images  &gt; 100 images\n\n                   &gt; Test  &gt; 30 random images\n<\/code><\/pre>\n<p>Permission - &gt; <strong>Block all public access<\/strong><\/p>\n<p><em><strong>Step 2:<\/strong><\/em><\/p>\n<p>Then I go to Amazon Sagemaker, and create an instance:<\/p>\n<p>I select the following<\/p>\n<pre><code> Name: some-xyz,\n Type: ml.t2.medium\n IAM : created new IAM role ( root access was enabled.)\n others: All others were in default\n<\/code><\/pre>\n<p>Then the notebook instance was created and opened.<\/p>\n<p><em><strong>Step 3:<\/strong><\/em><\/p>\n<p>Once I had the instance opened,<\/p>\n<pre><code>1. I used to prefer - conda_tensorflow2_p36 as interpreter\n2. Created a new Jupyter notebook and stated.\n3. I checked image classification examples but was confused, and most others used CSV files, but I want to retrieve images from S3 buckets. \n<\/code><\/pre>\n<p><em><strong>Question:<\/strong><\/em><\/p>\n<pre><code>1. How simply can we access the S3 bucket image dataset from the Jupiter Instances of Sagemaker? \n2. I exactly need the reference code to access the S3 bucket images. \n3. Is it a good approach to copy the data to the notebook or is it better to work from the S3 bucket.\n<\/code><\/pre>\n<p><em><strong>What I have tried was:<\/strong><\/em><\/p>\n<pre><code>import boto3\nclient = boto3.client('s3')\n\n# I tried this one and failed\n#path = 's3:\/\/iphone\/Train\/Yes_iphone_images\/100.png'\n\n# I tried this one and failed\npath = 's3:\/\/iphone\/Test\/10.png'\n\n# I uploaded to the notebook instance an image file and when I try to read it works\n#path = 'thiyaga.jpg'\nprint(path)\n\nimport cv2\nfrom matplotlib import pyplot as plt\nprint(cv2.__version__)\nplt.imshow(img)\n<\/code><\/pre>",
        "Challenge_closed_time":1597165677847,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596987527557,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is a machine learning engineer who wants to build a custom CNN in AWS to predict whether an image has an iPhone or not. They have created an S3 bucket with a folder structure for the iPhone classifier and have created a Jupyter notebook instance in Amazon Sagemaker. The user is seeking guidance on how to access the S3 bucket image dataset from the Jupyter instance and is looking for reference code to do so. They have tried importing boto3 and using the S3 path to access the images but have been unsuccessful. The user is also unsure whether it is better to copy the data to the notebook or work from the S3 bucket.",
        "Challenge_last_edit_time":1596989673190,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63328246",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.6,
        "Challenge_reading_time":28.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":49.4861916667,
        "Challenge_title":"How to read bucket image from AWS S3 into Sagemaker Jupyter Instance",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2065.0,
        "Challenge_word_count":315,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324277062387,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Finland",
        "Poster_reputation_count":677.0,
        "Poster_view_count":161.0,
        "Solution_body":"<p>If your image is binary-encoded, you could try this:<\/p>\n<pre><code>import boto3 \nimport matplotlib.pyplot as plt \n\n# Define Bucket and Key \ns3_bucket, s3_key = 'YOUR_BUCKET', 'YOUR_IMAGE_KEY'\n\nwith BytesIO() as f:\n    boto3.client(&quot;s3&quot;).download_fileobj(Bucket=s3_bucket, Key=s3_key, Fileobj=f)\n    f.seek(0)\n    img = plt.imread(f, format='png')\n<\/code><\/pre>\n<p>in other case, the following code works out (based on the <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/1.9.42\/guide\/s3-example-download-file.html\" rel=\"nofollow noreferrer\">documentation<\/a>):<\/p>\n<pre><code>s3 = boto3.resource('s3')\n\nimg = s3.Bucket(s3_bucket).download_file(s3_key, 'local_image.jpg')\n<\/code><\/pre>\n<p>In both cases, you can visualize the image with <code>plt.imshow(img)<\/code>.<\/p>\n<p>In your path example <code>path = 's3:\/\/iphone\/Test\/10.png'<\/code>, the bucket and key will be <code>s3_bucket = 'iphone'<\/code> and <code>s3_key=Test\/10.png<\/code><\/p>\n<p>Additional Resources: <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/guide\/s3-example-download-file.html\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/guide\/s3-example-download-file.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":17.9,
        "Solution_reading_time":16.23,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1124.7847222222,
        "Challenge_answer_count":0,
        "Challenge_body":"We should create a instance of MLflow for each project in order to see the experiments related to the current project.\r\n\r\n- [x] Create project operator to deploy a MLFlow instance for each project.\r\n- [x] Update KDL APP API to create the KDLProject custom resource in k8s.\r\n- [x] Update kdlctl.sh adding project-operator docker image building.\r\n- [x] Add project-operator to KDL server helm chart.\r\n- [x] Add github workflows to publish the project-operator in docker hub.",
        "Challenge_closed_time":1623230615000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619181390000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to create a model in MLflowCatalog due to an error message stating that the registered model with the given name does not exist.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/379",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":8.4,
        "Challenge_reading_time":6.3,
        "Challenge_repo_contributor_count":16.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":909.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1124.7847222222,
        "Challenge_title":"All MLflow experiments are visible for any user",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":80,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":15.4171983334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am looking at Azure's training modules and it states I can learn no-code models with Azure, but it also tells me I should know python. I'm a little confused at where I should spend time training in most efficient pathway. My goal is to just do predictive modeling within Azure. I have technical\/IT literacy however coding is at a basic level.   <\/p>\n<p>Ideally id like some sort of Certification, if possible from just &quot;Create no-code predictive models with Azure Machine Learning&quot;  <\/p>\n<p>Is &quot;Microsoft Certified: Azure Data Scientist Associate&quot; going to require a lot of pre work on python\/torch\/tensor? I'd ideally like Azure to be my entry. <\/p>",
        "Challenge_closed_time":1592924808467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592869306553,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is confused about the most efficient pathway to learn code-free predictive modeling in Azure, as they have basic coding skills and want to obtain a certification. They are unsure if the \"Microsoft Certified: Azure Data Scientist Associate\" certification will require pre-work on Python\/Torch\/Tensor and would prefer Azure to be their entry point.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/38841\/pathway-for-code-free-predictive-modeling",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":8.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":15.4171983334,
        "Challenge_title":"Pathway for code free predictive modeling",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":114,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Thanks for reaching out. Azure machine learning has a drag and drop interface (Designer) that supports code free predictive modeling. <a href=\"https:\/\/learn.microsoft.com\/en-us\/learn\/paths\/create-no-code-predictive-models-azure-machine-learning\/\">Create no-code predictive models with Azure Machine Learning<\/a> training modules is a great starting point and provides a pathway for <a href=\"https:\/\/learn.microsoft.com\/en-us\/learn\/certifications\/azure-data-scientist\">Azure Data Scientist Associate certification<\/a>. However, you also need programming experience and familiarity with various data science processes\/principles to be successful on the certification exam.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.9,
        "Solution_reading_time":8.98,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":68.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":28.1985233334,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I\u2019m trying to train a model, but I keep receiving an error that tells me \u201ctrain.py: error: unrecognized arguments: --save_period 1.\u201d<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755.png\" data-download-href=\"\/uploads\/short-url\/8oz1v5B3P0W95o6FWMDht4yfVLD.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755_2_690x337.png\" alt=\"image\" data-base62-sha1=\"8oz1v5B3P0W95o6FWMDht4yfVLD\" width=\"690\" height=\"337\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755_2_690x337.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755_2_1035x505.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1326\u00d7648 66 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><br>\nWhat issue do I have here?  Thanks in advance.<\/p>",
        "Challenge_closed_time":1636785650863,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636684136179,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to train a model, specifically with the \"--save_period 1\" argument. They are seeking assistance in resolving the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/save-period-not-working\/1264",
        "Challenge_link_count":6,
        "Challenge_participation_count":3,
        "Challenge_readability":28.8,
        "Challenge_reading_time":22.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":28.1985233334,
        "Challenge_title":"Save_period Not Working",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":264.0,
        "Challenge_word_count":71,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I think you have  a typo. According to the usage info, the argument name is <code>--save-period<\/code>  , not <code>--save_period<\/code><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.2,
        "Solution_reading_time":1.81,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1250158552416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Challenge_adjusted_solved_time":6.8599055556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to execute Python script from Azure machine learning studio. I had a script bundle(zip file) connect to the Python script as input. There are python files, txt files and other type of files in this zip file. My question is how do I get the file path from this zip file. For example, if I have language model in this  zip file, named lm.pcl, what's the file path of this language model? \nThanks!<\/p>",
        "Challenge_closed_time":1539582490283,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539557794623,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to execute a Python script from Azure machine learning studio using a script bundle (zip file) that contains various types of files including Python and txt files. The user is seeking guidance on how to access the file path of a specific language model (lm.pcl) within the zip file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52807787",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.7,
        "Challenge_reading_time":5.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":6.8599055556,
        "Challenge_title":"Azure machine learning studio get access to the file in upload zip file",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":679.0,
        "Challenge_word_count":88,
        "Platform":"Stack Overflow",
        "Poster_created_time":1337362023536,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":581.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>They're available under the <code>.\/Script Bundle<\/code> directory. For example, if you were to load a pickled model from the zip file, you'd write something along these lines:<\/p>\n\n<pre><code>import pandas as pd\nimport pickle\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n\n    model = pickle.load(open(\".\/Script Bundle\/model.pkl\", \"rb\"))\n    ...\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":4.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":3.9360030556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created a simply model and then registered in azure. How can I make a prediction?<\/p>\n<pre><code>from sklearn import svm\nimport joblib\nimport numpy as np\n\n# customer ages\nX_train = np.array([50, 17, 35, 23, 28, 40, 31, 29, 19, 62])\nX_train = X_train.reshape(-1, 1)\n# churn y\/n\ny_train = [&quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;]\n\nclf = svm.SVC(gamma=0.001, C=100.)\nclf.fit(X_train, y_train)\n\njoblib.dump(value=clf, filename=&quot;churn-model.pkl&quot;)\n<\/code><\/pre>\n<p>Registration:<\/p>\n<pre><code>from azureml.core import Workspace\nws = Workspace.get(name=&quot;myworkspace&quot;, subscription_id='My_subscription_id', resource_group='ML_Lingaro')\n\nfrom azureml.core.model import Model\nmodel = Model.register(workspace=ws, model_path=&quot;churn-model.pkl&quot;, model_name=&quot;churn-model-test&quot;)\n<\/code><\/pre>\n<p>Prediction:<\/p>\n<pre><code>from azureml.core.model import Model\nimport os\n\nmodel = Model(workspace=ws, name=&quot;churn-model-test&quot;)\nX_test = np.array([50, 17, 35, 23, 28, 40, 31, 29, 19, 62])\nmodel.predict(X_test) ???? \n<\/code><\/pre>\n<p>Error: <code>AttributeError: 'Model' object has no attribute 'predict'<\/code><\/p>",
        "Challenge_closed_time":1609737626627,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609722575690,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created and registered a model in Azure, but is facing an error while trying to make a prediction using the registered model. The error message states that the 'Model' object has no attribute 'predict'.",
        "Challenge_last_edit_time":1609723457016,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65556574",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":17.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":4.1808158334,
        "Challenge_title":"How to make prediction after model registration in azure?",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":540.0,
        "Challenge_word_count":120,
        "Platform":"Stack Overflow",
        "Poster_created_time":1605834001336,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":73.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>great question -- I also had the same misconception starting out. The missing piece is that there's a difference between model 'registration' and model 'deployment'. Registration is simply for tracking and for easy downloading at a later place and time. Deployment is what you're after, making a model available to be scored against.<\/p>\n<p>There's a <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python&amp;WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">whole section in the docs about deployment<\/a>. My suggestion would be to deploy it locally first for testing.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.8,
        "Solution_reading_time":8.06,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":75.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":29.7938888889,
        "Challenge_answer_count":0,
        "Challenge_body":"## Which example? Describe the issue\r\n\r\nexample:  az ml online-deployment create --name blue --endpoint-name amlarc-runner-simple-849b --resource-group lt-westus2-r6-amlarc-rg --workspace-name lt-westus2-r6-arc-ws --file azureml-examples\/cli\/endpoints\/online\/\/amlarc\/blue-deployment.yml --all-traffic\r\ndescription:\r\nFile \"\/var\/azureml-server\/entry.py\", line 1, in <module>\r\n    import create_app\r\n  File \"\/var\/azureml-server\/create_app.py\", line 3, in <module>\r\n    import aml_framework\r\n  File \"\/var\/azureml-server\/aml_framework.py\", line 9, in <module>\r\n    from synchronous.framework import *\r\n  File \"\/var\/azureml-server\/synchronous\/framework.py\", line 3, in <module>\r\n    from flask import Flask, request, g, Request, Response, Blueprint\r\n  File \"\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/__init__.py\", line 21, in <module>\r\n    from .app import Flask, Request, Response\r\n  File \"\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/app.py\", line 26, in <module>\r\n    from . import cli, json\r\n  File \"\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/json\/__init__.py\", line 21, in <module>\r\n    from itsdangerous import json as _json\r\nImportError: cannot import name 'json' from 'itsdangerous' (\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/itsdangerous\/__init__.py)\r\n\r\n## Additional context\r\n\r\nhttps:\/\/ml.azure.com\/endpoints\/realtime\/amlarc-runner-simple-849b\/logs?wsid=\/subscriptions\/589c7ae9-223e-45e3-a191-98433e0821a9\/resourcegroups\/lt-westus2-r6-amlarc-rg\/workspaces\/lt-westus2-r6-arc-ws&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\r\n\r\n-\r\n",
        "Challenge_closed_time":1645604942000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1645497684000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is reporting a bug related to updating test documentation to connect AzureML with GitHub actions. The steps to replicate the issue involve creating a new AzureML workspace, creating two new clusters, adding subscription ID to GitHub action secrets, installing Azure CLI, creating a Service Principal, and adding the output from the Service Principal as an action secret. The user has not mentioned any specific platform where the issue is happening.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/azureml-examples\/issues\/977",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.3,
        "Challenge_reading_time":24.77,
        "Challenge_repo_contributor_count":135.0,
        "Challenge_repo_fork_count":650.0,
        "Challenge_repo_issue_count":1974.0,
        "Challenge_repo_star_count":882.0,
        "Challenge_repo_watch_count":2756.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":29.7938888889,
        "Challenge_title":"ImportError: cannot import name 'json' from 'itsdangerous' (\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/itsdangerous\/__init__.py)",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":115,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This issue should be mitigated once the PR is merged: https:\/\/github.com\/Azure\/azureml-examples\/pull\/981",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.5,
        "Solution_reading_time":1.38,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1441425578580,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":433.0,
        "Answerer_view_count":56.0,
        "Challenge_adjusted_solved_time":12869.3964275,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I trained my model in Amazon-SageMaker and downloaded it to my local computer. Unfortunately, I don't have any idea how to run the model locally.<\/p>\n\n<p>The Model is in a directory with files like:<\/p>\n\n<pre><code>image-classification-0001.params\nimage-classification-0002.params\nimage-classification-0003.params\nimage-classification-0004.params\nimage-classification-0005.params\nimage-classification-symbol.json\nmodel-shapes.json\n<\/code><\/pre>\n\n<p>Would anyone know how to run this locally with Python, or be able to point me to a resource that could help? I am trying to avoid calling the model using the Amazon API.<\/p>\n\n<p>Edit: The model I used was created with code very similar to this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-fulltraining.ipynb\" rel=\"nofollow noreferrer\">example<\/a>.<\/p>\n\n<p>Any help is appreciated, I will award the bounty to whoever is most helpful, even if they don't completely solve the question. <\/p>",
        "Challenge_closed_time":1520719061830,
        "Challenge_comment_count":3,
        "Challenge_created_time":1520225638010,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has trained a model in Amazon-SageMaker and downloaded it to their local computer, but they are unsure how to run the model locally using Python. They are seeking guidance or resources to help them avoid calling the model using the Amazon API.",
        "Challenge_last_edit_time":1536012091088,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49103679",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":15.7,
        "Challenge_reading_time":14.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":137.0621722223,
        "Challenge_title":"How to Deploy Amazon-SageMaker Locally in Python",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3742.0,
        "Challenge_word_count":122,
        "Platform":"Stack Overflow",
        "Poster_created_time":1441425578580,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":433.0,
        "Poster_view_count":56.0,
        "Solution_body":"<p>Following SRC's advice, I was able to get it to work by following the instructions in this <a href=\"https:\/\/stackoverflow.com\/questions\/47190614\/how-to-load-a-trained-mxnet-model\">question<\/a> and this <a href=\"http:\/\/mxnet.incubator.apache.org\/tutorials\/python\/predict_image.html\" rel=\"nofollow noreferrer\">doc<\/a> which describe how to load a MXnet model.<\/p>\n\n<p>I loaded the model like so:<\/p>\n\n<pre><code>lenet_model = mx.mod.Module.load('model_directory\/image-classification',5)\nimage_l = 64\nimage_w = 64\nlenet_model.bind(for_training=False, data_shapes=[('data',(1,3,image_l,image_w))],label_shapes=lenet_model._label_shapes)\n<\/code><\/pre>\n\n<p>Then predicted using the slightly modified helper functions in the previously linked documentation:<\/p>\n\n<pre><code>import mxnet as mx\nimport matplotlib.pyplot as plot\nimport cv2\nimport numpy as np\nfrom mxnet.io import DataBatch\n\ndef get_image(url, show=False):\n    # download and show the image\n    fname = mx.test_utils.download(url)\n    img = cv2.cvtColor(cv2.imread(fname), cv2.COLOR_BGR2RGB)\n    if img is None:\n         return None\n    if show:\n         plt.imshow(img)\n         plt.axis('off')\n    # convert into format (batch, RGB, width, height)\n    img = cv2.resize(img, (64, 64))\n    img = np.swapaxes(img, 0, 2)\n    img = np.swapaxes(img, 1, 2)\n    img = img[np.newaxis, :]\n    return img\n\ndef predict(url, labels):\n    img = get_image(url, show=True)\n    # compute the predict probabilities\n    lenet_model.forward(DataBatch([mx.nd.array(img)]))\n    prob = lenet_model.get_outputs()[0].asnumpy()\n\n    # print the top-5\n    prob = np.squeeze(prob)\n    a = np.argsort(prob)[::-1]\n\n    for i in a[0:5]:\n       print('probability=%f, class=%s' %(prob[i], labels[i]))\n<\/code><\/pre>\n\n<p>Finally I called the prediction with this code:<\/p>\n\n<pre><code>labels = ['a','b','c', 'd','e', 'f']\npredict('https:\/\/eximagesite\/img_tst_a.jpg', labels )\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1582341918227,
        "Solution_link_count":3.0,
        "Solution_readability":12.5,
        "Solution_reading_time":23.62,
        "Solution_score_count":3.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":169.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1389887039672,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Singapore",
        "Answerer_reputation_count":5854.0,
        "Answerer_view_count":794.0,
        "Challenge_adjusted_solved_time":58.3565702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Can SageMaker Training have training data in NVMe volumes on compatible instances? (eg G4dn and P3dn). If so, if there an appropriate way to programmatically access that data?<\/p>",
        "Challenge_closed_time":1663457094040,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663247010387,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring if SageMaker Training can use training data in NVMe volumes on compatible instances such as G4dn and P3dn, and if there is a way to programmatically access that data.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73731665",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.5,
        "Challenge_reading_time":3.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":58.3565702778,
        "Challenge_title":"Can SageMaker Training have training data in NVMe volumes on compatible instances?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":16.0,
        "Challenge_word_count":39,
        "Platform":"Stack Overflow",
        "Poster_created_time":1507661294190,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":98.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>Yes on all nitro-backed instances EBS volumes that are exposed as NVMe block devices.<\/p>\n<p>In the Sagemaker Python SDK, you can specify the\u00a0<code>volume_size<\/code>\u00a0of the\u00a0<code>SM_TRAINING_CHANNEL<\/code>\u00a0path - the EBS (NVMe backed) will be in that path and when you go to actually run you pass the\u00a0<code>--train_dir<\/code>\u00a0path to your code.<\/p>\n<p>Code example below:<\/p>\n<pre><code>def main(aws_region,s3_location,instance_cout):\n    estimator = TensorFlow(\n        train_instance_type='ml.p3.16xlarge',\n            **train_volume_size=200,**\n        train_instance_count=int(instance_count),\n        framework_version='2.2',\n            py_version='py3',\n        image_name=&quot;231748552833.dkr.ecr.%s.amazonaws.com\/sage-py3-tf-hvd:latest&quot;%aws_region,\n<\/code><\/pre>\n<p>And then in your entry script<\/p>\n<pre><code>train_dir = os.environ.get('SM_CHANNEL_TRAIN')\nsubprocess.call(['python','-W ignore', 'deep-learning-models\/legacy\/models\/resnet\/tensorflow2\/train_tf2_resnet.py', \\\n            &quot;--data_dir=%s&quot;%train_dir, \\\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.2,
        "Solution_reading_time":13.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":79.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1512770138847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":71.702525,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>If I am not using the notebook on AWS but instead just the Sagemaker CLI and want to train a model, can I specify a local path to read from and write to?<\/p>",
        "Challenge_closed_time":1530570724340,
        "Challenge_comment_count":0,
        "Challenge_created_time":1530312595250,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to know if they can use AWS Sagemaker without S3 by specifying a local path to read from and write to while training a model using the Sagemaker CLI.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51110274",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.4,
        "Challenge_reading_time":2.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":71.702525,
        "Challenge_title":"Can I use AWS Sagemaker without S3",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":826.0,
        "Challenge_word_count":39,
        "Platform":"Stack Overflow",
        "Poster_created_time":1528500562963,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>If you use local mode with the SageMaker Python SDK, you can train using local data:<\/p>\n\n<pre><code>from sagemaker.mxnet import MXNet\n\nmxnet_estimator = MXNet('train.py',\n                        train_instance_type='local',\n                        train_instance_count=1)\n\nmxnet_estimator.fit('file:\/\/\/tmp\/my_training_data')\n<\/code><\/pre>\n\n<p>However, this only works if you are training a model locally, not on SageMaker. If you want to train on SageMaker, then yes, you do need to use S3.<\/p>\n\n<p>For more about local mode: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#local-mode\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#local-mode<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.3,
        "Solution_reading_time":8.2,
        "Solution_score_count":4.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":63.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":21.5095830556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>On the web\u201c <a href=\"https:\/\/learn.microsoft.com\/en-au\/dotnet\/machine-learning\/how-to-guides\/matchup-app-infer-net\">https:\/\/learn.microsoft.com\/en-au\/dotnet\/machine-learning\/how-to-guides\/matchup-app-infer-net<\/a> \u201dFor Infer.net Probability programming example of. In this example, there are only wins or losses, no draws. Can you give me an example of a draw, which can be used in the machine learning of E-sports Bo 2 or football, thank you<\/p>",
        "Challenge_closed_time":1593434922092,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593357487593,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking an example of a draw in Infer.net probability programming for machine learning in E-sports Bo 2 or football, as the provided example only includes wins or losses.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/40647\/can-you-give-me-an-example-of-a-draw-with-infer-ne",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":6.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":21.5095830556,
        "Challenge_title":"Can you give me an example of a draw with Infer.net",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":57,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The <a href=\"https:\/\/dotnet.github.io\/infer\/userguide\/Chess%20Analysis.html\">Chess Analysis<\/a> example in the Infer.NET documentation includes draws.  <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.8,
        "Solution_reading_time":2.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":12.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":17.5084694445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to build and push a custom ML model with docker to Amazon SageMaker. I know things are supposed to follow the general structure of being in opt\/ml. But there's no such bucket in Amazon S3??? Am I supposed to create this directory within my container before I build and push the image to AWS? I just have no idea where to put my training data, etc.<\/p>",
        "Challenge_closed_time":1565167576867,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565104546377,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in storing their model's training data and artifacts while building and pushing a custom ML model with docker to Amazon SageMaker. They are unsure about the location of the opt\/ml bucket in Amazon S3 and are seeking guidance on where to store their training data.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57379173",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.9,
        "Challenge_reading_time":5.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":17.5084694445,
        "Challenge_title":"Where do I store my model's training data, artifacts, etc?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1022.0,
        "Challenge_word_count":77,
        "Platform":"Stack Overflow",
        "Poster_created_time":1399251405187,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":145.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>SageMaker is automating the deployment of the Docker image with your code using the convention of channel->local-folder. Everything that you define with a channel in your <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-inputdataconfig\" rel=\"nofollow noreferrer\">input data configuration<\/a>, will be copied to the local Docker file system under <em>\/opt\/ml\/<\/em> folder, using the name of the channel as the name of the sub-folder.<\/p>\n\n<pre><code>{\n\"train\" : {\"ContentType\":  \"trainingContentType\", \n           \"TrainingInputMode\": \"File\", \n           \"S3DistributionType\": \"FullyReplicated\", \n           \"RecordWrapperType\": \"None\"},\n\"evaluation\" : {\"ContentType\":  \"evalContentType\", \n                \"TrainingInputMode\": \"File\", \n                \"S3DistributionType\": \"FullyReplicated\", \n                \"RecordWrapperType\": \"None\"},\n\"validation\" : {\"TrainingInputMode\": \"File\", \n                \"S3DistributionType\": \"FullyReplicated\", \n                \"RecordWrapperType\": \"None\"}\n} \n<\/code><\/pre>\n\n<p>to:<\/p>\n\n<pre><code>\/opt\/ml\/input\/data\/training\n\/opt\/ml\/input\/data\/validation\n\/opt\/ml\/input\/data\/testing\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":26.0,
        "Solution_reading_time":15.0,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1595479476676,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Massachusetts, USA",
        "Answerer_reputation_count":246.0,
        "Answerer_view_count":21.0,
        "Challenge_adjusted_solved_time":538.3183666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use API workflow (python code) to find a model version that has the best metric (for instance, \u201caccuracy\u201d) among several model versions. I understand we can use web UI to do so, but I would love to write python code to achieve this. Could someone help me?<\/p>",
        "Challenge_closed_time":1595480605940,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593542659820,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking help to find the best model version with a specific metric (such as accuracy) using Python code in MLflow API workflow. They are aware that this can be done through the web UI but prefer to achieve it through Python code.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62664183",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":4.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":538.3183666667,
        "Challenge_title":"MLflow: find model version with best metric using python code",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":445.0,
        "Challenge_word_count":60,
        "Platform":"Stack Overflow",
        "Poster_created_time":1419619351727,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":125.0,
        "Poster_view_count":16.0,
        "Solution_body":"<pre><code>import mlflow \nclient = mlflow.tracking.MlflowClient()\nruns = client.search_runs(&quot;my_experiment_id&quot;, &quot;&quot;, order_by=[&quot;metrics.rmse DESC&quot;], max_results=1)\nbest_run = runs[0]\n<\/code><\/pre>\n<p><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.search_runs\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.search_runs<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":29.4,
        "Solution_reading_time":6.76,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":17.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":105.0398277778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a training script in Sagemaker like,<\/p>\n\n<pre><code>def train(current_host, hosts, num_cpus, num_gpus, channel_input_dirs, model_dir, hyperparameters, **kwargs):\n    ... Train a network ...\n    return net\n\ndef save(net, model_dir):\n    # save the model\n    logging.info('Saving model')\n    y = net(mx.sym.var('data'))\n    y.save('%s\/model.json' % model_dir)\n    net.collect_params().save('%s\/model.params' % model_dir)\n\ndef model_fn(model_dir):\n    symbol = mx.sym.load('%s\/model.json' % model_dir)\n    outputs = mx.symbol.softmax(data=symbol, name='softmax_label')\n    inputs = mx.sym.var('data')\n    param_dict = gluon.ParameterDict('model_')\n    net = gluon.SymbolBlock(outputs, inputs, param_dict)\n    net.load_params('%s\/model.params' % model_dir, ctx=mx.cpu())\n    return net\n<\/code><\/pre>\n\n<p>Most of which I stole from the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mnist.py\" rel=\"nofollow noreferrer\">MNIST Example<\/a>.<\/p>\n\n<p>When I train, everything goes fine, but when trying to deploy like,<\/p>\n\n<pre><code>m = MXNet(\"lstm_trainer.py\", \n          role=role, \n          train_instance_count=1, \n          train_instance_type=\"ml.c4.xlarge\",\n          hyperparameters={'batch_size': 100, \n                         'epochs': 20, \n                         'learning_rate': 0.1, \n                         'momentum': 0.9, \n                         'log_interval': 100})\nm.fit(inputs) # No errors\npredictor = m.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>I get, (<a href=\"https:\/\/gist.github.com\/aidan-plenert-macdonald\/7eb7ba7402790b61596938b5cbf605b6\" rel=\"nofollow noreferrer\">full output<\/a>)<\/p>\n\n<pre><code>INFO:sagemaker:Creating model with name: sagemaker-mxnet-py2-cpu-2018-01-17-20-52-52-599\n---------------------------------------------------------------------------\n  ... Stack dump ...\nClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not find model data at s3:\/\/sagemaker-us-west-2-01234567890\/sagemaker-mxnet-py2-cpu-2018-01-17-20-52-52-599\/output\/model.tar.gz.\n<\/code><\/pre>\n\n<p>Looking in my S3 bucket <code>s3:\/\/sagemaker-us-west-2-01234567890\/sagemaker-mxnet-py2-cpu-2018-01-17-20-52-52-599\/output\/model.tar.gz<\/code>, I in fact don't see the model.<\/p>\n\n<p>What am I missing?<\/p>",
        "Challenge_closed_time":1516602490380,
        "Challenge_comment_count":2,
        "Challenge_created_time":1516224347000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error message \"Could not find model data\" when trying to deploy their model in Sagemaker. They have checked their S3 bucket and confirmed that the model is not present. The user has provided code snippets for their training script and deployment process.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48310237",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":16.3,
        "Challenge_reading_time":30.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":105.0398277778,
        "Challenge_title":"Sagemaker \"Could not find model data\" when trying to deploy my model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2950.0,
        "Challenge_word_count":164,
        "Platform":"Stack Overflow",
        "Poster_created_time":1384712661608,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Diego, CA, United States",
        "Poster_reputation_count":5151.0,
        "Poster_view_count":1038.0,
        "Solution_body":"<p>When you are calling the training job you should specify the output directory:<\/p>\n\n<pre><code>#Bucket location where results of model training are saved.\nmodel_artifacts_location = 's3:\/\/&lt;bucket-name&gt;\/artifacts'\n\nm = MXNet(entry_point='lstm_trainer.py',\n          role=role,\n          output_path=model_artifacts_location,\n          ...)\n<\/code><\/pre>\n\n<p>If you don't specify the output directory the function will use a default location, that it might not have the permissions to create or write to.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.6,
        "Solution_reading_time":6.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1574151667340,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":456.0,
        "Answerer_view_count":31.0,
        "Challenge_adjusted_solved_time":13.4095258333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I got this error when I was trying to have a model registered in the model registry. Could someone help me?<\/p>\n<pre><code>RestException: INVALID_PARAMETER_VALUE: Unsupported URI '.\/mlruns' for model registry store. \nSupported schemes are: ['postgresql', 'mysql', 'sqlite', 'mssql']. \nSee https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#storage for how to setup a compatible server.\n<\/code><\/pre>",
        "Challenge_closed_time":1596626369480,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596578095187,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user encountered an error while trying to register a model in the MLflow model registry. The error message indicates that the URI '.\/mlruns' is not supported for the model registry store and suggests using compatible servers such as PostgreSQL, MySQL, SQLite, or MSSQL. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":1598815085667,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63255631",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":6.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":10.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":13.4095258333,
        "Challenge_title":"MLflow: INVALID_PARAMETER_VALUE: Unsupported URI '.\/mlruns' for model registry store",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":12594.0,
        "Challenge_word_count":55,
        "Platform":"Stack Overflow",
        "Poster_created_time":1419619351727,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":125.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>Mlflow required DB as datastore for Model Registry\nSo you have to run tracking server with DB as backend-store and log model to this tracking server.\nThe easiest way to use DB is to use SQLite.<\/p>\n<pre><code>mlflow server \\\n    --backend-store-uri sqlite:\/\/\/mlflow.db \\\n    --default-artifact-root .\/artifacts \\\n    --host 0.0.0.0\n<\/code><\/pre>\n<p>And set MLFLOW_TRACKING_URI environment variable to <em>http:\/\/localhost:5000<\/em> or<\/p>\n<pre><code>mlflow.set_tracking_uri(&quot;http:\/\/localhost:5000&quot;)\n<\/code><\/pre>\n<p>After got to http:\/\/localhost:5000 and you can register a logged model from UI or from the code.<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":8.0,
        "Solution_reading_time":7.98,
        "Solution_score_count":27.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":72.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":214.3601405556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,    <\/p>\n<p>I am running an AutoML experiment for a regression task, and looking at the YAML file which is generated it seems that TensorFlowLinearRegressor and TensorFlowDNN models are listed as both 'supported_models' and 'blacklist_algos'.    <\/p>\n<p>I tried to deactivate the automatic blacklisting of models by specifying the parameter 'auto_blacklist' to False, and 'blacklist_models' and 'blacklist_algos' parameters to Null, but it doesn't change anything.    <\/p>\n<pre><code>automl_settings = {  \n    &quot;primary_metric&quot;: 'normalized_mean_absolute_error',  \n    &quot;featurization&quot;: 'auto',  \n    &quot;verbosity&quot;: logging.INFO,  \n    &quot;n_cross_validations&quot;: 5,  \n    &quot;auto_blacklist&quot;: False,  \n    &quot;blacklist_models&quot;: None,  \n    &quot;blacklist_algos&quot;: None  \n}  \nrun = experiment.submit(automl_config, show_output=True)  \n<\/code><\/pre>\n<p>The generated YAML file (excerpt):    <\/p>\n<pre><code>&quot;whitelist_models&quot;:null,  \n&quot;blacklist_algos&quot;:[&quot;TensorFlowDNN&quot;,&quot;TensorFlowLinearRegressor&quot;],  \n&quot;supported_models&quot;:[&quot;ElasticNet&quot;,&quot;GradientBoosting&quot;,&quot;LightGBM&quot;,&quot;TensorFlowLinearRegressor&quot;,&quot;TensorFlowDNN&quot;,&quot;LassoLars&quot;,&quot;DecisionTree&quot;,&quot;RandomForest&quot;,&quot;FastLinearRegressor&quot;,&quot;OnlineGradientDescentRegressor&quot;,&quot;ExtremeRandomTrees&quot;,&quot;TabnetRegressor&quot;,&quot;XGBoostRegressor&quot;,&quot;KNN&quot;,&quot;SGD&quot;],  \n&quot;private_models&quot;:[],  \n&quot;auto_blacklist&quot;:false  \n<\/code><\/pre>\n<p>Maybe the problem comes from the fact that Deep learning is set to 'Disabled' in the configuration settings, as shown on the following picture:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/205364-image.png?platform=QnA\" alt=\"205364-image.png\" \/>    <\/p>\n<p>Are deep learning models not supported anymore by AutoML?    <\/p>",
        "Challenge_closed_time":1654236226443,
        "Challenge_comment_count":3,
        "Challenge_created_time":1653464529937,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is running an AutoML experiment for a regression task and noticed that TensorFlowLinearRegressor and TensorFlowDNN models are listed as both 'supported_models' and 'blacklist_algos' in the generated YAML file. The user tried to deactivate the automatic blacklisting of models by specifying the parameter 'auto_blacklist' to False, and 'blacklist_models' and 'blacklist_algos' parameters to Null, but it didn't work. The user wonders if deep learning models are not supported anymore by AutoML as Deep learning is set to 'Disabled' in the configuration settings.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/863297\/automl-tensorflowdnn-and-tensorflowlinearregressor",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":26.5,
        "Challenge_reading_time":26.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":214.3601405556,
        "Challenge_title":"AutoML : TensorFlowDNN and TensorFlowLinearRegressor are blacklisted by default",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":142,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@ThierryL-3166  Thanks for the question.     <\/p>\n<p>As mentioned in the below document The following support models in AutoML TensorFlowDNN, TensorFlowLinearRegressor are deprecated.     <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-automl-core\/azureml.automl.core.shared.constants.supportedmodels.regression?view=azure-ml-py\">https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-automl-core\/azureml.automl.core.shared.constants.supportedmodels.regression?view=azure-ml-py<\/a><\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":32.5,
        "Solution_reading_time":6.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    }
]