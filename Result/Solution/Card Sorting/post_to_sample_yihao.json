[
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":4.6135575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Currently! I'm experimenting with the azure data labelling tool in the machine learning workspace for image classification, what I found was azure shows only the unlabelled data to each user i.e if a user has already labelled an image, other users won't be shown the same image again.   <br \/>\nIs there any setting that exists, which can be enabled or disabled so that we can let more than one labeller label the same data?   <\/p>",
        "Challenge_closed_time":1625073229547,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625056620740,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/458004\/azure-machine-learning-data-labelling-is-it-possib",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.7,
        "Challenge_reading_time":6.98,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":4.6135575,
        "Challenge_title":"Azure machine learning data labelling- Is it possible to assign different labelers to label same data in a single project to reach a consensus?",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":98,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>Thanks for reaching to us. This capability is currently in development, and expected to release soon.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":1.37,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":1.30024,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Trying to download a report as latex causes an instrument.js error, and the waiting symbol turns forever. I use chrome on MacOS.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/4\/4c88c4ac7b283a2fef287aa3cd58a712426c2f77.jpeg\" data-download-href=\"\/uploads\/short-url\/aV3jmQ0drwgzx2rJ9iyEpD7TJQj.jpeg?dl=1\" title=\"Bildschirmfoto 2023-02-13 um 17.42.14\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/4\/4c88c4ac7b283a2fef287aa3cd58a712426c2f77_2_690x307.jpeg\" alt=\"Bildschirmfoto 2023-02-13 um 17.42.14\" data-base62-sha1=\"aV3jmQ0drwgzx2rJ9iyEpD7TJQj\" width=\"690\" height=\"307\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/4\/4c88c4ac7b283a2fef287aa3cd58a712426c2f77_2_690x307.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/4\/4c88c4ac7b283a2fef287aa3cd58a712426c2f77_2_1035x460.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/4\/4c88c4ac7b283a2fef287aa3cd58a712426c2f77_2_1380x614.jpeg 2x\" data-dominant-color=\"959190\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Bildschirmfoto 2023-02-13 um 17.42.14<\/span><span class=\"informations\">1886\u00d7841 173 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Challenge_closed_time":1676311598268,
        "Challenge_comment_count":0,
        "Challenge_created_time":1676306917404,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/download-report-as-latex-causes-js-errors\/3872",
        "Challenge_link_count":5,
        "Challenge_participation_count":3,
        "Challenge_readability":24.9,
        "Challenge_reading_time":21.95,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1.30024,
        "Challenge_title":"Download report as latex causes js errors",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":210.0,
        "Challenge_word_count":76,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>Found a solution: When carefully loading each graph by scrolling slowly over the whole page, the download finally works.<\/p>",
        "Solution_comment_count":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.5,
        "Solution_reading_time":1.6,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":19.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":167.2076163889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In this <a href=\"https:\/\/github.com\/MicrosoftLearning\/DP100\/blob\/master\/07B%20-%20Creating%20a%20Batch%20Inferencing%20Service.ipynb\">example<\/a>, all data files for the parallel run step are stored in <strong>one<\/strong> folder.    <\/p>\n<p>I also want to create a parallel run step. The task for each of the several <strong>folders<\/strong>, in which the multiple data files are stored, is exactly identical.     <\/p>\n<p>The folders:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/182769-image.png?platform=QnA\" alt=\"182769-image.png\" \/>    <\/p>\n<p>The content of each folder:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/182833-image.png?platform=QnA\" alt=\"182833-image.png\" \/>    <\/p>\n<p>How should I define the <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-pipeline-steps\/azureml.pipeline.steps.parallelrunstep?view=azure-ml-py\">ParallelRunStep<\/a>-class so that the identical task for each folder (here 'a', 'b', 'c', 'd' and 'e') is executed in parallel?    <br \/>\nTwo folders should run simultaneously in parallel.    <\/p>\n<p>Moreover, I would like to ask how to get <strong>only<\/strong> the stored folder names or folder paths from a given directory path of a blob storage container.    <\/p>",
        "Challenge_closed_time":1647858343236,
        "Challenge_comment_count":1,
        "Challenge_created_time":1647256395817,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/771015\/list-of-folder-names-as-input-for-parallelrunstep",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":12.8,
        "Challenge_reading_time":17.16,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":167.2076163889,
        "Challenge_title":"list of folder names as input for ParallelRunStep-class",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":134,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>@@AlexanderPakakis-0994 Thanks, An Azure ML dataset is just metadata pointing to a path or collection of paths in an Azure storage account. You should first &quot;merge&quot; those datasets into a collection of adjacent folders (e.g. root\/dataset1\/, root\/dataset2\/, ...) and then run PRS against root\/**.<\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_link_count":0.0,
        "Solution_readability":7.7,
        "Solution_reading_time":3.94,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":0.0855555556,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nI am currently defining some machines configuration using machine-env1.yaml, machine-env2.yaml which basically contains node selectors and CPU, GPU, and TPU requests configuration, and then running:\n\npolyaxon run -f polyaxonfile.yaml -f machine-env1.yaml\n\nI have two problems with this approach:\n\nI need to copy the env files to all our git repos, which means if I make a change I need to perform several pull requests\nI need to tell the data-scientits to pull the last commit, sometimes that's not possible because they can not merge\/rebase the changes.\n\nBased on those two issues, in the end we tell data-scientists to just use:\n\nenvironment:\n  nodeSelector:\n    nodes: large-pool\n...\nrun:\n  ...\n  container:\n      resources:\n        limits:\n          cpu: 3000m\n          memory: 6000Mi\n        requests:\n          cpu: 2000m\n          memory: 4000Mi\n\nWhich is error prone and confusing for them, and make the files bigger and difficult to change.\n\nAny elegant way to abstract this type of configuration from the data-scientists?",
        "Challenge_closed_time":1649337274000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649336966000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1484",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.6,
        "Challenge_reading_time":13.74,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.0855555556,
        "Challenge_title":"I would like to configure Polyaxon in a way to avoid asking data-scientists to configure pre-emptible node-pools or request TPUs on their own",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":null,
        "Challenge_word_count":170,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"We have already shared a resource on how to configure the environments in this guide\n\nif you are using multiple git repos and you do not want to replicate the yaml files in all repos you can register those files as presets:\n\nUsers will be able to use --presets machine1 or --presets=env1\n\nNote that in the example in that link, it shows that it defines a queue but you do not have to define a queue, a preset is just any YAML file that can be used with the override operator -f main.yaml -f override1.yaml -f override2.yaml in this case override1.yaml and override2.yaml it can be saved as organization presets using the UI.\n\nMore info from the intro section about presets and the UI section\n\nAlso, when you define presets you can use them directly on the operation or component\n\npresets: [preset1, preset2]\n\nThis is similar to the CLI command\n\npolyaxon run -f polyaxon.yaml --presets preset1,preset2",
        "Solution_comment_count":0.0,
        "Solution_link_count":0.0,
        "Solution_readability":8.5,
        "Solution_reading_time":10.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":156.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":24.0520611111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Can we connect Azure ML Notebooks directly to Snowflake using Private end-points, my ML Workspace is inside a VNet.<\/p>",
        "Challenge_closed_time":1653034845860,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652948258440,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/855820\/connect-azure-ml-with-snowflake-using-private-endp",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.2,
        "Challenge_reading_time":2.25,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":24.0520611111,
        "Challenge_title":"Connect Azure ML with Snowflake using Private endpoint?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":null,
        "Challenge_word_count":26,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=03343194-9922-4c28-abc7-1d7c46b6d2d6\">@Varun  <\/a>     <\/p>\n<p>Thanks for reaching out to us, currently there is no internal way in Azure Machine Learning Studio to connect to Snowflake. I am sorry for all inconveniences.     <\/p>\n<p>But you can run a  Python 3 code to use the Snowflake python connector - <a href=\"https:\/\/docs.snowflake.com\/en\/user-guide\/python-connector.html\">https:\/\/docs.snowflake.com\/en\/user-guide\/python-connector.html<\/a>    <\/p>\n<p>With Azure ML Studio, there's no built-in support for SnowFlake, I will forward your feedback to product group to see if there any plan in the future.     <\/p>\n<p>Hope this helps!    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p><em>-Please kindly accept the answer if you feel helpful, thanks a lot for supporting the community.<\/em>     <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_link_count":1.0,
        "Solution_readability":9.0,
        "Solution_reading_time":10.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":103.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":176.3419125,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>I got this on win10,it\u2019s stucked<br>\nthe enviornment is<\/p>\n<ul>\n<li>python3.7.10<\/li>\n<li>wandb 0.12.9<\/li>\n<\/ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b2ce90f5e63f7db9be89aa163ee55f0ab468a430.png\" data-download-href=\"\/uploads\/short-url\/pvNysR6Ps6qxY0fl0Y5gjzfovBK.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b2ce90f5e63f7db9be89aa163ee55f0ab468a430.png\" alt=\"image\" data-base62-sha1=\"pvNysR6Ps6qxY0fl0Y5gjzfovBK\" width=\"547\" height=\"500\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b2ce90f5e63f7db9be89aa163ee55f0ab468a430_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">686\u00d7626 26.9 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Challenge_closed_time":1641690199763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641055368878,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/error-with-wandb-on-win10\/1656",
        "Challenge_link_count":3,
        "Challenge_participation_count":5,
        "Challenge_readability":25.9,
        "Challenge_reading_time":15.89,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":176.3419125,
        "Challenge_title":"Error with wandb on win10",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":235.0,
        "Challenge_word_count":52,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>I\u2019ve deleted wandb in docker and pip ,then I reinstalled them.<br>\nAnd I got the right page after waiting about 5 or 6 minutes.<br>\nBut I don\u2019t know whtether the reason is the versions are different or something.<br>\nThis time I didn\u2019t set the LOCAL_RESOTRE var, I don\u2019t know whether the time will decrease.<br>\nAnd I notice that once I get the right page, the next time I can get in immediately.<br>\nThanks a lot.<\/p>",
        "Solution_comment_count":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.6,
        "Solution_reading_time":5.1,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":75.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":9.7155897222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to follow the steps given here - <a href=\"https:\/\/learn.microsoft.com\/en-us\/learn\/modules\/explore-analyze-data-with-python\/2-exercise-explore-data\">https:\/\/learn.microsoft.com\/en-us\/learn\/modules\/explore-analyze-data-with-python\/2-exercise-explore-data<\/a>    <\/p>\n<p>I've tried regions east us2 and east us for creating the instance but it fails after taking more than half an hour. I tried virtual machine sizes - Standard_DS11_v2 &amp; Standard_DS3_v2.    <\/p>\n<p>Any help would be appreciated.     <\/p>\n<p>Edit - I don't have any other instances running in my subscription, so it should not be a quota issue. The error message says &quot;An internal server error occurred.&quot;.<\/p>",
        "Challenge_closed_time":1616282059876,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616247083753,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/323742\/unable-to-creata-a-compute-instance",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":12.5,
        "Challenge_reading_time":9.46,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":9.7155897222,
        "Challenge_title":"Unable to creata a compute instance",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":81,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>Good day <a href=\"\/users\/na\/?userid=79ab735d-44c2-44c3-954b-5a6233041e68\">@Aatish Suman  <\/a>      <\/p>\n<p>Did you read the comment in the compute page?    <\/p>\n<p>Please confirm that you are using an account which fit the limitations    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/79891-image.png?platform=QnA\" alt=\"79891-image.png\" \/>    <\/p>\n<p>For more information please check this post:    <\/p>\n<p><a href=\"https:\/\/azure.microsoft.com\/en-us\/blog\/update-2-on-microsoft-cloud-services-continuity\/\">https:\/\/azure.microsoft.com\/en-us\/blog\/update-2-on-microsoft-cloud-services-continuity\/<\/a>    <\/p>\n<p>Note: I followed the tutorial which you provided the link to and it is working well for me. Therefore, I assume the issue is related to the above comment.     <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_link_count":2.0,
        "Solution_readability":12.9,
        "Solution_reading_time":10.28,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":75.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":185.8020663889,
        "Challenge_answer_count":7,
        "Challenge_body":"<p>Dear W&amp;B Community,<\/p>\n<p>I have system metrics logged like the \u201c<em>time per step<\/em>\u201d or \u201c<em>time per backward pass<\/em>\u201d for a model.<br>\nWhen doing this on different hardware, I would like to compare the effect this has on these metrics.<br>\nIn the following examples, I profile the basic Torch CIFAR10 model on a 1,2,4,8,16 and 32 CPU VM.<\/p>\n<p>When looking at a <code>Linechart<\/code>, the full history of these metrics is visible, however, it is very hard to compare them due to the overlapping and oscillation:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351.png\" data-download-href=\"\/uploads\/short-url\/zZROm2jlGDN4WUrxx2lQXAA8jYZ.png?dl=1\" title=\"W&amp;amp;B Chart 9_19_2022, 6_22_16 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351_2_690x362.png\" alt=\"W&amp;B Chart 9_19_2022, 6_22_16 PM\" data-base62-sha1=\"zZROm2jlGDN4WUrxx2lQXAA8jYZ\" width=\"690\" height=\"362\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351_2_690x362.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351_2_1035x543.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351_2_1380x724.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">W&amp;amp;B Chart 9_19_2022, 6_22_16 PM<\/span><span class=\"informations\">3539\u00d71859 509 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>When using a <code>Barchart<\/code>, only the last value is visualized:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/48a4597177e867b3eb511112ad23b561f18f1137.png\" data-download-href=\"\/uploads\/short-url\/amCuG3pzRgnimYoyoJeru5muDMH.png?dl=1\" title=\"W&amp;amp;B Chart 9_19_2022, 6_20_31 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/48a4597177e867b3eb511112ad23b561f18f1137_2_690x362.png\" alt=\"W&amp;B Chart 9_19_2022, 6_20_31 PM\" data-base62-sha1=\"amCuG3pzRgnimYoyoJeru5muDMH\" width=\"690\" height=\"362\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/48a4597177e867b3eb511112ad23b561f18f1137_2_690x362.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/48a4597177e867b3eb511112ad23b561f18f1137_2_1035x543.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/48a4597177e867b3eb511112ad23b561f18f1137_2_1380x724.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/48a4597177e867b3eb511112ad23b561f18f1137_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">W&amp;amp;B Chart 9_19_2022, 6_20_31 PM<\/span><span class=\"informations\">3539\u00d71859 251 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>The functionality that would be nice is to group values based on their count or occurrence, as grouping by runs already works perfectly. Here\u2019s the same data but run through <code>seaborn.barplot<\/code>:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479.png\" data-download-href=\"\/uploads\/short-url\/7VTQur5SLq8cPHTQtTqGrDuwPRn.png?dl=1\" title=\"download\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479_2_690x427.png\" alt=\"download\" data-base62-sha1=\"7VTQur5SLq8cPHTQtTqGrDuwPRn\" width=\"690\" height=\"427\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479_2_690x427.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479_2_1035x640.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479_2_1380x854.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">download<\/span><span class=\"informations\">3777\u00d72341 159 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Would this be possible to implement? Or does anybody know a way to get that functionality?<\/p>\n<p>My current workaround is to download the data manually and run it through seaborn. Unfortunately, I did not understand the errors I\u2019ve gotten with the <code>Custom Chart<\/code> functionality when trying to port Vega examples to use wandb as a data basis.<\/p>\n<p>I\u2019d be very glad if anybody can point me to a tutorial on how to migrate existing Vega examples to be used with wandb (and the common problems, like differences between v3\/v4\/v5, as these seemed to be an issue for me).<\/p>",
        "Challenge_closed_time":1664274024356,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663605136917,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/barchart-grouping-by-time-step-count\/3157",
        "Challenge_link_count":18,
        "Challenge_participation_count":7,
        "Challenge_readability":21.3,
        "Challenge_reading_time":80.47,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":185.8020663889,
        "Challenge_title":"Barchart Grouping by Time\/Step\/Count",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":796.0,
        "Challenge_word_count":367,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>Hi Alexander,<\/p>\n<p>Thanks for sending this detailed explanation! I have been exploring it and I think that the issue here is that, in lines 22, 29 and 43 you have \u201cdata\u201d: \u201ctable\u201d but as the name has been changed to \u201cwandb\u201d, then you should have \u201cdata\u201d: \u201cwandb\u201d. To solve the error between lines 4 and 6, you can use <span class=\"chcklst-box fa fa-square-o fa-fw\"><\/span> and it is solved, but it seems that it is not affecting to the chart.<\/p>\n<pre><code>\"data\": [{ \"name\": \"wandb\" }]\n<\/code><\/pre>\n<p>Please let me know if this would be useful for you!<\/p>\n<p>Best,<br>\nLuis<\/p>",
        "Solution_comment_count":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":7.17,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":96.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":1.2138261111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a question about <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-register-datasets#tabulardataset\">the source of TabularDataset on Azure Machine Learnigng<\/a>.    <\/p>\n<p>Can I use compressed data saved Azure Data Lake Storage Gen2 like below on TablarDataset without expansion?    <\/p>\n<ul>\n<li> csv with bzip2(.bz2)    <\/li>\n<li> parquet with gzip(gz)    <\/li>\n<li> parquet with snappy    <\/li>\n<\/ul>",
        "Challenge_closed_time":1638238520327,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638234150553,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/645118\/can-i-use-compressed-data-on-tabulardataset",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.1,
        "Challenge_reading_time":6.16,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.2138261111,
        "Challenge_title":"Can I use compressed data on TabularDataset?",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":56,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>Hi, tabular dataset does not support compressed files. You'll need to extract the data as shown <a href=\"https:\/\/medium.com\/mlearning-ai\/load-json-gz-files-to-azure-ml-dataset-b7039ec9da34\">here<\/a> for example before creating a tabular dataset. However, file dataset supports any format.<\/p>\n<hr \/>\n<p>--- *Kindly <em><strong>Accept Answer<\/strong><\/em> if the information helps. Thanks.*<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_link_count":1.0,
        "Solution_readability":10.0,
        "Solution_reading_time":5.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":41.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":2.6086344444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hey experts, I am looking for a document of how features in SDK V1 mapping to SDK v2 to show my team and plan how we should move to SDK v2. I cannot find a summary for that. Can you please help with this <\/p>",
        "Challenge_closed_time":1682725999300,
        "Challenge_comment_count":0,
        "Challenge_created_time":1682716608216,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1266094\/sdk-features-mapping",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.4,
        "Challenge_reading_time":2.73,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":2.6086344444,
        "Challenge_title":"SDK features mapping",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":47,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>Hello <a href=\"https:\/\/learn.microsoft.com\/users\/na\/?userid=9603a4b0-3119-4f80-93b6-9637337c7a94\">@otto atler<\/a> <\/p>\n<p>Thanks for reaching out to us again, please see below list: <\/p>\n<p>For workspace - <\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.workspace.workspace\">Method\/API in SDK v1 (use links to ref docs)<\/a>    <\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.entities.workspace\">Method\/API in SDK v2 (use links to ref docs)<\/a><\/p>\n<p>For compute - <\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.compute.amlcompute(class)\">Method\/API in SDK v1 (use links to ref docs)<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.entities.amlcompute\">Method\/API in SDK v2 (use links to ref docs)<\/a><\/p>\n<p>For datastore -<\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_storage_datastore.azureblobdatastore?view=azure-ml-py&amp;preserve-view=true\">azureml_blob_datastore<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.entities.azuredatalakegen1datastore\">azureml_blob_datastore<\/a><\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_data_lake_datastore.azuredatalakedatastore?view=azure-ml-py&amp;preserve-view=true\">azureml_data_lake_gen1_datastore<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.entities.azuredatalakegen1datastore\">azureml_data_lake_gen1_datastore<\/a><\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_data_lake_datastore.azuredatalakegen2datastore?view=azure-ml-py&amp;preserve-view=true\">azureml_data_lake_gen2_datastore<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.entities.azuredatalakegen2datastore\">azureml_data_lake_gen2_datastore<\/a><\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_sql_database_datastore.azuresqldatabasedatastore?view=azure-ml-py&amp;preserve-view=true\">azuremlml_sql_database_datastore<\/a><\/p>\n<p>V2 Will be supported via import &amp; export functionalities|<\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_my_sql_datastore.azuremysqldatastore?view=azure-ml-py&amp;preserve-view=true\">azuremlml_my_sql_datastore<\/a><\/p>\n<p>V2 Will be supported via import &amp; export functionalities|<\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_postgre_sql_datastore.azurepostgresqldatastore?view=azure-ml-py&amp;preserve-view=true\">azuremlml_postgre_sql_datastore<\/a><\/p>\n<p>V2 Will be supported via import &amp; export functionalities|<\/p>\n<p>For data assets - <\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data\">Method\/API in SDK v1<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.entities\">Method\/API in SDK v2<\/a><\/p>\n<p>For model assets - <\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model(class)#azureml-core-model-register\">Model.register<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.mlclient#azure-ai-ml-mlclient-create-or-update\">ml_client.models.create_or_update<\/a><\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run#azureml-core-run-run-register-model\">run.register_model<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.mlclient#azure-ai-ml-mlclient-create-or-update\">ml_client.models.create_or_update<\/a><\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model(class)#azureml-core-model-deploy\">Model.deploy<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.mlclient#azure-ai-ml-mlclient-begin-create-or-update\">ml_client.begin_create_or_update(blue_deployment)<\/a><\/p>\n<p>I hope this helps, please let me know if you have any questions.<\/p>\n<p>Regards,<\/p>\n<p>Yutong<\/p>\n<p>-Please kindly accept the answer and vote 'Yes' if you feel helpful to support he community, thanks a lot.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_link_count":22.0,
        "Solution_readability":30.1,
        "Solution_reading_time":61.37,
        "Solution_score_count":0.0,
        "Solution_sentence_count":24.0,
        "Solution_word_count":197.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":13.2144094444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I work with Azure Machine Learning Service for modeling. To track and analyze the result of a binary classification problem, I use a method named <strong>score-classification<\/strong> in <em>azureml.training.tabular.score.scoring<\/em> library. I invoke the method like this:<\/p>\n<pre><code>metrics = score_classification( y_test, y_pred_probs, metrics_names_list, class_labels, train_labels, sample_weight=sample_weights, use_binary=True)\n\n<\/code><\/pre>\n<p>Input arguments are: <\/p>\n<ul>\n<li> <em>y_test<\/em> is an array of 0 and 1. <\/li>\n<li> <em>y_pred<\/em> is an array of float values for each item. <\/li>\n<li> <em>metrics_names_list<\/em> is the list of the name of the metrics I want to calculate:['f1_score_classwise', 'confusion_matrix']. <\/li>\n<li> <em>class_labels<\/em> is a two-item array of [0, 1].<\/li>\n<li> <em>train_labels<\/em> is a two-item list of ['False', 'True']. <\/li>\n<\/ul>\n<p>When it calculates the metrics I sent as <em>metrics_names_list<\/em>, the results are shown in the Azure ML portal in the metrics page. <\/p>\n<p>Confusion matrix is one of the metrics I draw each time. It has a combo box for the representation. This combo box could be set as <strong>Raw<\/strong> to show the number of items for each cell, and <strong>Normalized<\/strong> to show the percentage of the cells.<\/p>\n<p>The problem is that I see float value for the Raw configuration of this matrix! I do not know how to handle this issue? <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/92357b82-b9f4-4cc4-8630-9619d4584bfa?platform=QnA\" alt=\"enter image description here\" \/><\/p>",
        "Challenge_closed_time":1681173478840,
        "Challenge_comment_count":0,
        "Challenge_created_time":1681125906966,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1213742\/how-to-fix-the-bug-for-float-values-in-confusion-m",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":21.34,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":13.2144094444,
        "Challenge_title":"How to fix the bug for float values in confusion matrix in Azure ML service?",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":223,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=e2b5cca4-3304-4fb3-9c9d-7d0f840d76b8\">@Elahe Dorani  <\/a><\/p>\n<p>Thanks for reaching out to us, I am not very clear about your question, so if I am not in the right way, please let me know. It sounds like the issue you are experiencing is that the confusion matrix is being displayed as float values instead of integers when you select the &quot;Raw&quot; option in the combo box.\nOne possible explanation for this behavior is that the <strong><code>score_classification<\/code><\/strong> function is returning the confusion matrix as a numpy array of float values instead of integers. This could happen if the function is doing some kind of normalization or scaling of the values.<\/p>\n<p>To address this issue, you could try converting the confusion matrix to integers before passing it to the <strong><code>score_classification<\/code><\/strong> function. You can use the numpy <strong><code>round<\/code><\/strong> function to round the float values to the nearest integer:<\/p>\n<pre><code>pythonCopy code\nconfusion_matrix = np.round(confusion_matrix).astype(int)\n<\/code><\/pre>\n<p>Then, when you call the <strong><code>score_classification<\/code><\/strong> function, pass in the rounded confusion matrix instead of the original one.<\/p>\n<p>If this does not work, another option is to modify the <strong><code>score_classification<\/code><\/strong> function to return the confusion matrix as integers instead of floats. You can do this by using the numpy <strong><code>astype<\/code><\/strong> function to convert the matrix to the <strong><code>int<\/code><\/strong> data type:<\/p>\n<pre><code>arduinoCopy code\nconfusion_matrix = confusion_matrix.astype(int)\n<\/code><\/pre>\n<p>I hope this helps.<\/p>\n<p>Regards,\nYutong<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_link_count":0.0,
        "Solution_readability":12.8,
        "Solution_reading_time":22.39,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":219.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Challenge_adjusted_solved_time":0.3402777778,
        "Challenge_answer_count":1,
        "Challenge_body":"If I deploy a SageMaker model, am I incurring hosting charges even while no one is accessing my model?",
        "Challenge_closed_time":1592313864000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592312639000,
        "Challenge_favorite_count":0.0,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUlNS8ujYmQqePwWS-mgso3Q\/sagemaker-model-spend",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.6,
        "Challenge_reading_time":1.53,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":0.3402777778,
        "Challenge_title":"SageMaker Model Spend",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":148.0,
        "Challenge_word_count":21,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Solution_body":"When you deploy a SageMaker model, it deploys it behind a SageMaker endpoint for real-time inference. You are charged by the second for on-demand ML hosting. Check the model deployment section of each region on the [SageMaker Pricing page][1]. In some use cases, you can save on inference cost by hosting several models behind the same endpoint (check [this blog post][2]).\n\n\n  [1]: https:\/\/aws.amazon.com\/sagemaker\/pricing\/?nc1=h_ls\n  [2]: https:\/\/aws.amazon.com\/fr\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/",
        "Solution_comment_count":0.0,
        "Solution_link_count":2.0,
        "Solution_readability":13.1,
        "Solution_reading_time":7.23,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":65.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":213.2411183333,
        "Challenge_answer_count":9,
        "Challenge_body":"<p>I am loving <a href=\"http:\/\/wandb.ai\">wandb.ai<\/a> and all it can do for me. I have a question whose answer I cannot find anywhere.<br>\nAmong the various fields in the wandb.config file are a few that wandb generates automatically. One of them is <code>Description<\/code>. I tried setting it from a Python program via my configuration file, but to no avail. So I am wondering how to set the Description field programmatically. This will allow me to \u201cdescribe\u201d several hundred simulations for easy retrieval. Thanks,<\/p>",
        "Challenge_closed_time":1660860326460,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660092658434,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/description-field\/2881",
        "Challenge_link_count":1,
        "Challenge_participation_count":9,
        "Challenge_readability":6.9,
        "Challenge_reading_time":6.73,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":213.2411183333,
        "Challenge_title":"Description field",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":127.0,
        "Challenge_word_count":83,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/erlebacher\">@erlebacher<\/a> , It\u2019s pretty expensive to do pattern filtering in MySQL, especially on a large column like <code>notes<\/code> . The engineering team decided this feature will not be implemented. I will mark this resolved but please let me know if there is anything else I can answer for you.<\/p>",
        "Solution_comment_count":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.8,
        "Solution_reading_time":4.27,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":50.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Challenge_adjusted_solved_time":2.8071302778,
        "Challenge_answer_count":1,
        "Challenge_body":"I have a state-machine workflow with 3 following states:  \n\n[screenshot-of-my-workflow](https:\/\/i.stack.imgur.com\/4xJTE.png)   \n\n1. A 'Pass' block that adds a list of strings(SageMaker endpoint names) to the original input. (*this 'Pass' will be replaced by a call to DynamoDB to fetch list in future.*) \n2. Use map to call SageMaker endpoints dictated by the array(or list) from above result.\n3. Send the result of above 'Map' to a Lambda function and exit the workflow.\n\n\nHere's the entire workflow in .asl.json, inspired from [this aws blog](https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/sample-map-state.html).\n```\n{\n  \"Comment\": \"A description of my state machine\",\n  \"StartAt\": \"Pass\",\n  \"States\": {\n    \"Pass\": {\n      \"Type\": \"Pass\",\n      \"Next\": \"InvokeEndpoints\",\n      \"Result\": {\n        \"Endpoints\": [\n          \"sagemaker-endpoint-1\",\n          \"sagemaker-endpoint-2\",\n          \"sagemaker-endpoint-3\"\n        ]\n      },\n      \"ResultPath\": \"$.EndpointList\"\n    },\n    \"InvokeEndpoints\": {\n      \"Type\": \"Map\",\n      \"Next\": \"Post-Processor Lambda\",\n      \"Iterator\": {\n        \"StartAt\": \"InvokeEndpoint\",\n        \"States\": {\n          \"InvokeEndpoint\": {\n            \"Type\": \"Task\",\n            \"End\": true,\n            \"Parameters\": {\n              \"Body\": \"$.InvocationBody\",\n              \"EndpointName\": \"$.EndpointName\"\n            },\n            \"Resource\": \"arn:aws:states:::aws-sdk:sagemakerruntime:invokeEndpoint\",\n            \"ResultPath\": \"$.InvocationResult\"\n          }\n        }\n      },\n      \"ItemsPath\": \"$.EndpointList.Endpoints\",\n      \"MaxConcurrency\": 300,\n      \"Parameters\": {\n        \"InvocationBody.$\": \"$.body.InputData\",\n        \"EndpointName.$\": \"$$.Map.Item.Value\"\n      },\n      \"ResultPath\": \"$.InvocationResults\"\n    },\n    \"Post-Processor Lambda\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:states:::lambda:invoke\",\n      \"Parameters\": {\n        \"Payload.$\": \"$\",\n        \"FunctionName\": \"arn:aws:lambda:<my-region>:<my-account-id>:function:<my-lambda-function-name>:$LATEST\"\n      },\n      \"Retry\": [\n        {\n          \"ErrorEquals\": [\n            \"Lambda.ServiceException\",\n            \"Lambda.AWSLambdaException\",\n            \"Lambda.SdkClientException\"\n          ],\n          \"IntervalSeconds\": 2,\n          \"MaxAttempts\": 6,\n          \"BackoffRate\": 2\n        }\n      ],\n      \"End\": true\n    }\n  }\n}\n```\n\nAs can be seen in the workflow, I am iterating over the list from the previous 'Pass' block and mapping those to iterate inside 'Map' block and trying to access the Parameters of 'Map' block inside each iteration. Iteration works fine with number of iterators, but I can't access the Parameters inside the iteration. I get this error:\n```\n{\n  \"resourceType\": \"aws-sdk:sagemakerruntime\",\n  \"resource\": \"invokeEndpoint\",\n  \"error\": \"SageMakerRuntime.ValidationErrorException\",\n  \"cause\": \"1 validation error detected: Value '$.EndpointName' at 'endpointName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9])* (Service: SageMakerRuntime, Status Code: 400, Request ID: ed5cad0c-28d9-4913-853b-e5f9ac924444)\"\n}\n```\nSo, I presume the error is because \"$.EndpointName\" is not being filled with the relevant value. How do I avoid this.\n\nBut, when I open the failed execution and check the InvokeEndpoint block from graph-inspector, input to that is what I expected and above JSON-Paths to fetch the parameters should work, but they don't.  \n[screenshot-of-graph-inspector](https:\/\/i.stack.imgur.com\/3gXsM.jpg)\n\nWhat's causing the error and How do I fix this?",
        "Challenge_closed_time":1647513967263,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647503861594,
        "Challenge_favorite_count":0.0,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUDc1foN9TQhe3OYkkGzCKhQ\/aws-stepfunctions-sagemaker-s-invokeendpoint-block-throws-validation-error-when-fetching-parameters-for-itself-inside-iterator-of-map-block",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":13.7,
        "Challenge_reading_time":41.85,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":2.8071302778,
        "Challenge_title":"AWS StepFunctions - SageMaker's InvokeEndpoint block throws \"validation error\" when fetching parameters for itself inside iterator of Map block",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":296.0,
        "Challenge_word_count":336,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Solution_body":"In general (as mentioned [here in the parameters doc](https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/connect-parameters.html)), you also need to **end the parameter name** with `.$` when using a JSON Path.\n\nIt looks like you're doing that some places in your sample JSON (e.g. `\"InvocationBody.$\": \"$.body.InputData\"`), but not in others (`\"EndpointName\": \"$.EndpointName\"`), so I think the reason you're seeing the validation error here is that Step Functions is trying to interpret `$.EndpointName` as literally the name of the endpoint (which doesn't satisfy `^[a-zA-Z0-9](-*[a-zA-Z0-9])*`!)\n\nSo suggest you change to `EndpointName.$` and `Body.$` in your InvokeEndpoint parameters",
        "Solution_comment_count":0.0,
        "Solution_link_count":1.0,
        "Solution_readability":10.6,
        "Solution_reading_time":8.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":87.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":0.5504083334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm working for a campany located in Germany. We want to use Azure Machine Learning (and other stuff like that).   <br \/>\nWe are only allowed to use Azure in the Region &quot;Germany&quot;, because the data of our customers cannot left germany.  <\/p>\n<p>Now I saw, that a lot of stuff in Azure Machine Learning is not available in Germany?  <\/p>\n<p>Questions:  <\/p>\n<ol>\n<li> Is that true?  <\/li>\n<li> Does some one now, at what time Microsoft plans to make the stuff available in Germany?  <\/li>\n<\/ol>\n<p>Thank you for a answer!  <\/p>\n<p>Patrick  <\/p>",
        "Challenge_closed_time":1612862196647,
        "Challenge_comment_count":0,
        "Challenge_created_time":1612860215177,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/265151\/azure-machine-learning-(and-cognitive-services)-is",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.5,
        "Challenge_reading_time":7.73,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.5504083334,
        "Challenge_title":"Azure Machine Learning (and cognitive services) is not supported in Region \"Germany\"?",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":106,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=1d5d0740-76fa-4574-b01a-5fcee1ddf5b1\">@Patrick Huber  <\/a>     <br \/>\nYes, Azure Machine Learning is not available in Germany region.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/65687-image.png?platform=QnA\" alt=\"65687-image.png\" \/>    <\/p>\n<p><a href=\"https:\/\/feedback.azure.com\/forums\/34192--general-feedback\">Please check in Azure feedback<\/a>    <\/p>\n<p>If the Answer is helpful, please click <code>Accept Answer<\/code> and <strong>up-vote<\/strong>, this can be beneficial to other community members.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_link_count":2.0,
        "Solution_readability":14.1,
        "Solution_reading_time":7.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":48.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":59.2573102778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p><strong>Issue<\/strong>  <br \/>\nI am trying prepare and then submit a new experiment to Azure Machine Learning from an Azure Function in Python. I therefore register a new dataset for my Azure ML workspace, which contains the training data for my ML model using <code>dataset.register(...<\/code>. However, when I try to create this dataset with the following line of code  <\/p>\n<pre><code>dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths)\n<\/code><\/pre>\n<p>then I get a <code>Failure Exception: OSError: [Errno 30] Read-only file system ...<\/code>.  <\/p>\n<p><strong>Ideas<\/strong>  <\/p>\n<ol>\n<li> I know that I shouldn't write to the file system from within an Azure function if possible. But I actually don't want to write anything to the local file system. I only want to create the dataset as a reference to my blob storage under <code>datastore_path<\/code> and then register this to my Azure Machine Learning workspace. But it seems that the method <code>from_delimited_files<\/code> is trying to write to the file system anyway (maybe some caching?).  <\/li>\n<li> I also know that there is a temp folder in which writing temporary files is permitted. However, I belive I cannot really control where this method is writing data. I already tried changing the current working directory to this temp folder just before the function call using <code>os.chdir(tempfile.gettempdir())<\/code>, but that didn't help.  <\/li>\n<\/ol>\n<p>Any other ideas? I don't think I am doing something particularly unusually...  <\/p>\n<p><strong>Details<\/strong>  <br \/>\nI am using python 3.7 and azureml-sdk 1.9.0 and I can run the python script locally without problems. I currently deploy from VSCode using the Azure Functions extension version 0.23.0 (and an Azure DevOps pipeline for CI\/CD).  <\/p>\n<p>Here is my full stack trace:  <\/p>\n<pre><code>Microsoft.Azure.WebJobs.Host.FunctionInvocationException: Exception while executing function: Functions.HttpTrigger_Train\n ---&gt; Microsoft.Azure.WebJobs.Script.Workers.Rpc.RpcException: Result: Failure\nException: OSError: [Errno 30] Read-only file system: '\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/bin\/deps.lock'\nStack:   File &quot;\/azure-functions-host\/workers\/python\/3.7\/LINUX\/X64\/azure_functions_worker\/dispatcher.py&quot;, line 345, in _handle__invocation_request\n    self.__run_sync_func, invocation_id, fi.func, args)\n  File &quot;\/usr\/local\/lib\/python3.7\/concurrent\/futures\/thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;\/azure-functions-host\/workers\/python\/3.7\/LINUX\/X64\/azure_functions_worker\/dispatcher.py&quot;, line 480, in __run_sync_func\n    return func(**params)\n  File &quot;\/home\/site\/wwwroot\/HttpTrigger_Train\/__init__.py&quot;, line 11, in main\n    train()\n  File &quot;\/home\/site\/wwwroot\/shared_code\/train.py&quot;, line 70, in train\n    dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/data\/_loggerfactory.py&quot;, line 126, in wrapper\n    return func(*args, **kwargs)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/data\/dataset_factory.py&quot;, line 308, in from_delimited_files\n    quoting=support_multi_line)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/readers.py&quot;, line 100, in read_csv\n    df = Dataflow._path_to_get_files_block(path, archive_options)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/dataflow.py&quot;, line 2387, in _path_to_get_files_block\n    return datastore_to_dataflow(path)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_datastore_helper.py&quot;, line 41, in datastore_to_dataflow\n    datastore, datastore_value = get_datastore_value(source)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_datastore_helper.py&quot;, line 83, in get_datastore_value\n    _set_auth_type(workspace)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_datastore_helper.py&quot;, line 134, in _set_auth_type\n    get_engine_api().set_aml_auth(SetAmlAuthMessageArgument(AuthType.SERVICEPRINCIPAL, json.dumps(auth)))\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/api.py&quot;, line 18, in get_engine_api\n    _engine_api = EngineAPI()\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/api.py&quot;, line 55, in __init__\n    self._message_channel = launch_engine()\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/engine.py&quot;, line 300, in launch_engine\n    dependencies_path = runtime.ensure_dependencies()\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/runtime.py&quot;, line 141, in ensure_dependencies\n    with _FileLock(deps_lock_path, raise_on_timeout=timeout_exception):\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/runtime.py&quot;, line 113, in __enter__\n    self.acquire()\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/runtime.py&quot;, line 72, in acquire\n    self.lockfile = os.open(self.lockfile_path, os.O_CREAT | os.O_EXCL | os.O_RDWR)\n\n   at Microsoft.Azure.WebJobs.Script.Description.WorkerFunctionInvoker.InvokeCore(Object[] parameters, FunctionInvocationContext context) in \/src\/azure-functions-host\/src\/WebJobs.Script\/Description\/Workers\/WorkerFunctionInvoker.cs:line 85\n   at Microsoft.Azure.WebJobs.Script.Description.FunctionInvokerBase.Invoke(Object[] parameters) in \/src\/azure-functions-host\/src\/WebJobs.Script\/Description\/FunctionInvokerBase.cs:line 85\n   at Microsoft.Azure.WebJobs.Script.Description.FunctionGenerator.Coerce[T](Task`1 src) in \/src\/azure-functions-host\/src\/WebJobs.Script\/Description\/FunctionGenerator.cs:line 225\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionInvoker`2.InvokeAsync(Object instance, Object[] arguments) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionInvoker.cs:line 52\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.InvokeAsync(IFunctionInvoker invoker, ParameterHelper parameterHelper, CancellationTokenSource timeoutTokenSource, CancellationTokenSource functionCancellationTokenSource, Boolean throwOnTimeout, TimeSpan timerInterval, IFunctionInstance instance) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 587\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithWatchersAsync(IFunctionInstanceEx instance, ParameterHelper parameterHelper, ILogger logger, CancellationTokenSource functionCancellationTokenSource) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 532\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithLoggingAsync(IFunctionInstanceEx instance, ParameterHelper parameterHelper, IFunctionOutputDefinition outputDefinition, ILogger logger, CancellationTokenSource functionCancellationTokenSource) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 470\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithLoggingAsync(IFunctionInstanceEx instance, FunctionStartedMessage message, FunctionInstanceLogEntry instanceLogEntry, ParameterHelper parameterHelper, ILogger logger, CancellationToken cancellationToken) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 278\n   --- End of inner exception stack trace ---\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithLoggingAsync(IFunctionInstanceEx instance, FunctionStartedMessage message, FunctionInstanceLogEntry instanceLogEntry, ParameterHelper parameterHelper, ILogger logger, CancellationToken cancellationToken) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 325\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.TryExecuteAsyncCore(IFunctionInstanceEx functionInstance, CancellationToken cancellationToken) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 117\n<\/code><\/pre>",
        "Challenge_closed_time":1597616477787,
        "Challenge_comment_count":1,
        "Challenge_created_time":1597403151470,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/67126\/failure-exception-oserror-(errno-30)-read-only-fil",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":25.3,
        "Challenge_reading_time":114.76,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":73,
        "Challenge_solved_time":59.2573102778,
        "Challenge_title":"\u201cFailure Exception: OSError: [Errno 30] Read-only file system\u201d when using AzureML in Python Azure Function",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":null,
        "Challenge_word_count":575,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>The issue was an incompatible OS version in my virtual environment.    <\/p>\n<p>A huge thanks goes to <a href=\"https:\/\/learn.microsoft.com\/answers\/users\/111253\/pramodvalavala-msft.html\">PramodValavala-MSFT<\/a> for his idea to create a docker container! Following his suggestion, I suddenly got the following error message for the  <code>dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths)<\/code> command:    <\/p>\n<blockquote>\n<p>Exception: NotImplementedError: Unsupported Linux distribution debian 10.    <\/p>\n<\/blockquote>\n<p>which reminded me of the following warning in the azure machine learning documentation:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/17829-image.png?platform=QnA\" alt=\"17829-image.png\" \/>    <\/p>\n<p>Choosing the predefined docker image <code>2.0-python3.7<\/code> (running Debian 9) instead of  <code>3.0-python3.7<\/code> (running Debian 10) solved the issue (see <a href=\"https:\/\/hub.docker.com\/_\/microsoft-azure-functions-python\">https:\/\/hub.docker.com\/_\/microsoft-azure-functions-python<\/a>).    <\/p>\n<p>I suspect that the default virtual environment, which I was using originally, also ran on an incompatible OS.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_link_count":3.0,
        "Solution_readability":16.0,
        "Solution_reading_time":15.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":113.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":80.6917763889,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Hi!<br>\nI want to run a bayesian HP sweep with 5-fold CV. In other words I want the bayesian sweep to decide upon a configuration, run 5 runs with that configuration and log each run. The easiest way to do this would be to have a variable in the sweep, called e.g. fold_id which simply can take the values 1,2,3,4,5 and force the agent to always test all the fold_ids per configuration.<\/p>\n<p>Is there any way to make this possible? I.e force the sweep agent to always test a variable, even though running a bayesian sweep. In a way it would be like running a grid sweep over a bayesian sweep.<\/p>\n<p>One way I\u2019ve thought of is by making all parameters nested inside the fold_id variable but it still won\u2019t probably do what I\u2019m after.<\/p>\n<p>I\u2019ve seen the k-fold CV example code, but it\u2019s quite advanced and does not seem to work when running on CUDA and my understanding of multiprocessing is limited.<\/p>\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1663260668920,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662970178525,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/force-bayesian-sweep-to-run-certain-variable-tests\/3098",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":5.8,
        "Challenge_reading_time":11.87,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":80.6917763889,
        "Challenge_title":"Force Bayesian sweep to run certain variable tests",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":131.0,
        "Challenge_word_count":172,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>You can create a nested sweep where each fold could be a list and then you can then iterate over those values.  Make sure that the run name changes per run so that way the runs don\u2019t overwrite one another.<\/p>\n<p>Here\u2019s an example config of a nested sweep:<\/p>\n<pre><code class=\"lang-auto\">command:\n  - ${env}\n  - python3\n  - ${program}\n  - ${args}\nmethod: random\nparameters:\n  MULTI_STAGE_TRAINING:\n    value:\n      DEPTH_SCALE:\n        - 100\n        - 100\n      HEAD:\n        - OBJECT_DETECTION\n      NETWORK:\n        - net_a\n        - net_b\n        - net_c\n      NUM_EPOCHS_IN_EACH_STAGE:\n        - 0\n        - 1\n        - 2\n        - 3\n      NUM_STAGES:\n        - 0\n        - 1\n        - 2\n        - 3\n        - 4\n        - 5\n        - 6\n        - 7\n        - 8\n        - 9\n      OPTIMIZER_PARAMS_PER_STAGE:\n        lr:\n          - 0\n          - 1\n          - 2\n          - 3\n          - 4\n          - 5\n          - 6\n          - 7\n          - 8\n          - 9\n        momentum:\n          - 0\n          - 1\n          - 2\n          - 3\n          - 4\n          - 5\n          - 6\n          - 7\n          - 8\n          - 9\n        weight_decay:\n          - 0\n          - 1\n          - 2\n          - 3\n          - 4\n          - 5\n          - 6\n          - 7\n          - 8\n          - 9\n  epochs:\n    value: 10\nprogram: script.py\n<\/code><\/pre>\n<p>And here\u2019s a script that is able to run it:<\/p>\n<pre><code class=\"lang-auto\">import wandb\n\u200b\ndef create_sweep(\n    sweep_config:dict,\n    update:bool,\n    project:str,\n    entity:str):\n    \n    parameters_dict = {'MULTI_STAGE_TRAINING':\n                   {'value':\n                    {'NUM_STAGES':list(range(10)),\n                     'OPTIMIZER_PARAMS_PER_STAGE':\n                     {'lr':list(range(10)),'momentum': list(range(10)),'weight_decay':list(range(10))},\n                     'NUM_EPOCHS_IN_EACH_STAGE':list(range(4)),\n                     'NETWORK':['net_a','net_b','net_c'],\n                     'HEAD':['OBJECT_DETECTION'],\n                     'DEPTH_SCALE': [100,100]\n                     }\n                    }\n                   }\n    sweep_config['parameters'] = parameters_dict\n    \n    parameters_dict.update({\n    'epochs': {\n        'value': 10}\n    })\n    return wandb.sweep(sweep_config,entity=entity,project=project)\n\u200b\nif __name__ == '__main__':\n\u200b\n    SWEEP_CONFIG = {\n    'method': 'random',\n    'program':'script.py',\n    'command':['${env}', 'python3', '${program}','${args}']\n    }\n    ENTITY = 'demonstrations'\n    PROJECT = 'sweep_gm'\n    UPDATE = True\n\u200b\n    sweep = create_sweep(\n        sweep_config=SWEEP_CONFIG,\n        entity=ENTITY,\n        project=PROJECT,\n        update=UPDATE)\n<\/code><\/pre>\n<p>Let me know if you need any further help with this!<\/p>",
        "Solution_comment_count":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.8,
        "Solution_reading_time":23.25,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":197.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":0.2003183333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,  <\/p>\n<p>no practice exam for the Azure certification DP-100 seems to be available in the official channels. It would, however, be very helpful for preparing.  <br \/>\nBy any chance, do you plan to introduce such a resource any time soon?  <\/p>\n<p>Thanks and best regards  <br \/>\nTim<\/p>",
        "Challenge_closed_time":1593083339536,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593082618390,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/39948\/practice-exam-for-dp-100",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.9,
        "Challenge_reading_time":3.89,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.2003183333,
        "Challenge_title":"Practice Exam for DP-100",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":null,
        "Challenge_word_count":51,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>Hi,    <\/p>\n<p>Microsoft Certification \/ Exams are currently not supported in the Q&amp;A forums, the supported products are listed over here <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/products\">https:\/\/learn.microsoft.com\/en-us\/answers\/products<\/a> (more to be added later on).      <\/p>\n<p>You can ask the experts in the dedicated <strong>Microsoft Certification - Preparation Resources<\/strong> forum over here:        <br \/>\n<a href=\"https:\/\/trainingsupport.microsoft.com\/en-us\/mcp\/forum\/mcp_exams-mcp_prep\">https:\/\/trainingsupport.microsoft.com\/en-us\/mcp\/forum\/mcp_exams-mcp_prep<\/a>    <\/p>\n<p>(Please don't forget to accept helpful replies as answer)      <\/p>\n<p>Best regards,      <br \/>\nLeon    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_link_count":2.0,
        "Solution_readability":17.7,
        "Solution_reading_time":9.15,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":16.7097122222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm in classic Azure ML mode. I am working on my first ever experiment, so please be patient..    <\/p>\n<p>I cannot locate column selector for CSV data to filter out columns. I found this:    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/select-columns-in-dataset\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/select-columns-in-dataset<\/a>    <\/p>\n<p>And I'm following a tutorial (behind pay wall, from 2017) that shows it in the right hand side properties pane. It says in his example to add the &quot;Select columns in dataset&quot; and it shows the option of &quot;launch column selector&quot;.    <\/p>\n<p>I have browsed through every single choice in the left menu, but cannot locate it... I have no idea what I am missing.    <\/p>\n<p>I need to exclude columns from the data set. Then later I need to make some of the fields &quot;categorical&quot;. Input on that would be appreciated too, unless it becomes obvious from other information provided.    <\/p>\n<p>Please help me :) Thanks in advance for patience and\/or assistance.<\/p>",
        "Challenge_closed_time":1619488667407,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619428512443,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/371634\/beginner-question-cannot-locate-column-selector-fo",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":15.09,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":16.7097122222,
        "Challenge_title":"Beginner question - Cannot locate column selector for CSV data to filter out columns",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":162,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>Hello,    <\/p>\n<p>First you need to navigate to Data Transformation  - &gt; Manipulation -&gt; Select columns in dataset, drag that into your process.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/91523-image.png?platform=QnA\" alt=\"91523-image.png\" \/>    <\/p>\n<p>Then, left click on the module and click launch column selector.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/91497-image.png?platform=QnA\" alt=\"91497-image.png\" \/>    <\/p>\n<p>And you can do you want now.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/91524-image.png?platform=QnA\" alt=\"91524-image.png\" \/>    <\/p>\n<p>Please accept the answer if you feel helpful, thanks.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_link_count":3.0,
        "Solution_readability":13.7,
        "Solution_reading_time":9.81,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":69.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":6.0913719444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello, I am trying to add a user Id column to my dataset but I don't want the user Id to impact the results of the ML.  <\/p>\n<p>I am using Auto ML on my dataset to generate a model and then deployed the model to an endpoint.  <\/p>\n<p>Currently I am calling the endpoint like:  <\/p>\n<pre><code>{&quot;data&quot;:[\n       {\n          &quot;TEMP&quot;:&quot;X&quot;,\n        }\n    ]\n}\n<\/code><\/pre>\n<p>and I would like to call it like:  <\/p>\n<pre><code>{&quot;data&quot;:[\n    {\n      &quot;TEMP&quot;:&quot;X&quot;,\n      &quot;userID&quot;: 5434643\n     }\n  ]}\n<\/code><\/pre>\n<p>I'm wondering if there is a way I can do this? I've seen about using Clear Feature in Edit Metadata for the Designer but I'm wondering if something similar can be done for automated ML?  <\/p>\n<p>Thanks so much!  <\/p>",
        "Challenge_closed_time":1627949805196,
        "Challenge_comment_count":0,
        "Challenge_created_time":1627927876257,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/498759\/clear-feature-with-auto-ml",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.6,
        "Challenge_reading_time":9.42,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":6.0913719444,
        "Challenge_title":"Clear Feature with Auto ML",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":118,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>Hi, thanks for reaching out. You can customize featurization in automl to only include features relevant for prediction. Here's the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-features#customize-featurization\">documentation<\/a>. Hope it helps!    <\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_link_count":1.0,
        "Solution_readability":16.4,
        "Solution_reading_time":3.97,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":26.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":0.4346333334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi , i would like to know if it is possible to change the location of AzureML workspace after creating it ?  <br \/>\nRight now i do not find any option to change it manually on the UI. We want to move the server location to a different country.  <br \/>\nAny leads would be helpful. Thanks<\/p>",
        "Challenge_closed_time":1643796022067,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643794457387,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/719406\/change-location-of-azure-ml-workspace",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.4,
        "Challenge_reading_time":3.94,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.4346333334,
        "Challenge_title":"change location of Azure ML workspace?",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":59,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>Based on the below document, ML workspace can't be moved across region. Probably, you will have to create a new resource in target region and move artifacts \/ pipelines \/ child resources to it (not so familiar with ML)     <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/azure-resource-manager\/management\/move-support-resources#microsoftmachinelearning\">https:\/\/learn.microsoft.com\/en-us\/azure\/azure-resource-manager\/management\/move-support-resources#microsoftmachinelearning<\/a>    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-move-workspace#limitations\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-move-workspace#limitations<\/a>    <\/p>\n<p>----------    <\/p>\n<p>Please don't forget to <strong>Accept Answer<\/strong> and <strong>Up-vote<\/strong> if the response helped -- Vaibhav<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_link_count":2.0,
        "Solution_readability":20.9,
        "Solution_reading_time":11.43,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":59.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":20.1061447222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi,<br>\nI am tuning hyper-params with <code>wandb.sweep<\/code>. For now, in order to get the best group of hyper-params, I have to look for the best group on my own and record those params manually. I wonder whether there is a way to extract or collect reuslts of hyper-params automatically by <code>wandb<\/code>?<br>\nThanks a lot!<\/p>",
        "Challenge_closed_time":1682009719808,
        "Challenge_comment_count":0,
        "Challenge_created_time":1681937337687,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/collect-results-from-sweep\/4238",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.6,
        "Challenge_reading_time":4.54,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":20.1061447222,
        "Challenge_title":"Collect results from sweep",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":37.0,
        "Challenge_word_count":57,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>Hello!<\/p>\n<p>We have <a href=\"https:\/\/colab.research.google.com\/github\/wandb\/examples\/blob\/master\/colabs\/pytorch\/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb#scrollTo=G01IM4yVkc6u\" rel=\"noopener nofollow ugc\">Parallel Coordinate plots and Hyper Parameter Importance Plots<\/a> in the UI that can help with looking for the best group! In terms of collecting results of sweeps, the hyperparameters are automatically logged to the <code>config.yaml<\/code> file in your run\u2019s file tab.  However, if you want to collect the hyperparameters  yourself, you can also access individual hyperparameter values using <code>wandb.config['hyperparameter-name']<\/code> within the <code>main()<\/code> function you are running your sweep on. <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/config\">Here<\/a> is our documentation on ways to use access and update the config file.<\/p>",
        "Solution_comment_count":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.6,
        "Solution_reading_time":11.56,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":91.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":1.0,
        "Challenge_adjusted_solved_time":7.9730555556,
        "Challenge_answer_count":1,
        "Challenge_body":"Can Amazon SageMaker endpoints be fitted with multiple Amazon Elastic Inference accelerators?\n\nI see that [in EC2 it's possible][1], however I don't see it mentioned in Amazon SageMaker documentation.\n\n\n  [1]: https:\/\/docs.aws.amazon.com\/elastic-inference\/latest\/developerguide\/basics.html",
        "Challenge_closed_time":1604506639000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604477936000,
        "Challenge_favorite_count":0.0,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUwU8IHcSVQ3eH9-fGx0KZCA\/can-amazon-sagemaker-endpoints-be-fitted-with-multiple-amazon-elastic-inference-accelerators",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.2,
        "Challenge_reading_time":4.95,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":7.9730555556,
        "Challenge_title":"Can Amazon SageMaker endpoints be fitted with multiple Amazon Elastic Inference accelerators?",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":76.0,
        "Challenge_word_count":42,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Solution_body":"No, they cant be; multi-attach is only supported with EC2.",
        "Solution_comment_count":0.0,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":0.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":10.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":36.5390777778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi, here is the details of my issue.  <br \/>\nI want to execute a distributed training run with the Tensorflow framework and Horovod.  <br \/>\nTo do this, I've configured a environment called &quot;tf_env&quot; as follow :<\/p>\n<pre><code># Create the environment : the dependencies are in the .yml file\ntf_env = Environment.from_conda_specification(name=&quot;tensorflow_environment&quot;, file_path=&quot;experiments\/package-list.yml&quot;)\n\n# Register the environment\ntf_env.register(workspace=ws)\n\n# Specify a GPU base image\ntf_env.docker.enabled = True\ntf_env.docker.base_image = 'mcr.microsoft.com\/azureml\/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04'\n<\/code><\/pre>\n<p>Where my &quot;package-list.yml&quot; contains all the dependencies my &quot;train_script.py&quot; requires.  <br \/>\nI've defined my ScriptConfigRun as follow :<\/p>\n<pre><code>arguments = [\n    (... other arguments ...)\n    &quot;--ds&quot;,  images_ds.as_mount()\n]\n\nsrc = ScriptRunConfig(\n    source_directory=&quot;experiments&quot;,\n    script='train_script.py',\n    arguments=arguments,\n    compute_target=compute_target,\n    environment=tf_env,\n    distributed_job_config=MpiConfiguration(node_count=2)\n)\n<\/code><\/pre>\n<p>Then, when I want to submit the run :<\/p>\n<pre><code>run = best_model_experiment.submit(config=src)\n<\/code><\/pre>\n<p>... it raises this error I don't understand :<\/p>\n<pre><code>ExperimentExecutionException: ExperimentExecutionException:\n    Message: {\n    &quot;error_details&quot;: {\n        &quot;componentName&quot;: &quot;execution&quot;,\n        &quot;correlation&quot;: {\n            &quot;operation&quot;: &quot;***&quot;,\n            &quot;request&quot;: &quot;***&quot;\n        },\n        &quot;environment&quot;: &quot;westeurope&quot;,\n        &quot;error&quot;: {\n            &quot;code&quot;: &quot;UserError&quot;,\n            &quot;message&quot;: &quot;Error when parsing request; unable to deserialize request body&quot;\n        },\n        &quot;location&quot;: &quot;westeurope&quot;,\n        &quot;time&quot;: &quot;***&quot;\n    },\n    &quot;status_code&quot;: 400,\n    &quot;url&quot;: &quot;https:\/\/westeurope.experiments.azureml.net\/execution\/v1.0\/subscriptions\/***\/resourceGroups\/***\/providers\/Microsoft.MachineLearningServices\/workspaces\/***\/experiments\/experiment\/snapshotrun?runId=experiment***&quot;\n}\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;{\\n    \\&quot;error_details\\&quot;: {\\n        \\&quot;componentName\\&quot;: \\&quot;execution\\&quot;,\\n        \\&quot;correlation\\&quot;: {\\n            \\&quot;operation\\&quot;: \\&quot;***\\&quot;,\\n            \\&quot;request\\&quot;: \\&quot;***\\&quot;\\n        },\\n        \\&quot;environment\\&quot;: \\&quot;westeurope\\&quot;,\\n        \\&quot;error\\&quot;: {\\n            \\&quot;code\\&quot;: \\&quot;UserError\\&quot;,\\n            \\&quot;message\\&quot;: \\&quot;Error when parsing request; unable to deserialize request body\\&quot;\\n        },\\n        \\&quot;location\\&quot;: \\&quot;westeurope\\&quot;,\\n        \\&quot;time\\&quot;: \\&quot;***\\&quot;\\n    },\\n    \\&quot;status_code\\&quot;: 400,\\n    \\&quot;url\\&quot;: \\&quot;https:\/\/westeurope.experiments.azureml.net\/execution\/v1.0\/subscriptions\/***\/resourceGroups\/***\/providers\/Microsoft.MachineLearningServices\/workspaces\/***\/experiments\/experiment\/snapshotrun?runId=experiment_***\\&quot;\\n}&quot;\n    }\n}\n<\/code><\/pre>\n<p>Could you please help me decrypt this error ?  <br \/>\nThank you.<\/p>",
        "Challenge_closed_time":1620024521707,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619892981027,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/379458\/azure-machine-learning-experimentexecutionexceptio",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":24.5,
        "Challenge_reading_time":44.31,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":36.5390777778,
        "Challenge_title":"Azure Machine Learning ExperimentExecutionException while submitting a distributed training run !",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":216,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>Issue solved ! I've given a list in arguments to argparse so it could'nt deserialized the object.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":1.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Challenge_adjusted_solved_time":1.3832386111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello,\nAre there any drawbacks I should be aware of if we restrict user access to only a single region? \n\nWe use a variety of AWS services but mainly S3 and Sagemaker Studio. Our team is located in various locations so their default regions are different.  It has been a challenge to keep track of studio instances when they are created in different regions so we are now considering restricting access to a single region. Are there issues that we may face in that case? Any services we may miss?",
        "Challenge_closed_time":1642705784219,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642700804560,
        "Challenge_favorite_count":0.0,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUxt7fqO9HQrKWDfi4V4Lagg\/pros-and-cons-of-restricting-user-access-to-certain-regions",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":6.68,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.3832386111,
        "Challenge_title":"Pros and cons of restricting user access to certain regions",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":95.0,
        "Challenge_word_count":99,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Solution_body":"I would take a look at [this](https:\/\/docs.aws.amazon.com\/organizations\/latest\/userguide\/orgs_manage_policies_scps_examples_general.html#example-scp-deny-region) for some potential edge cases. In summary, you may need to allow us-east-1 and us-west-2 in addition to whatever regions your team is in since they host some of the global service endpoints (like IAM, Route 53, Global Accelerator, and a few others). For STS, I would use the [regional endpoints](https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/id_credentials_temp_enable-regions.html) if you aren't already.",
        "Solution_comment_count":1.0,
        "Solution_link_count":2.0,
        "Solution_readability":15.2,
        "Solution_reading_time":7.48,
        "Solution_score_count":4.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":39.0455230556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hello,<\/p>\n<p>As I cannot simply upload infinitely many weights using artifacts, I also want to store some locally.<br>\nFor naming, I would like to use the sweep id and\/or the run id.<\/p>\n<p>Can I access that somehow in the train function I hand over to the agent?<\/p>\n<p>Thanks<\/p>\n<p>Markus<\/p>",
        "Challenge_closed_time":1660860269032,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660719705149,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/access-sweep-id-and-run-id-within-train-function-for-local-weight-storage\/2948",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.5,
        "Challenge_reading_time":4.66,
        "Challenge_score_count":2.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":39.0455230556,
        "Challenge_title":"Access sweep_id and run_id within train() function for local weight storage",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":143.0,
        "Challenge_word_count":59,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>Hey <a class=\"mention\" href=\"\/u\/markuskarner\">@markuskarner<\/a>!<\/p>\n<p>The <code>wandb.Run<\/code> object that is returned from <code>wandb.init<\/code> contains this information as properties. You should be able to access <code>run.id<\/code> and <code>run.sweep_id<\/code> in the train function after calling <code>run = wandb.init(...)<\/code>.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":4.98,
        "Solution_score_count":null,
        "Solution_sentence_count":8.0,
        "Solution_word_count":36.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":1.7286111111,
        "Challenge_answer_count":0,
        "Challenge_body":"Context\n\nLet's say I have following structure\n\nroot\n\u251c\u2500\u2500model_files\n\u251c\u2500\u2500polyaxonfiles\n\u2502      \u251c\u2500\u2500build.yml\n\u2502      \u2514\u2500\u2500run.yml\n\u251c\u2500\u2500utils\n\u2514\u2500\u2500dockerfiles\n         \u251c\u2500\u2500Dockerfile.0\n         \u2514\u2500\u2500Dockerfile.1\n\n\nmy build looks as simple as:\n\nversion: 1.1\nkind: operation\nname: build\nparams:\n  context:\n    value: \"{{ globals.run_artifacts_path }}\/uploads\"\n  destination:\n    connection: docker-registry\n    value: machine-learning\/polyaxon-tutorial:1\n\nhubRef: kaniko\n\nRunning this with command\n\npolyaxon run -f polyaxonfiles\/build.yml -u\n\nGives me very much expected error:\n\nError: error resolving dockerfile path: please provide a valid path to a Dockerfile within the build context with --dockerfile\n\nSo the Kaniko expects Dockerfile to exist in root of the context by default. Does the polyaxonfile specification exposes a way to overwrite the default as suggested with error message?",
        "Challenge_closed_time":1620665885000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620659662000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1315",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.1,
        "Challenge_reading_time":11.08,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1.7286111111,
        "Challenge_title":"(How) Can I pass path to Dockerfile using Kaniko?",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":112,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"Hi @captainCapitalism, next release we will be pushing a new set of guides, including guides for building containers and how to pass a custom dockerfile context.\n\nWe will update this thread as soon as we start updating the documentation's guides section.",
        "Solution_comment_count":4.0,
        "Solution_link_count":0.0,
        "Solution_readability":11.3,
        "Solution_reading_time":3.13,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":41.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":115.1236055556,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Dear community,<\/p>\n<p>I want to use WandB locally in my VSCode project, but my Ipython kernel keeps dying. After restarting the kernel it always prints out the errore message: \u201cFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\u201d and \u201cwandb: Currently logged in as: XXX. Use <code>wandb login --relogin<\/code> to force relogin\u201d<\/p>\n<p>I already tried to import os, as well as setting the environment variabele to my local notebook, but this didnt change a thing. I am using python 3.9.12<\/p>\n<p>I hope you can help me in this matter<\/p>",
        "Challenge_closed_time":1675104696691,
        "Challenge_comment_count":0,
        "Challenge_created_time":1674690251711,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/using-wandb-in-visual-studio-code\/3752",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.1,
        "Challenge_reading_time":8.18,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":115.1236055556,
        "Challenge_title":"Using WandB in Visual Studio Code",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":123.0,
        "Challenge_word_count":107,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/martin-woschitz\">@martin-woschitz<\/a> thank you for reporting this issue. This first message is just a warning so it shouldn\u2019t cause any issues running your code, also the second message  is an informatio output about the user account that you\u2019ve logged in. When does the Ipython kernel stops working, is it when you\u2019re running a python script? would it be possible to make a new virtual environment and install <code>wandb<\/code> only there to test if that\u2019s what\u2019s causing the issue for you?<\/p>",
        "Solution_comment_count":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.3,
        "Solution_reading_time":6.6,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":82.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":0.1086111111,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nI would like to compare the top 4 experiments based on a specific metrics.\n\nCurrently I query the top experiment using the cli:\n\npolyaxon ops ls -q \"name: GROUP_NAME, metrics.loss:<0.002\"  -s \"metrics.loss\" -l 5\n\nAnd then I copy\/paste the run UUIDs to:\n\npolyaxon run --hub tensorboard:mulit-run -P uuids=UUID1,UUID2,UUID3,UUID4,UUID5",
        "Challenge_closed_time":1649336377000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649335986000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1483",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":8.0,
        "Challenge_reading_time":4.94,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.1086111111,
        "Challenge_title":"How can I start a Tensorboard for the top 5 experiments",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":60,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"From the UI or using a YAML file, you can run:\n\nversion: 1.1\nkind: operation\nhubRef: tensorboard:multi-run\njoins:\n- query: \"name: GROUP_NAME, metrics.loss:<0.002,  kind:job\"  \n  sort: \"metrics.loss\"\n  limit: 5\n  params:\n    uuids: {value: \"globals.uuid\"}\n\nNote that in the UI if create a filter \/ sort configuration, you can automatically create a multi-run Tensorboard based on that query, for example:",
        "Solution_comment_count":0.0,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":4.88,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":55.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":28.0968075,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>ML studio is, by default picking up Python 3.6 kernel, even when I'm specifying use Python 3.8 AzureML kernel. In UI, it's changed but not actually.     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/183312-image.png?platform=QnA\" alt=\"183312-image.png\" \/>    <\/p>",
        "Challenge_closed_time":1647451118727,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647349970220,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/772790\/azure-ml-studio-is-bugged-out-and-can-not-create-a",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":4.99,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":28.0968075,
        "Challenge_title":"Azure ML Studio is bugged out and can not create a Microsoft ticket under MSDN. Need a few suggestions",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":49,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>Hi, thanks for reaching out.  It looks like the command you ran isn't supported. A better command to test kernel changes is shown below:  <\/p>\n<pre><code>from platform import python_version\nprint(python_version())\n<\/code><\/pre>\n<p>Hope this helps!<\/p>\n",
        "Solution_comment_count":6.0,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":3.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":2.5933255556,
        "Challenge_answer_count":7,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I was wondering if anybody from the W&amp;B team can confirm that there is an outage at the moment.<\/p>\n<p>I\u2019ve been having issues starting runs and it seems like other folks are having issues syncing runs with a network time out error (<a href=\"https:\/\/github.com\/wandb\/wandb\/issues\/4424\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">[CLI]: canno't sync my runs \u00b7 Issue #4424 \u00b7 wandb\/wandb \u00b7 GitHub<\/a>). It\u2019s been ongoing for about 2 hours now.<\/p>\n<p>The status page is saying everything is fine - <a href=\"https:\/\/status.wandb.com\" rel=\"noopener nofollow ugc\">https:\/\/status.wandb.com<\/a><\/p>\n<p>All the best,<br>\nAlexey<\/p>",
        "Challenge_closed_time":1667343090214,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667333754242,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/w-b-outage-11-1-2022\/3360",
        "Challenge_link_count":3,
        "Challenge_participation_count":7,
        "Challenge_readability":8.0,
        "Challenge_reading_time":8.59,
        "Challenge_score_count":3.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":2.5933255556,
        "Challenge_title":"W&B Outage? 11\/1\/2022",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":106.0,
        "Challenge_word_count":84,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>Thank you for your patience! Our engineers were able to push a fix for this. There\u2019s still currently an issue regarding batch moving runs, but for the most part this issue has been resolved.<\/p>",
        "Solution_comment_count":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.2,
        "Solution_reading_time":2.41,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":34.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":19.7083872222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How to import data not by passing it as an argument,     <br \/>\nI do not want to do as the tutorial <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-with-datasets?source=docs\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-with-datasets?source=docs<\/a><\/p>",
        "Challenge_closed_time":1654106099667,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654035149473,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/872050\/azure-machine-learning-sdk",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":17.8,
        "Challenge_reading_time":4.54,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":19.7083872222,
        "Challenge_title":"azure machine learning SDK",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":26,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=fd7f30ec-b4f1-4575-a425-a49ca6a1a14e\">@ben wu  <\/a>     <\/p>\n<p>Thanks for reaching out to us, there is the code sample from engineering team    <\/p>\n<pre><code>from azureml.core import ScriptRunConfig  \n  \ninput_data=titanic_ds.as_named_input('input_data').as_mount()  \nsrc = ScriptRunConfig(source_directory=script_folder,  \n                      script='train_titanic.py',  \n                      compute_target=compute_target)  \nsrc.run_config.data = {input_data.name: input_data }  \n# Submit the run configuration for your training run  \nrun = experiment.submit(src)  \nrun.wait_for_completion(show_output=True)    \n<\/code><\/pre>\n<p>In your script, you can get the mounted path via environment variable, which is the value you specified in as_named_input. For the sample code above, the environment variable will be input_data.    <\/p>\n<p>I hopet this helps.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_link_count":0.0,
        "Solution_readability":12.0,
        "Solution_reading_time":12.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":103.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Challenge_adjusted_solved_time":1.3894444444,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nCustomer who loads the e-bike data to S3 wants to get AI\/ML insight from sensor data.\nThe e-bike sensor data are size about 4KB files each and posted in S3 buckets.\nThe sensor data is put into format like this\n\ntimestamp1, sensorA, sensorB, sensorC, ..., sensorZ\ntimestamp2, sensorA, sensorB, sensorC, ..., sensorZ\ntimestamp3, sensorA, sensorB, sensorC, ..., sensorZ\n...\n\nThen these sensor data are put into one file about 4KB size.\n\nThe plan I have is to\n\n* Read S3 objects\n* Parse S3 object with Lambda. I thought about Glue but wanted to put data in DynamoDB where Glue does not seem to support. Also, Glue seems to be more expensive.\n* Put the data in DynamoDB with bike ID as primary key and timestamp as sort key.\n* Use SageMaker to learn with the DynamoDB data. There will be separate discussion on choosing which model and making time-series inferencing.\n* If we need to re-learn, it will use the DynamoDB data, not from S3. I think it will be faster to get data from DynamoDB instead from the raw S3 data.\n* Also, I think we can filter out some bad input or apply little modification to DynamoDB data (shifting time stamps to the correct time, etc.)\n* Make inferencing output based on the model.\n\nWhat do you think? Would you agree? Would you approach the problem differently?\nWould you rather learn from S3 directly via Athena or direct S3 access?\nOr would you rather use Glue and Redshift?\nBut the data about 100MB would be sufficient to train the model we have in mind.\nGlue and Redshift maybe overkill.\nCurrently, Korea region does not support Timestream database. So, time series database closest in Korea could be DynamoDB.\n\nPlease share your thoughts.\n\nThanks!",
        "Challenge_closed_time":1607362919000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607357917000,
        "Challenge_favorite_count":0.0,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUebPx1UeWSGOb_3i0TXlBWA\/ai-ml-data-acquisition-and-preprocessing",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.1,
        "Challenge_reading_time":20.82,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":1.3894444444,
        "Challenge_title":"[AI\/ML] Data acquisition and preprocessing",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":81.0,
        "Challenge_word_count":289,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Solution_body":"**Thoughts about DynamoDB**\n\nPer GB, DynamoDB is around 5X more cost per GB of data stored. On top of that, you have RCU\/WCU cost.\n\nI would recommend keeping data in S3. Not only is it more cost effective, but with S3, you do not have to worry about RCU\/WCU cost or throughput of DynamoDB. \n\nSageMaker notebooks and training instances can read directly from S3, and S3 has high-throughput. I don't think you will have a problem with 100 MB datasets. \n\nIf you need to prep\/transform your data, you can do the transformations \"in place\" in S3 using Glue, Athena, Glue DataBrew, GlueStudio, etc. \n\n\n**Glue and DynamoDB**\n\n> I thought about Glue but wanted to put data in DynamoDB where Glue does not seem to support.\n\nGlue supports both Python and Spark jobs. If you use a Glue Python job, you can import the boto3 (AWS SDK) library and write to DynamoDB.\n\n**Other strategies**\n\nHow is your customer ingesting the sensor data \/ how is it being written to S3? Are they using AWS IoT Core? \n\nRegardless, the pattern you've described thus far is:\n\nDevice -> Sensor data in S3 -> Transform with Lambda -> store data in DynamoDB\n\nAn alternative approach you could consider is using Kinesis Firehose with Lambda transformations. This will allow you to do \"in-line\" parsing \/ transformation of your data before it is ever written to S3, this removing the need to re-read the data from S3 and apply transformations after the fact. Firehose also allows you to write the stored data in formats such as Parquet, which can help with cost and subsequent query performance. \n\nIf you want to store both raw data and transformed data, you can use a \"fanout\" pattern with Kinesis Streams\/Firehose, where one output is raw data to S3 and the other is a transformed stream.",
        "Solution_comment_count":0.0,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":20.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":299.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":7.2541636111,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I have a few questions regarding the hyperparameter sweeps from Python.<br>\nI am wanting to essentially start a few tmux sessions on my server, and connect them all to the same sweep agent, but no keyword in the sweep_config (that i have found) allow me to connect to a specific sweep ID, and rather just a sweep name that doesnt connect to the same sweep, but just makes multiple sweeps of the same name.  If this possible or strongly advised against due to computational usage or similar?<\/p>\n<p>Furthermore, sweeps take up a great deal of storage requirements due to saving all the models, is it possible to store the model file from the best model only, while keeping the statistics from all the models for plots and interpretation? This would allow me to keep the great information gathered from sweeps, while not taking up 100+ GB from a single sweep.<\/p>\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1641593349991,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641567235002,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/connecting-to-existing-sweep-from-python\/1721",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":12.3,
        "Challenge_reading_time":11.22,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":7.2541636111,
        "Challenge_title":"Connecting to existing sweep from Python",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":248.0,
        "Challenge_word_count":156,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>I found the issue, i was trying to create a new wandb.sweep(config, project, entity) and pass the ID into the config dictionary, but instead i just needed to take the ID directly, and just do sweep_id = sweep_id_string which worked.<\/p>",
        "Solution_comment_count":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.7,
        "Solution_reading_time":2.94,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":39.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Challenge_adjusted_solved_time":285.5134875,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I can't see a data drift module anywhere in v2 of the Azure ML Python SDK. Is this missing or what's the deal? If so, are there any plans of bringing it into v2?<\/p>",
        "Challenge_closed_time":1658311324112,
        "Challenge_comment_count":1,
        "Challenge_created_time":1657283475557,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/919651\/datadrift-in-azure-ml-sdk-v2",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":2.4,
        "Challenge_reading_time":2.34,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":285.5134875,
        "Challenge_title":"Datadrift in Azure ML SDK v2",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":39,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=1dc2a0bd-ac4b-413b-bae7-930e0079e70d\">@SH  <\/a>     <\/p>\n<p>I have a good news for you, I just got confirmation from product team, the datadrift function will be in SDK V2 for sure. But for now we don't have an exact date for when. I have forwarded this feedback to product group and we hope we can bring this feature in near future.     <\/p>\n<p>I hope this helps.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_link_count":0.0,
        "Solution_readability":5.3,
        "Solution_reading_time":6.49,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":85.0,
        "Tool":"Azure Machine Learning"
    }
]