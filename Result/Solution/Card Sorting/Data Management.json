[
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi, I am getting the error from the subject line when i try to inner join a dataset of 850K rows and 3 columns (parquet data file of around 4mb) with another with 300K rows and 10 columns (parquet data file is about 1mb). I'm using Azure ML Studio Designer  <\/p>\n<p>My compute is Standard Dv2 Family vCPUs (20% of utilization).  <\/p>\n<p>I was surprised by this hitting a limit. Any idea on how i should proceed?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619357291613,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/370636\/moduleexceptionmessage-moduleoutofmemory-memory-ha",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":6.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"ModuleExceptionMessage:ModuleOutOfMemory: Memory has been exhausted, unable to complete running of module.",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":87,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>i manage to do this by trainning the model in a subset of records (using the Sample model).    <\/p>\n<p>Also noted that the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/apply-sql-transformation\">documentation<\/a> implies that an out of memory error is dependant on the RAM of the client \/ Designer user machine not the compute selected (or at least that is my understanding of the note at the beginning of the doc)    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.0,
        "Solution_reading_time":5.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":7,
        "Challenge_body":"<p>I log values which have names in the form of <code>test\/temp_top-k.---1<\/code> (I want the dashes for sorting reasons). I can create graph panels with these values, but they do not show up in the column view. When I <code>Manage Columns<\/code> they are not listed in the <code>Hidden Columns<\/code>. When I search for them, it gives me no (an empty) result. Even when I select <code>Show All<\/code> they don\u2019t show up in the column view. A bug?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647509310562,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/logged-value-available-in-graph-panel-but-not-in-columns\/2100",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":5.6,
        "Challenge_reading_time":6.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Logged value available in graph panel, but not in columns",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":653.0,
        "Challenge_word_count":85,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Leslie,<\/p>\n<p>I apologize for not responding earlier. I assumed to be notified by e-mail when this thread is updated. Probably I need to check my settings, or \u201cwatch\u201d this thread.<\/p>\n<p>I log via Pytorch Lightning:<\/p>\n<pre><code class=\"lang-auto\">wandb_logger = WandbLogger(project=settings.project_name, log_model=True)\nwandb_logger.watch(model, log='gradients', log_freq=50, log_graph=True)\n<\/code><\/pre>\n<p>The actual code for the logging is this:<\/p>\n<pre><code class=\"lang-auto\">temp_accs_top_k = {f'{k:-&gt;4d}': v for k, v in zip(settings.ks, temp_accs)}\nlightning_module.log(f'{split}\/temp_top-k', temp_accs_top_k, batch_size=lightning_module.batch_size)\n<\/code><\/pre>\n<p>That looks a bit odd I suppose. The code is in a function that I call from several different <code>pl.LightningModule<\/code>s. The variable <code>lightning_module<\/code> refers to that module. The parameter <code>temp_accs_top_k<\/code> evaluates to (straight from the debugger):<\/p>\n<p><code>{'---1': 0.00019996000628452748, '---2': 0.00019996000628452748, '---3': 0.00039992001256905496, '---5': 0.0005998800043016672, '--10': 0.0005998800043016672, '--20': 0.001399720087647438, '--50': 0.004199160262942314, '-100': 0.007598480209708214, '1000': 0.08318336308002472}<\/code><\/p>\n<p>Which is wrong. But I am seeing the values in the graph panels (see attached screenshot).<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3ae06dc0b1399ce16eb917dfb94b60a5e0f77acd.png\" alt=\"Screen Shot 2022-03-22 at 21.52.37\" data-base62-sha1=\"8oQwPNwBrsRhQBp0p6SuZ6ht6NL\" width=\"412\" height=\"275\"><\/p>\n<p>I changed the code so that <code>temp_accs_top_k<\/code>now contains <code>{'test\/temp_top-k.---1': 0.2963850498199463, 'test\/temp_top-k.---2': 0.3962452709674835, 'test\/temp_top-k.---3': 0.44557619094848633, 'test\/temp_top-k.---5': 0.5052925944328308, 'test\/temp_top-k.--10': 0.5733972191810608, 'test\/temp_top-k.--20': 0.6277211904525757, 'test\/temp_top-k.--50': 0.6810465455055237, 'test\/temp_top-k.-100': 0.716596782207489, 'test\/temp_top-k.1000': 0.802676260471344}<\/code>.<\/p>\n<p>I log in a loop since Pytorch Lightning can\u2019t log a dict (I believe). I know that wandb does it, but I need the batch_size parameter (I have two dataloaders with different sizes\/lenghts and need to make sure that Pytorch Lightning does not get confused with steps\/epochs).<\/p>\n<pre><code class=\"lang-auto\">for k, v in temp_accs_top_k.items():\n    lightning_module.log(k, v, batch_size=lightning_module.batch_size)\n<\/code><\/pre>\n<p>Update: just realized that Pytorch Lightning has a <code>log_dict<\/code> function which lets me get rid of the awkward for loop.<\/p>\n<p>So the \u201cbug\u201d is more like \u201cwhy did it work in the first place (in the graph panels)?\u201d<\/p>\n<p>Hope that\u2019s not too much to digest and it is traceable.<\/p>\n<p>Best,<br>\nStephan<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.0,
        "Solution_reading_time":37.55,
        "Solution_score_count":null,
        "Solution_sentence_count":33.0,
        "Solution_word_count":281.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1467943515392,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Danang, H\u1ea3i Ch\u00e2u District, Da Nang, Vietnam",
        "Answerer_reputation_count":173.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to integrate MLFlow to my project. Because I'm using <code>tf.keras.fit_generator()<\/code> for my training so I take advantage of <code>mlflow.tensorflow.autolog()<\/code>(<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.tensorflow.html#mlflow.tensorflow.autolog\" rel=\"nofollow noreferrer\">docs<\/a> here) to enable automatic logging of metrics and parameters:<\/p>\n<pre><code>    model = Unet()\n    optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n\n    metrics = [IOUScore(threshold=0.5), FScore(threshold=0.5)]\n    model.compile(optimizer, customized_loss, metrics)\n\n    callbacks = [\n        tf.keras.callbacks.ModelCheckpoint(&quot;model.h5&quot;, save_weights_only=True, save_best_only=True, mode='min'),\n        tf.keras.callbacks.TensorBoard(log_dir='.\/logs', profile_batch=0, update_freq='batch'),\n    ]\n\n\n    train_dataset = Dataset(src_dir=SOURCE_DIR)\n\n    train_data_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True)\n\n   \n    with mlflow.start_run():\n        mlflow.tensorflow.autolog()\n        mlflow.log_param(&quot;batch_size&quot;, BATCH_SIZE)\n\n        model.fit_generator(\n            train_data_loader,\n            steps_per_epoch=len(train_data_loader),\n            epochs=EPOCHS,\n            callbacks=callbacks   \n            )\n<\/code><\/pre>\n<p>I expected something like this (just a demonstration taken from the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#visualizing-metrics\" rel=\"nofollow noreferrer\">docs<\/a>):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/eG56Z.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eG56Z.png\" alt=\"Visualization on the docs\" \/><\/a><\/p>\n<p>However, after the training finished, this is what I got:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fS1JD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fS1JD.png\" alt=\"f1_score visualization\" \/><\/a><\/p>\n<p>How can I configure so that the metric plot will update and display its value at each epoch instead of just showing the latest value?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593764146630,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1594008626392,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62711259",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":17.5,
        "Challenge_reading_time":26.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"Customize metric visualization in MLFlow UI when using mlflow.tensorflow.autolog()",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1035.0,
        "Challenge_word_count":145,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1467943515392,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Danang, H\u1ea3i Ch\u00e2u District, Da Nang, Vietnam",
        "Poster_reputation_count":173.0,
        "Poster_view_count":28.0,
        "Solution_body":"<p>After searching around, I found <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/2390\" rel=\"nofollow noreferrer\">this issue<\/a> related to my problem above. Actually, all my metrics just logged once each training (instead of each epoch as my intuitive thought). The reason is I didn't specify the <code>every_n_iter<\/code> parameter in <code>mlflow.tensorflow.autolog()<\/code>, which indicates how many 'iterations' must pass before MLflow logs metric executed (see the <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tensorflow.html#mlflow.tensorflow.autolog\" rel=\"nofollow noreferrer\">docs<\/a>). So, changing my code to:<\/p>\n<p><code>mlflow.tensorflow.autolog(every_n_iter=1)<\/code><\/p>\n<p>fixed the problem.<\/p>\n<p>P\/s: Remember that in TF 2.x, an 'iteration' is an epoch (in TF 1.x it's a batch).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.7,
        "Solution_reading_time":10.74,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":87.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Hello, I want to create a grid panel containing several linecharts but the selected runs are different.<br>\nTo elaborate on my need: I have several algorithms, evaluated across timesteps on several environments. I want one line chart per environment. To illustrate, my final requirement is to get something that looks like this:<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/f\/f2059dafd12562e424e2b66a3e2aabb2b4e1b204.png\" alt=\"Screenshot from 2023-01-19 13-12-26\" data-base62-sha1=\"yx1zXviDcru4wcBIHNfwKEhCyFK\" width=\"435\" height=\"213\"><\/p>\n<p>How would you do that?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1674130582264,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/different-run-sets-within-a-panel-grid\/3721",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":11.5,
        "Challenge_reading_time":8.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Different run sets within a panel grid",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":210.0,
        "Challenge_word_count":72,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/qgallouedec\">@qgallouedec<\/a>, thanks for writing in! As you\u2019d like to have a figure with independent charts inside, one option would be to use <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/custom-charts\">custom charts<\/a>. I let you <a href=\"https:\/\/vega.github.io\/vega\/examples\/barley-trellis-plot\/\" rel=\"noopener nofollow ugc\">here<\/a> and <a href=\"https:\/\/vega.github.io\/vega\/examples\/brushing-scatter-plots\/\" rel=\"noopener nofollow ugc\">here<\/a> two Vega examples that may be useful in order to build your figure. Other way I can think of for this is rendering the chart through <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log\/plots#matplotlib-and-plotly-plots\">Plotly\/Matplotlib<\/a> and then log it. Please let me know if any of these would be useful!<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":12.1,
        "Solution_reading_time":10.47,
        "Solution_score_count":null,
        "Solution_sentence_count":8.0,
        "Solution_word_count":85.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1538757797860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Dallas, TX, USA",
        "Answerer_reputation_count":5671.0,
        "Answerer_view_count":629.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm reading a file from my S3 bucket in a notebook in sagemaker studio (same account) using the following code:<\/p>\n<pre><code>dataset_path_in_h5=&quot;\/Mode1\/SingleFault\/SimulationCompleted\/IDV2\/Mode1_IDVInfo_2_100\/Run1\/processdata&quot;\ns3 = s3fs.S3FileSystem()\nh5_file = h5py.File(s3.open(s3url,'rb'), 'r')\ndata = h5_file.get(dataset_path_in_h5)\n<\/code><\/pre>\n<p>But I don't know what actually append behind the scene, does the whole h5 file is being transferred  ? that's seems unlikely as the code is executed quite fast while the whole file is 20GB. Or is just the dataset in dataset_path_in_h5 is transferred ?\nI suppose that if the whole file is transferred at each call it could cost me a lot.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662025046283,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73567221",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":9.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"reading hdf5 file from s3 to sagemaker, is the whole file transferred?",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":18.0,
        "Challenge_word_count":101,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1576136255052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":795.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p>When you open the file, a file object is created. It has a tiny memory footprint. The dataset values aren't read into memory until you access them.<\/p>\n<p>You are returning <code>data<\/code> as a NumPy array. That loads the entire dataset into memory. (NOTE: the <code>.get()<\/code> method you are using is deprecated. Current syntax is provided in the example.)<\/p>\n<p>As an alternative to returning an array, you can create a dataset object (which also has a small memory foorprint). When you do, the data is read into memory as you need it. Dataset objects behave like NumPy arrays. (Use of a dataset object vs NumPy array depends on downstream usage. Frequently you don't need an array, but sometimes they are required.) Also, if chunked I\/O was enabled when the dataset was created, datasets are read in chunks.<\/p>\n<p>Differences shown below. Note, I used Python's file context manager to open the file. It avoids problems if the file isn't closed properly (you forget or the program exits prematurely).<\/p>\n<pre><code>dataset_path_in_h5=&quot;\/Mode1\/SingleFault\/SimulationCompleted\/IDV2\/Mode1_IDVInfo_2_100\/Run1\/processdata&quot;\ns3 = s3fs.S3FileSystem()\nwith h5py.File(s3.open(s3url,'rb'), 'r') as h5_file:\n     # your way to get a numpy array -- .get() is depreciated:\n     data = h5_file.get(dataset_path_in_h5)\n     # this is the preferred syntax to return an array:\n     data_arr = h5_file[dataset_path_in_h5][()]\n     # this returns a h5py dataset object:\n     data_ds = h5_file[dataset_path_in_h5]  # deleted [()] \n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":19.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":207.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1221810788500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paderborn, North-Rhine-Westphalia, Germany",
        "Answerer_reputation_count":68522.0,
        "Answerer_view_count":7896.0,
        "Challenge_adjusted_solved_time":6278.0877944445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Python code generated from an ml software with mlflow to read a dataframe, perform some table operations and output a dataframe. I am able to run the code successfully and save the new dataframe as an artifact. However I am unable to log the model using log_model because it is not a lr or classifier model where we train and fit. I want to log a model for this so that it can be served with new data and deployed with a rest API<\/p>\n<pre><code>df = pd.read_csv(r&quot;\/home\/xxxx.csv&quot;)\n\n\nwith mlflow.start_run():\n    def getPrediction(row):\n        \n        perform_some_python_operaions \n\n        return [Status_prediction, Status_0_probability, Status_1_probability]\n    columnValues = []\n    for column in columns:\n        columnValues.append([])\n\n    for index, row in df.iterrows():\n        results = getPrediction(row)\n        for n in range(len(results)):\n            columnValues[n].append(results[n])\n\n    for n in range(len(columns)):\n        df[columns[n]] = columnValues[n]\n\n    df.to_csv('dataset_statistics.csv')\n    mlflow.log_artifact('dataset_statistics.csv')\n   \n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611586824463,
        "Challenge_favorite_count":3.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65887231",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.9,
        "Challenge_reading_time":13.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Use mlflow to serve a custom python model for scoring",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":3026.0,
        "Challenge_word_count":134,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1573739890560,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>MLflow supports <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#custom-python-models\" rel=\"nofollow noreferrer\">custom models<\/a> of mlflow.pyfunc flavor.  You can create a custom  class  inherited from the <code>mlflow.pyfunc.PythonModel<\/code>, that needs to provide function <code>predict<\/code> for performing predictions, and optional <code>load_context<\/code> to load the necessary artifacts, like this (adopted from the docs):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class MyModel(mlflow.pyfunc.PythonModel):\n\n    def load_context(self, context):\n        # load your artifacts\n\n    def predict(self, context, model_input):\n        return my_predict(model_input.values)\n<\/code><\/pre>\n<p>You can log to MLflow whatever artifacts you need for your models, define Conda environment if necessary, etc.<br \/>\nThen you can use <code>save_model<\/code> with your class to save your implementation, that could be loaded with <code>load_model<\/code> and do the <code>predict<\/code> using your model:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.pyfunc.save_model(\n        path=mlflow_pyfunc_model_path, \n        python_model=MyModel(), \n        artifacts=artifacts)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(mlflow_pyfunc_model_path)\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1634187940523,
        "Solution_link_count":1.0,
        "Solution_readability":17.3,
        "Solution_reading_time":16.79,
        "Solution_score_count":9.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":118.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running a pipeline using the Azure ML Python SDK v2. For one of the pipeline steps, a <code>.csv<\/code> file in blob storage is being passed as input using <code>InputOutputModes.DIRECT<\/code>. In my understanding, this means that the pipeline step will be receiving a uri filepath <code>azureml:\/\/[blah]<\/code> . Within the pipeline step, I am calling <code>pandas.read_csv()<\/code> on the input, but am receiving the error <code>protocol not known : azureml<\/code> . This same function call works in a notebook using the Python 3.10 - SDK v2 kernel. So, my question is what packages need to be in the pipeline step's environment in order to be able to call <code>pandas.read_csv()<\/code> with the uri filepath? I've tried many different things, the most recent environment I tried is below. Any help is appreciated...<\/p>\n<pre><code class=\"lang-yaml\">name: prs-env\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.7.6\n  - pip\n  - pip:\n      - matplotlib~=3.5.0\n      - psutil~=5.8.0\n      - tqdm~=4.62.0\n      - pandas~=1.3.0\n      - scipy~=1.7.0\n      - numpy~=1.21.0\n      - ipykernel~=6.0\n      - azureml-core==1.48.0\n      - azureml-defaults==1.48.0\n      - azureml-mlflow==1.48.0\n      - azureml-telemetry==1.48.0\n      - scikit-learn~=1.0.0\n      - debugpy~=1.6.3\n      - usaddress\n      - fsspec\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1682358462623,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1254146\/what-packages-are-needed-to-use-a-azure-uri-with-p",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.2,
        "Challenge_reading_time":16.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"What packages are needed to use a Azure URI with pandas",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":160,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Use the <a href=\"https:\/\/pypi.org\/project\/azureml-fsspec\/\"><code>azureml-fsspec<\/code><\/a> package:<\/p>\n<pre><code class=\"lang-bash\">pip install azureml-fsspec\n<\/code><\/pre>\n<p><strong>Note:<\/strong> The accepted URI format for the datastore URI is:\n<code>azureml:\/\/subscriptions\/([^\/]+)\/resourcegroups\/([^\/]+)\/workspaces\/([^\/]+)\/datastores\/([^\/]+)\/paths\/([^\/]+)<\/code><\/p>\n<p>This should technically work:<\/p>\n<pre><code class=\"lang-python\">import azureml-fsspec\nimport pandas as pd\n\n# credentials and variables\nsubscription = '&lt;subscription_id&gt;'\nresource_group = '&lt;resource_group&gt;'\nworkspace = '&lt;workspace&gt;'\ndatastore_name = '&lt;datastore&gt;'\npath_on_datastore '&lt;path&gt;'\nfile = '&lt;myfile.csv&gt;'\n\n# generate uri:\nuri = f'azureml:\/\/subscriptions\/{subscription}\/resourcegroups\/{resource_group}\/workspaces\/{workspace}\/datastores\/{datastore_name}\/paths\/{path_on_datastore}\/{file}'\n\n# read via pandas\ndf = pd.read_csv(uri)\n<\/code><\/pre>\n<p><em>See <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data-interactive?view=azureml-api-2&amp;tabs=adls\">Azure Machine Learning - Access Data from Azure Cloud Storage During Interactive Development<\/a> for details.<\/em><\/p>\n<p>or you could try the <code>AzureMachineLearningFileSystem<\/code> class from the package:<\/p>\n<pre><code class=\"lang-python\">import pandas\nfrom azureml.fsspec import AzureMachineLearningFileSystem\n\n# instantiate file system using following URI\nfs = AzureMachineLearningFileSystem('azureml:\/\/subscriptions\/&lt;subid&gt;\/resourcegroups\/&lt;rgname&gt;\/workspaces\/&lt;workspace_name&gt;\/datastore\/datastorename')\n\nfs.ls() # list folders\/files in datastore 'datastorename'\n\n# use an open context\nwith fs.open('.\/folder1\/file1.csv') as f:\n    # do some process\n    df = pandas.read_csv(f)\n<\/code><\/pre>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.4,
        "Solution_reading_time":24.62,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":118.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello everyone!    <\/p>\n<p>I am taking the <a href=\"https:\/\/learn.microsoft.com\/en-us\/learn\/modules\/create-regression-model-azure-machine-learning-designer\/explore-data\">Create a Regression Model with Azure Machine Learning designer<\/a> course in Microsoft Learn. When I perform the steps in the Explore Data section, after selecting the &quot;Edit column&quot; button of the &quot;Select Columns in Dataset&quot; module in Designer, it will be stuck in the &quot;loading&quot; state. Therefore, I cannot proceed to the next step.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/84160-1.png?platform=QnA\" alt=\"84160-1.png\" \/>    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/84253-2.png?platform=QnA\" alt=\"84253-2.png\" \/>    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/84294-3.png?platform=QnA\" alt=\"84294-3.png\" \/>    <\/p>\n<p>Thank you very much!    <\/p>\n<p>Best regards,    <br \/>\nLing    <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":7,
        "Challenge_created_time":1617523186537,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/343427\/after-selecting-the-edit-column-button-of-the-sele",
        "Challenge_link_count":4,
        "Challenge_participation_count":8,
        "Challenge_readability":13.4,
        "Challenge_reading_time":14.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"After selecting the \"Edit column\" button of the \"Select Columns in Dataset\" module in Designer, it will be stuck in the \"loading\" state.",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":107,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=2f4c69cd-f08d-480e-af96-29ddb1d93452\">@\u9ad8\u6977\u4fee  <\/a> This issue is now fixed in all regions and it does not require an additional parameter to be added to the URL. Please try and let us know if it works fine. <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":2.98,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":36.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I have a few questions regarding the hyperparameter sweeps from Python.<br>\nI am wanting to essentially start a few tmux sessions on my server, and connect them all to the same sweep agent, but no keyword in the sweep_config (that i have found) allow me to connect to a specific sweep ID, and rather just a sweep name that doesnt connect to the same sweep, but just makes multiple sweeps of the same name.  If this possible or strongly advised against due to computational usage or similar?<\/p>\n<p>Furthermore, sweeps take up a great deal of storage requirements due to saving all the models, is it possible to store the model file from the best model only, while keeping the statistics from all the models for plots and interpretation? This would allow me to keep the great information gathered from sweeps, while not taking up 100+ GB from a single sweep.<\/p>\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641567235002,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/connecting-to-existing-sweep-from-python\/1721",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":12.3,
        "Challenge_reading_time":11.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Connecting to existing sweep from Python",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":253.0,
        "Challenge_word_count":156,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I found the issue, i was trying to create a new wandb.sweep(config, project, entity) and pass the ID into the config dictionary, but instead i just needed to take the ID directly, and just do sweep_id = sweep_id_string which worked.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.7,
        "Solution_reading_time":2.94,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":39.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm very excited on the newly released Azure Machine Learning service (preview), which is a great step up from the previous (and deprecated) Machine Learning Workbench.<\/p>\n\n<p>However, I am thinking a lot about the best practice on structuring the folders and files in my project(s). I'll try to explain my thoughts.<\/p>\n\n<p>Looking at the documentation for the training of a model (e.g. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml#create-an-estimator\" rel=\"nofollow noreferrer\">Tutorial #1<\/a>), there seems to be good-practice to put all training scripts and necessary additional scripts inside a subfolder, so that it can be passed into the <code>Estimator<\/code> object without also passing all other files in the project. This is fine.<\/p>\n\n<p>But when working with the deployment of the service, specifically the deployment of the image, the documentation (e.g. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml#deploy-in-aci\" rel=\"nofollow noreferrer\">Tutorial #2<\/a>) seems to indicate that the scoring script need to be located in the root folder. If I try to refer to a script located in a subfolder, I get an error message saying<\/p>\n\n<p><code>WebserviceException: Unable to use a driver file not in current directory. Please navigate to the location of the driver file and try again.<\/code><\/p>\n\n<p>This may not be a big deal. Except, I have some additional scripts that I import both in the training script and in the scoring script, and I don't want to duplicate those additional scripts to be able to import them in both the training and the scoring scripts.<\/p>\n\n<p>I am working mainly in Jupyter Notebooks when executing the training and the deployment, and I could of course use some tricks to read the particular scripts from some other folder, save them to disk as a copy, execute the training or deployment while referring to the copies and finally delete the copies. This would be a decent workaround, but it seems to me that there should be a better way than just decent.<\/p>\n\n<p>What do you think?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539614031870,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52819122",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":28.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"What is the best practice on folder structure for Azure Machine Learning service (preview) projects",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":782.0,
        "Challenge_word_count":326,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1463756509236,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Uppsala, Sverige",
        "Poster_reputation_count":400.0,
        "Poster_view_count":43.0,
        "Solution_body":"<p>Currently, the score.py needs to be in current working directory, but dependency scripts - the <em>dependencies<\/em> argument to  <em>ContainerImage.image_configuration<\/em> - can be in a subfolder.<\/p>\n\n<p>Therefore, you should be able to use folder structure like this:<\/p>\n\n<pre><code>.\/score.py \n.\/myscripts\/train.py \n.\/myscripts\/common.py\n<\/code><\/pre>\n\n<p>Note that the relative folder structure is preserved during web service deployment; if you reference the common file in subfolder from your score.py, that reference should be valid within deployed image.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.1,
        "Solution_reading_time":7.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":69.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":7,
        "Challenge_body":"**Environment**:\r\n- NNI version: 2.0\r\n- NNI mode (local|remote|pai): remote\r\n- Client OS: Windows 10\r\n- Server OS (for remote mode only): Linux\r\n- Python version: 3.6.12\r\n- PyTorch\/TensorFlow version:  PyTorch1.7.1\r\n- Is conda\/virtualenv\/venv used?: conda\r\n- Is running in Docker?: No\r\n\r\n**Log message**:\r\n - nnimanager.log: \r\n [2021-04-07 15:24:48] INFO [ 'Datastore initialization done' ]\r\n[2021-04-07 15:24:48] INFO [ 'RestServer start' ]\r\n[2021-04-07 15:24:48] INFO [ 'RestServer base port is 8086' ]\r\n[2021-04-07 15:24:48] INFO [ 'Rest server listening on: http:\/\/0.0.0.0:8086' ]\r\n[2021-04-07 15:24:51] INFO [ 'NNIManager setClusterMetadata, key: aml_config, value: {\"subscriptionId\":\"xxxxxxxxxxxx\",\"resourceGroup\":\"xxxxxxxxxxxxxxx\",\"workspaceName\":\"xxxxxxxxxxxxxx\",\"computeTarget\":\"xxxxxxxxxxxxxxxx\"}' ]\r\n[2021-04-07 15:24:53] INFO [ 'NNIManager setClusterMetadata, key: nni_manager_ip, value: {\"nniManagerIp\":\"10.194.188.18\"}' ]\r\n[2021-04-07 15:24:55] INFO [ 'NNIManager setClusterMetadata, key: trial_config, value: {\"command\":\"python3 mnist.py\",\"codeDir\":\"C:\\\\\\\\Users\\\\\\\\yanmi\\\\\\\\nni\\\\\\\\examples\\\\\\\\trials\\\\\\\\mnist-pytorch\\\\\\\\.\",\"image\":\"msranni\/nni\"}' ]\r\n[2021-04-07 15:24:57] INFO [ 'Starting experiment: fy8bAx3K' ]\r\n[2021-04-07 15:24:57] INFO [ 'Change NNIManager status from: INITIALIZED to: RUNNING' ]\r\n[2021-04-07 15:24:57] INFO [ 'Add event listeners' ]\r\n[2021-04-07 15:24:57] INFO [ 'TrialDispatcher: started channel: AMLCommandChannel' ]\r\n[2021-04-07 15:24:57] INFO [ 'TrialDispatcher: copying code and settings.' ]\r\n[2021-04-07 15:25:06] INFO [ 'NNIManager received command from dispatcher: ID, ' ]\r\n[2021-04-07 15:25:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 0, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 128, \"lr\": 0.1, \"momentum\": 0.754420685055723}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:25:07] INFO [ 'Initialize environments total number: 0' ]\r\n[2021-04-07 15:25:07] INFO [ 'TrialDispatcher: run loop started.' ]\r\n[2021-04-07 15:25:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":0,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 0, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.754420685055723}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:25:12] INFO [ 'Assign environment service aml to environment XlEgg' ]\r\n[2021-04-07 15:25:24] INFO [ 'requested environment nni_exp_fy8bAx3K_1617834318_1a1683cd and job id is nni_exp_fy8bAx3K_env_XlEgg.' ]\r\n[2021-04-07 15:25:24] INFO [ 'requested new environment, live trials: 1, live environments: 0, neededEnvironmentCount: 1, requestedCount: 1' ]\r\n[2021-04-07 15:25:42] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from UNKNOWN to WAITING.' ]\r\n[2021-04-07 15:28:27] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from WAITING to RUNNING.' ]\r\n[2021-04-07 15:29:35] INFO [ 'TrialDispatcher: env nni_exp_fy8bAx3K_1617834318_1a1683cd received initialized message and runner is ready, env status: RUNNING.' ]\r\n[2021-04-07 15:29:35] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial KH7Ph.' ]\r\n[2021-04-07 15:29:36] INFO [ 'Trial job KH7Ph status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:34:06] INFO [ 'Trial job KH7Ph status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:34:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 1, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 128, \"lr\": 0.001, \"momentum\": 0.48989819362825704}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:34:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":1,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 1, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.001, \\\\\"momentum\\\\\": 0.48989819362825704}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:34:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial Uh6jK.' ]\r\n[2021-04-07 15:34:16] INFO [ 'Trial job Uh6jK status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:37:26] INFO [ 'Trial job Uh6jK status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:37:26] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 2, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 256, \"lr\": 0.01, \"momentum\": 0.7009004965885264}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:37:31] INFO [ 'submitTrialJob: form: {\"sequenceId\":2,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 2, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 16, \\\\\"hidden_size\\\\\": 256, \\\\\"lr\\\\\": 0.01, \\\\\"momentum\\\\\": 0.7009004965885264}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:37:32] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial JqjWi.' ]\r\n[2021-04-07 15:37:36] INFO [ 'Trial job JqjWi status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:41:26] INFO [ 'Trial job JqjWi status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:41:26] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 3, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 32, \"hidden_size\": 512, \"lr\": 0.1, \"momentum\": 0.6258856288476062}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:41:31] INFO [ 'submitTrialJob: form: {\"sequenceId\":3,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 3, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 32, \\\\\"hidden_size\\\\\": 512, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.6258856288476062}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:41:32] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial ijhph.' ]\r\n[2021-04-07 15:41:36] INFO [ 'Trial job ijhph status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:46:31] INFO [ 'Trial job ijhph status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:46:31] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 4, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 1024, \"lr\": 0.1, \"momentum\": 0.30905289366545063}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:46:36] INFO [ 'submitTrialJob: form: {\"sequenceId\":4,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 4, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.30905289366545063}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:46:38] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial bElKu.' ]\r\n[2021-04-07 15:46:41] INFO [ 'Trial job bElKu status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:52:06] INFO [ 'Trial job bElKu status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:52:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 5, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 1024, \"lr\": 0.0001, \"momentum\": 0.0003307910747289977}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:52:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":5,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 5, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.0001, \\\\\"momentum\\\\\": 0.0003307910747289977}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:52:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial upDtw.' ]\r\n[2021-04-07 15:52:16] INFO [ 'Trial job upDtw status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:56:07] INFO [ 'Trial job upDtw status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:56:07] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 6, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 64, \"hidden_size\": 128, \"lr\": 0.01, \"momentum\": 0.876381947693324}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:56:12] INFO [ 'submitTrialJob: form: {\"sequenceId\":6,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 6, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 64, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.01, \\\\\"momentum\\\\\": 0.876381947693324}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:56:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial Zgo5Q.' ]\r\n[2021-04-07 15:56:17] INFO [ 'Trial job Zgo5Q status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:59:32] INFO [ 'Trial job Zgo5Q status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:59:32] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 7, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 512, \"lr\": 0.1, \"momentum\": 0.2948365715286464}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:59:37] INFO [ 'submitTrialJob: form: {\"sequenceId\":7,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 7, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 512, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.2948365715286464}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:59:38] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial T92cL.' ]\r\n[2021-04-07 15:59:42] INFO [ 'Trial job T92cL status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:02:49] INFO [ 'Trial job T92cL status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:02:49] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 8, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 128, \"lr\": 0.001, \"momentum\": 0.5108633717497612}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:02:54] INFO [ 'submitTrialJob: form: {\"sequenceId\":8,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 8, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 16, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.001, \\\\\"momentum\\\\\": 0.5108633717497612}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 16:02:54] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial RoHBk.' ]\r\n[2021-04-07 16:02:59] INFO [ 'Trial job RoHBk status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:06:58] INFO [ 'Trial job RoHBk status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:06:58] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 9, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 32, \"hidden_size\": 1024, \"lr\": 0.1, \"momentum\": 0.1371728116640185}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:07:03] INFO [ 'submitTrialJob: form: {\"sequenceId\":9,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 9, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 32, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.1371728116640185}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 16:07:06] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial UURlR.' ]\r\n[2021-04-07 16:07:08] INFO [ 'Trial job UURlR status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:07:08] INFO [ 'Change NNIManager status from: RUNNING to: NO_MORE_TRIAL' ]\r\n[2021-04-07 16:10:36] INFO [ 'Trial job UURlR status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:10:36] INFO [ 'Change NNIManager status from: NO_MORE_TRIAL to: DONE' ]\r\n[2021-04-07 16:10:36] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 10, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 128, \"lr\": 0.01, \"momentum\": 0.5296207133227185}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:10:36] INFO [ 'Experiment done.' ]\r\n[2021-04-07 16:20:40] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from RUNNING to UNKNOWN.' ]\r\n[2021-04-07 16:21:10] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from UNKNOWN to SUCCEEDED.' ]\r\n\r\n - dispatcher.log:\r\n [2021-04-07 15:24:58] INFO (nni.runtime.msg_dispatcher_base\/MainThread) Dispatcher started\r\n[2021-04-07 15:25:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001968 seconds\r\n[2021-04-07 15:25:06] INFO (hyperopt.tpe\/Thread-1) TPE using 0 trials\r\n[2021-04-07 15:34:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 15:34:06] INFO (hyperopt.tpe\/Thread-1) TPE using 1\/1 trials with best loss -98.950000\r\n[2021-04-07 15:37:26] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001003 seconds\r\n[2021-04-07 15:37:26] INFO (hyperopt.tpe\/Thread-1) TPE using 2\/2 trials with best loss -98.950000\r\n[2021-04-07 15:41:26] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001019 seconds\r\n[2021-04-07 15:41:26] INFO (hyperopt.tpe\/Thread-1) TPE using 3\/3 trials with best loss -99.220000\r\n[2021-04-07 15:46:31] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001025 seconds\r\n[2021-04-07 15:46:31] INFO (hyperopt.tpe\/Thread-1) TPE using 4\/4 trials with best loss -99.220000\r\n[2021-04-07 15:52:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000998 seconds\r\n[2021-04-07 15:52:06] INFO (hyperopt.tpe\/Thread-1) TPE using 5\/5 trials with best loss -99.300000\r\n[2021-04-07 15:56:07] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000969 seconds\r\n[2021-04-07 15:56:07] INFO (hyperopt.tpe\/Thread-1) TPE using 6\/6 trials with best loss -99.300000\r\n[2021-04-07 15:59:32] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001000 seconds\r\n[2021-04-07 15:59:32] INFO (hyperopt.tpe\/Thread-1) TPE using 7\/7 trials with best loss -99.300000\r\n[2021-04-07 16:02:49] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001994 seconds\r\n[2021-04-07 16:02:49] INFO (hyperopt.tpe\/Thread-1) TPE using 8\/8 trials with best loss -99.300000\r\n[2021-04-07 16:06:58] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 16:06:58] INFO (hyperopt.tpe\/Thread-1) TPE using 9\/9 trials with best loss -99.300000\r\n[2021-04-07 16:10:36] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 16:10:36] INFO (hyperopt.tpe\/Thread-1) TPE using 10\/10 trials with best loss -99.340000\r\n\r\n - nnictl stdout and stderr:\r\n\r\n-----------------------------------------------------------------------\r\n                Experiment start time 2021-04-07 15:24:42\r\n-----------------------------------------------------------------------\r\n\r\n-----------------------------------------------------------------------\r\n                Experiment start time 2021-04-07 15:24:42\r\n-----------------------------------------------------------------------\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 message listeners added. Use emitter.setMaxListeners() to increase limit\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 error listeners added. Use emitter.setMaxListeners() to increase limit\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 close listeners added. Use emitter.setMaxListeners() to increase limit\r\n\r\n<!-- Where can you find the log files: [log](https:\/\/github.com\/microsoft\/nni\/blob\/master\/docs\/en_US\/Tutorial\/HowToDebug.md#experiment-root-director), [stdout\/stderr](https:\/\/github.com\/microsoft\/nni\/blob\/master\/docs\/en_US\/Tutorial\/Nnictl.md#nnictl%20log%20stdout) -->\r\n\r\n**What issue meet, what's expected?**:\r\nThe mnist_pytorch example training with Azure ML is unreasonably slow, each trial take about 3 to 5 mins. The entire experiment took nearly 50 mins. I was expecting it to be much faster given that it's using STANDARD_NC6 with GPU - 1 x NVIDIA Tesla K80.\r\n\r\n**How to reproduce it?**: \r\nFollow this doc https:\/\/nni.readthedocs.io\/en\/latest\/TrainingService\/AMLMode.html\r\n\r\n**Additional information**:\r\nTried adding gpuNum: 1 and useActiveGpu: true in config file, only made it even slower with trials spending more time in waiting status, also instead of doing all 10 trials in 1 run, each trial take 1 run.",
        "Challenge_closed_time":1662517.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617867548000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/nni\/issues\/3518",
        "Challenge_link_count":4,
        "Challenge_participation_count":7,
        "Challenge_readability":12.4,
        "Challenge_reading_time":208.42,
        "Challenge_repo_contributor_count":171.0,
        "Challenge_repo_fork_count":1727.0,
        "Challenge_repo_issue_count":5102.0,
        "Challenge_repo_star_count":12323.0,
        "Challenge_repo_watch_count":282.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":130,
        "Challenge_solved_time":null,
        "Challenge_title":"Training extremely slow with Azure Machine Learning",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":1467,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@yangmingwanli Each run only start one trial job, and then start new run? @SparkSnail After adding gpuNum: 1 and useActiveGpu: true, yes each run only start one trial job, and then start new run.\r\nWithout making these changes, it will finish all trials in one run, just very slowly. I reproduced this issue, and this seems to be a bug, will fix it ASAP. @SparkSnail , does it look like going to be a hard to fix bug? Is there any workaround before fix is released? Thanks!  Have you tried setting gpuNum:0, and resubmit the job? Just tried that, after setting gpuNum:0, training is still extremely slow, didn't start new run for new trial, but failed after two trials due to \"Converting circular structure to JSON\" error. @SparkSnail is it a bug that needs to be fixed? \r\n\r\n> \"Converting circular structure to JSON\" error.\r\n   \r\nthis error had been fixed in NNI v2.3.\r\n\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.1,
        "Solution_reading_time":10.34,
        "Solution_score_count":null,
        "Solution_sentence_count":11.0,
        "Solution_word_count":150.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1541802293200,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":163.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The short story is, when I try to submit an azure ML pipeline run (an <em>azure ML pipeline<\/em>, not an <em>Azure pipeline<\/em>) from a jupyter notebook, I get PermissionError: [Errno 13] Permission denied: '.\\NTUSER.DAT'.  More details:<\/p>\n\n<p>Relevant code:<\/p>\n\n<pre><code>from azureml.train.automl import AutoMLConfig\nfrom azureml.train.automl.runtime import AutoMLStep\nautoml_settings = {\n    \"iteration_timeout_minutes\": 20,\n    \"experiment_timeout_minutes\": 30,\n    \"n_cross_validations\": 3,\n    \"primary_metric\": 'r2_score',\n    \"preprocess\": True,\n    \"max_concurrent_iterations\": 3,\n    \"max_cores_per_iteration\": -1,\n    \"verbosity\": logging.INFO,\n    \"enable_early_stopping\": True,\n    'time_column_name': \"DateTime\"\n}\n\nautoml_config = AutoMLConfig(task = 'forecasting',\n                             debug_log = 'automl_errors.log',\n                             path = \".\",\n                             compute_target=compute_target,\n                             run_configuration=conda_run_config,                               \n                             training_data = financeforecast_dataset,\n                             label_column_name = 'TotalUSD',\n                             **automl_settings\n                            )\n\nautoml_step = AutoMLStep(\n    name='automl_module',\n    automl_config=automl_config,\n    allow_reuse=False)\n\ntraining_pipeline = Pipeline(\n    description=\"training_pipeline\",\n    workspace=ws,    \n    steps=[automl_step])\n\ntraining_pipeline_run = Experiment(ws, 'test').submit(training_pipeline)\n<\/code><\/pre>\n\n<p>The training_pipeline step runs for apx 20 seconds, and then I get a long trace, ending in:<\/p>\n\n<pre><code>~\\AppData\\Local\\Continuum\\anaconda2\\envs\\forecasting\\lib\\site- \npackages\\azureml\\pipeline\\core\\_module_builder.py in _hash_from_file_paths(hash_src)\n    100             hasher = hashlib.md5()\n    101             for f in hash_src:\n--&gt; 102                 with open(str(f), 'rb') as afile:\n    103                     buf = afile.read()\n    104                     hasher.update(buf)\n\nPermissionError: [Errno 13] Permission denied: '.\\\\NTUSER.DAT'\n<\/code><\/pre>\n\n<p>According to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-your-first-pipeline\" rel=\"nofollow noreferrer\">Azure's docs on this topic<\/a>, submitting a pipeline uploads a \"snapshot\" of the \"source directory\" you specified.  Initially, I hadn't specified a source directory, so, to test that out, I added: <\/p>\n\n<pre><code>default_source_directory=\"testing\",\n<\/code><\/pre>\n\n<p>as a parameter for the training_pipeline object, but saw the same behavior when I then tried to run it.  Not sure if that is the same source directory the documentation is referring to.  The docs also say that if no source directory is specified, the \"current local directory\" is uploaded.  I used print (os.getcwd()) to get the working directory and gave \"Everyone\" full control permissions on the directory (working in a windows env).<\/p>\n\n<p>All the preceding code works fine, and I can submit an experiment if I use a ScriptRunConfig and run it on attached compute rather than using a pipeline\/training cluster.  <\/p>\n\n<p>Any ideas?  Thanks in advance to anyone who tries to help.  P.S. There is no \"azure-machine-learning-pipelines\" tag, and I can't add one because I don't have enough reputation points.  Someone else could though!  <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-ml-pipelines\" rel=\"nofollow noreferrer\">General<\/a> info on what they are.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1577601580777,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59517355",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.4,
        "Challenge_reading_time":41.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":null,
        "Challenge_title":"Permission denied: '.\\NTUSER.DAT' when trying to run an Azure ML Pipeline",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":409.0,
        "Challenge_word_count":336,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1541802293200,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":163.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>I resolved this answer by setting the path and the data_script variables in the AutoMLConfig task object, like this (relevant code indicated by -->):<\/p>\n\n<pre><code>automl_config = AutoMLConfig(task = 'forecasting',\n                             debug_log = 'automl_errors.log',\n                             compute_target=compute_target,\n                             run_configuration=conda_run_config,\n                             --&gt;path = \"c:\\\\users\\\\me\",\n                             data_script =\"script.py\",&lt;--\n                             **automl_settings\n                            )\n<\/code><\/pre>\n\n<p>Setting the data_script variable to include the full path, as shown below, did not work.<\/p>\n\n<pre><code>automl_config = AutoMLConfig(task = 'forecasting',\n                             debug_log = 'automl_errors.log',\n                             path = \".\",\n                             --&gt;data_script = \"c:\\\\users\\\\me\\\\script.py\"&lt;--\n                             compute_target=compute_target,\n                             run_configuration=conda_run_config, \n                             **automl_settings\n                            )\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.7,
        "Solution_reading_time":10.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I used this code to create a confusion matrix:<\/p>\n<pre><code class=\"lang-auto\"># confusion matrix\n        wandb.log({\"confusion-matrix-test\": wandb.plot.confusion_matrix(\n            probs=None,\n            y_true=all_gt, preds=all_pre,\n            class_names=classes_names)})\n<\/code><\/pre>\n<p>However, Wanda\u2019s website only shows a table instead of the confusion matrix. This is a screenshot from the issue:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046.png\" data-download-href=\"\/uploads\/short-url\/8UGiFwpsOZ6Pivp7qmFXgL1OuvI.png?dl=1\" title=\"Screenshot from 2022-01-09 20-58-35\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_690x249.png\" alt=\"Screenshot from 2022-01-09 20-58-35\" data-base62-sha1=\"8UGiFwpsOZ6Pivp7qmFXgL1OuvI\" width=\"690\" height=\"249\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_690x249.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_1035x373.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_1380x498.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot from 2022-01-09 20-58-35<\/span><span class=\"informations\">1741\u00d7629 29.6 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641781829922,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-plot-confusion-matrix-just-show-a-table\/1744",
        "Challenge_link_count":6,
        "Challenge_participation_count":2,
        "Challenge_readability":27.3,
        "Challenge_reading_time":26.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Wandb.plot.confusion_matrix() just show a Table!",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":689.0,
        "Challenge_word_count":92,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<aside class=\"quote no-group\" data-username=\"fdaliran\" data-post=\"1\" data-topic=\"1744\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/f\/73ab20\/40.png\" class=\"avatar\"> fdaliran:<\/div>\n<blockquote>\n<pre><code class=\"lang-auto\">        wandb.log({\"confusion-matrix-test\": wandb.plot.confusion_matrix(\n            probs=None,\n            y_true=all_gt, preds=all_pre,\n            class_names=classes_names)})\n<\/code><\/pre>\n<\/blockquote>\n<\/aside>\n<p>If you click the section called \u201cCustom Charts\u201d above the Table, it\u2019ll show the line plot that you\u2019ve logged.<\/p>\n<p>Logging the Table also is expected behaviour because this will allow users to interactively explore the logged data in a W&amp;B Table after logging it.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.1,
        "Solution_reading_time":10.22,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":73.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":0.4277797222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to set up a DVC repository for machine learning data with different tagged versions of the dataset. I do this with something like:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ cd \/raid\/ml_data  # folder on a data drive\n$ git init\n$ dvc init\n$ [add data]\n$ [commit to dvc, git]\n$ git tag -a 1.0.0\n$ [add or change data]\n$ [commit to dvc, git]\n$ git tag -a 1.1.0\n<\/code><\/pre>\n<p>I have multiple projects that each need to reference some version of this dataset. The problem is I can't figure out how to set up those projects to reference a specific version. I'm able to track the <code>HEAD<\/code> of the repo with something like:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ cd ~\/my_proj  # different drive than the remote\n$ mkdir data\n$ git init\n$ dvc init\n$ dvc remote add -d local \/raid\/ml_data  # add the remote on my data drive\n$ dvc cache dir \/raid\/ml_data\/.dvc\/cache  # tell DVC to use the remote cache\n$ dvc checkout\n$ dvc run --external -d \/raid\/ml_data -o data\/ cp -r \/raid\/ml_data data\n<\/code><\/pre>\n<p>This gets me the latest version of the dataset, symlinked into my <code>data<\/code> folder, but what if I want some projects to use the <code>1.0.0<\/code> version and some to use the <code>1.1.0<\/code> version, or another version? Or for that matter, if I update the dataset to <code>2.0.0<\/code> but don't want my existing projects to necessarily track <code>HEAD<\/code> and instead keep the version with which they were set up?<\/p>\n<p>It's important to me to not create a ton of local copies of my dataset as the <code>\/home<\/code> drive is much smaller than the <code>\/raid<\/code> drive and some of these datasets are huge.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604349754297,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1604361023336,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64653042",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":21.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Control tracked version of external dependency",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":139.0,
        "Challenge_word_count":267,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1370629593700,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Colorado Springs, CO",
        "Poster_reputation_count":11685.0,
        "Poster_view_count":1329.0,
        "Solution_body":"<p>I think you are looking for the <a href=\"https:\/\/dvc.org\/doc\/start\/data-access\" rel=\"nofollow noreferrer\">data access<\/a> set of commands.<\/p>\n<p>In your particular case, <code>dvc import<\/code> makes sense:<\/p>\n<pre><code>$ dvc import \/raid\/ml_data data\n<\/code><\/pre>\n<p>if you want to get the most recent version (HEAD). Then you will be able to update it with the <code>dvc update<\/code> command (if 2.0.0 is released, for example).<\/p>\n<pre><code>$ dvc import \/raid\/ml_data data --rev 1.0.0\n<\/code><\/pre>\n<p>if you'd like to &quot;fix&quot; it to the specific version.<\/p>\n<h3>Avoiding copies<\/h3>\n<p>Make sure also, that <code>symlinks<\/code> are set for the second project, as described in the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\" rel=\"nofollow noreferrer\">Large Dataset Optimization<\/a>:<\/p>\n<pre><code>$ dvc config cache.type reflink,hardlink,symlink,copy\n<\/code><\/pre>\n<p>(there are config modifiers <code>--global<\/code>, <code>--local<\/code>, <code>--system<\/code> to set this setting for everyone at once, or just for one project, etc)<\/p>\n<p>Check the details instruction <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization#configuring-dvc-cache-file-link-type\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<hr \/>\n<p>Overall, it's a great setup, and looks like you got pretty much everything right. Please, don't hesitate to follow up and\/or create other questions here- we'll help you with this.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1604362563343,
        "Solution_link_count":3.0,
        "Solution_readability":10.6,
        "Solution_reading_time":18.98,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":165.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1608712064260,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":66.0,
        "Answerer_view_count":21.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to perform an MLFlow run but stuck with the following error after trying a lot of things.<\/p>\n<pre><code>\nrun = mlflow.active_run()\nif run:\n    print(&quot;Active run_id: {}&quot;.format(run.info.run_id))\n    mlflow.end_run()\n\nmlflow.set_experiment('TNF_EXP') \nmlflow.set_tracking_uri(&quot;http:\/\/localhost:5000\/&quot;) # Actual Server URI instead of localhost\nexperiment = mlflow.get_experiment_by_name(&quot;TNF_EXP&quot;)\n\nwith mlflow.start_run(experiment_id=experiment.experiment_id) as run:\n...\n...\n\nmlflow.end_run()\n<\/code><\/pre>\n<p>Error -<\/p>\n<pre><code>File &quot;\/...\/ModelTrainer.py&quot;, line 108, in train\n    with mlflow.start_run(experiment_id=experiment.experiment_id) as run:\n  File &quot;\/usr\/local\/lib\/python3.6\/site-packages\/mlflow\/tracking\/fluent.py&quot;, line 207, in start_run\n    &quot;arguments&quot;.format(existing_run_id)\nmlflow.exceptions.MlflowException: Cannot start run with ID e9953eb5918845bb9be1xxxxxx because active run ID does not match environment run ID. Make sure --experiment-name or --experiment-id matches experiment set with set_experiment(), or just use command-line arguments\n2021\/02\/11 09:41:36 ERROR mlflow.cli: === Run (ID 'e9953eb5918845bb9be1xxxxxx') failed ===\n<\/code><\/pre>\n<p>I noticed I had an <code>active run<\/code> earlier so I included the first <code>if block<\/code> to end that run. The code ran successfully and I was able to log the data on MLFlow UI but now when I run it I start getting the same issue. There are no active runs found before starting a new run currently.<\/p>\n<blockquote>\n<p>FYI, I am running the code on Azure server with the respective tracking URI mentioned in the code.<\/p>\n<\/blockquote>\n<p>However the code runs fine if I include an argument <code>--experiment-name=&quot;TNF_EXP&quot;<\/code> in the <code>mlflow run<\/code> command on the CLI<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613037319843,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66152375",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":11.4,
        "Challenge_reading_time":24.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"MLFlow active run does not match environment run id",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":4094.0,
        "Challenge_word_count":210,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559822816092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":861.0,
        "Poster_view_count":149.0,
        "Solution_body":"<p>That is primarily because you have started a run with <code>default experiment name<\/code> and then you are trying to set the <code>experiment_name<\/code> as &quot;TNF_EXP&quot;.<\/p>\n<p>Will suggest you to make use of <code>mlflow.run(..., experiment_name=&quot;TNF_EXP&quot;)<\/code> python method then running it from the <code>CLI<\/code>.<\/p>\n<p>You can find more information <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.run\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.9,
        "Solution_reading_time":6.61,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":50.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have imported packages:    <\/p>\n<p>library(dplyr)    <\/p>\n<p>Uploaded my dataset:    <\/p>\n<p>bike &lt;- readRDS(&quot;bike.rds&quot;)    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/16798-image.png?platform=QnA\" alt=\"16798-image.png\" \/>    <\/p>\n<p>But when I try simple &quot;filter&quot; it is not working:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/16867-image.png?platform=QnA\" alt=\"16867-image.png\" \/>    <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597096204317,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/63901\/simple-filter-is-not-working-in-azure-notebook-for",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":6.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Simple filter is not working in Azure notebook for R",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":43,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Fixed.  <\/p>\n<p>It looks azure notebook clean the session after some period of inactivity, there the package dplyr was not loaded after some time<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.5,
        "Solution_reading_time":1.88,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":24.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1348082104976,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":9564.0,
        "Answerer_view_count":894.0,
        "Challenge_adjusted_solved_time":3234.1247702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have hundreds of CSV files that I want to process similarly. For simplicity, we can assume that they are all in <code>.\/data\/01_raw\/<\/code> (like <code>.\/data\/01_raw\/1.csv<\/code>, <code>.\/data\/02_raw\/2.csv<\/code>) etc. I would much rather not give each file a different name and keep track of them individually when building my pipeline. I would like to know if there is any way to read all of them in bulk by specifying something in the <code>catalog.yml<\/code> file?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1588799145203,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1588803043230,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61645397",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.3,
        "Challenge_reading_time":6.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"How do I add many CSV files to the catalog in Kedro?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":806.0,
        "Challenge_word_count":83,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1453233461910,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":299.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>You are looking for <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/02_kedro_io.html#partitioned-dataset\" rel=\"nofollow noreferrer\">PartitionedDataSet<\/a>. In your example, the <code>catalog.yml<\/code> might look like this:<\/p>\n<pre><code>my_partitioned_dataset:\n  type: &quot;PartitionedDataSet&quot;\n  path: &quot;data\/01_raw&quot;\n  dataset: &quot;pandas.CSVDataSet&quot;\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1600445892403,
        "Solution_link_count":1.0,
        "Solution_readability":21.1,
        "Solution_reading_time":5.42,
        "Solution_score_count":8.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":25.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"```\r\n0: jdbc:hive2:\/\/localhost:10001\/default> CREATE MODEL ssd1 using \"mlflow:\/model\/ssd\"\r\n. . . . . . . . . . . . . . . . . . . .> ;\r\nError: org.apache.hive.service.cli.HiveSQLException: Error running query: org.mlflow.tracking.MlflowHttpException: statusCode=404 reasonPhrase=[NOT FOUND] bodyMessage=[{\"error_code\": \"RESOURCE_DOES_NOT_EXIST\", \"message\": \"Registered Model with name=ssd1 not found\"}]\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperti\r\n```",
        "Challenge_closed_time":1642206.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642187856000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/eto-ai\/rikai\/issues\/493",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":45.4,
        "Challenge_reading_time":14.23,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":715.0,
        "Challenge_repo_star_count":127.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Can not create model in MLflowCatalog",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":41,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Duplicated to #496 ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.2,
        "Solution_reading_time":0.24,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1412515367427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":161.0,
        "Answerer_view_count":24.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have followed an Amazon tutorial for using SageMaker and have used it to create the model in the tutorial (<a href=\"https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/<\/a>).<\/p>\n\n<p>This is my first time using SageMaker, so my question may be stupid.<\/p>\n\n<p>How do you actually view the model that it has created? I want to be able to see a) the final formula created with the parameters etc. b) graphs of plotted factors etc. as if I was reviewing a GLM for example.<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1557237604320,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56024351",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":9.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Beginners guide to Sagemaker",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":748.0,
        "Challenge_word_count":85,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1554397763220,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":327.0,
        "Poster_view_count":54.0,
        "Solution_body":"<p>If you followed the SageMaker tutorial you must have trained an XGBoost model. SageMaker places the model artifacts in a bucket that you own, check the output S3 location in the AWS SageMaker console. <\/p>\n\n<p>For more information about XGBoost you can check the AWS SageMaker documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#xgboost-sample-notebooks\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#xgboost-sample-notebooks<\/a> and the example notebooks, e.g. <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone.ipynb<\/a><\/p>\n\n<p>To consume the XGBoost artifact generated by SageMaker, check out the official documentation, which contains the following code:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># SageMaker XGBoost uses the Python pickle module to serialize\/deserialize \n# the model, which can be used for saving\/loading the model.\n# To use a model trained with SageMaker XGBoost in open source XGBoost\n# Use the following Python code:\n\nimport pickle as pkl \nmodel = pkl.load(open(model_file_path, 'rb'))\n# prediction with test data\npred = model.predict(dtest)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":19.6,
        "Solution_reading_time":18.79,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":131.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426685176247,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Athens, Greece",
        "Answerer_reputation_count":54268.0,
        "Answerer_view_count":22884.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Looks like I have 672 mission values, according to statistics. \nThere are NULL value in QuotedPremium column.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/p9Z3X.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/p9Z3X.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I implemented Clean Missing Data module where it should substitute missing values with 0, but for some reason I'm still seeing NULL values as QuotedPremium, but...it says that missing values are = 0<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/PDg97.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PDg97.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/BEdHD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BEdHD.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Here you see it tells me that missing values = 0, but there are still NULLs <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/WKlGZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WKlGZ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>So what really happened after I ran Clean Missing Data module? Why it ran succesfully but there are still NULL values, even though it tells that number of missing values are 0. <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1515028540763,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48087407",
        "Challenge_link_count":8,
        "Challenge_participation_count":4,
        "Challenge_readability":10.8,
        "Challenge_reading_time":17.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"How to deal with missing values in Azure Machine Learning Studio",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1556.0,
        "Challenge_word_count":144,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1457596845392,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Diego, CA, United States",
        "Poster_reputation_count":4046.0,
        "Poster_view_count":825.0,
        "Solution_body":"<p><code>NULL<\/code> is indeed a value; entries containing NULLs are <em>not<\/em> missing, hence they are neither cleaned with the 'Clean Missing Data' operator nor reported as missing.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.6,
        "Solution_reading_time":2.41,
        "Solution_score_count":2.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":26.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":3650.1353344444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I was looking at <code>iris<\/code> project example provided by kedro. Apart from logging the accuracy I also wanted to save the <code>predictions<\/code> and <code>test_y<\/code> as a csv.<\/p>\n<p>This is the example node provided by kedro.<\/p>\n<pre><code>def report_accuracy(predictions: np.ndarray, test_y: pd.DataFrame) -&gt; None:\n    &quot;&quot;&quot;Node for reporting the accuracy of the predictions performed by the\n    previous node. Notice that this function has no outputs, except logging.\n    &quot;&quot;&quot;\n    # Get true class index\n    target = np.argmax(test_y.to_numpy(), axis=1)\n    # Calculate accuracy of predictions\n    accuracy = np.sum(predictions == target) \/ target.shape[0]\n    # Log the accuracy of the model\n    log = logging.getLogger(__name__)\n    log.info(&quot;Model accuracy on test set: %0.2f%%&quot;, accuracy * 100)\n<\/code><\/pre>\n<p>I added the following to save the data.<\/p>\n<pre><code>data = pd.DataFrame({&quot;target&quot;: target , &quot;prediction&quot;: predictions})\ndata_set = CSVDataSet(filepath=&quot;data\/test.csv&quot;)\ndata_set.save(data)\n<\/code><\/pre>\n<p>This works as intended, however, my question is &quot;is it the kedro way of doing thing&quot; ? Can I provide the <code>data_set <\/code> in <code>catalog.yml<\/code> and later save <code>data<\/code> to it? If I want to do it, how do I access the <code>data_set<\/code> from <code>catalog.yml<\/code> inside a node.<\/p>\n<p>Is there a way to save data without creating a catalog inside a node like this <code>data_set = CSVDataSet(filepath=&quot;data\/test.csv&quot;)<\/code> ? I want this in <code>catalog.yml<\/code>, if possible and if it follows kedro convention!.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629897723887,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1629897818943,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68923747",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":21.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":null,
        "Challenge_title":"Saving data with DataCatalog",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":333.0,
        "Challenge_word_count":195,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1519724643532,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":453.0,
        "Poster_view_count":79.0,
        "Solution_body":"<p>Kedro actually abstracts this part for you. You don't need to access the datasets via their Python API.<\/p>\n<p>Your <code>report_accuracy<\/code> method does need to be tweaked to return the <code>DataFrame<\/code> instead of <code>None<\/code>.<\/p>\n<p>Your node needs to be defined as such:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>node(\n  func=report_accuracy,\n  inputs='dataset_a',\n  outputs='dataset_b'\n)\n<\/code><\/pre>\n<p>Kedro then looks at your catalog and will load\/save <code>dataset_a<\/code> and <code>dataset_b<\/code> as required:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>dataset_a:\n   type: pandas.CSVDataSet\n   path: xxxx.csv\n\ndataset_b:\n   type: pandas.ParquetDataSet\n   path: yyyy.pq\n<\/code><\/pre>\n<p>As you run the node\/pipeline Kedro will handle the load\/save operations for you. You also don't need to save every dataset if it's only used mid-way in a pipeline, you can read about <code>MemoryDataSet<\/code>s <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/11_tools_integration\/01_pyspark.html#use-memorydataset-for-intermediary-dataframe\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1643038306147,
        "Solution_link_count":1.0,
        "Solution_readability":11.2,
        "Solution_reading_time":14.51,
        "Solution_score_count":7.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":113.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":8,
        "Challenge_body":"<p>Hi, just started to use W&amp;B and managed to refactor some code to use artifact versioning today. What I could not find is (and sorry if this is very basic): during the first run of the program I would like to check if there is already some artifact (raw data) f\u00fcr that project \/ artifact name \/ type available: If yes, use it. If no, prepare it (might take a while). I am looking for the equivalent of <code>&lt;filename&gt;.is_file()<\/code> but for artifacts. I could use\/download the artifact in a <code>try, except<\/code> clause but that\u2019s not very pretty (throwing errors on the console, not sure what the correct Exception is). The API does not seem to provide such a functionality?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643300292817,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-can-i-check-whether-an-artifact-is-available\/1826",
        "Challenge_link_count":0,
        "Challenge_participation_count":8,
        "Challenge_readability":7.0,
        "Challenge_reading_time":9.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I check whether an artifact is available?",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":229.0,
        "Challenge_word_count":125,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey Stephan,<\/p>\n<p>Thanks for your response. I think the code you have written is the best way to check if an artifact exists if you do not know a priori if it really exists.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.4,
        "Solution_reading_time":2.53,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":36.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm following Sagemaker's <code>k_nearest_neighbors_covtype<\/code> example and had some questions about the way they pass their training data to the model.<\/p>\n\n<p>For those who have not seen it, they load data from the internet, run some preprocessing, then save it to an S3 bucket in some sort of binary format (protobuf\/recordIO). Their code is as follows:<\/p>\n\n<pre><code>import numpy as np\nimport boto3\nimport os\nimport sagemaker\nimport io\nimport sagemaker.amazon.common as smac\n\n# preprocess\nraw_data_file = os.path.join(data_dir, \"raw\", \"covtype.data.gz\")\nraw = np.loadtxt(raw_data_file, delimiter=',')\n\n# split into train\/test with a 90\/10 split\nnp.random.seed(0)\nnp.random.shuffle(raw)\ntrain_size = int(0.9 * raw.shape[0])\ntrain_features = raw[:train_size, :-1]\ntrain_labels = raw[:train_size, -1]\ntest_features = raw[train_size:, :-1]\ntest_labels = raw[train_size:, -1]\n\n# write to buffer\nbuf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(buf, train_features, train_labels)\nbuf.seek(0)\n\n# upload to s3\nbucket = sagemaker.Session().default_bucket()\nprefix = 'knn-blog-2018-04-17'\nkey = 'recordio-pb-data'\n\nboto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', key)).upload_fileobj(buf)\ns3_train_data = 's3:\/\/{}\/{}\/train\/{}'.format(bucket, prefix, key)\nprint('uploaded training data location: {}'.format(s3_train_data))\n<\/code><\/pre>\n\n<p>Later, when calling <code>model.fit()<\/code>, they pass the S3 bucket path as the training dataset.<\/p>\n\n<p>I'm having trouble understanding how the data needs to be structured from this example and I am also wondering if there is a simpler way to load data directly from a pandas dataframe.<\/p>\n\n<p><strong>My Question:<\/strong><\/p>\n\n<p>Let's say after preprocessing I have a pandas dataframe in the following format (~10k records):<\/p>\n\n<pre><code>type         brown   green   red     yellow\nNAME                                       \nawfulbrown     0.00   33.33   33.33   33.33\ncandyapple     0.00    0.00  100.00    0.00\ngrannysmith    2.96   95.19    0.00    0.72\n<\/code><\/pre>\n\n<p>I want to pass this to nearest neighbors and have it map\/cluster based on <code>type<\/code> (color) weights, with each point labeled by <code>NAME<\/code>. For example, point <code>candyapple<\/code> will be located at 100 on the <code>red<\/code> axis, 0.00 on the <code>green<\/code> and <code>yellow<\/code>. The intention then to pass a new set of color coordinates (eg. <code>red: 90.09, yellow: 0.33, green: 9.58<\/code> would return <code>candyapple<\/code>) and return the single nearest neighbor to that point (the closest approximation of those values we have stored in our records).<\/p>\n\n<ol>\n<li><p><strong>What further preprocessing do I need to perform on this dataframe before passing it to Sagemaker's KNN model?<\/strong><\/p><\/li>\n<li><p><strong>What is the simplest way to pass the dataframe? Is there a way to pass it directly to the model?<\/strong><\/p><\/li>\n<\/ol>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579712678183,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59864823",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.8,
        "Challenge_reading_time":36.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS Sagemaker: What data format to pass to Estimator?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":751.0,
        "Challenge_word_count":352,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1539701586583,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":623.0,
        "Poster_view_count":118.0,
        "Solution_body":"<p>You can't pass a dataframe directly to the built-in KNN algo. It supports two input training formats: CSV, or RecordIO protobuf: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/kNN-in-formats.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/kNN-in-formats.html<\/a>.<\/p>\n\n<p>The latter is more efficient, so it's the one we recommend.<\/p>\n\n<p>In your case, you would simply need to convert your dataframe to a numpy array with to_numpy(), and then you can reuse the code in the notebook.<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>import pandas as pd\nindex = [1, 2, 3, 4]\na = ['a', 'b', 'c', 'd']\nb = [1, 2, 3, 4]\ndf = pd.DataFrame({'A': a, 'B': b}, index=index)\nn = df.to_numpy()\nprint(n)\ntype(n)\n<\/code><\/pre>\n\n\n\n<p>The notebook you're using is actually showing how to use KNN for classification. This clustering example may be easier to understand: <a href=\"https:\/\/data.solita.fi\/machine-learning-building-blocks-in-aws-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/data.solita.fi\/machine-learning-building-blocks-in-aws-sagemaker\/<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":12.7,
        "Solution_reading_time":14.23,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":120.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":7,
        "Challenge_body":"<p>Hi everyone,<\/p>\n<p>regarding the different chart types, I am somehow missing the option \u2018<strong>Scatter plot<\/strong>\u2019 next to \u2018Line plot\u2019, \u2018Area plot\u2019, and \u2018Percent area plot\u2019. Would be great if you could add this feature in the future (or in case it can easily be done somehow else, please let me know how it works \u2013 I already tried custom scatter plots, but it seems as if they are meant for comparing different runs, not matrices from a single run).<\/p>\n<p>Thanks <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/wink.png?v=12\" title=\":wink:\" class=\"emoji\" alt=\":wink:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657120094265,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/scatter-plot-instead-of-line-plot\/2706",
        "Challenge_link_count":1,
        "Challenge_participation_count":7,
        "Challenge_readability":9.3,
        "Challenge_reading_time":8.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Scatter plot instead of Line plot",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":245.0,
        "Challenge_word_count":92,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/aichberger\">@aichberger<\/a> ,<\/p>\n<p>You can create a line chart with a dotted non connected line through the legend category within chart edit.  I believe that is what you are looking for, see image below. In terms of modifying how a metric is being logged, can you expand on your meaning behind this? You can currently update metrics for a run, after it has finished., see <a href=\"https:\/\/wandb.ai\/mohammadbakir\/Finance-Prediction?workspace=user-mohammadbakir\">here<\/a>.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/e4b37129b3b2dca4efe8af5082b5662e1e7fe5ea.png\" data-download-href=\"\/uploads\/short-url\/wDbjAXgfUtaaLAXhq2VTVU9YU6C.png?dl=1\" title=\"DotChart\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e4b37129b3b2dca4efe8af5082b5662e1e7fe5ea_2_690x198.png\" alt=\"DotChart\" data-base62-sha1=\"wDbjAXgfUtaaLAXhq2VTVU9YU6C\" width=\"690\" height=\"198\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e4b37129b3b2dca4efe8af5082b5662e1e7fe5ea_2_690x198.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e4b37129b3b2dca4efe8af5082b5662e1e7fe5ea_2_1035x297.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e4b37129b3b2dca4efe8af5082b5662e1e7fe5ea_2_1380x396.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e4b37129b3b2dca4efe8af5082b5662e1e7fe5ea_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">DotChart<\/span><span class=\"informations\">1819\u00d7524 30 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":24.0,
        "Solution_reading_time":26.56,
        "Solution_score_count":null,
        "Solution_sentence_count":10.0,
        "Solution_word_count":108.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nFor some reason, `mlflow deployment create ...` can fail unexpectedly. \r\n\r\n```\r\nmlflow deployments create -t triton --flavor triton --name sid-minibert-onnx -m models:\/sid-minibert-onnx\/1 -C \"version=1\"\r\nCopied \/mlflow\/artifacts\/0\/41f4069628e5429eb5c75728486a247a\/artifacts\/triton\/sid-minibert-onnx to \/common\/triton-model-repo\/sid-minibert-onnx\r\nSaved mlflow-meta.json to \/common\/triton-model-repo\/sid-minibert-onnx\r\nTraceback (most recent call last):\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/mlflow_triton\/deployments.py\", line 109, in create_deployment\r\n    self.triton_client.load_model(name)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/tritonclient\/http\/__init__.py\", line 622, in load_model\r\n    _raise_if_error(response)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/tritonclient\/http\/__init__.py\", line 64, in _raise_if_error\r\n    raise error\r\ntritonclient.utils.InferenceServerException: failed to load 'sid-minibert-onnx', no version is available\r\n```\r\n\r\nFix is to delete the mlflow pod and start over.\r\n\r\n**Steps\/Code to reproduce bug**\r\nFollow steps in docs\/source\/morpheus_quickstart_guide.md#model-deployment\r\n\r\n**Expected behavior**\r\nSuccessful deployment as described at docs\/source\/morpheus_quickstart_guide.md#model-deployment\r\n\r\n**Environment overview (please complete the following information)**\r\n - Environment location: LaunchPad\r\n - Method of Morpheus install: Kubernetes\r\n\r\n**Environment details**\r\nLaunchPad Helm deployment on A30. Unfortunately, unable to capture the print_env.sh output from ipykernel there.\r\n\r\n**Additional context**\r\nMLflow sqlite db likely gets corrupted or otherwise \"confused\". Possibly an issue in tritonclient?\r\nTriton logging complains about unable to read config.pbtxt\r\n",
        "Challenge_closed_time":1654018.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653424629000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/nv-morpheus\/Morpheus\/issues\/125",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.9,
        "Challenge_reading_time":23.83,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":43.0,
        "Challenge_repo_issue_count":536.0,
        "Challenge_repo_star_count":131.0,
        "Challenge_repo_watch_count":10.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] mlflow deployments create can fail (k8s\/Helm)",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":157,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Error in LaunchPad notebooks. There was a change in the triton upstream where you previously didn't need to specify the model suffix as the path. Now you do. ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":1.91,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":28.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1528790837107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":610.0,
        "Answerer_view_count":203.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I\u2019m having some issues trying to load a dataset in Azure ML Studio, a dataset containing a column that looks like a DateTime, but is in fact a string. Azure ML Studio converts the values to DateTimes internally, and no amount of wrangling seems to convince it of the that they\u2019re in fact strings.<\/p>\n\n<p>This is an issue, because during conversion the values lose precision and start appearing as duplicates whereas in fact they are unique. Does anybody know if ML Studio can be configured not to infer data types for columns while importing a dataset?<\/p>\n\n<p>Now, for the long(er) story :)<\/p>\n\n<p>I\u2019m working here with a public dataset - specifically <a href=\"https:\/\/www.kaggle.com\/c\/new-york-city-taxi-fare-prediction\" rel=\"noreferrer\">Kaggle\u2019s New York City Fare Prediction<\/a> competition. I wanted to see if I could do a quick-and-dirty solution using Azure ML Studio, however the dataset\u2019s unique key values are of the form\n<code>\n    2015-01-27 13:08:24.0000003\n    2015-01-27 13:08:24.0000002\n    2011-10-06 12:10:20.0000001\n<\/code>\nand so on. <\/p>\n\n<p>When importing them in my experiment the key values get converted to DateTime, making them no longer unique, even though they\u2019re unique in the csv. Needless to say, this prevents me from submitting any solution to Kaggle, since I can\u2019t identify the rows uniquely :).<\/p>\n\n<p>I\u2019ve tried the following:<\/p>\n\n<ul>\n<li>edit the metadata of the dataset after it has been loaded and setting the data type of the column to string, but this doesn\u2019t do much as the precision has already been lost<\/li>\n<li>import the dataset from an Azure blob, convert it to csv and then loading it in Jupyter\/Python - this brings me the same (duplicated) keys. <\/li>\n<li>loading the dataset locally with pandas works, as expected.<\/li>\n<\/ul>\n\n<p>I\u2019ve reproduced this behavior with both the big, 5.5GB <code>train<\/code> dataset, but also with the more manageable <code>sample_submission<\/code> dataset. <\/p>\n\n<p>Curious to know if there is some sort of workaround to tell ML Studio not to try converting this column while loading the dataset. I'm looking here specifically for Azure ML Studio-only solutions, as I don't want to do any preprocessing on the dataset.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533883639527,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51780562",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":28.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"How to prevent Azure ML Studio from converting a feature column to DateTime while importing a dataset",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":401.0,
        "Challenge_word_count":354,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1250158552416,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Romania",
        "Poster_reputation_count":7916.0,
        "Poster_view_count":801.0,
        "Solution_body":"<p>I have tried with you sample data and here is my quick and dirty solution:\n1) Add any symbol (I've added the '#') in front of each date\n2) Load it to AML Studio (it is now considered as a string feature)\n3) Add a Python\/R component to remove the '#' symbol and explicitly convert the column to string (as.string(columnname) or str(columnname))<\/p>\n\n<p>Hope this helps<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.2,
        "Solution_reading_time":4.54,
        "Solution_score_count":4.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi,<\/p>\n<p>I am trying to use <code>wandb.watch<\/code> for a pytorch model, unfortunately without success. I checked the documentation and these two threads:<\/p>\n<ul>\n<li>Wandb.watch not logging parameters<\/li>\n<li>When is one supposed to run wandb.watch so that weights and biases tracks params and gradients?<\/li>\n<\/ul>\n<p>But none of the suggested solutions solves my problem. I run in my environment the code from the colab notebook linked in <a href=\"https:\/\/community.wandb.ai\/t\/when-is-one-supposed-to-run-wandb-watch-so-that-weights-and-biases-tracks-params-and-gradients\/518\/3\">this post<\/a> (with <code>N, log_freq = 50, 2<\/code>) and still nothing is logged.<\/p>\n<p>Interestingly, if I set the <code>log_graph=True<\/code> there is a JSON file logged as a file, under <code>root \/ media \/ graph<\/code> in the files section. But I was expecting to get a result similar to <a href=\"https:\/\/wandb.ai\/ayush-thakur\/debug-neural-nets\/runs\/jh061uaf\/model\">this<\/a>.<\/p>\n<p>I am using wandb version 0.12.10.<\/p>\n<p>Kind regards,<br>\nMaciej<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647450501888,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-watch-with-pytorch-not-logging-anything\/2096",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":9.5,
        "Challenge_reading_time":14.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Wandb.watch with pytorch not logging anything",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1548.0,
        "Challenge_word_count":128,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi,<\/p>\n<p>Eureka! Everything was working correctly, but I always use <a href=\"http:\/\/wandb.ai\">wandb.ai<\/a> with project view or run groups view. When I opened the run view both the graph and gradient were there <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>However, there is one problem remaining: <code>parameters<\/code>. When running the colab notebook code with <code>wandb.watch(d, log_freq=log_freq, log=\"all\")<\/code> I still can see only gradients in the run view.<\/p>\n<p><a href=\"https:\/\/wandb.ai\/dmml-heg\/uncategorized\/runs\/2qovzwq9\">Link to run page<\/a>  executed with wandb version 0.12.11 in Google Colab.<\/p>\n<p>EDIT: I found it <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> Code in the notebook was using <code>forward()<\/code> instead of <code>__call__()<\/code>. Forward hooks were not executed.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.5,
        "Solution_reading_time":14.19,
        "Solution_score_count":null,
        "Solution_sentence_count":12.0,
        "Solution_word_count":107.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Hi W&amp;B Community,<\/p>\n<p>Is there a possibility to get additional live system metrics like the network read\/write rates, disk read\/write rates, virtual memory major\/minor page faults, filesystem inodes, and system context switches?<\/p>\n<p>Basically, most of the metrics that dstat provides with the following flags:<\/p>\n<ul>\n<li>\u2013disk<\/li>\n<li>\u2013mem (memory)<\/li>\n<li>\u2013net (network)<\/li>\n<li>\u2013sys (system)<\/li>\n<li>\u2013fs (filesystem)<\/li>\n<li>\u2013vm (virtual memory)<\/li>\n<\/ul>\n<p>I\u2019m deep into pipeline profiling and found that having these helps a lot when looking for performance tuning opportunities. Also, allowing to add to the system metric log might be helpful generally to have everything related to actual ML in one log, and everything related to system metrics in another.<\/p>\n<p>I saw that the <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/system-metrics\">current documentation<\/a> suggests that you use this script - github(.)com\/nicolargo\/nvidia-ml-py3\/blob\/master\/pynvml.py - to get the GPU metrics, however, I did not find the system metrics there.<\/p>\n<p>The first workaround for me would be to run  <code>dstat<\/code> in parallel to the process, save the profiling log,<br>\ndownload your system metrics and join over the <code>_timestamp<\/code>. This, however, would negate your wonderful automatic visualization.<\/p>\n<p>The other solution would be to use some system monitoring library and add manually via <code>wandb.log({'my_metric': x})<\/code> to the \u201cML\u201d-log. This would show the metric in your visualization but not at the correct place and would not be easily compared to the other system metrics. I do not know how well this would work in practice as there would need to be additions to this log ideally every (few) seconds. This would be an asynchronous running thread that is not inside of the training loop. <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log\/logging-faqs#what-if-i-want-to-log-some-metrics-on-batches-and-some-metrics-only-on-epochs\">The solution proposed here<\/a>  seems like it could work if I use \u201ctimestamps\u201d as the X-axis? This still does not seem like a clean solution.<\/p>\n<p>What are your thoughts on this proposed feature? I\u2019m very much a novice regarding your service so I might not know the in\u2019s and out\u2019s, maybe I have overlooked some trivial solution.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651079776562,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/additional-system-metrics-from-e-g-dstat\/2333",
        "Challenge_link_count":2,
        "Challenge_participation_count":5,
        "Challenge_readability":11.4,
        "Challenge_reading_time":29.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"Additional System Metrics From e.g., `dstat`",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":173.0,
        "Challenge_word_count":320,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Alex,<\/p>\n<p>Thank you for that very detailed and insightful response regarding your request! I definitely see why this could be useful for optimizing features now, I had never considered how the rate of context switches could have a performance impact on the performance of an ML pipeline.<\/p>\n<p>I\u2019ll definitely go ahead and make a feature request for this, and I\u2019ll keep you updated on the status of this request.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.1,
        "Solution_reading_time":5.58,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":72.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1359884693920,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":9637.0,
        "Answerer_view_count":609.0,
        "Challenge_adjusted_solved_time":2068.9864736111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>im trying to setup <a href=\"https:\/\/www.comet.ml\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml<\/a> to log my experiment details <\/p>\n\n<p>getting strange error:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"train.py\", line 7, in &lt;module&gt;\n    from comet_ml import Experiment\nImportError: No module named comet_ml\n<\/code><\/pre>\n\n<p>trying in python 2 and python3<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1506066265147,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1506066568087,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46359436",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":5.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"How to configure comet (comet.ml) to track Keras?",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1208.0,
        "Challenge_word_count":51,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1505841491572,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>it seems like comet isn't installed on your machine.<\/p>\n\n<p>you can use :<\/p>\n\n<pre><code>pip3 install comet_ml\npip install comet_ml\n<\/code><\/pre>\n\n<p>take a look at the example projects at: <\/p>\n\n<p><a href=\"https:\/\/github.com\/comet-ml\/comet-quickstart-guide\" rel=\"nofollow noreferrer\">https:\/\/github.com\/comet-ml\/comet-quickstart-guide<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1513514919392,
        "Solution_link_count":2.0,
        "Solution_readability":9.9,
        "Solution_reading_time":4.6,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":33.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi there, \r\nthank you for this powerful template! \r\nI run into a problem while trying to use wandb as logger\r\nI used the wandb-callbacks branch and after `python train.py logger=wandb` i get (cancelled by user after 130 iterations cause wandb login does not appear)\r\n\r\n````\r\n$ python train.py logger=wandb\r\n\u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502    \u2502 Name          \u2502 Type             \u2502 Params \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 0  \u2502 model         \u2502 SimpleDenseNet   \u2502  336 K \u2502\r\n\u2502 1  \u2502 model.model   \u2502 Sequential       \u2502  336 K \u2502\r\n\u2502 2  \u2502 model.model.0 \u2502 Linear           \u2502  200 K \u2502\r\n\u2502 3  \u2502 model.model.1 \u2502 BatchNorm1d      \u2502    512 \u2502\r\n\u2502 4  \u2502 model.model.2 \u2502 ReLU             \u2502      0 \u2502\r\n\u2502 5  \u2502 model.model.3 \u2502 Linear           \u2502 65.8 K \u2502\r\n\u2502 6  \u2502 model.model.4 \u2502 BatchNorm1d      \u2502    512 \u2502\r\n\u2502 7  \u2502 model.model.5 \u2502 ReLU             \u2502      0 \u2502\r\n\u2502 8  \u2502 model.model.6 \u2502 Linear           \u2502 65.8 K \u2502\r\n\u2502 9  \u2502 model.model.7 \u2502 BatchNorm1d      \u2502    512 \u2502\r\n\u2502 10 \u2502 model.model.8 \u2502 ReLU             \u2502      0 \u2502\r\n\u2502 11 \u2502 model.model.9 \u2502 Linear           \u2502  2.6 K \u2502\r\n\u2502 12 \u2502 criterion     \u2502 CrossEntropyLoss \u2502      0 \u2502\r\n\u2502 13 \u2502 train_acc     \u2502 Accuracy         \u2502      0 \u2502\r\n\u2502 14 \u2502 val_acc       \u2502 Accuracy         \u2502      0 \u2502\r\n\u2502 15 \u2502 test_acc      \u2502 Accuracy         \u2502      0 \u2502\r\n\u2502 16 \u2502 val_acc_best  \u2502 MaxMetric        \u2502      0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nTrainable params: 336 K\r\nNon-trainable params: 0\r\nTotal params: 336 K\r\nTotal estimated model params size (MB): 1\r\nEpoch 0    ----- ---------------------------------- 130\/939 0:00:04 \u2022 0:00:28 29.28it\/s loss: 0.252\r\nError executing job with overrides: ['logger=wandb']\r\n````\r\n_(Note the last line)_\r\n\r\nChanging `logger: wandb` in train.yaml does not work either. I'm a bit confused because i had it working once before but just don't know what to do anymore. I tried out different conda envs with different torch and pl versions. Does anyboady have an idea?\r\n\r\n\r\n**pip list**\r\n```\r\nPackage                 Version\r\n----------------------- ------------\r\nabsl-py                 1.1.0\r\naiohttp                 3.8.1\r\naiosignal               1.2.0\r\nalembic                 1.8.0\r\nantlr4-python3-runtime  4.8\r\nanyio                   3.6.1\r\nargon2-cffi             21.3.0\r\nargon2-cffi-bindings    21.2.0\r\nasttokens               2.0.5\r\nasync-timeout           4.0.2\r\natomicwrites            1.4.0\r\nattrs                   21.4.0\r\nautopage                0.5.1\r\nBabel                   2.10.1\r\nbackcall                0.2.0\r\nbeautifulsoup4          4.11.1\r\nblack                   22.3.0\r\nbleach                  5.0.0\r\ncachetools              5.2.0\r\ncertifi                 2022.5.18.1\r\ncffi                    1.15.0\r\ncfgv                    3.3.1\r\ncharset-normalizer      2.0.12\r\nclick                   8.1.3\r\ncliff                   3.10.1\r\ncmaes                   0.8.2\r\ncmd2                    2.4.1\r\ncolorama                0.4.4\r\ncolorlog                6.6.0\r\ncommonmark              0.9.1\r\ncycler                  0.11.0\r\ndebugpy                 1.6.0\r\ndecorator               5.1.1\r\ndefusedxml              0.7.1\r\ndistlib                 0.3.4\r\ndocker-pycreds          0.4.0\r\nentrypoints             0.4\r\nexecuting               0.8.3\r\nfastjsonschema          2.15.3\r\nfilelock                3.7.1\r\nflake8                  4.0.1\r\nfonttools               4.33.3\r\nfrozenlist              1.3.0\r\nfsspec                  2022.5.0\r\ngitdb                   4.0.9\r\nGitPython               3.1.27\r\ngoogle-auth             2.6.6\r\ngoogle-auth-oauthlib    0.4.6\r\ngreenlet                1.1.2\r\ngrpcio                  1.46.3\r\nhydra-colorlog          1.2.0\r\nhydra-core              1.1.0\r\nhydra-optuna-sweeper    1.2.0\r\nidentify                2.5.1\r\nidna                    3.3\r\nimportlib-metadata      4.11.4\r\nimportlib-resources     5.7.1\r\niniconfig               1.1.1\r\nipykernel               6.13.0\r\nipython                 8.4.0\r\nipython-genutils        0.2.0\r\nisort                   5.10.1\r\njedi                    0.18.1\r\nJinja2                  3.1.2\r\njoblib                  1.1.0\r\njson5                   0.9.8\r\njsonschema              4.6.0\r\njupyter-client          7.3.1\r\njupyter-core            4.10.0\r\njupyter-server          1.17.0\r\njupyterlab              3.4.2\r\njupyterlab-pygments     0.2.2\r\njupyterlab-server       2.14.0\r\nkiwisolver              1.4.2\r\nMako                    1.2.0\r\nMarkdown                3.3.7\r\nMarkupSafe              2.1.1\r\nmatplotlib              3.5.2\r\nmatplotlib-inline       0.1.3\r\nmccabe                  0.6.1\r\nmistune                 0.8.4\r\nmultidict               6.0.2\r\nmypy-extensions         0.4.3\r\nnbclassic               0.3.7\r\nnbclient                0.6.4\r\nnbconvert               6.5.0\r\nnbformat                5.4.0\r\nnest-asyncio            1.5.5\r\nnodeenv                 1.6.0\r\nnotebook                6.4.11\r\nnotebook-shim           0.1.0\r\nnumpy                   1.22.4\r\noauthlib                3.2.0\r\nomegaconf               2.1.2\r\noptuna                  2.10.0\r\npackaging               21.3\r\npandas                  1.4.2\r\npandocfilters           1.5.0\r\nparso                   0.8.3\r\npathspec                0.9.0\r\npathtools               0.1.2\r\npbr                     5.9.0\r\npickleshare             0.7.5\r\nPillow                  9.1.1\r\npip                     21.2.2\r\nplatformdirs            2.5.2\r\npluggy                  1.0.0\r\npre-commit              2.19.0\r\nprettytable             3.3.0\r\nprometheus-client       0.14.1\r\npromise                 2.3\r\nprompt-toolkit          3.0.29\r\nprotobuf                3.20.1\r\npsutil                  5.9.1\r\npudb                    2022.1.1\r\npure-eval               0.2.2\r\npy                      1.11.0\r\npyasn1                  0.4.8\r\npyasn1-modules          0.2.8\r\npycodestyle             2.8.0\r\npycparser               2.21\r\npyDeprecate             0.3.2\r\npyflakes                2.4.0\r\nPygments                2.12.0\r\npyparsing               3.0.9\r\npyperclip               1.8.2\r\npyreadline3             3.4.1\r\npyrsistent              0.18.1\r\npytest                  7.1.2\r\npython-dateutil         2.8.2\r\npython-dotenv           0.20.0\r\npytorch-lightning       1.6.4\r\npytz                    2022.1\r\npywin32                 304\r\npywinpty                2.0.5\r\nPyYAML                  6.0\r\npyzmq                   23.1.0\r\nrequests                2.27.1\r\nrequests-oauthlib       1.3.1\r\nrich                    12.4.4\r\nrsa                     4.8\r\nscikit-learn            1.1.1\r\nscipy                   1.8.1\r\nseaborn                 0.11.2\r\nSend2Trash              1.8.0\r\nsentry-sdk              1.5.12\r\nsetproctitle            1.2.3\r\nsetuptools              61.2.0\r\nsh                      1.14.2\r\nshortuuid               1.0.9\r\nsix                     1.16.0\r\nsmmap                   5.0.0\r\nsniffio                 1.2.0\r\nsoupsieve               2.3.2.post1\r\nSQLAlchemy              1.4.37\r\nstack-data              0.2.0\r\nstevedore               3.5.0\r\ntensorboard             2.9.0\r\ntensorboard-data-server 0.6.1\r\ntensorboard-plugin-wit  1.8.1\r\nterminado               0.15.0\r\nthreadpoolctl           3.1.0\r\ntinycss2                1.1.1\r\ntoml                    0.10.2\r\ntomli                   2.0.1\r\ntorch                   1.11.0+cu113\r\ntorchaudio              0.11.0+cu113\r\ntorchmetrics            0.9.0\r\ntorchvision             0.12.0+cu113\r\ntornado                 6.1\r\ntqdm                    4.64.0\r\ntraitlets               5.2.2.post1\r\ntyping_extensions       4.2.0\r\nurllib3                 1.26.9\r\nurwid                   2.1.2\r\nurwid-readline          0.13\r\nvirtualenv              20.14.1\r\nwandb                   0.12.17\r\nwcwidth                 0.2.5\r\nwebencodings            0.5.1\r\nwebsocket-client        1.3.2\r\nWerkzeug                2.1.2\r\nwheel                   0.37.1\r\nwincertstore            0.2\r\nyarl                    1.7.2\r\nzipp                    3.8.0\r\n```",
        "Challenge_closed_time":1654420.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654326486000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/328",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":4.4,
        "Challenge_reading_time":61.14,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":341.0,
        "Challenge_repo_issue_count":412.0,
        "Challenge_repo_star_count":2043.0,
        "Challenge_repo_watch_count":19.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":210,
        "Challenge_solved_time":null,
        "Challenge_title":"wandb logger not working",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":584,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"`wandb-callbacks` haven't been maintained for a while and it might not work correctly with recent lightning and hydra releases. \r\n\r\nHave you trained using the `main` branch?\r\n\r\nI'm preparing new release and will fix the callbacks when it's ready https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/308\r\n So i managed to get it working using a fresh conda environment: \r\ntorch==1.10.0 with CUDA10.2\r\npytorch-lightning==1.6.4\r\nwandb == 0.12.17\r\n\r\nI doesnt check if all the callbacks work properly but my initial problem is solved. Thank you for your help! ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.0,
        "Solution_reading_time":6.86,
        "Solution_score_count":null,
        "Solution_sentence_count":8.0,
        "Solution_word_count":77.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1482721500648,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3795.0,
        "Answerer_view_count":475.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Here's a picture of my data, the column of interest RUL is on the far right the names got cut off (I'm using the Turbo Engine Degradation dataset from NASA) can be found here: <a href=\"https:\/\/data.nasa.gov\/widgets\/vrks-gjie\" rel=\"nofollow noreferrer\">https:\/\/data.nasa.gov\/widgets\/vrks-gjie<\/a><\/p>\n\n<p>I'm doing this in Azure ML Studio but code snippet below, I have 2 helper functions get_engine_last_cycle (which when I unit test it seems to do as expected - compute the last cycle for that engine, for example engine 2 has a max cycle in this dataset of 287 when it fails). The final helper function I call get_engine_remainig_life, takes the engine and cycle as arguments and returns the max cycle - current cycle for that engine (again I've unit tested this and it seems to give me expected results).<\/p>\n\n<p>For some reason this isn't working when I run my notebook. The column which I call \"RUL\" should return a sequence of decreasing, positive integers for example 287, 286, 285 284, etc for engine #2. However, it's giving me negative values. I can't seem to figure out why but know the problem is likely with this one piece of code<\/p>\n\n<pre><code> df['RUL'] = df[['engine', 'cycle']].apply(lambda x: get_engine_remaining_life(*x), axis=1)\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/8IYOx.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8IYOx.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<pre><code>    def get_engine_last_cycle(engine):\n        return int(df.loc[engine, ['cycle']].max())\n\n\n    def get_engine_remaining_life(engine, cycle):\n        return get_engine_last_cycle(engine) - int(cycle)\n\n    df['RUL'] = df[['engine', 'cycle']].apply(lambda x: get_engine_remaining_life(*x), axis=1)\n\n    return df\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1545435517580,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1545438768860,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53891907",
        "Challenge_link_count":4,
        "Challenge_participation_count":4,
        "Challenge_readability":9.8,
        "Challenge_reading_time":23.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Pandas dataframe and apply - Can't figure out why resulting values are negative",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":66.0,
        "Challenge_word_count":235,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442451471347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":338.0,
        "Poster_view_count":79.0,
        "Solution_body":"<p>Just for the sake of trying, this is how I'd implement this. Maybe it will help you.<\/p>\n\n<pre><code>df['RUL'] = df.loc[:, ['engine', 'cycle']].groupby('engine').transform('max')\ndf['RUL'] = df['RUL'] - df['cycle']\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":2.98,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":7,
        "Challenge_body":"<p>Why does these code snippets produce different results?<\/p>\n<pre><code class=\"lang-auto\">for i in range(100):\n    wandb.log({\"train\/loss\": i}, step=i)\n    \nfor i in range(100):\n    wandb.log({\"val\/loss\": i**2}, step=i)\n<\/code><\/pre>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/wandb.ai\/dminn\/WandbHelp\/runs\/ioofli05?workspace=user-dminn\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/18b592222b97bc09df3743831bb4929dec23611c.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/wandb.ai\/dminn\/WandbHelp\/runs\/ioofli05?workspace=user-dminn\" target=\"_blank\" rel=\"noopener\">W&amp;B<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/626b9c1d1ceb6cb5d3bb90cf6ab8d2894a6b8b14.png\" class=\"thumbnail onebox-avatar\" width=\"128\" height=\"128\">\n\n<h3><a href=\"https:\/\/wandb.ai\/dminn\/WandbHelp\/runs\/ioofli05?workspace=user-dminn\" target=\"_blank\" rel=\"noopener\">dminn<\/a><\/h3>\n\n  <p>Weights &amp; Biases, developer tools for machine learning<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<pre><code class=\"lang-auto\">for i in range(100):\n    wandb.log({\"train\/loss\": i}, step=i)\n    wandb.log({\"val\/loss\": i**2}, step=i)\n<\/code><\/pre>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/wandb.ai\/dminn\/WandbHelp\/runs\/146hdnar?workspace=user-dminn\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/18b592222b97bc09df3743831bb4929dec23611c.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/wandb.ai\/dminn\/WandbHelp\/runs\/146hdnar?workspace=user-dminn\" target=\"_blank\" rel=\"noopener\">W&amp;B<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/626b9c1d1ceb6cb5d3bb90cf6ab8d2894a6b8b14.png\" class=\"thumbnail onebox-avatar\" width=\"128\" height=\"128\">\n\n<h3><a href=\"https:\/\/wandb.ai\/dminn\/WandbHelp\/runs\/146hdnar?workspace=user-dminn\" target=\"_blank\" rel=\"noopener\">dminn<\/a><\/h3>\n\n  <p>Weights &amp; Biases, developer tools for machine learning<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658196834539,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-log-inconsistent-behavior-with-step-parameter\/2771",
        "Challenge_link_count":10,
        "Challenge_participation_count":7,
        "Challenge_readability":21.6,
        "Challenge_reading_time":31.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"Wandb.log inconsistent behavior with step parameter",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":196.0,
        "Challenge_word_count":129,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>As far as I understand the step variable, once the step is incremented, the value is stored immutably. So, if chronologically, you execute:<\/p>\n<pre><code class=\"lang-python\">\nwandb.log({'potato': 1}, step=0}\nwandb.log({'tomato': 1}, step=0}\nwandb.log({'potato': 2}, step=1}\nwandb.log({'tomato': 2}, step=1}\n\n<\/code><\/pre>\n<p>It\u2019ll be fine but instead if you execute:<\/p>\n<pre><code class=\"lang-python\">\nwandb.log({'potato': 1}, step=0}\nwandb.log({'potato': 2}, step=1}\nwandb.log({'tomato': 1}, step=0}\nwandb.log({'tomato': 2}, step=1}\n\n<\/code><\/pre>\n<p>the third command (tomato = 1, step 0) will not be executed since the logger has already moved past step 0.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":8.59,
        "Solution_score_count":null,
        "Solution_sentence_count":10.0,
        "Solution_word_count":81.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"Under the Vertex AI - a dataset failed to create due to a constraint applied to the organization. It does not allow for the deletion of the dataset, I attempted using python (Delete a dataset \u00a0|\u00a0 Vertex AI \u00a0|\u00a0 Google Cloud) and the response was -\u00a0\"...is in failure state and cannot be deleted. It will be deleted automatically after a few days.\"\u00a0\u00a0but it didn't delete. There is not a gcloud command to correct. Short of a support request..how can the dataset be removed as I foresee this occuring as others attempt experiments. I have addressed the issue with the constraint.",
        "Challenge_closed_time":1677681.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1677567600000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Deleting-a-failed-dataset\/m-p\/527077#M1360",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":4.9,
        "Challenge_reading_time":7.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Deleting a failed dataset",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":96.0,
        "Challenge_word_count":101,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I tried running the same\u00a0code you used and I was able to delete a dataset that was successfully created. I suspect in your case, the failure state of the dataset is the problem. Also, there is indeed no gcloud command to manually delete it. I would still suggest you file a\u00a0ticket here so\u00a0Google Cloud's engineering team can further investigate.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.2,
        "Solution_reading_time":4.55,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":67.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1622632545867,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1.0,
        "Answerer_view_count":111.0,
        "Challenge_adjusted_solved_time":0.0243575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running a <code>JupyterLab<\/code> on <code>AWS SageMaker<\/code>. Kernel: <code>conda_amazonei_mxnet_p27<\/code><\/p>\n<p>The number of fields found: <code>saw 9<\/code> increments by 1, each run.<\/p>\n<p><strong>Error:<\/strong> <code>ParserError: Error tokenizing data. C error: Expected 2 fields in line 50, saw 9<\/code><\/p>\n<hr \/>\n<h3>Code:<\/h3>\n<p>Invocation (Error doesn't appear when running all cells before this but does when this is ran):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>train = open('train_textcorrupted.csv', 'a')\nval = open('val.csv', 'a')\nclasses = open('classes.txt', 'a')\nuni_label = 'Organisation\\tUniversity'\nn_pad = 4\nfor i in range(len(unis)-n_pad):\n    record = ' '.join(unis[i:(i+n_pad)])\n    full_record = f'{uni_label}\\t{record}\\n'\n    if random.random() &gt; 0.9:\n        val.write(full_record)\n    else:\n        train.write(full_record) \n\nclasses.write(uni_label)\nclasses.close() \nval.close()\ntrain.close()                      \n<\/code><\/pre>\n<h3>Traceback:<\/h3>\n<pre class=\"lang-sh prettyprint-override\"><code>---------------------------------------------------------------------------\nParserError                               Traceback (most recent call last)\n&lt;ipython-input-8-89b1728bd5a6&gt; in &lt;module&gt;\n      7       --gpus 1\n      8     &quot;&quot;&quot;.split()\n----&gt; 9 run_training(args)\n&lt;ipython-input-5-091daf2638a1&gt; in run_training(input)\n     55     csv_logger = pl.loggers.CSVLogger(save_dir=f'{args.modeldir}\/csv_logs')\n     56     loggers = [logger, csv_logger]\n---&gt; 57     dm = OntologyTaggerDataModule.from_argparse_args(args)\n     58     if args.model_uri:\n     59         local_model_uri = os.environ.get('SM_CHANNEL_MODEL', '.')\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pytorch_lightning\/core\/datamodule.py in from_argparse_args(cls, args, **kwargs)\n    324         datamodule_kwargs.update(**kwargs)\n    325 \n--&gt; 326         return cls(**datamodule_kwargs)\n    327 \n    328     @classmethod\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pytorch_lightning\/core\/datamodule.py in __call__(cls, *args, **kwargs)\n     47 \n     48         # Get instance of LightningDataModule by mocking its __init__ via __call__\n---&gt; 49         obj = type.__call__(cls, *args, **kwargs)\n     50 \n     51         return obj\n&lt;ipython-input-3-66ee2be72e78&gt; in __init__(self, traindir, train_file, validate_file, model_name, labels, batch_size)\n     30         print('tokenizer', tokenizer)\n     31         print('labels_file', labels_file)\n---&gt; 32         label_mapper = LabelMapper(labels_file)\n     33         self.batch_size = batch_size\n     34         self.num_classes = label_mapper.num_classes\n&lt;ipython-input-3-66ee2be72e78&gt; in __init__(self, classes_file)\n    102 \n    103     def __init__(self, classes_file):\n--&gt; 104         self._raw_labels = pd.read_csv(classes_file, header=None, sep='\\t')\n    105 \n    106         self._map = []\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\n    686     )\n    687 \n--&gt; 688     return _read(filepath_or_buffer, kwds)\n    689 \n    690 \n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py in _read(filepath_or_buffer, kwds)\n    458 \n    459     try:\n--&gt; 460         data = parser.read(nrows)\n    461     finally:\n    462         parser.close()\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py in read(self, nrows)\n   1196     def read(self, nrows=None):\n   1197         nrows = _validate_integer(&quot;nrows&quot;, nrows)\n-&gt; 1198         ret = self._engine.read(nrows)\n   1199 \n   1200         # May alter columns \/ col_dict\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py in read(self, nrows)\n   2155     def read(self, nrows=None):\n   2156         try:\n-&gt; 2157             data = self._reader.read(nrows)\n   2158         except StopIteration:\n   2159             if self._first_chunk:\npandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader.read()\npandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader._read_low_memory()\npandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader._read_rows()\npandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader._tokenize_rows()\npandas\/_libs\/parsers.pyx in pandas._libs.parsers.raise_parser_error()\nParserError: Error tokenizing data. C error: Expected 2 fields in line 50, saw 9\n<\/code><\/pre>\n<hr \/>\n<p><code>classes.txt<\/code> (tab-separated) Before runtime<\/p>\n<pre><code>Activity    Event\nActor   Person\nAgent   Person\nAlbum   Product\nAnimal  Object\nArchitecturalStructure  Location\nArtist  Person\nAthlete Person\nAutomobileEngine    Product\nAward   Object\nBiomolecule Object\nBird    Object\nBodyOfWater Location\nBuilding    Location\nChemicalSubstance   Object\nCompany Organisation\nCompetition Event\nDevice  Product\nDisease Object\nDistrict    Location\nEukaryote   Object\nEvent   Event\nFilm    Object\nFood    Object\nLanguage    Object\nLocation    Location\nMeanOfTransportation    Product\nMotorsportSeason    Event\nMunicipality    Location\nMusicalWork Product\nOrganisation    Organisation\nPainter Person\nPeriodicalLiterature    Product\nPerson  Person\nPersonFunction  Person\nPlant   Object\nPoet    Person\nPolitician  Person\nRiver   Location\nSchool  Organisation\nSettlement  Location\nSoftware    Product\nSong    Product\nSpecies Object\nSportsSeason    Event\nStation Location\nTown    Location\nVillage Location\nWriter  Person\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":6,
        "Challenge_created_time":1630938895957,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1631000955160,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69076270",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":19.1,
        "Challenge_reading_time":79.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":38,
        "Challenge_solved_time":null,
        "Challenge_title":".txt altered after save leads to CSV reader seeing too many fields",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":87.0,
        "Challenge_word_count":508,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1622632545867,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":111.0,
        "Solution_body":"<p>Problem Found:<\/p>\n<p>So no fault of my own, I keep ensuring these fields are on their own lines in <code>classes.txt<\/code> and <code>Ctrl+S<\/code>. Then when I reopen the file, <strong>after runtime<\/strong>, it'll have fields be on the same line again.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/UbRTm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/UbRTm.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>To fix this, on line <code>classes.write(uni_label)<\/code>.<\/p>\n<p>I replaced it with <code>classes.write('\\n'+uni_label)<\/code>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1631001042847,
        "Solution_link_count":2.0,
        "Solution_readability":7.8,
        "Solution_reading_time":7.46,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":60.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"Hi everybody,\r\n\r\nI am trying to use AWS built-in algorithms in Sagemaker Studio Lab. For that I need an execution role and region etc. \r\nWhen I try to run my code it outputs\r\n\r\nValueError: Must setup local AWS configuration with a region supported by SageMaker.\r\n\r\nIs it even possible to link access AWS resources in Studiolab?\r\n\r\nMany thanks in advance!\r\n\r\n\r\n",
        "Challenge_closed_time":1639666.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639662702000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/30",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":8.0,
        "Challenge_reading_time":5.16,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Can't configure profile with AWS CLI for using AWS Built-in sagemaker algorithms ",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":72,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi! The use case you are describing is exactly why we have this example notebook:\r\n- https:\/\/github.com\/aws\/studio-lab-examples\/blob\/main\/connect-to-aws\/Access_AWS_from_Studio_Lab.ipynb \r\n\r\nYour net net is:\r\n1\/ Ensure you have proper training and authorization to use your AWS access and secret keys. If you are an AWS account admin, you can find this in your IAM console. If you are not, work with your admin to determine if this pattern is appropriate for you. \r\n\r\n2\/ If you are qualified to manage your AWS keys, create a new file called `~\/.aws\/credentials`. There, copy and paste in your access and secret keys.\r\n\r\n3\/ Remove any cells you ran in a notebook to create or verify those files.\r\n\r\n4\/ Create a SageMaker execution role. The easiest way to do this is via the console - you can create a new SageMaker execution role when create a notebook instance or a Studio user profile. Once this is done, paste in the arn (Amazon Resource Name), for this execution role.\r\n\r\n5\/ Go forth and scale up on SageMaker! After that,  once you are using the SageMaker Python SDK, you should be able to use all code-based features within SageMaker, such as training, hosting, tuning, autopilot, pipelines, etc.   Hi @EmilyWebber. Might you also have an example that doesn't require AWS account information yet avoids \"ValueError: Must setup local AWS configuration with a region supported by SageMaker\" in the code below for \"role\"?\r\n\r\n```\r\n# create Hugging Face Model Class\r\nhuggingface_model = HuggingFaceModel(\r\n\ttransformers_version='4.6.1',\r\n\tpytorch_version='1.7.1',\r\n\tpy_version='py36',\r\n\tenv=hub,\r\n\trole=role, \r\n)\r\n```\r\nCode source: https:\/\/huggingface.co\/distilbert-base-uncased-finetuned-sst-2-english, where Deploy is Amazon SageMaker rather than Accelerated Inference \u2014 the latter [currently returns an error](https:\/\/discuss.huggingface.co\/t\/distilbert-accelerated-inference-error\/15027) with or without SageMaker\r\n\r\nSince the SageMaker Studio Lab website emphasizes no costs and no need to sign up for an AWS account, a team and I are exploring whether to have students try. Thank you in advance. Hi @derekschan - thanks for the comment. This is actually expected behavior right now - Studio Lab defaults to not having access to any AWS API's unless explicitly granted. To date, that is solved only by the pattern mentioned above in this issue, explicitly installing AWS key permissions to utilize them. \r\n\r\nShould that ever change in the future we will be sure to let you know! I'll mark your comment as a feature enhancement.  Thank you, @EmilyWebber.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":9.3,
        "Solution_reading_time":31.28,
        "Solution_score_count":null,
        "Solution_sentence_count":24.0,
        "Solution_word_count":372.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":10,
        "Challenge_body":"When `logger.log_metrics(metrics)` is called with a `CometLogger`, `metrics` may be modified in-place. This can lead to confusing errors. E.g. if the user does\r\n\r\n```python\r\ndef training_step(self, batch, batch_idx):\r\n    losses = self._get_losses(batch)\r\n    self.logger.log_metrics(losses)\r\n    return losses\r\n```\r\n\r\nthen `losses` will have all the tensors moved to the CPU and their gradients detached, leading to an error like `RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn` when backprop is attempted.\r\n\r\nNone of the other loggers change `metrics` in-place when `log_metrics` is called. All of them except neptune say that they just accept `metrics: Dict[str, float]`, though some others (e.g. the tensorboard logger) have code to handle `torch.Tensor`s or other types as well.\r\n\r\nThe `CSVLogger` uses the following for handling tensors:\r\n```python\r\ndef _handle_value(value):\r\n    if isinstance(value, torch.Tensor):\r\n        return value.item()\r\n    return value\r\n...\r\nmetrics = {k: _handle_value(v) for k, v in metrics_dict.items()}\r\n```\r\n\r\nThe `TensorBoardLogger` similarly has\r\n\r\n```python\r\nfor k, v in metrics.items():\r\n    if isinstance(v, torch.Tensor):\r\n        v = v.item()\r\n    ...\r\n    self.experiment.add_scalar(k, v, step)\r\n```\r\n\r\nIn the `CometLogger`, the current tensor conversion code is\r\n\r\n```python\r\nfor key, val in metrics.items():\r\n  if is_tensor(val):\r\n    metrics[key] = val.cpu().detach()\r\n```\r\n\r\nbut then the entire `metrics` dictionary is copied later in the function anyway, so it doesn't really make sense to do in-place modification then copy everything.\r\n\r\nI'm happy to submit a PR to fix this so that the `CometLogger` doesn't modify the original `metrics` dictionary. I just wanted to ask for a couple of opinions before changing things:\r\n\r\n1. Should I keep the current tensor conversion behavior for `CometLogger` (`val.cpu().detach()`) or switch to using `val.item()`? My preference would be the latter, though this does change the behavior (see at the end).\r\n2. Should I update the other loggers to all accept `metrics: Dict[str, Union[float, torch.Tensor]]` and have them all use the same method (probably imported from `loggers\/base.py`) to convert to a `Dict[str, float]`?\r\n3. * I don't know the other loggers, so I'm not sure if tensors are actually not supported or if the type annotation isn't precise and the conversion is happening in third-party code\r\n\r\n---\r\n\r\n`val.cpu().detach()` vs `val.item()`\r\n* Comet sort of has support for tensors with >1 element, so using the first method will make logging such tensors valid while the second method would throw an error. However, I don't think anybody would be using this behavior on purpose. If you do `logger.log_metrics({\"test\": torch.tensor([1.0, 10.0])})`, you get `COMET WARNING: Cannot safely convert array([ 1., 10.], dtype=float32) object to a scalar value, using its string representation for logging`. The metric itself doesn't even appear in the web interface for CometML, so I assume you can only access it if you query for it directly through their API.\r\n",
        "Challenge_closed_time":1630398.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618430187000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7021",
        "Challenge_link_count":0,
        "Challenge_participation_count":10,
        "Challenge_readability":8.2,
        "Challenge_reading_time":37.84,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":null,
        "Challenge_title":"CometLogger can modify logged metrics in-place ",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":439,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"PR on this is more than welcome! Great observation. Btw I believe we don't expect users to directly call `self.logger.log_metrics`, but we should still fix it :) \n\n\n> val.cpu().detach() vs val.item()\n\nDoes Comet accept scalar tensors? If it can do the tensor->Python conversion (why wouldn't it), I would go with `val.cpu().detach()` as in the other loggers. @neighthan still interested to send a fix for this?  This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n Hi @awaelchli! I am new to open source contribution and since this is a good first issue, I would like to try my hand at it! Dear @sohamtiwari3120,\r\n\r\nYes, feel free to take on this one and open a PR.\r\n\r\nBest,\r\nT.C Hi @tchaton,\r\n\r\nCan you please review my PR. There are a few checks that failed and I am unable to determine the exact cause for the same.\r\n\r\nSincerely,\r\nSoham Hey @ sohamtiwari3120,\r\n\r\nApproved. Mind adding a test to prevent regression ?\r\n\r\nBest,\r\nT.C Hi @tchaton \r\n\r\nI would love to try! However, it would be my first time writing tests. Therefore could you please help me with the following:\r\n- can you explain how will the test to prevent regression look like,\r\n- also could you provide any references useful for beginners in writing tests.\r\n\r\nSincerely,\r\nSoham Dear @sohamtiwari3120,\r\n\r\nCheck out this document: https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/.github\/CONTRIBUTING.md\r\n\r\nIn this case, the test should ensure the values aren't modified the logged metrics owned by the trainer.\r\n\r\nBest,\r\nT.C",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":6.7,
        "Solution_reading_time":22.68,
        "Solution_score_count":null,
        "Solution_sentence_count":25.0,
        "Solution_word_count":296.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":1455667285907,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":283.0,
        "Answerer_view_count":85.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a set of pre-processing stages in sklearn <code>Pipeline<\/code> and an estimator which is a <code>KerasClassifier<\/code> (<code>from tensorflow.keras.wrappers.scikit_learn import KerasClassifier<\/code>).<\/p>\n<p>My overall goal is to tune and log the whole sklearn pipeline in <code>mlflow<\/code> (in databricks evn). I get a confusing type error which I can't figure out how to reslove:<\/p>\n<blockquote>\n<p>TypeError: can't pickle _thread.RLock objects<\/p>\n<\/blockquote>\n<p>I have the following code (without tuning stage) which returns the above error:<\/p>\n<pre><code>conda_env = _mlflow_conda_env(\n    additional_conda_deps=None,\n    additional_pip_deps=[\n        &quot;cloudpickle=={}&quot;.format(cloudpickle.__version__),\n        &quot;scikit-learn=={}&quot;.format(sklearn.__version__),\n        &quot;numpy=={}&quot;.format(np.__version__),\n        &quot;tensorflow=={}&quot;.format(tf.__version__),\n    ],\n    additional_conda_channels=None,\n)\n\nsearch_space = {\n    &quot;estimator__dense_l1&quot;: 20,\n    &quot;estimator__dense_l2&quot;: 20,\n    &quot;estimator__learning_rate&quot;: 0.1,\n    &quot;estimator__optimizer&quot;: &quot;Adam&quot;,\n}\n\n\ndef create_model(n):\n\n    model = Sequential()\n    model.add(Dense(int(n[&quot;estimator__dense_l1&quot;]), activation=&quot;relu&quot;))\n    model.add(Dense(int(n[&quot;estimator__dense_l2&quot;]), activation=&quot;relu&quot;))\n    model.add(Dense(1, activation=&quot;sigmoid&quot;))\n    model.compile(\n        loss=&quot;binary_crossentropy&quot;,\n        optimizer=n[&quot;estimator__optimizer&quot;],\n        metrics=[&quot;accuracy&quot;],\n    )\n\n    return model\n\n\nmlflow.sklearn.autolog()\nwith mlflow.start_run(nested=True) as run:\n\n    classfier = KerasClassifier(build_fn=create_model, n=search_space)\n    # fit the pipeline\n    clf = Pipeline(steps=[(&quot;preprocessor&quot;, preprocessor), \n                          (&quot;estimator&quot;, classfier)])\n    h = clf.fit(\n        X_train,\n        y_train.values,\n        estimator__validation_split=0.2,\n        estimator__epochs=10,\n        estimator__verbose=2,\n    )\n\n    # log scores\n    acc_score = clf.score(X=X_test, y=y_test)\n    mlflow.log_metric(&quot;accuracy&quot;, acc_score)\n\n    signature = infer_signature(X_test, clf.predict(X_test))\n    # Log the model with a signature that defines the schema of the model's inputs and outputs.\n    mlflow.sklearn.log_model(\n        sk_model=clf, artifact_path=&quot;model&quot;, \n        signature=signature, \n        conda_env=conda_env\n    )\n<\/code><\/pre>\n<p>I also get this warning before the error:<\/p>\n<pre><code>\n    WARNING mlflow.sklearn.utils: Truncated the value of the key `steps`. Truncated value: `[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n                      transformer_weights=None,\n                      transformers=[('num',\n                                   Pipeline(memory=None,\n<\/code><\/pre>\n<p>note the the whole pipeline runs outside mlflow.\ncan someone help?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1631241004103,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69126555",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":18.5,
        "Challenge_reading_time":36.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":null,
        "Challenge_title":"how to log KerasClassifier model in a sklearn pipeline mlflow?",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":435.0,
        "Challenge_word_count":209,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1455667285907,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":283.0,
        "Poster_view_count":85.0,
        "Solution_body":"<p>I think I find sort of a workaround\/solution for this for now, but I think this issue needs to be addressed in MLFloow anyways.<\/p>\n<p>What I did is not the best way probably.\nI used a python package called <a href=\"https:\/\/scikeras.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">scikeras<\/a> that does this wrapping and then could log the model<\/p>\n<p>The code:<\/p>\n<pre><code>import scikeras \nimport tensorflow as tf \nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Flatten, Activation \n \nfrom scikeras.wrappers import KerasClassifier \n  \n \nclass ModelWrapper(mlflow.pyfunc.PythonModel): \n    def __init__(self, model): \n        self.model = model \n \n    def predict(self, context, model_input): \n        return self.model.predict(model_input) \n \nconda_env =  _mlflow_conda_env( \n      additional_conda_deps=None, \n      additional_pip_deps=[ \n        &quot;cloudpickle=={}&quot;.format(cloudpickle.__version__),  \n        &quot;scikit-learn=={}&quot;.format(sklearn.__version__), \n        &quot;numpy=={}&quot;.format(np.__version__), \n        &quot;tensorflow=={}&quot;.format(tf.__version__), \n        &quot;scikeras=={}&quot;.format(scikeras.__version__), \n      ], \n      additional_conda_channels=None, \n  ) \n \nparam = { \n   &quot;dense_l1&quot;: 20, \n   &quot;dense_l2&quot;: 20, \n   &quot;optimizer__learning_rate&quot;: 0.1, \n   &quot;optimizer&quot;: &quot;Adam&quot;, \n   &quot;loss&quot;:&quot;binary_crossentropy&quot;, \n} \n \n  \ndef create_model(dense_l1, dense_l2, meta): \n  \n  n_features_in_ = meta[&quot;n_features_in_&quot;] \n  X_shape_ = meta[&quot;X_shape_&quot;] \n  n_classes_ = meta[&quot;n_classes_&quot;] \n \n  model = Sequential() \n  model.add(Dense(n_features_in_, input_shape=X_shape_[1:], activation=&quot;relu&quot;)) \n  model.add(Dense(dense_l1, activation=&quot;relu&quot;)) \n  model.add(Dense(dense_l2, activation=&quot;relu&quot;)) \n  model.add(Dense(1, activation=&quot;sigmoid&quot;)) \n \n  return model   \n \nmlflow.sklearn.autolog() \nwith mlflow.start_run(run_name=&quot;sample_run&quot;): \n \n  classfier = KerasClassifier( \n    create_model, \n    loss=param[&quot;loss&quot;], \n    dense_l1=param[&quot;dense_l1&quot;], \n    dense_l2=param[&quot;dense_l2&quot;], \n    optimizer__learning_rate = param[&quot;optimizer__learning_rate&quot;], \n    optimizer= param[&quot;optimizer&quot;], \n) \n \n  # fit the pipeline \n  clf = Pipeline(steps=[('preprocessor', preprocessor), \n                      ('estimator', classfier)])   \n \n  h = clf.fit(X_train, y_train.values) \n  # log scores \n  acc_score = clf.score(X=X_test, y=y_test) \n  mlflow.log_metric(&quot;accuracy&quot;, acc_score) \n  signature = infer_signature(X_test, clf.predict(X_test)) \n  model_nn = ModelWrapper(clf,)  \n \n  mlflow.pyfunc.log_model( \n      python_model= model_nn, \n      artifact_path = &quot;model&quot;,  \n      signature = signature,  \n      conda_env = conda_env \n  ) \n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":19.3,
        "Solution_reading_time":35.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":180.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1232453837820,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sweden",
        "Answerer_reputation_count":27150.0,
        "Answerer_view_count":6735.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm working on a deep learning project with about 700GB of table-like time series data in thousands of .csv files (each about 15MB). <br>\nAll the data is on S3 and it needs some preprocessing before being fed into the model. The question is how to best go about automating the process of loading, preprocessing and training. <br><br>Is a custom keras generator with some built in preprocessing the best solution?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1537356638503,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52404879",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":6.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Efficient management of large amounts of data with SageMaker for training a keras model",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":423.0,
        "Challenge_word_count":83,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1488416220368,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":142.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Preprocessing implies that this is something you might want to decouple from the model execution and run separately, possibly on a schedule or in response to new data flowing in.<\/p>\n\n<p>If so, you'll probably want to do the preprocessing outside of SageMaker. You could orchestrate it using <a href=\"https:\/\/aws.amazon.com\/glue\/\" rel=\"nofollow noreferrer\">Glue<\/a>, or you could write a custom job and run it through <a href=\"https:\/\/aws.amazon.com\/batch\/\" rel=\"nofollow noreferrer\">AWS Batch<\/a> or alternatively on an EMR cluster.<\/p>\n\n<p>That way, your Keras notebook can load the already preprocessed data, train and test through SageMaker.<\/p>\n\n<p>With a little care, you should be able to perform at least some of the heavy lifting incrementally in the preprocessing step, saving both time and cost downstream in the Deep Learning pipeline.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.1,
        "Solution_reading_time":10.72,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":122.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Try to add the Redshift connection on SageMaker Canvas to import the data\n\n- The cluster identify: redshift-cluster-1\n- database name: dev\n- database user: awsuser\n- unload IAM Role: my-reshift-role\n- connection name: redshift\n- type: IAM\n\nmy-reshift-role trust-relationship is trust the \"redshift.amazonaws.com\" and \"sagemaker.amazonaws.com\"\n\nExpectation: create connection successfully\n\nActually result: \nRedshiftCreateConnectionError\nUnable to validate connection. An error occurred when trying to list schema from Redshift",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641573857407,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668605964856,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUJvjatAJaQv-Ist96WT1IIw\/sagemaker-canvas-connect-redshift-failed",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":7.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker Canvas connect Redshift failed",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":298.0,
        "Challenge_word_count":65,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The sagemaker canvas using sagemaker domain user, so need add the Redshift permission to the IAM Role attached to domain user. After add the permission, the connection can be setup",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1641634221918,
        "Solution_link_count":0.0,
        "Solution_readability":10.3,
        "Solution_reading_time":2.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":30.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,  <\/p>\n<p>I am trying to export data from Azure ML to an Azure SQL Database using the 'Export Data' module but the log file contains the following messages and no data is exported to the database.  <\/p>\n<p>&quot;Not exporting to run RunHistory as the exporter is either stopped or there is no data&quot;  <\/p>\n<p>&quot;Process exiting with code: 0  <\/p>\n<p>There is definitely data flowing to the 'Export Data' module from an 'Execute R Script' module as I have checked the Result dataset.  <\/p>\n<p>Would appreciate some assistance.  <\/p>\n<p>Thank you.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629008927050,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/514067\/no-data-being-exported-from-export-data-module-in",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":7.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"No Data being exported from 'Export Data' module in Azure ML",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":102,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi,   <\/p>\n<p>I have resolved this issue. I had set the export table to be dbo.TestTable rather than just TestTable. As the table dbo.TestTable did not exist the 'Export module' created it in the dbo schema so the table name effectively became dbo.dbo.TestTable.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.5,
        "Solution_reading_time":3.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1639972620503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1653.0,
        "Answerer_view_count":1212.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am really confused about organizing google Vertex Ai dataset and train the autoML model in GCP. Could any one please help me to understand?<\/p>\n<p>Let me explain scenarios in which I have confusion.<\/p>\n<p>Let\u2019s suppose if I have <em>Text entity extraction<\/em> dataset in vertex Ai \u201c<strong>contract_delivery_02<\/strong>\u201d with 25 files. I have 3 labels created (<em>DelIncoTerms, DelLocation and DelWindow<\/em>) and I have trained model. This is working great.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/KvWom.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KvWom.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Now, I have 10 more files to upload, where I have introduced 2 additional labels (<em>DelPrice &amp; DelDelivery<\/em>).<\/p>\n<p>My questions<\/p>\n<ol>\n<li>Do I require to do upload all the files (25 + 10) again ?<\/li>\n<li>Do I require to retrain my whole autoML model again ? or is there any other approach for this scenario?<\/li>\n<\/ol>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1656616780960,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72821008",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":7.4,
        "Challenge_reading_time":12.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Vertex AI updating dataset and train model",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":149.0,
        "Challenge_word_count":138,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1462469556836,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":509.0,
        "Poster_view_count":65.0,
        "Solution_body":"<p>For question #1, you don't have to upload all files again. In your <strong>Dataset<\/strong>, you just have to add your <strong>2 new labels<\/strong> and then upload your additional 10 files.\n<a href=\"https:\/\/i.stack.imgur.com\/rLep2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rLep2.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Once uploaded, you may now proceed to put labels on your newly added files (in your example, total of 10 files) and then assign the new labels on <strong>ALL files<\/strong> (25 + 10). You can do this by double-clicking the newly added text from the UI and then assign necessary labels.\n<a href=\"https:\/\/i.stack.imgur.com\/jqIaj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jqIaj.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>For question #2, since there are newly added labels and training texts, it is necessary for you to retrain the whole autoML for more accurate Model and better quality of results.<\/p>\n<p>You may refer to this <a href=\"https:\/\/cloud.google.com\/natural-language\/automl\/docs\/prepare#expandable-2\" rel=\"nofollow noreferrer\">Text Entity Extraction preparation of data<\/a> and <a href=\"https:\/\/cloud.google.com\/natural-language\/automl\/docs\/models\" rel=\"nofollow noreferrer\">Training Models<\/a> documentation for more details.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":12.9,
        "Solution_reading_time":17.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":155.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1425802890212,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA, USA",
        "Answerer_reputation_count":41.0,
        "Answerer_view_count":19.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Getting the following response even when I make one request (concurrency set to 200) to a web service. <\/p>\n\n<p>{ status: 503, headers: '{\"content-length\":\"174\",\"content-type\":\"application\/json; charset=utf-8\",\"etag\":\"\\\"8ce068bf420a485c8096065ea3e4f436\\\"\",\"server\":\"Microsoft-HTTPAPI\/2.0\",\"x-ms-request-id\":\"d5c56cdd-644f-48ba-ba2b-6eb444975e4c\",\"date\":\"Mon, 15 Feb 2016 04:54:01 GMT\",\"connection\":\"close\"}',  body: '{\"error\":{\"code\":\"ServiceUnavailable\",\"message\":\"Service is temporarily unavailable.\",\"details\":[{\"code\":\"NoMoreResources\",\"message\":\"No resources available for request.\"}]}}' }<\/p>\n\n<p>The request-response web service is a recommender retraining web service with the training set containing close to 200k records. The training set is already present in my ML studio dataset, only 10-15 extra records are passed in the request. The same experiment was working flawlessly till 13th Feb 2016. I have already tried increasing the concurrency but still the same issue. I even reduced the size of the training set to 20 records, still didn't work.<\/p>\n\n<p>I have two web service both doing something similar and both aren't working since 13th Feb 2016. <\/p>\n\n<p>Finally, I created a really small experiment ( skill.csv --> split row ---> web output )   which doesn't take any input. It just has to return some part of the dataset. Did not work, response code 503.<\/p>\n\n<p>The logs I got are as follows<\/p>\n\n<p>{\n  \"version\": \"2014-10-01\",\n  \"diagnostics\": [{\n    .....\n    {\n      \"type\": \"GetResourceEndEvent\",\n      \"timestamp\": 13.1362,\n      \"resourceId\": \"5e2d653c2b214e4dad2927210af4a436.865467b9e7c5410e9ebe829abd0050cd.v1-default-111\",\n      \"status\": \"Failure\",\n      \"error\": \"The Uri for the target storage location is not specified. Please consider changing the request's location mode.\"\n    },\n    {\n      \"type\": \"InitializationSummary\",\n      \"time\": \"2016-02-15T04:46:18.3651714Z\",\n      \"status\": \"Failure\",\n      \"error\": \"The Uri for the target storage location is not specified. Please consider changing the request's location mode.\"\n    }\n  ]\n}<\/p>\n\n<p>What am I missing? Or am I doing it completely wrong?<\/p>\n\n<p>Thank you in advance.<\/p>\n\n<p>PS: Data is stored in mongoDB and then imported as CSV<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1455545889500,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1456850010663,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35411741",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":28.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML: Getting Error 503: NoMoreResources to any web service API even when I only make 1 request",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":283.0,
        "Challenge_word_count":272,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1425802890212,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Boston, MA, USA",
        "Poster_reputation_count":41.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>This was an Azure problem. I quote the Microsoft guy, <\/p>\n\n<blockquote>\n  <p>We believe we have isolated the issue impacting tour service and we are currently working on a fix. We will be able to deploy this in the next couple of days. The problem is impacting only the ASIA AzureML region at this time, so if this is an option for you, might I suggest using a workspace in either the US or EU region until the fix gets rolled out here.<\/p>\n<\/blockquote>\n\n<p>To view the complete discussion, click <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/985e253e-5e54-45a5-a359-5c501152c445\/getting-error-503-nomoreresources-to-any-web-service-api-even-when-i-only-make-1-request?forum=MachineLearning&amp;prof=required\" rel=\"nofollow\">here<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.7,
        "Solution_reading_time":9.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":93.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nI have a problem saving xgboost run in mlflow server. The run has a status of UNFINISHED, no metrics or artifacts are created. \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/101572186\/183577670-53398204-debf-428b-8b0c-3c7ca83f4785.png)\r\n\r\nWhen I use `mlflow ui` everything is fine, but when I run mlflow server with SQLite as backend store the problem occurs.\r\nCommand used to run mlflow server- `mlflow server --host 0.0.0.0 --port 5000 --default-artifact-root \/mlflow\/artifacts\/ --backend-store-uri sqlite:\/\/\/\/\/mlflow\/experiments\/mlflow.db`\n\n### Reproducible Example\n\n```python\nimport mlflow\r\nfrom pycaret.classification import *\r\nimport pandas as pd\r\n\r\nmlflow.set_tracking_uri('http:\/\/localhost:5000')\r\n\r\ndata = pd.DataFrame({'V1': [-1.34419, -1.89211, 1.69421, 0.263328, 0.107918, 0.154241, 0.33468, 1.447778, -0.918269, 0.86319, -1.630049, 1.643798, 1.274341, -1.296742, -0.193585, 1.627422, -0.66805, -1.664491, -1.86911, 0.892885],\r\n                     'V2': [0.85556, -1.70503, -0.02896, 1.746258, -0.084151, 1.673185, 1.113326, -0.23231, 1.054817, -1.407584, 0.474997, 0.150687, -0.738246, -0.045513, 1.58637, 0.984249, 0.624333, 0.298866, 0.662204, 0.967942],\r\n                     'V3': [1.768638, -0.503169, -0.25622, -0.937752, -0.062189, -0.820652, -1.786942, -1.770495, 1.808681, -0.280286, -1.389736, 0.182212, -0.602959, -0.354683, -1.065631, 1.649264, 0.389538, -1.674815, 0.281824, -1.683662],\r\n                     'V4': [1.512828, 1.177697, -1.156862, -1.877876, 1.526013, 1.644001, -1.282481, -0.720543, 0.323963, -1.931616, 1.632839, 1.706752, 1.895627, 1.860705, -1.559702, 1.517466, 1.254323, 1.84415, -1.175013, -1.600652],\r\n                     'V5': [0.820483, -1.20923, -0.012221, 1.682836, 0.104248, 1.258085, 0.404062, 0.18019, 1.352545, -0.497071, 0.771277, 1.614052, -0.693854, 0.002655, 0.277743, -0.977744, -0.97259, -1.501586, -0.731194, -0.551264],\r\n                     'V6': [1.079115, -0.734152, -1.630816, -1.877664, 1.577477, -1.902078, 1.012828, -1.107726, 1.742781, -1.338595, 1.788969, -0.851507, 1.061596, -0.635559, -1.171469, -1.001642, 1.493507, 0.732088, 1.565327, -1.845441],\r\n                     'V7': [1.165929, 1.804607, 0.886589, -0.027458, -1.444197, -0.415643, 0.863924, -1.177661, 1.684514, 1.023797, -1.234116, -0.989024, 0.815575, -0.668453, 0.591911, -0.798925, 1.024032, -1.983963, 1.900752, 1.201001],\r\n                     'V8': [-0.536923, 0.641581, -0.585228, 1.061145, -0.303192, -0.652068, 0.858556, 0.11012, 1.839738, -1.51798, -0.942028, -0.736386, -0.098261, 0.699127, 0.173854, -1.16775, -0.417662, 0.021639, 1.745042, -1.119667],\r\n                     'V9': [0.643498, -1.090347, 0.120182, -0.819219, -1.296763, 0.530723, -1.367664, -0.708116, -1.304274, 1.486166, 1.656498, 1.645308, -0.257558, 0.400849, 1.356781, 1.693433, 0.42606, 0.370683, -0.239278, -0.541334],\r\n                     'V10': [-0.744989, 0.506658, 1.15586, 1.461127, 1.928769, -0.330472, 1.514159, -1.209056, -0.741453, -1.479674, 1.92057, -1.148481, 0.949433, 0.674107, -1.410627, 1.497083, -1.262624, -0.856706, -1.708155, 0.93153],\r\n                     'V11': [0.967242, 1.968385, -1.362337, -0.46194, 0.809224, 0.226177, 1.782128, -0.114595, 0.698243, -0.141743, -0.117251, 1.762656, -0.068839, 0.648945, -1.497037, -1.455443, -0.291242, 1.806048, -1.945438, 0.251282],\r\n                     'V12': [0.010432, -0.101522, -1.764095, 1.326967, -1.299122, -0.549148, 0.807092, -0.75387, 0.955056, 0.640369, -0.917832, 0.250338, 0.624729, 1.566922, 0.118619, 1.907585, -0.919995, 0.868393, -1.103909, 0.347108],\r\n                     'V13': [0.122315, -1.140017, -0.876424, -1.075771, 0.668814, 1.916654, -0.864906, 0.132892, 0.740058, 0.94469, -0.260381, 0.92833, -1.186423, -0.18321, 1.99266, -0.779091, -1.649025, -1.688821, 1.075145, -1.988603],\r\n                     'V14': [-1.494, 0.679776, 0.813194, 1.8687, -0.20273, -0.363265, 1.98902, 0.100025, 1.462866, 0.561017, 0.418922, 1.981837, -1.834009, -1.657952, 0.585069, -0.898764, 0.683234, 0.743215, -0.050289, -0.668302], \r\n                     'V15': [0.199787, 0.81829, 1.200156, -1.684249, 0.847466, 1.326102, 0.323103, -1.010648, -1.868355, -1.204467, 1.777393, 0.375692, -1.654002, 0.50357, -1.372448, -0.522425, 0.360716, 1.007605, 1.009369, -0.353638],\r\n                     'V16': [1.535552, -0.082278, -0.083154, 0.069432, 1.356735, -0.042527, -0.462543, 1.813852, -1.664882, 0.408013, -1.802172, -1.920202, 1.987332, -1.126771, 1.485496, 1.972345, -0.33345, 1.414685, -0.06674, 1.383197],\r\n                     'V17': [-0.249929, 1.668129, 0.860046, 0.013955, 0.085628, 1.285539, -0.754444, -0.306815, -1.244118, -0.61328, 0.711952, 1.384674, 1.710264, 1.337836, -0.029678, -1.382343, -1.963618, 0.088497, -0.110544, 0.954066],\r\n                     'V18': [0.665032, -1.214589, 0.486172, 1.184611, 1.152936, -0.192168, -1.096281, -0.762198, -0.338583, 0.170551, -0.045797, -0.897271, 0.433204, -0.986375, 0.430157, 1.846751, -0.905146, -1.398763, 1.790667, -1.580808],\r\n                     'V19': [1.347637, -0.356925, 0.414118, 0.277104, 0.41587, -1.237646, 0.580625, 1.468221, -0.254781, 0.245683, -1.25356, 0.241325, 1.15677, -1.74525, 1.970698, -0.038675, -0.314979, 0.114507, 1.378524, -0.139709],\r\n                     'V20': [-1.291686, -1.714475, 0.012188, 1.002238, -1.587334, 1.408967, 1.055095, -1.356865, 1.307388, 0.697003, -0.112676, 1.762375, 0.82697, 1.084934, 1.656421, 0.786079, -1.580991, 1.753751, -0.242525, 1.854008],\r\n                     'Class': [1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1]})\r\n\r\nsetup(data = data,\r\ntarget = 'Class', \r\nexperiment_name = 'xgb_test', \r\nfix_imbalance = True,\r\nlog_experiment = True, \r\nsilent=True, \r\nuse_gpu=True,\r\nfold=5,\r\npreprocess=False)\r\n\r\nmodels = ['xgboost','knn','rf']\r\ntop_models = compare_models(include = model)\r\ndd = pull()\n```\n\n\n### Expected Behavior\n\nArtifacts and metrics should be crated. \n\n### Actual Results\n\n```python-traceback\nError from logs.log:\r\n\r\n2022-08-09 06:11:05,384:ERROR:dashboard_logger.log_model() for XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\r\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\r\n              early_stopping_rounds=None, enable_categorical=False,\r\n              eval_metric=None, feature_types=None, gamma=0, gpu_id=0,\r\n              grow_policy='depthwise', importance_type=None,\r\n              interaction_constraints='', learning_rate=0.300000012,\r\n              max_bin=256, max_cat_to_onehot=4, max_delta_step=0, max_depth=6,\r\n              max_leaves=0, min_child_weight=1, missing=nan,\r\n              monotone_constraints='()', n_estimators=100, n_jobs=-1,\r\n              num_parallel_tree=1, objective='binary:logistic',\r\n              predictor='auto', random_state=989, ...) raised an exception:\r\n2022-08-09 06:11:05,385:ERROR:Traceback (most recent call last):\r\n  File \"\/home\/vscode\/.local\/lib\/python3.8\/site-packages\/pycaret\/internal\/tabular.py\", line 2362, in compare_models\r\n    dashboard_logger.log_model(\r\n  File \"\/home\/vscode\/.local\/lib\/python3.8\/site-packages\/pycaret\/loggers\/__init__.py\", line 93, in log_model\r\n    logger.log_params(params, model_name=full_name)\r\n  File \"\/home\/vscode\/.local\/lib\/python3.8\/site-packages\/pycaret\/loggers\/mlflow_logger.py\", line 46, in log_params\r\n    mlflow.log_params(params)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/tracking\/fluent.py\", line 675, in log_params\r\n    MlflowClient().log_batch(run_id=run_id, metrics=[], params=params_arr, tags=[])\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/tracking\/client.py\", line 918, in log_batch\r\n    self._tracking_client.log_batch(run_id, metrics, params, tags)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 315, in log_batch\r\n    self.store.log_batch(\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 309, in log_batch\r\n    self._call_endpoint(LogBatch, req_body)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 56, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/utils\/rest_utils.py\", line 256, in call_endpoint\r\n    response = verify_rest_response(response, endpoint)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/utils\/rest_utils.py\", line 185, in verify_rest_response\r\n    raise RestException(json.loads(response.text))\r\nmlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Invalid value [{'key': 'objective', 'value': 'binary:logistic'}, {'key': 'use_label_encoder', 'value': 'None'}, {'key': 'base_score', 'value': '0.5'}, {'key': 'booster', 'value': 'gbtree'}, {'key': 'callbacks', 'value': 'None'}, {'key': 'colsample_bylevel', 'value': '1'}, {'key': 'colsample_bynode', 'value': '1'}, {'key': 'colsample_bytree', 'value': '1'}, {'key': 'early_stopping_rounds', 'value': 'None'}, {'key': 'enable_categorical', 'value': 'False'}, {'key': 'eval_metric', 'value': 'None'}, {'key': 'feature_types', 'value': 'None'}, {'key': 'gamma', 'value': '0'}, {'key': 'gpu_id', 'value': '0'}, {'key': 'grow_policy', 'value': 'depthwise'}, {'key': 'importance_type', 'value': 'None'}, {'key': 'interaction_constraints', 'value': ''}, {'key': 'learning_rate', 'value': '0.300000012'}, {'key': 'max_bin', 'value': '256'}, {'key': 'max_cat_to_onehot', 'value': '4'}, {'key': 'max_delta_step', 'value': '0'}, {'key': 'max_depth', 'value': '6'}, {'key': 'max_leaves', 'value': '0'}, {'key': 'min_child_weight', 'value': '1'}, {'key': 'missing', 'value': 'nan'}, {'key': 'monotone_constraints', 'value': '()'}, {'key': 'n_estimators', 'value': '100'}, {'key': 'n_jobs', 'value': '-1'}, {'key': 'num_parallel_tree', 'value': '1'}, {'key': 'predictor', 'value': 'auto'}, {'key': 'random_state', 'value': '989'}, {'key': 'reg_alpha', 'value': '0'}, {'key': 'reg_lambda', 'value': '1'}, {'key': 'sampling_method', 'value': 'uniform'}, {'key': 'scale_pos_weight', 'value': '1'}, {'key': 'subsample', 'value': '1'}, {'key': 'tree_method', 'value': 'gpu_hist'}, {'key': 'validate_parameters', 'value': '1'}, {'key': 'verbosity', 'value': '0'}] for parameter 'params' supplied. Hint: Value was of type 'list'. See the API docs for more information about request parameters.\n```\n\n\n### Installed Versions\n\n<details>\r\npycaret- Version: 2.3.10 <\/br>\r\nmlflow- Version: 1.27.0 <\/br>\r\nxgboost-  Version: 2.0.0.dev0 <\/br>\r\n<\/details>\r\n",
        "Challenge_closed_time":1660286.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660026191000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2838",
        "Challenge_link_count":5,
        "Challenge_participation_count":1,
        "Challenge_readability":8.8,
        "Challenge_reading_time":137.91,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":66,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG]: MLflow server integration",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":925,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"With new mlflow release-1.28.0- and **[Tracking \/ Model Registry] Fix an mlflow server bug that rejected parameters and tags with empty string values (https:\/\/github.com\/mlflow\/mlflow\/pull\/6179, @dbczumar)** bug fixed, the problem no longer occurs and artifacts are saved correctly",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.0,
        "Solution_reading_time":3.6,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":36.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi! During training, my script crashed unexpectedly and did not save the latest epoch information.  I restarted training without being aware of it, and now my epochs are offset by a large number.<\/p>\n<p>Is it possible to edit the epoch number (index) and add a certain value to each entry? I have tried opening the \u201crun_name.wandb\u201d file and I can already see the \u2018_step\u2019 variable for each entry, but I was wondering if there is a cleaner way to perform such an update.<\/p>\n<p>Thank you in advance for your help!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658741360433,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/update-offline-run-before-syncing\/2794",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":6.2,
        "Challenge_reading_time":6.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Update offline run before syncing",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":149.0,
        "Challenge_word_count":94,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/vandrew\">@vandrew<\/a> , I understand what you are attempting to achieve now. At this time our API doesn\u2019t support offline mode to access local log files. We do have this planned as a future feature but I can\u2019t speak to a specific timeline. At this time you will have to sync your runs first in online mode, then update metrics using the API.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":4.63,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":64.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"When only `zn.Method` without `zn.params` is used in a Node the `dvc.yaml` will not depend on the `params.yaml`.\r\n",
        "Challenge_closed_time":1643235.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643228171000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/zincware\/ZnTrack\/issues\/211",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":2.8,
        "Challenge_reading_time":1.95,
        "Challenge_repo_contributor_count":3.0,
        "Challenge_repo_fork_count":4.0,
        "Challenge_repo_issue_count":459.0,
        "Challenge_repo_star_count":32.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"zn.Method does not add params to `dvc.yaml`",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":24,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":7,
        "Challenge_body":"### Contact Details [Optional]\n\nfrancogbocci@gmail.com\n\n### System Information\n\nZenML version: 0.20.5\r\nInstall path: \/Users\/f.bocci\/Library\/Caches\/pypoetry\/virtualenvs\/banana-bMSm4ime-py3.9\/lib\/python3.9\/site-packages\/zenml\r\nPython version: 3.9.6\r\nPlatform information: {'os': 'mac', 'mac_version': '10.15.7'}\r\nEnvironment: native\r\nIntegrations: ['gcp', 'graphviz', 'kubeflow', 'kubernetes', 'scipy', 'sklearn']\n\n### What happened?\n\nTrying to follow the [guide to run a pipeline using Vertex AI](https:\/\/blog.zenml.io\/vertex-ai-blog\/), it fails because ZenML does not now have a `metadata-store` stack category.\r\n\r\n```shell\r\n$ zenml\r\nStack Components:\r\n      alerter                 Commands to interact with alerters.\r\n      annotator               Commands to interact with annotators.\r\n      artifact-store          Commands to interact with artifact stores.\r\n      container-registry      Commands to interact with container registries.\r\n      data-validator          Commands to interact with data validators.\r\n      experiment-tracker      Commands to interact with experiment trackers.\r\n      feature-store           Commands to interact with feature stores.\r\n      model-deployer          Commands to interact with model deployers.\r\n      orchestrator            Commands to interact with orchestrators.\r\n      secrets-manager         Commands to interact with secrets managers.\r\n      step-operator           Commands to interact with step operators.\r\n$ zenml metadata-store\r\nError: No such command 'metadata-store'.\r\n```\n\n### Reproduction steps\n\n1. zenml metadata-store\r\n\r\nIf I don't add it and run the Vertex AI pipeline, it fails.\r\n\n\n### Relevant log output\n\n_No response_\n\n### Code of Conduct\n\n- [X] I agree to follow this project's Code of Conduct",
        "Challenge_closed_time":1667472.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666626693000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/zenml-io\/zenml\/issues\/1001",
        "Challenge_link_count":1,
        "Challenge_participation_count":7,
        "Challenge_readability":11.1,
        "Challenge_reading_time":20.77,
        "Challenge_repo_contributor_count":56.0,
        "Challenge_repo_fork_count":246.0,
        "Challenge_repo_issue_count":1160.0,
        "Challenge_repo_star_count":2571.0,
        "Challenge_repo_watch_count":37.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG]: Vertex AI blogpost is outdated after 0.20.0 release",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":186,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@francobocciDH Thanks for reporting the issue. We have recently undergone a [big architectural shift](https:\/\/blog.zenml.io\/zenml-revamped\/) and therefore a lot of the blog is a bit outdated! In particular, the metadata store is no longer a required stack component.\r\n\r\nIn order to make the vertex orchestrator work, I would suggest either taking a look at the [updated docs page](https:\/\/docs.zenml.io\/component-gallery\/orchestrators\/gcloud-vertexai), or taking a look at the [migration guide](https:\/\/docs.zenml.io\/guidelines\/migration-zero-twenty) that will help you update that blog's code to  the 0.20.5 world. Hey! Thanks for the quick reply. I followed the updated docs page. I checked the post as well to see if there is something different, but following the docs I'm still getting the error\r\n```\r\nMaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=8237): Max retries exceeded with url: \/api\/v1\/login (Caused by \r\nNewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f46e31ea0a0>: Failed to establish a new connection: [Errno 111] Connection refused'))\r\nConnectionError: HTTPConnectionPool(host='127.0.0.1', port=8237): Max retries exceeded with url: \/api\/v1\/login (Caused by \r\nNewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f46e31ea0a0>: Failed to establish a new connection: [Errno 111] Connection refused'))\r\n```\r\n\r\nFor what I saw following the traceback, it's something related to:\r\n`\/usr\/local\/lib\/python3.9\/site-packages\/zenml\/zen_stores\/base_zen_store.py:104`\r\nbut I haven't solved it yet.\r\n\r\nCould you follow the steps on the guide and make it work? I downloaded the image, got into the container and launch the entrypoint being used in Vertex AI:\r\n`python -m zenml.entrypoints.entrypoint --entrypoint_config_source zenml.integrations.gcp.orchestrators.vertex_entrypoint_configuration.VertexEntrypointConfiguration@zenml_0.20.5 --step_name importer --vertex_job_id test1234`\r\n\r\nAnd I got the same error. After that, I ran the ZenML Server (`zenml up`), and I got a different error (so apparently, something's missing?)\r\n\r\nThe error I'm getting now comes from `tfx` package and it's:\r\n```\r\nThe filesystem scheme 'gs:\/\/' is not available for use. For expanded filesystem scheme support, install the `tensorflow` package to enable additional filesystem plugins\r\n```\r\n\r\n I made it work locally. I had to:\r\n1) Register the `artifact-store` using GCS\r\n2) Set it as the artifact-store in the \"default\" stack\r\n3) Start zenml server\r\n\r\nShould this be done in some specific way by the user? @francobocciDH I think the main problem you are suffering from is that you have not deployed ZenML on Google before doing all this. Its our fault as I see that the Vertex orchestrator guide does not make this clear at all (only if you read the docs from the top, it does).\r\n\r\nPlease try [deploying ZenML](https:\/\/docs.zenml.io\/getting-started\/deploying-zenml) to google first. The easiest way to do it is to do:\r\n\r\n```\r\nzenml deploy\r\n```\r\n\r\nAfter you have done this, you can connect to the remote ZenML deployemnt, and re-register your stack as described in the Vertex AI docs, and then run your pipeline. It should work then! @francobocciDH Did this work out? Hey @htahir1 , yes, I deployed it and it worked. It could be clearer in the Vertex AI section of the docs, but it is clearly mentioned in other places of the documentation, so it's my fault for missing this. We can close this from my side. Let me know if there is anything I can help with \ud83d\udc4d ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":10.0,
        "Solution_reading_time":43.38,
        "Solution_score_count":null,
        "Solution_sentence_count":34.0,
        "Solution_word_count":480.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\n~~~\r\nfrom six.moves.collections_abc import Mapping, Sequence \r\nModuleNotFoundError: No module named 'six.moves.collections_abc'\r\n~~~\r\n\r\n**To Reproduce**\r\nRun on @ohsuz 's server.\r\n(Cannot reproduce on Intel i7 based local condition.)\r\n\r\n**Expected behavior**\r\nwandb should be properly imported.\r\n\r\n**Server (please complete the following information):**\r\n - OS: centOS\r\n",
        "Challenge_closed_time":1635333.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634732461000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/wisdomify\/wisdomify\/issues\/89",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":10.5,
        "Challenge_reading_time":5.07,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":9.0,
        "Challenge_repo_issue_count":124.0,
        "Challenge_repo_star_count":94.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"wandb import failure",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":45,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1577919980176,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hamburg, Germany",
        "Answerer_reputation_count":5588.0,
        "Answerer_view_count":398.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm a newbie in Sagemaker and i'm trying to load a pickle dataset into sagemaker notebook.\nI'm using the Python 3 (Data Science) kernel and ml.t3.medium instance.\nEither i load the pickle from S3 or I upload it directly from the studio like this:<\/p>\n<pre><code>import pickle5\nwith open('filename', 'rb') as f:\n    x = pickle.load(f)\n<\/code><\/pre>\n<p><strong>I get this Error:<\/strong><\/p>\n<pre><code>---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n\/opt\/conda\/lib\/python3.7\/site-packages\/IPython\/core\/formatters.py in __call__(self, obj)\n    700                 type_pprinters=self.type_printers,\n    701                 deferred_pprinters=self.deferred_printers)\n--&gt; 702             printer.pretty(obj)\n    703             printer.flush()\n    704             return stream.getvalue()\n\n..................... more errors here\n\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pandas\/core\/generic.py in __getattr__(self, name)\n   5268             or name in self._accessors\n   5269         ):\n-&gt; 5270             return object.__getattribute__(self, name)\n   5271         else:\n   5272             if self._info_axis._can_hold_identifiers_and_holds_name(name):\n\npandas\/_libs\/properties.pyx in pandas._libs.properties.AxisProperty.__get__()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pandas\/core\/generic.py in __getattr__(self, name)\n   5268             or name in self._accessors\n   5269         ):\n-&gt; 5270             return object.__getattribute__(self, name)\n   5271         else:\n   5272             if self._info_axis._can_hold_identifiers_and_holds_name(name):\n\nAttributeError: 'DataFrame' object has no attribute '_data'\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619992853797,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67361483",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":20.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS Sagemaker Studio, cannot load pickle files",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":237.0,
        "Challenge_word_count":141,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1553704286212,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":27.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Can you check your Pandas versions? This error typically occurs when the pickled file was written in an old Pandas version. Your Sagemaker notebook probably runs Pandas &gt; 1.1 where as the Pandas in which the dataframe was pickled is probably &lt; 1.1<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.3,
        "Solution_reading_time":3.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":43.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":7,
        "Challenge_body":"<p>Dear W&amp;B Community,<\/p>\n<p>I have system metrics logged like the \u201c<em>time per step<\/em>\u201d or \u201c<em>time per backward pass<\/em>\u201d for a model.<br>\nWhen doing this on different hardware, I would like to compare the effect this has on these metrics.<br>\nIn the following examples, I profile the basic Torch CIFAR10 model on a 1,2,4,8,16 and 32 CPU VM.<\/p>\n<p>When looking at a <code>Linechart<\/code>, the full history of these metrics is visible, however, it is very hard to compare them due to the overlapping and oscillation:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351.png\" data-download-href=\"\/uploads\/short-url\/zZROm2jlGDN4WUrxx2lQXAA8jYZ.png?dl=1\" title=\"W&amp;amp;B Chart 9_19_2022, 6_22_16 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351_2_690x362.png\" alt=\"W&amp;B Chart 9_19_2022, 6_22_16 PM\" data-base62-sha1=\"zZROm2jlGDN4WUrxx2lQXAA8jYZ\" width=\"690\" height=\"362\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351_2_690x362.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351_2_1035x543.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351_2_1380x724.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">W&amp;amp;B Chart 9_19_2022, 6_22_16 PM<\/span><span class=\"informations\">3539\u00d71859 509 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>When using a <code>Barchart<\/code>, only the last value is visualized:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/48a4597177e867b3eb511112ad23b561f18f1137.png\" data-download-href=\"\/uploads\/short-url\/amCuG3pzRgnimYoyoJeru5muDMH.png?dl=1\" title=\"W&amp;amp;B Chart 9_19_2022, 6_20_31 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/48a4597177e867b3eb511112ad23b561f18f1137_2_690x362.png\" alt=\"W&amp;B Chart 9_19_2022, 6_20_31 PM\" data-base62-sha1=\"amCuG3pzRgnimYoyoJeru5muDMH\" width=\"690\" height=\"362\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/48a4597177e867b3eb511112ad23b561f18f1137_2_690x362.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/48a4597177e867b3eb511112ad23b561f18f1137_2_1035x543.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/48a4597177e867b3eb511112ad23b561f18f1137_2_1380x724.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/48a4597177e867b3eb511112ad23b561f18f1137_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">W&amp;amp;B Chart 9_19_2022, 6_20_31 PM<\/span><span class=\"informations\">3539\u00d71859 251 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>The functionality that would be nice is to group values based on their count or occurrence, as grouping by runs already works perfectly. Here\u2019s the same data but run through <code>seaborn.barplot<\/code>:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479.png\" data-download-href=\"\/uploads\/short-url\/7VTQur5SLq8cPHTQtTqGrDuwPRn.png?dl=1\" title=\"download\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479_2_690x427.png\" alt=\"download\" data-base62-sha1=\"7VTQur5SLq8cPHTQtTqGrDuwPRn\" width=\"690\" height=\"427\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479_2_690x427.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479_2_1035x640.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479_2_1380x854.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">download<\/span><span class=\"informations\">3777\u00d72341 159 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Would this be possible to implement? Or does anybody know a way to get that functionality?<\/p>\n<p>My current workaround is to download the data manually and run it through seaborn. Unfortunately, I did not understand the errors I\u2019ve gotten with the <code>Custom Chart<\/code> functionality when trying to port Vega examples to use wandb as a data basis.<\/p>\n<p>I\u2019d be very glad if anybody can point me to a tutorial on how to migrate existing Vega examples to be used with wandb (and the common problems, like differences between v3\/v4\/v5, as these seemed to be an issue for me).<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663605136917,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/barchart-grouping-by-time-step-count\/3157",
        "Challenge_link_count":18,
        "Challenge_participation_count":7,
        "Challenge_readability":21.3,
        "Challenge_reading_time":80.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":null,
        "Challenge_title":"Barchart Grouping by Time\/Step\/Count",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":803.0,
        "Challenge_word_count":367,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Alexander,<\/p>\n<p>Thanks for sending this detailed explanation! I have been exploring it and I think that the issue here is that, in lines 22, 29 and 43 you have \u201cdata\u201d: \u201ctable\u201d but as the name has been changed to \u201cwandb\u201d, then you should have \u201cdata\u201d: \u201cwandb\u201d. To solve the error between lines 4 and 6, you can use <span class=\"chcklst-box fa fa-square-o fa-fw\"><\/span> and it is solved, but it seems that it is not affecting to the chart.<\/p>\n<pre><code>\"data\": [{ \"name\": \"wandb\" }]\n<\/code><\/pre>\n<p>Please let me know if this would be useful for you!<\/p>\n<p>Best,<br>\nLuis<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":7.17,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":96.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"This may lead to strange behaviour when called in interactive mode in another place thant the kedro project root.",
        "Challenge_closed_time":1602948.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595365457000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/30",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":10.5,
        "Challenge_reading_time":2.66,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"get_mlflow_config use the working directory instead of given path when called within load_context",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":31,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>&quot;Is it possible to save R object in Azure storage (not a trained model) and then use it in a new ML experiment?   <\/p>\n<p>My specific example is:  <br \/>\nRunning PCA with R (&quot;&quot;prcomp&quot;&quot;) and then apply the transformation over new data.   <\/p>\n<p>It can be done within a single R script component with the following code:  <\/p>\n<pre><code>prin_comp &lt;- prcomp(dataset1, scale= TRUE)\ndataset2&lt;-predict(prin_comp, newdata=dataset2)\n<\/code><\/pre>\n<p>But I want to save the new object &quot;&quot;prin_comp&quot;&quot; and call it in a new ML experiment.  <\/p>\n<p>Is it possible?  <\/p>\n<p>(I know there is PCA component in Azure ML but I'm still asking on this approach).&quot;  <\/p>\n<p>[Note: As we migrate from MSDN, this question has been posted by an\u202fAzure Cloud Engineer\u202fas a frequently asked question] Source: <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/7d52578e-1ede-4606-ba3d-267f8f254630\/save-r-objects-in-azure-ml-and-use-them-in-other-experiment?forum=MachineLearning\">MSDN<\/a>  <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1589328890080,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/26562\/save-r-objects-in-azure-ml-and-use-them-in-other-e",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":13.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Save R objects in Azure ML and use them in other experiment?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":141,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Thanks for reaching out. Currently, you cannot save an R object in one experiment and call it directly in another experiment. You need to export the data into your azure storage and import the data into another experiment using the export\/import data modules. In Azure ML (Preview), you can create\/register datasets to use across experiments.    <\/p>\n<p>Hope this helps. Thanks.    <\/p>\n<p>Source: <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/module-reference#data-preparation-modules\">Azure Documentation<\/a>    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.7,
        "Solution_reading_time":7.26,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hello MS team,<\/p>\n<p>I am using Azure devOps pipeline to submit a control script to the Azure-ML workspace. This control script in turn kicks off the Azure-ML pipeline containing pythonscriptsteps and hyperdrive step.<\/p>\n<p><strong>My directory structure:<\/strong><\/p>\n<p>.  <br \/>\n\u251c\u2500\u2500\u2500.vscode  <br \/>\n\u251c\u2500\u2500\u2500Automation  <br \/>\n\u251c\u2500\u2500\u2500Build  <br \/>\n\u2514\u2500\u2500\u2500Source  <br \/>\n\u251c\u2500\u2500\u2500.azureml  <br \/>\n\u251c\u2500\u2500\u2500.vscode  <br \/>\n\u251c\u2500\u2500\u2500amlcode  <br \/>\n\u2502 \u251c\u2500\u2500\u2500projectcode  <br \/>\n\u2502 \u2514\u2500\u2500\u2500<strong>pycache<\/strong>  <br \/>\n\u251c\u2500\u2500\u2500config  <br \/>\n\u251c\u2500\u2500\u2500Data  <br \/>\n\u251c\u2500\u2500\u2500setup  <br \/>\n\u251c\u2500\u2500\u2500tests  <br \/>\n\u2502 \u251c\u2500\u2500\u2500.pytest_cache  <br \/>\n\u2502 \u2502 \u2514\u2500\u2500\u2500v  <br \/>\n\u2502 \u2502 \u2514\u2500\u2500\u2500cache  <br \/>\n\u2502 \u2514\u2500\u2500\u2500<strong>pycache<\/strong>  <br \/>\n\u2514\u2500\u2500\u2500<strong>pycache<\/strong><\/p>\n<p>So here one of the azure cli task in Azure DevOps pipeline uses:<\/p>\n<p><em>az ml folder attach -w $(azureml.workspaceName) -g $(azureml.resourceGroup)<\/em><\/p>\n<p><strong>This command attaches my whole directory to the AML workspace and automatically creates &quot;.amlignore&quot; and &quot;.azureml&quot; is automatically added to that.<\/strong><\/p>\n<p>So it is throwing an authentication error as the <em>config.json()<\/em> is not found because it is generally put in the path <em>\/.azureml<\/em>.<\/p>\n<p>Where to put the config.json() then? What is the best practice?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643414978747,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/714713\/the-azure-cli-command-az-ml-attach-folder-is-direc",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.2,
        "Challenge_reading_time":18.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"The azure cli command \"az ml attach folder\" is directly adding .azureml directory to .amlignore , so where to put config.json when using Azure devops pipeline to submit script to aml workspace?",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":178,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=6755dac2-30f1-48a2-9d0d-4d2c96edc5d4\">@Shivapriya Katta  <\/a> The command az ml folder attach will create the directories and add the config file to .azureml to ensure the workspace resources are easily accessible. You can lookup the note section of the command for <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/reference-azure-machine-learning-cli\">reference<\/a>.    <\/p>\n<blockquote>\n<p>This command creates a .azureml subdirectory that contains example runconfig and conda environment files. It also contains a config.json file that is used to communicate with your Azure Machine Learning workspace.    <\/p>\n<\/blockquote>\n<p>The authentication error in your case could be because <code>az login<\/code> command might have been missed which allows the cli to authenticate interactively or service principal or MI and then run rest of the commands. You can try to run <a href=\"https:\/\/learn.microsoft.com\/en-us\/cli\/azure\/authenticate-azure-cli\">this<\/a> and check if the attach works successfully.     <\/p>\n<p>Also, with the devops pipeline I am not sure if <code>az devops login<\/code>  is required to be run but if the above command fails even after <code>az login<\/code> authentication you can try <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/devops\/cli\/?view=azure-devops\">az devops login<\/a>.    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":11.7,
        "Solution_reading_time":22.31,
        "Solution_score_count":1.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":189.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1467799702463,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bengaluru, Karnataka, India",
        "Answerer_reputation_count":550.0,
        "Answerer_view_count":50.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using AWS Sagemaker and trying to upload a data folder into S3 from Sagemaker. I am trying to do is to upload my data into the s3_train_data directory (the directory exists in S3). However, it wouldn't upload it in that bucket, but in a default Bucket that has been created, and in turn creates a new folder directory with the S3_train_data variables.<\/p>\n\n<p>code to input in directory<\/p>\n\n<pre><code>import os\nimport sagemaker\nfrom sagemaker import get_execution_role\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n\nbucket = &lt;bucket name&gt;\nprefix = &lt;folders1\/folders2&gt;\nkey = &lt;input&gt;\n\n\ns3_train_data = 's3:\/\/{}\/{}\/{}\/'.format(bucket, prefix, key)\n\n\n#path 'data' is the folder in the Jupyter Instance, contains all the training data\ninputs = sagemaker_session.upload_data(path= 'data', key_prefix= s3_train_data)\n<\/code><\/pre>\n\n<p>Is the problem in the code or more in how I created the notebook?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1517943817340,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1525621837032,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48650152",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.2,
        "Challenge_reading_time":12.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS uploading file into wrong bucket",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":722.0,
        "Challenge_word_count":131,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509392519216,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>You could look at the Sample notebooks, how to upload the data S3 bucket \nThere have many ways. I am just giving you hints to answer. \nAnd you forgot create a boto3 session to access the S3 bucket <\/p>\n\n<p><strong>It is one of the ways to do it.<\/strong> <\/p>\n\n<pre><code>import os \nimport urllib.request\nimport boto3\n\ndef download(url):\n    filename = url.split(\"\/\")[-1]\n    if not os.path.exists(filename):\n        urllib.request.urlretrieve(url, filename)\n\n\ndef upload_to_s3(channel, file):\n    s3 = boto3.resource('s3')\n    data = open(file, \"rb\")\n    key = channel + '\/' + file\n    s3.Bucket(bucket).put_object(Key=key, Body=data)\n\n\n# caltech-256\ndownload('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-train.rec')\nupload_to_s3('train', 'caltech-256-60-train.rec')\ndownload('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-val.rec')\nupload_to_s3('validation', 'caltech-256-60-val.rec')\n<\/code><\/pre>\n\n<p>link : <a href=\"https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-fulltraining.ipynb\" rel=\"nofollow noreferrer\">https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-fulltraining.ipynb<\/a><\/p>\n\n<p><strong>Another way to do it.<\/strong> <\/p>\n\n<pre><code>bucket = '&lt;your_s3_bucket_name_here&gt;'# enter your s3 bucket where you will copy data and model artifacts\nprefix = 'sagemaker\/breast_cancer_prediction' # place to upload training files within the bucket\n# do some processing then prepare to push the data. \n\nf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(f, train_X.astype('float32'), train_y.astype('float32'))\nf.seek(0)\n\nboto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', train_file)).upload_fileobj(f)\n<\/code><\/pre>\n\n<p>Link : <a href=\"https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_applying_machine_learning\/breast_cancer_prediction\/Breast%20Cancer%20Prediction.ipynb\" rel=\"nofollow noreferrer\">https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_applying_machine_learning\/breast_cancer_prediction\/Breast%20Cancer%20Prediction.ipynb<\/a><\/p>\n\n<p>Youtube link : <a href=\"https:\/\/www.youtube.com\/watch?v=-YiHPIGyFGo\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=-YiHPIGyFGo<\/a> - how to pull the data in S3 bucket.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":8.0,
        "Solution_readability":23.9,
        "Solution_reading_time":33.63,
        "Solution_score_count":1.0,
        "Solution_sentence_count":21.0,
        "Solution_word_count":158.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1597346132176,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<h2>Issue<\/h2>\n<p>I am trying prepare and then submit a new experiment to Azure Machine Learning from an Azure Function in Python. I therefore register a new dataset for my Azure ML workspace, which contains the training data for my ML model using <code>dataset.register(...<\/code>. However, when I try to create this dataset with the following line of code<\/p>\n<pre><code>dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths)\n<\/code><\/pre>\n<p>then I get a <code>Failure Exception: OSError: [Errno 30] Read-only file system ...<\/code>.<\/p>\n<h2>Ideas<\/h2>\n<ol>\n<li>I know that I shouldn't write to the file system from within an Azure function if possible. But I actually don't want to write anything to the local file system. I only want to create the dataset as a reference to my blob storage under <code>datastore_path<\/code> and then register this to my Azure Machine Learning workspace. But it seems that the method <code>from_delimited_files<\/code> is trying to write to the file system anyway (maybe some caching?).<\/li>\n<li>I also know that there is a temp folder in which writing temporary files is permitted. However, I belive I cannot really control where this method is writing data. I already tried changing the current working directory to this temp folder just before the function call using <code>os.chdir(tempfile.gettempdir())<\/code>, but that didn't help.<\/li>\n<\/ol>\n<p>Any other ideas? I don't think I am doing something particularly unusually...<\/p>\n<h2>Details<\/h2>\n<p>I am using python 3.7 and azureml-sdk 1.9.0 and I can run the python script locally without problems. I currently deploy from VSCode using the Azure Functions extension version 0.23.0 (and an Azure DevOps pipeline for CI\/CD).<\/p>\n<p>Here is my full stack trace:<\/p>\n<pre><code>Microsoft.Azure.WebJobs.Host.FunctionInvocationException: Exception while executing function: Functions.HttpTrigger_Train\n ---&gt; Microsoft.Azure.WebJobs.Script.Workers.Rpc.RpcException: Result: Failure\nException: OSError: [Errno 30] Read-only file system: '\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/bin\/deps.lock'\nStack:   File &quot;\/azure-functions-host\/workers\/python\/3.7\/LINUX\/X64\/azure_functions_worker\/dispatcher.py&quot;, line 345, in _handle__invocation_request\n    self.__run_sync_func, invocation_id, fi.func, args)\n  File &quot;\/usr\/local\/lib\/python3.7\/concurrent\/futures\/thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;\/azure-functions-host\/workers\/python\/3.7\/LINUX\/X64\/azure_functions_worker\/dispatcher.py&quot;, line 480, in __run_sync_func\n    return func(**params)\n  File &quot;\/home\/site\/wwwroot\/HttpTrigger_Train\/__init__.py&quot;, line 11, in main\n    train()\n  File &quot;\/home\/site\/wwwroot\/shared_code\/train.py&quot;, line 70, in train\n    dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/data\/_loggerfactory.py&quot;, line 126, in wrapper\n    return func(*args, **kwargs)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/data\/dataset_factory.py&quot;, line 308, in from_delimited_files\n    quoting=support_multi_line)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/readers.py&quot;, line 100, in read_csv\n    df = Dataflow._path_to_get_files_block(path, archive_options)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/dataflow.py&quot;, line 2387, in _path_to_get_files_block\n    return datastore_to_dataflow(path)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_datastore_helper.py&quot;, line 41, in datastore_to_dataflow\n    datastore, datastore_value = get_datastore_value(source)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_datastore_helper.py&quot;, line 83, in get_datastore_value\n    _set_auth_type(workspace)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_datastore_helper.py&quot;, line 134, in _set_auth_type\n    get_engine_api().set_aml_auth(SetAmlAuthMessageArgument(AuthType.SERVICEPRINCIPAL, json.dumps(auth)))\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/api.py&quot;, line 18, in get_engine_api\n    _engine_api = EngineAPI()\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/api.py&quot;, line 55, in __init__\n    self._message_channel = launch_engine()\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/engine.py&quot;, line 300, in launch_engine\n    dependencies_path = runtime.ensure_dependencies()\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/runtime.py&quot;, line 141, in ensure_dependencies\n    with _FileLock(deps_lock_path, raise_on_timeout=timeout_exception):\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/runtime.py&quot;, line 113, in __enter__\n    self.acquire()\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/runtime.py&quot;, line 72, in acquire\n    self.lockfile = os.open(self.lockfile_path, os.O_CREAT | os.O_EXCL | os.O_RDWR)\n\n   at Microsoft.Azure.WebJobs.Script.Description.WorkerFunctionInvoker.InvokeCore(Object[] parameters, FunctionInvocationContext context) in \/src\/azure-functions-host\/src\/WebJobs.Script\/Description\/Workers\/WorkerFunctionInvoker.cs:line 85\n   at Microsoft.Azure.WebJobs.Script.Description.FunctionInvokerBase.Invoke(Object[] parameters) in \/src\/azure-functions-host\/src\/WebJobs.Script\/Description\/FunctionInvokerBase.cs:line 85\n   at Microsoft.Azure.WebJobs.Script.Description.FunctionGenerator.Coerce[T](Task`1 src) in \/src\/azure-functions-host\/src\/WebJobs.Script\/Description\/FunctionGenerator.cs:line 225\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionInvoker`2.InvokeAsync(Object instance, Object[] arguments) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionInvoker.cs:line 52\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.InvokeAsync(IFunctionInvoker invoker, ParameterHelper parameterHelper, CancellationTokenSource timeoutTokenSource, CancellationTokenSource functionCancellationTokenSource, Boolean throwOnTimeout, TimeSpan timerInterval, IFunctionInstance instance) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 587\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithWatchersAsync(IFunctionInstanceEx instance, ParameterHelper parameterHelper, ILogger logger, CancellationTokenSource functionCancellationTokenSource) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 532\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithLoggingAsync(IFunctionInstanceEx instance, ParameterHelper parameterHelper, IFunctionOutputDefinition outputDefinition, ILogger logger, CancellationTokenSource functionCancellationTokenSource) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 470\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithLoggingAsync(IFunctionInstanceEx instance, FunctionStartedMessage message, FunctionInstanceLogEntry instanceLogEntry, ParameterHelper parameterHelper, ILogger logger, CancellationToken cancellationToken) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 278\n   --- End of inner exception stack trace ---\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithLoggingAsync(IFunctionInstanceEx instance, FunctionStartedMessage message, FunctionInstanceLogEntry instanceLogEntry, ParameterHelper parameterHelper, ILogger logger, CancellationToken cancellationToken) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 325\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.TryExecuteAsyncCore(IFunctionInstanceEx functionInstance, CancellationToken cancellationToken) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 117\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597357236433,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1613407306920,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63403985",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":25.2,
        "Challenge_reading_time":114.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":73,
        "Challenge_solved_time":null,
        "Challenge_title":"\"Failure Exception: OSError: [Errno 30] Read-only file system\" when using AzureML in Python Azure Function",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1092.0,
        "Challenge_word_count":563,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1597346132176,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":61.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>The issue was an incompatible OS version in my virtual environment.<\/p>\n<p>A huge thanks goes to <a href=\"https:\/\/docs.microsoft.com\/answers\/users\/111253\/pramodvalavala-msft.html\" rel=\"nofollow noreferrer\">PramodValavala-MSFT<\/a> for his idea to create a docker container! Following his suggestion, I suddenly got the following error message for the  <code>dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths)<\/code> command:<\/p>\n<blockquote>\n<p>Exception: NotImplementedError: Unsupported Linux distribution debian 10.<\/p>\n<\/blockquote>\n<p>which reminded me of the following warning in the azure machine learning documentation:<\/p>\n<blockquote>\n<p>Some dataset classes have dependencies on the azureml-dataprep\npackage, which is only compatible with 64-bit Python. For Linux users,\nthese classes are supported only on the following distributions: Red\nHat Enterprise Linux (7, 8), Ubuntu (14.04, 16.04, 18.04), Fedora (27,\n28), Debian (8, 9), and CentOS (7).<\/p>\n<\/blockquote>\n<p>Choosing the predefined docker image <code>2.0-python3.7<\/code> (running Debian 9) instead of  <code>3.0-python3.7<\/code> (running Debian 10) solved the issue (see <a href=\"https:\/\/hub.docker.com\/_\/microsoft-azure-functions-python\" rel=\"nofollow noreferrer\">https:\/\/hub.docker.com\/_\/microsoft-azure-functions-python<\/a>).<\/p>\n<p>I suspect that the default virtual environment, which I was using originally, also ran on an incompatible OS.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.9,
        "Solution_reading_time":18.86,
        "Solution_score_count":3.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":156.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>for a project we are doing azure ml assisted labeling (object detection). we also want to check the model for false positives\/negatives, by receiving the results of the camera and performing manual labeling on it. then we want to take the results from the check and put those in the original dataset that we created using azure labeling. is there a built in function for this or do we need to create this ourselfs?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1676376282820,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1180554\/feedback-loop-azure-ml-possibilities",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.4,
        "Challenge_reading_time":5.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"feedback loop azure ml possibilities?",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":78,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=6ab37d97-2a03-49e5-8bb2-90417150ab68\">Hamza Outa<\/a> Are you looking to add new labels after some feedback for your systems? <\/p>\n<p>I think there is an option to add new labels to a project by pausing it and then you have the following options:<\/p>\n<ul>\n<li> Start over, removing all existing labels. Choose this option if you want to start labeling from the beginning with the new full set of labels.<\/li>\n<li> Start over, keeping all existing labels. Choose this option to mark all data as unlabeled, but keep the existing labels as a default tag for images that were previously labeled.<\/li>\n<li> Continue, keeping all existing labels. Choose this option to keep all data already labeled as is, and start using the new label for data not yet labeled.<\/li>\n<\/ul>\n<p>Is this what you are looking for? I think the second option might work for you to add new labels if the default tags need to be changed. As per <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-image-labeling-projects#add-new-labels-to-a-project\">documentation <\/a>you can start using the new labels for labeling and the ML assisted labeling will start after a certain threshold is reached or you can manually start an ML assisted training run. I hope this helps!!<\/p>\n",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.6,
        "Solution_reading_time":16.69,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":195.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":7,
        "Challenge_body":"<p>I have a guild operation <code>main<\/code> that runs 3 steps which are other operations: <code>impute<\/code>, <code>evaluate<\/code>, and <code>predict<\/code>. The latter two require on the <code>impute<\/code> operation (specifically a model checkpoint and some data output).<br>\n(<a href=\"https:\/\/github.com\/davzaman\/autopopulus\/blob\/dev\/guild.yml\" rel=\"noopener nofollow ugc\">guild.yml<\/a> file for reference)<\/p>\n<ol>\n<li>When one of the steps fails (e.g. <code>evaluate<\/code>), the <code>main<\/code> op shows error and so does <code>evaluate<\/code>. If I fix the error in the code and restart the run with something like <code>for hash in $(guild select --operation evaluate --error --all); do guild run -y --background --restart $hash --force-sourcecode; done<\/code>,  then the <code>evaluate<\/code> op fixes to completed, but the <code>main<\/code> operation does not. It doesn\u2019t seem very possible to update it, but it is slightly unclean and annoying to keep track of what broke and what is fixed. I end up with something like:<\/li>\n<\/ol>\n<pre><code class=\"lang-plaintext\">[71:ec03c916]   evaluate  2023-02-20 14:43:57  completed  dvae myexperiment\n[72:957ecb30]   evaluate  2023-02-20 14:43:56  completed  dvae myexperiment\n[73:19493e6b]   evaluate  2023-02-20 14:43:56  completed  dvae myexperiment\n...\n[127:fe72a7ff]  predict   2023-02-18 20:58:56  completed  dvae \n[128:617bc8fd]  impute    2023-02-18 20:26:16  completed  dvae myexperiment\n[129:2b155ff0]  main      2023-02-18 20:26:14  error      dvae \n[130:39125144]  predict   2023-02-18 20:21:08  completed  dvae \n[131:5c4ed46a]  impute    2023-02-18 19:45:25  completed  dvae myexperiment\n[132:c542fcbe]  main      2023-02-18 19:45:24  error      dvae \n<\/code><\/pre>\n<p>It said <code>error<\/code> for <code>main<\/code> but it\u2019s really been fixed sine the <code>evaluate<\/code> op was fixed.<br>\nAnother issue is also what files are stored under each op which leads me to the next point, where ill use <code>run 132<\/code> as an example:<\/p>\n<ol start=\"2\">\n<li>If I look at what is stored under the <code>main<\/code> op I see:<\/li>\n<\/ol>\n<pre><code class=\"lang-shell\">me@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/c542fcbe24ab4a86b1ea0e33fabd839a$ ls\nevaluate  impute  options.yml  predict\n<\/code><\/pre>\n<p>If I drill into the directories I see:<\/p>\n<pre><code class=\"lang-plaintext\">me@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/c542fcbe24ab4a86b1ea0e33fabd839a$ cd evaluate\nme@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/c542fcbe24ab4a86b1ea0e33fabd839a\/evaluate$ ls\nF.O.  options.yml  serialized_models\n<\/code><\/pre>\n<p>If <code>evaluate<\/code> fails and I rerun it, does that mean that the <code>evaluate<\/code>folder will be updated too (is it a symlink)? There seems to be some redundancy too which leads me to:<\/p>\n<ol start=\"3\">\n<li>If I look at the output of the substeps <code>impute<\/code> and <code>predict<\/code> I see:<\/li>\n<\/ol>\n<pre><code class=\"lang-plaintext\"># impute op\nme@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/5c4ed46a12e145158b2351621ee81345\/serialized_models$ ls\nAEDitto_STATIC.pt  imputed_data.pkl  STATIC_test_dataloader.pt\n# predict op\nme@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/391251446fe041048d93d80deda6ac8a\/serialized_models$ ls\nAEDitto_STATIC.pt  imputed_data.pkl  STATIC_test_dataloader.pt\n<\/code><\/pre>\n<p>I also see<\/p>\n<pre><code class=\"lang-plaintext\"># impute op\nme@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/5c4ed46a12e145158b2351621ee81345$ ls F.O.\/0.33\/MNAR\\(G\\)\/dvae\/lightning_logs\/version_0\/\nevents.out.tfevents.1676780435.lambda2.6521.0\nevents.out.tfevents.1676780445.lambda2.6521.1\nevents.out.tfevents.1676780453.lambda2.6521.2\nhparams.yaml\n# predict op\nme@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/391251446fe041048d93d80deda6ac8a$ ls F.O.\/0.33\/MNAR\\(G\\)\/dvae\/lightning_logs\/version_0\/\nevents.out.tfevents.1676780435.lambda2.6521.0\nevents.out.tfevents.1676780445.lambda2.6521.1\nevents.out.tfevents.1676780453.lambda2.6521.2\nhparams.yaml\n<\/code><\/pre>\n<p>It looks like it copies over everything from the <code>impute<\/code> op top the parents: <code>main<\/code>, and dependent steps: <code>predict<\/code>, and <code>evaluate<\/code>. This is a lot of redundancy especially for expensive\/large models and artifacts. This is making me run out of space on my machine.<\/p>\n<p>My questions are<br>\na) How do I avoid redundancy in stored artifacts between parent and child steps like <code>main<\/code> having substeps.<br>\nb) How do I avoid redundancy amongst sibling runs where one may be dependent on another? While <code>evaluate<\/code> relies on the artifacts from <code>impute<\/code> I don\u2019t want it to store all the artifacts all over again (including the model checkpoints, data, and the logging files), I just want <code>evaluate<\/code> to use the checkpointed data and model. <a href=\"https:\/\/my.guild.ai\/t\/guild-file-cheatsheet\/192#required-operation-files-14\">I know there\u2019s a <code>select:<\/code> option<\/a> but it seems to be regex, making it complicated to select the checkpointed model AND data. Also even if that solves excluding the logged files, I don\u2019t want to copy over the files it relies on to the final logged artifacts.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1676938696104,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/my.guild.ai\/t\/confusion-on-multistep-operations-restarting-substeps-and-copied-files\/998",
        "Challenge_link_count":2,
        "Challenge_participation_count":7,
        "Challenge_readability":13.3,
        "Challenge_reading_time":67.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":null,
        "Challenge_title":"Confusion on multistep operations, restarting substeps, and copied files?",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":128.0,
        "Challenge_word_count":550,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a class=\"mention\" href=\"\/u\/davzaman\">@davzaman<\/a> It looks like there was a regression and Guild is indeed <em>copying<\/em> resolved operation dependency files. This is not the intended behavior and we\u2019ll fix that ASAP.<\/p>\n<p>As a workaround, avoid copying files by adding <code>target-type<\/code> to your dependency def like this:<\/p>\n<pre><code class=\"lang-yaml\">upstream: {}\n\ndownstream:\n  requires:\n    - operation: upstream\n      target-type: link  # tells Guild to link to the resolved files, not copy\n<\/code><\/pre>\n<p>The <code>downstream<\/code> operation is any operation that requires an upstream run.<\/p>\n<p>Sorry about that! This will make a big difference in disk space for you. We\u2019ll post here when the fix is applied, after which you can remove the explicit <code>target-type<\/code> in your dependencies.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.5,
        "Solution_reading_time":10.34,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":108.0,
        "Tool":"Guild AI"
    },
    {
        "Answerer_created_time":1611841996688,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":21.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've found an almost identical question <a href=\"https:\/\/stackoverflow.com\/questions\/63727235\/mlflow-artifacts-storing-artifactsgoogle-cloud-storage-but-not-displaying-them?newreg=923da08a362547daab64c7d7e2275423\">here<\/a> but don't have enough reputation to add comments so will ask again hoping that someone has found a solution in the mean time.<\/p>\n<p>I am using MLflow (1.13.1) to track model performance and GCP Storage to store model artifacts.\nMLflow is running on a GCP VM instance and my python application uses a service account with Storage Object Creator and Storage Object Viewer roles (and then I've also added storage.buckets.get permissions) to store artifacts in GCP buckets and read from them.\nEverything is working as expected with parameters and metrics correctly displaying in MLflow UI and model artifacts correctly stored in buckets. The problem is that the model artifacts do not show up in MLflow UI because of this error:<\/p>\n<pre><code>Unable to list artifacts stored under gs:\/******\/artifacts for the current run. \nPlease contact your tracking server administrator to notify them of this error, \nwhich can happen when the tracking server lacks permission to list artifacts under the current run's root artifact directory.\n<\/code><\/pre>\n<p>The quoted artifacts location exists and contains the correct model artifacts, and MLflow should be able to read the artifacts because of the Storage Object Viewer role and the storage.buckets.get permissions.<\/p>\n<p>Any suggestion on what could be wrong? Thank you.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611843895217,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65939058",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.7,
        "Challenge_reading_time":20.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"MLflow stores artifacts on GCP buckets but is not able to read them",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":428.0,
        "Challenge_word_count":223,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1611841996688,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":21.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>I've found the problem just after posting the question.\nI had forgotten to install the <code>google-cloud-storage<\/code> library on the GCP VM. Everything works as expected now.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.9,
        "Solution_reading_time":2.34,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":26.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1386279656143,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":1352.0,
        "Answerer_view_count":229.0,
        "Challenge_adjusted_solved_time":1.7925027778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Suppose we got some database (any database, that supports csv dumping), collecting raw data in real time for further usage in ML.\nOn the other side, we got DVC, that can work with csv files.<\/p>\n<p>I want to organize a scheduled run of stored SELECT to that DB with datetime parameters (and also support a manual run), to make a new csv files, and send them to DVC.<\/p>\n<p>In DVC documentation and examples I found, csv file already exists.<\/p>\n<p>Can I make this interaction with database with DVC itself, or I got something wrong, and there is a separate tool for csv dump?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619078487020,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67209146",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.9,
        "Challenge_reading_time":7.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"DVC - make scheduled csv dumps",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":65.0,
        "Challenge_word_count":107,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1580932981007,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>There are 3 steps in this process:<\/p>\n<ol>\n<li>Create a CSV dump. Many DBs have these tools but DVC does not support this natively.<\/li>\n<li>Version the CSV dump and move it to some storage. DVC does this job.<\/li>\n<li>Schedule periodical dump. You can use Cron (easy), AirFlow (not easy) or <a href=\"https:\/\/docs.github.com\/en\/actions\/reference\/events-that-trigger-workflows\" rel=\"nofollow noreferrer\">periodical jobs in GitHub Actions<\/a>\/<a href=\"https:\/\/docs.gitlab.com\/ee\/ci\/pipelines\/schedules.html\" rel=\"nofollow noreferrer\">GitLab CI\/CD<\/a>. Another project from the DVC team can help with CI\/CD option: <a href=\"https:\/\/cml.dev\" rel=\"nofollow noreferrer\">https:\/\/cml.dev<\/a>.<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1619084940030,
        "Solution_link_count":4.0,
        "Solution_readability":9.9,
        "Solution_reading_time":9.14,
        "Solution_score_count":4.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":78.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1464391892936,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Rio de Janeiro, State of Rio de Janeiro, Brazil",
        "Answerer_reputation_count":2243.0,
        "Answerer_view_count":148.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I use SageMaker for research purpose in my study, hope somebody can help me out.\nError I get\nClientError: lst should at least has three parts, but only has 1 parts for '1 0 class_iphone6splus\/i6 (1).jpg'<\/p>\n\n<p>It is possible to create my own training job using the SageMaker GUI only?\nCause I'm totally new to AWS...\nThe built in algorithm that I wan to use is image classification. <\/p>\n\n<p>I have 400 images in JPG format for the datasets. Those images are from two different phone models which is iPhone 6s plus and iPhone7plus so that the system will classify them into two different classes. Both 200 each.<\/p>\n\n<p>S3 bucket\nIn train folder, I have two different folder to store those images which are class_iphone6splus and \nclass_iphone7plus 200 each for one class. The .lst file which are created by own use notepad++ name as data.lst are put in these two folder with the images cause I not sure where to put it.\nWhile in validation folder, I also store the same 400 images into another class_iphone6splus and \nclass_iphone7plus folder seperately by their class.<\/p>\n\n<p>Things store in .lst file<br>\nExample altogether is 400lines<br>\n1 0 class_iphone6splus\/i6 (1).jpg<br>\nuntil<br>\n200 0 class_iphone6splus\/i6 (200).jpg  <\/p>\n\n<p>201 1 class_iphone7plus\/i7 (1).jpg<br>\nuntil<br>\n400 1 class_iphone7plus\/i7 (200).jpg  <\/p>\n\n<p>Should I create two different folder in the bucket to store the .lst file which are train_lst folder &amp; validation_lst folder. These two folders also should contains 400 images?  <\/p>\n\n<p>Resource configuration:<br>\nInstance type: ml.p2.xlarge<br>\nInstance count: 1<br>\nAdditional storage: 5GB  <\/p>\n\n<p>Hyperparameters:<br>\nnum_classes:2<br>\nnum_training_samples:400<br>\nothers parameters used default value by system.  <\/p>\n\n<p>Input data configuration:<br>\nI set 4 channels which are:  <\/p>\n\n<p>1) train\nS3 location: s3:\/\/datasets-for-testing\/train<\/p>\n\n<p>2) validation\nS3 location: s3:\/\/datasets-for-testing\/validation<\/p>\n\n<p>3) train_lst\nS3 location: s3:\/\/datasets-for-testing\/train<\/p>\n\n<p>4) validation_lst\nS3 location: s3:\/\/datasets-for-testing\/validation<\/p>\n\n<p>Input mode:file\nContent type: application\/jpeg or use application\/x-image will be better<\/p>\n\n<p>S3 output path\ns3:\/\/datasets-for-testing\/output<\/p>\n\n<p>These are all the configurations I choose before click on the 'Create training job'.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1564115966253,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1564126209647,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57213293",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.8,
        "Challenge_reading_time":30.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":null,
        "Challenge_title":"ClientError: lst should at least has three parts, but only has 1 parts for",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":239.0,
        "Challenge_word_count":337,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1564115013776,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>I create a training job that you have specified and had got the same error. To resolve the error <strong>ClientError: lst should at least has three parts, but only has 1 parts for<\/strong>, make sure that the file <strong>.lst<\/strong> is well-formatted with tab-separated like this:<\/p>\n\n<pre><code>5      1   iphone\/iphone7_1.jpg\n1000   0   iphone\/iphone6_1.jpg\n22     1   iphone\/iphone7_2.jpg\n<\/code><\/pre>\n\n<p>I used  <code>nano<\/code> on <strong>MAC OS X<\/strong> to validate the tab-separated format.<\/p>",
        "Solution_comment_count":9.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":6.27,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have a DataSet defined in my AzureML workspace that is linked to an Azure Blob Storage csv file of 1.6Gb.  This file contains timeseries information of around 10000 devices.  So, I could've also created 10000 smaller files (since I use ADF for the transmission pipeline).<\/p>\n\n<p>My question now is: is it possible to load a part of the AzureML DataSet in my python notebook or script instead of loading the entire file?<br>\nThe only code I have now load the full file:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>dataset = Dataset.get_by_name(workspace, name='devicetelemetry')\ndf = dataset.to_pandas_dataframe()\n<\/code><\/pre>\n\n<p>The only concept of partitions I found with regards to the AzureML datasets was around time series and partitioning of timestamps &amp; dates.  However, here I would love to partition per device, so I can very easily just do a load of all telemetry of a specific device.<\/p>\n\n<p>Any pointers to docs or any suggestions? (I couldn't find any so far)<\/p>\n\n<p>Thanks already<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1585756434767,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60975078",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":6.9,
        "Challenge_reading_time":13.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"How to only load one portion of an AzureML tabular dataset (linked to Azure Blob Storage)",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":560.0,
        "Challenge_word_count":169,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1360655430743,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belgium",
        "Poster_reputation_count":2947.0,
        "Poster_view_count":355.0,
        "Solution_body":"<p>You're right there are the <code>.time_*()<\/code> filtering methods available with a <code>TabularDataset<\/code>.<\/p>\n\n<p>I'm not aware of anyway to do filtering as you suggest (but I agree it would be a useful feature). To get per-device partitioning, my recommendation would be to structure your container like so:<\/p>\n\n<pre><code>- device1\n    - 2020\n        - 2020-03-31.csv\n        - 2020-04-01.csv\n- device2\n   - 2020\n        - 2020-03-31.csv\n        - 2020-04-01.csv\n<\/code><\/pre>\n\n<p>In this way you can define an all-up Dataset, but also per-device Datasets by passing folder of the device to the DataPath<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># all up dataset\nds_all = Dataset.Tabular.from_delimited_files(\n    path=DataPath(datastore, '*')\n)\n# device 1 dataset\nds_d1 = Dataset.Tabular.from_delimited_files(\n    path=DataPath(datastore, 'device1\/*')\n)\n<\/code><\/pre>\n\n<p><strong>CAVEAT<\/strong><\/p>\n\n<p>dataprep SDK is optimized for blobs around 200MB in size. So you can work with many small files, but sometimes it can be slower than expected, especially considering the overhead of enumerating all blobs in a container.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.4,
        "Solution_reading_time":14.03,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":133.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1582574311347,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have pretty big CSV that would not fit into memory, and I need to convert it into .parquet file to work with vaex.<\/p>\n\n<p>Here is my catalog:<\/p>\n\n<pre><code>raw_data:\n    type: kedro.contrib.io.pyspark.SparkDataSet\n    filepath: data\/01_raw\/data.csv\n    file_format: csv\n\nparquet_data:\n    type: ParquetLocalDataSet\n    filepath: data\/02_intermediate\/data.parquet\n<\/code><\/pre>\n\n<p>node:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def convert_to_parquet(data: SparkDataSet) -&gt; ParquetLocalDataSet:\n    return data.coalesce(1)\n<\/code><\/pre>\n\n<p>and a pipeline:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def create_pipeline(**kwargs):\n    return Pipeline(\n        [\n            node(\n                func=convert_to_parquet,\n                inputs=\"raw_data\",\n                outputs=\"parquet_data\",\n                name=\"data_to_parquet\",\n            ),\n        ]\n    )\n<\/code><\/pre>\n\n<p>But if I do <code>kedro run<\/code> I receive this error <code>kedro.io.core.DataSetError: Failed while saving data to data set ParquetLocalDataSet(engine=auto, filepath=data\/02_intermediate\/data.parquet, save_args={}).\n'DataFrame' object has no attribute 'to_parquet'<\/code><\/p>\n\n<p>What should I fix to get my dataset converted?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582572566660,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1583421031536,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60382704",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":15.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Convert csv into parquet in kedro",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":498.0,
        "Challenge_word_count":108,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324477592580,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1315.0,
        "Poster_view_count":91.0,
        "Solution_body":"<p>You could try the following. This has worked for me in the past.<\/p>\n\n<pre><code>parquet_data:\n    type: kedro.contrib.io.pyspark.SparkDataSet\n    file_format: 'parquet'\n    filepath: data\/02_intermediate\/data.parquet\n    save_args:\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.9,
        "Solution_reading_time":3.1,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":22.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1443426419048,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation_count":842.0,
        "Answerer_view_count":219.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>a friend have sent me a python3 notebook with his dataset to validate his notebook.<\/p>\n\n<p>but when i try to use his dataset on my azureml workspace i have an error saying that the dataset does not exist<\/p>\n\n<p>he sent me his datset code :<\/p>\n\n<pre><code>from azureml import Workspace\n\nws = Workspace(\n    workspace_id='toto',\n    authorization_token='titi',\n    endpoint='https:\/\/studioapi.azureml.net'\n)\nds = ws.datasets['mini.csv00']\nframe = ds.to_dataframe()\n\nframe\n<\/code><\/pre>\n\n<p>when i try to use it i have a :<\/p>\n\n<pre><code>ndexError                                Traceback (most recent call last)\n&lt;ipython-input-7-5f41120e38e4&gt; in &lt;module&gt;()\n----&gt; 1 ds = ws.datasets['mini.csv00']\n      2 frame = ds.to_dataframe()\n      3 \n      4 frame\n\n\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/azureml\/__init__.py in __getitem__(self, index)\n    461                     return self._create_dataset(dataset)\n    462 \n--&gt; 463         raise IndexError('A data set named \"{}\" does not exist'.format(index))\n    464 \n    465     def add_from_dataframe(self, dataframe, data_type_id, name, description):\n\nIndexError: A data set named \"mini.csv00\" does not exist\n<\/code><\/pre>\n\n<p>error ...<\/p>\n\n<p>But when i try it on my computer jupyter it works.\nAny ideas ?<\/p>\n\n<p>Thanks and regards<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1486668512167,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42145256",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":7.4,
        "Challenge_reading_time":16.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Share dataset between two azureml environnement",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":108.0,
        "Challenge_word_count":149,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1280999248528,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1048.0,
        "Poster_view_count":602.0,
        "Solution_body":"<p>I guess you are using Jupyter notebook on AzureML to do the experiment. In that case the <code>'mini.csv00'<\/code> should be in your experiments with <code>workspace_id='toto'<\/code>. <\/p>\n\n<p>Create a new experiment in your workspace named toto and put the dataset into it first. Then open the dataset using 'open in a new Notebook'. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ztIw0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ztIw0.png\" alt=\"enter image description here\"><\/a> <\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.6,
        "Solution_reading_time":6.55,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1431018627572,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":86.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a jupyter notebook in SageMaker in which I want to run the XGBoost algorithm. The data has to match 3 criteria: \n-No header row\n-Outcome variable in the first column, features in the rest of the columns \n-All columns need to be numeric<\/p>\n\n<p>The error I get is the following:<\/p>\n\n<pre><code>    Error for Training job xgboost-2019-03-13-16-21-25-000: \n    Failed Reason: ClientError: Blankspace and colon not found in firstline \n'0.0,0.0,99.0,314.07,1.0,0.0,0.0,0.0,0.48027846,0.0...' of file 'train.csv'\n<\/code><\/pre>\n\n<p>In the error itself it can be seen that there are no headers, the output is the first column (it just takes 1.0 and 0.0 values) and all features are numerical. The data is stored in its own bucket. <\/p>\n\n<p>I have seen a related question in GitHub but there are no solution there. Also, the example notebook that Amazon has does not take care of change the default sep or anything when saving a dataframe to csv for using it later on. <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1552497791307,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55147861",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":12.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Blankspace and colon not found in firstline",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":915.0,
        "Challenge_word_count":163,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1523298968403,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1754.0,
        "Poster_view_count":197.0,
        "Solution_body":"<p>The error message indicated XGBoost was expecting the input data set as libsvm format instead of csv. SageMaker XGBoost by default assumed the input data set was in libsvm format. For using input data set in csv, please explicitly specify <code>content-type<\/code> as <code>text\/csv<\/code>.<\/p>\n\n<p>For more information: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#InputOutput-XGBoost\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#InputOutput-XGBoost<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.8,
        "Solution_reading_time":7.04,
        "Solution_score_count":4.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":50.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1448964835883,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Z\u00fcrich, Schweiz",
        "Answerer_reputation_count":269.0,
        "Answerer_view_count":42.0,
        "Challenge_adjusted_solved_time":0.3934,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying a machine learning model and logging metrics using mlflow. But I am getting <code>TypeError: 'numpy.float32' object is not iterable<\/code>. I have tried using <code>.tolist()<\/code> and <code>dict()<\/code> but nothing seems to work.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def train(max_epochs, model, optimizer, scheduler, train_loader, valid_loader, project_name):\n    best_val_loss = 100\n    for epoch in range(max_epochs):\n        model.train()\n        running_loss = []\n        tq_loader = tqdm(train_loader)\n        o = {}\n        for samples in tq_loader:\n            optimizer.zero_grad()\n            outputs, interaction_map = model(\n                [samples[0].to(device), samples[1].to(device), torch.tensor(samples[2]).to(device),\n                 torch.tensor(samples[3]).to(device)])\n            l1_norm = torch.norm(interaction_map, p=2) * 1e-4\n            loss = loss_fn(outputs, torch.tensor(samples[4]).to(device).float()) + l1_norm\n            loss.backward()\n            optimizer.step()\n            loss = loss - l1_norm\n            running_loss.append(loss.cpu().detach())\n            tq_loader.set_description(\n                &quot;Epoch: &quot; + str(epoch + 1) + &quot;  Training loss: &quot; + str(np.mean(np.array(running_loss))))\n        model.eval()\n        val_loss, mae_loss = get_metrics(model, valid_loader)\n        scheduler.step(val_loss)\n        \n        #metrics mlflow\n        mlflow.log_metrics('train_loss',(np.mean(np.array(running_loss))).tolist())\n        mlflow.log_metrics('validation_loss',(val_loss).tolist())\n        mlflow.log_metrics('MAE Val_loss', (mae_loss).tolist())\n\n        print(&quot;Epoch: &quot; + str(epoch + 1) + &quot;  train_loss &quot; + str(np.mean(np.array(running_loss))) + &quot; Val_loss &quot; + str(\n            val_loss) + &quot; MAE Val_loss &quot; + str(mae_loss))\n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), &quot;.\/runs\/run-&quot; + str(project_name) + &quot;\/models\/best_model.tar&quot;)\n\nmlflow.set_experiment('CIGIN_V2')\nmlflow.start_run(nested=True)\ntrain(max_epochs, model, optimizer, scheduler, train_loader, valid_loader, project_name)\nmlflow.end_run()\n<\/code><\/pre>\n<p>Error<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>Epoch: 1  Training loss: 6770.575: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:04&lt;00:00,  4.35s\/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:03&lt;00:00,  3.86s\/it]\n\n---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\n&lt;ipython-input-96-8c3a6eb822c3&gt; in &lt;module&gt;()\n      1 mlflow.set_experiment('CIGIN_V2')\n      2 mlflow.start_run(nested=True)\n----&gt; 3 train(max_epochs, model, optimizer, scheduler, train_loader, valid_loader, project_name)\n      4 mlflow.end_run()\n\n&lt;ipython-input-95-ab0a6c80b65b&gt; in train(max_epochs, model, optimizer, scheduler, train_loader, valid_loader, project_name)\n     55 \n     56         #metrics mlflow\n---&gt; 57         mlflow.log_metrics('train_loss',dict(np.mean(np.array(running_loss))).tolist())\n     58         mlflow.log_metrics('validation_loss',dict(val_loss).tolist())\n     59         mlflow.log_metrics('MAE Val_loss', dict(mae_loss).tolist())\n\nTypeError: 'numpy.float32' object is not iterable\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1653902310097,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1653902478312,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72431938",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.0,
        "Challenge_reading_time":40.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":null,
        "Challenge_title":"TypeError: 'numpy.float32' object is not iterable when logging in mlflow",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":51.0,
        "Challenge_word_count":221,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651898762636,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":41.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>Youre logging a single value into log_metrics and i dont think thats correct based on the implementation of log_metric and log_metrics in the documentation:<\/p>\n<p><a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_metric\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_metric<\/a> and\n<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_metrics\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_metrics<\/a><\/p>\n<p>So i would suggest to maybe change the &quot;log_metrics&quot; to &quot;log_metric&quot; and leave the tolist out<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1653903894552,
        "Solution_link_count":4.0,
        "Solution_readability":16.6,
        "Solution_reading_time":9.3,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":49.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1331657670247,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3932.0,
        "Answerer_view_count":274.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>From a segmentation mask, I am trying to retrieve what labels are being represented in the mask. <\/p>\n\n<p>This is the image I am running through a semantic segmentation model in AWS Sagemaker.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/XbMMP.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XbMMP.png\" alt=\"Motorbike and everything else background\"><\/a><\/p>\n\n<p>Code for making prediction and displaying mask.<\/p>\n\n<pre><code>from sagemaker.predictor import json_serializer, json_deserializer, RealTimePredictor\nfrom sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n\n%%time\nss_predict = sagemaker.RealTimePredictor(endpoint=ss_model.endpoint_name, \n                                     sagemaker_session=sess,\n                                    content_type = 'image\/jpeg',\n                                    accept = 'image\/png')\n\nreturn_img = ss_predict.predict(img)\n\nfrom PIL import Image\nimport numpy as np\nimport io\n\nnum_labels = 21\nmask = np.array(Image.open(io.BytesIO(return_img)))\nplt.imshow(mask, vmin=0, vmax=num_labels-1, cmap='jet')\nplt.show()\n<\/code><\/pre>\n\n<p>This image is the segmentation mask that was created and it represents the motorbike and everything else is the background.<\/p>\n\n<p>[<img src=\"https:\/\/i.stack.imgur.com\/6FbVn.png\" alt=\"Segmented mask[2]\"><\/p>\n\n<p>As you can see from the code there are 21 possible labels and 2 were used in the mask, one for the motorbike and another for the background. What I would like to figure out now is how to print which labels were actually used in this mask out of the 21 possible options?<\/p>\n\n<p>Please let me know if you need any further information and any help is much appreciated. <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590644898463,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62057838",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":21.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"How to retrieve the labels used in a segmentation mask in AWS Sagemaker",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":489.0,
        "Challenge_word_count":197,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1449513251820,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":693.0,
        "Poster_view_count":56.0,
        "Solution_body":"<p>Somewhere you should have a mapping from label integers to label classes, e.g.<\/p>\n\n<pre><code>label_map = {0: 'background', 1: 'motorbike', 2: 'train', ...}\n<\/code><\/pre>\n\n<p>If you are using the Pascal VOC dataset, that would be (1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle, 6=bus, 7=car , 8=cat, 9=chair, 10=cow, 11=diningtable, 12=dog, 13=horse, 14=motorbike, 15=person, 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv\/monitor) - see here: <a href=\"http:\/\/host.robots.ox.ac.uk\/pascal\/VOC\/voc2012\/segexamples\/index.html\" rel=\"nofollow noreferrer\">http:\/\/host.robots.ox.ac.uk\/pascal\/VOC\/voc2012\/segexamples\/index.html<\/a><\/p>\n\n<p>Then you can simply use that map:<\/p>\n\n<pre><code>used_classes = np.unique(mask)\nfor cls in used_classes:\n    print(\"Found class: {}\".format(label_map[cls]))\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.8,
        "Solution_reading_time":10.68,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":76.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1250158552416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>How do I replace the values in a specific column with a particular value based on a condition in Azure ML Studio. I can do this using pandas in python as foolows:<\/p>\n\n<pre><code>df.loc[df['col_name'] &gt; 1990, 'col_name'] = 1\n<\/code><\/pre>\n\n<p>I'm trying to find a Module in Azure Machine Learning Studio that does the equivalent of this. <\/p>\n\n<p>I understand there is a replace option under the ConverToDataset module and a Replace Discrete Values module. But neither of these seems to do what I want. Is there an option to replace the values in just one column to a specific value based on a condition?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1557229871357,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56021977",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":8.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Replace values in a column based on a condition in Azure ML Studio",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":564.0,
        "Challenge_word_count":115,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1450260166772,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1587.0,
        "Poster_view_count":540.0,
        "Solution_body":"<p>You can use either the more general <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/apply-sql-transformation\" rel=\"nofollow noreferrer\">Apply SQL Transformation<\/a>, or the dedicated <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/clip-values\" rel=\"nofollow noreferrer\">Clip Values<\/a> module. If all else fails, there's also <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/execute-python-script\" rel=\"nofollow noreferrer\">Execute Python Script<\/a>.<\/p>\n\n<p>Personally, for your example I'd use <code>Clip Values<\/code> with <code>Clip Peaks<\/code> and <code>Upper Threshold<\/code> set. For more complex rules I'd use either <code>Apply SQL Transformation<\/code> or <code>Execute Python Script<\/code>, depending on the rules but favouring SQL :).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":17.8,
        "Solution_reading_time":11.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":71.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1551701850312,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Poland, Cracow",
        "Answerer_reputation_count":11273.0,
        "Answerer_view_count":1888.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to create a python utility that will take dataset from vertex ai datasets and will generate statistics for that dataset. But I am unable to check the dataset using jupyter notebook. Is there any way out for this?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1630410227747,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68998065",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":4.4,
        "Challenge_reading_time":3.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Read vertex ai datasets in jupyter notebook",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":737.0,
        "Challenge_word_count":47,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1616070829583,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":45.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>If I understand correctly, you want to use <a href=\"https:\/\/cloud.google.com\/vertex-ai\" rel=\"nofollow noreferrer\">Vertex AI<\/a> dataset inside <code>Jupyter Notebook<\/code>. I don't think that this is currently possible. You are able to export <code>Vertex AI<\/code> datasets to <code>Google Cloud Storage<\/code> in JSONL format:<\/p>\n<blockquote>\n<p>Your dataset will be exported as a list of text items in JSONL format. Each row contains a Cloud Storage path, any label(s) assigned to that item, and a flag that indicates whether that item is in the training, validation, or test set.<\/p>\n<\/blockquote>\n<p>At this moment, you can use <code>BigQuery<\/code> data inside <code>Notebook<\/code> using <code>%%bigquery<\/code> like it's mentioned in <a href=\"https:\/\/cloud.google.com\/bigquery\/docs\/visualize-jupyter\" rel=\"nofollow noreferrer\">Visualizing BigQuery data in a Jupyter notebook.<\/a> or use <code>csv_read()<\/code> from machine directory or <code>GCS<\/code> like it's showed in the <a href=\"https:\/\/stackoverflow.com\/questions\/61956470\/\">How to read csv file in Google Cloud Platform jupyter notebook<\/a> thread.<\/p>\n<p>However, you can fill a <code>Feature Request<\/code> in <a href=\"https:\/\/developers.google.com\/issue-tracker\" rel=\"nofollow noreferrer\">Google Issue Tracker<\/a> to add the possibility to use <code>VertexAI<\/code> dataset directly in the <code>Jupyter Notebook<\/code> which will be considered by the <code>Google Vertex AI Team<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.8,
        "Solution_reading_time":19.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":174.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am getting the following error when I try to convert a datetime variable to date.<\/p>\n\n<p><strong>My Code<\/strong><\/p>\n\n<pre><code>import datetime as dt \n\ndf['TXN_DATE_2'] = df['TXN_DATE'].dt.date\n<\/code><\/pre>\n\n<p><strong>Error<\/strong><\/p>\n\n<blockquote>\n  <p>raise NotImplementedError('Python Bridge conversion table not\n  implemented for type [{0}]'.format(value.getType()))\n  NotImplementedError: Python Bridge conversion table not implemented\n  for type [] Process returned with non-zero exit\n  code 1<\/p>\n<\/blockquote>\n\n<p>Can anyone please tell me what is going on.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1499274119590,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1499281298240,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44932098",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":8.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML- Execute Python Script -Datatime.date not working",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":449.0,
        "Challenge_word_count":70,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1499273894443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>Please try to use the code below to convert as you want.<\/p>\n\n<pre><code>import pandas as pd\nimport datetime as dt\ndf['TXN_DATE_2'] = pd.to_datetime(df['TXN_DATE']).dt.date\n<\/code><\/pre>\n\n<p>Hope it helps.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.7,
        "Solution_reading_time":2.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":26.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I\u2019ve been wanting to export the data on GPU usage for my algorithm, but when I export the CSV file there is a single line which does not contain all the data.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b17a445f7fc35f370a78c8a4d9ea242ef5797b42.png\" data-download-href=\"\/uploads\/short-url\/pk2t25q25HlYQzj2LOJ1W7xb1e2.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b17a445f7fc35f370a78c8a4d9ea242ef5797b42.png\" alt=\"image\" data-base62-sha1=\"pk2t25q25HlYQzj2LOJ1W7xb1e2\" width=\"690\" height=\"35\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b17a445f7fc35f370a78c8a4d9ea242ef5797b42_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1893\u00d797 7.21 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>For some reason all other data exports work, but any data which has to do with the GPU does not. I have 4 GPUs. The plot shows the right data:<\/p>\n<p>I could also not find the complete data using the API. Is this a bug?<\/p>\n<p>Best,<\/p>\n<p>Mario<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649321943188,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/exporting-gpu-utilization-power-usage-data\/2194",
        "Challenge_link_count":3,
        "Challenge_participation_count":3,
        "Challenge_readability":14.5,
        "Challenge_reading_time":19.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Exporting GPU utilization, power usage data",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":641.0,
        "Challenge_word_count":118,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/meerio\">@meerio<\/a>,<\/p>\n<p>You should be able to retrieve your System Metrics history from the run using the following line of code:<\/p>\n<pre><code class=\"lang-python\">metrics = run.history(stream='events')\n<\/code><\/pre>\n<p>where <code>run<\/code> is a <code>Run<\/code> object accessed through the API. This should allow you to access all your system metrics data. Please let me know if this does not work for you or if you need any further assistance.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":6.57,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":68.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":9,
        "Challenge_body":"<p>When uploading offline runs, there is no config shown in the dashboard.<br>\nAdditionally, there are also no system plots.<\/p>\n<p>Is there any way to fix this issue?<\/p>\n<p>Thank you very much for your help!<br>\nCedric<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651135719417,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/no-config-file-and-system-plots-for-offline-runs\/2337",
        "Challenge_link_count":0,
        "Challenge_participation_count":9,
        "Challenge_readability":5.5,
        "Challenge_reading_time":3.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"No config file and system plots for offline runs",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":745.0,
        "Challenge_word_count":43,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/nathank\">@nathank<\/a>,<\/p>\n<p>sorry for my late reply and  thank you very much for your help. <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><br>\nWe\u2019gve managed to get the config displayed in the dashboard.<\/p>\n<p>Unfortunately, we specified the wrong path for the upload, i.e., the experiment folder.<br>\nThe config is uploaded and displayed correctly when we specify offline run folder.<\/p>\n<p>Example<br>\nold : wandb sync \u2026\/experiment_name<br>\nnew : wandb sync \u2026\/experiment_name\/wandb\/offline-run-20220515_002356-jdlxek9r<\/p>\n<p>Apparently, now, we get a new error:<\/p>\n<p>.wandb: ERROR Metric data exceeds maximum size of 10.4MB (12.4MB)<br>\nwandb: ERROR Summary data exceeds maximum size of 10.4MB. Dropping it.<br>\ndone.<\/p>\n<p>However, the configuration is displayed correctly on the website.<br>\nThanks again for your help and if we can\u2019t get the new error under control, we\u2019ll  ask in a new issue.<\/p>\n<p>Bests,<br>\nCedric and Jannis<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.4,
        "Solution_reading_time":14.19,
        "Solution_score_count":null,
        "Solution_sentence_count":11.0,
        "Solution_word_count":133.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Currently I am working on a SageMaker notebook instance and trying to change my working directory to an AWS S3 bucket. I am using the following code:<\/p>\n\n<pre><code>os.chdir('s3:\/\/bucket-name')\n<\/code><\/pre>\n\n<p>The error generated says: <code>FileNotFoundError: [Errno 2] No such file or directory: 's3:\/\/bucket-name'<\/code> but I used the below code to upload a CSV file and it works:<\/p>\n\n<pre><code>import boto3\nimport pandas as pd\nfrom sagemaker import get_execution_role\n\nrole = get_execution_role()\nbucket='bucket-name'\ndata_key = 'some_file.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\n\ndf = pd.read_csv(data_location)\n<\/code><\/pre>\n\n<p>How can I change the working directory to an S3 bucket?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1563641337633,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1563642953048,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57126765",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.1,
        "Challenge_reading_time":9.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Changing the working directory to a S3 Bucket on AWS",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2377.0,
        "Challenge_word_count":97,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1541972092476,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Santa Clara, CA, USA",
        "Poster_reputation_count":731.0,
        "Poster_view_count":51.0,
        "Solution_body":"<p>S3 is not a file system and you can't just change directory to it. Many of the libraries such as Pandas can read and write directly from S3, but it requires specific libraries to make it work. <\/p>\n\n<p>The simplest option is to copy the files from S3 to the local drive (EBS or EFS) of the notebook instance:<\/p>\n\n<pre><code>aws s3 cp s3:\/\/bucket_name\/some_file.csv data\/\n<\/code><\/pre>\n\n<p>The AWS CLI is already installed on the notebook instance, and if you gave the right IAM permission when you launched your notebook instance, then the copy command should work.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.2,
        "Solution_reading_time":6.99,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":95.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1526368885180,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Munich, Germany",
        "Answerer_reputation_count":3944.0,
        "Answerer_view_count":217.0,
        "Challenge_adjusted_solved_time":0.0123147222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I feel like this should be possible, but I looked through the wandb SDK code and I can't find an easy\/logical way to do it. It <em>might<\/em> be possible to hack it by modifying the manifest entries at some point later (but maybe before the artifact is logged to wandb as then the manifest and the entries might be locked)? I saw things like this in the SDK code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>version = manifest_entry.extra.get(&quot;versionID&quot;)\netag = manifest_entry.extra.get(&quot;etag&quot;)\n<\/code><\/pre>\n<p>So, I figure we can probably edit those?<\/p>\n<h2>UPDATE<\/h2>\n<p>So, I tried to hack it together with something like this and it works but it feels wrong:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport wandb\nimport boto3\nfrom wandb.util import md5_file\n\nENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nPROJECT = os.environ.get(&quot;WANDB_PROJECT&quot;)\nAPI_KEY = os.environ.get(&quot;WANDB_API_KEY&quot;)\n\napi = api = wandb.Api(overrides={&quot;entity&quot;: ENTITY, &quot;project&quot;: ENTITY})\nrun = wandb.init(entity=ENTITY, project=PROJECT, job_type=&quot;test upload&quot;)\nfile = &quot;admin2Codes.txt&quot;  # &quot;admin1CodesASCII.txt&quot; # (both already on s3 with a couple versions)\nartifact = wandb.Artifact(&quot;test_data&quot;, type=&quot;dataset&quot;)\n\n# modify one of the local files so it has a new md5hash etc.\nwith open(file, &quot;a&quot;) as f:\n    f.write(&quot;new_line_1\\n&quot;)\n\n# upload local file to s3\nlocal_file_path = file\ns3_url = f&quot;s3:\/\/bucket\/prefix\/{file}&quot;\ns3_url_arr = s3_url.replace(&quot;s3:\/\/&quot;, &quot;&quot;).split(&quot;\/&quot;)\ns3_bucket = s3_url_arr[0]\nkey = &quot;\/&quot;.join(s3_url_arr[1:])\n\ns3_client = boto3.client(&quot;s3&quot;)\nfile_digest = md5_file(local_file_path)\ns3_client.upload_file(\n    local_file_path,\n    s3_bucket,\n    key,\n    # save the md5_digest in metadata,\n    # can be used later to only upload new files to s3,\n    # as AWS doesn't digest the file consistently in the E-tag\n    ExtraArgs={&quot;Metadata&quot;: {&quot;md5_digest&quot;: file_digest}},\n)\nhead_response = s3_client.head_object(Bucket=s3_bucket, Key=key)\nversion_id: str = head_response[&quot;VersionId&quot;]\nprint(version_id)\n\n# upload a link\/ref to this s3 object in wandb:\nartifact.add_reference(s3_dir)\n# at this point we might be able to modify the artifact._manifest.entries and each entry.extra.get(&quot;etag&quot;) etc.?\nprint([(name, entry.extra) for name, entry in artifact._manifest.entries.items()])\n# set these to an older version on s3 that we know we want (rather than latest) - do this via wandb public API:\ndataset_v2 = api.artifact(f&quot;{ENTITY}\/{PROJECT}\/test_data:v2&quot;, type=&quot;dataset&quot;)\n# artifact._manifest.add_entry(dataset_v2.manifest.entries[&quot;admin1CodesASCII.txt&quot;])\nartifact._manifest.entries[&quot;admin1CodesASCII.txt&quot;] = dataset_v2.manifest.entries[\n    &quot;admin1CodesASCII.txt&quot;\n]\n# verify that it did change:\nprint([(name, entry.extra) for name, entry in artifact._manifest.entries.items()])\n\nrun.log_artifact(artifact)  # at this point the manifest is locked I believe?\nartifact.wait()  # wait for upload to finish (blocking - but should be very quick given it is just an s3 link)\nprint(artifact.name)\nrun_id = run.id\nrun.finish()\ncurr_run = api.run(f&quot;{ENTITY}\/{PROJECT}\/{run_id}&quot;)\nused_artifacts = curr_run.used_artifacts()\nlogged_artifacts = curr_run.logged_artifacts()\n<\/code><\/pre>\n<p>Am I on the right track here? I guess the other workaround is to make a copy on s3 (so that older version is the latest again) but I wanted to avoid this as the 1 file that I want to use an old version of is a large NLP model and the only files I want to change are small config.json files etc. (so seems very wasteful to upload all files again).<\/p>\n<p>I was also wondering if when I copy an old version of an object back into the same key in the bucket if that creates a real copy or just like a pointer to the same underlying object. Neither boto3 nor AWS documentation makes that clear - although it seems like it is a proper copy.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641691804600,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1643119769043,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70637798",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":54.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":null,
        "Challenge_title":"wandb: artifact.add_reference() option to add specific (not current) versionId or ETag to stop the need for re-upload to s3?",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":121.0,
        "Challenge_word_count":484,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1526368885180,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Munich, Germany",
        "Poster_reputation_count":3944.0,
        "Poster_view_count":217.0,
        "Solution_body":"<p>I think I found the correct way to do it now:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport wandb\nimport boto3\nfrom wandb.util import md5_file\n\nENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nPROJECT = os.environ.get(&quot;WANDB_PROJECT&quot;)\n\n\ndef wandb_update_only_some_files_in_artifact(\n    existing_artifact_name: str,\n    new_s3_file_urls: list[str],\n    entity: str = ENTITY,\n    project: str = PROJECT,\n) -&gt; Artifact:\n    &quot;&quot;&quot;If you want to just update a config.json file for example,\n    but the rest of the artifact can remain the same, then you can\n    use this functions like so:\n    wandb_update_only_some_files_in_artifact(\n        &quot;old_artifact:v3&quot;,\n        [&quot;s3:\/\/bucket\/prefix\/config.json&quot;],\n    )\n    and then all the other files like model.bin will be the same as in v3,\n    even if there was a v4 or v5 in between (as the v3 VersionIds are used)\n\n    Args:\n        existing_artifact_name (str): name with version like &quot;old_artifact:v3&quot;\n        new_s3_file_urls (list[str]): files that should be updated\n        entity (str, optional): wandb entity. Defaults to ENTITY.\n        project (str, optional): wandb project. Defaults to PROJECT.\n\n    Returns:\n        Artifact: the new artifact object\n    &quot;&quot;&quot;\n    api = wandb.Api(overrides={&quot;entity&quot;: entity, &quot;project&quot;: project})\n    old_artifact = api.artifact(existing_artifact_name)\n    old_artifact_name = re.sub(r&quot;:v\\d+$&quot;, &quot;&quot;, old_artifact.name)\n    with wandb.init(entity=entity, project=project) as run:\n        new_artifact = wandb.Artifact(old_artifact_name, type=old_artifact.type)\n\n        s3_file_names = [s3_url.split(&quot;\/&quot;)[-1] for s3_url in new_s3_file_urls]\n        # add the new ones:\n        for s3_url, filename in zip(new_s3_file_urls, s3_file_names):\n            new_artifact.add_reference(s3_url, filename)\n        # add the old ones:\n        for filename, entry in old_artifact.manifest.entries.items():\n            if filename in s3_file_names:\n                continue\n            new_artifact.add_reference(entry, filename)\n            # this also works but feels hackier:\n            # new_artifact._manifest.entries[filename] = entry\n\n        run.log_artifact(new_artifact)\n        new_artifact.wait()  # wait for upload to finish (blocking - but should be very quick given it is just an s3 link)\n        print(new_artifact.name)\n        print(run.id)\n    return new_artifact\n\n\n# usage:\nlocal_file_path = &quot;config.json&quot; # modified file\ns3_url = &quot;s3:\/\/bucket\/prefix\/config.json&quot;\ns3_url_arr = s3_url.replace(&quot;s3:\/\/&quot;, &quot;&quot;).split(&quot;\/&quot;)\ns3_bucket = s3_url_arr[0]\nkey = &quot;\/&quot;.join(s3_url_arr[1:])\n\ns3_client = boto3.client(&quot;s3&quot;)\nfile_digest = md5_file(local_file_path)\ns3_client.upload_file(\n    local_file_path,\n    s3_bucket,\n    key,\n    # save the md5_digest in metadata,\n    # can be used later to only upload new files to s3,\n    # as AWS doesn't digest the file consistently in the E-tag\n    ExtraArgs={&quot;Metadata&quot;: {&quot;md5_digest&quot;: file_digest}},\n)\n\nwandb_update_only_some_files_in_artifact(\n    &quot;old_artifact:v3&quot;,\n    [&quot;s3:\/\/bucket\/prefix\/config.json&quot;],\n)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1643119813376,
        "Solution_link_count":0.0,
        "Solution_readability":14.0,
        "Solution_reading_time":38.99,
        "Solution_score_count":0.0,
        "Solution_sentence_count":30.0,
        "Solution_word_count":282.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1383611307000,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York",
        "Answerer_reputation_count":10846.0,
        "Answerer_view_count":984.0,
        "Challenge_adjusted_solved_time":10.0108433334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I <code>dvc add<\/code>-ed a file I did not mean to add. I have not yet committed.<\/p>\n\n<p>How do I undo this operation? In Git, you would do <code>git rm --cached &lt;filename&gt;<\/code>.<\/p>\n\n<p>To be clear: I want to make DVC forget about the file, and I want the file to remain untouched in my working tree. This is the opposite of what <code>dvc remove<\/code> does.<\/p>\n\n<p>One <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1524\" rel=\"nofollow noreferrer\">issue<\/a> on the DVC issue tracker suggests that <code>dvc unprotect<\/code> is the right command. But reading the <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/unprotect\" rel=\"nofollow noreferrer\">manual page<\/a> suggests otherwise.<\/p>\n\n<p>Is this possible with DVC?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568689927047,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57966851",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":8.9,
        "Challenge_reading_time":9.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Undo 'dvc add' operation",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1304.0,
        "Challenge_word_count":100,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1383611307000,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"New York",
        "Poster_reputation_count":10846.0,
        "Poster_view_count":984.0,
        "Solution_body":"<p>As per mroutis on the DVC Discord server:<\/p>\n\n<ol>\n<li><code>dvc unprotect<\/code> the file; this won't be necessary if you don't use <code>symlink<\/code> or <code>hardlink<\/code> caching, but it can't hurt.<\/li>\n<li>Remove the .dvc file<\/li>\n<li>If you need to delete the cache entry itself, run <code>dvc gc<\/code>, or look up the MD5 in <code>data.dvc<\/code> and manually remove it from <code>.dvc\/cache<\/code>.<\/li>\n<\/ol>\n\n<p><em>Edit<\/em> -- there is now an issue on their Github page to add this to the manual: <a href=\"https:\/\/github.com\/iterative\/dvc.org\/issues\/625\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc.org\/issues\/625<\/a><\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1568725966083,
        "Solution_link_count":2.0,
        "Solution_readability":9.6,
        "Solution_reading_time":8.49,
        "Solution_score_count":7.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":79.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a question about <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-register-datasets#tabulardataset\">the source of TabularDataset on Azure Machine Learnigng<\/a>.    <\/p>\n<p>Can I use compressed data saved Azure Data Lake Storage Gen2 like below on TablarDataset without expansion?    <\/p>\n<ul>\n<li> csv with bzip2(.bz2)    <\/li>\n<li> parquet with gzip(gz)    <\/li>\n<li> parquet with snappy    <\/li>\n<\/ul>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638234150553,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/645118\/can-i-use-compressed-data-on-tabulardataset",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.1,
        "Challenge_reading_time":6.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Can I use compressed data on TabularDataset?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":56,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, tabular dataset does not support compressed files. You'll need to extract the data as shown <a href=\"https:\/\/medium.com\/mlearning-ai\/load-json-gz-files-to-azure-ml-dataset-b7039ec9da34\">here<\/a> for example before creating a tabular dataset. However, file dataset supports any format.<\/p>\n<hr \/>\n<p>--- *Kindly <em><strong>Accept Answer<\/strong><\/em> if the information helps. Thanks.*<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.0,
        "Solution_reading_time":5.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":41.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1317676233236,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, United States",
        "Answerer_reputation_count":2348.0,
        "Answerer_view_count":206.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created an S3 bucket 'testshivaproject' and uploaded an image in it. When I try to access it in sagemaker notebook, it throws an error 'No such file or directory'.<\/p>\n\n<pre><code># import libraries\nimport boto3, re, sys, math, json, os, sagemaker, urllib.request\nfrom sagemaker import get_execution_role\nimport numpy as np                                   \n\n# Define IAM role\nrole = get_execution_role()\n\nmy_region = boto3.session.Session().region_name # set the region of the instance\n\nprint(\"success :\"+my_region)\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> success :us-east-2<\/p>\n\n<pre><code>role\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> 'arn:aws:iam::847047967498:role\/service-role\/AmazonSageMaker-ExecutionRole-20190825T121483'<\/p>\n\n<pre><code>bucket = 'testprojectshiva2' \ndata_key = 'ext_image6.jpg' \ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key) \nprint(data_location)\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> s3:\/\/testprojectshiva2\/ext_image6.jpg<\/p>\n\n<pre><code>test = load_img(data_location)\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> No such file or directory<\/p>\n\n<p>There are similar questions raised (<a href=\"https:\/\/stackoverflow.com\/questions\/48264656\/load-s3-data-into-aws-sagemaker-notebook\">Load S3 Data into AWS SageMaker Notebook<\/a>) but did not find any solution?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1566834582570,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57661142",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":15.1,
        "Challenge_reading_time":17.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS S3 and Sagemaker: No such file or directory",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":4622.0,
        "Challenge_word_count":121,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442731325232,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hyderabad",
        "Poster_reputation_count":187.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>Thanks for using Amazon SageMaker!<\/p>\n\n<p>I sort of guessed from your description, but are you trying to use the Keras load_img function to load images directly from your S3 bucket?<\/p>\n\n<p>Unfortunately, <a href=\"https:\/\/github.com\/keras-team\/keras\/issues\/11684\" rel=\"nofollow noreferrer\">the load_img function is designed to only load files from disk<\/a>, so passing an s3:\/\/ URL to that function will always return a <code>FileNotFoundError<\/code>.<\/p>\n\n<p>It's common to first download images from S3 before using them, so you can use boto3 or the AWS CLI to download the file before calling load_img.<\/p>\n\n<p><strong>Alternatively<\/strong>, since the load_img function simply creates a <a href=\"https:\/\/en.wikipedia.org\/wiki\/Python_Imaging_Library\" rel=\"nofollow noreferrer\">PIL Image<\/a> object, you can create the PIL object directly from the data in S3 using boto3, and not use the load_img function at all.<\/p>\n\n<p>In other words, you could do something like this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from PIL import Image\n\ns3 = boto3.client('s3')\ntest = Image.open(BytesIO(\n    s3.get_object(Bucket=bucket, Key=data_key)['Body'].read()\n    ))\n<\/code><\/pre>\n\n<p>Hope this helps you out in your project!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.4,
        "Solution_reading_time":15.67,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":151.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created a toy example in Azure.\nI have the following dataset:<\/p>\n\n<pre><code>  amounts       city code user_id\n1    2.95 Colleferro  100     999\n2    2.95    Subiaco  100     111\n3   14.95   Avellino  101     333\n4   14.95 Colleferro  101     999\n5   14.95  Benevento  101     444\n6  -14.95    Subiaco  110     111\n7  -14.95   Sgurgola  110     555\n8  -14.95       Roma  110     666\n9  -14.95 Colleferro  110     999\n<\/code><\/pre>\n\n<p>I create an AzureML experiment that simply plots the column of the amounts.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/TgRTW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TgRTW.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The code into the R script module is the following:<\/p>\n\n<pre><code>data.set &lt;- maml.mapInputPort(1) # class: data.frame  \n#-------------------\nplot(data.set$amounts);\ntitle(\"This title is a very long title. That is not a problem for R, but it becomes a problem when Azure manages it in the visualization.\")\n#-------------------\nmaml.mapOutputPort(\"data.set\");\n<\/code><\/pre>\n\n<p>Now, if you click on the right output port of the R script and then on \"Visualize\"<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/pTkSH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/pTkSH.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>you will see the Azure page where the outputs are shown.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Iv5GM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Iv5GM.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Now, the following happens:<\/p>\n\n<ol>\n<li>The plot is <em>stucked<\/em> into an estabilished space (example: the title is cut!!!)<\/li>\n<li>The image produced is a <em>low resolution<\/em> one.<\/li>\n<li>The JSON produced by Azure is \"dirty\" (making the <em>decoding<\/em> in C# difficult).<\/li>\n<\/ol>\n\n<p>It seems that this is not the best way to get the images produced by the AzureML experiment. <\/p>\n\n<p>Possible solution: I would like <\/p>\n\n<blockquote>\n  <p>to send the picture produced in my experiment to a space like the blob\n  storage. <\/p>\n<\/blockquote>\n\n<p>This would be also a great solution when I have a web-app and I have to pick the image produced by Azure and put it on my Web App page.\nDo you know if there is a way to send the image somewhere?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1469435784157,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1469500135808,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38563051",
        "Challenge_link_count":6,
        "Challenge_participation_count":2,
        "Challenge_readability":7.2,
        "Challenge_reading_time":28.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":null,
        "Challenge_title":"Getting the images produced by AzureML experiments back",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":593.0,
        "Challenge_word_count":309,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436432728608,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Colleferro, Italy",
        "Poster_reputation_count":809.0,
        "Poster_view_count":361.0,
        "Solution_body":"<p>To saving the images into Azure Blob Storage with R, you need to do two steps, which include getting the images from the R device output of <code>Execute R Script<\/code> and uploading the images to Blob Storage.<\/p>\n\n<p>There are two ways to implement the steps above.<\/p>\n\n<ol>\n<li><p>You can publish the experiment as a webservice, then get the images with base64 encoding from the response of the webservice request and use Azure Blob Storage <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/dd179451.aspx\" rel=\"nofollow\">REST API<\/a> with R to upload the images. Please refer to the article <a href=\"https:\/\/blogs.msdn.microsoft.com\/benjguin\/2014\/10\/24\/how-to-retrieve-r-data-visualization-from-azure-machine-learning\/\" rel=\"nofollow\">How to retrieve R data visualization from Azure Machine Learning<\/a>.<\/p><\/li>\n<li><p>You can directly add a module in C# to get &amp; upload the images from the output of <code>Execute R Script<\/code>. Please refer to the article <a href=\"https:\/\/blogs.msdn.microsoft.com\/data_insights_global_practice\/2015\/12\/15\/accessing-a-visual-generated-from-r-code-in-azureml\/\" rel=\"nofollow\">Accessing a Visual Generated from R Code in AzureML<\/a>.<\/p><\/li>\n<\/ol>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.8,
        "Solution_reading_time":15.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":139.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Is it possible to deploy an existing model artifact from SageMaker to Redshift ML? \n\nFor example, with an **Aurora ML** you can reference a SageMaker endpoint and then use it as a UDF in a `SELECT` statement. \n**Redshift ML** works a bit differently - when you call `CREATE MODEL` - the model is trained with **SageMaker Autopilot** and then deployed to the **Redshift Cluster**. \n\nWhat if I already have a trained model, can i deploy it to a Redshift Cluster and then use a UDF for Inference?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609954586000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668536452452,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUCMYCx28qRe-MOCIfj91Y2g\/redshift-ml-sagemaker-deploy-an-existing-model-artifact-to-a-redshift-cluster",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":6.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Redshift ML \/ SageMaker - Deploy an existing model artifact to a Redshift Cluster",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":148.0,
        "Challenge_word_count":96,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"As of January 30 2021, you can't deploy an existing model artifact from SageMaker to Redshift ML directly with currently announced Redshift ML preview features. But you can reference  sagemaker endpoint through a lambda function and use that lambda function as an user defined function in Redshift.\n\nBelow would be the steps:\n\n1. Train and deploy your SageMaker model in a SageMaker Endpoint. \n2. Use Lambda function to [reference sagemaker endpoint](https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/). \n3. Create a [Redshift Lambda UDF](https:\/\/aws.amazon.com\/blogs\/big-data\/accessing-external-components-using-amazon-redshift-lambda-udfs\/) referring above lambda function to run predictions.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1612026491936,
        "Solution_link_count":2.0,
        "Solution_readability":15.1,
        "Solution_reading_time":10.02,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":84.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1558539179612,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":16.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'd like to (a) plot SHAP values out of the SageMaker (b) AutoML pipeline. To achieve (a), debugger shall be used according to: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/ml-explainability-with-amazon-sagemaker-debugger\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/ml-explainability-with-amazon-sagemaker-debugger\/<\/a>.<\/p>\n\n<p>But how to enable the debug model in the AutoPilot without hacking into the background?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591056499550,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62142825",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":17.8,
        "Challenge_reading_time":6.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How to Enable SageMaker Debugger in the SageMaker AutoPilot",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":215.0,
        "Challenge_word_count":50,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1392607100776,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sydney NSW, Australia",
        "Poster_reputation_count":133.0,
        "Poster_view_count":29.0,
        "Solution_body":"<p>SageMaker Autopilot doesn't support SageMaker Debugger out of the box currently (as of Dec 2020). You can hack the Hyperparameter Tuning job to pass in a debug parameter.<\/p>\n<p>However, there is a way to use SHAP with Autopilot models. Take a look at this blog post explaining how to use SHAP with SageMaker Autopilot: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/explaining-amazon-sagemaker-autopilot-models-with-shap\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/explaining-amazon-sagemaker-autopilot-models-with-shap\/<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.1,
        "Solution_reading_time":7.55,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":58.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1467237684900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":613.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a problem with SageMaker when I try to upload Data into S3 bucket . I get this error : <\/p>\n\n<blockquote>\n  <hr>\n\n<pre><code>NameError                                 Traceback (most recent call last)\n&lt;ipython-input-26-d21b1cb0fcab&gt; in &lt;module&gt;()\n     19 download('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-train.rec')\n     20 \n---&gt; 21 upload_to_s3('train', 'caltech-256-60-train.rec')\n\n&lt;ipython-input-26-d21b1cb0fcab&gt; in upload_to_s3(channel, file)\n     13     data = open(file, \"rb\")\n     14     key = channel + '\/' + file\n---&gt; 15     s3.Bucket(bucket).put_object(Key=key, Body=data)\n     16 \n     17 \n\nNameError: name 'bucket' is not defined\n<\/code><\/pre>\n<\/blockquote>\n\n<p>Here is the script:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>import os\nimport urllib.request\nimport boto3\n\ndef download(url):\n    filename = url.split(\"\/\")[-1]\n    if not os.path.exists(filename):\n        urllib.request.urlretrieve(url, filename)\n\n\ndef upload_to_s3(channel, file):\n    s3 = boto3.resource('s3')\n    data = open(file, \"rb\")\n    key = channel + '\/' + file\n    s3.Bucket(bucket).put_object(Key=key, Body=data)\n\n\n# caltech-256 download('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-train.rec')\n\nupload_to_s3('train', 'caltech-256-60-train.rec')\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1524475888687,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1544136807192,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49977679",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":16.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"upload data to S3 with sagemaker",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":9575.0,
        "Challenge_word_count":108,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1364379638763,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":187.0,
        "Poster_view_count":108.0,
        "Solution_body":"<p>It is exactly as the error say, the variable <code>bucket<\/code> is not defined. \nyou might want to do something like <\/p>\n\n<pre><code>bucket = &lt;name of already created bucket in s3&gt;\n<\/code><\/pre>\n\n<p>before you call <\/p>\n\n<pre><code>s3.Bucket(bucket).put_object(Key=key, Body=data)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.4,
        "Solution_reading_time":3.88,
        "Solution_score_count":8.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":37.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1378136257732,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Budapest, Hungary",
        "Answerer_reputation_count":8162.0,
        "Answerer_view_count":283.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working on Azure ML implementation on text analytics with NLTK, the following execution is throwing <\/p>\n\n<pre><code>AssertionError: 1 columns passed, passed data had 2 columns\\r\\nProcess returned with non-zero exit code 1\n<\/code><\/pre>\n\n<p>Below is the code <\/p>\n\n<pre><code># The script MUST include the following function,\n# which is the entry point for this module:\n# Param&lt;dataframe1&gt;: a pandas.DataFrame\n# Param&lt;dataframe2&gt;: a pandas.DataFrame\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    # import required packages\n    import pandas as pd\n    import nltk\n    import numpy as np\n    # tokenize the review text and store the word corpus\n    word_dict = {}\n    token_list = []\n    nltk.download(info_or_id='punkt', download_dir='C:\/users\/client\/nltk_data')\n    nltk.download(info_or_id='maxent_treebank_pos_tagger', download_dir='C:\/users\/client\/nltk_data')\n    for text in dataframe1[\"tweet_text\"]:\n        tokens = nltk.word_tokenize(text.decode('utf8'))\n        tagged = nltk.pos_tag(tokens)\n\n\n      # convert feature vector to dataframe object\n    dataframe_output = pd.DataFrame(tagged, columns=['Output'])\n    return [dataframe_output]\n<\/code><\/pre>\n\n<p>Error is throwing here <\/p>\n\n<pre><code> dataframe_output = pd.DataFrame(tagged, columns=['Output'])\n<\/code><\/pre>\n\n<p>I suspect this to be the tagged data type passed to dataframe, can some one let me know the right approach to add this to dataframe.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1471040597197,
        "Challenge_favorite_count":3.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38927230",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":18.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Panda AssertionError columns passed, passed data had 2 columns",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":48200.0,
        "Challenge_word_count":158,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1370924418390,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":1748.0,
        "Poster_view_count":339.0,
        "Solution_body":"<p>Try this:<\/p>\n\n<pre><code>dataframe_output = pd.DataFrame(tagged, columns=['Output', 'temp'])\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.4,
        "Solution_reading_time":1.5,
        "Solution_score_count":13.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":7.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1415722650716,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Verona, VR, Italy",
        "Answerer_reputation_count":4811.0,
        "Answerer_view_count":713.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>On the limited Azure Machine Learning Studio, one can import data from an On-Premises SQL Server Database.\nWhat about the ability to do the exact same thing on a python jupyter notebook on a virtual machine from the Azure Machine Learning Services workspace ?<\/p>\n\n<p>It does not seem possible from what I've found in the documentation.\nData sources would be limited in Azure ML Services : \"Currently, the list of supported Azure storage services that can be registered as datastores are Azure Blob Container, Azure File Share, Azure Data Lake, Azure Data Lake Gen2, Azure SQL Database, Azure PostgreSQL, and Databricks File System\"<\/p>\n\n<p>Thank you in advance for your assistance<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1558448804447,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56240481",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.7,
        "Challenge_reading_time":9.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Can I import data from On-Premises SQL Server Database to Azure Machine Learning virtual machine?",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1147.0,
        "Challenge_word_count":123,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1558447987352,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>As of today, <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-load-data#load-sql-data\" rel=\"nofollow noreferrer\">you can load SQL data, but only a MS SQL Server source (also on-premise) is supported<\/a>.<\/p>\n\n<p>Using <code>azureml.dataprep<\/code>, code would read along the lines of<\/p>\n\n<pre><code>import azureml.dataprep as dprep\n\nsecret = dprep.register_secret(value=\"[SECRET-PASSWORD]\", id=\"[SECRET-ID]\")\n\nds = dprep.MSSQLDataSource(server_name=\"[SERVER-NAME]\",\n                           database_name=\"[DATABASE-NAME]\",\n                           user_name=\"[DATABASE-USERNAME]\",\n                           password=secret)\n\ndflow = dprep.read_sql(ds, \"SELECT top 100 * FROM [YourDB].[ATable]\")\n# print first records\ndflow.head(5)\n<\/code><\/pre>\n\n<p>As far as I understand the APIs are under heavy development and <code>azureml.dataprep<\/code> may be soon superseded by functionality provided by the <a href=\"https:\/\/aka.ms\/azureml\/concepts\/datasets\" rel=\"nofollow noreferrer\">Dataset class<\/a>.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.0,
        "Solution_reading_time":12.72,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":82.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1402536093248,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":817703.0,
        "Answerer_view_count":106500.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have dataframe with columns <\/p>\n\n<pre><code>date    open    high    low     close   adjclose    volume\n<\/code><\/pre>\n\n<p>I want to add one more column named \"result\"(1 if close > open, 0 if close &lt; open)<\/p>\n\n<p>I do<\/p>\n\n<pre><code># Map 1-based optional input ports to variables\ndata &lt;- maml.mapInputPort(1) # class: data.frame\n\n\n\n# calculate pass\/fail\ndata$result &lt;- as.factor(sapply(data$close,function(res) \n    if (res - data$open &gt;= 0) '1' else '0'))\n\n# Select data.frame to be sent to the output Dataset port\nmaml.mapOutputPort(\"data\");\n<\/code><\/pre>\n\n<p>But I have only 1 in result. Where is the problem?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1554571406233,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55551617",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.3,
        "Challenge_reading_time":7.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML and r scripts",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":77.0,
        "Challenge_word_count":86,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1553239567403,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":67.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>The <code>if\/else<\/code> can return only a single TRUE\/FALSE and is not vectorized for length > 1.  It may be suitable to use <code>ifelse<\/code> (but that is also not required and would be less efficient compared to direct coersion of logical vector to binary (<code>as.integer<\/code>).   In the OP's code, the 'close' column elements are looped  (<code>sapply<\/code>) and subtracted from the whole 'open' column.  The intention might be to do elementwise subtraction.  In that case, <code>-<\/code> between the columns is much cleaner and efficient (as these operations are vectorized)<\/p>\n\n<pre><code>data$result &lt;- with(data, factor(as.integer((close - open) &gt;= 0)))\n<\/code><\/pre>\n\n<p>In the above, we get the difference between the columns ('close', 'open'), check if it is greater than or equal to 0 (returns logical vector), convert it to binary (<code>as.integer<\/code> - TRUE -> 1, FALSE -> 0) and then change it to <code>factor<\/code> type (if needed)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.9,
        "Solution_reading_time":12.1,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":137.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1280505139752,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bangalore, India",
        "Answerer_reputation_count":4265.0,
        "Answerer_view_count":403.0,
        "Challenge_adjusted_solved_time":957.2375636111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>For the Python API for tabular dataset of AzureML (<code>azureml.data.TabularDataset<\/code>), there are two experimental methods which have been introduced:<\/p>\n<ol>\n<li><code>download(stream_column, target_path=None, overwrite=False, ignore_not_found=True)<\/code><\/li>\n<li><code>mount(stream_column, mount_point=None)<\/code><\/li>\n<\/ol>\n<p>Parameter <code>stream_column<\/code> has been defined as The stream column to mount or download.<\/p>\n<p>What is the actual meaning of <code>stream_column<\/code>? I don't see any example any where?<\/p>\n<p>Any pointer will be helpful.<\/p>\n<p>The stack trace:<\/p>\n<pre><code>Method download: This is an experimental method, and may change at any time. Please see https:\/\/aka.ms\/azuremlexperimental for more information.\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n\/tmp\/ipykernel_11561\/3904436543.py in &lt;module&gt;\n----&gt; 1 tab_dataset.download(target_path=&quot;..\/data\/tabular&quot;)\n\n\/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/_base_sdk_common\/_docstring_wrapper.py in wrapped(*args, **kwargs)\n     50     def wrapped(*args, **kwargs):\n     51         module_logger.warning(&quot;Method {0}: {1} {2}&quot;.format(func.__name__, _method_msg, _experimental_link_msg))\n---&gt; 52         return func(*args, **kwargs)\n     53     return wrapped\n     54 \n\n\/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/data\/_loggerfactory.py in wrapper(*args, **kwargs)\n    130             with _LoggerFactory.track_activity(logger, func.__name__, activity_type, custom_dimensions) as al:\n    131                 try:\n--&gt; 132                     return func(*args, **kwargs)\n    133                 except Exception as e:\n    134                     if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):\n\nTypeError: download() missing 1 required positional argument: 'stream_column'\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1644217302490,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1645197572643,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71014584",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":14.4,
        "Challenge_reading_time":25.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML Tabular Dataset : missing 1 required positional argument: 'stream_column'",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":356.0,
        "Challenge_word_count":166,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1280505139752,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, India",
        "Poster_reputation_count":4265.0,
        "Poster_view_count":403.0,
        "Solution_body":"<p><strong>Update on 5th March, 2022<\/strong><\/p>\n<p>I posted this as a support ticket with Azure. Following is the answer I have received:<\/p>\n<blockquote>\n<p>As you can see from our documentation of <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.tabulardataset?view=azure-ml-py\" rel=\"nofollow noreferrer\">TabularDataset Class<\/a>,\nthe \u201cstream_column\u201d parameter is required. So, that error is occurring\nbecause you are not passing any parameters when you are calling the\ndownload method.    The \u201cstream_column\u201d parameter should have the\nstream column to download\/mount. So, you need to pass the column name\nthat contains the paths from which the data will be streamed.<br \/>\nPlease find an example <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-labeled-dataset#explore-labeled-datasets-via-pandas-dataframe\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1648643627872,
        "Solution_link_count":2.0,
        "Solution_readability":11.5,
        "Solution_reading_time":12.09,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":97.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442430064503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":6147.0,
        "Answerer_view_count":1230.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I create a model in Azure ML studio. \nI deployed the web service.<\/p>\n\n<p>Now, I know how to check one record at a time, but how can I load a csv file and made the algorithm go through all records ?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/1tHuM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1tHuM.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>If I click on Batch Execution - it will ask me to create an account for Azure storage. <\/p>\n\n<p>Is any way to execute multiple records from csv file without creating any other accounts?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/90zP7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/90zP7.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1515448065933,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48158545",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":8.1,
        "Challenge_reading_time":10.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"How to execute multiple rows in web service Azure Machine Learning Studio",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":204.0,
        "Challenge_word_count":103,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1457596845392,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Diego, CA, United States",
        "Poster_reputation_count":4046.0,
        "Poster_view_count":825.0,
        "Solution_body":"<p>Yes, there is a way and it is simple. What you need is an excel add-in. You need not create any other account.<\/p>\n\n<p>You can either read <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/excel-add-in-for-web-services\" rel=\"nofollow noreferrer\">Excel Add-in for Azure Machine Learning web services doc<\/a> or you can watch <a href=\"https:\/\/www.youtube.com\/watch?v=ju1CzDjiOMQ\" rel=\"nofollow noreferrer\">Azure ML Excel Add-in video<\/a>. <\/p>\n\n<p>If you search for <a href=\"https:\/\/www.google.co.in\/search?q=excel%20add%20in%20for%20azure%20ml&amp;client=firefox-b-ab&amp;dcr=0&amp;source=lnms&amp;tbm=vid&amp;sa=X&amp;ved=0ahUKEwinqP3a_67ZAhXBr48KHdiYAXUQ_AUICigB&amp;biw=1280&amp;bih=616\" rel=\"nofollow noreferrer\">videos on excel add in for azure ml<\/a>, you get other useful videos too. <\/p>\n\n<p>I hope this is the solution you are looking for.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":11.6,
        "Solution_reading_time":11.61,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":84.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hi, I am using Azure Machine Learning studio.  <\/p>\n<p>I trained my deep learning model on computing cluster, and then outputs images onto 'outputs' directory.  <br \/>\nHowever, in Experiment &gt; Run &gt; outputs + logs tab, directories was displayed instead of my images.  <\/p>\n<p>Is it possible to get my images from 'outputs' directory?  <\/p>\n<p>After investigation, I recognized that images which have filename containing square brackets is not shown in the tab.  <br \/>\nFor instance, if I created 'outputs\/test[0].jpg', then 'outputs\/test' directory was shown in the tab.  <\/p>\n<p>[Sample code]  <\/p>\n<pre><code>import cv2\nimport numpy as np\nfrom pathlib import Path\n\n# Check whether output directory exists or not\noutput_dir = Path('.\/outputs\/')\nif not output_dir.exists():\n    output_dir.mkdir()\n\n# Generate test image\nIMAGE_SIZE = (28, 28)\n\ntest_image = np.zeros(IMAGE_SIZE)\n\n# Save image with filename which contained \/ not contained square brackets\n# 'test.jpg' and 'test' directory will be shown in the 'outputs' directory, however 'test[].jpg' and 'test[0].jpg' are disappeared.\ncv2.imwrite(str(output_dir \/ 'test.jpg'), test_image)\ncv2.imwrite(str(output_dir \/ 'test[].jpg'), test_image)\ncv2.imwrite(str(output_dir \/ 'test[0].jpg'), test_image)\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1610588261997,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/229808\/azure-machine-learning-studio-images-which-have-fi",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":8.3,
        "Challenge_reading_time":17.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning studio - Images which have filename containing square brackets is not shown in outputs folder.",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":175,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, it probably recognizes the square bracket as wildcard. Try using a double-backticks to escape the brackets, otherwise, I recommend you rename without the brackets and train again.  <\/p>\n<pre><code>cv2.imwrite(str(output_dir \/ 'test``[``].jpg'), test_image)\n<\/code><\/pre>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":3.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":33.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":11,
        "Challenge_body":"## \ud83d\udc1b Bug \r\n\r\nThe Comet logger cannot be pickled after an experiment (at least an OfflineExperiment) has been created.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\ninitialize the logger object (works fine)\r\n```\r\nfrom pytorch_lightning.loggers import CometLogger\r\nimport tests.base.utils as tutils\r\nfrom pytorch_lightning import Trainer\r\nimport pickle\r\n\r\nmodel, _ = tutils.get_default_model()\r\nlogger = CometLogger(save_dir='test')\r\npickle.dumps(logger)\r\n```\r\n\r\ninitialize a Trainer object with the logger (works fine)\r\n```\r\ntrainer = Trainer(\r\n    max_epochs=1,\r\n    logger=logger\r\n)\r\npickle.dumps(logger)\r\npickle.dumps(trainer)\r\n```\r\n\r\naccess the `experiment` attribute which creates the OfflineExperiment object (fails)\r\n```\r\nlogger.experiment\r\npickle.dumps(logger)\r\n>> TypeError: can't pickle _thread.lock objects\r\n```\r\n\r\n### Expected behavior\r\n\r\nWe should be able to pickle loggers for distributed training.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n        - available:         False\r\n        - version:           None\r\n* Packages:\r\n        - numpy:             1.18.1\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.4.0\r\n        - pytorch-lightning: 0.7.5\r\n        - tensorboard:       2.1.0\r\n        - tqdm:              4.42.0\r\n* System:\r\n        - OS:                Darwin\r\n        - architecture:\r\n                - 64bit\r\n                - \r\n        - processor:         i386\r\n        - python:            3.7.6\r\n        - version:           Darwin Kernel Version 19.3.0: Thu Jan  9 20:58:23 PST 2020; root:xnu-6153.81.5~1\/RELEASE_X86_64\r\n\r\n",
        "Challenge_closed_time":1591023.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1588303817000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/1682",
        "Challenge_link_count":0,
        "Challenge_participation_count":11,
        "Challenge_readability":11.1,
        "Challenge_reading_time":16.81,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"Comet logger cannot be pickled after creating an experiment",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":144,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@ceyzaguirre4 pls ^^ I don't know if it can help or if it is the right place, but a similar error occurswhen running in ddp mode with the WandB logger.\r\n\r\nWandB uses a lambda function at some point.\r\n\r\nDoes the logger have to pickled ? Couldn't it log only on rank 0 at epoch_end ?\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"..\/train.py\", line 140, in <module>\r\n    main(args.gpus, args.nodes, args.fast_dev_run, args.mixed_precision, project_config, hparams)\r\n  File \"..\/train.py\", line 117, in main\r\n    trainer.fit(model)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 751, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/site-packages\/torch\/multiprocessing\/spawn.py\", line 200, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/site-packages\/torch\/multiprocessing\/spawn.py\", line 149, in start_processes\r\n    process.start()\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/process.py\", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/context.py\", line 283, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object 'TorchHistory.add_log_hooks_to_pytorch_module.<locals>.<lambda>'\r\n```\r\n\r\nalso related: \r\n#1704 I had the same error as @jeremyjordan  `can't pickle _thread.lock objects`. This happened when I added the  `logger` and additional `callbacks` in `from_argparse_args`, as explained here https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/hyperparameters.html\r\n\r\n```\r\ntrainer = pl.Trainer.from_argparse_args(hparams, logger=logger, callbacks=[PrinterCallback(), ])\r\n```\r\nI could make the problem go away by directly overwriting the members of `Trainer`\r\n\r\n```\r\ntrainer = pl.Trainer.from_argparse_args(hparams)\r\ntrainer.logger = logger\r\ntrainer.callbacks.append(PrinterCallback())\r\n``` Same issue as @F-Barto using a wandb logger across 2 nodes with `ddp`. same issue when using wandb logger with ddp same here.. @joseluisvaz your workaround doesn't solve the callback issue.. when I try to add a callback like this it is simply being ignored :\/ but adding it the Trainer init call normally works.. so I'm pretty sure the error is thrown by the logger (I'm using TB) not the callbacks. Same issue, using wandb logger with 8 gpus in an AWS p2.8xlarge machine  With CometLogger, I get this error only when the experiment name is declared. If it is not declared, I get no issue. I still have this error with 1.5.10 on macOS\r\n\r\n```\r\nError executing job with overrides: ['train.pl_trainer.fast_dev_run=False', 'train.pl_trainer.gpus=0', 'train.pl_trainer.precision=32', 'logging.wandb_arg.mode=offline']\r\nTraceback (most recent call last):\r\n  File \"\/Users\/ric\/Documents\/PhD\/Projects\/ed-experiments\/src\/train.py\", line 78, in main\r\n    train(conf)\r\n  File \"\/Users\/ric\/Documents\/PhD\/Projects\/ed-experiments\/src\/train.py\", line 70, in train\r\n    trainer.fit(pl_module, datamodule=pl_data_module)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 740, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 685, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 777, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1199, in _run\r\n    self._dispatch()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1279, in _dispatch\r\n    self.training_type_plugin.start_training(self)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/plugins\/training_type\/training_type_plugin.py\", line 202, in start_training\r\n    self._results = trainer.run_stage()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1289, in run_stage\r\n    return self._run_train()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1311, in _run_train\r\n    self._run_sanity_check(self.lightning_module)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1375, in _run_sanity_check\r\n    self._evaluation_loop.run()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/loops\/base.py\", line 145, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/loops\/dataloader\/evaluation_loop.py\", line 110, in advance\r\n    dl_outputs = self.epoch_loop.run(dataloader, dataloader_idx, dl_max_batches, self.num_dataloaders)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/loops\/base.py\", line 140, in run\r\n    self.on_run_start(*args, **kwargs)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/loops\/epoch\/evaluation_epoch_loop.py\", line 86, in on_run_start\r\n    self._dataloader_iter = _update_dataloader_iter(data_fetcher, self.batch_progress.current.ready)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/loops\/utilities.py\", line 121, in _update_dataloader_iter\r\n    dataloader_iter = enumerate(data_fetcher, batch_idx)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/fetching.py\", line 198, in __iter__\r\n    self._apply_patch()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/fetching.py\", line 133, in _apply_patch\r\n    apply_to_collections(self.loaders, self.loader_iters, (Iterator, DataLoader), _apply_patch_fn)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/fetching.py\", line 181, in loader_iters\r\n    loader_iters = self.dataloader_iter.loader_iters\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/supporters.py\", line 537, in loader_iters\r\n    self._loader_iters = self.create_loader_iters(self.loaders)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/supporters.py\", line 577, in create_loader_iters\r\n    return apply_to_collection(loaders, Iterable, iter, wrong_dtype=(Sequence, Mapping))\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/apply_func.py\", line 104, in apply_to_collection\r\n    v = apply_to_collection(\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/apply_func.py\", line 96, in apply_to_collection\r\n    return function(data, *args, **kwargs)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/supporters.py\", line 177, in __iter__\r\n    self._loader_iter = iter(self.loader)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/torch\/utils\/data\/dataloader.py\", line 359, in __iter__\r\n    return self._get_iterator()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/torch\/utils\/data\/dataloader.py\", line 305, in _get_iterator\r\n    return _MultiProcessingDataLoaderIter(self)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/torch\/utils\/data\/dataloader.py\", line 918, in __init__\r\n    w.start()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/process.py\", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/context.py\", line 224, in _Popen\r\n    return _default_context.get_context().Process._Popen(process_obj)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object 'TorchHistory.add_log_hooks_to_pytorch_module.<locals>.<lambda>'\r\n``` I still see this bug as well with WandB logger. Currently having this issue with wandbLogger.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":19.8,
        "Solution_reading_time":127.3,
        "Solution_score_count":null,
        "Solution_sentence_count":105.0,
        "Solution_word_count":638.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>What are the learning paths in MS Learn for Data Science, Machine Learning, and Deep Learning with Python as the base programming language? How to get the Microsoft Certification.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1684303074093,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1286478\/what-is-the-learning-path-to-became-a-data-scienti",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":2.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"What is the Learning path to Became a data scientist?",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":38,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The certification path for Data Scientist includes 6 exams. <a href=\"https:\/\/learn.microsoft.com\/en-us\/certifications\/browse\/?roles=data-scientist\">https:\/\/learn.microsoft.com\/en-us\/certifications\/browse\/?roles=data-scientist<\/a><\/p>\n<p>Of these 6, the core exam is DP-100, passing it will earn you <strong>Microsoft Certified: Azure Data Scientist Associate<\/strong>.<\/p>\n<p>The DP-100 exam page features the Learning path collection with all of the modules to prepare for the exam; <a href=\"https:\/\/learn.microsoft.com\/en-us\/certifications\/azure-data-scientist\/\">https:\/\/learn.microsoft.com\/en-us\/certifications\/azure-data-scientist\/<\/a><\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.7,
        "Solution_reading_time":8.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":51.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hey,  \n  \nI'm trying to use Ground Truth to do image classification but with a different set of label options for each image. I have the custom labeling task template and pre-\/post-labeling Lambda functions set up and I figured I could pass in the labels through the manifest file.  \n  \nMy issue is that the Ground Truth job ignores the attributes in the manifest file that are not \"source-ref\" (or \"source\"). This causes the pre-processing Lambda function to fail because the request it is passed only contains the \"source-ref\" attribute, but the Lambda function also references a different attribute. Are augmented manifest files supported for Ground Truth and if they are, how can I make use of the extra attributes?  \n  \nReferences:  \nGround Truth Input Data: <https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-input.html>  \nSageMaker Augmented Manifest Files: <https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/augmented-manifest.html>  \n  \nExample:  \n  \nA normal Ground Truth manifest file:\n\n```\n{\"source-ref\":\"s3:\/\/some_bucket\/images\/img1.png\"}\r\n{\"source-ref\":\"s3:\/\/some_bucket\/images\/img2.png\"}\r\n...\n```\n\nWhat I want to be able to use:\n\n```\n{\"source-ref\":\"s3:\/\/some_bucket\/images\/img1.png\",\"labels\":[\"pen\",\"pencil\",\"stick\"]}\r\n{\"source-ref\":\"s3:\/\/some_bucket\/images\/img2.png\",\"labels\":[\"tv\",\"laptop\",\"phone\"]}\r\n...\n```\n\n",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1546562747000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668535733171,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU1LLbT-AYQDO-XXrjPUFl9w\/how-to-use-an-augmented-manifest-file-for-aws-sagemaker-ground-truth",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":17.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"How to use an Augmented Manifest File for AWS SageMaker Ground Truth?",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":241.0,
        "Challenge_word_count":159,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi sageuser, I'm an engineer at AWS. Augmented manifests are not supported for custom workflows, and so it is not possible to pass through additional parameters, e.g., \"labels\" in your example. We appreciate that you are using the service and welcome customer feedback. We can always be reached at https:\/\/aws.amazon.com\/contact-us\/.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1546888840000,
        "Solution_link_count":1.0,
        "Solution_readability":8.4,
        "Solution_reading_time":4.17,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":50.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"After https:\/\/github.com\/iterative\/dvc\/pull\/5265\r\nWe do not allow ignoring lockfile. `dvc-bench` is running currently on some older version of `dvc`, though it would be good to adjust it so that it works with `>2.0.0`.",
        "Challenge_closed_time":1628758.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616672577000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/iterative\/dvc-bench\/issues\/244",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":7.7,
        "Challenge_reading_time":3.07,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":9.0,
        "Challenge_repo_issue_count":400.0,
        "Challenge_repo_star_count":19.0,
        "Challenge_repo_watch_count":17.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"requirements: update dvc",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":34,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@pared Sorry, not sure I understand what do we need to update here. Could you elaborate, please? Fixed by #267, forgot to close.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.8,
        "Solution_reading_time":1.56,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":23.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>The usage in the <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/environment-variables#optional-environment-variables\">Docs<\/a> is:<\/p>\n<blockquote>\n<p>Set this to a comma separated list of file globs to ignore. These files will not be synced to the cloud<\/p>\n<\/blockquote>\n<p>So, is the below code correct?<\/p>\n<pre><code class=\"lang-python\">os.environ['WANDB_IGNORE_GLOBS'] = '[*.pth, *.npy]'\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668582283551,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-set-the-environment-variable-wandb-ignore-globs-correctly\/3423",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":11.8,
        "Challenge_reading_time":6.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How to set the environment variable WANDB_IGNORE_GLOBS correctly?",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":142.0,
        "Challenge_word_count":48,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/geyao\">@geyao<\/a> thank you for writing in! Could you please check if the following would work for you?<\/p>\n<pre><code class=\"lang-auto\">os.environ['WANDB_IGNORE_GLOBS'] = '*.pth,*.npy'\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":3.07,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":24.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":14.2520105556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have just removed a DVC tracking file by mistake using the command <code>dvc remove training_data.dvc -p<\/code>, which led to all my training dataset gone completely. I know in Git, we can easily revert a deleted branch based on its hash. Does anyone know how to revert all my lost data in DVC?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592445622650,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62441146",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.5,
        "Challenge_reading_time":4.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Revert a dvc remove -p command",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":687.0,
        "Challenge_word_count":58,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1467943515392,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Danang, H\u1ea3i Ch\u00e2u District, Da Nang, Vietnam",
        "Poster_reputation_count":173.0,
        "Poster_view_count":28.0,
        "Solution_body":"<p>You should be safe (at least data is not gone) most likely. From the <code>dvc remove<\/code> <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remove\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n\n<blockquote>\n  <p>Note that it does not remove files from the DVC cache or remote storage (see dvc gc). However, remember to run <code>dvc push<\/code> to save the files you actually want to use or share in the future.<\/p>\n<\/blockquote>\n\n<p>So, if you created <code>training_data.dvc<\/code> as with <code>dvc add<\/code> and\/or <code>dvc run<\/code> and <code>dvc remove -p<\/code> didn't ask\/warn you about anything, means that data is cached similar to Git in the <code>.dvc\/cache<\/code>. <\/p>\n\n<p>There are ways to retrieve it, but I would need to know a little bit more details - how exactly did you add your dataset? Did you commit <code>training_data.dvc<\/code> or it's completely gone? Was it the only data you have added so far? (happy to help you in comments).<\/p>\n\n<h2>Recovering a directory<\/h2>\n\n<p>First of all, <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvc-files-and-directories#structure-of-cache-directory\" rel=\"nofollow noreferrer\">here<\/a> is the document that describes briefly how DVC stores directories in the cache.<\/p>\n\n<p>What we can do is to find all <code>.dir<\/code> files in the <code>.dvc\/cache<\/code>:<\/p>\n\n<p><code>find .dvc\/cache -type f -name \"*.dir\"<\/code><\/p>\n\n<p>outputs something like:<\/p>\n\n<pre><code>.dvc\/cache\/20\/b786b6e6f80e2b3fcf17827ad18597.dir\n.dvc\/cache\/00\/db872eebe1c914dd13617616bb8586.dir\n.dvc\/cache\/2d\/1764cb0fc973f68f31f5ff90ee0883.dir\n<\/code><\/pre>\n\n<p>(if the local cache is lost and we are restoring data from the remote storage, the same logic applies, commands (e.g. to find files on S3 with .dir extension) look different)<\/p>\n\n<p>Each <code>.dir<\/code> file is a JSON with a content of one version of a directory (file names, hashes, etc). It has all the information needed to restore it. The next thing we need to do is to understand which one do we need. There is no one single rule for that, what I would recommend to check (and pick depending on your use case):<\/p>\n\n<ul>\n<li>Check the date modified (if you remember when this data was added).<\/li>\n<li>Check the content of those files - if you remember a specific file name that was present only in the directory you are looking for - just grep it.<\/li>\n<li>Try to restore them one by one and check the directory content.<\/li>\n<\/ul>\n\n<p>Okay, now let's imagine we decided that we want to restore <code>.dvc\/cache\/20\/b786b6e6f80e2b3fcf17827ad18597.dir<\/code>, (e.g. because content of it looks like:<\/p>\n\n<pre><code>[\n{\"md5\": \"6f597d341ceb7d8fbbe88859a892ef81\", \"relpath\": \"test.tsv\"}, {\"md5\": \"32b715ef0d71ff4c9e61f55b09c15e75\", \"relpath\": \"train.tsv\"}\n]\n<\/code><\/pre>\n\n<p>and we want to get a directory with <code>train.tsv<\/code>).<\/p>\n\n<p>The only thing we need to do is to create a <code>.dvc<\/code> file that references this directory:<\/p>\n\n<pre class=\"lang-yaml prettyprint-override\"><code>outs:\n- md5: 20b786b6e6f80e2b3fcf17827ad18597.dir\n  path: my-directory\n<\/code><\/pre>\n\n<p>(note, that path \/20\/b786b6e6f80e2b3fcf17827ad18597.dir became a hash value: 20b786b6e6f80e2b3fcf17827ad18597.dir)<\/p>\n\n<p>And run <code>dvc pull<\/code> on this file.<\/p>\n\n<p>That should be it.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1592496929888,
        "Solution_link_count":2.0,
        "Solution_readability":7.9,
        "Solution_reading_time":41.6,
        "Solution_score_count":3.0,
        "Solution_sentence_count":36.0,
        "Solution_word_count":420.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [X] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nmlflow logs the name of both models \"Least Angle Regression\" and \"Lasso Least Angle Regression\" as \"Least Angle Regression\".\r\n\r\nWhen looking into the `get_logs()` you can see both of those models have unique `run_id` but both have the same `tags.mlflow.runName`.\r\n\r\nPython Version: 3.9.5\r\nPyCaret Version: '3.0.0.rc3'\r\nPandas Version: 1.4.3\r\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\nfrom pycaret.regression import *\r\nfrom pycaret.datasets import get_data\r\ndataset = get_data('diamond')\r\n\r\nEXPERIMENT_NAME = 'diamond_experiment'\r\ns = setup(data=dataset, target='Price', log_experiment=True, experiment_name=EXPERIMENT_NAME, session_id=42, verbose=True)\r\n\r\nmodel = compare_models(verbose=False)\r\n\r\nprint(f\"Notice Least Angle Regression is not unique:\\n{get_logs(experiment_name=EXPERIMENT_NAME)['tags.mlflow.runName'].value_counts()}\")\r\n\r\n# Loop through all models in the `compare_models()` (20 models) function and get the length of the dataframe of that specific model in the logs\r\n# There should be a single unique value for each model\r\nfor model in pull().Model.tolist():\r\n    print(f\"{model} - {len(get_logs(experiment_name=EXPERIMENT_NAME)[get_logs(experiment_name=EXPERIMENT_NAME)['tags.mlflow.runName'] == model])}\")\r\n\r\n# Further investigation: model Least Angle Regression has 2 instances (should be Lasso Least Angle Regression and Least Angle Regression)\r\nget_logs(experiment_name=EXPERIMENT_NAME)[get_logs(experiment_name=EXPERIMENT_NAME)['tags.mlflow.runName'] == 'Least Angle Regression']\r\n```\n```\n\n\n### Expected Behavior\n\n`tags.mlflow.runName` parameter from `get_logs()` is unique (given a single experiment) and contains all model names from `compare_models()`\n\n### Actual Results\n\n```python-traceback\nWhen looking into the `tags.mlflow.runName` you can see they are all unique but Least Angle Regression is there twice and Lasso Least Angle Regression isn't there at all. Could this be logged incorrectly?\r\n\r\nGradient Boosting Regressor - 1\r\nCatBoost Regressor - 1\r\nLight Gradient Boosting Machine - 1\r\nExtreme Gradient Boosting - 1\r\nLasso Regression - 1\r\nRidge Regression - 1\r\nLinear Regression - 1\r\nLasso Least Angle Regression - 0\r\nLeast Angle Regression - 2\r\nExtra Trees Regressor - 1\r\nRandom Forest Regressor - 1\r\nAdaBoost Regressor - 1\r\nDecision Tree Regressor - 1\r\nOrthogonal Matching Pursuit - 1\r\nElastic Net - 1\r\nHuber Regressor - 1\r\nBayesian Ridge - 1\r\nK Neighbors Regressor - 1\r\nDummy Regressor - 1\r\nPassive Aggressive Regressor - 1\n```\n\n\n### Installed Versions\n\n<details>\r\nSystem:\r\n    python: 3.9.5 (v3.9.5:0a7dcbdb13, May  3 2021, 13:17:02)  [Clang 6.0 (clang-600.0.57)]\r\nexecutable: PATH_TO_ENV\/venv\/bin\/python\r\n   machine: macOS-10.16-x86_64-i386-64bit\r\n\r\nPyCaret required dependencies:\r\n                 pip: 21.1.1\r\n          setuptools: 56.0.0\r\n             pycaret: 3.0.0.rc3\r\n             IPython: 8.4.0\r\n          ipywidgets: 8.0.0rc0\r\n                tqdm: 4.64.0\r\n               numpy: 1.21.6\r\n              pandas: 1.4.3\r\n              jinja2: 3.1.2\r\n               scipy: 1.5.4\r\n              joblib: 1.1.0\r\n             sklearn: 1.1.1\r\n                pyod: Installed but version unavailable\r\n            imblearn: 0.9.1\r\n   category_encoders: 2.5.0\r\n            lightgbm: 3.3.2\r\n               numba: 0.55.2\r\n            requests: 2.28.0\r\n          matplotlib: 3.5.2\r\n          scikitplot: 0.3.7\r\n         yellowbrick: 1.4\r\n              plotly: 5.9.0\r\n             kaleido: 0.2.1\r\n         statsmodels: 0.13.2\r\n              sktime: 0.11.4\r\n               tbats: Installed but version unavailable\r\n            pmdarima: 1.8.5\r\n              psutil: 5.9.1\r\n<\/details>\r\n",
        "Challenge_closed_time":1670522.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659210438000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2811",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":47.39,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":49,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG]: mlflow incorrectly logging models \"Lasso Least Angle Regression\" and \"Least Angle Regression\"",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":420,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@Yard1 Can you give me a hand here? I ended up spending a lot of time in figuring out where is it coming from. The names inside `containers\/regression.py` seems to be fine but even then the run name is wrong.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/54699234\/204138706-db0d0cc3-9a08-46d3-8037-fbaee414876b.png)\r\n\r\nAny ideas?",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":6.6,
        "Solution_reading_time":4.25,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":43.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1611181716003,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":119.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am currently developing an Azure ML pipeline that as one of its outputs is maintaining a SQL table holding all of the unique items that are fed into it. There is no way to know in advance if the data fed into the pipeline is new unique items or repeats of previous items, so before updating the table that it maintains it pulls the data already in that table and drops any of the new items that already appear.<\/p>\n<p>However, due to this there are cases where this self-reference results in zero new items being found, and as such there is nothing to export to the SQL table. When this happens Azure ML throws an error, as it is considered an error for there to be zero lines of data to export. In my case, however, this is expected behaviour, and as such absolutely fine.<\/p>\n<p>Is there any way for me to suppress this error, so that when it has zero lines of data to export it just skips the export module and moves on?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622071490553,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67713876",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.6,
        "Challenge_reading_time":12.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Is there a way to stop Azure ML throwing an error when exporting zero lines of data?",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":94.0,
        "Challenge_word_count":192,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1611181716003,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":119.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>This issue has been resolved by an update to Azure Machine Learning; You can now run pipelines with a flag set to &quot;Continue on Failure Step&quot;, which means that steps following the failed data export will continue to run.<\/p>\n<p>This does mean you will need to design your pipeline to be able to handles upstream failures in its downstream modules; this must be done very carefully.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.8,
        "Solution_reading_time":4.88,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I connect the ML model endpoint with Power BI, it doesn't show me the model attributes that match the PBI dataset. Can you please help with the expected data format in Power BI.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1653284389440,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/859568\/consuming-ml-models-on-power-bi",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":6.0,
        "Challenge_reading_time":2.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Consuming ML models on Power BI",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":39,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=78d653c4-ee0a-4cd3-b540-08c38c4bd217\">@Arjun  <\/a> Thanks, Can you try define the input schema and follow the below sample.    <\/p>\n<p> Here is the <a href=\"https:\/\/github.com\/WipadaChan\/pbi_demo_repo\/tree\/master\/03_DeployH2O_PBI\">sample<\/a> to Deploy trained model to Azure ML and use Power BI to score new data.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.4,
        "Solution_reading_time":4.5,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm having a problem trying to run &quot;dvc pull&quot; on Google Colab. I have two repositories (let's call them A and B) where repository A is for my machine learning codes and repository B is for my dataset.<\/p>\n<p>I've successfully pushed my dataset to repository B with DVC (using gdrive as my remote storage) and I also managed to successfully run &quot;dvc import&quot; (as well as &quot;dvc pull\/update&quot;) on my local project of repository A.<\/p>\n<p>The problem comes when I use colab to run my project. So what I did was the following:<\/p>\n<ol>\n<li>Created a new notebook on colab<\/li>\n<li>Successfully git-cloned my machine learning project (repository A)<\/li>\n<li>Ran &quot;!pip install dvc&quot;<\/li>\n<li>Ran &quot;!dvc pull -v&quot; (This is what causes the error)<\/li>\n<\/ol>\n<p>On step 4, I got the error (this is the full stack trace. Note that I changed the repo URL in the stack trace for confidentiality reasons)<\/p>\n<pre><code>2022-03-08 08:53:31,863 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/config.local' to gitignore file.\n2022-03-08 08:53:31,866 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/tmp' to gitignore file.\n2022-03-08 08:53:31,866 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/cache' to gitignore file.\n2022-03-08 08:53:31,916 DEBUG: Creating external repo https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git@3a3f4559efabff8ec74486da39b86688d1b98d75\n2022-03-08 08:53:31,916 DEBUG: erepo: git clone 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to a temporary dir\nEverything is up to date.\n2022-03-08 08:53:32,154 ERROR: failed to pull data from the cloud - Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/backend\/gitpython.py&quot;, line 185, in clone\n    tmp_repo = clone_from()\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/repo\/base.py&quot;, line 1148, in clone_from\n    return cls._clone(git, url, to_path, GitCmdObjectDB, progress, multi_options, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/repo\/base.py&quot;, line 1079, in _clone\n    finalize_process, decode_streams=False)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/cmd.py&quot;, line 176, in handle_process_output\n    return finalizer(process)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/util.py&quot;, line 386, in finalize_process\n    proc.wait(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/cmd.py&quot;, line 502, in wait\n    raise GitCommandError(remove_password_if_present(self.args), status, errstr)\ngit.exc.GitCommandError: Cmd('git') failed due to: exit code(128)\n  cmdline: git clone -v --no-single-branch --progress https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git \/tmp\/tmp2x7y7xgedvc-clone\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/scm.py&quot;, line 104, in clone\n    return Git.clone(url, to_path, progress=pbar.update_git, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/__init__.py&quot;, line 121, in clone\n    backend.clone(url, to_path, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/backend\/gitpython.py&quot;, line 190, in clone\n    raise CloneError(url, to_path) from exc\nscmrepo.exceptions.CloneError: Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/command\/data_sync.py&quot;, line 41, in run\n    glob=self.args.glob,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/pull.py&quot;, line 38, in pull\n    run_cache=run_cache,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/fetch.py&quot;, line 50, in fetch\n    revs=revs,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 437, in used_objs\n    with_deps=with_deps,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/index.py&quot;, line 190, in used_objs\n    filter_info=filter_info,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/stage\/__init__.py&quot;, line 660, in get_used_objs\n    for odb, objs in out.get_used_objs(*args, **kwargs).items():\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/output.py&quot;, line 918, in get_used_objs\n    return self.get_used_external(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/output.py&quot;, line 973, in get_used_external\n    return dep.get_used_objs(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/dependency\/repo.py&quot;, line 94, in get_used_objs\n    used, _ = self._get_used_and_obj(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/dependency\/repo.py&quot;, line 108, in _get_used_and_obj\n    locked=locked, cache_dir=local_odb.cache_dir\n  File &quot;\/usr\/lib\/python3.7\/contextlib.py&quot;, line 112, in __enter__\n    return next(self.gen)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 35, in external_repo\n    path = _cached_clone(url, rev, for_write=for_write)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 155, in _cached_clone\n    clone_path, shallow = _clone_default_branch(url, rev, for_write=for_write)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/decorators.py&quot;, line 45, in wrapper\n    return deco(call, *dargs, **dkwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/flow.py&quot;, line 274, in wrap_with\n    return call()\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/decorators.py&quot;, line 66, in __call__\n    return self._func(*self._args, **self._kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 220, in _clone_default_branch\n    git = clone(url, clone_path)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/scm.py&quot;, line 106, in clone\n    raise CloneError(str(exc))\ndvc.scm.CloneError: Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n------------------------------------------------------------\n2022-03-08 08:53:32,161 DEBUG: Analytics is enabled.\n2022-03-08 08:53:32,192 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmp4x5js0dk']'\n2022-03-08 08:53:32,193 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmp4x5js0dk']'\n<\/code><\/pre>\n<p>And btw this is how I cloned my git repository (repo A)<\/p>\n<pre><code>!git config - global user.name &quot;Zharfan&quot;\n!git config - global user.email &quot;zharfan@myemail.com&quot;\n!git clone https:\/\/&lt;MyTokenName&gt;:&lt;MyToken&gt;@link-to-my-repo-A.git\n<\/code><\/pre>\n<p>Does anyone know why? Any help would be greatly appreciated. Thank you in advance!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":12,
        "Challenge_created_time":1646641948613,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1652856778060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71378280",
        "Challenge_link_count":7,
        "Challenge_participation_count":13,
        "Challenge_readability":13.5,
        "Challenge_reading_time":96.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":76,
        "Challenge_solved_time":null,
        "Challenge_title":"Error with DVC on Google Colab - dvc.scm.CloneError: Failed to clone repo",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":707.0,
        "Challenge_word_count":614,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1525227015312,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>To summarize the discussion in the comments thread.<\/p>\n<p>Most likely it's happening since DVC can't get access to a private repo on GitLab. (The error message is obscure and should be fixed.)<\/p>\n<p>The same way you would not be able to run:<\/p>\n<pre><code>!git clone https:\/\/gitlab.com\/org\/&lt;private-repo&gt;\n<\/code><\/pre>\n<p>It also returns a pretty obscure error:<\/p>\n<pre><code>Cloning into '&lt;private-repo&gt;'...\nfatal: could not read Username for 'https:\/\/gitlab.com': No such device or address\n<\/code><\/pre>\n<p>(I think it's something related to how tty is setup in Colab?)<\/p>\n<p>The best approach to solve this is to use SSH like described <a href=\"https:\/\/medium.com\/@sadiaafrinpurba\/how-to-clone-private-github-repo-in-google-colab-using-ssh-77384cfef18f\" rel=\"nofollow noreferrer\">here<\/a> for example.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":9.5,
        "Solution_reading_time":10.72,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":99.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1459778195087,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Czech Republic",
        "Answerer_reputation_count":622.0,
        "Answerer_view_count":59.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running <code>mlflow ui<\/code> and PostgreSQL db in docker compose.<\/p>\n<p>Mlflow UI container runs like this: <code>mlflow ui --backend-store-uri &quot;postgresql+psycopg2:\/\/postgres:passw0rd@database:5432\/postgres&quot; --host 0.0.0.0<\/code><\/p>\n<p>Then I run my models locally from jupyter, e.g.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>remote_server_uri = &quot;postgresql+psycopg2:\/\/postgres:passw0rd@localhost:5432\/postgres&quot;\nmlflow.set_tracking_uri(remote_server_uri)\nmlflow.set_experiment(&quot;exp2&quot;)\n\nX = np.array([-2, -1, 0, 1, 2, 1]).reshape(-1, 1)\ny = np.array([0, 0, 1, 1, 1, 0])\nlr = LogisticRegression()\nlr.fit(X, y)\nscore = lr.score(X, y)\nprint(&quot;Score: %s&quot; % score)\nwith mlflow.start_run():\n    mlflow.log_metric(&quot;score&quot;, score)\n<\/code><\/pre>\n<p>Everything works fine - experiments get logged into PostgreSQL and mlflow UI can read it from PostgreSQL .<\/p>\n<p>One thing that bothers me is that artifacts are stored locally into .\/mlruns folder. How to change it to save it somewhere else?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642929527630,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1642929807488,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70820661",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.8,
        "Challenge_reading_time":14.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Track to database, artifacts to specific destination",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":361.0,
        "Challenge_word_count":114,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459778195087,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Czech Republic",
        "Poster_reputation_count":622.0,
        "Poster_view_count":59.0,
        "Solution_body":"<p>So apparently <code>--default-artifact-root<\/code> argument has to be used when launching server\/ui. The only downside is that that default artifact root is relative to development environment, so if you are running mlflow server in docker and specify default-artifact-root to e.g. <code>some\/path<\/code> then the artifacts are going to be saved to your <strong>local machine<\/strong> to that path (<strong>not inside docker container<\/strong>). Probably the best solution is to use remote storage such as S3\/Blob.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.6,
        "Solution_reading_time":6.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":71.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1403084852692,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2210.0,
        "Answerer_view_count":262.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to setup a MLFlow tracking server on a remote machine as a systemd service.\nI have a sftp server running and created a SSH key pair.<\/p>\n<p>Everything seems to work fine except the artifact logging. MLFlow seems to not have permissions to list the artifacts saved in the <code>mlruns<\/code> directory.<\/p>\n<p>I create an experiment and log artifacts in this way:<\/p>\n<pre><code>uri = 'http:\/\/192.XXX:8000' \nmlflow.set_tracking_uri(uri)\n\nmlflow.create_experiment('test', artifact_location='sftp:\/\/192.XXX:_path_to_mlruns_folder_')\n\nexperiment=mlflow.get_experiment_by_name('test')\nwith mlflow.start_run(experiment_id=experiment.experiment_id, run_name=run_name) as run:\n       mlflow.log_param(_parameter_name_, _parameter_value_)     \n       mlflow.log_artifact(_an_artifact_, _artifact_folder_name_)\n<\/code><\/pre>\n<p>I can see the metrics in the UI and the artifacts in the correct destination folder on the remote machine. However, in the UI I receive this message when trying to see the artifacts:<\/p>\n<blockquote>\n<p>Unable to list artifacts stored\nunder sftp:\/\/192.XXX:<em>path_to_mlruns_folder<\/em>\/<em>run_id<\/em>\/artifacts\nfor the current run. Please contact your tracking server administrator\nto notify them of this error, which can happen when the tracking\nserver lacks permission to list artifacts under the current run's root\nartifact directory.<\/p>\n<\/blockquote>\n<p>I cannot figure out why as the <code>mlruns<\/code> folder has <code>drwxrwxrwx<\/code> permissions and all the subfolders have <code>drwxrwxr-x<\/code>. What am I missing?<\/p>\n<hr \/>\n<p>UPDATE\nLooking at it with fresh eyes, it seems weird that it tries to list files through <code>sftp:\/\/192.XXX:<\/code>, it should just look in the folder <code>_path_to_mlruns_folder_\/_run_id_\/artifacts<\/code>. However, I still do not know how to circumvent that.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615383956893,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1615451686848,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66566031",
        "Challenge_link_count":5,
        "Challenge_participation_count":1,
        "Challenge_readability":11.7,
        "Challenge_reading_time":24.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"MLFLow artifact logging and retrieve on remote server",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2283.0,
        "Challenge_word_count":223,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403084852692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2210.0,
        "Poster_view_count":262.0,
        "Solution_body":"<p>The problem seems to be that by default the systemd service is run by root.\nSpecifying a user and creating a ssh key pair for that user to access the same remote machine worked.<\/p>\n<pre><code>[Unit]\n\nDescription=MLflow server\n\nAfter=network.target \n\n[Service]\n\nRestart=on-failure\n\nRestartSec=20\n\nUser=_user_\n\nGroup=_group_\n\nExecStart=\/bin\/bash -c 'PATH=_yourpath_\/anaconda3\/envs\/mlflow_server\/bin\/:$PATH exec mlflow server --backend-store-uri postgresql:\/\/mlflow:mlflow@localhost\/mlflow --default-artifact-root sftp:\/\/_user_@192.168.1.245:_yourotherpath_\/MLFLOW_SERVER\/mlruns -h 0.0.0.0 -p 8000' \n\n[Install]\n\nWantedBy=multi-user.target\n<\/code><\/pre>\n<p><code>_user_<\/code> and <code>_group_<\/code> should be the same listed by <code>ls -la<\/code> in the <code>mlruns<\/code> directory.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.2,
        "Solution_reading_time":10.37,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":75.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hello I'm practicing in Microsoft Machine Learning Studio.    <\/p>\n<p>In a experiment I use de control &quot;Import data&quot;, then in the properties I use the data source: <a href=\"https:\/\/github.com\/rdiazconcha\/lil-azure-machine-learning-y-ai\/blob\/master\/modulo-2\/power-export_min.csv\">https:\/\/github.com\/rdiazconcha\/lil-azure-machine-learning-y-ai\/blob\/master\/modulo-2\/power-export_min.csv<\/a>    <\/p>\n<p>That is the file to practice in my course.    <\/p>\n<p>The resto of the fields are filled like this:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/107720-image.png?platform=QnA\" alt=\"107720-image.png\" \/>    <\/p>\n<p>But, when I use the choice visualize, appears like this:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/107744-image.png?platform=QnA\" alt=\"107744-image.png\" \/>    <\/p>\n<p>But, those are not the names of the columns.    <br \/>\nWhat I'm doing wrong?    <\/p>\n<p>Thanks a lot for your help.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1624312518567,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/445579\/the-columns-appears-with-another-name",
        "Challenge_link_count":3,
        "Challenge_participation_count":4,
        "Challenge_readability":13.3,
        "Challenge_reading_time":13.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"The columns appears with another name",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":95,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=12e650b3-31a2-48d9-b48f-0bd109b119b7\">@CASTANEDA RODRIGUEZ, DAMIAN  <\/a> Hello, I got you! Please click &quot;raw&quot; and use the url then to use the resource file. <a href=\"https:\/\/raw.githubusercontent.com\/rdiazconcha\/lil-azure-machine-learning-y-ai\/master\/modulo-2\/power-export_min.csv\">https:\/\/raw.githubusercontent.com\/rdiazconcha\/lil-azure-machine-learning-y-ai\/master\/modulo-2\/power-export_min.csv<\/a>    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/108210-image.png?platform=QnA\" alt=\"108210-image.png\" \/>    <\/p>\n<p>Then you should be good! Thanks.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":20.6,
        "Solution_reading_time":8.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":40.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"`TypeError: object of type 'NoneType' has no len()` happens when suggested [VSCode configuration for kedro](https:\/\/kedro.readthedocs.io\/en\/stable\/09_development\/01_set_up_vscode.html) is used for debugging. The error is due to commandline arguments being `None` when running pipeline directly through `run.py`.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/__main__.py\", line 45, in <module>\r\n    cli.main()\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/..\/debugpy\/server\/cli.py\", line 430, in main\r\n    run()\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/..\/debugpy\/server\/cli.py\", line 267, in run_file\r\n    runpy.run_path(options.target, run_name=compat.force_str(\"__main__\"))\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/pyzypad_example\/run.py\", line 75, in <module>\r\n    run_package()\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/pyzypad_example\/run.py\", line 71, in run_package\r\n    project_context.run()\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/context\/context.py\", line 725, in run\r\n    run_params=record_data, pipeline=filtered_pipeline, catalog=catalog\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/hooks.py\", line 286, in __call__\r\n    return self._hookexec(self, self.get_hookimpls(), kwargs)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/manager.py\", line 93, in _hookexec\r\n    return self._inner_hookexec(hook, methods, kwargs)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/manager.py\", line 87, in <lambda>\r\n    firstresult=hook.spec.opts.get(\"firstresult\") if hook.spec else False,\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 208, in _multicall\r\n    return outcome.get_result()\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 80, in get_result\r\n    raise ex[1].with_traceback(ex[2])\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 187, in _multicall\r\n    res = hook_impl.function(*args)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/kedro-mlflow\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py\", line 85, in before_pipeline_run\r\n    pipeline_name=run_params[\"pipeline_name\"],\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/kedro-mlflow\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py\", line 136, in _generate_kedro_command\r\n    if len(from_inputs) > 0:\r\nTypeError: object of type 'NoneType' has no len()\r\n```",
        "Challenge_closed_time":1601893.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601890192000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/78",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":22.2,
        "Challenge_reading_time":48.48,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":34,
        "Challenge_solved_time":null,
        "Challenge_title":"TypeError in _generate_kedro_command when debugging run in VSCode",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":212,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I see its fixed now so I'm closing this issue.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.5,
        "Solution_reading_time":0.54,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":10.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1490674180056,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"%Temp%",
        "Answerer_reputation_count":302.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How do i schedule Azure ML Experiments which is not deployed as web service?<\/p>\n\n<p>I have developed a Azure Experiment which imports data from on-premise database and exports data to SQL db. How can i schedule that to run weekly?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1501076325577,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45328657",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":3.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Scheduling Azure Machine Learning Experimnets",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":520.0,
        "Challenge_word_count":44,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1359784845430,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":400.0,
        "Poster_view_count":147.0,
        "Solution_body":"<p>You can use <strong>Azure PowerShell<\/strong> for automating this task, and use <strong>Windows Task Scheduler<\/strong> to schedule this script to run automatically.<\/p>\n\n<p>For Azure PowerShell,<\/p>\n\n<p>You may visit <a href=\"https:\/\/github.com\/hning86\/azuremlps\" rel=\"nofollow noreferrer\"><strong>this page<\/strong><\/a> to setup an Azure PowerShell script. It's a long journey, but it's worth it. Make sure to <strong><em>follow the prerequisites to be installed on your local PC (Azure-PowerShell v4.0.1)<\/em><\/strong>.<\/p>\n\n<p>For Windows Task Scheduler,<\/p>\n\n<p>Visit <a href=\"https:\/\/www.metalogix.com\/help\/Content%20Matrix%20Console\/SharePoint%20Edition\/002_HowTo\/004_SharePointActions\/012_SchedulingPowerShell.htm\" rel=\"nofollow noreferrer\"><strong>this link<\/strong><\/a> to schedule your created Azure PowerShell script to run at a scheduled\/repeated time.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.7,
        "Solution_reading_time":11.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":84.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,  <\/p>\n<p>I am trying to figure out the folder structure of Azure ML workspace in my storage account.  <br \/>\nI want to be able to delete old pipeline runs and experiments that have piled up in my workspace directly from Azure Storage Explorer without breaking the system.  <br \/>\nMy datastores and folder structure are as follows:  <\/p>\n<p>Datastore: workspaceartifactstore  <br \/>\nBlob container: azureml  <br \/>\nFolder structure:  <br \/>\n\u251c\u2500 ComputeRecord  <br \/>\n\u251c\u2500 Dataset  <br \/>\n\u251c\u2500 ExperimentRun  <br \/>\n\u251c\u2500 LocalUpload  <\/p>\n<p>Datastore: workspaceblobstore (Default)  <br \/>\nBlob container: azureml-blobstore-<em>(a series of numbers)<\/em>  <br \/>\nFolder structure:  <br \/>\n\u251c\u2500 azureml  <br \/>\n\u2502   \u251c\u2500\u2500 <em>(a series of numbers)<\/em>-setup  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 _tracer.py  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 azureml_globals.py  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 context_managers.py  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 job_prep.py  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 log_history_status.py  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 request_utilities.py  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 run_token_provider.py  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 utility_context_managers.py  <br \/>\n\u2502   \u251c\u2500\u2500 <em>(another series of numbers)<\/em>-setup  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 <em>sames files as above<\/em>  <\/p>\n<p>It would help if I understood what does each of these containers actually store.  <br \/>\nI already tried to delete all blobs stored in 'workspaceblobstore', but it didn't remove any pipeline or experiment from ML Studio.  <br \/>\nI have a few datasets registered in my workspace, and I don't want to delete them (nor unregister them).  <\/p>\n<p>Can I set a data retention policy on both containers in order to delete old blobs?  <br \/>\nCan I safely delete the blobs (folders) stored in 'workspaceartifactstore' too? Will they be recreated automatically when I run a new experiment?  <br \/>\nWhy are there two separate 'azureml' and 'azureml-blobstore-<em>(a series of numbers)<\/em>' containers? Is it possible to merge them?  <\/p>\n<p>Thanks.  <\/p>\n<p>Thank you.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647500925610,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/775834\/azure-ml-workspace-blob-structure-can-i-safely-del",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":23.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML workspace blob structure \/ Can I safely delete these blobs?",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":253,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. I've worked on a <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/issues\/60501\">similar inquiry<\/a> and the advise is to not delete data stored in default datastore to avoid weird errors. The option to easily delete experiment runs is on the roadmap. Here's a similar <a href=\"https:\/\/stackoverflow.com\/questions\/57497332\/how-to-delete-an-experiment-from-an-azure-machine-learning-workspace\">thread<\/a>. Feel free to raise and track feature request on <a href=\"https:\/\/feedback.azure.com\/d365community\/forum\/b9a0c624-ad25-ec11-b6e6-000d3a4f09d0\">ideas portal<\/a>.    <\/p>\n<blockquote>\n<p>According to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#prerequisites\">documentation<\/a>, when you create a workspace, an Azure blob container and an Azure file share are automatically registered as datastores to the workspace. They're named workspaceblobstore and workspacefilestore, respectively. The workspaceblobstore is used to store workspace artifacts and your machine learning experiment logs. It's also set as the default datastore and can't be deleted from the workspace. The workspacefilestore is used to store notebooks and R scripts authorized via compute instance.    <\/p>\n<\/blockquote>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.2,
        "Solution_reading_time":16.56,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":134.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1430233500800,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":212.0,
        "Answerer_view_count":25.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Currently I am exploring AWS sagemaker and I am facing a problem e.g. If I want to train my network on 1000s of epochs I cant stay active all the time. But as I logout my the notebook instance also stop execution. Is there any way to keep the instance active even after you logout ? <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1541090908080,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53105741",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":4.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Amazon Sagemaker Notebook instance stop execution as I logout",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2598.0,
        "Challenge_word_count":64,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1492699347027,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":37.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>Do you mean logging out of AWS console or your laptop? Your training job should still be running on the notebook instance whether you have notebook open or not. Notebook instance will always be active until you manually stop it[1]. You can always access the notebook instance again by opening the notebook through console.<\/p>\n\n<p>[1]<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StopNotebookInstance.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StopNotebookInstance.html<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.8,
        "Solution_reading_time":7.02,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":58.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I use DVC to track my media files. I use MacOS and I want\".DS_Store\" files to be ignored by DVC. According to DVC documentation I can achieve it with  <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvcignore\" rel=\"nofollow noreferrer\">.dvcignore<\/a>. I created <code>.dvcignore<\/code> file with \".DS_Store\" rule. However every time \".DS_Store\" is created <code>dvc status<\/code> still says that content has changed<\/p>\n\n<p>Here is the little test to reproduce my issue:<\/p>\n\n<pre><code>$ git init\n$ dvc init\n\n# create directory to store data\n# and track it's content with DVC\n$ mkdir data\n$ dvc add data\n\n# Ignore .DS_Store files created by MacOS\n$ echo \".DS_Store\" &gt; .dvcignore\n\n# create .DS_Store in data dir\n$ touch \"data\/.DS_Store\"\n<\/code><\/pre>\n\n<p>If I understand DVC documentation correctly then <code>dvc status<\/code> should print something like \"Pipeline is up to date. Nothing to reproduce\". However <code>dvc status<\/code> gives me:<\/p>\n\n<pre><code>data.dvc:\n        changed outs:\n                modified:           data\n<\/code><\/pre>\n\n<p>How I can really ignore \".DS_Store\" files?<\/p>\n\n<p><strong>UPDATE:<\/strong> The .dvcignore support noticeably improved in latest versions and the problem is no more relevant.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1559722020690,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1568948355943,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56456463",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":9.0,
        "Challenge_reading_time":15.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"Unable to ignore .DS_Store files in DVC",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":326.0,
        "Challenge_word_count":163,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1522254698710,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Russia",
        "Poster_reputation_count":784.0,
        "Poster_view_count":77.0,
        "Solution_body":"<p>The current implementation of <code>.dvcignore<\/code> is very limited. Read more on it <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvcignore\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Please, mention that you are interested in this feature here - <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1876\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc\/issues\/1876<\/a>. That would help our team to prioritize issues properly.<\/p>\n\n<p>The possible workaround for now would be to use one of these approaches - <a href=\"https:\/\/stackoverflow.com\/questions\/18015978\/how-to-stop-creating-ds-store-on-mac\">How to stop creating .DS_Store on Mac?<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":13.9,
        "Solution_reading_time":8.7,
        "Solution_score_count":3.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":60.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":6,
        "Challenge_body":"### Describe the bug a clear and concise description of what the bug is.\n\nWhen trying to install mlflow chart I'm trying to migrate from old mlflow version to the new one. I'm using `backendStore.databaseMigration: true` value for that. But mlflow pod failed to start with error:\r\n```\r\nmlflow.exceptions.MlflowException: Detected out-of-date database schema (found version c48cb773bb87, but expected cc1f77228345). Take a backup of your database, then run 'mlflow db upgrade <database_uri>' to migrate your database to the latest schema. NOTE: schema migration may result in database downtime - please consult your database's documentation for more detail.\r\n```\r\n\r\nFrom the looks of things migration Job should have `pre-install,pre-upgrade` hooks instead of `post-install,post-upgrade` but I can be wrong here. \r\n\r\nRunning Job from the chart manually with kubectl fixed this issue for me, but it will probably appear with the next release.\r\n\r\nThanks!\n\n### What's your helm version?\n\nv3.9.3\n\n### What's your kubectl version?\n\nv1.24.3\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.6.0\n\n### What happened?\n\n_No response_\n\n### What you expected to happen?\n\nDB migration job should run before mlflow pod upgrade. \n\n### How to reproduce it?\n\n1. Install mlflow with old DB schema (1.23.1)\r\n2. Try to upgrade with 0.6.0 helm chart\n\n### Enter the changed values of values.yaml?\n\n```\r\nmlflow:\r\n  nodeSelector:\r\n    redacted: Shared\r\n  \r\n  ingress:\r\n    enabled: true\r\n  \r\n  artifactRoot:\r\n    s3:\r\n      enabled: true\r\n      bucket: \"redacted\"\r\n      awsAccessKeyId: \"\"\r\n      awsSecretAccessKey: \"\"\r\n  \r\n  extraEnvVars:\r\n    AWS_DEFAULT_REGION: eu-central-1\r\n    MLFLOW_S3_ENDPOINT_URL: https:\/\/bucket.redacted.s3.eu-central-1.vpce.amazonaws.com\r\n  \r\n  backendStore:\r\n    databaseMigration: true\r\n    databaseConnectionCheck: true\r\n    mysql:\r\n      enabled: true\r\n      host: \"redacted.eu-central-1.rds.amazonaws.com\"\r\n      database: \"mlflow\"\r\n      user: \"\"\r\n      password: \"\"\r\n```\n\n### Enter the command that you execute and failing\/misfunctioning.\n\nhelm upgrade --install --values override.yaml --wait --create-namespace --atomic --timeout 15m0s -f secrets:\/\/secrets.yaml shared-services .\/shared-services\n\n### Anything else we need to know?\n\nChart was installed as a part of another umbrella chart",
        "Challenge_closed_time":1660908.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660748480000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/35",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":7.0,
        "Challenge_reading_time":27.79,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":8.0,
        "Challenge_repo_issue_count":39.0,
        "Challenge_repo_star_count":9.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":29,
        "Challenge_solved_time":null,
        "Challenge_title":"[mlflow] Migration Job should run before upgrade",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":277,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @faceless7171 \r\n\r\nThank you very much for reporting the issue. Yes, your suggestion can work. Let me create a PR and test it. Well, we can't use the pre-hook option because we need a configuration file for the DB connection. And I don't want to make secrets visible in the container.\r\n\r\n```console\r\n\u2502 Events:                                                                                                                                                          \u2502\r\n\u2502   Type     Reason       Age               From               Message                                                                                             \u2502\r\n\u2502   ----     ------       ----              ----               -------                                                                                             \u2502\r\n\u2502   Normal   Scheduled    22s               default-scheduler  Successfully assigned default\/mlflow-bzb8s to minikube                                              \u2502\r\n\u2502   Warning  FailedMount  7s (x6 over 22s)  kubelet            MountVolume.SetUp failed for volume \"migrations-config\" : configmap \"mlflow-migrations\" not found   \u2502\r\n\u2502   Warning  FailedMount  7s (x6 over 22s)  kubelet            MountVolume.SetUp failed for volume \"dbchecker\" : configmap \"mlflow-dbchecker\" not found            \u2502\r\n\u2502                                                                                                                                                                  \u2502\r\n```\r\n\r\nSo, we have another option. Maybe we can use the init container pattern for this purpose. Let me try. @all-contributors please add @faceless7171  for bug @burakince \n\nI've put up [a pull request](https:\/\/github.com\/community-charts\/helm-charts\/pull\/37) to add @faceless7171! :tada: Hi @faceless7171 \r\n\r\nCould you please try again with mlflow chart minimum 0.7.0 version? @burakince tested on 0.7.1 version. Everything is working now. Thanks for the fix.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.2,
        "Solution_reading_time":15.1,
        "Solution_score_count":null,
        "Solution_sentence_count":16.0,
        "Solution_word_count":161.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I can only draw image with [wandb.Image(Numpy.array()),] like this<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/discourse.aicrowd.com\/t\/maskrcnn-integrated-with-wandb-and-direct-submit-from-colab\/3961\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b48aa9bfc94603e638f56ff8452ed88b900f00db.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/discourse.aicrowd.com\/t\/maskrcnn-integrated-with-wandb-and-direct-submit-from-colab\/3961\" target=\"_blank\" rel=\"noopener nofollow ugc\" title=\"11:54AM - 20 November 2020\">AIcrowd Forum \u2013 20 Nov 20<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:600\/325;\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/e05a631c976b165047261523c356b3fa7e5eab41.gif\" class=\"thumbnail animated\" width=\"600\" height=\"325\"><\/div>\n\n<h3><a href=\"https:\/\/discourse.aicrowd.com\/t\/maskrcnn-integrated-with-wandb-and-direct-submit-from-colab\/3961\" target=\"_blank\" rel=\"noopener nofollow ugc\">MaskRCNN integrated with WandB and DIRECT SUBMIT FROM COLAB!<\/a><\/h3>\n\n  <p>Hi everyone!    @rohitmidha23 and me have been following this challenge for quite a while. We have written a starter notebook using MaskRCNN. We further integrate MaskRCNN with WandB which really helps to keep track of the various experiments that...<\/p>\n\n  <p>\n    <span class=\"label1\">Reading time: 1 mins \ud83d\udd51<\/span>\n      <span class=\"label2\">Likes: 17 \u2764<\/span>\n  <\/p>\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n<p>\nBut how can I draw many images with a bar like this<br>\n<a href=\"https:\/\/sooftware.io\/static\/fd6ffa741fe53de299a57e6a8852f68d\/f312c\/wandb_image.webp\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https:\/\/sooftware.io\/static\/fd6ffa741fe53de299a57e6a8852f68d\/f312c\/wandb_image.webp<\/a><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652175740664,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-draw-many-images-with-a-bar\/2391",
        "Challenge_link_count":7,
        "Challenge_participation_count":3,
        "Challenge_readability":17.9,
        "Challenge_reading_time":26.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"How to draw many images with a bar",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":186.0,
        "Challenge_word_count":144,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Oh\uff0cyes! I got it ,the step slider.<br>\nIt\u2019s on the left top of my panel. Thanks!<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/7477d027660f227b355c1b7090095a0ca0e72264.png\" alt=\"FireShot Capture 043 - warm-sea-50 - deepfillv2_512x512_dv5_0pv8_1 \u2013 Weights &amp; Biases_ - 192.168.23.40\" data-base62-sha1=\"gCk5gzKgcCCqUAxrDVgTMn9CtF2\" width=\"274\" height=\"249\"><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.7,
        "Solution_reading_time":5.42,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":30.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1506516283190,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Torino, TO, Italia",
        "Answerer_reputation_count":875.0,
        "Answerer_view_count":52.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello Stackoverflowers,<\/p>\n\n<p>I'm using azureml and I'm wondering if it is possible to log a confusion matrix of the xgboost model I'm training, together with the other metrics I'm already logging. Here's a sample of the code I'm using:<\/p>\n\n<pre><code>from azureml.core.model import Model\nfrom azureml.core import Workspace\nfrom azureml.core.experiment import Experiment\nfrom azureml.core.authentication import ServicePrincipalAuthentication\nimport json\n\nwith open('.\/azureml.config', 'r') as f:\n    config = json.load(f)\n\nsvc_pr = ServicePrincipalAuthentication(\n   tenant_id=config['tenant_id'],\n   service_principal_id=config['svc_pr_id'],\n   service_principal_password=config['svc_pr_password'])\n\n\nws = Workspace(workspace_name=config['workspace_name'],\n                        subscription_id=config['subscription_id'],\n                        resource_group=config['resource_group'],\n                        auth=svc_pr)\n\ny_pred = model.predict(dtest)\n\nacc = metrics.accuracy_score(y_test, (y_pred&gt;.5).astype(int))\nrun.log(\"accuracy\",  acc)\nf1 = metrics.f1_score(y_test, (y_pred&gt;.5).astype(int), average='binary')\nrun.log(\"f1 score\",  f1)\n\n\ncmtx = metrics.confusion_matrix(y_test,(y_pred&gt;.5).astype(int))\nrun.log_confusion_matrix('Confusion matrix', cmtx)\n<\/code><\/pre>\n\n<p>The above code raises this kind of error:<\/p>\n\n<pre><code>TypeError: Object of type ndarray is not JSON serializable\n<\/code><\/pre>\n\n<p>I already tried to transform the matrix in a simpler one, but another error occurred as before I logged a \"manual\" version of it (<code>cmtx = [[30000, 50],[40, 2000]]<\/code>).<\/p>\n\n<pre><code>run.log_confusion_matrix('Confusion matrix', [list([int(y) for y in x]) for x in cmtx])\n\nAzureMLException: AzureMLException:\n    Message: UserError: Resource Conflict: ArtifactId ExperimentRun\/dcid.3196bf92-4952-4850-9a8a-    c5103b205379\/Confusion matrix already exists.\n    InnerException None\n    ErrorResponse \n{\n    \"error\": {\n        \"message\": \"UserError: Resource Conflict: ArtifactId ExperimentRun\/dcid.3196bf92-4952-4850-9a8a-c5103b205379\/Confusion matrix already exists.\"\n    }\n}\n<\/code><\/pre>\n\n<p>This makes me think that I'm not properly handling the command <code>run.log_confusion_matrix()<\/code>. So, again, which is the best way I can log a confusion matrix to my azureml experiments?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591960207257,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62343056",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":29.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":null,
        "Challenge_title":"How to log a confusion matrix to azureml platform using python",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1418.0,
        "Challenge_word_count":216,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1506516283190,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Torino, TO, Italia",
        "Poster_reputation_count":875.0,
        "Poster_view_count":52.0,
        "Solution_body":"<p>I eventually found a solution thanks to colleague of mine. I'm hence answering myself, in order to close the question and, maybe, help somebody else.<\/p>\n<p>You can find the proper function in this link: <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#log-confusion-matrix-name--value--description----\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#log-confusion-matrix-name--value--description----<\/a>.<\/p>\n<p>Anyway, you also have to consider that, apparently, Azure doesn't work with the standard confusion matrix format returned by sklearn. It accepts indeed ONLY list of list, instead of numpy array, populated with numpy.int64 elements. So you also have to transform the matrix in a simpler format (for the sake of simplicity I used the nested list comprehension in the command below:<\/p>\n<pre><code>cmtx = metrics.confusion_matrix(y_test,(y_pred&gt;.5).astype(int))\ncmtx = {\n\n&quot;schema_type&quot;: &quot;confusion_matrix&quot;,\n&quot;parameters&quot;: params,\n &quot;data&quot;: {&quot;class_labels&quot;: [&quot;0&quot;, &quot;1&quot;],\n          &quot;matrix&quot;: [[int(y) for y in x] for x in cmtx]}\n}\nrun.log_confusion_matrix('Confusion matrix - error rate', cmtx)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.4,
        "Solution_reading_time":17.29,
        "Solution_score_count":6.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":126.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi, I'm confused with the automated labelling feature of SM. Usually when people label things it is to train their own models afterwards. Is the goal of this feature to replace the downstream ML model that would use the labelled dataset? some sort of code free computer vision system?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544611168000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1667965191088,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUp6wm80kUT0GZJp8meQtgTg\/why-sagemaker-ground-truth-automated-labelling",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.3,
        "Challenge_reading_time":4.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"why SageMaker Ground Truth automated labelling?",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":108.0,
        "Challenge_word_count":54,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Automated data labeling is labeling of data using machine learning. Amazon SageMaker Ground Truth will first select a random sample of data and send it to humans to be labeled. The results are then used to train a labeling model that attempts to label a new sample of raw data automatically. The labels are committed when the model can label the data with a confidence score that meets or exceeds a high threshold. Where the confidence score falls below this threshold, the data is sent to human labelers. Some of the data labeled by humans is used to generate a new training dataset for the labeling model, and the model is automatically retrained to improve its accuracy. This process repeats with each sample of raw data to be labeled. The labeling model becomes more capable of automatically labeling raw data with each iteration, and less data is routed to humans.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925583503,
        "Solution_link_count":0.0,
        "Solution_readability":10.6,
        "Solution_reading_time":10.58,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":150.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> to track and version data that is stored locally on the file system and in Azure Blob storage.<\/p>\n<p>My setup is as follows:<\/p>\n<ul>\n<li><p><code>DataProject1<\/code>, it uses a local file location as a remote therefore it does not require any authentication.<\/p>\n<\/li>\n<li><p><code>DataProject2<\/code>, it uses Azure Blob Storage as a remote, it is using sas_token for authentication, I can push pull data to\/from the remote when I'm within this project.<\/p>\n<\/li>\n<li><p><code>MLProject<\/code>, it uses dvc import to import data from <code>DataProjec1<\/code> and <code>DataProject2<\/code>.<\/p>\n<\/li>\n<\/ul>\n<p>When I run the import with the command against <code>DataProject1<\/code> everything works fine:<\/p>\n<p><code>dvc import -o 'data\/project1' 'https:\/\/company.visualstudio.com\/DefaultCollection\/proj\/_git\/DataProject1' 'data\/project1'<\/code> - Successful<\/p>\n<p>Howevever when I run a similar command against <code>DataProject2<\/code> the command fails:<\/p>\n<p><code>dvc import -o 'data\/project2' 'https:\/\/company.visualstudio.com\/DefaultCollection\/proj\/_git\/DataProject2' 'data\/project2'<\/code> - it fails with:<\/p>\n<blockquote>\n<p>ERROR: unexpected error - Operation returned an invalid status 'This\nrequest is not authorized to perform this operation using this\npermission.'  ErrorCode:AuthorizationPermissionMismatch.<\/p>\n<\/blockquote>\n<p>I would like to configure the <code>dvc import<\/code> so that I can set the required <code>sas_token<\/code> but I cannot find a way to do that.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1662648529560,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1662650337132,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73651050",
        "Challenge_link_count":3,
        "Challenge_participation_count":3,
        "Challenge_readability":16.4,
        "Challenge_reading_time":20.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"DVC imports authentication to blob storage",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":34.0,
        "Challenge_word_count":185,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1248452771430,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, United Kingdom",
        "Poster_reputation_count":3317.0,
        "Poster_view_count":296.0,
        "Solution_body":"<p>This happens since DVC is not using <code>MLProject<\/code>'s config when it clones and does <code>dvc fetch<\/code> in the <code>DataProject2<\/code> during the <code>import<\/code>. And it doesn't know where it can find the token (clearly, it's not in the Git repo, right?).<\/p>\n<p>There are a few ways to specify it: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#--system\" rel=\"nofollow noreferrer\"><code>global\/system<\/code> configs<\/a> and\/or <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#authenticate-with-environment-variables\" rel=\"nofollow noreferrer\">environment variables<\/a>.<\/p>\n<p>To implement the first option:<\/p>\n<p>On a machine where you do <code>dvc import<\/code>, you could create a remote in the <code>--global<\/code>, or <code>--system<\/code> configs with the same name and specify the token there. Global config fields will be merged with the config in the <code>DataProject2<\/code> repo when DVC is pulling data to import.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>dvc remote add --global &lt;DataProject2-remote-name&gt; azure:\/\/DataProject2\/storage\ndvc remote modify --global &lt;DataProject2-remote-name&gt; account_name &lt;name&gt;\ndvc remote modify --global &lt;DataProject2-remote-name&gt; sas_token &lt;token&gt;\n<\/code><\/pre>\n<p>The second option:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>export AZURE_STORAGE_SAS_TOKEN='mysecret'\nexport AZURE_STORAGE_ACCOUNT='myaccount'\n<\/code><\/pre>\n<p>Please give it a try, let me know if that works or not.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.1,
        "Solution_reading_time":20.14,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":158.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1254957460063,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"North Carolina, USA",
        "Answerer_reputation_count":2484.0,
        "Answerer_view_count":362.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm playing around with Azure ML Studio. Now I would like to add a new column in my dataset to calculate and in a further step to cluster my data. What's the best way to do it? I tried to add a column with sql (alter table) but it didn't work.<\/p>\n\n<p>btw. the \"add columns\" function only adds columns from another dataset...<\/p>\n\n<p>Thanks in advance!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1525258877877,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50133056",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":3.8,
        "Challenge_reading_time":4.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Add a column in Microsoft Azure ML Studio",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":517.0,
        "Challenge_word_count":72,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1463242510132,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>The \"Apply SQL Transformation\" module should be able to do it. For example, I have a dataset with an <em>age<\/em> column and here's the SQL to create another column called <em>double_age<\/em>:<\/p>\n\n<pre><code>select age, age * 2 as double_age from t1;\n<\/code><\/pre>\n\n<p>Which produces a dataset with just the <em>age<\/em> and <em>double_age<\/em> columns:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/uZblo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uZblo.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.3,
        "Solution_reading_time":6.83,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":60.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is there an option to export the Azure ML Designer to code so we can copy between workspaces?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646150939773,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/755142\/azure-ml-designer-export-code",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.4,
        "Challenge_reading_time":1.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML Designer: Export Code",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":22,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, this feature is currently not supported as mentioned on this <a href=\"https:\/\/stackoverflow.com\/questions\/60306240\/export-azure-ml-studio-designer-project-as-jupyter-notebook\">thread<\/a>. However, it's on the roadmap.  <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.4,
        "Solution_reading_time":3.1,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,  <\/p>\n<p>We are currently annotating images in a data labeling instance segmentation (polygon) project. Our images are rather blueish, which makes it difficult to use the polygon &quot;draw polygon region&quot; tool, which draws the polygon in blue.  <\/p>\n<p>Is it possible to change the color to, for example, black?  <\/p>\n<p>Thanks and BR,  <br \/>\nMaite<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1647528569083,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/776555\/azure-ml-data-labeling-change-polygon-color",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":5.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML data labeling change polygon color",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":62,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=54dc3768-e4dc-41b2-8290-e72ad5c207f3\">@Maite  <\/a>     <\/p>\n<p>Thanks for reaching out to us, I am sorry we are using only blue for the polygon color. I will forward your feedback to product team for future release.     <\/p>\n<p>One workaround may help with your scenario is, you can change the brightness to &quot;-100&quot; when you draw and revert the brightness back when you done as below screenshot. This will help to make things clear.     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/184541-image.png?platform=QnA\" alt=\"184541-image.png\" \/>    <\/p>\n<p>Hope this helps and thanks for the feedback again.     <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p><em>-Please kindly accept the answer if you feel helpful, thanks.<\/em>    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.7,
        "Solution_reading_time":9.77,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":100.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1501163272143,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":118.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Assume I have an Execute R Script that calculates multiple variables, say X and Y.\nIs it possible to save X as a dataset ds_X and Y as a dataset ds_Y?<\/p>\n\n<p>The problem is that there is only 1 output port available that needs to be mapped to a data.frame. Am I missing an option to add more output ports?\nSame problem for input ports. I may connect 2 of the \"Enter Data Manually\" modules to it, but what if I need 3? The current workaround is to put CSV files in a ZIP file and connect that. Are there easier solution?<\/p>\n\n<p><strong>Example of what i tried:<\/strong><\/p>\n\n<p>I tried adding ds_X and ds_Y to a list. The idea is to pass this list to multiple \"Execute R Script\" modules and use the required list elements there.\nMapping a list to an output port does not seem to work though:<\/p>\n\n<pre><code># Calculate lots of stuff - results are ds_X and ds_Y\nds_X &lt;- mtcars\nds_Y &lt;- cars\nout &lt;- list(ds_X, ds_Y)\n\nmaml.mapOutputPort(\"out\")\n<\/code><\/pre>\n\n<p>results in an error:<\/p>\n\n<pre><code>Error: Mapped variable must be of class type data.frame at this time.\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":4,
        "Challenge_created_time":1505987948480,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1505991363608,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46340959",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":6.3,
        "Challenge_reading_time":13.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Multiple Inputs\/Outputs from Execute R Script",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":754.0,
        "Challenge_word_count":193,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1389795136836,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":625.0,
        "Poster_view_count":36.0,
        "Solution_body":"<p>You can author custom R Modules. <\/p>\n\n<p>Here is some documentation: \n<a href=\"https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/04\/23\/build-your-own-r-modules-in-azure-ml\/\" rel=\"nofollow noreferrer\">https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/04\/23\/build-your-own-r-modules-in-azure-ml\/<\/a>\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-custom-r-modules\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-custom-r-modules<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":47.3,
        "Solution_reading_time":7.62,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to export the data from my batch flow with the use of the data export module. Tried multiple file shares but get the following error.    <\/p>\n<p>User program failed with UserError: ScriptExecutionException was caused by WriteStreamsException.    <br \/>\n  WriteStreamsException was caused by UnexpectedException.    <br \/>\n    Unexpected exception while writing files with writer 'delimited'.  <br \/>\n      StreamAccessException was caused by NotFoundException.  <br \/>\n        File Share '[REDACTED]' does not exist at '[REDACTED]'.  <br \/>\n| session_id=e1ee8699-3a94-4ea6-ab5d-f5bb945d56f3    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/32402-image.png?platform=QnA\" alt=\"32402-image.png\" \/>    <\/p>\n<p>The file share name is the Azure named file share name or is this something else. Cannot find an eample.    <\/p>\n<p>The export works to local ML workspace, but this doesn't accept folders creation.    <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1602687603067,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/126502\/azure-machine-learning-data-export-module-failure",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.4,
        "Challenge_reading_time":12.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure machine learning data export module failure",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":116,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello,    <\/p>\n<p>Thanks for reaching out to us. The file share name is the name of FILE_SHARE_CONTAINER     <\/p>\n<p>Please refer to below document for more details:    <\/p>\n<p>Export data module: <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/export-data\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/export-data<\/a>    <\/p>\n<p>Data storage - Azure File Share: <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#azure-file-share\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#azure-file-share<\/a>    <\/p>\n<p>Please let me know if you have more questions.    <\/p>\n<p>Regards,    <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.9,
        "Solution_reading_time":9.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Currently, it will display always display\r\n`========== Make your selection, Press \"h\" for help ==========`\r\neven if there is no selection to make since the list of files is empty\r\n\r\nhttps:\/\/github.com\/DAGsHub\/fds\/blob\/a8fea54f59131d3ddea4df5184adeee3ecc9998f\/fds\/services\/dvc_service.py#L119",
        "Challenge_closed_time":1622551.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622117855000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/37",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":4.48,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":139.0,
        "Challenge_repo_star_count":357.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Only display the DVC add prompt if there is anything to add",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":40,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Fixed in #46 ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-2.7,
        "Solution_reading_time":0.15,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1394078070848,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":913.0,
        "Answerer_view_count":88.0,
        "Challenge_adjusted_solved_time":23.4023241667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there anything in the Python API that lets you alter the artifact subdirectories? For example, I have a .json file stored here:<\/p>\n<p><code>s3:\/\/mlflow\/3\/1353808bf7324824b7343658882b1e45\/artifacts\/feature_importance_split.json<\/code><\/p>\n<p>MlFlow creates a <code>3\/<\/code> key in s3. Is there a way to change to modify this key to something else (a date or the name of the experiment)?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1626152546053,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68356746",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.0,
        "Challenge_reading_time":5.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Changing subdirectory of MLflow artifact store",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1493.0,
        "Challenge_word_count":57,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1394078070848,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":913.0,
        "Poster_view_count":88.0,
        "Solution_body":"<p>As I commented above, yes, <code>mlflow.create_experiment()<\/code> does allow you set the artifact location using the <code>artifact_location<\/code> parameter.<\/p>\n<p>However, sort of related, the problem with setting the <code>artifact_location<\/code> using the <code>create_experiment()<\/code> function is that once you create a experiment, MLflow will throw an error if you run the <code>create_experiment()<\/code> function again.<\/p>\n<p>I didn't see this in the docs but it's confirmed that if an experiment already exists in the backend-store, MlFlow will not allow you to run the same <code>create_experiment()<\/code> function again. And as of this post, MLfLow does not have <code>check_if_exists<\/code> flag or a <code>create_experiments_if_not_exists()<\/code> function.<\/p>\n<p>To make things more frustrating, you cannot set the <code>artifcact_location<\/code> in the <code>set_experiment()<\/code> function either.<\/p>\n<p>So here is a pretty easy work around, it also avoids the &quot;ERROR mlflow.utils.rest_utils...&quot; stdout logging as well.\n:<\/p>\n<pre><code>import os\nfrom random import random, randint\n\nfrom mlflow import mlflow,log_metric, log_param, log_artifacts\nfrom mlflow.exceptions import MlflowException\n\ntry:\n    experiment = mlflow.get_experiment_by_name('oof')\n    experiment_id = experiment.experiment_id\nexcept AttributeError:\n    experiment_id = mlflow.create_experiment('oof', artifact_location='s3:\/\/mlflow-minio\/sample\/')\n\nwith mlflow.start_run(experiment_id=experiment_id) as run:\n    mlflow.set_tracking_uri('http:\/\/localhost:5000')\n    print(&quot;Running mlflow_tracking.py&quot;)\n\n    log_param(&quot;param1&quot;, randint(0, 100))\n    \n    log_metric(&quot;foo&quot;, random())\n    log_metric(&quot;foo&quot;, random() + 1)\n    log_metric(&quot;foo&quot;, random() + 2)\n\n    if not os.path.exists(&quot;outputs&quot;):\n        os.makedirs(&quot;outputs&quot;)\n    with open(&quot;outputs\/test.txt&quot;, &quot;w&quot;) as f:\n        f.write(&quot;hello world!&quot;)\n\n    log_artifacts(&quot;outputs&quot;)\n<\/code><\/pre>\n<p>If it is the user's first time creating the experiment, the code will run into an AttributeError since <code>experiment_id<\/code> does not exist and the <code>except<\/code> code block gets executed creating the experiment.<\/p>\n<p>If it is the second, third, etc the code is run, it will only execute the code under the <code>try<\/code> statement since the experiment now exists. Mlflow will now create a 'sample' key in your s3 bucket. Not fully tested but it works for me at least.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1626236794420,
        "Solution_link_count":1.0,
        "Solution_readability":12.7,
        "Solution_reading_time":32.42,
        "Solution_score_count":1.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":267.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen an error is raised during training with `MLFlowLogger`, status of a `mlflow.entities.run_info.RunInfo` object should be updated to be 'FAILED', while it remains 'RUNNING'.\r\nDue to the problem, when you look at MLFlow Tracking Server screen, It seams as if training is still in progress even though it has been terminated with an error.\r\n\r\n### To Reproduce\r\n\r\n<!--\r\nPlease reproduce using the BoringModel!\r\n\r\nYou can use the following Colab link:\r\nhttps:\/\/colab.research.google.com\/drive\/1HvWVVTK8j2Nj52qU4Q4YCyzOm0_aLQF3?usp=sharing\r\nIMPORTANT: has to be public.\r\n\r\nor this simple template:\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n\r\nIf you could not reproduce using the BoringModel and still think there's a bug, please post here\r\nbut remember, bugs with code are fixed faster!\r\n-->\r\n```py\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.loggers import MLFlowLogger ##### added #####\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        raise Exception ##### added #####\r\n        return {\"loss\": loss}\r\n        \r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    \r\n    mlf_logger = MLFlowLogger() ##### added #####\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        # enable_model_summary=False,\r\n        logger=mlf_logger ##### added #####\r\n    )\r\n    try:\r\n        trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n        trainer.test(model, dataloaders=test_data)\r\n    finally:\r\n        print(trainer.logger.experiment.get_run(trainer.logger._run_id).info.status) # This should be 'FAILED'\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\nStatus of each MLFlow's run is correctly updated when `pl.Trainer.fit` failed.\r\n\r\n### Environment\r\n\r\n<!--\r\nPlease copy and paste the output from our environment collection script:\r\nhttps:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/requirements\/collect_env_details.py\r\n(For security purposes, please check the contents of the script before running it)\r\n\r\nYou can get the script and run it with:\r\n```bash\r\nwget https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/requirements\/collect_env_details.py\r\npython collect_env_details.py\r\n```\r\n\r\nYou can also fill out the list below manually.\r\n-->\r\n\r\n- PyTorch Lightning Version: 1.4.9\r\n- MLFlow Version: 1.12.0\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Challenge_closed_time":1640642.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636290176000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/10397",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":45.61,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":null,
        "Challenge_title":"`MLFlowLogger` does not update its status when `trainer.fit` failed",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":338,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":2.67,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":36.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"```\r\ntests\/conftest.py:4: in <module>\r\n    from rikai.spark.sql import init\r\n..\/rikai\/python\/rikai\/__init__.py:19: in <module>\r\n    from rikai.spark.sql.codegen import mlflow_logger as mlflow\r\n..\/rikai\/python\/rikai\/spark\/sql\/codegen\/mlflow_logger.py:19: in <module>\r\n    import mlflow\r\nE   ModuleNotFoundError: No module named 'mlflow'\r\n```",
        "Challenge_closed_time":1617994.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617993087000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/eto-ai\/rikai\/issues\/207",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":10.0,
        "Challenge_reading_time":4.61,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":715.0,
        "Challenge_repo_star_count":127.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Leaking mlflow dependency",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":30,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1313736279736,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA",
        "Answerer_reputation_count":207794.0,
        "Answerer_view_count":16864.0,
        "Challenge_adjusted_solved_time":26.7536241667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a data set with 16 columns and 100,000 rows which I'm trying to prepare for a matrix-factorization training. I'm using the following code to split it and turn it into a sparse matrix.<\/p>\n\n<pre><code>X=data.drop([data.columns[0]],axis='columns')\ny=data[[1]]\nX=lil_matrix(100000,15).astype('float32')\ny=np.array(y).astype('float32')\nX\n<\/code><\/pre>\n\n<p>But when I run it, I get this error:<\/p>\n\n<blockquote>\n  <p>&lt;1x1 sparse matrix of type ''  with 1 stored\n  elements in LInked List format> .<\/p>\n<\/blockquote>\n\n<p>When I try to plug it into a training\/testing split it gives me further errors:<\/p>\n\n<blockquote>\n  <p>Found input variables with inconsistent numbers of samples: [1,\n  100000]<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":6,
        "Challenge_created_time":1562690689400,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1562705343183,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56957206",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":10.0,
        "Challenge_reading_time":9.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"how to turn a matrix into a sparse matrix and protobuf it",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":498.0,
        "Challenge_word_count":104,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545524391676,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kansas City, MO, USA",
        "Poster_reputation_count":91.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>Your linked <code>notebook<\/code> is creating a 'blank' sparse matrix, and setting selected elements from data it reads from a <code>csv<\/code>.<\/p>\n\n<p>A simple example of this:<\/p>\n\n<pre><code>In [565]: from scipy import sparse                                                                           \nIn [566]: M = sparse.lil_matrix((10,5), dtype=float)                                                         \nIn [567]: M                                                                                                  \nOut[567]: \n&lt;10x5 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 0 stored elements in LInked List format&gt;\n<\/code><\/pre>\n\n<p>Note that I use <code>(10,5)<\/code> to specify the matrix shape.  The () matter!  That's why I stressed reading the <code>docs<\/code>.  In the link the relevant line is:<\/p>\n\n<pre><code>X = lil_matrix((lines, columns)).astype('float32')\n<\/code><\/pre>\n\n<p>Now I can set a couple elements, just as I would an dense array:<\/p>\n\n<pre><code>In [568]: M[1,2] = 12.3                                                                                      \nIn [569]: M[3,1] = 1.1                                                                                       \nIn [570]: M                                                                                                  \nOut[570]: \n&lt;10x5 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 2 stored elements in LInked List format&gt;\n<\/code><\/pre>\n\n<p>I can use <code>toarray<\/code> to display the matrix as a dense array (don't try this with large dimensions).<\/p>\n\n<pre><code>In [571]: M.toarray()                                                                                        \nOut[571]: \narray([[ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. , 12.3,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  1.1,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ]])\n<\/code><\/pre>\n\n<hr>\n\n<p>If I omit the (), it makes a (1,1) matrix with just one element, the first number.<\/p>\n\n<pre><code>In [572]: sparse.lil_matrix(10,5)                                                                            \nOut[572]: \n&lt;1x1 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 1 stored elements in LInked List format&gt;\nIn [573]: _.A                                                                                                \nOut[573]: array([[10]], dtype=int64)\n<\/code><\/pre>\n\n<p>Look again at your code.  You set the <code>X<\/code> value twice, once it is a dataframe.  The second time is this bad <code>lil<\/code> initialization.  The second time does not make use of the first <code>X<\/code>.<\/p>\n\n<pre><code>X=data.drop([data.columns[0]],axis='columns')\n...\nX=lil_matrix(100000,15).astype('float32')\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1562801656230,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":25.52,
        "Solution_score_count":0.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":283.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1254957460063,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"North Carolina, USA",
        "Answerer_reputation_count":2484.0,
        "Answerer_view_count":362.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In Azure Machine Learning studio I need to convert a column of data that has three categorical values 'yes', 'no' and 'maybe', and wish to combine the 'no' and 'maybe' values as just 'no'. <\/p>\n\n<p>I can do this easily using SQL, R, or Python but for these purposes I need to show if it is possible to do this without using these languages. I can't seem to find a way to do this. <\/p>\n\n<p>Does anyone have any ideas? I'm fine if the answer is no but I don't want to say it's not possible if it is. <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1527572052450,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50576929",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":6.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Replacing values in dataset within Azure Machine Learning Studio",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":484.0,
        "Challenge_word_count":108,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1367782461047,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1647.0,
        "Poster_view_count":321.0,
        "Solution_body":"<p>It can be done! :)<\/p>\n\n<p>You would just use the \"Group Categorical Values\" module. Choose the column that has the data you want to group, and you can set the values like the following:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/UGhrR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/UGhrR.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>What's going on here is that the default, which will get used if the other levels aren't caught, is set to \"yes\". Then when any values are \"no\", or \"maybe\", it gets grouped into a category of \"no\".<\/p>\n\n<p>However, this will error unless you make that column a categorical type, so you would need to use the \"Edit Metadata\" module to do that.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/45s2Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/45s2Q.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The example I used is <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Replace-Values-in-Dataset\" rel=\"nofollow noreferrer\">published to the gallery<\/a>, if you need to reference it.<\/p>\n\n<p>If you need more info, just let me know.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":9.1,
        "Solution_reading_time":14.41,
        "Solution_score_count":3.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":142.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following the mnist-2 guide from the aws github documentation to implement my own training job <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving<\/a>. I have wrote my code using a similar structure, but I would like to visualise the training and validation metrics from Cloudwatch while the job is running. Do I need to manually specify the metrics I am trying to observe? The AWS guide states &quot;<em>SageMaker automatically parses the logs for metrics that built-in algorithms emit and sends those metrics to CloudWatch.<\/em>&quot; I am only using Tensorflow's training and validation accuracy and loss metrics, which I am not sure if they are built-in, or if I need to call them manually.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616769637850,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66819026",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.4,
        "Challenge_reading_time":12.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Output model metrics to Cloudwatch",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":475.0,
        "Challenge_word_count":115,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1533753580750,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":344.0,
        "Poster_view_count":63.0,
        "Solution_body":"<p>If you are not using a built-in algorithm, like in the example you linked, you have to define your metrics when you create the training job. You have to define regex expressions to grab from the logs the metric values, then cloudwatch will plot for you. The x axis will be the timestamp, you cannot change it.\nBasically just run your traning job and observe how the metrics are outputted, then you can build the appropriate regex. For example, since I am using coco metrics in tensorflow which periodically produce this:<\/p>\n<pre><code>INFO:tensorflow:Saving dict for global step 1109: DetectionBoxes_Precision\/mAP = 0.111895345, DetectionBoxes_Precision\/mAP (large) = 0.12102994, DetectionBoxes_Precision\/mAP (medium) = 0.050807837, DetectionBoxes_Precision\/mAP (small) = -1.0, DetectionBoxes_Precision\/mAP@.50IOU = 0.33130914, DetectionBoxes_Precision\/mAP@.75IOU = 0.03787096, DetectionBoxes_Recall\/AR@1 = 0.18493989, DetectionBoxes_Recall\/AR@10 = 0.36792925, DetectionBoxes_Recall\/AR@100 = 0.48543888, DetectionBoxes_Recall\/AR@100 (large) = 0.5131599, DetectionBoxes_Recall\/AR@100 (medium) = 0.21598063, DetectionBoxes_Recall\/AR@100 (small) = -1.0, Loss\/classification_loss = 0.8041124, Loss\/localization_loss = 0.35313264, Loss\/regularization_loss = 0.15211834, Loss\/total_loss = 1.30936, global_step = 1109, learning_rate = 0.28119853, loss = 1.30936\n<\/code><\/pre>\n<p>I use to grab the total loss for example:<\/p>\n<pre><code>INFO.*Loss\\\/total_loss = ([0-9\\.]+) \n<\/code><\/pre>\n<p>That's it, cloudwatch automatically plot the total_loss in time.<\/p>\n<p>You can define metrics either in the console or in the notebook, like this (just an example from my code):<\/p>\n<pre><code>metrics = [{'Name': 'Loss', 'Regex': 'loss: ([0-9\\.]+)'},\n           {'Name': 'Accuracy', 'Regex': 'acc: ([0-9\\.]+)'},\n           {'Name': 'Epoch', 'Regex': 'Epoch ([0-9\\.]+)'},\n           {'Name': 'Validation_Acc', 'Regex': 'val_acc: ([0-9\\.]+)'},\n           {'Name': 'Validation_Loss', 'Regex': 'val_loss: ([0-9\\.]+)'}]\n\ntf_estimator = TensorFlow(entry_point='training.py', \n                          role=get_execution_role(),\n                          train_instance_count=1, \n                          train_instance_type='ml.p2.xlarge',\n                          train_max_run=172800,\n                          output_path=s3_output_location,\n                          framework_version='1.12',\n                          py_version='py3',\n                          metric_definitions = metrics,\n                          hyperparameters = hyperparameters)\n<\/code><\/pre>\n<p>In order to test your regex, you can use a tool like <a href=\"https:\/\/regex101.com\/\" rel=\"nofollow noreferrer\">this<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.0,
        "Solution_reading_time":31.42,
        "Solution_score_count":1.0,
        "Solution_sentence_count":31.0,
        "Solution_word_count":239.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi:  <\/p>\n<p>I wonder when will the classic Machine Learning Studio retire?  <\/p>\n<p>Also in order to save my data and file, any preparation or migration should be done to avoid any loss?  <\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648415916797,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/789066\/machine-learning-studio",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.9,
        "Challenge_reading_time":2.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"machine learning Studio",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":37,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=3f0f48d2-f878-404d-bae7-0861ffefc027\">@dontbelazy  <\/a>     <\/p>\n<p>Thanks for reaching out to us, I think you are mentioning Azure Machine Learning Studio(classic). Machine Learning Studio (classic) will retire on 31 August 2024.     <\/p>\n<p>From now through 31 August 2024, you can continue to use the existing Machine Learning Studio (classic). Beginning 1 December 2021, you will not be able to create new Machine Learning Studio (classic) resources.    <\/p>\n<p>Required action to avoid loss:     <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/migrate-overview\">Follow these steps<\/a> to transition using Azure Machine Learning before 31 August 2024. Pricing may be subject to change, please view <a href=\"https:\/\/azure.microsoft.com\/en-us\/pricing\/details\/machine-learning\/#:%7E:text=Consumed%20Azure%20resources%20%28e.g.%20compute%2C%20storage%29%20%28No%20Azure,%24-%20%2B%20per%20vCPU%20hour%20Edition%3A%20Basic%20Enterprise\">pricing<\/a> here.    <\/p>\n<p>Hope this helps!    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p><em>-Please kindly accept the answer if you feel helpful, thanks!<\/em>    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.2,
        "Solution_reading_time":15.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":117.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello , I'm new to azureML and I have a question about Azure ML designer , after creating  a workflow (drag and drop components , let's say problem of linear  regression ) can I export, download a file of the  metadata of the workflow that  contains the names of components used and its parameters.  <br \/>\nif it's possible can automate the process of creating a workflow in azureML designer by using already exsiting metadata file (json, xml , yaml ... ) similar to the one mentioned previously.   <\/p>\n<p>if there any other services in azure that's capable of solving this issue please feel free to mention it   <\/p>\n<p>thank youu.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645272036680,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/742573\/export-metadata-file-of-componenets-and-its-parame",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":8.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Export metadata file of componenets and its parameters  of trained and submitted model  in AzureML  designer",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":118,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=ca3ed354-350d-4b5d-be80-486f8db9ec5b\">@Achraf DRIDI  <\/a> Yes, you can export your designer experiment as a pipeline. The option to export is available from the designer from the top right hand corner. This is basically a cli command that helps you export the experiment in two ways.    <\/p>\n<ol>\n<li> Shallow     <\/li>\n<li> Deep    <\/li>\n<\/ol>\n<p><strong>UPDATE<\/strong>    <br \/>\nThe feature mentioned above is in private preview and not available to all users. Exact ETA not available at this point of time.    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.8,
        "Solution_reading_time":11.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":105.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nWe have been using pycaret 2.2 for the model training procedure and registration to the mlflow server. (python based) My company uses a managed version of this in Azure Databricks. After the registration has been completed, we call the calibrated algorithm in a separate notebook and are trying to score new data with a binary response 0|1. We would also like to leverage the scikit learn function \"predict_model\" to create the probabilities in addition to the predicted value. This is not working in pycaret and appears to be a bug of some sort. It is also important to note that we are able to see the \"predict_model\" during the model training but not when we call the algorithm for a separate scoring function. \n\n### Reproducible Example\n\n```python\n# import mlflow.sklearn\r\n# model = mlflow.sklearn.load_model(production_algorithm)\r\n# model.predict_prob(X)\n```\n\n\n### Expected Behavior\n\nwe should see the probabilities model.predict_prob(X) but this code errors out. Another example would be the following: predictions_prob = production_algorithm.predict_prob(pd.DataFrame(X))\r\n\n\n### Actual Results\n\n```python-traceback\nthe end result of the prediction should be a numeric value between 0 and 1. Ex. 0.4278\n```\n\n\n### Installed Versions\n\n<details>\r\nSystem:\r\n    python: 3.8.10 (default, Mar 15 2022, 12:22:08)  [GCC 9.4.0]\r\nexecutable: \/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-ca5e1db9-faed-4291-83c9-f55dfcbb8112\/bin\/python\r\n   machine: Linux-5.4.0-1083-azure-x86_64-with-glibc2.29\r\n\r\nPyCaret required dependencies:\r\n                 pip: 21.0.1\r\n          setuptools: 52.0.0\r\n             pycaret: 3.0.0.rc3\r\n             IPython: 7.22.0\r\n          ipywidgets: 7.7.1\r\n                tqdm: 4.64.0\r\n               numpy: 1.21.6\r\n              pandas: 1.4.3\r\n              jinja2: 3.1.2\r\n               scipy: 1.6.2\r\n              joblib: 1.1.0\r\n             sklearn: 1.1.1\r\n                pyod: Installed but version unavailable\r\n            imblearn: 0.8.1\r\n   category_encoders: 2.5.0\r\n            lightgbm: 3.3.2\r\n               numba: 0.55.1\r\n            requests: 2.28.1\r\n          matplotlib: 3.4.2\r\n          scikitplot: 0.3.7\r\n         yellowbrick: 1.4\r\n              plotly: 5.9.0\r\n             kaleido: 0.2.1\r\n         statsmodels: 0.12.2\r\n              sktime: 0.11.4\r\n               tbats: Installed but version unavailable\r\n            pmdarima: 1.8.4\r\n              psutil: 5.9.1\r\n<\/details>\r\n",
        "Challenge_closed_time":1669247.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658846256000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2801",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":32.33,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":48,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG]: pycaret + mlflow integration does not allow probabilities for classification and binary response models",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":316,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@DerekKane We need more details than `code error out` to be able to help. Just tested this scenario with latest pycaret==3.0.0rc4 and it works fine.\r\n\r\nIf you are still facing issue, please feel free to open new ticket.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.5,
        "Solution_reading_time":2.63,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":38.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>You can add files to a Sagemaker notebook instance by using the &quot;upload&quot; button.  When you do this, to which directory are the files uploaded, and how can I view this in the command line?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596133200537,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63179080",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":3.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"When I upload data into an Sagemaker Notebook instance, in which directory does the data live and how do I access it?",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":3454.0,
        "Challenge_word_count":56,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1446748214680,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":306.0,
        "Poster_view_count":44.0,
        "Solution_body":"<p>SageMaker Notebooks home is on <code>\/home\/ec2-user\/SageMaker<\/code><\/p>\n<ul>\n<li>Everything you send to <code>\/home\/ec2-user\/SageMaker<\/code> will be visible in\nthe Jupyter home page<\/li>\n<li>Everything you upload in the Jupyter home page\nwill be visible in the terminal via <code>ls \/home\/ec2-user\/SageMaker<\/code><\/li>\n<li>The content of <code>\/home\/ec2-user\/SageMaker<\/code> is persisted in a storage volume called the &quot;ML Storage Volume&quot;, that is charged additionally to the\ninstance compute pricing and defaults at 5GB. It can be up to 16TB in\nsize. Content saved there stays persisted even when you switch off\nthe notebook instance. On the other hand, anything you save anywhere\nelse will be lost when you switch off the instance<\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.5,
        "Solution_reading_time":9.64,
        "Solution_score_count":5.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":105.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1363352099923,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Washington, DC, USA",
        "Answerer_reputation_count":828.0,
        "Answerer_view_count":530.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create a custom Image Classifier in Amazon SageMaker. It is giving me the following error:<\/p>\n\n<p><code>\"ClientError: Data download failed:NoSuchKey (404): The specified key does not exist.\"<\/code><\/p>\n\n<p>I'm assuming this means one of the pictures in my <code>.lst<\/code> file is missing from the directory. Is there some way to find out which <code>.lst<\/code> listing it is specifically having trouble with?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1534309681237,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1534322930320,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51853249",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":5.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Error Tracking in Amazon SageMaker",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":305.0,
        "Challenge_word_count":67,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1363352099923,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Washington, DC, USA",
        "Poster_reputation_count":828.0,
        "Poster_view_count":530.0,
        "Solution_body":"<p>Upon further examination (of the log files), it appears the issue does not lie with the .lst file itself, but with the image files it was referencing (which now leaves me wondering why AWS doesn't just say that instead of saying the .lst file is corrupt). I'm going through the image files one-by-one to verify they are correct, hopefully that will solve the problem.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":4.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Azure Machine Learning Studio to design pipelines to analyze data.  <br \/>\nIs there any possibility to export data to sharepoint?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1631064917827,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/543361\/azure-machine-learning-(automl)-export-data-to-sha",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":2.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning (AutoML) export data to SharePoint",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":30,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi @MiaZhangWHQWistron-2092     <br \/>\nPer my research, there is no way to export data from Azure Machine Learning Studio to SharePoint directly.    <\/p>\n<p>As an alternative, you could export data to Azure SQL database first:    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/export-to-azure-sql-database\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/export-to-azure-sql-database<\/a>    <\/p>\n<p>Then export data from Azure SQL database to SharePoint list:    <br \/>\n<a href=\"https:\/\/social.technet.microsoft.com\/wiki\/contents\/articles\/39170.azure-sql-db-with-sharepoint-online-as-external-list-using-business-connectivity-services.aspx\">https:\/\/social.technet.microsoft.com\/wiki\/contents\/articles\/39170.azure-sql-db-with-sharepoint-online-as-external-list-using-business-connectivity-services.aspx<\/a>    <\/p>\n<hr \/>\n<p>If an Answer is helpful, please click &quot;<strong>Accept Answer<\/strong>&quot; and upvote it.    <\/p>\n<p>Note: Please follow the steps in <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/support\/email-notifications\">our documentation<\/a> to enable e-mail notifications if you want to receive the related email notification for this thread.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":22.2,
        "Solution_reading_time":16.76,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":92.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"`ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden`\r\n\r\n`dvc pull` needs mantis creds so a reader will not be able to follow. we need to make the bucket public and read only.",
        "Challenge_closed_time":1668173.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634634148000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/MantisAI\/Rasa-MLOPs\/issues\/5",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":9.3,
        "Challenge_reading_time":3.57,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":14.0,
        "Challenge_repo_star_count":2.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Remote storage is not publicly accessible (dvc pull fails)",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":46,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"So:\r\n1. You will need to have aws credentials set up with `aws configure`, having installed awscli (which is in the virtualenv)\r\n2. I'm having some issues getting the mantisnlp-public bucket to be accessible to dvc with a non mantis aws profile. I don't know if this is related but did you try `--acl public-read`? I had some problems with public buckets in grants tagger and for me it was resolved by adding this flag. example https:\/\/github.com\/wellcometrust\/grants_tagger\/blob\/970abbc63b448c4d14d7b70fa13ca29760a897ce\/Makefile#L94 I've done this at the bucket level, not at the individual object level, because they are added by dvc. It _should_ be working... btw this issue is probably badly named because:\r\n1. You only need to set `AWS_PROFILE` if you have more than one set of aws credentials\r\n2. You can also set `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` in the folder to the same effect, and users can decide how best to do this.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.4,
        "Solution_reading_time":11.62,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":149.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"Explicitly creating a CometLogger instance and passing it to Trainer using trainer(logger=my_comet_logger) raises a NotImplementedError because CometLogger does not implement the name() and version() class methods.\r\n\r\nBelow is the traceback:\r\n`\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 126, in <module>\r\n    trainer.fit(model)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 351, in fit\r\n    self.single_gpu_train(model)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/dp_mixin.py\", line 77, in single_gpu_train\r\n    self.run_pretrain_routine(model)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 471, in run_pretrain_routine\r\n    self.train()\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 60, in train\r\n    self.run_training_epoch()\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 99, in run_training_epoch\r\n    output = self.run_training_batch(batch, batch_nb)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 255, in run_training_batch\r\n    self.main_progress_bar.set_postfix(**self.training_tqdm_dict)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 309, in training_tqdm_dict\r\n    if self.logger is not None and self.logger.version is not None:\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/logging\/base.py\", line 76, in version\r\n    raise NotImplementedError(\"Sub-classes must provide a version property\")\r\n`\r\n\r\n",
        "Challenge_closed_time":1573531.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1573092782000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/470",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":22.1,
        "Challenge_reading_time":25.49,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":null,
        "Challenge_title":"CometLogger does not implement name() and version() class methods",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":123,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have approximately 100k rows of text data (initially PDF documents that have been OCR). Most are rows of less than 5000 characters. Each of the source documents are addressed to some department. These are typically in the form of the below examples where the target department would 'Urology' (there are several departments).    <\/p>\n<ul>\n<li> Urologly Department  <\/li>\n<li> Urologly Clinic  <\/li>\n<li> Urology Out Patients  <\/li>\n<li> Urology  <\/li>\n<li> Dear urology team  <\/li>\n<\/ul>\n<p>I have read a bit on ML Text Analysis and it seems I should be able to make a pretty good model by reviewing several hundred documents for each department (I have built an App to help me do this) and manually Classifying those documents. Some documents may mention urology but are actually addressed to another department. Typically the addressed department text is at the top third (first 3-7 lines) of the text body.               <\/p>\n<p>I cannot use any online tools, i.e. I can't upload any of the Document text to servers to process I need a client side library. I have read and completed several tutorials using the ML.net but these are pretty basic (sentiment, entity detection without any initial training), and read an excellent blog at <a href=\"+https:\/\/monkeylearn.com\/blog\/how-to-create-text-classifiers-machine-learning\/\">MonkeyLearn<\/a>: which seems to acknowledge that can do what I imagine I should be able to do.    <\/p>\n<p>So can anybody point me in the right direction, can I use some offline Microsoft client library to complete my task? Is there some other Open Source client library i should look at. Will I have to learn Go, or python to complete the task (currently a C# dev).   <\/p>\n<p>Note: I could get fairly good matches simply using SQL Text search and a bit of C# with plenty of hard coded rules, but I thought I'd try ML -- however its a nest of complications at the moment   and i am going around in circles.       <\/p>\n<p>Many Thanks  <br \/>\nMike.        <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637112667940,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/629917\/best-approach-to-clientside-machine-learning-for-t",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":24.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"Best Approach to Clientside Machine Learning for Text Classification",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":333,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=68a37785-70de-4e30-ac8d-ab0658ca67f4\">@Mike Shapleski  <\/a> I see two possible solutions for your scenario.    <\/p>\n<ol>\n<li> Extracting text from your documents using the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/computer-vision\/vision-api-how-to-topics\/call-read-api\">computer vision API<\/a> and passing the required text as input to Azure <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/language-service\/text-analytics-for-health\/overview?tabs=ner\">Text Analytics for Health API<\/a>    <\/li>\n<li> Using <a href=\"https:\/\/learn.microsoft.com\/en-in\/azure\/search\/search-what-is-azure-search\">Azure cognitive search<\/a> to upload the documents and creating a search service and enabling specific <a href=\"https:\/\/learn.microsoft.com\/en-in\/azure\/search\/cognitive-search-predefined-skills\">skills<\/a> on the service to extract PII data or entities    <\/li>\n<\/ol>\n<p>The first solution can help you achieve this and ensure everything is offline or using docker containers without uploading any of your data to any storage externally. For billing purposes the containers need to connect to a metering endpoint on Azure to bill your usage of both these services(<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/computer-vision\/computer-vision-how-to-install-containers?tabs=version-3-2\">Computer Vision API<\/a> &amp; <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/language-service\/text-analytics-for-health\/how-to\/use-containers\">Azure text analytics<\/a> containers). Also, you can use C# client library to call the local endpoint of these containers. The setup could take time to configure docker containers and passing the PDF documents to the computer vision read API to extract text. The extracted text can then be directly used or stored, to call the text analytics for health API.    <\/p>\n<p>The second solution can be used to index all the documents by using the search service by having your data in the cloud or behind a firewall to index the documents and make them searchable. There are some skills that can be enabled on the search service to extract entities and other PII information but this may not extract the same data as text analytics for health. This solution can be faster to setup because you can directly query your data after uploading the documents.     <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":8.0,
        "Solution_readability":14.5,
        "Solution_reading_time":35.84,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":300.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"This is more like a suggestion than a bug. The `config` parameter to the WandBLogger is supposed to be of type `args.namespace`. Therefore it converts it to a dictionary inside its `arge_parse` function using `vars(.)`. This might be restrictive in some cases if someone wants to pass configs directly as a dictionary (for example when hyperparameters are loaded from a YAML file). Wouldn't it be better to do the conversion outside the logger to make it more general in terms of config input?\r\n\r\nThanks :)",
        "Challenge_closed_time":1635948.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635882064000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ContinualAI\/avalanche\/issues\/797",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.9,
        "Challenge_reading_time":6.51,
        "Challenge_repo_contributor_count":56.0,
        "Challenge_repo_fork_count":208.0,
        "Challenge_repo_issue_count":1067.0,
        "Challenge_repo_star_count":1173.0,
        "Challenge_repo_watch_count":30.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Config type in WandBLogger",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":87,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I agree, we can easily add support for plain dictionary. @digantamisra98 are you still working on the logger right? Can you take care of this? I've made a simple fix to it by removing the conversion inside WandBLogger. It works with plain dictionaries now. I also made a PR just in case.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.3,
        "Solution_reading_time":3.47,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":52.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm doing a pipeline in Azure ML SDK. After I had run the pipeline for some amount of times it reported I had reached the Snapshot limit of 300MB. I followed some of the fixes that was proposed:<\/p>\n<ul>\n<li>   Each step script is moved to a separate subfolder<\/li>\n<li>   I added a datastore to the pipeline<\/li>\n<li>   This line was added: <code>azureml._restclient.snapshots_client.SNAPSHOT_MAX_SIZE_BYTES = 1000<\/code><\/li>\n<\/ul>\n<p>But then a new Snapshot error occurred after I submitted my pipeline:<\/p>\n<pre><code>pipeline1 = Pipeline(default_source_directory=&quot;.&quot;, default_datastore=def_blob_store, workspace=ws, steps=[prep_step, hd_step, register_model_step])\n<\/code><\/pre>\n<p>THE ERROR MESSAGE:<\/p>\n<pre><code>WARNING:root:If 'script' has been provided here and a script file name has been specified in 'run_config', 'script' provided in ScriptRunConfig initialization will take precedence.\n---------------------------------------------------------------------------\nSnapshotException                         Traceback (most recent call last)\n&lt;ipython-input-14-05c5aa4991aa&gt; in &lt;module&gt;\n----&gt; 1 pipeline1 = Pipeline(default_source_directory=&quot;.&quot;, default_datastore=def_blob_store, workspace=ws, steps=[prep_step, hd_step, register_model_step])\n      2 pipeline1.validate()\n      3 pipeline_run = Experiment(ws, 'health_insuarance').submit(pipeline1, regenerate_outputs=False)\n      4 RunDetails(pipeline_run).show()\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/_experiment_method.py in wrapper(self, *args, **kwargs)\n     95             &quot;&quot;&quot;\n     96             ExperimentSubmitRegistrar.register_submit_function(self.__class__, submit_function)\n---&gt; 97             return init_func(self, *args, **kwargs)\n     98         return wrapper\n     99     return real_decorator\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/pipeline.py in __init__(self, workspace, steps, description, default_datastore, default_source_directory, resolve_closure, _workflow_provider, _service_endpoint, **kwargs)\n    175                 raise ValueError('parameter %s is not recognized for Pipeline ' % key)\n    176         self._enable_email_notification = enable_email_notification\n--&gt; 177         self._graph = self._graph_builder.build(self._name, steps, finalize=False)\n    178 \n    179     def _set_experiment_name(self, name):\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/builder.py in build(self, name, steps, finalize, regenerate_outputs)\n   1479                 pass\n   1480 \n-&gt; 1481         graph = self.construct(name, steps)\n   1482         if finalize:\n   1483             graph.finalize(regenerate_outputs=regenerate_outputs)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/builder.py in construct(self, name, steps)\n   1501         self._graph = Graph(name, self._context)\n   1502         self._nodeStack.append([])\n-&gt; 1503         self.process_collection(steps)\n   1504         for builder in self._builderStack[::-1]:\n   1505             builder.apply_rules()\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/builder.py in process_collection(self, collection)\n   1537         self._nodeStack.append([])\n   1538         self._builderStack.append(builder)\n-&gt; 1539         builder.process_collection(collection)\n   1540         added_nodes = self._nodeStack.pop()\n   1541         self._nodeStack[-1].extend(added_nodes)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/builder.py in process_collection(self, collection)\n   1828         &quot;&quot;&quot;\n   1829         for item in collection:\n-&gt; 1830             self._base_builder.process_collection(item)\n   1831 \n   1832     def apply_rules(self):\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/builder.py in process_collection(self, collection)\n   1531         # just a step?\n   1532         if isinstance(collection, PipelineStep):\n-&gt; 1533             return self.process_step(collection)\n   1534 \n   1535         # delegate to correct builder\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/builder.py in process_step(self, step)\n   1575             return self._step2node[step]\n   1576 \n-&gt; 1577         node = step.create_node(self._graph, self._default_datastore, self._context)\n   1578         self.assert_node_valid(step, self._graph, node)\n   1579 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/steps\/hyper_drive_step.py in create_node(self, graph, default_datastore, context)\n    247         &quot;&quot;&quot;\n    248         hyperdrive_config, reuse_hashable_config = self._get_hyperdrive_config(context._workspace,\n--&gt; 249                                                                                context._experiment_name)\n    250         self._params[HyperDriveStep._run_config_param_name] = json.dumps(hyperdrive_config)\n    251         self._params[HyperDriveStep._run_reuse_hashable_config] = json.dumps(reuse_hashable_config)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/steps\/hyper_drive_step.py in _get_hyperdrive_config(self, workspace, experiment_name)\n    323 \n    324         hyperdrive_dto = _search._create_experiment_dto(self._hyperdrive_config, workspace,\n--&gt; 325                                                         experiment_name, telemetry_values)\n    326 \n    327         hyperdrive_config = hyperdrive_dto.as_dict()\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/train\/hyperdrive\/_search.py in _create_experiment_dto(hyperdrive_config, workspace, experiment_name, telemetry_values, activity_logger, **kwargs)\n     41     if hyperdrive_config.source_directory is not None:\n     42         snapshot_client = SnapshotsClient(workspace.service_context)\n---&gt; 43         snapshot_id = snapshot_client.create_snapshot(hyperdrive_config.source_directory)\n     44 \n     45         if activity_logger is not None:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_restclient\/snapshots_client.py in create_snapshot(self, file_or_folder_path, retry_on_failure, raise_on_validation_failure)\n     83         exclude_function = ignore_file.is_file_excluded\n     84 \n---&gt; 85         self._validate_snapshot_size(file_or_folder_path, exclude_function, raise_on_validation_failure)\n     86 \n     87         # Get the previous snapshot for this project\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_restclient\/snapshots_client.py in _validate_snapshot_size(self, file_or_folder_path, exclude_function, raise_on_validation_failure)\n     61                             &quot;\\n&quot;.format(file_or_folder_path, SNAPSHOT_MAX_SIZE_BYTES \/ ONE_MB)\n     62             if raise_on_validation_failure:\n---&gt; 63                 raise SnapshotException(error_message)\n     64             else:\n     65                 self._logger.warning(error_message)\n\nSnapshotException: SnapshotException:\n    Message: ====================================================================\n\nWhile attempting to take snapshot of .\/train\/\nYour total snapshot size exceeds the limit of 0.00095367431640625 MB.\nPlease see http:\/\/aka.ms\/aml-largefiles on how to work with large files.\n\n====================================================================\n\n\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;====================================================================\\n\\nWhile attempting to take snapshot of .\/train\/\\nYour total snapshot size exceeds the limit of 0.00095367431640625 MB.\\nPlease see http:\/\/aka.ms\/aml-largefiles on how to work with large files.\\n\\n====================================================================\\n\\n&quot;\n    }\n}\n\u200b\n<\/code><\/pre>\n<p>Any idea how I fix this?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1612425944867,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/258581\/how-do-i-fix-this-snapshot-exception",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":18.3,
        "Challenge_reading_time":96.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":61,
        "Challenge_solved_time":null,
        "Challenge_title":"How do I fix this Snapshot Exception?",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":498,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Alright, so I found the fix.  <\/p>\n<p>I changed this line by adding a number equvilant to 1GB: <code>azureml._restclient.snapshots_client.SNAPSHOT_MAX_SIZE_BYTES = 1000000000<\/code>  <\/p>\n<p>For some reason, you have to define the size in BYTES and not megabytes even though the default is 300 MB. Not especially intuitive.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.3,
        "Solution_reading_time":4.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Good morning,     <br \/>\nI am on a text classification project and I test the text classification services on Microsoft Aeure.     <br \/>\nI would like to know the difference between Microsoft Azure Machine Learning Studio Data Labeling and Text Classifition de cognitive service (Langage Studio)?     <br \/>\nDoes one service perform better than the other?     <\/p>\n<p>Thank you in advance for your help     <\/p>\n<p>Cordially     <br \/>\nLysa <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1671445270327,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1134081\/what-is-the-difference-between-data-labeling-text",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":6.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"What is the difference between Data Labeling Text Azure Machine Learning Studio and cognitive service Text Classifition (Langage Studio) ?",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":85,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=2d96c782-8477-4987-b5a4-5ea497b49eb9\">@AMROUN Lysa  <\/a> Thanks for the question. In Azure ML you will be able to train and customize text classification. In Language studio base line model provided to do the classification.    <\/p>\n<p>With Custom text classification, you can build custom AI models to classify text into pre-defined custom classes. By creating a custom text classification project, you can iteratively label data, train, evaluate, and improve model performance before deploying your model and making it available for consumption.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.7,
        "Solution_reading_time":7.35,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":78.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1353403873547,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":4909.0,
        "Answerer_view_count":583.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Optuna's FAQ has a <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/faq.html#id10\" rel=\"nofollow noreferrer\">clear answer<\/a> when it comes to dynamically adjusting the range of parameter during a study: it poses no problem since each sampler is defined individually.<\/p>\n<p>But what about adding and\/or removing parameters? Is Optuna able to handle such adjustments?<\/p>\n<p>One thing I noticed when doing this is that in the results dataframe these parameters get <code>nan<\/code> entries for other trials. Would there be any benefit to being able to set these <code>nan<\/code>s to their (default) value that they had when not being sampled? Is the study still sound with all these unknown values?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608315900387,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65362133",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":9.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"What happens when I add\/remove parameters dynamically during an Optuna study?",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":227.0,
        "Challenge_word_count":110,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1353403873547,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":4909.0,
        "Poster_view_count":583.0,
        "Solution_body":"<p>Question was answered <a href=\"https:\/\/github.com\/optuna\/optuna\/issues\/2141\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<blockquote>\n<p>Thanks for the question. Optuna internally supports two types of sampling: <code>optuna.samplers.BaseSampler.sample_independent<\/code> and <code>optuna.samplers.BaseSampler.sample_relative<\/code>.<\/p>\n<p>The former <code>optuna.samplers.BaseSampler.sample_independent<\/code> is a method that samples independently on each parameter, and is not affected by the addition or removal of parameters. The added parameters are taken into account from the timing when they are added.<\/p>\n<p>The latter <code>optuna.samplers.BaseSampler.sample_relative<\/code> is a method that samples by considering the correlation of parameters and is affected by the addition or removal of parameters. Optuna's default search space for correlation is the product set of the domains of the parameters that exist from the beginning of the hyperparameter tuning to the present. Developers who implement samplers can implement their own search space calculation method <code>optuna.samplers.BaseSampler.infer_relative_search_space<\/code>. This may allow correlations to be considered for hyperparameters that have been added or removed, but this depends on the sampling algorithm, so there is no API for normal users to modify.<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.3,
        "Solution_reading_time":17.6,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":157.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1637238945423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jakarta, Indonesia",
        "Answerer_reputation_count":31.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":164.8295133333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm importing a text dataset to Google Vertex AI and got the following error:<\/p>\n<pre><code>Hello Vertex AI Customer,\n\nDue to an error, Vertex AI was unable to import data into \ndataset [dataset_name].\nAdditional Details:\nOperation State: Failed with errors\nResource Name: [resoure_link]\nError Messages: There are too many rows in the jsonl\/csv file. Currently we \nonly support 1000000 lines. Please cut your files to smaller size and run \nmultiple import data pipelines to import.\n<\/code><\/pre>\n<p>I checked my dataset which I generated from pandas and the actual CSV file, it only have 600k lines.<\/p>\n<p>Anyone got similar errors?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":4,
        "Challenge_created_time":1637835565680,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1638435090680,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70109346",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":7.8,
        "Challenge_reading_time":9.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Vertex AI was unable to import data into dataset. It says maximum 1M lines while my dataset only have 600k",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":212.0,
        "Challenge_word_count":117,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1637238945423,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Jakarta, Indonesia",
        "Poster_reputation_count":31.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>So it turns out to be an error in my CSV formatting.<\/p>\n<p>I forgot to trim newlines and extra whitespaces in my text dataset. This solved the 1M lines count. But after doing that, I then get error telling me I have too much labels while it was only 2.<\/p>\n<pre><code>Error Messages: There are too many AnnotationSpecs in the dataset. Up to \n5000 AnnotationSpecs are allowed in one Dataset.\n<\/code><\/pre>\n<p>And this is because I created the text dataset using to_csv() method in Pandas dataframe. Creating a CSV file this way, it will automatically put quotes when your text include a &quot;,&quot; (comma character) only. So the CSV file will look like:<\/p>\n<pre><code>&quot;this is a sentence, with a comma&quot;, 0\nthis is a sentence without a comma, 1\n<\/code><\/pre>\n<p>Meanwhile, what Vertex AutoML Text wants the CSV is to look like this:<\/p>\n<pre><code>&quot;this is a sentence, with a comma&quot;, 0\n&quot;this is a sentence without a comma&quot;, 1\n<\/code><\/pre>\n<p>i.e. you have to put quotes on every line.<\/p>\n<p>Which you can achieve by writing your own CSV formatter, or if you insist on using Pandas to_csv(), you can pass csv.QUOTE_ALL to the quoting parameter. It will look like this:<\/p>\n<pre><code>import csv\ndf.to_csv(&quot;file.csv&quot;, index=False, quoting=csv.QUOTE_ALL, header=False)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1639028476928,
        "Solution_link_count":0.0,
        "Solution_readability":6.3,
        "Solution_reading_time":16.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":206.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1460437080990,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":386.0,
        "Answerer_view_count":42.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How do you rename an Azure ML Experiment?\u00a0I cannot see any property field you can set. All I can find is Save\u00a0As something else, then delete the\u00a0original\u00a0experiment. <\/p>\n\n<p>When I\u00a0Save for the first time, it doesn't ask me for a name, it just saves it with a standard date. <\/p>\n\n<p>Am I missing something simple and obvious? <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1485651412897,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41916570",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.3,
        "Challenge_reading_time":4.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Rename an Azure ML experiment",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":753.0,
        "Challenge_word_count":67,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1407201889907,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Redmond, WA",
        "Poster_reputation_count":2674.0,
        "Poster_view_count":355.0,
        "Solution_body":"<p>put the cursor on the name text when an experiment is open, and edit away.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":0.98,
        "Solution_score_count":3.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":15.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"![10a2a57d-e765-4359-915e-a60163bd6ec8](https:\/\/user-images.githubusercontent.com\/58698728\/205865653-bf35fb85-19cb-4e95-958e-619d13015db0.jpg)\r\n",
        "Challenge_closed_time":1670919.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1670317034000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/csia-pme\/csia-pme\/issues\/39",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":24.5,
        "Challenge_reading_time":2.53,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":39.0,
        "Challenge_repo_star_count":4.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":1,
        "Challenge_solved_time":null,
        "Challenge_title":"Resolve DVC Bad request with Minio",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":6,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1428454496052,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":35.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to build a training set for Sagemaker using the Linear Learner algorithm. This algorithm supports recordIO wrapped protobuf and csv as format for the training data. As the training data is generated using spark I am having issues to generate a csv file from a dataframe (this seem broken for now), so I am trying to use protobuf. <\/p>\n\n<p>I managed to create a binary file for the training dataset using Protostuff which is a library that allows to generate protobuf messages from POJO objects. The problem is when triggering the training job I receive that message from SageMaker:\nClientError: No training data processed. Either the training channel is empty or the mini-batch size is too high. Verify that training data contains non-empty files and the mini-batch size is less than the number of records per training host.<\/p>\n\n<p>The training file is certainly not null. I suspect the way I generate the training data to be incorrect as I am able to train models using the libsvm format. Is there a way to generate IOrecord using the Sagemaker java client ?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1525984750483,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50281188",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.8,
        "Challenge_reading_time":13.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker Java client generate IOrecord",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":222.0,
        "Challenge_word_count":188,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1428454496052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Answering my own question. It was an issue in the algorithm configuration. I reduced mini batch size and it worked fine.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":1.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":21.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi,<\/p>\n<p>I am having trouble accessing run data keys in several of my runs. Specifically, I have logged a metric in my code, the metric is tracked in the online wandb UI, but when I try accessing the data using the following code<\/p>\n<pre><code class=\"lang-auto\">import wandb\napi = wandb.Api()\nrun = api.run(\"xxxxxx\")\nrun.history()[['_step', 'metric_name']]\n<\/code><\/pre>\n<p>It throws a <code>KeyError: \"['metric_name'] not in index\"<\/code>.<\/p>\n<p>When I print out <code>run.history()<\/code> in table format, it does show \u2018metric_name\u2019 as one of the columns; \u2018metric_name\u2019 also appears as a key in <code>run.summary<\/code>. I wonder what is the issue here?<\/p>\n<p>Would appreciate any help. Thank you!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1670785361235,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/cannot-access-run-data-via-run-history\/3530",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":7.8,
        "Challenge_reading_time":9.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Cannot access run data via run.history()",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":131.0,
        "Challenge_word_count":104,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/toruowo\">@toruowo<\/a> thank you for writing in! Can you please change your last line to:<\/p>\n<pre><code class=\"lang-auto\">run.history(keys=['_step', 'metric_name'])\n<\/code><\/pre>\n<p>Would this work for you?  Please let me know if you have any further issues or questions. You may also find some more <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide#public-api-examples\">API examples here<\/a> if that helps.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.8,
        "Solution_reading_time":5.95,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":51.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Thanks to <a href=\"https:\/\/www.dabeaz.com\/generators\/\" rel=\"nofollow noreferrer\">David Beazley's slides on Generators<\/a> I'm quite taken with using generators for data processing in order to keep memory consumption minimal. Now I'm working on my first kedro project, and my question is how I can use generators in kedro. When I have a node that yields a generator, and then run it with <code>kedro run --node=example_node<\/code>, I get the following error:<\/p>\n<pre><code>DataSetError: Failed while saving data to data set MemoryDataSet().\ncan't pickle generator objects\n<\/code><\/pre>\n<p>Am I supposed to always load all my data into memory when working with kedro?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661264859883,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73460511",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":8.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"How to use generators with kedro?",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":46.0,
        "Challenge_word_count":101,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1481393494750,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":19.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>Hi @ilja to do this you may need to change the type of <code>assignment<\/code> operation that <code>MemoryDataSet<\/code> applies.<\/p>\n<p>In your catalog, declare your datasets explicitly, change the <code>copy_mode<\/code> to one of <code>copy<\/code> or <code>assign<\/code>. I think <code>assign<\/code> may be your best bet here...<\/p>\n<p><a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.MemoryDataSet.html\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.MemoryDataSet.html<\/a><\/p>\n<p>I hope this works, but am not 100% sure.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.8,
        "Solution_reading_time":7.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":56.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"In a fresh conda environment, I get several warnings that halt the script execution:\r\n```\r\n...\r\nFailure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (docker 5.0.0 (c:\\dev\\miniconda\\envs\\xxx\\lib\\site-packages), Requirement.parse('docker<5.0.0'), {'azureml-core'}).\r\n...\r\n```\r\n\r\nMy environment is specified by:\r\n```yaml\r\nname: xxx\r\nchannels:\r\n  - anaconda\r\n  - pytorch-lts\r\ndependencies:\r\n  - python=3.6\r\n  - pandas=1.1.3\r\n  - numpy=1.19.2\r\n  - scikit-learn=0.23.2\r\n  - matplotlib\r\n  - mkl=2020.2\r\n  - pytorch=1.8.1\r\n  - cpuonly=1.0\r\n  - pip\r\n  - pip:\r\n      - azureml-sdk==1.31.0\r\n      - azureml-defaults==1.31.0\r\n      - azure-storage-blob==12.8.1\r\n      - mlflow==1.18.0\r\n      - azureml-mlflow==1.31.0\r\n      - pytorch-lightning==1.3.8\r\n      - onnxruntime==1.8.0\r\n      - docker<5.0.0 # this is the fix needed\r\n```\r\nThe fix is to specify `docker<5.0.0`. Perhaps, there are some wrong deps checks somewhere.\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: eb938463-51c2-43f3-d528-76a07a28bec8\r\n* Version Independent ID: e15753c0-6fe1-100a-0efc-08c1f845dc83\r\n* Content: [Azure Machine Learning SDK for Python - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/docs-ref-conceptual\/index.md](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/docs-ref-conceptual\/index.md)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @trevorbye\r\n* Microsoft Alias: **trbye**",
        "Challenge_closed_time":1630367.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625218579000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1537",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":21.68,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"Bug: Failure while loading azureml_run_type_providers",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":129,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for the report! azureml-sdk==1.13.0 does specify docker<5.0.0, while mlflow==1.18.0 requires 5.0.0. \r\n\r\nI'm going to close this issue as there is no action for azureml-sdk.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.9,
        "Solution_reading_time":2.22,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1375058329287,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":762.0,
        "Answerer_view_count":51.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following the <a href=\"https:\/\/blog.dataversioncontrol.com\/data-version-control-tutorial-9146715eda46\" rel=\"nofollow noreferrer\">tutorial<\/a> about <a href=\"https:\/\/github.com\/iterative\/dvc\" rel=\"nofollow noreferrer\">Data Version Control<\/a> using <code>mingw32<\/code> on Windows 7.<\/p>\n\n<p>I am getting very strange error when I try to use <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/run\" rel=\"nofollow noreferrer\">run<\/a>:<\/p>\n\n<pre><code>$ dvc run -v echo \"hello\"\nDebug: updater is not old enough to check for updates\nDebug: PRAGMA user_version;\nDebug: fetched: [(2,)]\nDebug: CREATE TABLE IF NOT EXISTS state (inode INTEGER PRIMARY KEY, mtime TEXT NOT NULL, md5 TEXT NOT NULL, timestamp TEXT NOT NULL)\nDebug: CREATE TABLE IF NOT EXISTS state_info (count INTEGER)\nDebug: CREATE TABLE IF NOT EXISTS link_state (path TEXT PRIMARY KEY, inode INTEGER NOT NULL, mtime TEXT NOT NULL)\nDebug: INSERT OR IGNORE INTO state_info (count) SELECT 0 WHERE NOT EXISTS (SELECT * FROM state_info)\nDebug: PRAGMA user_version = 2;\nRunning command:\n        echo hello\n\/c: \/c: Is a directory\nDebug: SELECT count from state_info WHERE rowid=1\nDebug: fetched: [(1,)]\nDebug: UPDATE state_info SET count = 1 WHERE rowid = 1\nError: Traceback (most recent call last):\n  File \"dvc\\command\\run.py\", line 18, in run\n  File \"dvc\\project.py\", line 265, in run\n  File \"dvc\\stage.py\", line 435, in run\nStageCmdFailedError: Stage 'Dvcfile' cmd echo hello failed\n\nError: Failed to run command: Stage 'Dvcfile' cmd echo hello failed\n<\/code><\/pre>\n\n<h3>Question:<\/h3>\n\n<p>Where does the <code>\/c: \/c: Is a directory<\/code> come from?  How can I fix it? <\/p>\n\n<h3>My findings<\/h3>\n\n<ol>\n<li><p>I supposed that it was resolving path to echo, but ech is a builtin.<\/p>\n\n<pre><code>$ type echo\necho is a shell builtin\n<\/code><\/pre>\n\n<p>I tried also with <code>exit<\/code> and <code>cd<\/code> but I am getting the same error.<\/p><\/li>\n<li><p>Calling commands without dvc works fine.<\/p><\/li>\n<li><p><code>dvc<\/code> with <code>--no-exec<\/code> flag works fine, but when later executed with <code>repro<\/code> gives the same error. <\/p><\/li>\n<\/ol>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539857086453,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1539881078543,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52871630",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":11.3,
        "Challenge_reading_time":27.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Resolving paths in mingw fails with Data Version Control",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":109.0,
        "Challenge_word_count":287,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1508231047660,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Krak\u00f3w, Poland",
        "Poster_reputation_count":860.0,
        "Poster_view_count":118.0,
        "Solution_body":"<p>I'm one of the dvc developers. Similar error has affected dvc running on cygwin. We've released a fix for it in <code>0.20.0<\/code>. Please upgrade.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.2,
        "Solution_reading_time":1.94,
        "Solution_score_count":4.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":24.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"### System Info\r\n\r\n```shell\r\n- mlflow==1.25.1\r\n- `transformers` version: 4.19.0.dev0\r\n- Platform: Linux-5.10.76-linuxkit-aarch64-with-glibc2.31\r\n- Python version: 3.9.7\r\n- Huggingface_hub version: 0.2.1\r\n- PyTorch version (GPU?): 1.10.2 (False)\r\n```\r\n\r\n\r\n### Who can help?\r\n\r\nShould be fixed by #17067\r\n\r\n### Information\r\n\r\n- [X] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [X] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nSteps to reproduce:\r\n1. Follow Training tutorial as per https:\/\/huggingface.co\/docs\/transformers\/training\r\n2. Change the training arguments to use `TrainingArguments(output_dir=\"test_trainer\", report_to=['mlflow'], run_name=\"run0\")`\r\n3. On `trainer.train()` the MLFlow UI should report a run with a Run Name of `run0` which is not currently the case.\r\n\r\nCause of the Issue:\r\n```\r\n>> import mlflow\r\n>> print(mlflow.active_run is None, mlflow.active_run() is None)\r\nFalse True\r\n```\r\n\r\nIn `src\/transformers\/integrations.py` the line `if self._ml_flow.active_run is None:` need to be replaced by `if self._ml_flow.active_run() is None:`\r\n\r\n### Expected behavior\r\n\r\nPR #14894 introduce support for run_name in the MLflowCallback. Though, this does not work as expected since the active run is checked using a method reference that always returns true. Bug introduced by #16131.\r\n",
        "Challenge_closed_time":1651758.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651602226000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17066",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":6.6,
        "Challenge_reading_time":18.14,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17176.0,
        "Challenge_repo_issue_count":20644.0,
        "Challenge_repo_star_count":75873.0,
        "Challenge_repo_watch_count":862.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":null,
        "Challenge_title":"Incorrect check for MLFlow active run in MLflowCallback",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":177,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":10,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nWhen testing a model with `Trainer.test` metrics are not logged to Comet if the model was previously trained using `Trainer.fit`. While training metrics are logged correctly.\r\n\r\n\r\n#### Code sample\r\n```\r\n    comet_logger = CometLogger()\r\n    trainer = Trainer(logger=comet_logger)\r\n    model = get_model()\r\n\r\n    trainer.fit(model) # Metrics are logged to Comet\r\n    trainer.test(model) # No metrics are logged to Comet\r\n```\r\n\r\n### Expected behavior\r\n\r\nTest metrics should also be logged in to Comet.\r\n\r\n### Environment\r\n\r\n```\r\n- PyTorch version: 1.3.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.168\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 418.67\r\ncuDNN version: \/usr\/local\/cuda-10.1\/targets\/x86_64-linux\/lib\/libcudnn.so.7.6.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.4\r\n[pip3] pytorch-lightning==0.6.0\r\n[pip3] torch==1.3.0\r\n[pip3] torchvision==0.4.1\r\n[conda] Could not collect\r\n```\r\n\r\n### Additional context\r\n\r\nI believe the issue is caused because at the [end of the training routine](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/deffbaba7ffb16ff57b56fe65f62df761f25fbd6\/pytorch_lightning\/trainer\/training_loop.py#L366), `logger.finalize(\"success\")` is called. This in turn calls `experiment.end()` inside the logger and the `Experiment` object doesn't expect to send more information after this.\r\n\r\nAn alternative is to create another `Trainer` object, with another logger but this means that the metrics will be logged into a different Comet experiment from the original. This issue can be solved using the `ExistingExperiment` object form the Comet SDK, but the solution seems a little hacky and the `CometLogger` currently doesn't support this kind of experiment.\r\n",
        "Challenge_closed_time":1582760.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580225794000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/760",
        "Challenge_link_count":1,
        "Challenge_participation_count":10,
        "Challenge_readability":8.6,
        "Challenge_reading_time":26.63,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":null,
        "Challenge_title":"Test metrics not logging to Comet after training",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":277,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Did you find a solution?\r\nMind submitting a PR?\r\n@fdelrio89  I did solve the issue but in a kind of hacky way. It's not that elegant but it works for me, and I haven't had the time to think of a better solution.\r\n\r\nI solved it by getting the experiment key and creating another logger and trainer with it.\r\n```\r\n    comet_logger = CometLogger()\r\n    trainer = Trainer(logger=comet_logger)\r\n    model = get_model()\r\n\r\n    trainer.fit(model)\r\n\r\n    experiment_key = comet_logger.experiment.get_key()\r\n    comet_logger = CometLogger(experiment_key=experiment_key)\r\n    trainer = Trainer(logger=comet_logger)\r\n\r\n    trainer.test(model)\r\n```\r\n\r\nFor this to work, I had to modify the `CometLogger` class to accept the `experiment_key` and create a `CometExistingExperiment` from the Comet SDK when this param is present.\r\n\r\n```\r\nclass CometLogger(LightningLoggerBase):\r\n     ...\r\n\r\n    @property\r\n    def experiment(self):\r\n        ...\r\n\r\n        if self.mode == \"online\":\r\n            if self.experiment_key is None:\r\n                self._experiment = CometExperiment(\r\n                    api_key=self.api_key,\r\n                    workspace=self.workspace,\r\n                    project_name=self.project_name,\r\n                    **self._kwargs\r\n                )\r\n            else:\r\n                self._experiment = CometExistingExperiment(\r\n                    api_key=self.api_key,\r\n                    workspace=self.workspace,\r\n                    project_name=self.project_name,\r\n                    previous_experiment=self.experiment_key,\r\n                    **self._kwargs\r\n                )\r\n        else:\r\n            ...\r\n\r\n        return self._experiment\r\n```\r\n\r\nI can happily do the PR if this solution is acceptable for you guys, but I think a better solution can be achieved I haven't had the time to think about it @williamFalcon. @williamFalcon Any progress on this Issue? I am facing the same problem.\r\n @fdelrio89 Since the logger object is available for the lifetime of the trainer, maybe you can refactor to store the `experiment_key` directly in the logger object itself, instead of having to re-instantiate the logger.  @xssChauhan good idea, I just submitted a PR (https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/pull\/892) considering this. Thanks!\r\n I assume that it was fixed by #892\r\n if you have some other problems feel free to reopen or create a new... :robot:  Actually I'm still facing the problem. @dvirginz are you using the latest master? may you provide a minimal example? > @dvirginz are you using the latest master? may you provide a minimal example?\r\n\r\nYou are right, sorry. \r\nAfter building from source it works.  I should probably open a new issue, but it happens with Weights & Biases logger too. I haven't had the time to delve deep into it yet.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.6,
        "Solution_reading_time":29.86,
        "Solution_score_count":null,
        "Solution_sentence_count":31.0,
        "Solution_word_count":312.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hi there! I was wondering, how do I deal with having multiple variables to log, but one of those variables I only want to log every 100 timesteps? The wandb docs seem to suggest that I need to collect all my metrics into one log function call, but in my scenario above where I want to track one variable every step and another variable every 100 steps, I would need multiple log calls. I saw the docs for the define metrics function, but I\u2019m not quite sure if that\u2019s the way to handle this. How do I approach this in PyTorch? Thanks!<\/p>\n<p>As an example, I currently have this Tensorboard logging that I\u2019m trying to convert to wandb:<\/p>\n<pre><code class=\"lang-auto\">print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\nwriter.add_scalar(\"charts\/episodic_return\", info[\"episode\"][\"r\"], global_step)\n\nif global_step % 100 == 0:\n    writer.add_scalar(\n        \"losses\/qf1_values\", qf1_a_values.mean().item(), global_step\n    )\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1673362179466,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-log-two-variables-at-different-increments-of-timesteps\/3674",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.5,
        "Challenge_reading_time":12.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"How to log two variables at different increments of timesteps?",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":150.0,
        "Challenge_word_count":142,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/chulabhaya\">@chulabhaya<\/a> , happy to help. The approach you are considering is correct. You can set a check in place and log a dictionary with the values you want and set the step value.<\/p>\n<pre><code class=\"lang-auto\">for i in range (300):\n    if i%100==0:\n        wandb.log({\"value\": i, \"value\": 100}, step =i)\n    else:\n        wandb.log({\"value\": 100})\n<\/code><\/pre>\n<p>The <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log#customize-axes-and-summaries-with-define_metric\">defined metric<\/a> function allows you to have more control over the representation of your x axis and also how that axes is incremented. There are a few examples listed in the linked doc on how it functions. Please let me know if you have any questions.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.8,
        "Solution_reading_time":9.48,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":101.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":30.0886975,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Im trying to learn Azure, with little luck (yet). All the tutorials show using PipelineData just as a file, when configured in &quot;upload&quot; mode. However, im getting &quot;FileNotFoundError: [Errno 2] No such file or directory: ''&quot; error. I would love to ask a more specific question, but i just can't see what im doing wrong.<\/p>\n<pre><code>from azureml.core import Workspace, Datastore,Dataset,Environment\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\nfrom azureml.core.runconfig import RunConfiguration\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.pipeline.steps import PythonScriptStep\nfrom azureml.pipeline.core import Pipeline, PipelineData\nimport os\n\nws = Workspace.from_config()\ndatastore = ws.get_default_datastore()\n\ncompute_name = &quot;cpucluster&quot;\ncompute_target = ComputeTarget(workspace=ws, name=compute_name)\naml_run_config = RunConfiguration()\naml_run_config.target = compute_target\naml_run_config.environment.python.user_managed_dependencies = False\naml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n    conda_packages=['pandas','scikit-learn'], \n    pip_packages=['azureml-sdk', 'azureml-dataprep[fuse,pandas]'], \n    pin_sdk_version=False)\n\noutput1 = PipelineData(&quot;processed_data1&quot;,datastore=datastore, output_mode=&quot;upload&quot;)\nprep_step = PythonScriptStep(\n    name=&quot;dataprep&quot;,\n    script_name=&quot;dataprep.py&quot;,\n    source_directory=os.path.join(os.getcwd(),'dataprep'),\n    arguments=[&quot;--output&quot;, output1],\n    outputs = [output1],\n    compute_target=compute_target,\n    runconfig=aml_run_config,\n    allow_reuse=True\n)\n<\/code><\/pre>\n<p>In the dataprep.py i hve the following:<\/p>\n<pre><code>import numpy, argparse, pandas\nfrom azureml.core import Run\nrun = Run.get_context()\nparser = argparse.ArgumentParser()\nparser.add_argument('--output', dest='output', required=True)\nargs = parser.parse_args()\ndf = pandas.DataFrame(numpy.random.rand(100,3))\ndf.iloc[:, 2] = df.iloc[:,0] + df.iloc[:,1]\nprint(df.iloc[:5,:])\ndf.to_csv(args.output)\n\n<\/code><\/pre>\n<p>So, yeah. pd is supposed to write to the output, but my compute cluster says the following:<\/p>\n<pre><code>&quot;User program failed with FileNotFoundError: [Errno 2] No such file or directory: ''\\&quot;.\n<\/code><\/pre>\n<p>When i dont include the to_csv() function, the cluster does not complain<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1626541888290,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1626559748576,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68422680",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":15.6,
        "Challenge_reading_time":32.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":null,
        "Challenge_title":"how to write to Azure PipelineData properly?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":404.0,
        "Challenge_word_count":207,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1588424911652,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":59.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>Here is an <a href=\"https:\/\/github.com\/james-tn\/highperformance_python_in_azure\/blob\/master\/parallel_python_processing\/pipeline_definition.ipynb\" rel=\"nofollow noreferrer\">example<\/a> for PRS.\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.pipelinedata?view=azure-ml-py\" rel=\"nofollow noreferrer\">PipelineData<\/a> was intended to represent &quot;transient&quot; data from one step to the next one, while OutputDatasetConfig was intended for capturing the final state of a dataset (and hence why you see features like lineage, ADLS support, etc). PipelineData always outputs data in a folder structure like {run_id}{output_name}. OutputDatasetConfig allows to decouple the data from the run and hence it allows you to control where to land the data (although by default it will produce similar folder structure). The OutputDatasetConfig allows even to register the output as a Dataset, where getting rid of such folder structure makes sense. From the docs itself: &quot;Represent how to copy the output of a run and be promoted as a FileDataset. The OutputFileDatasetConfig allows you to specify how you want a particular local path on the compute target to be uploaded to the specified destination&quot;.<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-pipeline-batch-scoring-classification#create-dataset-objects\" rel=\"nofollow noreferrer\">OutFileDatasetConfig<\/a> is a control plane concept to pass data between pipeline steps.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1626668067887,
        "Solution_link_count":3.0,
        "Solution_readability":14.5,
        "Solution_reading_time":19.88,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":167.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":11,
        "Challenge_body":"When a Sagemaker notebook is launched with an associated study, the study folders are not mounted. Digging into this, I see that in the `~\/.aws` folder there is no credentials file, which is generated by the `mount_s3.sh` script.\r\n\r\n**To Reproduce**\r\n- Create a new data source (or use an existing one).\r\n- Select it and create a new Sagemaker instance using those studies. (Ensure that the folder has files so you can see them if they mount.)\r\n- After the instance launches, connect to it and see if the study folders are connected. If not, open a terminal and run `ls ~\/.aws` to see if the credential file is there.\r\n\r\n**Expected behavior**\r\nThe study folders are mounted using the assumed roles in the AWS credentials file, generated by `mount_s3.sh` script.\r\n\r\n**Screenshots**\r\n<img width=\"702\" alt=\"Screen Shot 2022-11-30 at 11 27 45 AM\" src=\"https:\/\/user-images.githubusercontent.com\/21109191\/204853707-789607d5-6fca-4a77-911d-50a5c36a549b.png\">\r\n<img width=\"526\" alt=\"Screen Shot 2022-11-30 at 11 27 36 AM\" src=\"https:\/\/user-images.githubusercontent.com\/21109191\/204853710-824ddc98-dfb5-4c8b-abbe-f19fa2453640.png\">\r\n\r\n**Versions (please complete the following information):**\r\n - Versions 5.0.0 & 4.3.1\r\n\r\n**Additional context**\r\nThis may or may not be associated with the other bug I noted with mounting s3 studies folders, [[Bug] SWB Sagemaker Study permission denied](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1067). It seems both of these issues are new, within the last couple weeks. And the environment has had no new deployments or changes within that time. Previous to these last couple weeks we had no issues with Sagemaker and study folders mounting.\r\n",
        "Challenge_closed_time":1671215.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1669825870000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1073",
        "Challenge_link_count":3,
        "Challenge_participation_count":11,
        "Challenge_readability":8.3,
        "Challenge_reading_time":21.62,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"[Bug] Sagemaker AWS Credentials not Populating",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":232,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for the bug report! We are aware of the issue mounting studies to Sagemaker instances and are actively trying to resolve it. We will let you know if we need any more info to help in our debugging. Although I am able to successfully mount studies to RStudio instances, I have noticed that it is inconsistent. \r\nWhen an RStudio instance is first launched, the user must open the terminal. This triggers some code to run automatically which then mounts the studies, making them visible in the file viewer. However, we have noticed that sometimes the studies are never mounted and the sub-folders are not visible to the user. This appears to happen randomly. When we stop, restart, and click into the terminal tab, the issue is resolved. @srpiatt please check the latest release [v5.2.5](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/releases\/tag\/v5.2.5) for the fix to this issue.  Hi! I also want to note that you may need to stop and start any affected instances after upgrade and deploying SWB v5.2.5.\r\n\r\nIf this fixes your issue, please go ahead and close this issue. I am going to mark as closing-soon-if-no-response so we will close in about 7 days if we do not hear that this did not resolve the issue.\r\n\r\nThank you for the report! I pulled the changes committed for v5.2.5 into our forked repository, which is locked at 5.0.0 version. Credentials file is created and the studies folder is able to mount. Thank you! :D Same happen to me in version 5.2.6, terminal trick didn't resolve the issue  This worked for a while, but I just tried launching another instance today to test if we're still encountering a different permissions issue, and encountered this bug again. We have the changes from the v5.2.5 SWB version pulled, as I noted before. I verified that we haven't regressed and that the changes are present in this instance. This is what we're seeing in this new instance, as of today.\r\n\r\n<img width=\"586\" alt=\"Screen Shot 2023-01-09 at 4 29 30 PM\" src=\"https:\/\/user-images.githubusercontent.com\/21109191\/211414692-dd7f4d08-3564-43e3-93b8-839c46f33646.png\">\r\n\r\nCan anyone confirm that they're also seeing this bug again? A new issue popped up that cause this issue to appear again. Please try updating to version 5.2.7. Thanks! I'll give it a try. Is the change related to this commit, [https:\/\/github.com\/awslabs\/service-workbench-on-aws\/commit\/bdf978b1c909c37207902f1b29b0fd05b5b24208](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/commit\/bdf978b1c909c37207902f1b29b0fd05b5b24208)? Until we update our fork to the latest version, we have to cherry pick these sorts of fixes. If yes, I can test that out today and close the ticket again if all good. :) yep, that's it! The changes worked for me. Thanks again, Tim! :D",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":6.8,
        "Solution_reading_time":33.96,
        "Solution_score_count":null,
        "Solution_sentence_count":35.0,
        "Solution_word_count":421.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1436184843608,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":305.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have seen on various examples (even in Azure ML) that you are able to create appealing charts using R in Visual Studio (not R Studio!), but I have no clue how they did it. I am experienced with R, but if someone could point me in the right direction of how to visualize data sets in Visual Studio and Azure ML; I would really appreciate it.\nHere is an example I would like to duplicate (in both Azure ML and Visual Studio): <a href=\"http:\/\/i.stack.imgur.com\/2aoGB.jpg\" rel=\"nofollow\">Visual studio chart<\/a><\/p>\n\n<p>Image source: <a href=\"https:\/\/regmedia.co.uk\/2016\/03\/09\/r_vis_studio_plot.jpg?x=648&amp;y=348&amp;crop=1\" rel=\"nofollow\">https:\/\/regmedia.co.uk\/2016\/03\/09\/r_vis_studio_plot.jpg?x=648&amp;y=348&amp;crop=1<\/a><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1465389537790,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1485875676343,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37702759",
        "Challenge_link_count":3,
        "Challenge_participation_count":4,
        "Challenge_readability":11.3,
        "Challenge_reading_time":10.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"How to visualize charts in Visual Studio and Azure ML through R script?",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1548.0,
        "Challenge_word_count":105,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1465381666536,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>You can install ggplot2 in your solution in the Visual Studio extension Open R (<a href=\"https:\/\/www.visualstudio.com\/en-us\/features\/rtvs-vs.aspx\" rel=\"nofollow noreferrer\">https:\/\/www.visualstudio.com\/en-us\/features\/rtvs-vs.aspx<\/a>) through this line of code and visualize it within the R Plot window in Visual Studio after creating your R-project: <\/p>\n\n<pre><code>install.packages('ggplot2', dep = TRUE)\n\nlibrary(ggplot2)\n<\/code><\/pre>\n\n<p>The reason I have \u00ablibrary(ggplot2)\u00bb is to check if the package got successfully installed, else you would get an error like this: <strong>Error in library(ggplot2) : there is no package called \u2018ggplot2\u2019<\/strong><\/p>\n\n<p>So if you don\u2019t get that error; you should be good to go.<\/p>\n\n<p>For your question about how to output charts; you simply have to populate the ggplot2 charts from a datasource, like in my example below (csv-file):<\/p>\n\n<pre><code>dataset1 &lt;- read.csv(\"Adult Census Income Binary Classification dataset.csv\", header = TRUE, sep = \",\", quote = \"\", fill = TRUE, comment.char = \"\")\n\nhead(dataset1)\n\ninstall.packages('ggplot2', dep = TRUE)\n\nlibrary(ggplot2)\n\nnames(dataset1) &lt;- sub(pattern = ',', replacement = '.', x = names(dataset1))\n\nfoo = qplot(age, data = dataset1, geom = \"histogram\", fill = income, position = \"dodge\");\n\nprint(foo)\n\nbar = qplot(age, data = dataset1, geom = \"density\", alpha = 1, fill = income);\n\nprint(bar)\n<\/code><\/pre>\n\n<p>Here you can see that I create two charts, one histogram and one density-chart.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/sLxMN.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sLxMN.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>In Azure ML, the same charts (this time I included a histogram for Relationships as well), would look like this:<\/p>\n\n<pre><code>\/\/ Map 1-based optional input ports to variables\n\ndataset1 &lt;- maml.mapInputPort(1) # class: data.frame\n\nlibrary(ggplot2)\n\nlibrary(data.table)\n\nnames(dataset1) &lt;- sub(pattern=',', replacement='.', x=names(dataset1))\n\n\/\/ This time we need to specify the X to be sex; which we didn\u2019t need in Visual Studio\n\nfoo = qplot(x=sex, data=dataset1, geom=\"histogram\", fill=income, position=\"dodge\");\n\nprint(foo)\n\nfoo = qplot(x=relationship, data=dataset1, geom=\"histogram\", fill=income, position=\"dodge\");\n\nprint(foo)\n\nfoo = qplot(x=age, data=dataset1, geom=\"density\", alpha=0.5, fill=income);\n\nprint(foo)\n\n\/\/ Select data.frame to be sent to the output Dataset port maml.mapOutputPort(\"dataset1\");\n<\/code><\/pre>\n\n<p>Remember to put all of this in a \u201cExecute R Script module\u201d in order to run it correctly. After that, you can right lick the module and visualize the result.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/2vTlD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2vTlD.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":10.8,
        "Solution_reading_time":36.18,
        "Solution_score_count":3.0,
        "Solution_sentence_count":24.0,
        "Solution_word_count":315.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following this <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_mscoco_multi_label\/Image-classification-multilabel-lst.ipynb\" rel=\"nofollow noreferrer\">tutorial<\/a> with my custom data and my custom S3 buckets where train and validation data are. I am getting the following error:<\/p>\n<pre><code>Customer Error: imread read blank (None) image for file: \/opt\/ml\/input\/data\/train\/s3:\/\/image-classification\/image_classification_model_data\/train\/img-001.png\n<\/code><\/pre>\n<p>I have all my training data are in one folder named '<code>train<\/code>' I have set up my <code>lst<\/code> file like this suggested by <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">doc<\/a>,<\/p>\n<pre><code>22  1   s3:\/\/image-classification\/image_classification_model_data\/train\/img-001.png\n86  0   s3:\/\/image-classification\/image_classification_model_data\/train\/img-002.png\n...\n<\/code><\/pre>\n<p>My other configurations:<\/p>\n<pre><code>s3_bucket = 'image-classification'\nprefix =  'image_classification_model_data'\n\n\ns3train = 's3:\/\/{}\/{}\/train\/'.format(s3_bucket, prefix)\ns3validation = 's3:\/\/{}\/{}\/validation\/'.format(s3_bucket, prefix)\n\ns3train_lst = 's3:\/\/{}\/{}\/train_lst\/'.format(s3_bucket, prefix)\ns3validation_lst = 's3:\/\/{}\/{}\/validation_lst\/'.format(s3_bucket, prefix)\n\n\n\ntrain_data = sagemaker.inputs.TrainingInput(s3train, distribution='FullyReplicated', \n                        content_type='application\/x-image', s3_data_type='S3Prefix')\n\nvalidation_data = sagemaker.inputs.TrainingInput(s3validation, distribution='FullyReplicated', \n                             content_type='application\/x-image', s3_data_type='S3Prefix')\n\ntrain_data_lst = sagemaker.inputs.TrainingInput(s3train_lst, distribution='FullyReplicated', \n                        content_type='application\/x-image', s3_data_type='S3Prefix')\n\nvalidation_data_lst = sagemaker.inputs.TrainingInput(s3validation_lst, distribution='FullyReplicated', \n                             content_type='application\/x-image', s3_data_type='S3Prefix')\n\n\ndata_channels = {'train': train_data, 'validation': validation_data, 'train_lst': train_data_lst, \n                 'validation_lst': validation_data_lst}\n<\/code><\/pre>\n<p>I checked the images downloaded and checked physically, I see the image. Now sure what this error gets thrown out as <code>blank<\/code>. Any suggestion would be great.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616651745117,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1616652513867,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66793845",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":30.4,
        "Challenge_reading_time":32.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"Customer Error: imread read blank (None) image for file- Sagemaker AWS",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":164.0,
        "Challenge_word_count":160,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1519936486960,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Minneapolis, MN, USA",
        "Poster_reputation_count":1113.0,
        "Poster_view_count":122.0,
        "Solution_body":"<p>Sagemaker copies the input data you specify in <code>s3train<\/code> into the instance in <code>\/opt\/ml\/input\/data\/train\/<\/code> and that's why you have an error, because as you can see from the error message is trying to concatenate the filename in the <code>lst<\/code> file with the path where it expect the image to be. So just put only the filenames in your <code>lst<\/code>and should be fine (remove the s3 path).<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.0,
        "Solution_reading_time":5.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"MLFlow logging issue in compare_models of time_series\r\n\r\n```\r\nfrom pycaret.datasets import get_data\r\ndata = get_data('airline')\r\n\r\nfrom pycaret.time_series import *\r\ns = setup(data, fold = 5, fh = 12, session_id = 123, log_experiment=True, experiment_name = 'airline')\r\n\r\nbest = compare_models()\r\n\r\n!mlflow ui\r\n```\r\n\r\nCheck localhost:5000:\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/54699234\/132992636-db293fe7-5461-43d9-baed-97d8790fd9bd.png)\r\n\r\nAll the runs fail in `compare_models`. Parameters are logged but metrics and artifacts didn't. \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/54699234\/132992655-e0d81e16-9a59-49a5-ace3-d22ea2ea10b8.png)\r\n\r\nI cannot reproduce this with `create_model` it means `create_model` works just fine! ",
        "Challenge_closed_time":1634125.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1631459045000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/1568",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":13.6,
        "Challenge_reading_time":10.31,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"MLFlow logging issue in compare_models of time_series",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":67,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@moezali1 merged a fix, please recheck  Done.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/54699234\/137127496-454bff0f-d3a4-410f-9d78-b8e4b1a3f547.png)",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":18.2,
        "Solution_reading_time":2.19,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":8.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1487197909143,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Brussels, Belgium",
        "Answerer_reputation_count":2206.0,
        "Answerer_view_count":169.0,
        "Challenge_adjusted_solved_time":0.3434,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm new to gunicorn and heroku so I would appreciate any help. I want to deploy my python Dash app on to heroku and I know I need a Procfile. The thing is that my project structure uses the Kedro structure and my structure looks like this:<\/p>\n<pre><code>myproject\n    .... # Kedro-generated files\n    src\/\n        package1\/\n            package2\/\n                __init__.py\n                index.py\n    Procfile\n<\/code><\/pre>\n<p>index.py is a Dash application like so<\/p>\n<pre><code>#imports up here\n\napp = dash.Dash(__name__, external_stylesheets=external_stylesheets)\nserver = app.server\n\n.......  # main code chunk\n\nif __name__ == '__main__':\napp.run_server(debug=True)\n<\/code><\/pre>\n<p>Currently, my Procfile looks like this:<\/p>\n<pre><code>web: gunicorn src frontend.index:app\n<\/code><\/pre>\n<p>My project uploads to heroku just fine but I'm getting this error in my log:<\/p>\n<pre><code>2020-08-21T06:46:46.433935+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 941, in _find_and_load_unlocked\n2020-08-21T06:46:46.433935+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n2020-08-21T06:46:46.433936+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 994, in _gcd_import\n2020-08-21T06:46:46.433936+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load\n2020-08-21T06:46:46.433936+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 953, in _find_and_load_unlocked\n2020-08-21T06:46:46.433962+00:00 app[web.1]: ModuleNotFoundError: No module named 'frontend'\n2020-08-21T06:46:46.434082+00:00 app[web.1]: [2020-08-21 06:46:46 +0000] [11] [INFO] Worker exiting (pid: 11)\n2020-08-21T06:46:46.464346+00:00 app[web.1]: Traceback (most recent call last):\n2020-08-21T06:46:46.464367+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 202, in run\n2020-08-21T06:46:46.464715+00:00 app[web.1]: self.manage_workers()\n2020-08-21T06:46:46.464732+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 545, in manage_workers\n2020-08-21T06:46:46.465049+00:00 app[web.1]: self.spawn_workers()\n2020-08-21T06:46:46.465054+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 617, in spawn_workers\n2020-08-21T06:46:46.465412+00:00 app[web.1]: time.sleep(0.1 * random.random())\n2020-08-21T06:46:46.465417+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 242, in handle_chld\n2020-08-21T06:46:46.465617+00:00 app[web.1]: self.reap_workers()\n2020-08-21T06:46:46.465622+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 525, in reap_workers\n2020-08-21T06:46:46.465905+00:00 app[web.1]: raise HaltServer(reason, self.WORKER_BOOT_ERROR)\n2020-08-21T06:46:46.465950+00:00 app[web.1]: gunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;\n2020-08-21T06:46:46.465964+00:00 app[web.1]: \n2020-08-21T06:46:46.465965+00:00 app[web.1]: During handling of the above exception, another exception occurred:\n2020-08-21T06:46:46.465965+00:00 app[web.1]: \n2020-08-21T06:46:46.465969+00:00 app[web.1]: Traceback (most recent call last):\n2020-08-21T06:46:46.465969+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/bin\/gunicorn&quot;, line 8, in &lt;module&gt;\n2020-08-21T06:46:46.466103+00:00 app[web.1]: sys.exit(run())\n2020-08-21T06:46:46.466107+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in run\n2020-08-21T06:46:46.466254+00:00 app[web.1]: WSGIApplication(&quot;%(prog)s [OPTIONS] [APP_MODULE]&quot;).run()\n2020-08-21T06:46:46.466258+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py&quot;, line 228, in run\n2020-08-21T06:46:46.466464+00:00 app[web.1]: super().run()\n2020-08-21T06:46:46.466470+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py&quot;, line 72, in run\n2020-08-21T06:46:46.466601+00:00 app[web.1]: Arbiter(self).run()\n2020-08-21T06:46:46.466606+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 229, in run\n2020-08-21T06:46:46.466790+00:00 app[web.1]: self.halt(reason=inst.reason, exit_status=inst.exit_status)\n2020-08-21T06:46:46.466794+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 342, in halt\n2020-08-21T06:46:46.467031+00:00 app[web.1]: self.stop()\n2020-08-21T06:46:46.467032+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 393, in stop\n2020-08-21T06:46:46.467262+00:00 app[web.1]: time.sleep(0.1)\n2020-08-21T06:46:46.467267+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 242, in handle_chld\n2020-08-21T06:46:46.467468+00:00 app[web.1]: self.reap_workers()\n2020-08-21T06:46:46.467469+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 525, in reap_workers\n2020-08-21T06:46:46.467750+00:00 app[web.1]: raise HaltServer(reason, self.WORKER_BOOT_ERROR)\n2020-08-21T06:46:46.467754+00:00 app[web.1]: gunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;\n2020-08-21T06:46:46.559947+00:00 heroku[web.1]: Process exited with status 1\n2020-08-21T06:46:46.610907+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T06:47:03.000000+00:00 app[api]: Build succeeded\n2020-08-21T06:49:12.915422+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/&quot; host=protected-coast-07061.herokuapp.com request_id=781ff03f-db0d-40ad-996f-1d25ff3fd026 fwd=&quot;115.66.91.134&quot; dyno= connect= service= status=503 bytes= protocol=https\n2020-08-21T06:49:13.357185+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/&quot; host=protected-coast-07061.herokuapp.com request_id=51bff951-d0fa-4e01-ba38-60f44cbe373b fwd=&quot;18.217.223.118&quot; dyno= connect= service= status=503 bytes= protocol=http\n2020-08-21T06:49:13.955353+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/favicon.ico&quot; host=protected-coast-07061.herokuapp.com request_id=632cc0a9-e052-43c9-a90c-62f99dfbba5c fwd=&quot;115.66.91.134&quot; dyno= connect= service= status=503 bytes= protocol=https\n<\/code><\/pre>\n<pre><code>2020-08-21T06:52:18.372623+00:00 heroku[web.1]: State changed from crashed to starting\n2020-08-21T06:52:32.487313+00:00 heroku[web.1]: Starting process with command `gunicorn src frontend.index`\n2020-08-21T06:52:34.595212+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Starting gunicorn 20.0.4\n2020-08-21T06:52:34.595933+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Listening at: http:\/\/0.0.0.0:17241 (4)\n2020-08-21T06:52:34.596051+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Using worker: sync\n2020-08-21T06:52:34.600183+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [10] [INFO] Booting worker with pid: 10\n2020-08-21T06:52:34.603725+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:52:34.603887+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [10] [INFO] Worker exiting (pid: 10)\n2020-08-21T06:52:34.626625+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [11] [INFO] Booting worker with pid: 11\n2020-08-21T06:52:34.629877+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:52:34.629978+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [11] [INFO] Worker exiting (pid: 11)\n2020-08-21T06:52:34.733270+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Shutting down: Master\n2020-08-21T06:52:34.733356+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Reason: App failed to load.\n2020-08-21T06:52:34.800675+00:00 heroku[web.1]: Process exited with status 4\n2020-08-21T06:52:34.837697+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T06:52:34.839731+00:00 heroku[web.1]: State changed from crashed to starting\n2020-08-21T06:52:49.188229+00:00 heroku[web.1]: Starting process with command `gunicorn src frontend.index`\n2020-08-21T06:52:50.000000+00:00 app[api]: Build succeeded\n2020-08-21T06:52:51.154243+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Starting gunicorn 20.0.4\n2020-08-21T06:52:51.154956+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Listening at: http:\/\/0.0.0.0:46031 (4)\n2020-08-21T06:52:51.155075+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Using worker: sync\n2020-08-21T06:52:51.158999+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [10] [INFO] Booting worker with pid: 10\n2020-08-21T06:52:51.162147+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:52:51.162261+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [10] [INFO] Worker exiting (pid: 10)\n2020-08-21T06:52:51.189291+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Shutting down: Master\n2020-08-21T06:52:51.189375+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Reason: App failed to load.\n2020-08-21T06:52:51.249579+00:00 heroku[web.1]: Process exited with status 4\n2020-08-21T06:52:51.281288+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T06:53:27.313026+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/&quot; host=protected-coast-07061.herokuapp.com request_id=67b1f83d-37a8-4ad1-b522-2e7cc0bd7b7d fwd=&quot;115.66.91.134&quot; dyno= connect= service= status=503 bytes= protocol=https\n2020-08-21T06:53:28.196639+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/favicon.ico&quot; host=protected-coast-07061.herokuapp.com request_id=0d13d80e-ca9b-4857-970e-47cfcf602017 fwd=&quot;115.66.91.134&quot; dyno= connect= service= status=503 bytes= protocol=https\n2020-08-21T06:57:12.000000+00:00 app[api]: Build started by user \n2020-08-21T06:58:51.667324+00:00 app[api]: Deploy 1f77e9e8 by user \n2020-08-21T06:58:51.667324+00:00 app[api]: Release v12 created by user \n2020-08-21T06:58:51.832220+00:00 heroku[web.1]: State changed from crashed to starting\n2020-08-21T06:59:07.062252+00:00 heroku[web.1]: Starting process with command `gunicorn src frontend.index:app`\n2020-08-21T06:59:10.383357+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Starting gunicorn 20.0.4\n2020-08-21T06:59:10.384213+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Listening at: http:\/\/0.0.0.0:54641 (4)\n2020-08-21T06:59:10.384357+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Using worker: sync\n2020-08-21T06:59:10.388913+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [10] [INFO] Booting worker with pid: 10\n2020-08-21T06:59:10.392276+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:59:10.392426+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [10] [INFO] Worker exiting (pid: 10)\n2020-08-21T06:59:10.403239+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [11] [INFO] Booting worker with pid: 11\n2020-08-21T06:59:10.407880+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:59:10.408006+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [11] [INFO] Worker exiting (pid: 11)\n2020-08-21T06:59:10.525402+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Shutting down: Master\n2020-08-21T06:59:10.525558+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Reason: App failed to load.\n2020-08-21T06:59:10.607473+00:00 heroku[web.1]: Process exited with status 4\n2020-08-21T06:59:11.643239+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T06:59:54.000000+00:00 app[api]: Build succeeded\n2020-08-21T07:08:53.300472+00:00 heroku[web.1]: State changed from crashed to starting\n2020-08-21T07:09:16.319403+00:00 heroku[web.1]: Starting process with command `gunicorn src frontend.index:app`\n2020-08-21T07:09:19.182910+00:00 heroku[web.1]: Process exited with status 4\n2020-08-21T07:09:19.228761+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T07:09:19.057971+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Starting gunicorn 20.0.4\n2020-08-21T07:09:19.058760+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Listening at: http:\/\/0.0.0.0:25408 (4)\n2020-08-21T07:09:19.058888+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Using worker: sync\n2020-08-21T07:09:19.063236+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [10] [INFO] Booting worker with pid: 10\n2020-08-21T07:09:19.066629+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T07:09:19.066758+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [10] [INFO] Worker exiting (pid: 10)\n2020-08-21T07:09:19.102247+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Shutting down: Master\n2020-08-21T07:09:19.102349+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Reason: App failed to load.\n<\/code><\/pre>\n<p>Apologies, I am new to this so I am not sure where to even start with the debugging as well. To summarise: I think my gunicorn is not firing as my line may be wrong; and I am not sure what is causing my app to not launch. How do I solve this issue?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597994063423,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63518174",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":180.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":143,
        "Challenge_solved_time":null,
        "Challenge_title":"using gunicorn for nested folders",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":586.0,
        "Challenge_word_count":1101,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1597993100672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>The logic of gunicorn is the following:\n<code>.<\/code> (dot) for directories, <code>:<\/code> (column) for objects defined inside a file.<\/p>\n<p>Assuming the given structure, you should have something like this:<\/p>\n<pre><code>$ cat Procfile\nweb: gunicorn src.package1.package2.index:app\n<\/code><\/pre>\n<p>[EDIT] If you get an error, you should consider using <code>server<\/code> instead of <code>app<\/code>. As an example, these files are from <a href=\"https:\/\/gitlab.com\/qmeeus\/datathon\" rel=\"nofollow noreferrer\">one of my old projects<\/a> (also a Dash app):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># app.py\n\nimport flask\nfrom src import dashboard\n\nserver = flask.Flask(__name__)\nserver.secret_key = os.environ.get('secret_key', str(randint(0, 1000000)))\napp = dashboard.main(server)\n\nif __name__ == '__main__':\n    app.server.run(debug=True, threaded=True)\n<\/code><\/pre>\n<pre class=\"lang-sh prettyprint-override\"><code># Procfile\nweb: gunicorn app:server --timeout 300\n<\/code><\/pre>\n<pre class=\"lang-sh prettyprint-override\"><code>$ ls *\nProcfile app.py\n\nsrc:\nconfig.py  dashboard.py ...\n \n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1597995299663,
        "Solution_link_count":1.0,
        "Solution_readability":12.0,
        "Solution_reading_time":14.56,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":113.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I posted a similar question last week and didn't get a response to that yet so I'm posting another one now.  <\/p>\n<p>The code below is what I use to pull data into the compute instance from the Datastore. I transfer data from a Datastore to the compute instance and then save the data to my directory as a csv. The data originates from a SCOPE script and is transferred from Cosmos to the Datastore via Azure Data Factory.   <\/p>\n<p>Once the data is in the directory as a csv, I then utilize R to pull in the data into an RStudio session and then I run various tasks that create new data sets. I also save these new data sets to the compute instance directory as csv's. These new data sets are the ones I'd like to push back to the Datastore so they can be transferred elsewhere via Azure Data Factory and later consumed by a PowerBI app we're looking to create.  <\/p>\n<p>I tried using Designer and it ran for 4 days without completing before I cancelled the job and started looking for an alternative route. I don't know if it would have completed or if it ran into memory issues and simply didn't fail. When I pull data into the compute instance from the datastore it takes less than a few minutes to complete so I'm not sure why it would take Designer multiple days to attempt to do the reverse operation.  <\/p>\n<p>I've looked through a bunch of documentation and I am not able to find anything that tells us how we can transfer data from the compute instance back to the Datastore aside from Designer which is too slow or unable to handle.  <\/p>\n<p>This task seems like one that should be obvious for use and a major selling point of Azure Machine Learning so I'm a bit dumbfounded to see that this is a challenge figuring out how to do and that the documentation doesn't clearly show users how to achieve this task, assuming it's even possible. If it's not possible then I need to figure out a whole new system to use to get my work done. If it's not possible, the Azure Machine Learning team should enable this functionality as soon as possible.   <\/p>\n<pre><code># Azure management\nfrom azureml.core import Workspace, Dataset\n\n# MetaData\nsubscription_id = '09b5fdb3-165d-4e2b-8ca0-34f998d176d5'\nresource_group = 'xCloudData'\nworkspace_name = 'xCloudML'\n\n# Create workspace \nworkspace = Workspace(subscription_id, resource_group, workspace_name)\n\n# 1. Retention_Engagement_CombinedData\ndataset = Dataset.get_by_name(workspace, name='retention-engagement-combineddata')\n\n# Save data to file\ndf = dataset.to_pandas_dataframe()\ndf.to_csv('\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/v-aantico1\/code\/RetentionEngagement_CombinedData.csv')\n\n# 2. TitleNameJoin\ndataset = Dataset.get_by_name(workspace, name='TitleForJoiningInR')\n\n# Save data to file\ndf = dataset.to_pandas_dataframe()\ndf.to_csv('\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/v-aantico1\/code\/TitleNameJoin.csv')\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1632158641093,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/559227\/how-can-i-transfer-a-csv-file-on-an-azure-machine",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":37.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I transfer a csv file on an Azure Machine Learning compute instance directory back to the Datastore?",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":447,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=1e7638a3-d560-4c0d-85b3-9061fd2bc218\">@Adrian Antico (TEKsystems, Inc.)  <\/a> Have you tried the following to upload data to your datastore?    <\/p>\n<pre><code>from azureml.core import Workspace  \nws = Workspace.from_config()  \ndatastore = ws.get_default_datastore()  \n  \ndatastore.upload(src_dir='.\/data',  \n                 target_path='datasets\/',  \n                 overwrite=True)  \n<\/code><\/pre>\n<p>I think <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_storage_datastore.azureblobdatastore?view=azure-ml-py#upload-src-dir--target-path-none--overwrite-false--show-progress-true-\">datastore.upload()<\/a> should work for you to upload the required datafiles from your compute instance to datastore.    <\/p>\n",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":19.0,
        "Solution_reading_time":9.87,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":50.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>\u63b2\u984c\u306e\u4ef6\u306b\u3064\u304d\u307e\u3057\u3066\u3001\u73fe\u5728Machine Learning\u3092\u4f7f\u7528\u3057\u3066\u6a5f\u68b0\u5b66\u7fd2\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002  <br \/>\n\u305d\u3053\u3067\u8cea\u554f\u306b\u306a\u308b\u306e\u3067\u3059\u304c\u3001\u30c7\u30b6\u30a4\u30ca\u30fc\u6a5f\u80fd\u3092\u4f7f\u7528\u3057\u3066\u5b66\u7fd2\u7d50\u679c\u3092CSV\u3067\u30a8\u30af\u30b9\u30dd\u30fc\u30c8\u3057\u3088\u3046\u3068\u3057\u3066\u3044\u308b\u306e\u3067\u3059\u304c\u3001  <br \/>\nExport Data\u30e2\u30c7\u30eb\u3067CSV\u5f62\u5f0f\u306b\u8a2d\u5b9a\u3057\u3066\u3044\u3066\u3082CSV\u3067\u306f\u306a\u3044\u5f62\u5f0f\u3067\u5171\u6709\u305b\u308c\u3066\u3057\u307e\u3046\u306e\u3067\u3059\u304c\u3001\u539f\u56e0\u304c\u308f\u304b\u3089\u306a\u3044\u72b6\u6cc1\u3067\u3059\u3002  <br \/>\n\u3054\u6559\u793a\u306e\u307b\u3069\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u3044\u305f\u3057\u307e\u3059\u3002<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1631251067517,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/546760\/machine-learning",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.2,
        "Challenge_reading_time":3.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":1,
        "Challenge_solved_time":null,
        "Challenge_title":"Machine Learning\u306b\u3064\u3044\u3066\u306e\u8cea\u554f",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":10,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=8f940edc-4c98-48e4-8a54-287e99830334\">@\u6817\u7530\u771f\u5b5d  <\/a> Are you referring to the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/export-data\">export data module<\/a> of the designer from ml.azure.com?    <br \/>\nI think I understand the issue, Are you seeing that the .csv format of file is not listed on the blob storage?    <\/p>\n<p>Since the input is a dataframe directory to export module the output format selected should still be the format you selected, in this case CSV. The file name extension only might be missing. You can still open the csv file in excel and it will recognize the delimiters and headers so you can convert it into excel files.     <\/p>\n<p>You can also avoid this by providing the .csv extension in the path itself in export settings and file will be exported as a csv file directly.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/131128-image.png?platform=QnA\" alt=\"131128-image.png\" \/>    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/543261\/index.html\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":12.7,
        "Solution_reading_time":26.56,
        "Solution_score_count":0.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":226.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"The artifact folder by default is not reemplacing the `$ARTIFACTS_BUCKET` env var",
        "Challenge_closed_time":1623230.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619181542000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/380",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":1.51,
        "Challenge_repo_contributor_count":16.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":909.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Bad MLflow artifact folder by default",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":17,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This problem has been solved adding the variable of `$ARTIFACTS_BUCKET` between `()` like this `$(ARTIFACTS_BUCKET)` in the deployment.yaml of the project-operator.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.9,
        "Solution_reading_time":2.12,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":20.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1540226093150,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":119.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using Amazon's SageMaker Studio Lab to train a model using a certain dataset.<br>\nThe code is as follow (which saves the History object in history variable):<\/p>\n<pre><code>model = tf.keras.models.load_model('best_model.hdf5')  # Every run after runtime end, use the last saved model\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\ncheckpointer = ModelCheckpoint(filepath='best_model.hdf5', verbose=1, save_best_only=True)\ncsv_logger = CSVLogger('history.log')\n\nhistory = model.fit_generator(train_generator,\n                    steps_per_epoch = nb_train_samples \/\/ batch_size,\n                    validation_data=validation_generator,\n                    validation_steps=nb_validation_samples \/\/ batch_size,\n                    epochs=30,\n                    verbose=1,\n                    callbacks=[csv_logger, checkpointer])\n<\/code><\/pre>\n<p>I had to make several pauses due to ending runtime, and with each pause I saved the .log file. Now after appending those .log files, I'm trying to access them using the standard accuracy and loss plotting methods:<\/p>\n<pre><code>def plot_accuracy(history,title):\n    plt.title(title)\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train_accuracy', 'validation_accuracy'], loc='best')\n    plt.show()\ndef plot_loss(history,title):\n    plt.title(title)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train_loss', 'validation_loss'], loc='best')\n    plt.show()\n<\/code><\/pre>\n<p>But the issue is I can't seem to manage to receive a working History object file.<br>\nAmong the things I tried:<br>\nI read about this possible method, of re-loading the model and trying to get it's history, but it didn't work, &quot;'NoneType' object has no attribute 'history'&quot;<\/p>\n<pre><code>model = tf.keras.models.load_model('best_model.hdf5')\nhistory = model.history\n<\/code><\/pre>\n<p>Another try was using pandas package and loading the file, which generated an error &quot;'DataFrame' object has no attribute 'history'&quot;:<\/p>\n<pre><code>history = pd.read_csv('history.log', sep=',', engine='python')\n<\/code><\/pre>\n<p>And this try generated a CSVLogger object, &quot;'CSVLogger' object has no attribute 'history'&quot;:<\/p>\n<pre><code>history = CSVLogger('history.log')\n<\/code><\/pre>\n<p>Appreciate any help on how to recover the History object, so I can plot those results (if it's even possible?)...<br>\nThanks.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662213868913,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73592834",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":17.6,
        "Challenge_reading_time":33.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":null,
        "Challenge_title":"Access .log files acc\\loss after training model (using .fit_generator) with multiple pauses",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":54.0,
        "Challenge_word_count":248,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540226093150,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":119.0,
        "Poster_view_count":32.0,
        "Solution_body":"<p>Instead of recreating the History object, what I did was read the .log file using pandas package, <code>read_csv<\/code> method, and create a DataFrame data structure with the wanted columns and plot it. Code below:<\/p>\n<pre><code>history = pd.read_csv('history.log')\nhistory_acc = pd.DataFrame(history, columns=[&quot;accuracy&quot;, &quot;val_accuracy&quot;])\nhistory_loss = pd.DataFrame(history, columns=[&quot;loss&quot;, &quot;val_loss&quot;])\nplot_accuracy(history_acc,'plot title...')\nplot_loss(history_loss,'plot title...')\n<\/code><\/pre>\n<br>\n<pre><code>def plot_accuracy(history,title):\n    plt.title(title)\n    plt.plot(history)\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train_accuracy', 'validation_accuracy'], loc='best')\n    plt.show()\ndef plot_loss(history,title):\n    plt.title(title)\n    plt.plot(history)\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train_loss', 'validation_loss'], loc='best')\n    plt.show()\n<\/code><\/pre>\n<p>Hope this helps someone having the same issue as I did in the future.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.9,
        "Solution_reading_time":13.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":85.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1488575811772,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bothell, WA, United States",
        "Answerer_reputation_count":51.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a model in AzureML that scores incoming values from a csv.<\/p>\n\n<p>The flow is ...->(Score Model using one-class SVM)->(Normalize Data)->(Convert to CSV)->(Convert to Dataset)->(Web Service Output)<\/p>\n\n<p>When the experiment is run I can download the csv from the (Convert to CSV) module output and it will contain Scored Probabilities column.<\/p>\n\n<p>But when I'm using a streaming job I don't know how to access the Scored Probabilities column using Query SQL. How do I do it?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1487482693377,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42324035",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":6.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"How to select Scored Probabilities from azure prediction model",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":576.0,
        "Challenge_word_count":85,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1300047702248,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":586.0,
        "Poster_view_count":108.0,
        "Solution_body":"<p>You can access the response using the amlresult.[Scored Probabilities] notation, where amlresult is an alias for the return value from your AzureML call.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.0,
        "Solution_reading_time":2.03,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":23.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"Users are reporting issues with mlflow 1.19\r\n\r\nCreating an issue here to track. Details will be added as we investigate further",
        "Challenge_closed_time":1634667.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634141491000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/338",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":5.7,
        "Challenge_reading_time":2.09,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":86.0,
        "Challenge_repo_issue_count":1012.0,
        "Challenge_repo_star_count":1924.0,
        "Challenge_repo_watch_count":28.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"[Bug\/Feature Request] Support mlflow 1.19",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":25,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"```Azure ML SDK Version:  1.11.0```\r\n\r\nIn a ```PythonScriptStep``` I'm getting a crash error that: \"\r\n```\r\nazureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\r\n```\r\n\r\nHere is my RunConfiguration:\r\n```\r\ncompute_target = ComputeTarget(workspace=f.ws, name=compute_name)\r\n\r\ncd = CondaDependencies.create(\r\n    pip_packages=[\"pandas\", \"numpy\",\r\n                  \"azureml-defaults\", \"azureml-sdk[explain,automl]\", \"azureml-train-automl-runtime\"],\r\n    conda_packages=[\"xlrd\", \"scikit-learn\", \"numpy\", \"pyyaml\", \"pip\"])\r\namlcompute_run_config = RunConfiguration(conda_dependencies=cd)\r\namlcompute_run_config.environment.docker.enabled = True\r\n```\r\n\r\nhere is the step:\r\n```\r\nadd_vendor_sets = PythonScriptStep(\r\n    name='Add Vendor set',\r\n    script_name='add_vendor_set.py',\r\n    arguments=['--respondent_dir', level_respondent,\r\n                '--my_dir', my_raw,\r\n                '--output_dir', factset_processed],\r\n    compute_target=compute_target,\r\n    inputs=[level_respondent, my_raw],\r\n    outputs=[my_processed],\r\n    runconfig=amlcompute_run_config,\r\n    source_directory=os.path.join(os.getcwd(), 'pipes\/add_vendor_set'),\r\n    allow_reuse=True\r\n)\r\n```\r\n\r\nThe environment is obviously included, but also definitely missing.  I'm stuck and now none of my pipelines, that were running in previous version, will work. \r\n\r\n",
        "Challenge_closed_time":1598387.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1598046974000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1111",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":18.0,
        "Challenge_reading_time":18.26,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"error: azureml-train-automl-runtime is required however it is included",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":115,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"can you share the full stacktrace? and is the error happening when you submit the pipeline script? or is it happening in the logs of the `PythonScriptStep`? ```\r\n\"error\": {\r\n        \"code\": \"UserError\",\r\n        \"message\": \"azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\",\r\n        \"detailsUri\": \"https:\/\/aka.ms\/azureml-known-errors\",\r\n        \"details\": [],\r\n        \"debugInfo\": {\r\n            \"type\": \"UserScriptException\",\r\n            \"message\": \"UserScriptException:\\n\\tMessage: azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\\n\\tInnerException OptionalDependencyMissingException:\\n\\tMessage: azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\\n\\tInnerException: None\\n\\tErrorResponse \\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"inner_error\\\": {\\n            \\\"code\\\": \\\"ValidationError\\\",\\n            \\\"inner_error\\\": {\\n                \\\"code\\\": \\\"ScenarioNotSuported\\\",\\n                \\\"inner_error\\\": {\\n                    \\\"code\\\": \\\"OptionalDependencyMissing\\\"\\n                }\\n            }\\n        },\\n        \\\"message\\\": \\\"azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\\\"\\n    }\\n}\\n\\tErrorResponse \\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\\\"\\n    }\\n}\",\r\n            \"stackTrace\": \"  File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/pjx-d-cu1-mlw-models\/azureml\/d881de2a-9dba-4e74-805e-d3c6eaab2076\/mounts\/workspaceblobstore\/azureml\/d881de2a-9dba-4e74-805e-d3c6eaab2076\/azureml-setup\/context_manager_injector.py\\\", line 197, in execute_with_context\\n    raise UserScriptException(baseEx).with_traceback(exceptionInfo[2])\\n  File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/pjx-d-cu1-mlw-models\/azureml\/d881de2a-9dba-4e74-805e-d3c6eaab2076\/mounts\/workspaceblobstore\/azureml\/d881de2a-9dba-4e74-805e-d3c6eaab2076\/azureml-setup\/context_manager_injector.py\\\", line 166, in execute_with_context\\n    runpy.run_path(sys.argv[0], globals(), run_name=\\\"__main__\\\")\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/runpy.py\\\", line 263, in run_path\\n    pkg_name=pkg_name, script_name=fname)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/runpy.py\\\", line 96, in _run_module_code\\n    mod_name, mod_spec, pkg_name, script_name)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/runpy.py\\\", line 85, in _run_code\\n    exec(code, run_globals)\\n  File \\\"run_models.py\\\", line 286, in \\n    main()\\n  File \\\"run_models.py\\\", line 197, in main\\n    run = experiment.submit(config=automl_config, tags=tags)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/core\/experiment.py\\\", line 211, in submit\\n    run = submit_func(config, self.workspace, self.name, **kwargs)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\\\", line 97, in _automl_static_submit\\n    show_output)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\\\", line 255, in _start_execution\\n    automl_run = _default_execution(experiment, settings_obj, fit_params, True, show_output, parent_run_id)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\\\", line 121, in _default_execution\\n    return automl_estimator.fit(**fit_params)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/_azureautomlclient.py\\\", line 349, in fit\\n    \\\"azureml-train-automl-runtime must be installed in the current environment to run local in \\\"\\n\"\r\n        },\r\n        \"messageFormat\": \"azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\",\r\n        \"messageParameters\": {}\r\n    },\r\n    \"time\": \"0001-01-01T00:00:00.000Z\"\r\n}\r\n``` Here is my stack trace from the 70_driver_log.txt:\r\n```\r\nTraceback (most recent call last):\r\n  File \"run_models.py\", line 286, in <module>\r\n    main()\r\n  File \"run_models.py\", line 197, in main\r\n    run = experiment.submit(config=automl_config, tags=tags)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/core\/experiment.py\", line 211, in submit\r\n    run = submit_func(config, self.workspace, self.name, **kwargs)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\", line 97, in _automl_static_submit\r\n    show_output)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\", line 255, in _start_execution\r\n    automl_run = _default_execution(experiment, settings_obj, fit_params, True, show_output, parent_run_id)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\", line 121, in _default_execution\r\n    return automl_estimator.fit(**fit_params)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/_azureautomlclient.py\", line 349, in fit\r\n    \"azureml-train-automl-runtime must be installed in the current environment to run local in \"\r\nUserScriptException: UserScriptException:\r\n\tMessage: azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\r\n``` @swatig007 this is an error, @BillmanH is experiencing when submitting an AutoML run from within a `PythonScriptStep` rather than using an `AutoMLStep`. This approach worked for over a year, but is now throwing an error about `azureml-train-automl-runtime` not being installed. upgraded to 1.12.0, which solved this problem and opened other issues. ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.5,
        "Solution_reading_time":83.64,
        "Solution_score_count":null,
        "Solution_sentence_count":50.0,
        "Solution_word_count":486.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":8,
        "Challenge_body":"<p>Hello, I ran a Unet on an other machine whithout internet and retrieved the offline Run. I am trying to sync it with wandb but, there are en issu that say:<\/p>\n<p>wandb: ERROR Uploading artifact file failed. Artifact won\u2019t be committed.<br>\nwandb: ERROR Error uploading [Path of the artefact on the other machine] : FileNotFoundError, [Errno 2] No such file or directory: [Path of the artefact on the other machine]<\/p>\n<p>When I searched for a solution, I couldn\u2019t find anything that realy helped and all the last topics had been finished whithout solution or answer from person who questions.<\/p>\n<p>I tryed this:<\/p>\n<ul>\n<li>\n<p>!wandb sync --project [the name] --entity [the name] [the path]<\/p>\n<\/li>\n<li>\n<p>!wandb sync  [the path]<\/p>\n<\/li>\n<li>\n<p>wandbId=wandb.util.generate_id()<br>\n!wandb sync [the path] --id wandbId<\/p>\n<\/li>\n<\/ul>\n<p>Thank you for your time, I hope you can help me<\/p>\n<p>Have a good day<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1681458415797,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/sync-offline-run-in-a-other-machine\/4218",
        "Challenge_link_count":0,
        "Challenge_participation_count":8,
        "Challenge_readability":7.5,
        "Challenge_reading_time":11.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Sync offline Run in a other machine",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":158.0,
        "Challenge_word_count":151,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/seirihiri\">@seirihiri<\/a>, thanks for the explanation! You can set the artifacts folder with the <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/environment-variables#optional-environment-variables\">env variable<\/a> <code>WANDB_CACHE_DIR <\/code> and point that to the folder you want. Regarding the artifacts created when running offline, it depends if you\u2019re creating them or not (they can be created not only when logging an <a href=\"https:\/\/docs.wandb.ai\/guides\/artifacts#how-it-works\">artifact<\/a> but also when logging tables for example). Let me know if this helps!<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.6,
        "Solution_reading_time":7.9,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":68.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1357215436787,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Netherlands",
        "Answerer_reputation_count":12551.0,
        "Answerer_view_count":1671.0,
        "Challenge_adjusted_solved_time":0.4334583333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to upload my <code>Excel<\/code> Workbook into Azure Machine Learning Studio. The reason is I have some data that I would like to join into my other <code>.csv<\/code> files to create a training data set. \nWhen I upload my <code>Excel<\/code>, I don't get <code>.xlsx<\/code>, or <code>.xls<\/code>, but other extensions such as <code>.csv<\/code>, <code>.txt<\/code> etc.. <\/p>\n\n<p>This is how it looks,\n<a href=\"https:\/\/i.stack.imgur.com\/SrSon.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/SrSon.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I uploaded anyways and now, I am getting weird characters. How can I get excel workbook uploaded and get my sheets, so, I can join data and do, data preparation. Any suggestions?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1530198936390,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51086377",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":10.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Upload Microsoft Excel Workbook with Many Sheets into Azure ML Studio",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":577.0,
        "Challenge_word_count":112,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1519936486960,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Minneapolis, MN, USA",
        "Poster_reputation_count":1113.0,
        "Poster_view_count":122.0,
        "Solution_body":"<p>You could save the workbook as a (set of) CSV file(s) and upload them separately.<\/p>\n\n<p>A CSV file, a '<a href=\"https:\/\/en.wikipedia.org\/wiki\/Comma-separated_values\" rel=\"nofollow noreferrer\">Comma Separated Values<\/a>' file, is exactly that. A flat file with some values separated by a comma. If you load an Excel file it will mess up since there's way more information in an Excel file than just values separated by comma's. Have a look at <code>File<\/code> -> <code>Save as<\/code> -> <code>Save as type<\/code> where you can select 'CSV (comma delimited) (*.csv)'<\/p>\n\n<p><em>Disclaimer: no, it's not always a comma...<\/em>  <\/p>\n\n<blockquote>\n  <p>In addition, the term \"CSV\" also denotes some closely related delimiter-separated formats that use different field delimiters. These include tab-separated values and space-separated values. A delimiter that is not present in the field data (such as tab) keeps the format parsing simple. These alternate delimiter-separated files are often even given a .csv extension despite the use of a non-comma field separator.<\/p>\n<\/blockquote>\n\n<p><strong>Edit<\/strong><br>\nSo apparently Excel files <em>are<\/em> supported: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/desktop-workbench\/data-prep-appendix2-supported-data-sources\" rel=\"nofollow noreferrer\">Supported data sources for Azure Machine Learning data preparation<\/a>  <\/p>\n\n<p><em>Excel (.xls\/.xlsx)<\/em><br>\nRead an Excel file one sheet at a time by specifying sheet name or number.<\/p>\n\n<p>But also, only UTF-8 is supported: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/import-data#bkmk_Notes\" rel=\"nofollow noreferrer\">Import Data - Technical notes<\/a><\/p>\n\n<blockquote>\n  <p>Azure Machine Learning requires UTF-8 encoding. If the data you are importing uses a different encoding, or was exported from a data source that uses a different default encoding, various problems might appear in the text.<\/p>\n<\/blockquote>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1530200496840,
        "Solution_link_count":3.0,
        "Solution_readability":12.1,
        "Solution_reading_time":25.46,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":241.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"## Description\r\n\r\nWhen I launch `kedro run` and the run fails, the `on_pipeline_error` closes all the mlflow runs (to avoid interactions with further runs)\r\n\r\n## Context\r\n\r\nI cannot distinguish failed runs from sucessful ones in the mlflow ui.\r\n\r\n## Steps to Reproduce\r\n\r\nLaunch a failing pipeline with kedro run.\r\n\r\n## Expected Result\r\n\r\nThe mlflow ui should display the run with a red cross\r\n\r\n## Actual Result\r\n\r\nThe mlflow ui displays the run with a green tick\r\n\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes.\r\n\r\n## Potential solution: \r\n\r\nReplace these lines:\r\n\r\n`https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L193-L194`\r\n\r\nwith \r\n\r\n```python\r\nwhile mlflow.active_run():\r\n    mlflow.end_run(mlflow.entities.RunStatus.FAILED)\r\n```\r\nor even better, retrieve current run status from mlflow?\r\n",
        "Challenge_closed_time":1606515.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605982845000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/121",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.8,
        "Challenge_reading_time":11.93,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"RunStatus of mlflow run is \"FINISHED\" instead of \"FAILED\" when the kedro run fails",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":117,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Good catch ! \r\nSince we catch the Error and manually end the run, mlflow do not receive the \"error code 1\" of the current process. If we no longer end run manually, mlflow will tag the run as FAILED. But since we want to control the pipeline error, we can apply your suggestion (specifiying the status as failed) Yes, but we need to terminate the run manually when it failed and one use it interactively (in CLI, tis makes no difference because it gets the error code as you say) to avoid further interference.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.0,
        "Solution_reading_time":6.1,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":93.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1603750656892,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New Zealand",
        "Answerer_reputation_count":31.0,
        "Answerer_view_count":4.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p><strong>Objective<\/strong>: Generate a down-sampled FileDataset using random sampling from a larger FileDataset to be used in a Data Labeling project.<\/p>\n<hr \/>\n<p><strong>Details<\/strong>: I have a large FileDataset containing millions of images. Each filename contains details about the 'section' it was taken from. A section may contain thousands of images. I want to randomly select a specific number of <strong>sections<\/strong> and all the images associated with those sections. Then register the sample as a new dataset.<\/p>\n<p>Please note that the code below is not a direct copy and paste as there are elements such as filepaths and variables that have been renamed for confidentiality reasons.<\/p>\n<pre><code>import azureml.core\nfrom azureml.core import Dataset, Datastore, Workspace\n\n# Load in work space from saved config file\nws = Workspace.from_config()\n\n# Define full dataset of interest and retrieve it\ndataset_name = 'complete_2017'\ndata = Dataset.get_by_name(ws, dataset_name)\n\n# Extract file references from dataset as relative paths\nrel_filepaths = data.to_path()\n\n# Stitch back in base directory path to get a list of absolute paths\nsrc_folder = '\/raw-data\/2017'\nabs_filepaths = [src_folder + path for path in rel_filepaths]\n\n# Define regular expression pattern for extracting source section\nimport re\npattern = re.compile('\\\/(S.+)_image\\d+.jpg')\n\n# Create new list of all unique source sections\nsections = sorted(set([m.group(1) for m in map(pattern.match, rel_filepaths) if m]))\n\n# Randomly select sections\nnum_sections = 5\nset_seed = 221020\nrandom.seed(set_seed)   # for repeatibility\nsample_sections = random.choices(sections, k = num_sections)\n\n# Extract images related to the selected sections\nmatching_images = [filename for filename in abs_filepaths if any(section in filename for section in sample_sections)]\n\n# Define datastore of interest\ndatastore = Datastore.get(ws, 'ml-datastore')\n\n# Convert string paths to Azure Datapath objects and relate back to datastore\nfrom azureml.data.datapath import DataPath\ndatastore_path = [DataPath(datastore, filepath) for filepath in matching_images]\n\n# Generate new dataset using from_files() and filtered list of paths\nsample = Dataset.File.from_files(datastore_path)\n\nsample_name = 'random-section-sample'\nsample_dataset = sample.register(workspace = ws, name = sample_name, description = 'Sampled sections from full dataset using set seed.')\n<\/code><\/pre>\n<hr \/>\n<p><strong>Issue<\/strong>: The code I've written in Python SDK runs and the new FileDataset registers, but when I try to look at the dataset details or use it for a Data Labeling project I get the following error even as <em>Owner<\/em>.<\/p>\n<pre><code>Access denied: Failed to authenticate data access with Workspace system assigned identity. Make sure to add the identity as Reader of the data service.\n<\/code><\/pre>\n<p>Additionally, under the details tab <strong>Files in dataset<\/strong> is <em>Unknown<\/em> and <strong>Total size of files in dataset<\/strong> is <em>Unavailable<\/em>.<\/p>\n<p>I haven't come across this issue anywhere else. I'm able to generate datasets in other ways, so I suspect it's an issue with the code given that I'm working with the data in an unconventional way.<\/p>\n<hr \/>\n<p><strong>Additional Notes<\/strong>:<\/p>\n<ul>\n<li>Azure ML version is 1.15.0<\/li>\n<\/ul>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1603756226340,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64546521",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.1,
        "Challenge_reading_time":43.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML FileDataset registers, but cannot be accessed for Data Labeling project",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":801.0,
        "Challenge_word_count":432,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1603750656892,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"New Zealand",
        "Poster_reputation_count":31.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>One of my colleagues discovered that the managed identities were preventing the preview functionality. Once this aspect of the identities was modified, we could examine the data and use it for a data labelling project.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.3,
        "Solution_reading_time":2.81,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":35.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":7,
        "Challenge_body":"<p>I\u2019m using AWS Sagemaker to train a Keras model with the Wandb callback. In my Sagemaker script, I save checkpoints to <code>'\/opt\/ml\/checkpoints\/'<\/code> which it redirects to an s3 bucket continuously. After the model has finished training, I create my artifact and add a reference to that bucket.<\/p>\n<p>Later, if I try to download the model with:<\/p>\n<pre><code class=\"lang-auto\">model_path = run.use_artifact(...)\nmodel_path.download()\n<\/code><\/pre>\n<p>I get the following error:<\/p>\n<blockquote>\n<p>ValueError: Digest mismatch for object s3:\/\/\u2026\/variables\/variables.data-00000-of-00001: expected 4f8d37a52a3e87f1f0ee2d3101688848-3 but found 8ad5ef5242d547d7edaa76f620597b60-3<\/p>\n<\/blockquote>\n<p>My guess is that I\u2019ve added the reference to the artifact before Sagemaker has pushed the final model from the local directory to S3. I\u2019m not sure how to get around this, is there a better way to have my Artifacts be linked to an S3 bucket?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666051892610,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/digest-mismatch-error-when-trying-to-download-model-artifact-from-s3\/3269",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":9.9,
        "Challenge_reading_time":12.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Digest mismatch error when trying to download model artifact from S3",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":308.0,
        "Challenge_word_count":136,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/dspectrum\">@dspectrum<\/a>,<\/p>\n<p>Looking at your error and tracing back through our code - looks like versioning is not enabled on your S3 bucket, which means the artifact is changing the file itself, leading to different hashes. I would suggest turning on versioning on your S3 bucket and letting me know if you still run into the same error.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.6,
        "Solution_reading_time":4.74,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":59.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1256573050950,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Auckland, New Zealand",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":606.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I found deleting a <code>run<\/code> only change the state from <code>active<\/code> to <code>deleted<\/code>, because the run is still visible in the UI if searching by <code>deleted<\/code>. <\/p>\n\n<p>Is it possible to remove a <code>run<\/code> from the UI to save the space? \nWhen removing a run, does the artifact correspond to the run is also removed?<\/p>\n\n<p>If not, can the run be removed through rest call?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568794247973,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1568796468592,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57987999",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.5,
        "Challenge_reading_time":6.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Delete a run in the experiment of mlflow from the UI so the run does not exist in backend store",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":3582.0,
        "Challenge_word_count":84,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408370821672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berlin, Germany",
        "Poster_reputation_count":2521.0,
        "Poster_view_count":197.0,
        "Solution_body":"<p>You can't do it via the web UI but you can from a python terminal<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\n\nmlflow.delete_experiment(69)\n<\/code><\/pre>\n\n<p>Where 69 is the experiment ID<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.7,
        "Solution_reading_time":2.76,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":27.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1350768619470,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tunisia",
        "Answerer_reputation_count":13575.0,
        "Answerer_view_count":1140.0,
        "Challenge_adjusted_solved_time":187.9993902778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I was evaluating what is needed to write your own Estimator in Sagemaker. I was following this example <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\/container\" rel=\"nofollow noreferrer\">here<\/a> and it's well explained and quite simple.<\/p>\n<p>My question is regarding the inference <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/container\/decision_trees\/predictor.py\" rel=\"nofollow noreferrer\">here<\/a>. I see an example in which we can feed the <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/e648e9a6f596263c7683635d1a55f1729b08277d\/advanced_functionality\/scikit_bring_your_own\/container\/decision_trees\/predictor.py#L60\" rel=\"nofollow noreferrer\">invocations endpoint<\/a> a CSV. What if I want to just post a string or even individual parameters? What's the best practise for that? I see there is a condition like:<\/p>\n<pre><code>if flask.request.content_type == &quot;text\/csv&quot;:\n<\/code><\/pre>\n<p>Should we add more like those to support different formats or should we create a new endpoint?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630420918507,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69000752",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":16.2,
        "Challenge_reading_time":16.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Amazon Sagemaker: write your own inference",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":168.0,
        "Challenge_word_count":108,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1322785469840,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cork, Ireland",
        "Poster_reputation_count":482.0,
        "Poster_view_count":40.0,
        "Solution_body":"<p>You need to add support for more content types.<\/p>\n<p>Since you would like to pass a string or a parameter, I suggest you add support for &quot;application\/json&quot; MIME media type (<a href=\"https:\/\/stackoverflow.com\/questions\/477816\/what-is-the-correct-json-content-type\">What is the correct JSON content type?<\/a>). Then your users will call the API with a Json that you can parse and extract parameters from in the backend.<\/p>\n<p>For example, if you have two parameters <code>age<\/code> and <code>gender<\/code> you want to pass to your model. You can put them in the following Json datastructure:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n &quot;age&quot;: ...,\n &quot;gender&quot;: ...\n}\n<\/code><\/pre>\n<p>Then add support for loading the Json and extracting the parameters in the backend as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>if flask.request.content_type == &quot;application\/json&quot;:\n    data = flask.request.data.decode(&quot;utf-8&quot;)\n    data = json.loads(data)\n    parameter1 = data['age']\n    parameter2 = data['gender']\n    ...\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1631097716312,
        "Solution_link_count":1.0,
        "Solution_readability":11.5,
        "Solution_reading_time":14.03,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":121.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>I\u2019ve been trying to find some documentation, I don\u2019t want to save all the hyperparameters each epoch, just the learning rate.<br>\nWould be so great if you can help me out.<\/p>\n<p>Cheers,<\/p>\n<p>Oli<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1677518425190,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-log-the-learning-rate-with-pytorch-lightning-when-using-a-scheduler\/3964",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":5.3,
        "Challenge_reading_time":3.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"How to log the learning rate with pytorch lightning when using a scheduler?",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":516.0,
        "Challenge_word_count":45,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I\u2019m also wondering how this is done! Whether within a sweep configuration or not - when using a lr scheduler, I am trying to track the lr at epoch during training, as it is now dynamic. Even within a sweep, you will have some initial lr  determined during the sweep, but it will not stay constant for the duration of training.<\/p>\n<p>edit:<\/p>\n<p>The example on the <a href=\"https:\/\/pytorch-lightning.readthedocs.io\/en\/1.2.10\/api\/pytorch_lightning.callbacks.lr_monitor.html#learning-rate-monitor\" rel=\"noopener nofollow ugc\">lightning site here<\/a> worked for me:<\/p>\n<pre><code class=\"lang-auto\">&gt;&gt;&gt; from pytorch_lightning.callbacks import LearningRateMonitor\n&gt;&gt;&gt; lr_monitor = LearningRateMonitor(logging_interval='step')\n&gt;&gt;&gt; trainer = Trainer(callbacks=[lr_monitor])\n<\/code><\/pre>\n<p>Passing the <code>WandBLogger<\/code> to the trainer I see my lr is logged on the <code>wandb<\/code> dashboard.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.8,
        "Solution_reading_time":12.09,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":104.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1604093818187,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Krakow, Poland",
        "Answerer_reputation_count":1200.0,
        "Answerer_view_count":263.0,
        "Challenge_adjusted_solved_time":562.4734544444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm new to Google Cloud Platform and I'm trying to create a Feature Store to fill with values from a csv file from Google Cloud Storage. The aim is to do that from a local notebook in Python.\nI'm basically following the code <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/master\/notebooks\/official\/feature_store\/gapic-feature-store.ipynb\" rel=\"nofollow noreferrer\">here<\/a>, making the appropriate changes since I'm working with the credit card public dataset.\nThe error that raises when I run the code is the following:<\/p>\n<pre><code>GoogleAPICallError: None Unexpected state: Long-running operation had neither response nor error set.\n<\/code><\/pre>\n<p>and it happens during the ingestion of the data from the csv file.<\/p>\n<p>Here it is the code I'm working on:<\/p>\n<pre><code>import os\nfrom datetime import datetime\nfrom google.cloud import bigquery\nfrom google.cloud import aiplatform\nfrom google.cloud.aiplatform_v1.types import feature as feature_pb2\nfrom google.cloud.aiplatform_v1.types import featurestore as featurestore_pb2\nfrom google.cloud.aiplatform_v1.types import \\\n    featurestore_service as featurestore_service_pb2\nfrom google.cloud.aiplatform_v1.types import entity_type as entity_type_pb2\nfrom google.cloud.aiplatform_v1.types import FeatureSelector, IdMatcher\n\ncredential_path = r&quot;C:\\Users\\...\\.json&quot;\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path\n\n## Constants\nPROJECT_ID = &quot;my-project-ID&quot;\nREGION = &quot;us-central1&quot;\nAPI_ENDPOINT = &quot;us-central1-aiplatform.googleapis.com&quot;\nINPUT_CSV_FILE = &quot;my-input-file.csv&quot;\nFEATURESTORE_ID = &quot;fraud_detection&quot;\n\n## Output dataset\nDESTINATION_DATA_SET = &quot;fraud_predictions&quot;\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\nDESTINATION_DATA_SET = &quot;{prefix}_{timestamp}&quot;.format(\n    prefix=DESTINATION_DATA_SET, timestamp=TIMESTAMP\n)\n\n## Output table. Make sure that the table does NOT already exist; \n## the BatchReadFeatureValues API cannot overwrite an existing table\nDESTINATION_TABLE_NAME = &quot;training_data&quot;\n\nDESTINATION_PATTERN = &quot;bq:\/\/{project}.{dataset}.{table}&quot;\nDESTINATION_TABLE_URI = DESTINATION_PATTERN.format(\n    project=PROJECT_ID, dataset=DESTINATION_DATA_SET, \n    table=DESTINATION_TABLE_NAME\n)\n\n## Create dataset\nclient = bigquery.Client(project=PROJECT_ID)\ndataset_id = &quot;{}.{}&quot;.format(client.project, DESTINATION_DATA_SET)\ndataset = bigquery.Dataset(dataset_id)\ndataset.location = REGION\ndataset = client.create_dataset(dataset)\nprint(&quot;Created dataset {}.{}&quot;.format(client.project, dataset.dataset_id))\n\n## Create client for CRUD and data_client for reading feature values.\nclient = aiplatform.gapic.FeaturestoreServiceClient(\n    client_options={&quot;api_endpoint&quot;: API_ENDPOINT})\ndata_client = aiplatform.gapic.FeaturestoreOnlineServingServiceClient(\n    client_options={&quot;api_endpoint&quot;: API_ENDPOINT})\nBASE_RESOURCE_PATH = client.common_location_path(PROJECT_ID, REGION)\n\n## Create featurestore (only the first time)\ncreate_lro = client.create_featurestore(\n    featurestore_service_pb2.CreateFeaturestoreRequest(\n        parent=BASE_RESOURCE_PATH,\n        featurestore_id=FEATURESTORE_ID,\n        featurestore=featurestore_pb2.Featurestore(\n            online_serving_config=featurestore_pb2.Featurestore.OnlineServingConfig(\n                fixed_node_count=1\n            ),\n        ),\n    )\n)\n\n## Wait for LRO to finish and get the LRO result.\nprint(create_lro.result())\n\nclient.get_featurestore(\n    name=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID)\n)\n\n## Create credit card entity type (only the first time)\ncc_entity_type_lro = client.create_entity_type(\n    featurestore_service_pb2.CreateEntityTypeRequest(\n        parent=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID),\n        entity_type_id=&quot;creditcards&quot;,\n        entity_type=entity_type_pb2.EntityType(\n            description=&quot;Credit card entity&quot;,\n        ),\n    )\n)\n\n## Create fraud entity type (only the first time)\nfraud_entity_type_lro = client.create_entity_type(\n    featurestore_service_pb2.CreateEntityTypeRequest(\n        parent=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID),\n        entity_type_id=&quot;frauds&quot;,\n        entity_type=entity_type_pb2.EntityType(\n            description=&quot;Fraud entity&quot;,\n        ),\n    )\n)\n\n## Create features for credit card type (only the first time)\nclient.batch_create_features(\n    parent=client.entity_type_path(PROJECT_ID, REGION, FEATURESTORE_ID, &quot;creditcards&quot;),\n    requests=[\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v1&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v2&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v3&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v4&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v5&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v6&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v7&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v8&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v9&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v10&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v11&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v12&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v13&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v14&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v15&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v16&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v17&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v18&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v19&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v20&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v21&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v22&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v23&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v24&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v25&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v26&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v27&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v28&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;amount&quot;,\n        ),\n    ],\n).result()\n\n## Create features for fraud type (only the first time)\nclient.batch_create_features(\n    parent=client.entity_type_path(PROJECT_ID, REGION, FEATURESTORE_ID, &quot;frauds&quot;),\n    requests=[\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;class&quot;,\n        ),\n    ],\n).result()\n\n## Import features values for credit cards\nimport_cc_request = aiplatform.gapic.ImportFeatureValuesRequest(\n    entity_type=client.entity_type_path(\n        PROJECT_ID, REGION, FEATURESTORE_ID, &quot;creditcards&quot;),\n    csv_source=aiplatform.gapic.CsvSource(gcs_source=aiplatform.gapic.GcsSource(\n        uris=[&quot;gs:\/\/fraud-detection-19102021\/dataset\/cc_details_train.csv&quot;])),\n    entity_id_field=&quot;cc_id&quot;,\n    feature_specs=[\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v1&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v2&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v3&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v4&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v5&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v6&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v7&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v8&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v9&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v10&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v11&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v12&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v13&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v14&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v15&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v16&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v17&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v18&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v19&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v20&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v21&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v22&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v23&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v24&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v25&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v26&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v27&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v28&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;amount&quot;),\n    ],\n    feature_time_field='time',\n    worker_count=1,\n)\n\n## Start to import\ningestion_lro = client.import_feature_values(import_cc_request)\n\n## Polls for the LRO status and prints when the LRO has completed\ningestion_lro.result()\n\n## Import features values for frauds\nimport_fraud_request = aiplatform.gapic.ImportFeatureValuesRequest(\n    entity_type=client.entity_type_path(\n        PROJECT_ID, REGION, FEATURESTORE_ID, &quot;frauds&quot;),\n    csv_source=aiplatform.gapic.CsvSource(gcs_source=aiplatform.gapic.GcsSource(\n        uris=[&quot;gs:\/\/fraud-detection-19102021\/dataset\/data_fraud_train.csv&quot;])),\n    entity_id_field=&quot;fraud_id&quot;,\n    feature_specs=[\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;class&quot;),\n    ],\n    feature_time_field='time',\n    worker_count=1,\n)\n\n## Start to import\ningestion_lro = client.import_feature_values(import_fraud_request)\n\n## Polls for the LRO status and prints when the LRO has completed\ningestion_lro.result()\n<\/code><\/pre>\n<p>When I check the <code>Ingestion Jobs<\/code> from the <code>Feature<\/code> section of Google Cloud Console I see that the job has finished but no values are added to my features.<\/p>\n<p>Any advice it is really precious.<\/p>\n<p>Thank you all.<\/p>\n<p><strong>EDIT 1<\/strong>\nIn the image below there is an example of the first row of the csv file I used as input (<code>cc_details_train.csv<\/code>). All the unseen features  are similar, the feature <code>class<\/code> can assume 0 or 1 values.\nThe injection job lasts about 5 minutes to import (ideally) 3000 rows, but it ends without error and without importing any value.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Z34hG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Z34hG.png\" alt=\"Rows of my csv file\" \/><\/a><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":7,
        "Challenge_created_time":1635242363740,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1636446864247,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69721067",
        "Challenge_link_count":3,
        "Challenge_participation_count":8,
        "Challenge_readability":32.3,
        "Challenge_reading_time":202.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":90,
        "Challenge_solved_time":null,
        "Challenge_title":"GoogleAPICallError: None Unexpected state: Long-running operation had neither response nor error set",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":357.0,
        "Challenge_word_count":714,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1616589293616,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Alatri, Frosinone, FR",
        "Poster_reputation_count":67.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p><strong>VERTEX AI recomendations when using CSV to ImportValues \/ using ImportFeatureValuesRequest<\/strong><\/p>\n<p>Its possible that when using this feature you might end not able to import any data at all. You must pay attention to the time field you are using as it must be in compliance with google time formats.<\/p>\n<ol>\n<li>feature_time_field, must follow the time constraint rule set by google which is RFC3339, ie: '2021-04-15T08:28:14Z'. You can check details about the field <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.featurestores.entityTypes\/importFeatureValues#request-body\" rel=\"nofollow noreferrer\">here<\/a> and details about timestamp format can be found <a href=\"https:\/\/developers.google.com\/protocol-buffers\/docs\/reference\/google.protobuf#timestamp\" rel=\"nofollow noreferrer\">here<\/a>.<\/li>\n<li>Other columns, fields must match is designed value. One exception is field entity_id_field, As it can be any value.<\/li>\n<\/ol>\n<p>Note: I my test i found that if i do not properly set up the time field as google recommended date format it will just not upload any feature value at all.<\/p>\n<p><em>test.csv<\/em><\/p>\n<pre><code>cc_id,time,v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,v11,v12,v13,v14,v15,v16,v17,v18,v19,v20,v21,v22,v23,v24,v25,v26,v27,v28,amount\n100,2021-04-15T08:28:14Z,-1.359807,-0.072781,2.534897,1.872351,2.596267,0.465238,0.923123,0.347986,0.987354,1.234657,2.128645,1.958237,0.876123,-1.712984,-0.876436,1.74699,-1.645877,-0.936121,1.456327,0.087623,1.900872,2.876234,1.874123,0.923451,0.123432,0.000012,1.212121,0.010203,1000\n<\/code><\/pre>\n<p><em>output:<\/em><\/p>\n<pre><code>imported_entity_count: 1\nimported_feature_value_count: 29\n<\/code><\/pre>\n<p><strong>About optimization and working with features<\/strong><\/p>\n<p>You can check the official documentation <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/prepare-text#single-label-classification\" rel=\"nofollow noreferrer\">here<\/a> to see the min and max amount of records recommended for processing. As a piece of advice you should only use the actual working features to run and the recommended amount of values for it.<\/p>\n<p><strong>See your running ingested job<\/strong><\/p>\n<p>Either if you use VertexUI or code to generated the ingested job. You can track its run by going into the UI to this path:<\/p>\n<pre><code>VertexAI &gt; Features &gt; View Ingested Jobs \n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1638471768683,
        "Solution_link_count":3.0,
        "Solution_readability":13.0,
        "Solution_reading_time":31.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":239.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1221810788500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paderborn, North-Rhine-Westphalia, Germany",
        "Answerer_reputation_count":68522.0,
        "Answerer_view_count":7896.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I just started MLflow today and fail to display the log result on MLflow ui interface.\nWill appreciate a lot if someone can give me some hint..<\/p>\n<p>tried the sample code below<\/p>\n<pre><code>import os\nfrom random import random, randint\nfrom mlflow import log_metric, log_param, log_artifacts\n\nif __name__ == &quot;__main__&quot;:\n    # Log a parameter (key-value pair)\n    log_param(&quot;param1&quot;, randint(0, 100))\n\n    # Log a metric; metrics can be updated throughout the run\n    log_metric(&quot;foo&quot;, random())\n    log_metric(&quot;foo&quot;, random() + 1)\n    log_metric(&quot;foo&quot;, random() + 2)\n\n    # Log an artifact (output file)\n    if not os.path.exists(&quot;outputs&quot;):\n        os.makedirs(&quot;outputs&quot;)\n    with open(&quot;outputs\/test.txt&quot;, &quot;w&quot;) as f:\n        f.write(&quot;hello world!&quot;)\n    log_artifacts(&quot;outputs&quot;)\n<\/code><\/pre>\n<p>ran the script above for 3 times and it gave me the result in the following structure. 3 folders representing 3 runs separately:<\/p>\n<pre><code>file:\/\/\/home\/devuser\/project\/mlruns\/0\n0 - 0737fec7d4824384b6320070cd688b78\n    355d57e092a242b7aa263451d280b497 \n    ed2614ffe2fd4f2db991d5d7166635f8  \n    meta.yaml\n<\/code><\/pre>\n<p>with folders\/files <code>artifacts, meta.yaml, metrics, params, tags<\/code> in each folder separately.<\/p>\n<p>I ran <code>mlflow ui<\/code> under <code>file:\/\/\/home\/devuser\/project\/mlruns\/<\/code> but nothing was showed on the interface. tried to look this up but no one has come across this problem with this kind of simple code.<\/p>\n<p>Appreciate a lot if someone could kindly let me know how I can change my setting.. Thank you..<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622801740880,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67835498",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":21.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"MLflow - How to point interface path to show the expected result",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":379.0,
        "Challenge_word_count":193,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1445157877636,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":267.0,
        "Poster_view_count":111.0,
        "Solution_body":"<p>You need to run <code>mlflow ui<\/code> in the project directory itself, not inside the <code>mlruns<\/code> - if you look into the <a href=\"https:\/\/mlflow.org\/docs\/latest\/cli.html#mlflow-ui\" rel=\"nofollow noreferrer\">documentation for <code>mlflow ui<\/code> command<\/a>, it says:<\/p>\n<blockquote>\n<p><code>--default-artifact-root &lt;URI&gt;<\/code><\/p>\n<p>Path to local directory to store artifacts, for new experiments. Note that this flag does not impact already-created experiments. <strong>Default: .\/mlruns<\/strong><\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.1,
        "Solution_reading_time":7.12,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":55.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"I have an issue while getting Catboost image URI. It is a function for generating ECR image URIs for pre-built SageMaker Docker images. Here is my code catboost_container = sagemaker.image_uris.retrieve(\"catboost\", my_region, \"latest\")",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658343315554,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668463143111,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU3HFACW88SuKcGZ2izeOsuA\/filenotfounderror-errno-2-no-such-file-or-directory-home-ec2-user-anaconda3-envs-python3-lib-python3-8-site-packages-sagemaker-image-uri-config-catboost-json",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":14.6,
        "Challenge_reading_time":5.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"FileNotFoundError: [Errno 2] No such file or directory: '\/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.8\/site-packages\/sagemaker\/image_uri_config\/catboost.json'",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":167.0,
        "Challenge_word_count":39,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"As illustrated [here in the docs for the algorithm](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/catboost.html#catboost-modes), the parameters for retrieving this URI are a bit different: It's more like using the new JumpStart models (if you're familiar with that) than the old-style pre-built algorithms.\n\n```\ntrain_model_id, train_model_version, train_scope = \"catboost-classification-model\", \"*\", \"training\"\ntraining_instance_type = \"ml.m5.xlarge\"\n\n# Retrieve the docker image\ntrain_image_uri = image_uris.retrieve(\n    region=None,\n    framework=None,\n    model_id=train_model_id,\n    model_version=train_model_version,\n    image_scope=train_scope,\n    instance_type=training_instance_type\n)\n```\n\nI tested the above snippet from the doc page on SageMaker Studio and it worked OK. If you still see errors, it's likely your SageMaker Python SDK version is outdated (which can happen if for example you don't restart SM Studio apps or SM Notebook Instances regularly). Can check with `sagemaker.__version__` and upgrade with `!pip install --upgrade sagemaker` if needed.",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1658369922830,
        "Solution_link_count":1.0,
        "Solution_readability":13.0,
        "Solution_reading_time":13.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":116.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nWhat parquet data loading logic is known to work well to train with SageMaker on parquet? ml-io? pyarrow? any examples? That would be to train a classifier, either logistic regression, XGBoost or custom TF.",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1588841008000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668588105088,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUCqvDUq4hSQqRT97tBUvE8Q\/training-a-classifier-on-parquet-with-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.8,
        "Challenge_reading_time":3.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Training a classifier on parquet with SageMaker ?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":424.0,
        "Challenge_word_count":42,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"XGBoost as a framework container (v0.90+) can read parquet for training (see example [notebook][1]).  \nThe full list of valid content types are CSV, LIBSVM, PARQUET, RECORDIO_PROTOBUF (see [source][2]) \n\nAdditionally:  \n[Uber Petastorm][3] for reading parquet into Tensorflow, Pytorch, and PySpark inputs.   \nAs XGBoost accepts numpy, you can convert from PySpark to numpy\/pandas using the mentioned PyArrow.\n\n\n  [1]: https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/caf9363c0242d0da2de7f5765e7318fd843ce4c3\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_parquet_input_training.ipynb\n  [2]: https:\/\/github.com\/aws\/sagemaker-xgboost-container\/blob\/5e778770e009ce989e288e7bbc1255556129e75b\/src\/sagemaker_xgboost_container\/data_utils.py#L40\n  [3]: https:\/\/github.com\/uber\/petastorm",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925592848,
        "Solution_link_count":3.0,
        "Solution_readability":19.1,
        "Solution_reading_time":10.59,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":61.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1605831014392,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using AWS Sagemaker Ground Truth for a Custom Labeling Task that involves editing bounding boxes and their labels.  Ground Truth's UI has built-in keyboard shortcuts for doing things like choosing the label for a box, but it seems to lack shortcuts for other built-in UI elements like &quot;No adjustments needed&quot; or the &quot;Submit&quot; button.<\/p>\n<p>Is there a way to add such shortcuts?  I've looked at the crowd-html-elements for customizing the appearance of the page, but can't find anything in there about keyboard shortcuts.  It doesn't even look like crowd-button or crowd-icon-button support specifying a shortcut as an attribute.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1602271699933,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64286191",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":8.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"How to add keyboard shortcuts to AWS Ground Truth labeler UI?",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":412.0,
        "Challenge_word_count":109,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1289772110723,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Mt Kisco, NY",
        "Poster_reputation_count":309.0,
        "Poster_view_count":40.0,
        "Solution_body":"<p>Could try something like:<\/p>\n<pre><code>document.addEventListener('keydown', function(event) {\n  if (event.shiftKey &amp;&amp; event.keyCode === 13) {\n    document.getElementsByTagName('crowd-bounding-box')[0].shadowRoot.getElementById('nothing-to-adjust').querySelector('label').click();\n  }\n});\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":38.8,
        "Solution_reading_time":4.25,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":13.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1456986606312,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":757.0,
        "Answerer_view_count":80.0,
        "Challenge_adjusted_solved_time":2717.5608175,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I've got some data on S3 bucket that I want to work with. <\/p>\n\n<p>I've imported it using:<\/p>\n\n<pre><code>import boto3\nimport dask.dataframe as dd\n\ndef import_df(key):\n        s3 = boto3.client('s3')\n        df = dd.read_csv('s3:\/\/...\/' + key ,encoding='latin1')\n        return df\n\nkey = 'Churn\/CLEANED_data\/file.csv'\ntrain = import_df(key)\n<\/code><\/pre>\n\n<p>I can see that the data has been imported correctly using:<\/p>\n\n<pre><code>train.head()\n<\/code><\/pre>\n\n<p>but when I try simple operation (<a href=\"https:\/\/docs.dask.org\/en\/latest\/dataframe.html\" rel=\"nofollow noreferrer\">taken from this dask doc<\/a>):<\/p>\n\n<pre><code>train_churn = train[train['CON_CHURN_DECLARATION'] == 1]\ntrain_churn.compute()\n<\/code><\/pre>\n\n<p>I've got Error:<\/p>\n\n<blockquote>\n  <p>AttributeError                            Traceback (most recent call\n  last)  in ()<\/p>\n  \n  <p>1 train_churn = train[train['CON_CHURN_DECLARATION'] == 1]<\/p>\n  \n  <p>----> 2 train_churn.compute()<\/p>\n  \n  <p>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/dask\/base.py in\n  compute(self, **kwargs)\n      152         dask.base.compute\n      153         \"\"\"\n  --> 154         (result,) = compute(self, traverse=False, **kwargs)\n      155         return result\n      156<\/p>\n  \n  <p>AttributeError: 'DataFrame' object has no attribute '_getitem_array'<\/p>\n<\/blockquote>\n\n<p>Full error here: <a href=\"https:\/\/textuploader.com\/11lg7\" rel=\"nofollow noreferrer\">Error Upload<\/a><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1564579246737,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57291797",
        "Challenge_link_count":2,
        "Challenge_participation_count":5,
        "Challenge_readability":10.7,
        "Challenge_reading_time":18.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Dask: AttributeError: 'DataFrame' object has no attribute '_getitem_array'",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2742.0,
        "Challenge_word_count":129,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1429630461500,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Warszawa, Polska",
        "Poster_reputation_count":938.0,
        "Poster_view_count":137.0,
        "Solution_body":"<p>I was facing a similar issue when trying to read from s3 files, ultimately solved by updating dask to most recent version (I think the one sagemaker instances start with by default is deprecated)<\/p>\n\n<h2>Install\/Upgrade packages and dependencies (from notebook)<\/h2>\n\n<pre><code>! python -m pip install --upgrade dask\n! python -m pip install fsspec\n! python -m pip install --upgrade s3fs\n<\/code><\/pre>\n\n<p>Hope this helps!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1574362465680,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":5.35,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1361290436103,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":690.0,
        "Answerer_view_count":38.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":8,
        "Challenge_body":"<p>I've just started to experiment with AWS SageMaker and would like to load data from an S3 bucket into a pandas dataframe in my SageMaker python jupyter notebook for analysis.<\/p>\n\n<p>I could use boto to grab the data from S3, but I'm wondering whether there is a more elegant method as part of the SageMaker framework to do this in my python code?<\/p>\n\n<p>Thanks in advance for any advice.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1516025246727,
        "Challenge_favorite_count":15.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48264656",
        "Challenge_link_count":0,
        "Challenge_participation_count":8,
        "Challenge_readability":8.3,
        "Challenge_reading_time":5.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":57.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Load S3 Data into AWS SageMaker Notebook",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":78494.0,
        "Challenge_word_count":75,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1487350945116,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":673.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>If you have a look <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-dg.pdf\" rel=\"noreferrer\">here<\/a> it seems you can specify this in the <em>InputDataConfig<\/em>. Search for \"S3DataSource\" (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_S3DataSource.html\" rel=\"noreferrer\">ref<\/a>) in the document. The first hit is even in Python, on page 25\/26.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.2,
        "Solution_reading_time":5.17,
        "Solution_score_count":11.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":36.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nI have several runs with the named RUN_NAME each run is logging an artifact named ARTIFACT_NAME. I would like to query all these artifacts from all this subset of runs.\n\nWhat I\u2019m using to try to achieve this is calling RunClient.client.runs_v1.get_runs_artifacts_lineage, but in this function I get all artifacts from all runs ever independently from the run name.\nAlso, when listing the artifacts from these runs, for some reason the path is always None, even though if I use the function you suggested last time, get_artifacts_lineage , for a specific run, I do get values on the path field.\nSo my two main issues are:\n\nHow do I get all artifacts from a set of runs with the same run_name?\nWhy the path is empty in case this is the correct function to use?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649410426000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1486",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":8.7,
        "Challenge_reading_time":10.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"How to retrieve all the artifacts lineage for a set of runs, based on both the name of the run, and the name of the artifact?",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":160,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"To filter all artifacts lineage directly by run name and artifact name:\n\nfrom polyaxon.client import RunClient\n\nRunClient.client.runs_v1.get_runs_artifacts_lineage(project=\"PROJECT_NAME\", query=\"run.name: RUN_NAME, kind: ARTIFACT_KIND, name: ARTIFACT_NAME\")",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":18.1,
        "Solution_reading_time":3.42,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":23.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1310893185208,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Thiruvananthapuram, Kerala, India",
        "Answerer_reputation_count":2763.0,
        "Answerer_view_count":851.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to connect mlflow with Minio server, both are running on my local machine, I am able to connect my client code to minio by adding the below lines to the code,<\/p>\n<pre><code>os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] =&quot;xxxx&quot;\nos.environ['AWS_SECRET_ACCESS_KEY'] =&quot;xxxxxx&quot; \nos.environ['MLFLOW_TRACKING_URI'] = 'http:\/\/localhost:5000'\n<\/code><\/pre>\n<p>But the mlflow server is not getting connected to Minio. To run Mlflow server, command I use:<\/p>\n<pre><code>mlflow server -h 0.0.0.0 -p 5000 --default-artifact-root s3:\/\/mlbucket --backend-store-uri sqlite:\/\/\/mlflow.db\n<\/code><\/pre>\n<p>The mlflow server runs, but while accessing the artifacts page the server, it throws the error:<\/p>\n<pre><code>raise NoCredentialsError()\nbotocore.exceptions.NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>\n<p>So how can I pass the credentials of the Minio to the mlflow server command?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":5,
        "Challenge_created_time":1631902626293,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69227917",
        "Challenge_link_count":2,
        "Challenge_participation_count":6,
        "Challenge_readability":10.2,
        "Challenge_reading_time":13.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Connect MLflow server to minio in local",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1136.0,
        "Challenge_word_count":116,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1310893185208,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Thiruvananthapuram, Kerala, India",
        "Poster_reputation_count":2763.0,
        "Poster_view_count":851.0,
        "Solution_body":"<p>Just add the below environment variables:<\/p>\n<pre><code>export AWS_ACCESS_KEY_ID=&lt;your-aws-access-key-id&gt;\nexport AWS_SECRET_ACCESS_KEY = &lt;your-aws-secret-access-key&gt;\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":22.1,
        "Solution_reading_time":2.69,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":12.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"I am considering migrating a data science project from Datarobot to Sagemaker. I am familiar with writing Python and have been going through one of the tutorial Jupyter notebooks to see how to explore the data and to build and deploy and estimator. But, I cannot see how to specify the target feature. I have entirely numerical data in a csv file. One of the fields in that file is the intended target for estimation, the rest are information from which the estimate is to be made. \n\nHow do I specify the column that is to be estimated?\nThe code I expect should have this is ...\n\n\n```\ncontainer = sm.image_uris.retrieve(\"xgboost\", session.boto_region_name, \"1.5-1\")\n\nxgb = sm.estimator.Estimator(\n    container,\n    role,\n    instance_count=1,\n    instance_type=\"ml.m4.xlarge\",\n    output_path=\"s3:\/\/xxxxxx001\/\",\n    sagemaker_session=session,\n)\n\nxgb.set_hyperparameters(\n    max_depth=5,\n    eta=0.2,\n    gamma=4,\n    min_child_weight=6,\n    subsample=0.8,\n    verbosity=0,\n    num_round=100,\n)\ns3_input_train = TrainingInput(\n    s3_data=\"s3:\/\/xxxxxx001\/data.csv\", content_type=\"csv\"\n)\nxgb.fit({\"train\": s3_input_train})\n\n```",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661221322082,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1667926553400,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUoW_FqSbIQKW0MqNJLlA2AA\/how-to-specify-target-feature-in-sagemaker-xgboost",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":14.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"How to specify target feature in Sagemaker XGBoost?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":44.0,
        "Challenge_word_count":139,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"On a badly formatted page on the AWS documentation, I found a statement that - the CSV file must have no headers and the target field must be the first field. So, apparently, it is not possible to specify the target. So primitive, yeah?",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1661298174174,
        "Solution_link_count":0.0,
        "Solution_readability":7.7,
        "Solution_reading_time":2.84,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":43.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"https:\/\/wandb.ai\/alvarobartt\/resnet-pytorch\/runs\/39mhvmwp\/files\/this\/is\/just\/for\/testing",
        "Challenge_closed_time":1658480.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658474901000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/alvarobartt\/wandbfsspec\/issues\/7",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":35.5,
        "Challenge_reading_time":2.12,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":11.0,
        "Challenge_repo_star_count":6.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":1,
        "Challenge_solved_time":null,
        "Challenge_title":"`WandbFileSystem.ls` not working fine with nested directories",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":7,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1499682655627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Heidelberg, Germany",
        "Answerer_reputation_count":193.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":0.7683222222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm very beginner with wandb , so this is very basic question.\nI have dataframe which has my x features and y values.\nI'm tryin to follow <a href=\"https:\/\/docs.wandb.ai\/examples\" rel=\"nofollow noreferrer\">this tutorial<\/a>  to train model from my pandas dataframe . However, when I try to create wandb table from my pandas dataframe, I get an error:<\/p>\n<pre><code>\nwandb.init(project='my-xgb', config={'lr': 0.01})\n\n#the log didn't work  so I haven't run it at the moment (the log 'loss') \n#wandb.log({'loss': loss, ...})\n\n\n# Create a W&amp;B Table with your pandas dataframe\ntable = wandb.Table(df1)\n<\/code><\/pre>\n<blockquote>\n<p>AssertionError: columns argument expects a <code>list<\/code> object<\/p>\n<\/blockquote>\n<p>I have no idea why is this happen, and why it excpect a list. In the tutorial it doesn't look like the dataframe is list.<\/p>\n<p>My end goal - to be able to create wandb table.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655983093280,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72729259",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":6.6,
        "Challenge_reading_time":12.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"wandb.Table raises error: AssertionError: columns argument expects a `list` object",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":63.0,
        "Challenge_word_count":139,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1572256318027,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Israel",
        "Poster_reputation_count":1387.0,
        "Poster_view_count":224.0,
        "Solution_body":"<p><strong>Short answer<\/strong>: <code>table = wandb.Table(dataframe=my_df)<\/code>.<\/p>\n<p>The explanation of your specific case is at the bottom.<\/p>\n<hr \/>\n<p><strong>Minimal example<\/strong> of using <code>wandb.Table<\/code> with a DataFrame:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import wandb\nimport pandas as pd\n\niris_path = 'https:\/\/raw.githubusercontent.com\/mwaskom\/seaborn-data\/master\/iris.csv'\niris = pd.read_csv(iris_path)\ntable = wandb.Table(dataframe=iris)\nwandb.log({'dataframe_in_table': table})\n<\/code><\/pre>\n<p>(Here the dataset is called the Iris dataset that consists of &quot;3 different types of irises\u2019 (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray&quot;)<\/p>\n<p>There are two ways of creating W&amp;B <code>Table<\/code>s according to <a href=\"https:\/\/docs.wandb.ai\/guides\/data-vis\/log-tables#create-tables\" rel=\"nofollow noreferrer\">the official documentation<\/a>:<\/p>\n<ul>\n<li><strong>List of Rows<\/strong>: Log named columns and rows of data. For example: <code>wandb.Table(columns=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], data=[[&quot;1a&quot;, &quot;1b&quot;, &quot;1c&quot;], [&quot;2a&quot;, &quot;2b&quot;, &quot;2c&quot;]])<\/code> generates a table with two rows and three columns.<\/li>\n<li><strong>Pandas DataFrame<\/strong>: Log a DataFrame using <code>wandb.Table(dataframe=my_df)<\/code>. Column names will be extracted from the DataFrame.<\/li>\n<\/ul>\n<hr \/>\n<p><strong>Explanation<\/strong>: Why <code>table = wandb.Table(my_df)<\/code> gives error &quot;columns argument expects a <code>list<\/code> object&quot;? Because <code>wandb.Table<\/code>'s init function looks like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def __init__(\n        self,\n        columns=None,\n        data=None,\n        rows=None,\n        dataframe=None,\n        dtype=None,\n        optional=True,\n        allow_mixed_types=False,\n    ):\n<\/code><\/pre>\n<p>If one passes a DataFrame without telling it's a DataFrame, <code>wandb.Table<\/code> will assume the argument is <code>columns<\/code>.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1655985859240,
        "Solution_link_count":2.0,
        "Solution_readability":11.0,
        "Solution_reading_time":26.77,
        "Solution_score_count":2.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":182.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1604747085276,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working on a CNN project and I would like to log the model.summary to neptune.ai. The intention of that is to have an idea about the model parameters while comparing different models. Any help\/tips would be much appreciated!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604660980577,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1660057709880,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64713492",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.1,
        "Challenge_reading_time":3.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Is there a way to log the keras model summary to neptune?",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":285.0,
        "Challenge_word_count":51,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604146329127,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>You can log <code>model.summary<\/code> (assuming it's keras), like this:<\/p>\n<pre><code>neptune.init('workspace\/project')\nneptune.create_experiment()\n\nmodel = keras.Sequential(...)\nmodel.summary(print_fn=lambda x: neptune.log_text('model_summary', x))\n<\/code><\/pre>\n<p>This will log entire summary as lines of text. You can later browse it in the <em>Logs<\/em> section of the experiment. Look for tile: &quot;model_summary&quot; in this <a href=\"https:\/\/ui.neptune.ai\/o\/USERNAME\/org\/example-project\/e\/HELLO-325\/logs\" rel=\"nofollow noreferrer\">example<\/a>.<\/p>\n<p>Another option - for easier compare - is to log hyper-parameters at experiment creation, like this:<\/p>\n<pre><code># Define parameters as Python dict\nPARAMS = {'batch_size': 64,\n          'n_epochs': 100,\n          'shuffle': True,\n          'activation': 'elu'}\n\n# Pass PARAMS dict to params at experiment creation\nneptune.create_experiment(params=PARAMS)\n<\/code><\/pre>\n<p>You will have them in <em>Parameters<\/em> tab of the experiment, like in this <a href=\"https:\/\/ui.neptune.ai\/o\/USERNAME\/org\/example-project\/e\/HELLO-44\/parameters\" rel=\"nofollow noreferrer\">example<\/a>. You will be able to add each parameter as a column to the dashboard for quick compare. Look for greenish columns in this <a href=\"https:\/\/ui.neptune.ai\/o\/USERNAME\/org\/example-project\/experiments?viewId=d7f80ebe-5bfe-4d12-97c1-2b1e6184a2ed\" rel=\"nofollow noreferrer\">dashboard<\/a>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":14.0,
        "Solution_reading_time":18.48,
        "Solution_score_count":2.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":132.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nCustomer who loads the e-bike data to S3 wants to get AI\/ML insight from sensor data.\nThe e-bike sensor data are size about 4KB files each and posted in S3 buckets.\nThe sensor data is put into format like this\n\ntimestamp1, sensorA, sensorB, sensorC, ..., sensorZ\ntimestamp2, sensorA, sensorB, sensorC, ..., sensorZ\ntimestamp3, sensorA, sensorB, sensorC, ..., sensorZ\n...\n\nThen these sensor data are put into one file about 4KB size.\n\nThe plan I have is to\n\n* Read S3 objects\n* Parse S3 object with Lambda. I thought about Glue but wanted to put data in DynamoDB where Glue does not seem to support. Also, Glue seems to be more expensive.\n* Put the data in DynamoDB with bike ID as primary key and timestamp as sort key.\n* Use SageMaker to learn with the DynamoDB data. There will be separate discussion on choosing which model and making time-series inferencing.\n* If we need to re-learn, it will use the DynamoDB data, not from S3. I think it will be faster to get data from DynamoDB instead from the raw S3 data.\n* Also, I think we can filter out some bad input or apply little modification to DynamoDB data (shifting time stamps to the correct time, etc.)\n* Make inferencing output based on the model.\n\nWhat do you think? Would you agree? Would you approach the problem differently?\nWould you rather learn from S3 directly via Athena or direct S3 access?\nOr would you rather use Glue and Redshift?\nBut the data about 100MB would be sufficient to train the model we have in mind.\nGlue and Redshift maybe overkill.\nCurrently, Korea region does not support Timestream database. So, time series database closest in Korea could be DynamoDB.\n\nPlease share your thoughts.\n\nThanks!",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607357917000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1667926383752,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUebPx1UeWSGOb_3i0TXlBWA\/ai-ml-data-acquisition-and-preprocessing",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.1,
        "Challenge_reading_time":20.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":null,
        "Challenge_title":"[AI\/ML] Data acquisition and preprocessing",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":85.0,
        "Challenge_word_count":289,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"**Thoughts about DynamoDB**\n\nPer GB, DynamoDB is around 5X more cost per GB of data stored. On top of that, you have RCU\/WCU cost.\n\nI would recommend keeping data in S3. Not only is it more cost effective, but with S3, you do not have to worry about RCU\/WCU cost or throughput of DynamoDB. \n\nSageMaker notebooks and training instances can read directly from S3, and S3 has high-throughput. I don't think you will have a problem with 100 MB datasets. \n\nIf you need to prep\/transform your data, you can do the transformations \"in place\" in S3 using Glue, Athena, Glue DataBrew, GlueStudio, etc. \n\n\n**Glue and DynamoDB**\n\n> I thought about Glue but wanted to put data in DynamoDB where Glue does not seem to support.\n\nGlue supports both Python and Spark jobs. If you use a Glue Python job, you can import the boto3 (AWS SDK) library and write to DynamoDB.\n\n**Other strategies**\n\nHow is your customer ingesting the sensor data \/ how is it being written to S3? Are they using AWS IoT Core? \n\nRegardless, the pattern you've described thus far is:\n\nDevice -> Sensor data in S3 -> Transform with Lambda -> store data in DynamoDB\n\nAn alternative approach you could consider is using Kinesis Firehose with Lambda transformations. This will allow you to do \"in-line\" parsing \/ transformation of your data before it is ever written to S3, this removing the need to re-read the data from S3 and apply transformations after the fact. Firehose also allows you to write the stored data in formats such as Parquet, which can help with cost and subsequent query performance. \n\nIf you want to store both raw data and transformed data, you can use a \"fanout\" pattern with Kinesis Streams\/Firehose, where one output is raw data to S3 and the other is a transformed stream.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925563027,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":20.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":299.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1403392071732,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":91.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>This command:<\/p>\n\n<pre><code>BUCKET_TO_READ='my-bucket'\nFILE_TO_READ='myFile'\ndata_location = 's3:\/\/{}\/{}'.format(BUCKET_TO_READ, FILE_TO_READ)\ndf=pd.read_csv(data_location)\n<\/code><\/pre>\n\n<p>is failing with a <\/p>\n\n<pre><code>ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden\n<\/code><\/pre>\n\n<p>Error and I'm unable to figure out why.  That should work according to <a href=\"https:\/\/stackoverflow.com\/a\/50244897\/3763782\">https:\/\/stackoverflow.com\/a\/50244897\/3763782<\/a> <\/p>\n\n<p>Here are my permissions on the bucket:<\/p>\n\n<pre><code>            \"Action\": [\n                \"s3:ListMultipartUploadParts\",\n                \"s3:ListBucket\",\n                \"s3:GetObjectVersionTorrent\",\n                \"s3:GetObjectVersionTagging\",\n                \"s3:GetObjectVersionAcl\",\n                \"s3:GetObjectVersion\",\n                \"s3:GetObjectTorrent\",\n                \"s3:GetObjectTagging\",\n                \"s3:GetObjectAcl\",\n                \"s3:GetObject\"\n<\/code><\/pre>\n\n<p>And these commands work as expected: <\/p>\n\n<pre><code>role = get_execution_role()\nregion = boto3.Session().region_name\nprint(role)\nprint(region)\n\ns3 = boto3.resource('s3')\nbucket = s3.Bucket(BUCKET_TO_READ)\nprint(bucket.creation_date)\n\nfor my_bucket_object in bucket.objects.all():\n    print(my_bucket_object)\n    FILE_TO_READ = my_bucket_object.key\n    break\n\nobj = s3.Object(BUCKET_TO_READ, FILE_TO_READ)\nprint(obj)\n\n<\/code><\/pre>\n\n<p>All of those print statements worked just fine.  <\/p>\n\n<p>I'm not sure if it matters, but each file is within a folder, so my FILE_TO_READ looks like <code>folder\/file<\/code>.<\/p>\n\n<p>This command which should download the file to sagemaker also falied with a 403:<\/p>\n\n<pre><code>import boto3\ns3 = boto3.resource('s3')\ns3.Object(BUCKET_TO_READ, FILE_TO_READ).download_file(FILE_TO_READ)\n<\/code><\/pre>\n\n<p>This is also happening when I open a terminal and use <\/p>\n\n<pre><code>aws s3 cp AWSURI local_file_name\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582298932137,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1582306425572,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60341782",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":15.0,
        "Challenge_reading_time":24.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"Reading a file from s3 to sagemaker on AWS gives 403 forbidden error, but other operations work on the file",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2322.0,
        "Challenge_word_count":179,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403392071732,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":91.0,
        "Poster_view_count":28.0,
        "Solution_body":"<p>The reason was that we granted permission to the bucket not the objects.  That would be granting <code>\"Resource\": \"arn:aws:s3:::bucket-name\/\"<\/code> but not <code>\"Resource\": \"arn:aws:s3:::bucket-name\/*\"<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.0,
        "Solution_reading_time":2.86,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":23.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am new to AWS Sagemaker and I wrote data to my S3 bucket.\nBut these datasets also appear in the working tree of my jupyter instance.<\/p>\n<p>How can I move data directly to S3 without saving it &quot;locally&quot;?<\/p>\n<p>My code:<\/p>\n<pre><code>import os\nimport pandas as pd\n\nimport sagemaker, boto3\nfrom sagemaker import get_execution_role\nfrom sagemaker.inputs import TrainingInput\nfrom sagemaker.serializers import CSVSerializer\n\n# please provide your own bucket and folder path of your bucket here\nbucket = &quot;test-bucket2342343&quot;\nsm_sess = sagemaker.Session(default_bucket=bucket)\nfile_path = &quot;Use Cases\/Sagemaker Demo\/xgboost&quot;\n\n# data \ndf_train = pd.DataFrame({'X':[0,100,200,400,450,  550,600,800,1600],\n                         'y':[0,0,  0,  0,  0,    1,  1,  1,  1]})\n\ndf_test = pd.DataFrame({'X':[10,90,240,459,120,  650,700,1800,1300],\n                        'y':[0,0,  0,  0,  0,    1,  1,  1,  1]})\n\n# move to S3 \ndf_train[['y','X']].to_csv('train.csv', header=False, index=False)\n\ndf_val = df_test.copy()\ndf_val[['y','X']].to_csv('val.csv', header=False, index=False)\n\nboto3.Session().resource(&quot;s3&quot;).Bucket(bucket) \\\n.Object(os.path.join(file_path, &quot;train.csv&quot;)).upload_file(&quot;train.csv&quot;)\n\nboto3.Session().resource(&quot;s3&quot;).Bucket(bucket) \\\n.Object(os.path.join(file_path, &quot;val.csv&quot;)).upload_file(&quot;val.csv&quot;)\n\n<\/code><\/pre>\n<p>It successfully appears in my S3 bucket.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/d1yCy.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/d1yCy.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But it also appears here:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/RjGZr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RjGZr.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661336683943,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1661337201248,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73471486",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":24.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"How to prevent storing data in Jupyter project tree when writing data from Sagemaker to S3",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":17.0,
        "Challenge_word_count":170,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1565941261083,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":188.0,
        "Poster_view_count":43.0,
        "Solution_body":"<p>with Pandas you can save to S3 directly (<a href=\"https:\/\/stackoverflow.com\/a\/56275519\/121956\">relevant answer<\/a>). For example:<\/p>\n<pre><code>import pandas as pd\ndf = pd.DataFrame( [ [1, 1, 1], [2, 2, 2] ], columns=['a', 'b', 'c'])\ndf.to_csv('s3:\/\/test-bucket2342343\/\/tmp.csv', index=False)\n<\/code><\/pre>\n<p>Or, use what you currently do and delete the local files:<\/p>\n<pre><code>import os\nos.remove('train.csv')\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.0,
        "Solution_reading_time":5.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":46.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1408370821672,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Berlin, Germany",
        "Answerer_reputation_count":2521.0,
        "Answerer_view_count":197.0,
        "Challenge_adjusted_solved_time":6.5096966667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I run mlflow as following:<\/p>\n\n<p><code>Dockerfile<\/code> contains the the following CMD command<\/p>\n\n<pre><code>CMD mlflow server \\\n    --host 0.0.0.0 \\\n    --backend-store-uri \"${BACKEND_STORE_URI}\" \\\n    --default-artifact-root \"${DEFAULT_ARTIFACT_ROOT}\"\n<\/code><\/pre>\n\n<p>after <code>docker run --rm --name mlflow -p 5000:5000 -e BACKEND_STORE_URI=mssql+pymssql:\/\/user:pass@mybackendstoreuri\/mlflow mlflow<\/code><\/p>\n\n<p>it shows <\/p>\n\n<pre><code>INFO  [alembic.runtime.migration] Context impl MSSQLImpl.\nINFO  [alembic.runtime.migration] Will assume transactional DDL.\nINFO  [alembic.runtime.migration] Context impl MSSQLImpl.\nINFO  [alembic.runtime.migration] Will assume transactional DDL.\n<\/code><\/pre>\n\n<p>but then, the container exits without starting the server. <\/p>\n\n<p>Without specifying <code>backend store uri<\/code>, I can see the logs related to binding to host, and the container does not exist<\/p>\n\n<p>How to run mlflow tracking server and use backend store uri ?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569314315910,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1569315366808,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58076263",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":13.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"mlflow tracking server does not start after specifying backend-store-uri",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":3168.0,
        "Challenge_word_count":110,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408370821672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berlin, Germany",
        "Poster_reputation_count":2521.0,
        "Poster_view_count":197.0,
        "Solution_body":"<p>The root cause is <\/p>\n\n<pre><code>MLflow UI and client code expects a default experiment with ID 0.\nThis method uses SQL insert statement to create the default experiment as a hack, since\nexperiment table uses 'experiment_id' column is a PK and is also set to auto increment.\nMySQL and other implementation do not allow value '0' for such cases.\n<\/code><\/pre>\n\n<p>ref: <a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/v1.2.0\/mlflow\/store\/sqlalchemy_store.py#L171\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/blob\/v1.2.0\/mlflow\/store\/sqlalchemy_store.py#L171<\/a> <\/p>\n\n<p>No error is raised during migration, so no error shows up, and alembic version is the newest when fail silently. \nref:<a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/v1.2.0\/mlflow\/store\/db_migrations\/env.py#L71\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/blob\/v1.2.0\/mlflow\/store\/db_migrations\/env.py#L71<\/a><\/p>\n\n<p>If using the same idea as the MySQL test(<a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/v1.2.0\/mlflow\/store\/sqlalchemy_store.py#L171\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/blob\/v1.2.0\/mlflow\/store\/sqlalchemy_store.py#L171<\/a>), the exception is raised - <code>Cannot insert explicit value for identity column in table 'experiment' when IDENTITY_INSERT is set to OFF.<\/code><\/p>\n\n<p>Test snippet:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>class TestSqlAlchemyStoreMssqlDb(unittest.TestCase):\n    \"\"\"\n    Run tests against a MSSQL database\n    \"\"\"\n    def setUp(self):\n        db_username = \"test\"\n        db_password = \"test\"\n        host = \"test\"\n        db_name = \"TEST_DB\"\n\n        db_server_url = \"mssql+pymssql:\/\/%s:%s@%s\" % (db_username, db_password, host)\n        self._engine = sqlalchemy.create_engine(db_server_url)\n\n        self._db_url = \"%s\/%s\" % (db_server_url, db_name)\n        print(\"Connect to %s\" % self._db_url)\n\n    def test_store(self):\n        self.store = SqlAlchemyStore(db_uri=self._db_url, default_artifact_root=ARTIFACT_URI)\n<\/code><\/pre>\n\n<p>Using postgres server completes the migration as the log shows.<\/p>\n\n<pre><code>mlflow_1    | 2019\/09\/24 09:03:55 INFO mlflow.store.sqlalchemy_store: Creating initial MLflow database tables...\nmlflow_1    | 2019\/09\/24 09:03:55 INFO mlflow.store.db.utils: Updating database tables at postgresql:\/\/postgres:postgres@postgres:5432\/postgres\nmlflow_1    | INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\nmlflow_1    | INFO  [alembic.runtime.migration] Will assume transactional DDL.\nmlflow_1    | INFO  [alembic.runtime.migration] Running upgrade  -&gt; 451aebb31d03, add metric step\nmlflow_1    | INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -&gt; 90e64c465722, migrate user column to tags\nmlflow_1    | INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -&gt; 181f10493468, allow nulls for metric values\nmlflow_1    | INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -&gt; df50e92ffc5e, Add Experiment Tags Table\nmlflow_1    | INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -&gt; 7ac759974ad8, Update run tags with larger limit\nmlflow_1    | INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\nmlflow_1    | INFO  [alembic.runtime.migration] Will assume transactional DDL.\nmlflow_1    | [2019-09-24 09:03:55 +0000] [15] [INFO] Starting gunicorn 19.9.0\nmlflow_1    | [2019-09-24 09:03:55 +0000] [15] [INFO] Listening at: http:\/\/0.0.0.0:5000 (15)\nmlflow_1    | [2019-09-24 09:03:55 +0000] [15] [INFO] Using worker: sync\nmlflow_1    | [2019-09-24 09:03:55 +0000] [18] [INFO] Booting worker with pid: 18\nmlflow_1    | [2019-09-24 09:03:56 +0000] [22] [INFO] Booting worker with pid: 22\nmlflow_1    | [2019-09-24 09:03:56 +0000] [26] [INFO] Booting worker with pid: 26\nmlflow_1    | [2019-09-24 09:03:56 +0000] [27] [INFO] Booting worker with pid: 27\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1569338801716,
        "Solution_link_count":7.0,
        "Solution_readability":12.1,
        "Solution_reading_time":48.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":34.0,
        "Solution_word_count":356.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>See screenshots below. Each model has been trained for 3 epochs so far. They all started from 0, and the info panel shows that the most recent epoch was 2 and \u201cbest\u201d is 0. Looking at the plots though, it looks like all the plots cover epochs 2 through 4.<\/p>\n<p>I\u2019m calling keras <code>model.fit<\/code> with <code>from_epoch=0<\/code>. I\u2019m running inside a <code>ray<\/code> worker so maybe that\u2019s causing some problems?<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/6\/674f340076209ecba401fa6747a6418e31e1f2fe.png\" data-download-href=\"\/uploads\/short-url\/eJUS1BdlLEVdGhqEUtHrBXnO9Q2.png?dl=1\" title=\"Screenshot 2022-12-24 at 8.45.46 am\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/6\/674f340076209ecba401fa6747a6418e31e1f2fe_2_690x288.png\" alt=\"Screenshot 2022-12-24 at 8.45.46 am\" data-base62-sha1=\"eJUS1BdlLEVdGhqEUtHrBXnO9Q2\" width=\"690\" height=\"288\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/6\/674f340076209ecba401fa6747a6418e31e1f2fe_2_690x288.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/6\/674f340076209ecba401fa6747a6418e31e1f2fe.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/6\/674f340076209ecba401fa6747a6418e31e1f2fe.png 2x\" data-dominant-color=\"202121\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot 2022-12-24 at 8.45.46 am<\/span><span class=\"informations\">788\u00d7329 16.3 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/b\/b82b054b707b56a829f444969855ec256c85aee5.png\" alt=\"Screenshot 2022-12-24 at 8.44.29 am\" data-base62-sha1=\"qhe1BKuwPFD8WbPpt39n8Mz0TKR\" width=\"456\" height=\"269\"><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1671832217765,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/all-my-plots-start-at-epoch-2\/3593",
        "Challenge_link_count":6,
        "Challenge_participation_count":4,
        "Challenge_readability":14.5,
        "Challenge_reading_time":28.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"All my plots start at epoch 2",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":336.0,
        "Challenge_word_count":135,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Tom,<\/p>\n<p>Thanks for writing in! So the step is calculated every time there is a call to <code>wandb.log(<\/code>, you can see more details in our docs. If you click on the Edit panel button (a pencil) in the top right corner of the chart, you can select the epoch as X axis. I just checked in your project and it starts in 0. Please let me know if I can help you in any other way!<\/p>\n<p>Best,<br>\nLuis<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.0,
        "Solution_reading_time":4.94,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":79.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1221810788500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paderborn, North-Rhine-Westphalia, Germany",
        "Answerer_reputation_count":68522.0,
        "Answerer_view_count":7896.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running an MLflow experiment as a part of it I would like to log a few artifacts as a python pickle.<\/p>\n<p>Ex: Trying out different categorical encoders, so wanted to log the encoder objects as a pickle file.<\/p>\n<p>Is there a way to achieve this?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622538922663,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67786052",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.3,
        "Challenge_reading_time":3.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Log Pickle files as a part of Mlflow run",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1843.0,
        "Challenge_word_count":55,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1411361217027,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":569.0,
        "Poster_view_count":123.0,
        "Solution_body":"<p>There are two functions for there:<\/p>\n<ol>\n<li><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_artifact\" rel=\"nofollow noreferrer\">log_artifact<\/a> - to log a local file or directory as an artifact<\/li>\n<li><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_artifacts\" rel=\"nofollow noreferrer\">log_artifacts<\/a> - to log a contents of a local directory<\/li>\n<\/ol>\n<p>so it would be as simple as:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with mlflow.start_run():\n    mlflow.log_artifact(&quot;encoder.pickle&quot;)\n<\/code><\/pre>\n<p>And you will need to use the <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">custom MLflow model<\/a> to use that pickled file, something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow.pyfunc\n\nclass my_model(mlflow.pyfunc.PythonModel):\n    def __init__(self, encoders):\n        self.encoders = encoders\n\n    def predict(self, context, model_input):\n        _X = ...# do encoding using self.encoders.\n        return str(self.ctx.predict([_X])[0])\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":14.3,
        "Solution_reading_time":14.72,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":91.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to save Kedro memory dataset in azure as a file and still want to have it in memory as my pipeline will be using this later in the pipeline. Is this possible in Kedro. I tried to look at Transcoding datasets but looks like not possible. Is there any other way to acheive this?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1642516914763,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70757448",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":5.1,
        "Challenge_reading_time":4.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How to save kedro dataset in azure and still have it in memory",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":231.0,
        "Challenge_word_count":68,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1495105930728,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>This may be a good opportunity to use <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.CachedDataSet.html\" rel=\"nofollow noreferrer\">CachedDataSet<\/a> this allows you to wrap any other dataset, but once it's read into memory - make it available to downstream nodes without re-performing the IO operations.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.0,
        "Solution_reading_time":4.16,
        "Solution_score_count":4.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":37.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1347532849952,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tel Aviv, Israel",
        "Answerer_reputation_count":2529.0,
        "Answerer_view_count":172.0,
        "Challenge_adjusted_solved_time":1.6824691667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to download a file to sagemaker from my S3 bucket.<\/p>\n\n<p>the path of the file is\n<code>s3:\/\/vemyone\/input\/dicom-images-train\/1.2.276.0.7230010.3.1.2.8323329.1000.1517875165.878026\/1.2.276.0.7230010.3.1.3.8323329.1000.1517875165.878025\/1.2.276.0.7230010.3.1.4.8323329.1000.1517875165.878027.dcm<\/code><\/p>\n\n<p>The path of that file is stored as a list element at <code>train_fns[0]<\/code>.<\/p>\n\n<p>the value of <code>train_fns[0]<\/code> is <\/p>\n\n<p><code>input\/dicom-images-train\/1.2.276.0.7230010.3.1.2.8323329.1000.1517875165.878026\/1.2.276.0.7230010.3.1.3.8323329.1000.1517875165.878025\/1.2.276.0.7230010.3.1.4.8323329.1000.1517875165.878027.dcm<\/code><\/p>\n\n<p>I used the following code:<\/p>\n\n<pre><code>s3 = boto3.resource('s3')\nbucketname = 'vemyone'\n\ns3.Bucket(bucketname).download_file(train_fns[0][:], train_fns[0])\n<\/code><\/pre>\n\n<p>but I get the following error:<\/p>\n\n<p><code>FileNotFoundError: [Errno 2] No such file or directory: 'input\/dicom-images-train\/1.2.276.0.7230010.3.1.2.8323329.1000.1517875165.878026\/1.2.276.0.7230010.3.1.3.8323329.1000.1517875165.878025\/1.2.276.0.7230010.3.1.4.8323329.1000.1517875165.878027.dcm.5b003ba1'<\/code><\/p>\n\n<p>I notice that some characters have appended itself at the end of the path.<\/p>\n\n<p>how do I solve this problem?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562758462203,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56969859",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":18.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS: FileNotFoundError: [Errno 2] No such file or directory",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":14766.0,
        "Challenge_word_count":95,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521737834360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pondicherry, Puducherry, India",
        "Poster_reputation_count":1303.0,
        "Poster_view_count":139.0,
        "Solution_body":"<p>please see <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Bucket.download_file\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Bucket.download_file<\/a><\/p>\n\n<p>by the doc, first argument is file key, second argument is path for local file:<\/p>\n\n<pre><code>s3 = boto3.resource('s3')\nbucketname = 'vemyone'\n\ns3.Bucket(bucketname).download_file(train_fns[0], '\/path\/to\/local\/file')\n<\/code><\/pre>",
        "Solution_comment_count":8.0,
        "Solution_last_edit_time":1562764519092,
        "Solution_link_count":2.0,
        "Solution_readability":30.3,
        "Solution_reading_time":6.99,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1618467374027,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created a Tabular Dataset using Azure ML python API. Data under question is a bunch of parquet files (~10K parquet files each of size of 330 KB) residing in Azure Data Lake Gen 2 spread across multiple partitions. When I trigger &quot;Generate Profile&quot; operation for the dataset, it throws following error while handling empty parquet file and then the profile generation stops.<\/p>\n<pre><code>User program failed with ExecutionError: \nError Code: ScriptExecution.StreamAccess.Validation\nValidation Error Code: NotSupported\nValidation Target: ParquetFile\nFailed Step: 77866d0a-8243-4d3d-8bc6-599d466488dd\nError Message: ScriptExecutionException was caused by StreamAccessException.\n  Failed to read Parquet file at: &lt;my_blob_path&gt;\/20211217.parquet\n    Current parquet file is not supported.\n      Exception of type 'Thrift.Protocol.TProtocolException' was thrown.\n| session_id=6be4db0b-bdc1-4dd6-b8a6-6e9466f7bc54\n\n<\/code><\/pre>\n<p>By empty parquet file, I mean that the if I read the individual parquet file using pandas (<code>pd.read_parquet<\/code>), it results in an empty DF (df.empty == True).<\/p>\n<p>Any suggestion to avoid this error will be appreciated.<\/p>\n<p><strong>Update<\/strong>\nThe issue has been fixed in the following version:<\/p>\n<ul>\n<li>azureml-dataprep : 3.0.1<\/li>\n<li>azureml-core :  1.40.0<\/li>\n<\/ul>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644490387177,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1648643496672,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71063820",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.2,
        "Challenge_reading_time":17.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"AzureML: Dataset Profile fails when parquet file is empty",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":255.0,
        "Challenge_word_count":169,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1280505139752,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, India",
        "Poster_reputation_count":4265.0,
        "Poster_view_count":403.0,
        "Solution_body":"<p>Thanks for reporting it.\nThis is a bug in handling of the parquet files with columns but empty row set. This has been fixed already and will be included in next release.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.9,
        "Solution_reading_time":2.13,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":32.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"When creating a model with Redshift ML which instances are actually being used and can I specify the instance type?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1683292120516,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1683868274950,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUfC7d9EmJTKWFUqWjSCLTyw\/redshift-ml-instance-types",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":1.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Redshift ML Instance Types",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":57.0,
        "Challenge_word_count":23,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You cannot specify the instance type with Redshift ML. It leverages Amazon SageMaker behind the scenes which is a fully managed machine learning service. Amazon SageMaker will choose the appropriate instance type based on the model being created.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1683474638196,
        "Solution_link_count":0.0,
        "Solution_readability":9.4,
        "Solution_reading_time":3.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":38.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1452222077950,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"China",
        "Answerer_reputation_count":2187.0,
        "Answerer_view_count":894.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I use next command to save output results:<\/p>\n\n<pre><code>ws.datasets.add_from_dataframe(data, 'GenericCSV', 'output.csv', 'Uotput results')\n<\/code><\/pre>\n\n<p>where <code>ws<\/code> is <code>azureml.Workspace<\/code> object and <code>data<\/code> is <code>pandas.DataFrame<\/code>.<\/p>\n\n<p>It works fine if my dataset size less than 4 mb. Otherwise I got a error:<\/p>\n\n<pre><code>AzureMLHttpError: Maximum request length exceeded.\n<\/code><\/pre>\n\n<p>As I understood this is the error raised by Azure environment limits and the maximum size of the dataset could not be changed. <\/p>\n\n<p>I could split my dataset to 4 mb parts and download them from Azure ML studio, but it is very inconvinient if size of my output dataset is more than 400 mb.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1457889087747,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1457963855452,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35973168",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":6.9,
        "Challenge_reading_time":10.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"How could I save dataset from ipython notebook in Azure ML Studio?",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":3128.0,
        "Challenge_word_count":112,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1452426675696,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":77.0,
        "Poster_view_count":21.0,
        "Solution_body":"<p>I have read the source code in the python package <strong>azureml<\/strong>, and found out that they are using a simple request post when uploading a dataset, which has a limited content length 4194304 bytes.<\/p>\n\n<p>I tried to modify the code inside \"http.py\" within the python package <strong>azureml<\/strong>. I posted the request with a chunked data, and I got the following error:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \".\\azuremltest.py\", line 10, in &lt;module&gt;\n    ws.datasets.add_from_dataframe(frame, 'GenericCSV', 'output2.csv', 'Uotput results')\n  File \"C:\\Python34\\lib\\site-packages\\azureml\\__init__.py\", line 507, in add_from_dataframe\n    return self._upload(raw_data, data_type_id, name, description)\n  File \"C:\\Python34\\lib\\site-packages\\azureml\\__init__.py\", line 550, in _upload\nraw_data, None)\n  File \"C:\\Python34\\lib\\site-packages\\azureml\\http.py\", line 135, in upload_dataset\n    upload_result = self._send_post_req(api_path, raw_data)\n  File \"C:\\Python34\\lib\\site-packages\\azureml\\http.py\", line 197, in _send_post_req\n    raise AzureMLHttpError(response.text, response.status_code)\nazureml.errors.AzureMLHttpError: Chunked transfer encoding is not permitted. Upload size must be indicated in the Content-Length header.\nRequest ID: 7b692d82-845c-4106-b8ec-896a91ecdf2d 2016-03-14 04:32:55Z\n<\/code><\/pre>\n\n<p>The REST API in <strong>azureml<\/strong> package does not support chunked transfer encoding. Hence, I took a look at how the Azure ML studio implements this, and I found out this:<\/p>\n\n<ol>\n<li><p>It post a request with content-length=0 to <code>https:\/\/studioapi.azureml.net\/api\/resourceuploads\/workspaces\/&lt;workspace_id&gt;\/?userStorage=true&amp;dataTypeId=GenericCSV<\/code>, which will return an id in the response body.<\/p><\/li>\n<li><p>Break the .csv file into chunks less than 4194304 bytes, and post them to <code>https:\/\/studioapi.azureml.net\/api\/blobuploads\/workspaces\/&lt;workspace_id&gt;\/?numberOfBlocks=&lt;the number of chunks&gt;&amp;blockId=&lt;index of chunk&gt;&amp;uploadId=&lt;the id you get from previous request&gt;&amp;dataTypeId=GenericCSV<\/code><\/p><\/li>\n<\/ol>\n\n<p>If you really want this functionality, you can implement it with python and the above REST API.<\/p>\n\n<p>If you think it's too complicated, report the issue to <a href=\"https:\/\/github.com\/Azure\/Azure-MachineLearning-ClientLibrary-Python\/issues\" rel=\"nofollow\">this<\/a>. The <strong>azureml<\/strong> python package is still under development, so your suggestion would be very helpful for them.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1457939067983,
        "Solution_link_count":3.0,
        "Solution_readability":11.2,
        "Solution_reading_time":33.05,
        "Solution_score_count":4.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":258.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>AzureML's Python Script module requires to return a Pandas DataFrame. I want to return only a value and I do this:<\/p>\n\n<pre><code>result=7\ndataframe1=pd.DataFrame(numpy.zeros(1))\ndataframe1[0][0]=result\n<\/code><\/pre>\n\n<p>by which I am able to return just a single value in Azure ML's Python Script module. <\/p>\n\n<p><strong>What is a proper way to create a pandas DataFrame with a single value?<\/strong><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":4,
        "Challenge_created_time":1496828726750,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44409066",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":6.0,
        "Challenge_reading_time":5.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Python Pandas DataFrame with only a single number stored?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1701.0,
        "Challenge_word_count":65,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1251372839052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":48616.0,
        "Poster_view_count":3348.0,
        "Solution_body":"<p>Following code should work:<\/p>\n\n<pre><code>import pandas as pd\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    result = pd.DataFrame({'mycol': [123]})\n    return result,\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":2.48,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to transfer a generated csv file <code>test_df.csv<\/code> from my Azure ML notebook folder which has a path <code>\/Users\/Ankit19.Gupta\/test_df.csv<\/code> to a datastore which has a web path <code>https:\/\/abc.blob.core.windows.net\/azureml\/LocalUpload\/f3db18b6<\/code>. I have written the python code as<\/p>\n<pre><code>from azureml.core import Workspace\nws = Workspace.from_config()\ndatastore = ws.get_default_datastore()\n    \ndatastore.upload_files('\/Users\/Ankit19.Gupta\/test_df.csv',\n                  target_path='https:\/\/abc.blob.core.windows.net\/azureml\/LocalUpload\/f3db18b6',\n                  overwrite=True)\n<\/code><\/pre>\n<p>But it is showing the following error message:<\/p>\n<pre><code>UserErrorException: UserErrorException:\n    Message: '\/' does not point to a file. Please upload the file to cloud first if running in a cloud notebook.\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;message&quot;: &quot;'\/' does not point to a file. Please upload the file to cloud first if running in a cloud notebook.&quot;\n    }\n}\n<\/code><\/pre>\n<p>I have tried <a href=\"https:\/\/stackoverflow.com\/questions\/67897947\/how-to-transfer-data-from-azure-ml-notebooks-to-a-storage-container\">this<\/a> but it is not working for me. Can anyone please help me to resolve this issue. Any help would be appreciated.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663473633117,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73760033",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":10.5,
        "Challenge_reading_time":18.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"How to transfer a csv file from notebook folder to a datastore",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":43.0,
        "Challenge_word_count":143,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540154634483,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":237.0,
        "Poster_view_count":173.0,
        "Solution_body":"<p>The way the path was mentioned is not accurate. The datastore path will be different manner.\nReplace the below code for the small change in the calling path.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4o1WU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4o1WU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<pre><code>from azureml.core import Workspace\nws = Workspace.from_config()\ndatastore = ws.get_default_datastore()\n    \ndatastore.upload_files('.\/Users\/foldername\/filename.csv',\n                  target_path=\u2019your targetfolder',\n                  overwrite=True)\n<\/code><\/pre>\n<p>We need to call all the parent folders before the folder.  <strong><code>\u201c.\/\u201d<\/code><\/strong> is the way we can call the dataset from datastore.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.9,
        "Solution_reading_time":9.46,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":73.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am managing a data pipeline using Kedro and at the last step I have a huge csv file stored in a S3 bucket and I need to load it back to SQL Server.<\/p>\n<p>I'd normally go about that with a <a href=\"https:\/\/towardsdatascience.com\/use-python-and-bulk-insert-to-quickly-load-data-from-csv-files-into-sql-server-tables-ba381670d376\" rel=\"nofollow noreferrer\">bulk insert<\/a>, but not quite sure how to fit that into the <strong>kedro<\/strong> templates. This are the destination table and the S3 Bucket as configured in the <code>catalog.yml<\/code><\/p>\n<pre><code>flp_test:\n  type: pandas.SQLTableDataSet\n  credentials: dw_dev_credentials\n  table_name: flp_tst\n  load_args:\n    schema: 'dwschema'\n  save_args:\n    schema: 'dwschema'\n    if_exists: 'replace'\n\nbulk_insert_input:\n   type: pandas.CSVDataSet\n   filepath: s3:\/\/your_bucket\/data\/02_intermediate\/company\/motorbikes.csv\n   credentials: dev_s3\n\n\ndef insert_data(self, conn, csv_file_nm, db_table_nm):\n    qry = &quot;BULK INSERT &quot; + db_table_nm + &quot; FROM '&quot; + csv_file_nm + &quot;' WITH (FORMAT = 'CSV')&quot;\n    # Execute the query\n    cursor = conn.cursor()\n    success = cursor.execute(qry)\n    conn.commit()\n    cursor.close\n<\/code><\/pre>\n<ul>\n<li>How do I point <code>csv_file_nm<\/code> to my <code>bulk_insert_input<\/code> S3 catalog?<\/li>\n<li>Is there a proper way to indirectly access <code>dw_dev_credentials<\/code> to do the insert?<\/li>\n<\/ul>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1626182174053,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1626187242808,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68363180",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":18.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"How to use SQL Server Bulk Insert in Kedro Node?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":241.0,
        "Challenge_word_count":156,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1271930452580,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":5469.0,
        "Poster_view_count":232.0,
        "Solution_body":"<p>Kedro's <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.extras.datasets.pandas.SQLTableDataSet.html\" rel=\"nofollow noreferrer\">pandas.SQLTableDataSet.html<\/a> uses the <a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/version\/0.23.4\/generated\/pandas.DataFrame.to_sql.html\" rel=\"nofollow noreferrer\">pandas.to_sql<\/a> method as is. To use this as is you would need one <code>pandas.CSVDataSet<\/code> into a <code>node<\/code> which then writes to a target <code>pandas.SQLDataTable<\/code> dataset in order to write it to SQL. If you have Spark available this will be faster than Pandas.<\/p>\n<p>In order to use the built in <code>BULK INSERT<\/code> query I think you will need to define a <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/03_custom_datasets.html\" rel=\"nofollow noreferrer\">custom dataset<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":11.9,
        "Solution_reading_time":11.09,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":76.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a use case wherein I need to refer to the input dataset in the ACI\/AKS which is in a blob (same used for training model). I'm not able to find related resources in the Microsoft official documentation. If anyone suggests to me how to do it, that will be very helpful.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1626172285177,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68360738",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.7,
        "Challenge_reading_time":3.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"AKS an ACI Deployment with blob mount",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":57.0,
        "Challenge_word_count":59,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1448994884167,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":265.0,
        "Poster_view_count":156.0,
        "Solution_body":"<p>It will be supported in the near future, Running Python scripts on Azure with Azure Container Instances to connect the blob.\n<a href=\"https:\/\/kohera.be\/tutorials-2\/running-python-scripts-on-azure-with-azure-container-instances\/\" rel=\"nofollow noreferrer\">https:\/\/kohera.be\/tutorials-2\/running-python-scripts-on-azure-with-azure-container-instances\/<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":25.9,
        "Solution_reading_time":4.92,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1334762714136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation_count":6557.0,
        "Answerer_view_count":2005.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When I upload dataset with more then 100 columns I can see only part of them in the visualisation block. Can I see stats for all columns from dataset? Thanks<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1459460172740,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1459517035743,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36344278",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":3.5,
        "Challenge_reading_time":2.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure machine learning. How can I see all columns",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":736.0,
        "Challenge_word_count":38,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459459387887,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>If you are an owner in the workspace, you can open your dataset in Python inside of a Jupyter Notebook. By the visualize should be an open in notebook button. Then just execute the code that is provided for you, and it should print your dataset. You can then also select specific columns to visualize as well.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":3.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":57.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1506221099907,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Kolkata, West Bengal, India",
        "Answerer_reputation_count":374.0,
        "Answerer_view_count":53.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Weights &amp; Biases (<a href=\"https:\/\/wandb.ai\/\" rel=\"nofollow noreferrer\">link<\/a>) to manage hyperparameter optimization and log the results. I am training using Keras with a Tensorflow backend, and I am using the out-of-the-box logging functionality of Weights &amp; Biases, in which I run<\/p>\n<pre><code>wandb.init(project='project_name', entity='username', config=config)\n<\/code><\/pre>\n<p>and then add a <code>WandbCallback()<\/code> to the callbacks of <code>classifier.fit()<\/code>. By default, Weights &amp; Biases appears to save the model parameters (i.e., the model's weights and biases) and store them in the cloud. This eats up my account's storage quota, and it is unnecessary --- I only care about tracking the model loss\/accuracy as a function of the hyperparameters.<\/p>\n<p>Is it possible for me to train a model and log the loss and accuracy using Weights &amp; Biases, but not store the model parameters in the cloud? How can I do this?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650896835733,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72001154",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":13.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"How to prevent Weights & Biases from saving best model parameters",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":204.0,
        "Challenge_word_count":147,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1514243890900,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":167.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>In order to not save the trained model weights during hyperparam optimization you do something like this:<\/p>\n<pre><code>classifier.fit(..., callbacks=[WandbCallback(.., save_model=False)]\n<\/code><\/pre>\n<p>This will only track the metrics (train\/validation loss\/acc, etc.).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.9,
        "Solution_reading_time":3.69,
        "Solution_score_count":4.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":30.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1346946252340,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":136.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a CSV file I'm trying to RCF on.  If I put a date or string in the CSV then I get an error like the one below.  If I limit it to just the integer and float fields the script runs fine.  Is there some way to process dates and string?  I see the taxi example from AWS and it has dates which appear the same as mine<\/p>\n<pre><code>eventData = pd.read_csv(data_location, delimiter=&quot;,&quot;, header=None, parse_dates=True)\n\nprint('Starting RCF Training')\n# specify general training job information\nrcf = RandomCutForest(role=sagemaker.get_execution_role(),\n                      instance_count=1,\n                      instance_type='ml.m4.xlarge',\n                      data_location=data_location,\n                      output_path='s3:\/\/{}\/{}\/output'.format(bucket, prefix),\n                      base_job_name=&quot;ad-rcf&quot;,\n                      num_samples_per_tree=512,\n                      num_trees=50)\n\nrcf.fit(rcf.record_set(eventData.values))\n<\/code><\/pre>\n<p>CSV Data that fails<\/p>\n<pre><code>392507,1613744,1\/2\/2020 19:11,1577238693,2469,3.30E+01,-9.67E+01\n691381,1888551,12\/10\/2019 9:22,1575641745,3460,2.37E+01,9.04E+01\n392507,1613744,1\/2\/2020 19:20,1577236815,1797,3.30E+01,-9.67E+01\n392507,1613744,1\/29\/2020 19:04,1577264188,1797,3.30E+01,-9.67E+01\n<\/code><\/pre>\n<p>Error output<\/p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-35-ba19bf5d66a2&gt; in &lt;module&gt;\n---&gt; 21 rcf.fit(rcf.record_set(eventData.values))\n     22 \n     23 print('Done RCF Training')\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/amazon\/amazon_estimator.py in record_set(self, train, labels, channel, encrypt)\n    281         logger.debug(&quot;Uploading to bucket %s and key_prefix %s&quot;, bucket, key_prefix)\n    282         manifest_s3_file = upload_numpy_to_s3_shards(\n--&gt; 283             self.instance_count, s3, bucket, key_prefix, train, labels, encrypt\n    284         )\n    285         logger.debug(&quot;Created manifest file %s&quot;, manifest_s3_file)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/amazon\/amazon_estimator.py in upload_numpy_to_s3_shards(num_shards, s3, bucket, key_prefix, array, labels, encrypt)\n    443                 s3.Object(bucket, key_prefix + file).delete()\n    444         finally:\n--&gt; 445             raise ex\n    446 \n    447 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/amazon\/amazon_estimator.py in upload_numpy_to_s3_shards(num_shards, s3, bucket, key_prefix, array, labels, encrypt)\n    424                     write_numpy_to_dense_tensor(file, shard, label_shards[shard_index])\n    425                 else:\n--&gt; 426                     write_numpy_to_dense_tensor(file, shard)\n    427                 file.seek(0)\n    428                 shard_index_string = str(shard_index).zfill(len(str(len(shards))))\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/amazon\/common.py in write_numpy_to_dense_tensor(file, array, labels)\n    154             )\n    155         resolved_label_type = _resolve_type(labels.dtype)\n--&gt; 156     resolved_type = _resolve_type(array.dtype)\n    157 \n    158     # Write each vector in array into a Record in the file object\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/amazon\/common.py in _resolve_type(dtype)\n    288     if dtype == np.dtype(&quot;float32&quot;):\n    289         return &quot;Float32&quot;\n--&gt; 290     raise ValueError(&quot;Unsupported dtype {} on array&quot;.format(dtype))\n    291 \n    292 \n\nValueError: Unsupported dtype object on array\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1614969857063,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66497968",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.6,
        "Challenge_reading_time":43.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS Sagemaker ValueError: Unsupported dtype object on array when using strings and dates",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1395.0,
        "Challenge_word_count":274,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1346946252340,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":136.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>Figured out my issue, the RCF can't handle dates and strings.  There's this page for the Kenesis offering from AWS that covers the same Random Cut Forest algorithm <a href=\"https:\/\/docs.aws.amazon.com\/kinesisanalytics\/latest\/sqlref\/sqlrf-random-cut-forest.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/kinesisanalytics\/latest\/sqlref\/sqlrf-random-cut-forest.html<\/a>  It says the function only supports &quot;The algorithm accepts the DOUBLE, INTEGER, FLOAT, TINYINT, SMALLINT, REAL, and BIGINT data types.&quot;<\/p>\n<p>The gotcha part that AWS does with the NYC Taxi example is they use .value which is referring to only the value column of the data.  They are basically dropping the dates from the RCF as a feature.  It doesn't help that .values on the array does work and looks very similar to .value<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.4,
        "Solution_reading_time":10.49,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":106.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554424491243,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":51.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>From <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform.html#google.cloud.aiplatform.AutoMLTabularTrainingJob.run\" rel=\"nofollow noreferrer\">the docs<\/a> it says that<\/p>\n<blockquote>\n<p>The value of the key values of the key (the values in the column) must be in RFC 3339 date-time format, where time-offset = \u201cZ\u201d (e.g. 1985-04-12T23:20:50.52Z)<\/p>\n<\/blockquote>\n<p>The dataset that I'm pointing to is a CSV in cloud storage, where the data is in the format suggested by the docs:<\/p>\n<pre><code>$ gsutil cat gs:\/\/my-data.csv | head | xsv select TS_SPLIT_COL\nTS_SPLIT_COL\n2021-01-18T00:00:00.00Z\n2021-01-18T00:00:00.00Z\n2021-01-04T00:00:00.00Z\n2021-03-06T00:00:00.00Z\n2021-01-15T00:00:00.00Z\n2021-02-11T00:00:00.00Z\n2021-02-05T00:00:00.00Z\n2021-05-20T00:00:00.00Z\n2021-01-05T00:00:00.00Z\n<\/code><\/pre>\n<p>But I receive a <code>Training pipeline failed with error message: The timestamp column must have valid timestamp entries.<\/code> error when I try to run a training job<\/p>\n<p>EDIT: this should hopefully make it more reproducible<\/p>\n<p>data: <a href=\"https:\/\/pastebin.com\/qEDqvzX6\" rel=\"nofollow noreferrer\">https:\/\/pastebin.com\/qEDqvzX6<\/a><\/p>\n<p>Code I'm running:<\/p>\n<pre><code>from google.cloud import aiplatform\n\nPROJECT = &quot;my-project&quot;\nDATASET_ID = &quot;dataset-id&quot;  # points to CSV \n\naiplatform.init(project=PROJECT)\n\ndataset = aiplatform.TabularDataset(DATASET_ID)\n\njob = aiplatform.AutoMLTabularTrainingJob(\n    display_name=&quot;so-58454722&quot;,\n    optimization_prediction_type=&quot;classification&quot;,\n    optimization_objective=&quot;maximize-au-roc&quot;,\n)\n\nmodel = job.run(\n    dataset=dataset,\n    model_display_name=&quot;so-58454722&quot;,\n    target_column=&quot;Y&quot;,\n    training_fraction_split=0.8,\n    validation_fraction_split=0.1,\n    test_fraction_split=0.1,\n    timestamp_split_column_name=&quot;TS_SPLIT_COL&quot;,\n)\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1647473614997,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1647534964220,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71505415",
        "Challenge_link_count":3,
        "Challenge_participation_count":4,
        "Challenge_readability":13.2,
        "Challenge_reading_time":26.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"\"The timestamp column must have valid timestamp entries.\" error when using `timestamp_split_column_name` arg in `AutoMLTabularTrainingJob.run`",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":157.0,
        "Challenge_word_count":165,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1519933306780,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3576.0,
        "Poster_view_count":203.0,
        "Solution_body":"<p>Try this timestamp format instead:<\/p>\n<p><code>2022-03-18T01:23:45.123456+00:00<\/code><\/p>\n<p>It uses <code>+00:00<\/code> instead of <code>Z<\/code> to specify timezone.<\/p>\n<p>This change will eliminate the &quot;The timestamp column must have valid timestamp entries.&quot; error<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":3.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":29.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1436818579270,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Eugene, OR, USA",
        "Answerer_reputation_count":474.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using a \"Split Data\" module set to recommender split to split data for training and testing a matchbox recommender. The input data is a valid user-item-rating tuple (for example, 575978 - 157381 - 3) and I've left the parameters for the recommender split as default (0s for everything), besides changing it to a .75 and .25 split. However, when this module finishes, it returns the complete, unsplit dataset for dataset1 and a completely empty (but labelled) dataset for dataset2. This also happens when doing a stratified split using the \"Split Rows\" mode. Any idea what's going on?<\/p>\n\n<p>Thanks.<\/p>\n\n<p>Edit: Including a sample of my data.<\/p>\n\n<pre><code>UserID  ItemID  Rating\n835793  165937  3\n154738  11214   3\n938459  748288  3\n819375  789768  6\n738571  98987   3\n847509  153777  3\n991757  124458  3\n968685  288070  2\n236349  8337    3\n127299  545885  3\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":4,
        "Challenge_created_time":1529519087767,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1529527790943,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50954802",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":7.9,
        "Challenge_reading_time":11.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Recommender Split Returning Empty Dataset",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":88.0,
        "Challenge_word_count":142,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436818579270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Eugene, OR, USA",
        "Poster_reputation_count":474.0,
        "Poster_view_count":28.0,
        "Solution_body":"<p>Figured it out. In my \"Remove Duplicate Rows\" module up the chain a bit I was only removing duplicates by UserID instead of UserID <em>and<\/em> ItemID. This still left quite a bit of rows but I'm assuming it messed with the stratification. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":3.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Does Azure ML Python SDK support cross validation? In the graphical Designer, there is a cross validation module but I haven't found anything similar in the SDK documentation.  <\/p>\n<p>Of course, there probably exist many frameworks for cross-validation in Python but it would be nice to have a native Azure ML module for cross-validation, similar to the Hyperparameter tuning module. I would like to be able to give a single Dataset object and the Azure ML framework would take care of slicing the source Dataset into separate fold datasets. Azure ML should also take care of assigning different folds to the compute nodes and running the folds in parallel.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1591343397597,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/32461\/cross-validation-with-azure-ml-python-sdk",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":9.6,
        "Challenge_reading_time":8.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Cross validation with Azure ML Python SDK",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":115,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a>@LauriLehman-8626<\/a> It's not supported to use Designer built-in module in python SDK today but we are working on the module SDK private preview which can enable this. We would like understand more about your use-case, can you please send an email to Azcommunity@microsoft.com so that we can invite you to the private preview if you are interested.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.5,
        "Solution_reading_time":4.45,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":57.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create (what I thought was) a simple image classification pipeline between s3 and SageMaker.<\/p>\n\n<p>Images are stored in an s3 bucket with their class labels in their file names currently, e.g.<\/p>\n\n<p><strong>My-s3-bucket-dir<\/strong><\/p>\n\n<pre><code>cat-1.jpg\ndog-1.jpg\ncat-2.jpg\n..\n<\/code><\/pre>\n\n<p>I've been trying to leverage several related example .py scripts, but most seem to be download data sets already in .rec format or containing special manifest or annotation files I don't have.<\/p>\n\n<p>All I want is to pass the images from s3 to the SageMaker image classification algorithm that's located in the same region, IAM account, etc. I suppose this means I need a <code>.lst<\/code> file<\/p>\n\n<p>When I try to manually create the <code>.lst<\/code> it doesn't seem to like it and it also takes too long doing manual work to be a good practice.<\/p>\n\n<p>How can I automatically generate the <code>.lst<\/code> file (or otherwise send the images\/classes for training)? <\/p>\n\n<p>Things I read made it sound like <code>im2rec.py<\/code> was a solution, but I don't see how. The example I'm working with now is <\/p>\n\n<p><code>Image-classification-fulltraining-highlevel.ipynb<\/code><\/p>\n\n<p>but it seems to download the data as <code>.rec<\/code>, <\/p>\n\n<pre><code>download('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-train.rec')\ndownload('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-val.rec')\n<\/code><\/pre>\n\n<p>which just skips working with the .jpeg files. I found another that converts them to <code>.rec<\/code> but again it has essentially the <code>.lst<\/code> already as <code>.json<\/code> and just converts it.<\/p>\n\n<p>I have mostly been working in a Python Jupyter notebook within the AWS console (in my browser) but I have also tried using their GUI. <\/p>\n\n<p>How can I simply and automatically generate the <code>.lst<\/code> or otherwise get the data\/class info into SageMaker without manually creating a <code>.lst<\/code> file?<\/p>\n\n<p><strong><em>Update<\/em><\/strong><\/p>\n\n<p>It looks like im2py can't be run against s3. You'd have to completely download everything from all s3 buckets into the notebook's storage...<\/p>\n\n<blockquote>\n  <p>Please note that [...] im2rec.py is running locally,\n  therefore cannot take input from the S3 bucket. To generate the list\n  file, you need to download the data and then use the im2rec tool. - AWS SageMaker Team<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559921851153,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1560181849816,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56497428",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.4,
        "Challenge_reading_time":31.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":null,
        "Challenge_title":"Use images in s3 with SageMaker without .lst files",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":937.0,
        "Challenge_word_count":336,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1399301338467,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Columbia, MD, USA",
        "Poster_reputation_count":21413.0,
        "Poster_view_count":6465.0,
        "Solution_body":"<p>There are 3 options to provide annotated data to the Image Classification algo: (1) packing labels in recordIO files, (2) storing labels in a JSON manifest file (\"augmented manifest\" option), (3) storing labels in a list file. All options are documented here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html<\/a>.<\/p>\n\n<p>Augmented Manifest and .lst files option are quick to do since they just require you to create an annotation file with a usually quick <code>for<\/code> loop for example. RecordIO requires you to use <code>im2rec.py<\/code> tool, which is a little more work.<\/p>\n\n<p>Using .lst files is <strong>another option<\/strong> that is reasonably easy: you just need to create annotation them with a quick for loop, like this:<\/p>\n\n<pre><code># assuming train_index, train_class, train_pics store the pic index, class and path\n\nwith open('train.lst', 'a') as file:\n    for index, cl, pic in zip(train_index, train_class, train_pics):\n        file.write(str(index) + '\\t' + str(cl) + '\\t' + pic + '\\n')\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.2,
        "Solution_reading_time":14.7,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":143.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a machine learning model deployed in azure designer studio. I need to retrain it everyday with new data through python code. I need to keep the existing csv data in the blob storage and also add some more data to the existing csv and retrain it. If I retrain the model with only the new data, the old data is lost so I need to retrain the model by appending new data to existing data. Is there any way to do it through python coding?<\/p>\n<p>I have also researched about append blob but they add only in the end of the blob. In the documentation, they have mentioned we cannot update or add to an existing blob.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1615438269293,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1615455239928,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66576663",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":7.5,
        "Challenge_reading_time":8.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Can we append data to an existing csv file stored in Azure blob storage through python?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1079.0,
        "Challenge_word_count":134,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1598612683116,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Madurai, Tamil Nadu, India",
        "Poster_reputation_count":60.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>I'm not sure why it has to be one csv file. There are many Python-based libraries for working with a dataset spread across multiple csvs.<\/p>\n<ul>\n<li><a href=\"https:\/\/docs.dask.org\/en\/latest\/dataframe.html\" rel=\"nofollow noreferrer\">Dask DataFrame API<\/a><\/li>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/37639956\/how-to-import-multiple-csv-files-in-a-single-load\"><code>pyspark<\/code><\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-register-datasets?WT.mc_id=AI-MVP-5003930#create-a-tabulardataset\" rel=\"nofollow noreferrer\">Azure Machine Learning Tabular Dataset<\/a><\/li>\n<\/ul>\n<p>In all of the examples, you pass a <a href=\"https:\/\/en.wikipedia.org\/wiki\/Glob_%28programming%29\" rel=\"nofollow noreferrer\"><code>glob<\/code> pattern<\/a>, that will match multiple files. This pattern works very naturally with Azure ML Dataset which you can use as your input. See this excerpt from the docs link above.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace, Datastore, Dataset\n\ndatastore_name = 'your datastore name'\n\n# get existing workspace\nworkspace = Workspace.from_config()\n    \n# retrieve an existing datastore in the workspace by name\ndatastore = Datastore.get(workspace, datastore_name)\n\n# create a TabularDataset from 3 file paths in datastore\ndatastore_paths = [(datastore, 'weather\/2018\/11.csv'),\n                   (datastore, 'weather\/2018\/12.csv'),\n                   (datastore, 'weather\/2019\/*.csv')] # here's the glob pattern\n\nweather_ds = Dataset.Tabular.from_delimited_files(path=datastore_paths)\n<\/code><\/pre>\n<p>Assuming that all the csvs can fit into memory, you can turn these datasets easily into <code>pandas<\/code> dataframes. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-with-datasets#access-dataset-in-training-script\" rel=\"nofollow noreferrer\">with Azure ML Datasets,<\/a> you call<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># get the input dataset by name\ndataset = Dataset.get_by_name(ws, name=dataset_name)\n# load the TabularDataset to pandas DataFrame\ndf = dataset.to_pandas_dataframe()\n<\/code><\/pre>\n<p>With Dask Dataframe, <a href=\"https:\/\/github.com\/dask\/dask\/issues\/1651#issuecomment-253187846\" rel=\"nofollow noreferrer\">this GitHub issue<\/a> says you can call<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>df = my_dask_df.compute()\n<\/code><\/pre>\n<p>As far as output datasets, you can control this by reading in the output CSV as a dataframe, appending data to it then overwriting it to the same location.<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1615448010367,
        "Solution_link_count":6.0,
        "Solution_readability":12.1,
        "Solution_reading_time":33.61,
        "Solution_score_count":2.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":232.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"Hi Team,  \nI'm trying to package sagemaker dependencies as external dependcies to upload to lambda.  \nBut I'm getting the max size limit error. Package size is more than allowed size limit i.e..  deployment package size is 50 MB.  \nAnd the reason I'm trying to do this is, 'get_image_uri' api is not accessible with boto3.  \nsample code for this api :   \n#Import the get_image_url utility function Amazon SageMaker Python SDK and get the location of the XGBoost container.  \n  \nimport sagemaker  \nfrom sagemaker.amazon.amazon_estimator import get_image_uri  \ncontainer = get_image_uri(boto3.Session().region_name, 'xgboost')  \n  \nAny reference would be of great help. Thank you.",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568641875000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668590352780,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUg5l3Jjl4SISDvnjIVYcqaA\/not-able-to-add-sagemaker-dependencies-as-external-dependencies-to-lambda",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.7,
        "Challenge_reading_time":9.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"not able to add sagemaker dependencies as external dependencies to lambda",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":338.0,
        "Challenge_word_count":104,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Could you explain in more detail why do you want to have sagemaker inside of a lambda please?",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1568642184000,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":1.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1471320203283,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":66.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I try to read metrics in this way:<\/p>\n\n<pre><code> data, info = mlflow.get_run(run_id)\n print(data[1].metrics)\n # example of output: {'loss': 0.01}\n<\/code><\/pre>\n\n<p>But it get only last value. It is possible to read manually all steps of a particular metric?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1583838649443,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1647258268603,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60616430",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":7.0,
        "Challenge_reading_time":4.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"MLflow: how to read metrics or params from an existing run?",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2995.0,
        "Challenge_word_count":47,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1547467399036,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Busto Arsizio, VA, Italia",
        "Poster_reputation_count":171.0,
        "Poster_view_count":31.0,
        "Solution_body":"<p>I ran into this same problem and was able to do get all of the values for the metric by using using <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.get_metric_history\" rel=\"noreferrer\"><code>mlflow.tracking.MlflowClient().get_metric_history<\/code><\/a>. This will return every value you logged using <code>mlflow.log_metric(key, value)<\/code>.<\/p>\n<p>Quick example (untested)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\ntrackingDir = 'file:\/\/\/....'\nregistryDir = 'file:\/\/\/...'\nrunID = 'my run id'\nmetricKey = 'loss'\n\nclient = mlflow.tracking.MlflowClient(\n            tracking_uri=trackingDir,\n            registry_uri=registryDir,\n        )\n\nmetrics = client.get_metric_history(runID, metricKey)\n<\/code><\/pre>\n<p><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.get_metric_history\" rel=\"noreferrer\">From the docs<\/a><\/p>\n<blockquote>\n<p>get_metric_history(run_id, key)[source] Return a list of metric\nobjects corresponding to all values logged for a given metric.<\/p>\n<p>Parameters run_id \u2013 Unique identifier for run<\/p>\n<p>key \u2013 Metric name within the run<\/p>\n<p>Returns A list of mlflow.entities.Metric entities if logged, else\nempty list<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from mlflow.tracking import MlflowClient\n\ndef print_metric_info(history):\n    for m in history:\n        print(&quot;name: {}&quot;.format(m.key))\n        print(&quot;value: {}&quot;.format(m.value))\n        print(&quot;step: {}&quot;.format(m.step))\n        print(&quot;timestamp: {}&quot;.format(m.timestamp))\n        print(&quot;--&quot;)\n\n# Create a run under the default experiment (whose id is &quot;0&quot;). Since this is low-level\n# CRUD operation, the method will create a run. To end the run, you'll have\n# to explicitly end it. \nclient = MlflowClient() \nexperiment_id = &quot;0&quot; \nrun = client.create_run(experiment_id) \nprint(&quot;run_id:{}&quot;.format(run.info.run_id))\nprint(&quot;--&quot;)\n\n# Log couple of metrics, update their initial value, and fetch each\n# logged metrics' history. \nfor k, v in [(&quot;m1&quot;, 1.5), (&quot;m2&quot;, 2.5)]:\n    client.log_metric(run.info.run_id, k, v, step=0)\n    client.log_metric(run.info.run_id, k, v + 1, step=1)\n    print_metric_info(client.get_metric_history(run.info.run_id, k))\nclient.set_terminated(run.info.run_id) \n<\/code><\/pre>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.7,
        "Solution_reading_time":31.1,
        "Solution_score_count":5.0,
        "Solution_sentence_count":24.0,
        "Solution_word_count":204.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1351154914716,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2564.0,
        "Answerer_view_count":451.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created a Vetex AI dataset in <code>us-central1<\/code> and confirm it exists using:<\/p>\n<pre><code>vertex_ai.TabularDataset.list()\n<\/code><\/pre>\n<p>When I look at the UI I don't see any datsets, but I see a region drop-down, but no <code>us-central1<\/code>. Why is that? (The project is the correct one).<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652177244937,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72184371",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.1,
        "Challenge_reading_time":4.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Why is my Vertex AI dataset not displayed?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":89.0,
        "Challenge_word_count":51,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>It <strong>is<\/strong> there but at the beginning of the list, not with the other US ones.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":1.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":16.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hola a todos, perdon quiza sea muy basica mi pregunta, no se como importar un excel como Dataset. Solo puedo importar CSV, etc. Muchas gracias<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1614968855700,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/301247\/excel-en-microsoft-azure",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":2.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Excel en Microsoft Azure",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":28,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. Excel is not a <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.tabulardataset?view=azure-ml-py\">supported format<\/a> for Azure ML Tabular datasets. I recommend that you convert your excel file to .csv file (save as .csv) before importing to Azure ML. Hope this helps!    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":6.7,
        "Solution_reading_time":4.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":40.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"After successfully uploading CSV files from S3 to SageMaker notebook instance, I am stuck on doing the reverse.  \n  \nI have a dataframe and want to upload that to S3 Bucket as CSV or JSON. The code that I have is below:  \n  \nbucket='bucketname'  \ndata_key = 'test.csv'  \ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)  \ndf.to_csv(data_location)  \nI assumed since I successfully used pd.read_csv() while loading, using df.to_csv() would also work but it didn't. Probably it is generating error because this way I cannot pick the privacy options while uploading a file manually to S3. Is there a way to upload the data to S3 from SageMaker?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562042043000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668613629012,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUfoMiB7A8SFOpr5uklZZuNg\/uploading-a-dataframe-to-aws-s3-bucket-from-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":8.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Uploading a Dataframe to AWS S3 Bucket from SageMaker",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2074.0,
        "Challenge_word_count":106,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"One way to solve this would be to save the CSV to the local storage on the SageMaker notebook instance, and then use the S3 API's via boto3 to upload the file as an s3 object. S3 docs for upload_file() available here.  \n  \nNote, you'll need to ensure that your SageMaker hosted notebook instance has proper ReadWrite permissions in its IAM role, otherwise you'll receive a permissions error.  \n  \n# code you already have, saving the file locally to whatever directory you wish  \nfile_name = \"mydata.csv\"   \ndf.to_csv(file_name)  \n# instantiate S3 client and upload to s3  \nimport boto3  \n  \ns3 = boto3.resource('s3')  \ns3.meta.client.upload_file(file_name, 'YOUR_S3_BUCKET_NAME', 'DESIRED_S3_OBJECT_NAME')  \nAlternatively, upload_fileobj() may help for parallelizing as a multi-part upload.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1562042063000,
        "Solution_link_count":0.0,
        "Solution_readability":11.4,
        "Solution_reading_time":9.58,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":107.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":92.1736986111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have some data with very particular format (e.g., tdms files generated by NI systems) and I stored them in a S3 bucket. Typically, for reading this data in python if the data was stored in my local computer, I would use npTDMS package. But, how should is read this tdms files when they are stored in a S3 bucket? One solution is to download the data for instance to the EC2 instance and then use npTDMS package for reading the data into python. But it does not seem to be a perfect solution. Is there any way that I can read the data similar to reading CSV files from S3? <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1576870865157,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59430560",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":6.6,
        "Challenge_reading_time":7.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Reading Data from AWS S3",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":4393.0,
        "Challenge_word_count":116,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1534965197292,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seattle, WA",
        "Poster_reputation_count":320.0,
        "Poster_view_count":48.0,
        "Solution_body":"<p>Some Python packages (such as Pandas) support reading data directly from S3, as it is the most popular location for data. See <a href=\"https:\/\/stackoverflow.com\/questions\/37703634\/how-to-import-a-text-file-on-aws-s3-into-pandas-without-writing-to-disk\">this question<\/a> for example on the way to do that with Pandas.<\/p>\n\n<p>If the package (npTDMS) doesn't support reading directly from S3, you should copy the data to the local disk of the notebook instance.<\/p>\n\n<p>The simplest way to copy is to run the AWS CLI in a cell in your notebook<\/p>\n\n<pre><code>!aws s3 cp s3:\/\/bucket_name\/path_to_your_data\/ data\/\n<\/code><\/pre>\n\n<p>This command will copy all the files under the \"folder\" in S3 to the local folder <code>data<\/code><\/p>\n\n<p>You can use more fine-grained copy using the filtering of the files and other specific requirements using the boto3 rich capabilities. For example:<\/p>\n\n<pre><code>s3 = boto3.resource('s3')\nbucket = s3.Bucket('my-bucket')\nobjs = bucket.objects.filter(Prefix='myprefix')\nfor obj in objs:\n   obj.download_file(obj.key)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1577202690472,
        "Solution_link_count":1.0,
        "Solution_readability":10.8,
        "Solution_reading_time":13.62,
        "Solution_score_count":3.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":133.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nIt is not possible to store a ``PartitionedDataSet`` as an mlflow artifact with the ``MlflowArtifactDataSet``.\r\n\r\n## Context\r\n\r\nI had a use case where I need to save a dict with many small result tables to mlflow, and I tried to use ``PartitionedDataSet`` for this.\r\n\r\n## Steps to Reproduce\r\n\r\n```yaml\r\n# catalog.yml\r\n\r\nmy_dataset:\r\n    type: kedro_mlflow.io.artifacts.MlflowArtifactDataSet\r\n    data_set:\r\n        type: PartitionedDataSet  # or any valid kedro DataSet\r\n        path: \/path\/to\/a\/local\/folder # the attribute is \"path\", and not \"filepath\"!\r\n        dataset: \"pandas.CSVDataSet\"\r\n```\r\n\r\nthen save a dict using this dataset:\r\n\r\n```\r\ncatalog.save(\"my_dataset\", dict(\"a\": pd.DataFrame(data=[1,2,3], columns=[\"a\"], \"b\": pd.DataFrame(data=[1,2,3], columns=[\"b\"])\r\n```\r\n## Expected Result\r\n\r\nThe 2 Dataframes should be logged as artifacts in the current mlflow run.\r\n\r\n## Actual Result\r\n\r\nAn error ``dataset has not attribute \"_filepath\"`` is raised.\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Potential solution\r\n\r\nThe error comes from this line:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/904207ad505b71391d78d8088aaed151ca6a011d\/kedro_mlflow\/io\/artifacts\/mlflow_artifact_dataset.py#L53\r\n\r\nmaybe we can add a better condition here to default to \"path\" if there is no \"filepath\" attribute.",
        "Challenge_closed_time":1644674.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636062318000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/258",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":7.6,
        "Challenge_reading_time":17.08,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"MlflowArtifactDataSet does not work with PartitionedDataSet",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":156,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1558713599190,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":58.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>we are running a Spark job against our Kubernetes cluster and try to log the model to MLflow. We are running Spark 3.2.1 and MLflow 1.26.1 and we are using the following jars to communicate with s3 <code>hadoop-aws-3.2.2.jar<\/code> and <code>aws-java-sdk-bundle-1.11.375.jar<\/code> and configure our spark-submit job with the following parameters:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>  --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider \\\n  --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\\n  --conf spark.hadoop.fs.s3a.fast.upload=true \\\n<\/code><\/pre>\n<p>When we try to save our Spark model with <code>mlflow.spark.log_model()<\/code> we are getting the following exception:<\/p>\n<pre class=\"lang-java prettyprint-override\"><code>22\/06\/24 13:27:21 ERROR Instrumentation: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme &quot;s3&quot;\n    at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n    at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n    at org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:673)\n    at org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n    at org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n    at org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n    at org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n    at org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n    at org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n    at org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n    at org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n    at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n    at scala.util.Try$.apply(Try.scala:213)\n    at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n    at org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n    at java.base\/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at java.base\/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n    at java.base\/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n    at java.base\/java.lang.reflect.Method.invoke(Unknown Source)\n    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n    at py4j.Gateway.invoke(Gateway.java:282)\n    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)\n    at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n    at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n    at java.base\/java.lang.Thread.run(Unknown Source)\n<\/code><\/pre>\n<p>We tried to start our MLflow server with <code>-default-artifact-root<\/code> set to <code>s3a:\/\/...<\/code> but when we run our spark job and we call <code>mlflow.get_artifact_uri()<\/code> (which is also used to construct the upload uri in <code>mlflow.spark.log_model()<\/code>) the result starts with <code>s3<\/code> which probably cause the former mentioned exception.\nSince Hadoop dropped support for the <code>s3:\/\/<\/code> filesystem does anyone know how to log spark models to s3 using MLflow?<\/p>\n<p>Cheers<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1656078725497,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1656165970247,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72745109",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":24.1,
        "Challenge_reading_time":54.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":48,
        "Challenge_solved_time":null,
        "Challenge_title":"No FileSystem for scheme \"s3\" exception when using spark with mlflow",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":148.0,
        "Challenge_word_count":234,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442649179160,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":773.0,
        "Poster_view_count":58.0,
        "Solution_body":"<p>Additional to the <code>spark.hadoop.fs.s3a.impl<\/code> config parameter, you can try to also set <code>spark.hadoop.fs.s3.impl<\/code> to <code>org.apache.hadoop.fs.s3a.S3AFileSystem<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.3,
        "Solution_reading_time":2.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":15.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1472202466916,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":505.0,
        "Answerer_view_count":52.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm using the following code to do a sklearn transformation job in sagemaker:<\/p>\n<pre><code>region = boto3.session.Session().region_name\nrole = sagemaker.get_execution_role()\nsklearn_processor = SKLearnProcessor(\n    framework_version=&quot;1.0-1&quot;, role=role,\n    instance_type=&quot;ml.m5.xlarge&quot;, instance_count=1,\n    # sagemaker_session = Session()\n)\nout_path = os.path.join(bucket, prefix, f'test_transform\/data.csv')\nsklearn_processor.run(\n    code=&quot;preprocess.py&quot;,\n    inputs = [\n        ProcessingInput(source = 'my_package\/', destination = '\/opt\/ml\/processing\/input\/code\/my_package\/')\n    ],\n    outputs=[\n        ProcessingOutput(output_name=&quot;test_transform_data&quot;, \n                         source = '\/opt\/ml\/processing\/output\/test_transform',\n                         destination = out_path),\n    ],\n    arguments=[&quot;--time-slot-minutes&quot;, &quot;30min&quot;]\n)\n<\/code><\/pre>\n<p>Within the above code, it's running preprocess.py, and (within) preprocess.py loads the data from snowflake database using the credentials saved in aws secretsmanager:<\/p>\n<pre><code>region = boto3.Session().region_name\nsecrets_client = boto3.client(service_name='secretsmanager', region_name=region)\n<\/code><\/pre>\n<p>So here's where error happen: first line above returns region as None, so the the second line of code raises <code>botocore.exceptions.NoRegionError: You must specify a region<\/code><\/p>\n<p>In this case, how can I pass the region to SKLearnProcessor or is there any other way to make the code working within the processing job instance?<\/p>\n<p>FYI:\nthe source of input <code>'my_package\/'<\/code> is in the structure below to install packages and include py dependencies used in <code>preprocess.py<\/code><\/p>\n<pre><code>\u251c\u2500\u2500 my_package\n\u2502   \u251c\u2500\u2500 file1.py\n\u2502   \u251c\u2500\u2500 file2.py\n\u2502   \u2514\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 preprocess.py\n<\/code><\/pre>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663640822130,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73781018",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":15.4,
        "Challenge_reading_time":24.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"How to pass region to the SKLearnProcessor - botocore.exceptions.NoRegionError: You must specify a region",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":20.0,
        "Challenge_word_count":168,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1472202466916,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":505.0,
        "Poster_view_count":52.0,
        "Solution_body":"<p>set following in the code preprocess.py solved the issue:<\/p>\n<pre><code>os.environ['AWS_DEFAULT_REGION'] = 'us-west-2' \n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.8,
        "Solution_reading_time":1.82,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":12.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":169.8337822222,
        "Challenge_answer_count":1,
        "Challenge_body":"I have setup the pdf labelling task in Sagemaker groud truth following this link - https:\/\/github.com\/aws-samples\/amazon-comprehend-semi-structured-documents-annotation-tools\n\nAfter sometime, the job is failed saying  \"**Complete with labeling errors**\". I found the below log in cloudwatch logs\n\n```\n{\n    \"event-name\": \"HUMAN_TASK_FAILED\",\n    \"event-log-message\": \"ERROR: Human task failed for line 694.\",\n    \"labeling-job-name\": \"resume-labeling-job-20221201T182336\"\n}\n```\n\nNot sure what happened. Does anyone have any way to identify the root cause behind this failure ?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1670920092769,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1671266492940,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUirlDbmZPS8SCqnmC2RFA6A\/human-task-failed-sagemaker-ground-truth-labelling-jobs",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":7.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Human Task Failed - Sagemaker ground truth labelling jobs",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":91.0,
        "Challenge_word_count":70,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Since I didn't get answer here, I reached out to tech support to seek guidance. It looks like the job duration is elapsed. \n\n\"failure-reason\": \"ClientError: Annotation tasks expired. \n\nReasons are TaskAvailabilityLifetimeInSeconds parameter is too small. \n\nYou can validate this configuration by running the following AWS CLI command from your environment:\naws sagemaker describe-labeling-job --labeling-job-name resume-labeling-job-20221212T094103\n\n\u2014References\u2014\n[1] https:\/\/docs.aws.amazon.com\/sagemaker\/\/latest\/APIReference\/API_HumanTaskConfig.html#API_HumanTaskConfig_Contents",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1671877894556,
        "Solution_link_count":1.0,
        "Solution_readability":14.9,
        "Solution_reading_time":7.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":58.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have two datasets with multiple columns. I would like to join the two tables with the following keys: zip code, year, month, data, hour<\/p>\n\n<p>However whenever I use a <strong>Join Module<\/strong> on these two tables, the Join doesn't happen, and I just get a Table with Columns from Right Table with empty values.<\/p>\n\n<p>Here is the R equivalent of what I am trying to do:<\/p>\n\n<pre><code>YX &lt;- leftTableDT\nYX %&lt;&gt;% merge( rightTableDT, all.x = TRUE, by=c('zip','year','month','day','hour') )\n<\/code><\/pre>\n\n<p>Any ideas on why Join Module in Azure ML Studio doesn't work for multiple keys?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1492982700450,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/43576656",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":8.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Join 2 tables with with mutiple keys in Azure ML Studio",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":582.0,
        "Challenge_word_count":102,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1281811007747,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Capitola, CA",
        "Poster_reputation_count":3675.0,
        "Poster_view_count":638.0,
        "Solution_body":"<p>Double-check that you've selected \"Allow duplicates and preserve column order in selection\" in column selection options, so it matches the columns in listed order.<\/p>\n\n<p>Also, you could try Apply SQL Transformation module to join datasets.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.5,
        "Solution_reading_time":3.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":35.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>We are experimenting with programmatic report generation with WandB.<br>\nI would like to be able to add a Confusion Matrix to a report, but this is not one of the base types (as far as I can tell). Is there a good way to do this?<br>\nI could generate a PNG\/Image and insert it, but I can\u2019t figure out how to add an Image to a report yet (see recent question).  Are there other ways?<br>\nThanks.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1665987764533,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/adding-confusion-matrix-to-report-programatically\/3267",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.4,
        "Challenge_reading_time":5.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Adding Confusion Matrix to report programatically",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":140.0,
        "Challenge_word_count":81,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/kevinashaw\">@kevinashaw<\/a> I am also posting here <a href=\"https:\/\/colab.research.google.com\/drive\/1Fepp-JLFvK-wLL2BZ_BnkCG_fAC6HFbo#scrollTo=An_example_with_all_of_the_blocks_and_panels\" rel=\"noopener nofollow ugc\">this Colab<\/a> and the Python SDK commands of our <a href=\"https:\/\/docs.wandb.ai\/guides\/reports\/edit-a-report#add-plots\">Reports reference docs<\/a> which may be helpful.<\/p>\n<p>The confusion matrix isn\u2019t <a href=\"https:\/\/github.com\/wandb\/wandb\/blob\/main\/wandb\/apis\/reports\/panels.py\" rel=\"noopener nofollow ugc\">currently exposed<\/a> but I have increased this feature requests for our engineering team. We will reach out to you once this is implemented. I hope this helps, please let me know if you have any further questions or issues with the Reports API.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.7,
        "Solution_reading_time":10.75,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":81.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":1.4294077778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to convert a csv file from s3 into a table in Athena. When I run the query on Athena console it works but when I run it on Sagemaker Jupyter notebook with boto3 client it returns: <\/p>\n\n<pre><code>\"**InvalidRequestException**: An error occurred (InvalidRequestException) when calling the StartQueryExecution operation: line 1:8: no viable alternative at input 'CREATE EXTERNAL'\"\n<\/code><\/pre>\n\n<p>Here is my code <\/p>\n\n<pre><code>def run_query(query):\n    client = boto3.client('athena')\n    response = client.start_query_execution(\n        QueryString=query,\n        ResultConfiguration={\n            'OutputLocation': 's3:\/\/path\/to\/s3output',\n            }\n        )\n    print('Execution ID: ' + response['QueryExecutionId'])\n    return response\n\ncreateTable = \\\n\"\"\"CREATE EXTERNAL TABLE TestTable (\n    ID string,\n    CustomerId string, \n    Ip string,\n    MessageFilename string\n\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES (\n   'separatorChar' = ',',\n   'quoteChar' = '\\\"',\n   'escapeChar' = '\\\\'\n )\nSTORED AS TEXTFILE\nLOCATION 's3:\/\/bucket_name\/results\/csv\/'\nTBLPROPERTIES (\"skip.header.line.count\"=\"1\")\"\"\"\n\nresponse = run_query(createTable, s3_output)\nprint(response)\n<\/code><\/pre>\n\n<p>I have run queries through boto3 client in json format (so, using  ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe') which have worked well but somehow this doesn't. I have tried changing names, syntax, quotes but that doesn't seem to work. <\/p>\n\n<p>Any suggestion would be very appreciated, \nThank you! <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1532122235887,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1532122372580,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51450610",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":19.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Athena query works in console but not with boto3 client in sagemaker (convert csv into table)",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1635.0,
        "Challenge_word_count":176,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1395796123356,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Atlanta, GA, USA",
        "Poster_reputation_count":23.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Thanks for sharing complete example. The issue is with the escaping in <code>SERDEPROPERTIES<\/code>. After modifying <code>createTable<\/code> as below it works<\/p>\n\n<pre><code>createTable = \\\n\"\"\"CREATE EXTERNAL TABLE testtable (\n    `id` string,\n    `customerid` string, \n    `ip` string,\n    `messagefilename` string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES (\n  'separatorChar' = ',', \n  'quoteChar' = '\\\\\\\"', \n  'escapeChar' = '\\\\\\\\' )\nSTORED AS TEXTFILE\nLOCATION 's3:\/\/bucket_name\/results\/csv\/'\nTBLPROPERTIES (\"skip.header.line.count\"=\"1\");\"\"\"\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1532127518448,
        "Solution_link_count":0.0,
        "Solution_readability":14.1,
        "Solution_reading_time":7.64,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":50.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":6,
        "Challenge_body":"**Describe the bug**\r\nGetting error \"FileNotFoundError: [WinError 2] The system cannot find the file specified\" while running \"mlflow ui\".\r\n\r\n**To Reproduce**\r\nRun \"mlflow ui\"\r\n\r\n\r\n**Expected behavior**\r\nIt should run without any issues\r\n\r\n\r\n**Versions**\r\n2.3.10\r\n\r\nNot sure if this is the right forum to post this issue. If it is not, please ignore.\r\n<!-- Thanks for contributing! -->\r\n",
        "Challenge_closed_time":1650449.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650277642000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2425",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":5.9,
        "Challenge_reading_time":4.86,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] mlflow ui never runs",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":59,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@maverick-scientist there is not enough information here to recreate or debug the issue. Please provide a complete reproducible example so we can debug. Also, note that if the data is proprietary, you can create a fake dataset yourself to recreate the issue. Refer to the following for more details:\r\n\r\n\ud83d\udccc Overview: http:\/\/ow.ly\/LoIv50IL5RQ\r\n\ud83d\udccc Examples (Do's and Dont's): http:\/\/ow.ly\/AXKm50IL5RR\r\n\r\n- The do's and dont's have an example of how to create the fake data - minimal to reproduce the problem.\r\n- Alternately, you could try to reproduce the issue with a publicly available dataset or one available in pycaret itself.\r\n\r\nThanks!\r\n Hi Nikhil,\nCan we please discuss this over teams call? I can share a python file with\nyou for this issue but I feel if we can discuss this on a call it would\nsave me some time.\n\nWhat do you think?\n\nPlease advise.\n\nOn Mon, 18 Apr 2022 at 22:55, Nikhil Gupta ***@***.***> wrote:\n\n> @maverick-scientist <https:\/\/github.com\/maverick-scientist> there is not\n> enough information here to recreate or debug the issue. Please provide a\n> complete reproducible example so we can debug. Also, note that if the data\n> is proprietary, you can create a fake dataset yourself to recreate the\n> issue. Refer to the following for more details:\n>\n> \ud83d\udccc Overview: http:\/\/ow.ly\/LoIv50IL5RQ\n> \ud83d\udccc Examples (Do's and Dont's): http:\/\/ow.ly\/AXKm50IL5RR\n>\n>    - The do's and dont's have an example of how to create the fake data -\n>    minimal to reproduce the problem.\n>    - Alternately, you could try to reproduce the issue with a publicly\n>    available dataset or one available in pycaret itself.\n>\n> Thanks!\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/pycaret\/pycaret\/issues\/2425#issuecomment-1101586641>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AUVHRACMIA55LOVAGIZZKPLVFWLKHANCNFSM5TVQ4MTQ>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n @maverick-scientist Honestly, I would prefer not to do that since it sets the wrong precedent for the open-source community and is not sustainable in the long run. The globally accepted best practice is to provide a minimal reproducible example and I would encourage you to do that.\r\n\r\nThanks! Hi Nikhil,\r\nAttaching the code file to reproduce this issue. Could you please check and tell me what's wrong with it or my machine?\r\n\r\nThanks & Regards,\r\nAbhinav\r\n\r\n[Simple MLflow.zip](https:\/\/github.com\/pycaret\/pycaret\/files\/8511837\/Simple.MLflow.zip)\r\n\r\n @maverick-scientist The example that you posted has no reference to pycaret. It is a generic MLFlow example. How is it related to pycaret and this repo? Hi Nikhil,\nThank you for your reply. However, if you read the issue carefully, I\u2019d\nclearly mentioned my dilemma whether it was the right forum to post this\nissue.\n\nAnyways, thanks for your support. I\u2019d try and see what\u2019s preventing the\nMLFlow to run properly on my machine.\n\nOn Wed, 20 Apr 2022 at 15:21, Nikhil Gupta ***@***.***> wrote:\n\n> @maverick-scientist <https:\/\/github.com\/maverick-scientist> The example\n> that you posted has no reference to pycaret. It is a generic MLFlow\n> example. How is it related to pycaret and this repo?\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/pycaret\/pycaret\/issues\/2425#issuecomment-1103727978>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AUVHRADHBVLOBH4SACXZHNLVF7HTHANCNFSM5TVQ4MTQ>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":11.0,
        "Solution_readability":7.7,
        "Solution_reading_time":43.39,
        "Solution_score_count":null,
        "Solution_sentence_count":42.0,
        "Solution_word_count":477.0,
        "Tool":"MLflow"
    }
]