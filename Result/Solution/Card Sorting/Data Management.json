[
    {
        "Answerer_created_time":1403885399456,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":402.0,
        "Answerer_view_count":90.0,
        "Challenge_adjusted_solved_time":262.8901736111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I registered a model in an Azure ML notebook along with its datasets. In ML Studio I can see the model listed under the dataset, but no dataset gets listed under the model. What should I do to have datasets listed under models?<\/p>\n<ul>\n<li>Model listed under dataset:<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fh3bV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fh3bV.png\" alt=\"model dataset\" \/><\/a><\/p>\n<ul>\n<li>Dataset not listed under the model:<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Ueys2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Ueys2.png\" alt=\"dataset model\" \/><\/a><\/p>\n<ul>\n<li>Notebook code:<\/li>\n<\/ul>\n<pre><code>import pickle\nimport sys\nfrom azureml.core import Workspace, Dataset, Model\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import assert_all_finite\n\nworkspace = Workspace('&lt;snip&gt;', '&lt;snip&gt;', '&lt;snip&gt;')\ndataset = Dataset.get_by_name(workspace, name='creditcard')\ndata = dataset.to_pandas_dataframe()\ndata.dropna(inplace=True)\nX = data.drop(labels=[&quot;Class&quot;], axis=1, inplace=False)\ny = data[&quot;Class&quot;]\n\nmodel = make_pipeline(StandardScaler(), GradientBoostingClassifier())\nmodel.fit(X, y)\n\nwith open('creditfraud_sklearn_model.pkl', 'wb') as outfile:\n    pickle.dump(model, outfile)\n\nModel.register(\n    Workspace = workspace,\n    model_name = 'creditfraud_sklearn_model',\n    model_path = 'creditfraud_sklearn_model.pkl',\n    description = 'Gradient Boosting classifier for Kaggle credit-card fraud',\n    model_framework = Model.Framework.SCIKITLEARN,\n    model_framework_version = sys.modules['sklearn'].__version__,\n    sample_input_dataset = dataset,\n    sample_output_dataset = dataset)\n<\/code><\/pre>",
        "Challenge_closed_time":1627145635292,
        "Challenge_comment_count":0,
        "Challenge_created_time":1626199230667,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has registered a model in an Azure ML notebook along with its datasets, but in ML Studio, the datasets are not listed under the model. The user is seeking guidance on how to have datasets listed under models.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68367348",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":24.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":262.8901736111,
        "Challenge_title":"Azure ML Studio not showing datasets under models",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":29.0,
        "Challenge_word_count":162,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403885399456,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":402.0,
        "Poster_view_count":90.0,
        "Solution_body":"<p>It looks like <code>add_dataset_references()<\/code> needs to be called to have datasets displayed under models:<\/p>\n<pre><code>model_registration.add_dataset_references([(&quot;input dataset&quot;, dataset)])\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.9,
        "Solution_reading_time":3.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":243.3152777778,
        "Challenge_answer_count":0,
        "Challenge_body":"Only allow access to project members for the given MLflow.",
        "Challenge_closed_time":1620648494000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619772559000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a \"Connection aborted\" error while trying to perform multi-label classification with \"doc_classification_multilabel.py\". The error occurred during the training process and the user confirmed that their internet connection was stable. The error message suggests that the remote end closed the connection without response. The user is seeking clarification on why this error occurred.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/404",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":5.2,
        "Challenge_reading_time":1.2,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":933.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":243.3152777778,
        "Challenge_title":"Users can access to any MLflow project",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":16,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":147.8475,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nWhen I launch `kedro run` and the run fails, the `on_pipeline_error` closes all the mlflow runs (to avoid interactions with further runs)\r\n\r\n## Context\r\n\r\nI cannot distinguish failed runs from sucessful ones in the mlflow ui.\r\n\r\n## Steps to Reproduce\r\n\r\nLaunch a failing pipeline with kedro run.\r\n\r\n## Expected Result\r\n\r\nThe mlflow ui should display the run with a red cross\r\n\r\n## Actual Result\r\n\r\nThe mlflow ui displays the run with a green tick\r\n\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes.\r\n\r\n## Potential solution: \r\n\r\nReplace these lines:\r\n\r\n`https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L193-L194`\r\n\r\nwith \r\n\r\n```python\r\nwhile mlflow.active_run():\r\n    mlflow.end_run(mlflow.entities.RunStatus.FAILED)\r\n```\r\nor even better, retrieve current run status from mlflow?\r\n",
        "Challenge_closed_time":1606515096000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1605982845000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with MlflowDataSet failing to log on remote storage when the underlying dataset filepath is converted as a PurePosixPath. The error occurs when the local path is Linux and the `mlflow_tracking_uri` is an Azure blob storage. The issue can be fixed by replacing `self._filepath` by `self._filepath.as_posix()` in two locations.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/121",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.8,
        "Challenge_reading_time":11.93,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":21.0,
        "Challenge_repo_issue_count":414.0,
        "Challenge_repo_star_count":145.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":147.8475,
        "Challenge_title":"RunStatus of mlflow run is \"FINISHED\" instead of \"FAILED\" when the kedro run fails",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":117,
        "Discussion_body":"Good catch ! \r\nSince we catch the Error and manually end the run, mlflow do not receive the \"error code 1\" of the current process. If we no longer end run manually, mlflow will tag the run as FAILED. But since we want to control the pipeline error, we can apply your suggestion (specifiying the status as failed) Yes, but we need to terminate the run manually when it failed and one use it interactively (in CLI, tis makes no difference because it gets the error code as you say) to avoid further interference.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1479363468550,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pune, Maharashtra, India",
        "Answerer_reputation_count":108.0,
        "Answerer_view_count":35.0,
        "Challenge_adjusted_solved_time":131.9191969444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to do some kind of web job application that can run for period time and make prediction on azure machine learning studio. After that i want get the result of this experiment and do something with that in my console application. What is the best way to do this in azure with machine learning or maybe some similiar stuff to prediction data from data series ? <\/p>",
        "Challenge_closed_time":1486537080116,
        "Challenge_comment_count":0,
        "Challenge_created_time":1486062508683,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a web job application that can run for a period of time and make predictions on Azure Machine Learning Studio. They are seeking advice on the best way to pass input for the experiment from their console application and retrieve the results.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42010405",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":5.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":131.8253980556,
        "Challenge_title":"The way to pass input for azure machine experiment from app ( for example console app )",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":62.0,
        "Challenge_word_count":83,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432141466928,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":327.0,
        "Poster_view_count":78.0,
        "Solution_body":"<p>You can try using Azure Data Factory to create a Machine Learning pipeline or use Azure ML Studio's Predictive Web Services.<\/p>\n\n<ol>\n<li><p>With Azure Data Factory\nFollow <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/data-factory\/data-factory-azure-ml-batch-execution-activity\" rel=\"nofollow noreferrer\">this link<\/a> for details. Azure Data Factory implementations would seem difficult at first but they do work great with Azure ML experiments. <\/p>\n\n<p>Azure Data Factory can run your ML Experiment on a schedule or one-off at a specified time (I guess you can set only for UTC Timezone right now) and monitor it through a dashboard (which is pretty cool).<\/p>\n\n<p>As an example you can look @ <a href=\"https:\/\/github.com\/Microsoft\/azure-docs\/blob\/master\/articles\/data-factory\/data-factory-azure-ml-batch-execution-activity.md\" rel=\"nofollow noreferrer\">ML Batch Execution<\/a>. I used this in one of our implementations (we do have latency issues, but trying to solve that).<\/p><\/li>\n<li><p>If you directly want to use the experiment in your console (assuming it is a web application), use create a Predictive Web service out of your ML Experiment, details <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-walkthrough-5-publish-web-service\" rel=\"nofollow noreferrer\">here<\/a><\/p><\/li>\n<\/ol>\n\n<p>I couldn't exactly understand your use case so I posted two alternatives that should help you. Hope this might lead you to a better solution\/approach.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1486537417792,
        "Solution_link_count":3.0,
        "Solution_readability":14.4,
        "Solution_reading_time":19.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":180.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1399363600132,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Heidelberg, Germany",
        "Answerer_reputation_count":8423.0,
        "Answerer_view_count":1313.0,
        "Challenge_adjusted_solved_time":2.4000266667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using the <code>HuggingFacePredictor<\/code> from <code>sagemaker.huggingface<\/code> to inference some text and I would like to get all label scores.<\/p>\n<p>Is there any way of getting, as response from the endpoint:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;labels&quot;: [&quot;help&quot;, &quot;Greeting&quot;, &quot;Farewell&quot;] ,\n    &quot;score&quot;: [0.81, 0.1, 0.09],\n}\n<\/code><\/pre>\n<p>(or similar)<\/p>\n<p>Instead of:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;label&quot;: &quot;help&quot;,\n    &quot;score&quot;: 0.81,\n}\n<\/code><\/pre>\n<p>Here is some example code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nfrom sagemaker.huggingface import HuggingFacePredictor\nfrom sagemaker.session import Session\n\nsagemaker_session = Session(boto_session=boto3.session.Session())\n\npredictor = HuggingFacePredictor(\n    endpoint_name=project, sagemaker_session=sagemaker_session\n)\nprediciton = predictor.predict({&quot;inputs&quot;: text})[0]\n<\/code><\/pre>",
        "Challenge_closed_time":1643809235227,
        "Challenge_comment_count":1,
        "Challenge_created_time":1643804083130,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using HuggingFacePredictor from sagemaker.huggingface to inference some text and wants to get all label scores as a response from the endpoint instead of just one label and score. The user has provided example code for reference.",
        "Challenge_last_edit_time":1643805132912,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70955450",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":15.1,
        "Challenge_reading_time":14.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":1.4311380556,
        "Challenge_title":"How to return all labels and scores in SageMaker Inference?",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":193.0,
        "Challenge_word_count":91,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1632991357332,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Barcelona",
        "Poster_reputation_count":373.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>With your current code sample, it is not quite clear what specific task you are performing, but for the sake of this answer, I'll assume you're doing text classification.<\/p>\n<p>Most importantly, though, we can read the following in <a href=\"https:\/\/huggingface.co\/docs\/sagemaker\/reference#inference-toolkit-api\" rel=\"nofollow noreferrer\">Huggingface's Sagemaker reference document<\/a> (bold highlight by me):<\/p>\n<blockquote>\n<p>The Inference Toolkit accepts inputs in the inputs key, and <strong>supports additional <code>pipelines<\/code> parameters in the <code>parameters<\/code> key<\/strong>. You can provide any of the supported <code>kwargs<\/code> from <code>pipelines<\/code> as parameters.<\/p>\n<\/blockquote>\n<p>If we check out the <a href=\"https:\/\/huggingface.co\/docs\/transformers\/v4.16.2\/en\/main_classes\/pipelines#transformers.TextClassificationPipeline.__call__\" rel=\"nofollow noreferrer\">accepted arguments by the <code>TextClassificationPipeline<\/code><\/a>, we can see that there is indeed one that returns all samples:<\/p>\n<blockquote>\n<p><code>return_all_scores<\/code> (bool, optional, defaults to False) \u2014 Whether to return scores for all labels.<\/p>\n<\/blockquote>\n<p>While I unfortunately don't have access to Sagemaker inference, I can run a sample to illustrate the output with a local pipeline:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import pipeline\n# uses 2-way sentiment classification model per default\npipe = pipeline(&quot;text-classification&quot;) \n\npipe(&quot;I am really angry right now &gt;:(&quot;, return_all_scores=True)\n# Output: [[{'label': 'NEGATIVE', 'score': 0.9989138841629028},\n#           {'label': 'POSITIVE', 'score': 0.0010860705515369773}]]\n<\/code><\/pre>\n<p>Based on the slightly different input format expected by Sagemaker, coupled with the example given in <a href=\"https:\/\/github.com\/huggingface\/notebooks\/blob\/master\/sagemaker\/10_deploy_model_from_s3\/deploy_transformer_model_from_s3.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a>, I would assume that a corrected input in your own example code should look like this:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;inputs&quot;: text,\n    &quot;parameters&quot;: {&quot;return_all_scores&quot;: True}\n}\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1643813773008,
        "Solution_link_count":3.0,
        "Solution_readability":17.8,
        "Solution_reading_time":29.73,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":222.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1430233500800,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":212.0,
        "Answerer_view_count":25.0,
        "Challenge_adjusted_solved_time":197.9953861111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Currently I am exploring AWS sagemaker and I am facing a problem e.g. If I want to train my network on 1000s of epochs I cant stay active all the time. But as I logout my the notebook instance also stop execution. Is there any way to keep the instance active even after you logout ? <\/p>",
        "Challenge_closed_time":1541803691470,
        "Challenge_comment_count":0,
        "Challenge_created_time":1541090908080,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with Amazon Sagemaker Notebook instance where the execution stops as they log out, making it difficult to train networks on multiple epochs. They are seeking a solution to keep the instance active even after logging out.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53105741",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":4.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":197.9953861111,
        "Challenge_title":"Amazon Sagemaker Notebook instance stop execution as I logout",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2598.0,
        "Challenge_word_count":64,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1492699347027,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":37.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>Do you mean logging out of AWS console or your laptop? Your training job should still be running on the notebook instance whether you have notebook open or not. Notebook instance will always be active until you manually stop it[1]. You can always access the notebook instance again by opening the notebook through console.<\/p>\n\n<p>[1]<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StopNotebookInstance.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StopNotebookInstance.html<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.8,
        "Solution_reading_time":7.02,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":58.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.6311111111,
        "Challenge_answer_count":1,
        "Challenge_body":"Some Amazon SageMaker algorithms can train with a manifest JSON file that stores the mapping between images and their Amazon S3 ARNs and metadata, such as labels. This is a great option, because the manifest file is much smaller than the dataset itself. Because the manifest files are small, they can be used easily in versioning tools or saved as part of the model artifact. This appears to be the best construct enabling exact dataset versioning within SageMaker. i.e., if we exclude the creation of a unique training set hard copy per training job that can't be scaled to large datasets. Is my understanding accurate?",
        "Challenge_closed_time":1607684202000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607681930000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is asking if Amazon SageMaker manifest files allow for dataset versioning, as they are smaller than the dataset and can be easily used in versioning tools or saved as part of the model artifact. They are wondering if this is the best way to enable exact dataset versioning within SageMaker, aside from creating a unique training set hard copy per training job that cannot be scaled to large datasets.",
        "Challenge_last_edit_time":1667981435996,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUq44kZCYWTiOnwXblHSQSTA\/do-amazon-sagemaker-manifest-files-enable-dataset-versioning",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":8.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.6311111111,
        "Challenge_title":"Do Amazon SageMaker manifest files enable dataset versioning?",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":124.0,
        "Challenge_word_count":112,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"If you create the conditions for immutability of the assets the manifest points to, then manifest enables exact dataset versioning with SageMaker. You can have a data store in Amazon S3 with all versions of the data assets and use the manifest files for creating and versioning datasets for specific usage.\n\nIf you don't guarantee immutability for the assets that the manifest points to, then your manifest becomes invalid.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925565630,
        "Solution_link_count":0.0,
        "Solution_readability":12.3,
        "Solution_reading_time":5.2,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":69.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1460436951967,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":156.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":0.2497888889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have an experiment in azure machine learning studio, and I would like to the see entire scored dataset.<\/p>\n\n<p>Naturally I used the 'visualise' option on the scored dataset but these yields only 100 rows (the test dataset is around 500 rows)<\/p>\n\n<p>I also tired the 'save as dataset' option, but then file does not open well with excel or text editor (special character encoding)<\/p>\n\n<p>Basically, I want to see the entire test data with scored labels as table or download as .csv maybe<\/p>",
        "Challenge_closed_time":1460437058280,
        "Challenge_comment_count":0,
        "Challenge_created_time":1460436159040,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in downloading the entire scored dataset from Azure machine learning studio. The 'visualise' option only shows 100 rows and the 'save as dataset' option results in a file that does not open well with excel or text editor due to special character encoding. The user wants to see the entire test data with scored labels as a table or download it as a .csv file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36563769",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.1,
        "Challenge_reading_time":6.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.2497888889,
        "Challenge_title":"How to download the entire scored dataset from Azure machine studio?",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":5296.0,
        "Challenge_word_count":94,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1406266059940,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Link\u00f6ping, Sweden",
        "Poster_reputation_count":1677.0,
        "Poster_view_count":221.0,
        "Solution_body":"<p>Please try the Convert to CSV module: <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/faa6ba63-383c-4086-ba58-7abf26b85814\" rel=\"noreferrer\">https:\/\/msdn.microsoft.com\/library\/azure\/faa6ba63-383c-4086-ba58-7abf26b85814<\/a><\/p>\n\n<p>After you run the experiment, right click on the output of the module to download the CSV file.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.5,
        "Solution_reading_time":4.51,
        "Solution_score_count":14.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":28.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.51785,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hola a todos, perdon quiza sea muy basica mi pregunta, no se como importar un excel como Dataset. Solo puedo importar CSV, etc. Muchas gracias<\/p>",
        "Challenge_closed_time":1614974319960,
        "Challenge_comment_count":0,
        "Challenge_created_time":1614968855700,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is having difficulty importing an Excel file as a dataset in Microsoft Azure and is only able to import CSV files. They are seeking assistance on how to import an Excel file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/301247\/excel-en-microsoft-azure",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":2.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1.51785,
        "Challenge_title":"Excel en Microsoft Azure",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":28,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. Excel is not a <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.tabulardataset?view=azure-ml-py\">supported format<\/a> for Azure ML Tabular datasets. I recommend that you convert your excel file to .csv file (save as .csv) before importing to Azure ML. Hope this helps!    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":6.7,
        "Solution_reading_time":4.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":40.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426093220648,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bengaluru, India",
        "Answerer_reputation_count":1861.0,
        "Answerer_view_count":294.0,
        "Challenge_adjusted_solved_time":7.1692591667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using PartitionedDataSet to load multiple csv files from azure blob storage. I defined my data set in the datacatalog as below.<\/p>\n<pre><code>my_partitioned_data_set:\n          type: PartitionedDataSet\n          path: my\/azure\/folder\/path\n          credentials: my credentials\n          dataset: pandas.CSVDataSet\n          load_args:\n                sep: &quot;;&quot;\n                encoding: latin1\n<\/code><\/pre>\n<p>I also defined a node to combine all the partitions. But while loading each file as a CSVDataSet kedro is not considering the load_args, so I am getting the below error.<\/p>\n<pre><code>Failed while loading data from data set CSVDataSet(filepath=my\/azure\/folder\/path, load_args={}, protocol=abfs, save_args={'index': False}).\n'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte \n<\/code><\/pre>\n<p>The error shows that while loading the CSVDataSet kedro is not considering the load_args defined in the PartitionedDataSet. And passing an empty dict as a load_args parameter to CSVDataSet.\nI am following the documentation\n<code>https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/02_kedro_io.html#partitioned-dataset<\/code>\nI am not getting where I am doing mistakes.<\/p>",
        "Challenge_closed_time":1638684474152,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638659712850,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a DataSetError while loading PartitionedDataSet to load multiple CSV files from Azure Blob Storage. The load_args defined in the PartitionedDataSet are not being considered by kedro while loading each file as a CSVDataSet, resulting in an error. The user is following the documentation but is unsure where they are making a mistake.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70230262",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":15.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":6.8781394444,
        "Challenge_title":"kedro DataSetError while loading PartitionedDataSet",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":381.0,
        "Challenge_word_count":143,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1495105930728,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>Move <code>load_args<\/code> inside dataset<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>my_partitioned_data_set:\n  type: PartitionedDataSet\n  path: my\/azure\/folder\/path\n  credentials: my credentials\n  dataset:\n    type: pandas.CSVDataSet\n    load_args:\n      sep: &quot;;&quot;\n      encoding: latin1\n<\/code><\/pre>\n<ul>\n<li><p><code>load_args<\/code> mentioned outside dataset is passed into <code>find()<\/code> method of the corresponding filesystem implementation<\/p>\n<\/li>\n<li><p>To pass granular configuration to underlying dataset put it inside <code>dataset<\/code> as above.<\/p>\n<\/li>\n<\/ul>\n<p>You can check out the details in the docs<\/p>\n<p><a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/02_kedro_io.html?highlight=partitoned%20dataset#partitioned-dataset-definition\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/02_kedro_io.html?highlight=partitoned%20dataset#partitioned-dataset-definition<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1638685522183,
        "Solution_link_count":2.0,
        "Solution_readability":22.8,
        "Solution_reading_time":12.65,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":67.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":52.2182897222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello everyone!    <\/p>\n<p>I am taking the <a href=\"https:\/\/learn.microsoft.com\/en-us\/learn\/modules\/create-regression-model-azure-machine-learning-designer\/explore-data\">Create a Regression Model with Azure Machine Learning designer<\/a> course in Microsoft Learn. When I perform the steps in the Explore Data section, after selecting the &quot;Edit column&quot; button of the &quot;Select Columns in Dataset&quot; module in Designer, it will be stuck in the &quot;loading&quot; state. Therefore, I cannot proceed to the next step.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/84160-1.png?platform=QnA\" alt=\"84160-1.png\" \/>    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/84253-2.png?platform=QnA\" alt=\"84253-2.png\" \/>    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/84294-3.png?platform=QnA\" alt=\"84294-3.png\" \/>    <\/p>\n<p>Thank you very much!    <\/p>\n<p>Best regards,    <br \/>\nLing    <\/p>",
        "Challenge_closed_time":1617711172380,
        "Challenge_comment_count":7,
        "Challenge_created_time":1617523186537,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue while taking the \"Create a Regression Model with Azure Machine Learning designer\" course in Microsoft Learn. After selecting the \"Edit column\" button of the \"Select Columns in Dataset\" module in Designer, it gets stuck in the \"loading\" state, preventing the user from proceeding to the next step.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/343427\/after-selecting-the-edit-column-button-of-the-sele",
        "Challenge_link_count":4,
        "Challenge_participation_count":8,
        "Challenge_readability":13.4,
        "Challenge_reading_time":14.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":52.2182897222,
        "Challenge_title":"After selecting the \"Edit column\" button of the \"Select Columns in Dataset\" module in Designer, it will be stuck in the \"loading\" state.",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":107,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=2f4c69cd-f08d-480e-af96-29ddb1d93452\">@\u9ad8\u6977\u4fee  <\/a> This issue is now fixed in all regions and it does not require an additional parameter to be added to the URL. Please try and let us know if it works fine. <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":2.98,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":36.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":524.7811111111,
        "Challenge_answer_count":3,
        "Challenge_body":"Hello,  \nI have followed the DeepAR Chicago Traffic violations notebook example. The Model and Endpoint has been created and the forecasting is working.  \n  \nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_applying_machine_learning\/deepar_chicago_traffic_violations\/deepar_chicago_traffic_violations.ipynb  \n  \nHowevr, I haven't deleted the model nor the endpoint in order to use it externally. I have created a Python script on an EC2 that tries to load the endpoint and passes the data to it to get a prediction, and here is what I am doing:  \n  \n1. Loading the CSV exactly the way I did it on the notebook  \n2. Parsing the CSV the same way I did on the notebook for the \"predictor.predict\" command  \n3. Instead of using the \"predictor.predict\", I am using \"invoke_endpoint\" to load the endpoint and passing the data from the previous point  \n4. Instead of getting the same response I got on the notebook, I am getting the following message:  \n\"type: <class 'list'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object\"  \n  \nNot sure what the issue is, seems that it requires a byte data... I guess I cannot send the data as a list to the endpoint and I need to serialize it or to encode it? convert to to JSON? to Bytes?  \n  \nAny help will be appreciated.  \nRegards",
        "Challenge_closed_time":1626970917000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625081705000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has followed the DeepAR Chicago Traffic violations notebook example and created a model and endpoint for forecasting. However, when trying to load the endpoint and pass data to it using a Python script on an EC2, they are getting an error message that requires byte data. They are unsure how to serialize or encode the data and need help to resolve the issue.",
        "Challenge_last_edit_time":1668423337511,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUpamBayk2RT6c6KuNop0DQQ\/how-to-pass-data-to-an-endpoint",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":9.0,
        "Challenge_reading_time":16.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":524.7811111111,
        "Challenge_title":"How to pass data to an endpoint",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":613.0,
        "Challenge_word_count":205,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello,  \n  \nSo the issue here is the predictor.predict command converts the data to the format necessary for the endpoint to understand, thus you need to serialize or encode the payload by yourself. To do this you can work with something like json.dumps(payload) or for a byte array json.dumps(payload).encode().  \n  \n If you want to use the predictor class this is taken care of by the serializer option. The serializer encodes\/decodes the data for us and lets you simply call the endpoint through the predictor class. An example of this is the following code snippet:   \n  \nfrom sagemaker.serializers import IdentitySerializer  \nfrom sagemaker.deserializers import JSONDeserializer  \nserializer=IdentitySerializer(content_type=\"application\/json\")  \n  \nHope this helps!  \n  \nTo check out the various serializer options that can work for your different use cases check the following link.   \nSerializers: https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html  \n  \nEdited by: rvegira-aws on Jul 22, 2021 9:22 AM  \n  \nEdited by: rvegira-aws on Jul 22, 2021 9:24 AM",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1626971072000,
        "Solution_link_count":1.0,
        "Solution_readability":13.1,
        "Solution_reading_time":13.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":143.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1331657670247,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3932.0,
        "Answerer_view_count":274.0,
        "Challenge_adjusted_solved_time":484.7536388889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>From a segmentation mask, I am trying to retrieve what labels are being represented in the mask. <\/p>\n\n<p>This is the image I am running through a semantic segmentation model in AWS Sagemaker.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/XbMMP.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XbMMP.png\" alt=\"Motorbike and everything else background\"><\/a><\/p>\n\n<p>Code for making prediction and displaying mask.<\/p>\n\n<pre><code>from sagemaker.predictor import json_serializer, json_deserializer, RealTimePredictor\nfrom sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n\n%%time\nss_predict = sagemaker.RealTimePredictor(endpoint=ss_model.endpoint_name, \n                                     sagemaker_session=sess,\n                                    content_type = 'image\/jpeg',\n                                    accept = 'image\/png')\n\nreturn_img = ss_predict.predict(img)\n\nfrom PIL import Image\nimport numpy as np\nimport io\n\nnum_labels = 21\nmask = np.array(Image.open(io.BytesIO(return_img)))\nplt.imshow(mask, vmin=0, vmax=num_labels-1, cmap='jet')\nplt.show()\n<\/code><\/pre>\n\n<p>This image is the segmentation mask that was created and it represents the motorbike and everything else is the background.<\/p>\n\n<p>[<img src=\"https:\/\/i.stack.imgur.com\/6FbVn.png\" alt=\"Segmented mask[2]\"><\/p>\n\n<p>As you can see from the code there are 21 possible labels and 2 were used in the mask, one for the motorbike and another for the background. What I would like to figure out now is how to print which labels were actually used in this mask out of the 21 possible options?<\/p>\n\n<p>Please let me know if you need any further information and any help is much appreciated. <\/p>",
        "Challenge_closed_time":1592390011563,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590644898463,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to retrieve the labels used in a segmentation mask in AWS Sagemaker. They have successfully created a segmentation mask with 2 labels out of 21 possible options, but they need to know which labels were used in the mask. They are seeking assistance in printing the labels used in the mask.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62057838",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":21.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":484.7536388889,
        "Challenge_title":"How to retrieve the labels used in a segmentation mask in AWS Sagemaker",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":489.0,
        "Challenge_word_count":197,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1449513251820,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":693.0,
        "Poster_view_count":56.0,
        "Solution_body":"<p>Somewhere you should have a mapping from label integers to label classes, e.g.<\/p>\n\n<pre><code>label_map = {0: 'background', 1: 'motorbike', 2: 'train', ...}\n<\/code><\/pre>\n\n<p>If you are using the Pascal VOC dataset, that would be (1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle, 6=bus, 7=car , 8=cat, 9=chair, 10=cow, 11=diningtable, 12=dog, 13=horse, 14=motorbike, 15=person, 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv\/monitor) - see here: <a href=\"http:\/\/host.robots.ox.ac.uk\/pascal\/VOC\/voc2012\/segexamples\/index.html\" rel=\"nofollow noreferrer\">http:\/\/host.robots.ox.ac.uk\/pascal\/VOC\/voc2012\/segexamples\/index.html<\/a><\/p>\n\n<p>Then you can simply use that map:<\/p>\n\n<pre><code>used_classes = np.unique(mask)\nfor cls in used_classes:\n    print(\"Found class: {}\".format(label_map[cls]))\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.8,
        "Solution_reading_time":10.68,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":76.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.6135575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Currently! I'm experimenting with the azure data labelling tool in the machine learning workspace for image classification, what I found was azure shows only the unlabelled data to each user i.e if a user has already labelled an image, other users won't be shown the same image again.   <br \/>\nIs there any setting that exists, which can be enabled or disabled so that we can let more than one labeller label the same data?   <\/p>",
        "Challenge_closed_time":1625073229547,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625056620740,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is experimenting with Azure's data labelling tool for image classification and has found that each user is only shown unlabelled data, meaning that if one user has already labelled an image, other users won't be shown the same image again. The user is asking if there is a setting that can be enabled or disabled to allow multiple labelers to label the same data and reach a consensus.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/458004\/azure-machine-learning-data-labelling-is-it-possib",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.7,
        "Challenge_reading_time":6.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":4.6135575,
        "Challenge_title":"Azure machine learning data labelling- Is it possible to assign different labelers to label same data in a single project to reach a consensus?",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":98,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Thanks for reaching to us. This capability is currently in development, and expected to release soon.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":1.37,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1592301866083,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3565.0,
        "Answerer_view_count":366.0,
        "Challenge_adjusted_solved_time":163.6035563889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>According to this documentation:\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-automl-client\/azureml.train.automl.automlconfig.automlconfig?view=azure-ml-py\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-automl-client\/azureml.train.automl.automlconfig.automlconfig?view=azure-ml-py<\/a><\/p>\n<p>training_data can be either a dataframe or a dataset.<\/p>\n<p>However when I use a dataframe I get this error:<\/p>\n<pre><code>\nConfigException: ConfigException:\n    Message: Input of type 'Unknown' is not supported. Supported types: [azureml.data.tabular_dataset.TabularDataset, azureml.pipeline.core.pipeline_output_dataset.PipelineOutputTabularDataset]\n    InnerException: None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;message&quot;: &quot;Input of type 'Unknown' is not supported. Supported types: [azureml.data.tabular_dataset.TabularDataset, azureml.pipeline.core.pipeline_output_dataset.PipelineOutputTabularDataset]&quot;,\n        &quot;details_uri&quot;: &quot;https:\/\/aka.ms\/AutoMLConfig&quot;,\n        &quot;target&quot;: &quot;training_data&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;BadArgument&quot;,\n            &quot;inner_error&quot;: {\n                &quot;code&quot;: &quot;ArgumentInvalid&quot;,\n                &quot;inner_error&quot;: {\n                    &quot;code&quot;: &quot;InvalidInputDatatype&quot;\n                }\n            }\n        }\n    }\n}\n<\/code><\/pre>\n<p>My code is really simple:<\/p>\n<pre><code>\nclient = CosmosClient(HOST, MASTER_KEY)\ndatabase = client.get_database_client(database=DATABASE_ID)\ncontainer = database.get_container_client(CONTAINER_ID)\n\nitem_list = list(container.read_all_items(max_item_count=10))\ndf = pd.DataFrame(item_list)\n\nfrom azureml.core.workspace import Workspace\nws = Workspace.from_config()\n\nfrom azureml.automl.core.forecasting_parameters import ForecastingParameters\n\nforecasting_parameters = ForecastingParameters(time_column_name='EventEnqueuedUtcTime', \n                                               forecast_horizon=50,\n                                               time_series_id_column_names=[&quot;eui&quot;],\n                                               freq='H',\n                                               target_lags='auto',\n                                               target_rolling_window_size=10)\n\nfrom azureml.core.workspace import Workspace\nfrom azureml.core.experiment import Experiment\nfrom azureml.train.automl import AutoMLConfig\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nimport logging\n\namlcompute_cluster_name = &quot;computecluster&quot;\ncompute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\nexperiment_name = 'iot-forecast'\n\nexperiment = Experiment(ws, experiment_name)\n\nautoml_config = AutoMLConfig(task='forecasting',\n                             primary_metric='normalized_root_mean_squared_error',\n                             experiment_timeout_minutes=100,\n                             enable_early_stopping=True,\n                             training_data=df,\n                             compute_target = compute_target,\n                             label_column_name='TempC_DS',\n                             n_cross_validations=5,\n                             enable_ensembling=False,\n                             verbosity=logging.INFO,\n                             forecasting_parameters=forecasting_parameters)\n\nremote_run = experiment.submit(automl_config, show_output=True)\n<\/code><\/pre>\n<p>what am I missing here?<\/p>",
        "Challenge_closed_time":1614750517020,
        "Challenge_comment_count":0,
        "Challenge_created_time":1614161544217,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while using a dataframe as training data in their code for Azure AutoML. The error message states that the input type 'Unknown' is not supported and only TabularDataset and PipelineOutputTabularDataset are supported. The user has provided their code and is seeking assistance in identifying the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66348756",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":28.4,
        "Challenge_reading_time":42.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":163.6035563889,
        "Challenge_title":"Input of type 'Unknown' is not supported. Supported types: [azureml.data.tabular_dataset.TabularDataset, azureml.pipeline.core",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":315.0,
        "Challenge_word_count":169,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1302030303092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brussels, B\u00e9lgica",
        "Poster_reputation_count":30340.0,
        "Poster_view_count":2937.0,
        "Solution_body":"<p>Looks like you are trying to run the experiment remotely, AFAIK and as per the doc <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train#data-source-and-format?WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">here<\/a> :<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/CXYyE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/CXYyE.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>You could refer this article to understand creating <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-register-datasets?WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">Azure ML TabularDataset<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":21.0,
        "Solution_reading_time":9.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":132.1636936111,
        "Challenge_answer_count":7,
        "Challenge_body":"<p>I log values which have names in the form of <code>test\/temp_top-k.---1<\/code> (I want the dashes for sorting reasons). I can create graph panels with these values, but they do not show up in the column view. When I <code>Manage Columns<\/code> they are not listed in the <code>Hidden Columns<\/code>. When I search for them, it gives me no (an empty) result. Even when I select <code>Show All<\/code> they don\u2019t show up in the column view. A bug?<\/p>",
        "Challenge_closed_time":1647985099859,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647509310562,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is logging values with names in a specific format and is able to create graph panels with these values, but they do not show up in the column view. The values are not listed in the hidden columns and do not show up even when the user selects \"Show All\". The user suspects a bug.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/logged-value-available-in-graph-panel-but-not-in-columns\/2100",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":5.6,
        "Challenge_reading_time":6.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":132.1636936111,
        "Challenge_title":"Logged value available in graph panel, but not in columns",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":653.0,
        "Challenge_word_count":85,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Leslie,<\/p>\n<p>I apologize for not responding earlier. I assumed to be notified by e-mail when this thread is updated. Probably I need to check my settings, or \u201cwatch\u201d this thread.<\/p>\n<p>I log via Pytorch Lightning:<\/p>\n<pre><code class=\"lang-auto\">wandb_logger = WandbLogger(project=settings.project_name, log_model=True)\nwandb_logger.watch(model, log='gradients', log_freq=50, log_graph=True)\n<\/code><\/pre>\n<p>The actual code for the logging is this:<\/p>\n<pre><code class=\"lang-auto\">temp_accs_top_k = {f'{k:-&gt;4d}': v for k, v in zip(settings.ks, temp_accs)}\nlightning_module.log(f'{split}\/temp_top-k', temp_accs_top_k, batch_size=lightning_module.batch_size)\n<\/code><\/pre>\n<p>That looks a bit odd I suppose. The code is in a function that I call from several different <code>pl.LightningModule<\/code>s. The variable <code>lightning_module<\/code> refers to that module. The parameter <code>temp_accs_top_k<\/code> evaluates to (straight from the debugger):<\/p>\n<p><code>{'---1': 0.00019996000628452748, '---2': 0.00019996000628452748, '---3': 0.00039992001256905496, '---5': 0.0005998800043016672, '--10': 0.0005998800043016672, '--20': 0.001399720087647438, '--50': 0.004199160262942314, '-100': 0.007598480209708214, '1000': 0.08318336308002472}<\/code><\/p>\n<p>Which is wrong. But I am seeing the values in the graph panels (see attached screenshot).<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3ae06dc0b1399ce16eb917dfb94b60a5e0f77acd.png\" alt=\"Screen Shot 2022-03-22 at 21.52.37\" data-base62-sha1=\"8oQwPNwBrsRhQBp0p6SuZ6ht6NL\" width=\"412\" height=\"275\"><\/p>\n<p>I changed the code so that <code>temp_accs_top_k<\/code>now contains <code>{'test\/temp_top-k.---1': 0.2963850498199463, 'test\/temp_top-k.---2': 0.3962452709674835, 'test\/temp_top-k.---3': 0.44557619094848633, 'test\/temp_top-k.---5': 0.5052925944328308, 'test\/temp_top-k.--10': 0.5733972191810608, 'test\/temp_top-k.--20': 0.6277211904525757, 'test\/temp_top-k.--50': 0.6810465455055237, 'test\/temp_top-k.-100': 0.716596782207489, 'test\/temp_top-k.1000': 0.802676260471344}<\/code>.<\/p>\n<p>I log in a loop since Pytorch Lightning can\u2019t log a dict (I believe). I know that wandb does it, but I need the batch_size parameter (I have two dataloaders with different sizes\/lenghts and need to make sure that Pytorch Lightning does not get confused with steps\/epochs).<\/p>\n<pre><code class=\"lang-auto\">for k, v in temp_accs_top_k.items():\n    lightning_module.log(k, v, batch_size=lightning_module.batch_size)\n<\/code><\/pre>\n<p>Update: just realized that Pytorch Lightning has a <code>log_dict<\/code> function which lets me get rid of the awkward for loop.<\/p>\n<p>So the \u201cbug\u201d is more like \u201cwhy did it work in the first place (in the graph panels)?\u201d<\/p>\n<p>Hope that\u2019s not too much to digest and it is traceable.<\/p>\n<p>Best,<br>\nStephan<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.0,
        "Solution_reading_time":37.55,
        "Solution_score_count":null,
        "Solution_sentence_count":33.0,
        "Solution_word_count":281.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":5.0489333333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created a toy example in Azure.\nI have the following dataset:<\/p>\n\n<pre><code>  amounts       city code user_id\n1    2.95 Colleferro  100     999\n2    2.95    Subiaco  100     111\n3   14.95   Avellino  101     333\n4   14.95 Colleferro  101     999\n5   14.95  Benevento  101     444\n6  -14.95    Subiaco  110     111\n7  -14.95   Sgurgola  110     555\n8  -14.95       Roma  110     666\n9  -14.95 Colleferro  110     999\n<\/code><\/pre>\n\n<p>I create an AzureML experiment that simply plots the column of the amounts.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/TgRTW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TgRTW.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The code into the R script module is the following:<\/p>\n\n<pre><code>data.set &lt;- maml.mapInputPort(1) # class: data.frame  \n#-------------------\nplot(data.set$amounts);\ntitle(\"This title is a very long title. That is not a problem for R, but it becomes a problem when Azure manages it in the visualization.\")\n#-------------------\nmaml.mapOutputPort(\"data.set\");\n<\/code><\/pre>\n\n<p>Now, if you click on the right output port of the R script and then on \"Visualize\"<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/pTkSH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/pTkSH.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>you will see the Azure page where the outputs are shown.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Iv5GM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Iv5GM.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Now, the following happens:<\/p>\n\n<ol>\n<li>The plot is <em>stucked<\/em> into an estabilished space (example: the title is cut!!!)<\/li>\n<li>The image produced is a <em>low resolution<\/em> one.<\/li>\n<li>The JSON produced by Azure is \"dirty\" (making the <em>decoding<\/em> in C# difficult).<\/li>\n<\/ol>\n\n<p>It seems that this is not the best way to get the images produced by the AzureML experiment. <\/p>\n\n<p>Possible solution: I would like <\/p>\n\n<blockquote>\n  <p>to send the picture produced in my experiment to a space like the blob\n  storage. <\/p>\n<\/blockquote>\n\n<p>This would be also a great solution when I have a web-app and I have to pick the image produced by Azure and put it on my Web App page.\nDo you know if there is a way to send the image somewhere?<\/p>",
        "Challenge_closed_time":1469518311968,
        "Challenge_comment_count":0,
        "Challenge_created_time":1469435784157,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created an AzureML experiment that plots a column of data, but the resulting image is low resolution and has formatting issues. The user is looking for a solution to send the image produced in the experiment to a space like blob storage.",
        "Challenge_last_edit_time":1469500135808,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38563051",
        "Challenge_link_count":6,
        "Challenge_participation_count":2,
        "Challenge_readability":7.2,
        "Challenge_reading_time":28.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":22.9243919444,
        "Challenge_title":"Getting the images produced by AzureML experiments back",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":593.0,
        "Challenge_word_count":309,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436432728608,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Colleferro, Italy",
        "Poster_reputation_count":809.0,
        "Poster_view_count":361.0,
        "Solution_body":"<p>To saving the images into Azure Blob Storage with R, you need to do two steps, which include getting the images from the R device output of <code>Execute R Script<\/code> and uploading the images to Blob Storage.<\/p>\n\n<p>There are two ways to implement the steps above.<\/p>\n\n<ol>\n<li><p>You can publish the experiment as a webservice, then get the images with base64 encoding from the response of the webservice request and use Azure Blob Storage <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/dd179451.aspx\" rel=\"nofollow\">REST API<\/a> with R to upload the images. Please refer to the article <a href=\"https:\/\/blogs.msdn.microsoft.com\/benjguin\/2014\/10\/24\/how-to-retrieve-r-data-visualization-from-azure-machine-learning\/\" rel=\"nofollow\">How to retrieve R data visualization from Azure Machine Learning<\/a>.<\/p><\/li>\n<li><p>You can directly add a module in C# to get &amp; upload the images from the output of <code>Execute R Script<\/code>. Please refer to the article <a href=\"https:\/\/blogs.msdn.microsoft.com\/data_insights_global_practice\/2015\/12\/15\/accessing-a-visual-generated-from-r-code-in-azureml\/\" rel=\"nofollow\">Accessing a Visual Generated from R Code in AzureML<\/a>.<\/p><\/li>\n<\/ol>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.8,
        "Solution_reading_time":15.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":139.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1455667285907,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":283.0,
        "Answerer_view_count":85.0,
        "Challenge_adjusted_solved_time":97.8512702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a set of pre-processing stages in sklearn <code>Pipeline<\/code> and an estimator which is a <code>KerasClassifier<\/code> (<code>from tensorflow.keras.wrappers.scikit_learn import KerasClassifier<\/code>).<\/p>\n<p>My overall goal is to tune and log the whole sklearn pipeline in <code>mlflow<\/code> (in databricks evn). I get a confusing type error which I can't figure out how to reslove:<\/p>\n<blockquote>\n<p>TypeError: can't pickle _thread.RLock objects<\/p>\n<\/blockquote>\n<p>I have the following code (without tuning stage) which returns the above error:<\/p>\n<pre><code>conda_env = _mlflow_conda_env(\n    additional_conda_deps=None,\n    additional_pip_deps=[\n        &quot;cloudpickle=={}&quot;.format(cloudpickle.__version__),\n        &quot;scikit-learn=={}&quot;.format(sklearn.__version__),\n        &quot;numpy=={}&quot;.format(np.__version__),\n        &quot;tensorflow=={}&quot;.format(tf.__version__),\n    ],\n    additional_conda_channels=None,\n)\n\nsearch_space = {\n    &quot;estimator__dense_l1&quot;: 20,\n    &quot;estimator__dense_l2&quot;: 20,\n    &quot;estimator__learning_rate&quot;: 0.1,\n    &quot;estimator__optimizer&quot;: &quot;Adam&quot;,\n}\n\n\ndef create_model(n):\n\n    model = Sequential()\n    model.add(Dense(int(n[&quot;estimator__dense_l1&quot;]), activation=&quot;relu&quot;))\n    model.add(Dense(int(n[&quot;estimator__dense_l2&quot;]), activation=&quot;relu&quot;))\n    model.add(Dense(1, activation=&quot;sigmoid&quot;))\n    model.compile(\n        loss=&quot;binary_crossentropy&quot;,\n        optimizer=n[&quot;estimator__optimizer&quot;],\n        metrics=[&quot;accuracy&quot;],\n    )\n\n    return model\n\n\nmlflow.sklearn.autolog()\nwith mlflow.start_run(nested=True) as run:\n\n    classfier = KerasClassifier(build_fn=create_model, n=search_space)\n    # fit the pipeline\n    clf = Pipeline(steps=[(&quot;preprocessor&quot;, preprocessor), \n                          (&quot;estimator&quot;, classfier)])\n    h = clf.fit(\n        X_train,\n        y_train.values,\n        estimator__validation_split=0.2,\n        estimator__epochs=10,\n        estimator__verbose=2,\n    )\n\n    # log scores\n    acc_score = clf.score(X=X_test, y=y_test)\n    mlflow.log_metric(&quot;accuracy&quot;, acc_score)\n\n    signature = infer_signature(X_test, clf.predict(X_test))\n    # Log the model with a signature that defines the schema of the model's inputs and outputs.\n    mlflow.sklearn.log_model(\n        sk_model=clf, artifact_path=&quot;model&quot;, \n        signature=signature, \n        conda_env=conda_env\n    )\n<\/code><\/pre>\n<p>I also get this warning before the error:<\/p>\n<pre><code>\n    WARNING mlflow.sklearn.utils: Truncated the value of the key `steps`. Truncated value: `[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n                      transformer_weights=None,\n                      transformers=[('num',\n                                   Pipeline(memory=None,\n<\/code><\/pre>\n<p>note the the whole pipeline runs outside mlflow.\ncan someone help?<\/p>",
        "Challenge_closed_time":1631593268676,
        "Challenge_comment_count":0,
        "Challenge_created_time":1631241004103,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to tune and log a whole sklearn pipeline in mlflow, which includes a KerasClassifier estimator. However, they are encountering a type error \"can't pickle _thread.RLock objects\" and a warning related to truncation of the value of the key 'steps'. The user has shared their code and is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69126555",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":18.5,
        "Challenge_reading_time":36.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":97.8512702778,
        "Challenge_title":"how to log KerasClassifier model in a sklearn pipeline mlflow?",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":435.0,
        "Challenge_word_count":209,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1455667285907,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":283.0,
        "Poster_view_count":85.0,
        "Solution_body":"<p>I think I find sort of a workaround\/solution for this for now, but I think this issue needs to be addressed in MLFloow anyways.<\/p>\n<p>What I did is not the best way probably.\nI used a python package called <a href=\"https:\/\/scikeras.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">scikeras<\/a> that does this wrapping and then could log the model<\/p>\n<p>The code:<\/p>\n<pre><code>import scikeras \nimport tensorflow as tf \nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Flatten, Activation \n \nfrom scikeras.wrappers import KerasClassifier \n  \n \nclass ModelWrapper(mlflow.pyfunc.PythonModel): \n    def __init__(self, model): \n        self.model = model \n \n    def predict(self, context, model_input): \n        return self.model.predict(model_input) \n \nconda_env =  _mlflow_conda_env( \n      additional_conda_deps=None, \n      additional_pip_deps=[ \n        &quot;cloudpickle=={}&quot;.format(cloudpickle.__version__),  \n        &quot;scikit-learn=={}&quot;.format(sklearn.__version__), \n        &quot;numpy=={}&quot;.format(np.__version__), \n        &quot;tensorflow=={}&quot;.format(tf.__version__), \n        &quot;scikeras=={}&quot;.format(scikeras.__version__), \n      ], \n      additional_conda_channels=None, \n  ) \n \nparam = { \n   &quot;dense_l1&quot;: 20, \n   &quot;dense_l2&quot;: 20, \n   &quot;optimizer__learning_rate&quot;: 0.1, \n   &quot;optimizer&quot;: &quot;Adam&quot;, \n   &quot;loss&quot;:&quot;binary_crossentropy&quot;, \n} \n \n  \ndef create_model(dense_l1, dense_l2, meta): \n  \n  n_features_in_ = meta[&quot;n_features_in_&quot;] \n  X_shape_ = meta[&quot;X_shape_&quot;] \n  n_classes_ = meta[&quot;n_classes_&quot;] \n \n  model = Sequential() \n  model.add(Dense(n_features_in_, input_shape=X_shape_[1:], activation=&quot;relu&quot;)) \n  model.add(Dense(dense_l1, activation=&quot;relu&quot;)) \n  model.add(Dense(dense_l2, activation=&quot;relu&quot;)) \n  model.add(Dense(1, activation=&quot;sigmoid&quot;)) \n \n  return model   \n \nmlflow.sklearn.autolog() \nwith mlflow.start_run(run_name=&quot;sample_run&quot;): \n \n  classfier = KerasClassifier( \n    create_model, \n    loss=param[&quot;loss&quot;], \n    dense_l1=param[&quot;dense_l1&quot;], \n    dense_l2=param[&quot;dense_l2&quot;], \n    optimizer__learning_rate = param[&quot;optimizer__learning_rate&quot;], \n    optimizer= param[&quot;optimizer&quot;], \n) \n \n  # fit the pipeline \n  clf = Pipeline(steps=[('preprocessor', preprocessor), \n                      ('estimator', classfier)])   \n \n  h = clf.fit(X_train, y_train.values) \n  # log scores \n  acc_score = clf.score(X=X_test, y=y_test) \n  mlflow.log_metric(&quot;accuracy&quot;, acc_score) \n  signature = infer_signature(X_test, clf.predict(X_test)) \n  model_nn = ModelWrapper(clf,)  \n \n  mlflow.pyfunc.log_model( \n      python_model= model_nn, \n      artifact_path = &quot;model&quot;,  \n      signature = signature,  \n      conda_env = conda_env \n  ) \n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":19.3,
        "Solution_reading_time":35.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":180.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1491327759476,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":111.3944786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there any way to use a tsv instead of a csv as the input into sagemaker's autopilot ?<\/p>\n\n<p>Currently I'm inputting the data as such:<\/p>\n\n<pre><code>input_data_config = [{\n      'DataSource': {\n        'S3DataSource': {\n          'S3DataType': 'S3Prefix',\n          'S3Uri': 's3:\/\/{}\/{}\/train'.format(bucket,prefix)\n        }\n      },\n      'TargetAttributeName': 'sentiment'\n    }\n  ]\n<\/code><\/pre>\n\n<p>this seems to work file for .csv files but fails for my .tsv files.<\/p>",
        "Challenge_closed_time":1580778531436,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580377511313,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while using a TSV file as input into sagemaker's autopilot. The current method of inputting data works fine for CSV files but fails for TSV files.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59983062",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":5.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":111.3944786111,
        "Challenge_title":"TSV as input to sagemaker",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":204.0,
        "Challenge_word_count":54,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1479115407580,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Earth",
        "Poster_reputation_count":2944.0,
        "Poster_view_count":381.0,
        "Solution_body":"<p>I am a developer at AWS SageMaker. Autopilot currently only supports CSV data. While we are working on extending the support to more file formats: JSON, TSV, etc, this might be something that you can try to convert your .tsv file to .csv:<\/p>\n\n<pre><code>import csv\n\n# read tab-delimited file\nwith open('yourfile.tsv','rb') as fin:\n    cr = csv.reader(fin, delimiter='\\t')\n    filecontents = [line for line in cr]\n\n# write comma-delimited file (comma is the default delimiter)\nwith open('yourfile.csv','wb') as fou:\n    cw = csv.writer(fou, quotechar='', quoting=csv.QUOTE_NONE)\n    cw.writerows(filecontents)\n<\/code><\/pre>\n\n<p>Hope this helps.<\/p>\n\n<p>Ref: <a href=\"https:\/\/stackoverflow.com\/questions\/5590631\/how-to-convert-a-tab-separated-file-to-csv-format\">How to convert a tab separated file to CSV format?<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.7,
        "Solution_reading_time":10.34,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":94.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.1852777778,
        "Challenge_answer_count":0,
        "Challenge_body":"Hi everybody,\r\n\r\nI am trying to use AWS built-in algorithms in Sagemaker Studio Lab. For that I need an execution role and region etc. \r\nWhen I try to run my code it outputs\r\n\r\nValueError: Must setup local AWS configuration with a region supported by SageMaker.\r\n\r\nIs it even possible to link access AWS resources in Studiolab?\r\n\r\nMany thanks in advance!\r\n\r\n\r\n",
        "Challenge_closed_time":1639666969000,
        "Challenge_comment_count":4,
        "Challenge_created_time":1639662702000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue with Pytorch images where all prints in stderr are not being caught and are being ignored. The user has provided a code snippet and logs to demonstrate the issue. The stdout is being printed, but the stderr is being ignored. The issue persists even in distant mode.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/30",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":8.0,
        "Challenge_reading_time":5.16,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":113.0,
        "Challenge_repo_issue_count":218.0,
        "Challenge_repo_star_count":408.0,
        "Challenge_repo_watch_count":20.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.1852777778,
        "Challenge_title":"Can't configure profile with AWS CLI for using AWS Built-in sagemaker algorithms ",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":72,
        "Discussion_body":"Hi! The use case you are describing is exactly why we have this example notebook:\r\n- https:\/\/github.com\/aws\/studio-lab-examples\/blob\/main\/connect-to-aws\/Access_AWS_from_Studio_Lab.ipynb \r\n\r\nYour net net is:\r\n1\/ Ensure you have proper training and authorization to use your AWS access and secret keys. If you are an AWS account admin, you can find this in your IAM console. If you are not, work with your admin to determine if this pattern is appropriate for you. \r\n\r\n2\/ If you are qualified to manage your AWS keys, create a new file called `~\/.aws\/credentials`. There, copy and paste in your access and secret keys.\r\n\r\n3\/ Remove any cells you ran in a notebook to create or verify those files.\r\n\r\n4\/ Create a SageMaker execution role. The easiest way to do this is via the console - you can create a new SageMaker execution role when create a notebook instance or a Studio user profile. Once this is done, paste in the arn (Amazon Resource Name), for this execution role.\r\n\r\n5\/ Go forth and scale up on SageMaker! After that,  once you are using the SageMaker Python SDK, you should be able to use all code-based features within SageMaker, such as training, hosting, tuning, autopilot, pipelines, etc.   Hi @EmilyWebber. Might you also have an example that doesn't require AWS account information yet avoids \"ValueError: Must setup local AWS configuration with a region supported by SageMaker\" in the code below for \"role\"?\r\n\r\n```\r\n# create Hugging Face Model Class\r\nhuggingface_model = HuggingFaceModel(\r\n\ttransformers_version='4.6.1',\r\n\tpytorch_version='1.7.1',\r\n\tpy_version='py36',\r\n\tenv=hub,\r\n\trole=role, \r\n)\r\n```\r\nCode source: https:\/\/huggingface.co\/distilbert-base-uncased-finetuned-sst-2-english, where Deploy is Amazon SageMaker rather than Accelerated Inference \u2014 the latter [currently returns an error](https:\/\/discuss.huggingface.co\/t\/distilbert-accelerated-inference-error\/15027) with or without SageMaker\r\n\r\nSince the SageMaker Studio Lab website emphasizes no costs and no need to sign up for an AWS account, a team and I are exploring whether to have students try. Thank you in advance. Hi @derekschan - thanks for the comment. This is actually expected behavior right now - Studio Lab defaults to not having access to any AWS API's unless explicitly granted. To date, that is solved only by the pattern mentioned above in this issue, explicitly installing AWS key permissions to utilize them. \r\n\r\nShould that ever change in the future we will be sure to let you know! I'll mark your comment as a feature enhancement.  Thank you, @EmilyWebber.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":91.5201041667,
        "Challenge_answer_count":1,
        "Challenge_body":"I try to download image(.jpg, .png.) from S3 to Endpoint(made by Sagemaker) with s3url(s3:\/\/~~)\n\nBecause At the endpoint made by sagemaker, To send s3url is faster than to send image.\n\nI can download image at sagemaker notebook, from s3 to sagemaker local.\n\nbut I can't download image from s3 to sagemaker endpoint.\n\nThat local download code can not work.",
        "Challenge_closed_time":1661216110040,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660886637665,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to download an image from S3 to an endpoint made by Sagemaker using an s3url. While they are able to download the image from S3 to Sagemaker locally, they are unable to do so on the Sagemaker endpoint. The code used for local download does not work for the endpoint.",
        "Challenge_last_edit_time":1668577805956,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QULB64ZDsXSPuHBjqAWik3hQ\/solved-download-image-from-s3-to-endpoint-made-by-sagemaker-with-s3url-s3",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.5,
        "Challenge_reading_time":5.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":91.5201041667,
        "Challenge_title":"[Solved]download image from S3 to Endpoint(made by Sagemaker) with s3url(s3:\/\/~~)",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":287.0,
        "Challenge_word_count":68,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I solve this! I try to download image at endpoint.\nbut endpoint can not connect outside network except Lambda.\n1. I make request with s3url\n2. Download image from s3 to lambda\n3. Transmit image from lambda to endpoint",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1661216167531,
        "Solution_link_count":0.0,
        "Solution_readability":3.5,
        "Solution_reading_time":2.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":39.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1250158552416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Challenge_adjusted_solved_time":3708.3794758333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The input data in the model includes column ControlNo.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/eqULH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eqULH.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But I don't want this column being part of learning process so I'm using <code>Select Columns in Dataset<\/code> to exclude <code>ControlNo<\/code> column.<\/p>\n<p>But as a output I want those columns:<\/p>\n<pre><code>ControlNo, Score Label, Score Probability\n<\/code><\/pre>\n<p>So basically I need NOT to include column <code>ControlNo<\/code> into learning process,\nbut have it as output along with <code>Score Label<\/code> column.<\/p>\n<p>How can I do that?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/iPlpa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/iPlpa.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1533111465720,
        "Challenge_comment_count":3,
        "Challenge_created_time":1519761299607,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to exclude the \"ControlNo\" column from the learning process in Azure ML but include it as output along with \"Score Label\" column. They are using \"Select Columns in Dataset\" to exclude the \"ControlNo\" column but need guidance on how to include it as output.",
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49016896",
        "Challenge_link_count":4,
        "Challenge_participation_count":4,
        "Challenge_readability":11.1,
        "Challenge_reading_time":12.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":3708.3794758333,
        "Challenge_title":"How to bypass ID column without being used in the training model but have it as output - Azure ML",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":582.0,
        "Challenge_word_count":110,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1457596845392,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Diego, CA, United States",
        "Poster_reputation_count":4046.0,
        "Poster_view_count":825.0,
        "Solution_body":"<p>Instead of removing the ControlNo column from the dataset, you can use the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/edit-metadata\" rel=\"nofollow noreferrer\">Edit Metadata<\/a> module to clear its \"Feature\" flag - just select the column and set <strong>Fields<\/strong> to <strong>Clear feature<\/strong>. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/EUy9A.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EUy9A.png\" alt=\"Edit Metadata settings\"><\/a><\/p>\n\n<p>This will cause the Azure ML Studio algorithms to ignore it during training, and you'll be able to return it as part of your output. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.2,
        "Solution_reading_time":8.65,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":69.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1408529239847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Dublin, Ireland",
        "Answerer_reputation_count":1652.0,
        "Answerer_view_count":293.0,
        "Challenge_adjusted_solved_time":15.6380441667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I used this example that was provided by WandB. However, the web interface just shows a table instead of a figure.<\/p>\n<pre><code>data = [[i, random.random() + math.sin(i \/ 10)] for i in range(100)]\n        table = wandb.Table(data=data, columns=[&quot;step&quot;, &quot;height&quot;])\n        wandb.log({'line-plot1': wandb.plot.line(table, &quot;step&quot;, &quot;height&quot;)})\n<\/code><\/pre>\n<p>This is a screenshot from WandB's web interface:\n<a href=\"https:\/\/i.stack.imgur.com\/INmPU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/INmPU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Also, I have the same problem with other kinds of figures and charts that use a table.<\/p>",
        "Challenge_closed_time":1641809697276,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641753400317,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with wandb.plot.line as it is not displaying the figure on the web interface and instead showing a table. The problem is also occurring with other types of figures and charts that use a table.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70644326",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":9.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":15.6380441667,
        "Challenge_title":"wandb.plot.line does not work and it just shows a table",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":184.0,
        "Challenge_word_count":83,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1445719444550,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Iran, Canada",
        "Poster_reputation_count":331.0,
        "Poster_view_count":40.0,
        "Solution_body":"<blockquote>\n<p>X-Post from the wandb forum<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/community.wandb.ai\/t\/wandb-plot-confusion-matrix-just-show-a-table\/1744\" rel=\"nofollow noreferrer\">https:\/\/community.wandb.ai\/t\/wandb-plot-confusion-matrix-just-show-a-table\/1744<\/a><\/p>\n<p>If you click the section called \u201cCustom Charts\u201d above the Table, it\u2019ll show the line plot that you\u2019ve logged.<\/p>\n<p>Logging the Table also is expected behaviour, because this will allow users to interactively explore the logged data in a W&amp;B Table after logging it.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.6,
        "Solution_reading_time":7.23,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":55.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":30.43637,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Hello, I want to create a grid panel containing several linecharts but the selected runs are different.<br>\nTo elaborate on my need: I have several algorithms, evaluated across timesteps on several environments. I want one line chart per environment. To illustrate, my final requirement is to get something that looks like this:<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/f\/f2059dafd12562e424e2b66a3e2aabb2b4e1b204.png\" alt=\"Screenshot from 2023-01-19 13-12-26\" data-base62-sha1=\"yx1zXviDcru4wcBIHNfwKEhCyFK\" width=\"435\" height=\"213\"><\/p>\n<p>How would you do that?<\/p>",
        "Challenge_closed_time":1674240153196,
        "Challenge_comment_count":0,
        "Challenge_created_time":1674130582264,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to create a grid panel with multiple line charts, but the selected runs are different. The user has several algorithms evaluated across timesteps on different environments and wants one line chart per environment. The user has provided an example of the desired output.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/different-run-sets-within-a-panel-grid\/3721",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":11.5,
        "Challenge_reading_time":8.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":30.43637,
        "Challenge_title":"Different run sets within a panel grid",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":210.0,
        "Challenge_word_count":72,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/qgallouedec\">@qgallouedec<\/a>, thanks for writing in! As you\u2019d like to have a figure with independent charts inside, one option would be to use <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/custom-charts\">custom charts<\/a>. I let you <a href=\"https:\/\/vega.github.io\/vega\/examples\/barley-trellis-plot\/\" rel=\"noopener nofollow ugc\">here<\/a> and <a href=\"https:\/\/vega.github.io\/vega\/examples\/brushing-scatter-plots\/\" rel=\"noopener nofollow ugc\">here<\/a> two Vega examples that may be useful in order to build your figure. Other way I can think of for this is rendering the chart through <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log\/plots#matplotlib-and-plotly-plots\">Plotly\/Matplotlib<\/a> and then log it. Please let me know if any of these would be useful!<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":12.1,
        "Solution_reading_time":10.47,
        "Solution_score_count":null,
        "Solution_sentence_count":8.0,
        "Solution_word_count":85.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":22.3175444444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi all,    <\/p>\n<p>I've registered in Azure Machine Learning a Data Lake Gen2 datastore that point to a container with a hierarchy of folders that contain avro files and on top of it I registered a folder_uri dataset (ML v2).    <\/p>\n<p>Now I want to access to these folders from a notebook, convert them in a pandas dataframe in order to do some data exploration.    <\/p>\n<p>I search on the documentation, and I only found examples that run job and using this type of dataset as input, but I need to be able to explore it using notebook.     <\/p>\n<p>Is it possible? How can I do it?    <\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1666874705140,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666794361980,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has registered a Data Lake Gen2 datastore in Azure Machine Learning that contains avro files in a hierarchy of folders. They have also registered a folder_uri dataset (ML v2) and want to access these folders from a notebook to convert them into a pandas dataframe for data exploration. The user is seeking guidance on how to explore the dataset using a notebook instead of running a job.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1063867\/azure-machine-learning-access-uri-folder-dataset-(",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":8.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":22.3175444444,
        "Challenge_title":"Azure Machine Learning - Access uri_folder dataset (ML v2) from notebook (not job)",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":120,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=c637b1ab-bffd-0006-0000-000000000000\">@G Cocci  <\/a> Thanks for the question. Currently it's not supported to access the avro files. Here is the document for accessing the datastore using folder_uri dataset.    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/migrate-to-v2-resource-datastore\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/migrate-to-v2-resource-datastore<\/a>    <\/p>\n<p>Mapping Data Flow supports AVRO as a source type <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/data-factory\/data-flow-source#supported-sources\">https:\/\/learn.microsoft.com\/en-us\/azure\/data-factory\/data-flow-source#supported-sources<\/a><\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":20.8,
        "Solution_reading_time":9.48,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":77.7987138889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I got this error when running a job in Azure Machine Learning Studio. Any ideas about how to fix it?<\/p>\n<pre><code>Error Code: ScriptExecution.StreamAccess.Unexpected\nNative Error: error in streaming from input data sources\n\tStreamError(Unknown(&quot;unsuccessful status code 409 Conflict, body &quot;, None))\n=&gt; unsuccessful status code 409 Conflict, body \n\tUnknown(&quot;unsuccessful status code 409 Conflict, body &quot;, None)\nError Message: Got unexpected error: unsuccessful status code 409 Conflict, body . | session_id=96032e2f-c1e6-423c-8225-c1c460b3192f\n<\/code><\/pre>",
        "Challenge_closed_time":1676870036790,
        "Challenge_comment_count":0,
        "Challenge_created_time":1676589961420,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error code ScriptExecution.StreamAccess.Unexpected while running a job in Azure Machine Learning Studio, which resulted in an unsuccessful status code 409 Conflict. The user is seeking help to fix this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1181563\/error-code-scriptexecution-streamaccess-unexpected",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":8.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":77.7987138889,
        "Challenge_title":"Error Code: ScriptExecution.StreamAccess.Unexpected when running a job in AzureML Studio",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":77,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=2506b2a8-0b14-4610-bdd6-41ac619af16a\">@Maria Rivera Araya  <\/a>The error message &quot;unsuccessful status code 409 Conflict, body&quot; suggests that there is a conflict with the input data sources. This error can occur when the input data sources are being modified while the job is running.&lt;sup&gt;<a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/main\/articles\/machine-learning\/component-reference\/designer-error-codes.md\">[2]<\/a>&lt;\/sup&gt;<\/p>\n<p>You can try the following steps to resolve the issue: Wait for the input data sources to finish being modified.<\/p>\n<ol>\n<li> If the input data sources are not being modified, try restarting the job.<\/li>\n<li> If the issue persists, try using a different input data source.<\/li>\n<\/ol>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.9,
        "Solution_reading_time":10.12,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1402536093248,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":817703.0,
        "Answerer_view_count":106500.0,
        "Challenge_adjusted_solved_time":0.2592397222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have dataframe with columns <\/p>\n\n<pre><code>date    open    high    low     close   adjclose    volume\n<\/code><\/pre>\n\n<p>I want to add one more column named \"result\"(1 if close > open, 0 if close &lt; open)<\/p>\n\n<p>I do<\/p>\n\n<pre><code># Map 1-based optional input ports to variables\ndata &lt;- maml.mapInputPort(1) # class: data.frame\n\n\n\n# calculate pass\/fail\ndata$result &lt;- as.factor(sapply(data$close,function(res) \n    if (res - data$open &gt;= 0) '1' else '0'))\n\n# Select data.frame to be sent to the output Dataset port\nmaml.mapOutputPort(\"data\");\n<\/code><\/pre>\n\n<p>But I have only 1 in result. Where is the problem?<\/p>",
        "Challenge_closed_time":1554572339496,
        "Challenge_comment_count":1,
        "Challenge_created_time":1554571406233,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to add a new column named \"result\" to a dataframe using Azure ML and R scripts. The column should contain 1 if the \"close\" value is greater than the \"open\" value, and 0 if it is less. However, the user is encountering an issue where only 1 is being added to the column.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55551617",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.3,
        "Challenge_reading_time":7.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.2592397222,
        "Challenge_title":"Azure ML and r scripts",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":77.0,
        "Challenge_word_count":86,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1553239567403,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":67.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>The <code>if\/else<\/code> can return only a single TRUE\/FALSE and is not vectorized for length > 1.  It may be suitable to use <code>ifelse<\/code> (but that is also not required and would be less efficient compared to direct coersion of logical vector to binary (<code>as.integer<\/code>).   In the OP's code, the 'close' column elements are looped  (<code>sapply<\/code>) and subtracted from the whole 'open' column.  The intention might be to do elementwise subtraction.  In that case, <code>-<\/code> between the columns is much cleaner and efficient (as these operations are vectorized)<\/p>\n\n<pre><code>data$result &lt;- with(data, factor(as.integer((close - open) &gt;= 0)))\n<\/code><\/pre>\n\n<p>In the above, we get the difference between the columns ('close', 'open'), check if it is greater than or equal to 0 (returns logical vector), convert it to binary (<code>as.integer<\/code> - TRUE -> 1, FALSE -> 0) and then change it to <code>factor<\/code> type (if needed)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.9,
        "Solution_reading_time":12.1,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":137.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1558713599190,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":58.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":45.7170236111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>we are running a Spark job against our Kubernetes cluster and try to log the model to MLflow. We are running Spark 3.2.1 and MLflow 1.26.1 and we are using the following jars to communicate with s3 <code>hadoop-aws-3.2.2.jar<\/code> and <code>aws-java-sdk-bundle-1.11.375.jar<\/code> and configure our spark-submit job with the following parameters:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>  --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider \\\n  --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\\n  --conf spark.hadoop.fs.s3a.fast.upload=true \\\n<\/code><\/pre>\n<p>When we try to save our Spark model with <code>mlflow.spark.log_model()<\/code> we are getting the following exception:<\/p>\n<pre class=\"lang-java prettyprint-override\"><code>22\/06\/24 13:27:21 ERROR Instrumentation: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme &quot;s3&quot;\n    at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n    at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n    at org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:673)\n    at org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n    at org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n    at org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n    at org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n    at org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n    at org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n    at org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n    at org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n    at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n    at scala.util.Try$.apply(Try.scala:213)\n    at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n    at org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n    at java.base\/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at java.base\/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n    at java.base\/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n    at java.base\/java.lang.reflect.Method.invoke(Unknown Source)\n    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n    at py4j.Gateway.invoke(Gateway.java:282)\n    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)\n    at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n    at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n    at java.base\/java.lang.Thread.run(Unknown Source)\n<\/code><\/pre>\n<p>We tried to start our MLflow server with <code>-default-artifact-root<\/code> set to <code>s3a:\/\/...<\/code> but when we run our spark job and we call <code>mlflow.get_artifact_uri()<\/code> (which is also used to construct the upload uri in <code>mlflow.spark.log_model()<\/code>) the result starts with <code>s3<\/code> which probably cause the former mentioned exception.\nSince Hadoop dropped support for the <code>s3:\/\/<\/code> filesystem does anyone know how to log spark models to s3 using MLflow?<\/p>\n<p>Cheers<\/p>",
        "Challenge_closed_time":1656330551532,
        "Challenge_comment_count":1,
        "Challenge_created_time":1656078725497,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an exception \"No FileSystem for scheme 's3'\" when attempting to save a Spark model to MLflow using jars to communicate with s3 and configuring spark-submit job with specific parameters. The MLflow server was started with \"-default-artifact-root\" set to \"s3a:\/\/...\" but calling \"mlflow.get_artifact_uri()\" results in a uri starting with \"s3\" which may be causing the exception. The user is seeking a solution to log Spark models to s3 using MLflow.",
        "Challenge_last_edit_time":1656165970247,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72745109",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":24.1,
        "Challenge_reading_time":54.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":48,
        "Challenge_solved_time":69.9516763889,
        "Challenge_title":"No FileSystem for scheme \"s3\" exception when using spark with mlflow",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":148.0,
        "Challenge_word_count":234,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442649179160,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":773.0,
        "Poster_view_count":58.0,
        "Solution_body":"<p>Additional to the <code>spark.hadoop.fs.s3a.impl<\/code> config parameter, you can try to also set <code>spark.hadoop.fs.s3.impl<\/code> to <code>org.apache.hadoop.fs.s3a.S3AFileSystem<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.3,
        "Solution_reading_time":2.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":15.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":121.7916666667,
        "Challenge_answer_count":0,
        "Challenge_body":"Explicitly creating a CometLogger instance and passing it to Trainer using trainer(logger=my_comet_logger) raises a NotImplementedError because CometLogger does not implement the name() and version() class methods.\r\n\r\nBelow is the traceback:\r\n`\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 126, in <module>\r\n    trainer.fit(model)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 351, in fit\r\n    self.single_gpu_train(model)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/dp_mixin.py\", line 77, in single_gpu_train\r\n    self.run_pretrain_routine(model)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 471, in run_pretrain_routine\r\n    self.train()\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 60, in train\r\n    self.run_training_epoch()\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 99, in run_training_epoch\r\n    output = self.run_training_batch(batch, batch_nb)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 255, in run_training_batch\r\n    self.main_progress_bar.set_postfix(**self.training_tqdm_dict)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 309, in training_tqdm_dict\r\n    if self.logger is not None and self.logger.version is not None:\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/logging\/base.py\", line 76, in version\r\n    raise NotImplementedError(\"Sub-classes must provide a version property\")\r\n`\r\n\r\n",
        "Challenge_closed_time":1573531232000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1573092782000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the learning rate plot in Comet while trying to keep track of learning rate updates. The learning rate being plotted is not the expected one, especially when using the learning_rate_warmup_epochs option. The plotted learning rate is constant for the first few epochs and eventually decreases due to reduce_learning_rate_on_plateau. The user is unsure if this issue is related to the error message \"Failed to extract parameters from Optimizer.init()\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/470",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":22.1,
        "Challenge_reading_time":25.49,
        "Challenge_repo_contributor_count":444.0,
        "Challenge_repo_fork_count":2922.0,
        "Challenge_repo_issue_count":15116.0,
        "Challenge_repo_star_count":23576.0,
        "Challenge_repo_watch_count":234.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":121.7916666667,
        "Challenge_title":"CometLogger does not implement name() and version() class methods",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":123,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2210.2472222222,
        "Challenge_answer_count":0,
        "Challenge_body":"\r\nWhen I try to run a pipeline with target as \"local\" it gives me an error. \r\nValueError: Please specify a remote compute_target. \r\nThis should be mentioned somewhere in the end of the page under target section. \r\nAlso please specify why pipelines cannot be run on local target? People like me waste a lot of time trying this & then realize its a shortcoming in the Azure ML Python SDK. \r\nPlease update this documentation page as soon as possible.\r\n![image](https:\/\/user-images.githubusercontent.com\/17008122\/106663751-73fe0000-65a4-11eb-87f7-fcc7613dd42f.png)\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: f2c8e18c-8443-67fe-b1f9-531de3599c8f\r\n* Version Independent ID: a8c897b7-c44b-1a72-52f2-f81bbdbce753\r\n* Content: [azureml.core.runconfig.RunConfiguration class - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.runconfig.runconfiguration?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.runconfig.RunConfiguration.yml](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.runconfig.RunConfiguration.yml)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @DebFro\r\n* Microsoft Alias: **debfro**",
        "Challenge_closed_time":1620257629000,
        "Challenge_comment_count":8,
        "Challenge_created_time":1612300739000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue with Azure's TabularDataset implementation when creating or reading parquet files that were originally written by Pandas\/Python. An index, \\_\\_index\\_level_0\\_\\_, is introduced which causes errors if not handled when making changes to datasets. The issue occurs when an index is unnamed but has been modified at some point. The user has provided an example notebook to reproduce the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1316",
        "Challenge_link_count":3,
        "Challenge_participation_count":8,
        "Challenge_readability":15.0,
        "Challenge_reading_time":19.6,
        "Challenge_repo_contributor_count":58.0,
        "Challenge_repo_fork_count":2387.0,
        "Challenge_repo_issue_count":1906.0,
        "Challenge_repo_star_count":3704.0,
        "Challenge_repo_watch_count":2001.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":2210.2472222222,
        "Challenge_title":"Local execution is not supported for Azure ML pipelines. ValueError: Please specify a remote compute_target. ",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":134,
        "Discussion_body":"apologies, we understand the frustration and are working to fully support local execution through Azure Machine Learning with our v2 developer experience, which is approaching public preview While it is allowed to Run AzureML experiments in Local Target using the Python SDK, I am expecting the pipelines as well to be allowed to run on local target. If this is an exception then it should be clearly flagged out & documented by Microsoft at all relevant places. Below 2 pages should definitely contain this note\r\n1. \r\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.workspace(class)?view=azure-ml-py#azureml_core_Workspace_compute_targets\r\n(under compute_targets section)\r\n\r\n2.\r\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.runconfig.runconfiguration?view=azure-ml-py\r\n(under target section)\r\n\r\nAlso please mention the target release date of v2 developer experience unfortunately the initial preview of v2 will not address this issue, I will allow the Pipelines team to give a more clear ETA for that. but initial preview is tentatively March 2021 Thank you for quick reply. I would be happy if this feature is included in the 2.0 release. Let me know if there is any way to rate this feature on higher priority.\r\n\r\nPS: Please change your screen name,  \"lostmygithubaccount\" is very confusing & unprofessional.  Hi @lostmygithubaccount and @meghalv .  I'm currently blocked by this issue.  I'm unable to allocate a remote Compute Target and I don't find an example on how to use my local computer.\r\n\r\nIs this feature already delivered?.  Do you have an example? Hi @lostmygithubaccount, \r\n\r\nwhat is the status of local execution of Pipelines in Azure Machine Learning? Why was this issue closed without any conclusive information or workaround? \r\n\r\nThis missing feature is blocking customers that want to use local IDE and debugging. The local pipeline is still in development. We don't have an ETA for the release date. Hi, I just wanted to contribute to the conversation and say that this feature would be much appreciated. Currently, it is difficult to bounce between local debugging and cloud deployment. This is because the lack of local pipeline support requires change in data-flow as well as various azureml-core variables that are accessible during pipeline runs. ",
        "Discussion_score_count":4.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":56.8854483334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>My question centers around working with AML models that have been published as web services, as described here:    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-consume-web-service?tabs=azure-portal\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-consume-web-service?tabs=azure-portal<\/a>    <\/p>\n<p>Are there any endpoints or ways of obtaining more detailed information about a published model? For example, the documentation states that inputs to the model are passed in via a &quot;data&quot; property, and obviously, this will vary my the model:    <\/p>\n<p>{    <br \/>\n    &quot;data&quot;:  <br \/>\n        [  <br \/>\n            &lt;model-specific-data-structure&gt;  <br \/>\n        ]  <br \/>\n}    <\/p>\n<p>Is there a way to programatically find out what the model expects as input?     <\/p>\n<p>The full 'wish-list' of metadata info we'd like is listed here:     <\/p>\n<ul>\n<li> What models are available for serving    <\/li>\n<li> What is the model prediction endpoint    <\/li>\n<li> What are the required inputs and their data types    <\/li>\n<li> What are the model outputs and data types    <\/li>\n<\/ul>\n<p>Are there any endpoints or any way at getting to this information?    <\/p>",
        "Challenge_closed_time":1649630016347,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649425228733,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information about obtaining more detailed metadata about published AML models, including the available models for serving, the model prediction endpoint, required inputs and their data types, and model outputs and data types. They are asking if there are any endpoints or ways to programmatically find out what the model expects as input.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/805976\/endpoints-for-getting-metadata-about-published-mod",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":16.7,
        "Challenge_reading_time":15.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":56.8854483334,
        "Challenge_title":"Endpoints for getting metadata about published models?",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":157,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=32325e98-f4ed-442a-83f3-7d1edc203dea\">@MK RP  <\/a>    <\/p>\n<p>Thanks for reaching out to us, I will answer your question below, at the meantime, if you feel like I am not getting your point well, please point it out and correct me.    <\/p>\n<p>I think you are mentioning how to monitor published model and collect data, there are several choice depends on the data you want to collect:    <\/p>\n<ol>\n<li> Collect data from models in production - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-enable-data-collection\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-enable-data-collection<\/a>    <\/li>\n<\/ol>\n<p>The following data can be collected:    <\/p>\n<p><strong>Model input data<\/strong> from web services deployed in an AKS cluster. Voice audio, images, and video are not collected.    <br \/>\n<strong>Model predictions<\/strong> using production input data.    <\/p>\n<p>Once collection is enabled, the data you collect helps you:    <\/p>\n<p>Monitor data drifts on the production data you collect.    <br \/>\nAnalyze collected data using Power BI or Azure Databricks    <br \/>\nMake better decisions about when to retrain or optimize your model.    <br \/>\nRetrain your model with the collected data.    <\/p>\n<p>2 . Monitor and collect data from ML web service endpoints - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-enable-app-insights\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-enable-app-insights<\/a>    <\/p>\n<p>You can use Azure Application Insights to collect the following data from an endpoint:    <\/p>\n<p>Output data    <br \/>\nResponses    <br \/>\nRequest rates, response times, and failure rates    <br \/>\nDependency rates, response times, and failure rates    <br \/>\nExceptions    <\/p>\n<p>3 . More details from Data Drift - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-monitor-datasets?tabs=python\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-monitor-datasets?tabs=python<\/a>    <\/p>\n<p>With Azure Machine Learning dataset monitors (preview), you can:    <\/p>\n<p>Analyze drift in your data to understand how it changes over time.    <br \/>\nMonitor model data for differences between training and serving datasets. Start by collecting model data from deployed models.    <br \/>\nMonitor new data for differences between any baseline and target dataset.    <br \/>\nProfile features in data to track how statistical properties change over time.    <br \/>\nSet up alerts on data drift for early warnings to potential issues.    <br \/>\nCreate a new dataset version when you determine the data has drifted too much.    <\/p>\n<p>Hope above information helps, please let us know if you need further helps!    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p><em>-Please kindly accept the answer if you feel helpful, thanks a lot.<\/em>    <\/p>\n",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.1,
        "Solution_reading_time":35.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":21.0,
        "Solution_word_count":349.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11.7354919444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi, I'm using Azure ML Designer to run a pipeline. The pipeline performs a few steps and then it cancels the work throwing an error message with no further details.  <\/p>\n<p>If I re-submit the pipeline it completes the previously failed step but fails on the next step. If I re-submit the same thing happens (completes previously failed step to then fail the next step)... until it gets stuck in a specific sql transform step (see log below)  <\/p>\n<p>Here is a sequence of  run ids related with the issue:  <br \/>\nd33d23a2-2e60-4198-a6b6-f47e6e27ef4e  <br \/>\n57e04c1e-73e8-4ddf-91a8-c407cd1ad5ef  <br \/>\nad7dc826-6549-4eb3-9536-9a801d8e8c0b  <br \/>\ne6623f6f-b7b9-4f19-9501-c8c28f53ab23  <\/p>\n<p>It may be due to the way my pipeline is built but seems like JOIN, SQL Transform and SELECT Column operations tend to fail the most.  <\/p>\n<p>Would much appreciate any help on this.  <\/p>\n<pre><code>2021\/05\/11 01:57:24 Starting App Insight Logger for task:  runTaskLet\n2021\/05\/11 01:57:24 Attempt 1 of http call to http:\/\/10.0.0.6:16384\/sendlogstoartifacts\/info\n2021\/05\/11 01:57:24 Attempt 1 of http call to http:\/\/10.0.0.6:16384\/sendlogstoartifacts\/status\n[2021-05-11T01:57:24.912444] Entering context manager injector.\n[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'Dataset:context_managers.Datasets', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['urldecode_invoker.py', 'python', '-m', 'azureml.designer.modules.datatransform.invoker', 'ApplySqlTransModule', '--dataset', 'DatasetOutputConfig:Result_dataset', '--t1=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpmflqzlpr', '--t2=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpl9h5snzy', '--t3=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpuhf3n5ji', '--sqlquery=%22select+b.*%2cc.*%0d%0afrom+(%0d%0a++++select+a.customer_id%2c+a.sku_id%0d%0a++++from+(%0d%0a++++++++select+*+from+t1+cross+join+t2%0d%0a++++)+a%0d%0a++++where+exists+(%0d%0a++++++++select+t3.top_skus%0d%0a++++++++from+t3%0d%0a++++++++where+t3.sku_id+%3d+a.sku_id%0d%0a++++)%0d%0a)+b%0d%0ainner+join+(%0d%0a++++select+distinct+sku_id%2c+top_skus%0d%0a++++from+t3%0d%0a)+c%0d%0aon+c.sku_id+%3d+b.sku_id%22'])\nScript type = None\n[2021-05-11T01:57:26.142183] Entering Run History Context Manager.\n[2021-05-11T01:57:26.734197] Current directory: \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/mounts\/workspaceblobstore\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\n[2021-05-11T01:57:26.734493] Preparing to call script [urldecode_invoker.py] with arguments:['python', '-m', 'azureml.designer.modules.datatransform.invoker', 'ApplySqlTransModule', '--dataset', '$Result_dataset', '--t1=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpmflqzlpr', '--t2=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpl9h5snzy', '--t3=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpuhf3n5ji', '--sqlquery=%22select+b.*%2cc.*%0d%0afrom+(%0d%0a++++select+a.customer_id%2c+a.sku_id%0d%0a++++from+(%0d%0a++++++++select+*+from+t1+cross+join+t2%0d%0a++++)+a%0d%0a++++where+exists+(%0d%0a++++++++select+t3.top_skus%0d%0a++++++++from+t3%0d%0a++++++++where+t3.sku_id+%3d+a.sku_id%0d%0a++++)%0d%0a)+b%0d%0ainner+join+(%0d%0a++++select+distinct+sku_id%2c+top_skus%0d%0a++++from+t3%0d%0a)+c%0d%0aon+c.sku_id+%3d+b.sku_id%22']\n[2021-05-11T01:57:26.734551] After variable expansion, calling script [urldecode_invoker.py] with arguments:['python', '-m', 'azureml.designer.modules.datatransform.invoker', 'ApplySqlTransModule', '--dataset', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpnbybe4mu', '--t1=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpmflqzlpr', '--t2=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpl9h5snzy', '--t3=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpuhf3n5ji', '--sqlquery=%22select+b.*%2cc.*%0d%0afrom+(%0d%0a++++select+a.customer_id%2c+a.sku_id%0d%0a++++from+(%0d%0a++++++++select+*+from+t1+cross+join+t2%0d%0a++++)+a%0d%0a++++where+exists+(%0d%0a++++++++select+t3.top_skus%0d%0a++++++++from+t3%0d%0a++++++++where+t3.sku_id+%3d+a.sku_id%0d%0a++++)%0d%0a)+b%0d%0ainner+join+(%0d%0a++++select+distinct+sku_id%2c+top_skus%0d%0a++++from+t3%0d%0a)+c%0d%0aon+c.sku_id+%3d+b.sku_id%22']\n\nSession_id = 4b5b4c29-cfda-4ab6-a715-47fee287c468\nInvoking module by urldecode_invoker 0.0.8.\n\nModule type: custom module.\n\nUsing runpy to invoke module 'azureml.designer.modules.datatransform.invoker'.\n\n\/azureml-envs\/azureml_7c975cabc8bb1dc19c3de94457d707fd\/lib\/python3.6\/site-packages\/azureml\/designer\/modules\/datatransform\/tools\/dataframe_utils.py:2: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n  from pandas.util.testing import assert_frame_equal\n2021-05-11 01:57:27,324 [             invoker] [    INFO] .[main] Start custom modules\n2021-05-11 01:57:27,337 [             invoker] [    INFO] .[main] Module version: 0.0.74\n2021-05-11 01:57:27,344 [             invoker] [    INFO] .[main] args: azureml.designer.modules.datatransform.invoker, ApplySqlTransModule, --dataset, \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpnbybe4mu, --t1=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpmflqzlpr, --t2=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpl9h5snzy, --t3=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpuhf3n5ji, --sqlquery=select b.*,c.*\nfrom (\n    select a.customer_id, a.sku_id\n    from (\n        select * from t1 cross join t2\n    ) a\n    where exists (\n        select t3.top_skus\n        from t3\n        where t3.sku_id = a.sku_id\n    )\n) b\ninner join (\n    select distinct sku_id, top_skus\n    from t3\n) c\non c.sku_id = b.sku_id\n2021-05-11 01:57:27,352 [             invoker] [    INFO] .[main] &quot;transform_module_class_name&quot;: ApplySqlTransModule\n2021-05-11 01:57:27,444 [         module_base] [    INFO] ...[get_arg_parser] Construct arg parser\n2021-05-11 01:57:27,460 [         module_base] [    INFO] ...[get_arg_parser] arg: t1\n2021-05-11 01:57:27,468 [         module_base] [    INFO] ...[get_arg_parser] arg: t2\n2021-05-11 01:57:27,476 [         module_base] [    INFO] ...[get_arg_parser] arg: t3\n2021-05-11 01:57:27,484 [         module_base] [    INFO] ...[get_arg_parser] arg: dataset\n2021-05-11 01:57:27,492 [         module_base] [    INFO] ...[get_arg_parser] arg: sqlquery\n2021-05-11 01:57:27,500 [         module_base] [    INFO] ..[parse_and_insert_args] invoker args:\n module_classname = ApplySqlTransModule\n t1 = \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpmflqzlpr\n t2 = \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpl9h5snzy\n t3 = \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpuhf3n5ji\n dataset = \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpnbybe4mu\n sqlquery = select b.*,c.*\nfrom (\n    select a.customer_id, a.sku_id\n    from (\n        select * from t1 cross join t2\n    ) a\n    where exists (\n        select t3.top_skus\n        from t3\n        where t3.sku_id = a.sku_id\n    )\n) b\ninner join (\n    select distinct sku_id, top_skus\n    from t3\n) c\non c.sku_id = b.sku_id\n\n2021-05-11 01:57:27,508 [             invoker] [    INFO] .[main] start to run custom module: ApplySqlTransModule\n2021-05-11 01:57:27,516 [apply_sql_trans_module] [    INFO] ...[run] Construct SQLite Server\n2021-05-11 01:57:27,530 [    module_parameter] [    INFO] ......[data] Read data from \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpmflqzlpr\n2021-05-11 01:57:29,215 [apply_sql_trans_module] [    INFO] ....[_transform_df_to_sql] Insert t1 with only column names\n2021-05-11 01:57:29,227 [    module_parameter] [    INFO] ......[data] Read data from \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpl9h5snzy\n2021\/05\/11 01:57:29 Not exporting to RunHistory as the exporter is either stopped or there is no data.\nStopped: false\nOriginalData: 1\nFilteredData: 0.\n2021-05-11 01:57:30,093 [apply_sql_trans_module] [    INFO] ....[_transform_df_to_sql] Insert t2 with only column names\n2021-05-11 01:57:30,106 [    module_parameter] [    INFO] ......[data] Read data from \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpuhf3n5ji\n2021-05-11 01:57:30,876 [apply_sql_trans_module] [    INFO] ....[_transform_df_to_sql] Insert t3 with only column names\n2021-05-11 01:57:30,888 [apply_sql_trans_module] [    INFO] ...[run] Read SQL script query\n2021-05-11 01:57:30,895 [apply_sql_trans_module] [    INFO] ...[run] Validate SQL script query\n2021-05-11 01:57:30,912 [apply_sql_trans_module] [    INFO] ...[run] Insert data to SQLite Server\n2021-05-11 01:57:30,919 [apply_sql_trans_module] [    INFO] ....[_transform_df_to_sql] Insert t1\n2021-05-11 01:57:30,930 [apply_sql_trans_module] [    INFO] ....[_transform_df_to_sql] Insert t2\n2021-05-11 01:57:30,970 [apply_sql_trans_module] [    INFO] ....[_transform_df_to_sql] Insert t3\n2021-05-11 01:57:31,053 [apply_sql_trans_module] [    INFO] ...[run] Generate SQL query result from SQLite Server\n<\/code><\/pre>",
        "Challenge_closed_time":1620739797128,
        "Challenge_comment_count":2,
        "Challenge_created_time":1620697549357,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering issues with their Azure ML pipeline, specifically with the JOIN, SQL Transform, and SELECT Column operations. The pipeline cancels work and throws an error message with no further details. Re-submitting the pipeline completes the previously failed step but fails on the next step until it gets stuck in a specific SQL Transform step. The user has provided a sequence of run IDs related to the issue and is seeking help to resolve the problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/390003\/azure-ml-pipeline-fails-at-sql-transform-task",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":17.3,
        "Challenge_reading_time":130.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":69,
        "Challenge_solved_time":11.7354919444,
        "Challenge_title":"azure ml pipeline fails at sql transform task",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":609,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Found the problem.   <\/p>\n<p>There was a task failing but due to the size of the canvas I wasn't able to spot it at first (working late hours didn't help also).   <\/p>\n<p>However it certainly didn't help the fact that the error message didn't provide any info regarding which task failed, so maybe the AML team would like to add more descriptive messages in cases like this one.  <\/p>\n<p>thanks<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":4.8,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":70.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":6083.8681666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I wonder if it is possible to design ML experiments without using the drag&amp;drop functionality (which is very nice btw)? I want to use Python code in the notebook (within Azure ML studio) to access the algorithms (e.g., matchbox recommender, regression models, etc) in the studio and design experiments? Is this possible?<\/p>\n\n<p>I appreciate any information and suggestion!<\/p>",
        "Challenge_closed_time":1479709395027,
        "Challenge_comment_count":0,
        "Challenge_created_time":1479644536303,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information on whether it is possible to create ML experiments using Python code in the notebook within Azure ML studio without using the drag and drop functionality. They want to access algorithms such as matchbox recommender and regression models to design experiments.",
        "Challenge_last_edit_time":1479668976323,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40703961",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":5.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":18.0163122222,
        "Challenge_title":"Creating azure ml experiments merely using python notebook within azure ml studio",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":238.0,
        "Challenge_word_count":69,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1353596362916,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"USA",
        "Poster_reputation_count":8007.0,
        "Poster_view_count":792.0,
        "Solution_body":"<p>The algorithms used as modules in Azure ML Studio are not currently able to be used directly as code for Python programming.<\/p>\n\n<p>That being said: you can attempt to publish the outputs of the out-of-the-box algorithms as web services, which can be consumed by Python code in the Azure ML Studio notebooks. You can also create your own algorithms and use them as custom Python or R modules.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1501570901723,
        "Solution_link_count":0.0,
        "Solution_readability":9.8,
        "Solution_reading_time":4.88,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":68.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1361240380856,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3349.0,
        "Answerer_view_count":465.0,
        "Challenge_adjusted_solved_time":46.4306352778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In data exploration phrase, I need some visualization to check the relationship between data columns. I have tried Azure ML built-in visualization and felt it's limited. R ggplot allows me to choose the chart I want and do some customization.<\/p>\n\n<p>I am very new in using Azure ML. When I am trying Azure ML R script and type:<\/p>\n\n<pre><code>library(\"ggplot2\")\n\np &lt;- ggplot(train, aes(x= Item_Visibility, y = Item_Outlet_Sales)) +\ngeom_point(size = 2.5, color = \"navy\") + xlab(\"Item Visibility\") + ylab(\"Item Outlet Sales\")\n<\/code><\/pre>\n\n<p>Cannot find where is my visualization... It is not in visualization result, neither in log ourput\nI have also tried Azure ML built-in plot(), it cannot find the column in my dataset... <\/p>\n\n<p>So, is there anyway, when I am using ggplot in Azure ML R Script, I can find the visualization results?<\/p>",
        "Challenge_closed_time":1461446546867,
        "Challenge_comment_count":2,
        "Challenge_created_time":1461279396580,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in using ggplot through Azure ML R Script for data visualization as they are unable to find the visualization results in the output or log. They have also tried using Azure ML built-in plot() but it cannot find the column in their dataset. The user is seeking a solution to find the visualization results when using ggplot in Azure ML R Script.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36781843",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.0,
        "Challenge_reading_time":11.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":46.4306352778,
        "Challenge_title":"how to use ggplot through Azure ML R Script",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":425.0,
        "Challenge_word_count":137,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361240380856,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3349.0,
        "Poster_view_count":465.0,
        "Solution_body":"<p>Finally I found the visualization. Just put the code in AzureML Studio Execute R Script module will be fine.\nAfter the code has been executed successfully, right click Execute R Script module, and choose R device, the visualization is there<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/1aZ0y.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1aZ0y.png\" alt=\"ggplot through AzureML R Script\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.6,
        "Solution_reading_time":5.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":50.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1629385138956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":395.0,
        "Answerer_view_count":38.0,
        "Challenge_adjusted_solved_time":26.6152038889,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I would like to know what is the OPTIMAL way to store the result of a Google BigQuery table query, to Google Cloud storage. My code, which is currently being run in some Jupyter Notebook (in Vertex AI Workbench, same project than both the BigQuery data source, as well as the Cloud Storage destination), looks as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># CELL 1 OF 2\n\nfrom google.cloud import bigquery\nbqclient = bigquery.Client()\n\n# The query string can vary:\nquery_string = &quot;&quot;&quot;\n        SELECT *  \n        FROM `my_project-name.my_db.my_table` \n        LIMIT 2000000\n        &quot;&quot;&quot;\n\ndataframe = (\n    bqclient.query(query_string)\n    .result()\n    .to_dataframe(\n        create_bqstorage_client=True,\n    )\n)\nprint(&quot;Dataframe shape: &quot;, dataframe.shape)\n\n# CELL 2 OF 2:\n\nimport pandas as pd\ndataframe.to_csv('gs:\/\/my_bucket\/test_file.csv', index=False)\n<\/code><\/pre>\n<p>This code takes around 7.5 minutes to successfully complete.<\/p>\n<p><strong>Is there a more OPTIMAL way to achive what was done above?<\/strong> (It would mean <em>faster<\/em>, but maybe something else could be improved).<\/p>\n<p>Some additional notes:<\/p>\n<ol>\n<li>I want to run it &quot;via a Jupyter Notebook&quot; (in Vertex AI Workbench), because sometimes some data preprocessing, or special filtering must be done, which cannot be easily accomplished via SQL queries.<\/li>\n<li>For the first part of the code, I have discarded <a href=\"https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_gbq.html\" rel=\"nofollow noreferrer\">pandas.read_gbq<\/a>, as it was giving me some weird EOF errors, when (experimentally) &quot;storing as .CSV and reading back&quot;.<\/li>\n<li>Intuitively, I would focus the optimization efforts in the second half of the code (<code>CELL 2 OF 2<\/code>), as the first one was borrowed from <a href=\"https:\/\/cloud.google.com\/bigquery\/docs\/bigquery-storage-python-pandas\" rel=\"nofollow noreferrer\">the official Google documentation<\/a>. I have tried <a href=\"https:\/\/stackoverflow.com\/a\/57404119\/16706763\">this<\/a> but it does not work, however in the same thread <a href=\"https:\/\/stackoverflow.com\/a\/60644694\/16706763\">this<\/a> option worked OK.<\/li>\n<li>It is likley that this code will be included in some Docker image afterwards, so &quot;as little libraries as possible&quot; must be used.<\/li>\n<\/ol>\n<p>Thank you.<\/p>",
        "Challenge_closed_time":1651612810316,
        "Challenge_comment_count":2,
        "Challenge_created_time":1651600671333,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to know the optimal way to store the result of a Google BigQuery table query to Google Cloud storage. The user's code is currently running in a Jupyter Notebook in Vertex AI Workbench, and takes around 7.5 minutes to complete. The user wants to optimize the code to make it faster and is looking for suggestions. The user has discarded pandas.read_gbq and is focusing on optimizing the second half of the code. The user also wants to use as few libraries as possible and plans to include the code in a Docker image.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72103557",
        "Challenge_link_count":4,
        "Challenge_participation_count":5,
        "Challenge_readability":9.2,
        "Challenge_reading_time":30.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":3.3719397222,
        "Challenge_title":"Save the result of a query in a BigQuery Table, in Cloud Storage",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1409.0,
        "Challenge_word_count":292,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1629385138956,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":395.0,
        "Poster_view_count":38.0,
        "Solution_body":"<p>After some experiments, I think I have got to a solution for my original post. First, the updated code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd  # Just one library is imported this time\n\n# This SQL query can vary, modify it to match your needs\nquery_string = &quot;&quot;&quot;\nSELECT *\nFROM `my_project.my_db.my_table`\nLIMIT 2000000\n&quot;&quot;&quot;\n\n# One liner to query BigQuery data.\ndownloaded_dataframe = pd.read_gbq(query_string, dialect='standard', use_bqstorage_api=True)\n\n# Data processing (OPTIONAL, modify it to match your needs)\n# I won't do anything this time, just upload the previously queried data\n\n# Data store in GCS\ndownloaded_dataframe.to_csv('gs:\/\/my_bucket\/uploaded_data.csv', index=False)\n<\/code><\/pre>\n<p>Some final notes:<\/p>\n<ol>\n<li>I have not done an &quot;in-depth research&quot; about the processing speed VS the number of rows existing in a BigQuery table, however I saw that the processing time with the updated code and the original query, now takes ~6 minutes; that's enough for the time being. <em>This answer might have some room for further improvements<\/em> therefore, but it's better than the original situation.<\/li>\n<li>The EOF error I mentioned in  my original post was: <code>ParserError: Error tokenizing data. C error: EOF inside string starting at row 70198<\/code>. In the end I got to realize that it did not have anything to do with <a href=\"https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_gbq.html\" rel=\"nofollow noreferrer\">pandas_gbq<\/a> function, but with &quot;how I was saving the data&quot;. See, <em>I was 'experimentally' storing the .csv file in the Vertex AI Workbench local storage, then downloading it to my local device, and when trying to open that data from my local device, I kept stumbling upon that error, however not getting the same when downloading the .csv data from Cloud Storage<\/em> ... Why? Well, it happens that if you download the .csv data &quot;very quickly&quot; after &quot;it gets generated&quot; (i.e., after few seconds), from Vertex AI Workbench local storage, the data is simply still incomplete, but it does not give any error or warning message: it will simply &quot;let you start with the download&quot;. For this reason, I think it is safer to export your data to Cloud Storage, and then download safely from there. This behaviour is more noticeable on large files (i.e. my own generated file, which had ~3.1GB in size).<\/li>\n<\/ol>\n<p>Hope this helps.<\/p>\n<p>Thank you.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1651696486067,
        "Solution_link_count":1.0,
        "Solution_readability":9.6,
        "Solution_reading_time":31.41,
        "Solution_score_count":2.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":357.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1546969667040,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":1689.0,
        "Answerer_view_count":170.0,
        "Challenge_adjusted_solved_time":0.7390766667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am new to AWS environment and trying to solve how the data flow works. After successfully uploading CSV files from S3 to SageMaker notebook instance, I am stuck on doing the reverse. <\/p>\n\n<p>I have a dataframe and want to upload that to S3 Bucket as CSV or JSON. The code that I have is below:<\/p>\n\n<pre><code>bucket='bucketname'\ndata_key = 'test.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\ndf.to_csv(data_location)\n<\/code><\/pre>\n\n<p>I assumed since I successfully used <code>pd.read_csv()<\/code> while loading, using <code>df.to_csv()<\/code> would also work but it didn't. Probably it is generating error because this way I cannot pick the privacy options while uploading a file manually to S3. Is there a way to upload the data to S3 from SageMaker?<\/p>",
        "Challenge_closed_time":1561684844023,
        "Challenge_comment_count":2,
        "Challenge_created_time":1561682183347,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is trying to upload a dataframe to an AWS S3 bucket from SageMaker, but is encountering an error when using the <code>df.to_csv()<\/code> method. They suspect that this is because they cannot pick the privacy options while uploading a file manually to S3. The user is seeking a solution to upload the data to S3 from SageMaker.",
        "Challenge_last_edit_time":1561688539710,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56799763",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":7.3,
        "Challenge_reading_time":10.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":9.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.7390766667,
        "Challenge_title":"Uploading a Dataframe to AWS S3 Bucket from SageMaker",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":16471.0,
        "Challenge_word_count":123,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1541972092476,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Santa Clara, CA, USA",
        "Poster_reputation_count":731.0,
        "Poster_view_count":51.0,
        "Solution_body":"<p>One way to solve this would be to save the CSV to the local storage on the SageMaker notebook instance, and then use the S3 API's via <code>boto3<\/code> to upload the file as an s3 object. \n<a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file\" rel=\"noreferrer\">S3 docs for <code>upload_file()<\/code> available here.<\/a><\/p>\n\n<p>Note, you'll need to ensure that your SageMaker hosted notebook instance has proper <code>ReadWrite<\/code> permissions in its IAM role, otherwise you'll receive a permissions error.<\/p>\n\n<pre><code># code you already have, saving the file locally to whatever directory you wish\nfile_name = \"mydata.csv\" \ndf.to_csv(file_name)\n<\/code><\/pre>\n\n<pre><code># instantiate S3 client and upload to s3\nimport boto3\n\ns3 = boto3.resource('s3')\ns3.meta.client.upload_file(file_name, 'YOUR_S3_BUCKET_NAME', 'DESIRED_S3_OBJECT_NAME')\n<\/code><\/pre>\n\n<p>Alternatively, <code>upload_fileobj()<\/code> may help for parallelizing as a multi-part upload. <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1561686321643,
        "Solution_link_count":1.0,
        "Solution_readability":15.4,
        "Solution_reading_time":13.32,
        "Solution_score_count":10.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":114.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":35.6138888889,
        "Challenge_answer_count":0,
        "Challenge_body":"This is the error I get  \"plot_model() got an unexpected keyword argument 'system'\"\r\n\r\n",
        "Challenge_closed_time":1650470329000,
        "Challenge_comment_count":5,
        "Challenge_created_time":1650342119000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a bug while trying to store mlflow artifacts in Azure blob storage for new experiments in pycaret. The issue occurs when an experiment name is given to the pycaret setup function, causing the artifacts to be stored in the local directory instead of the Azure blob.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2439",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":6.8,
        "Challenge_reading_time":1.48,
        "Challenge_repo_contributor_count":105.0,
        "Challenge_repo_fork_count":1603.0,
        "Challenge_repo_issue_count":2975.0,
        "Challenge_repo_star_count":7363.0,
        "Challenge_repo_watch_count":128.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":35.6138888889,
        "Challenge_title":"plots are not saving through MLFLOW",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":18,
        "Discussion_body":"@akashg116414 We can't help you with this. Can you please describe a little bit more, show the code, show the complete error, etc. this error happens when i install pycaret[full]==2.3.9 and pycaret-ts-alpha both and tried basic classification example\r\nexample:\r\n\r\n```python\r\nfrom pycaret.datasets import get_data\r\ndata = get_data('juice')\r\nfrom pycaret.classification import *\r\nclf1 = setup(data, target = 'Purchase', session_id=123, log_experiment=True, experiment_name='juice1',log_plots=True)\r\n``` > this error happens when i install pycaret[full]==2.3.9 and pycaret-ts-alpha both and tried basic classification example\r\n\r\nAre you installing both of them in the same environment? That should not be done as there may be conflicts at this time (until we officially release the 3.0.0 pycaret version which will have time-series integrated with the main pycaret package).\r\n\r\nTry installing one of them in a fresh (clean) environment and see if it works. @akashg116414 By the way, I am not able to recreate the issue with the information (code) you have provided. It works fine for me (see attached notebook below).\r\n\r\nhttps:\/\/gist.github.com\/ngupta23\/f7f33a5361928cac2f36f855f7398c88\r\n\r\nPlease provide a completely reproducible example that shows the steps to get to the error you are getting. Since we are unable to recreate this and information seems to be missing, we will close this for now. Feel free to create a new issue. We add a new Bug template that will guide you through the process of submitting a reproducible example. \r\n\r\nThanks!",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1298484007147,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY, United States",
        "Answerer_reputation_count":9271.0,
        "Answerer_view_count":1819.0,
        "Challenge_adjusted_solved_time":6.4954608333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Cannot read AWS open data datasets into Sagemaker. Error is<\/p>\n\n<pre><code>download failed: s3:\/\/fast-ai-imageclas\/cifar100.tgz to ..\/..\/..\/tmp\/fastai-images\/cifar100.tgz An error occurred (AccessDenied) when calling the GetObject operation: Access Denied\n<\/code><\/pre>\n\n<p>code\n<a href=\"https:\/\/i.stack.imgur.com\/2b73H.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2b73H.png\" alt=\"sagemaker notebook s3 download access denied\"><\/a><\/p>\n\n<p>The user has the s3:getObjects * permission<\/p>\n\n<p>The user's permissions are the full s3 read policy and the full Sagemaker policies. The policies are<\/p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:Get*\",\n                \"s3:List*\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"application-autoscaling:DeleteScalingPolicy\",\n                \"application-autoscaling:DeleteScheduledAction\",\n                \"application-autoscaling:DeregisterScalableTarget\",\n                \"application-autoscaling:DescribeScalableTargets\",\n                \"application-autoscaling:DescribeScalingActivities\",\n                \"application-autoscaling:DescribeScalingPolicies\",\n                \"application-autoscaling:DescribeScheduledActions\",\n                \"application-autoscaling:PutScalingPolicy\",\n                \"application-autoscaling:PutScheduledAction\",\n                \"application-autoscaling:RegisterScalableTarget\",\n                \"aws-marketplace:ViewSubscriptions\",\n                \"cloudwatch:DeleteAlarms\",\n                \"cloudwatch:DescribeAlarms\",\n                \"cloudwatch:GetMetricData\",\n                \"cloudwatch:GetMetricStatistics\",\n                \"cloudwatch:ListMetrics\",\n                \"cloudwatch:PutMetricAlarm\",\n                \"cloudwatch:PutMetricData\",\n                \"codecommit:BatchGetRepositories\",\n                \"codecommit:CreateRepository\",\n                \"codecommit:GetRepository\",\n                \"codecommit:ListBranches\",\n                \"codecommit:ListRepositories\",\n                \"cognito-idp:AdminAddUserToGroup\",\n                \"cognito-idp:AdminCreateUser\",\n                \"cognito-idp:AdminDeleteUser\",\n                \"cognito-idp:AdminDisableUser\",\n                \"cognito-idp:AdminEnableUser\",\n                \"cognito-idp:AdminRemoveUserFromGroup\",\n                \"cognito-idp:CreateGroup\",\n                \"cognito-idp:CreateUserPool\",\n                \"cognito-idp:CreateUserPoolClient\",\n                \"cognito-idp:CreateUserPoolDomain\",\n                \"cognito-idp:DescribeUserPool\",\n                \"cognito-idp:DescribeUserPoolClient\",\n                \"cognito-idp:ListGroups\",\n                \"cognito-idp:ListIdentityProviders\",\n                \"cognito-idp:ListUserPoolClients\",\n                \"cognito-idp:ListUserPools\",\n                \"cognito-idp:ListUsers\",\n                \"cognito-idp:ListUsersInGroup\",\n                \"cognito-idp:UpdateUserPool\",\n                \"cognito-idp:UpdateUserPoolClient\",\n                \"ec2:CreateNetworkInterface\",\n                \"ec2:CreateNetworkInterfacePermission\",\n                \"ec2:CreateVpcEndpoint\",\n                \"ec2:DeleteNetworkInterface\",\n                \"ec2:DeleteNetworkInterfacePermission\",\n                \"ec2:DescribeDhcpOptions\",\n                \"ec2:DescribeNetworkInterfaces\",\n                \"ec2:DescribeRouteTables\",\n                \"ec2:DescribeSecurityGroups\",\n                \"ec2:DescribeSubnets\",\n                \"ec2:DescribeVpcEndpoints\",\n                \"ec2:DescribeVpcs\",\n                \"ecr:BatchCheckLayerAvailability\",\n                \"ecr:BatchGetImage\",\n                \"ecr:CreateRepository\",\n                \"ecr:GetAuthorizationToken\",\n                \"ecr:GetDownloadUrlForLayer\",\n                \"ecr:Describe*\",\n                \"elastic-inference:Connect\",\n                \"glue:CreateJob\",\n                \"glue:DeleteJob\",\n                \"glue:GetJob\",\n                \"glue:GetJobRun\",\n                \"glue:GetJobRuns\",\n                \"glue:GetJobs\",\n                \"glue:ResetJobBookmark\",\n                \"glue:StartJobRun\",\n                \"glue:UpdateJob\",\n                \"groundtruthlabeling:*\",\n                \"iam:ListRoles\",\n                \"kms:DescribeKey\",\n                \"kms:ListAliases\",\n                \"lambda:ListFunctions\",\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:DescribeLogStreams\",\n                \"logs:GetLogEvents\",\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ecr:SetRepositoryPolicy\",\n                \"ecr:CompleteLayerUpload\",\n                \"ecr:BatchDeleteImage\",\n                \"ecr:UploadLayerPart\",\n                \"ecr:DeleteRepositoryPolicy\",\n                \"ecr:InitiateLayerUpload\",\n                \"ecr:DeleteRepository\",\n                \"ecr:PutImage\"\n            ],\n            \"Resource\": \"arn:aws:ecr:*:*:repository\/*sagemaker*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"codecommit:GitPull\",\n                \"codecommit:GitPush\"\n            ],\n            \"Resource\": [\n                \"arn:aws:codecommit:*:*:*sagemaker*\",\n                \"arn:aws:codecommit:*:*:*SageMaker*\",\n                \"arn:aws:codecommit:*:*:*Sagemaker*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:CreateSecret\",\n                \"secretsmanager:DescribeSecret\",\n                \"secretsmanager:ListSecrets\",\n                \"secretsmanager:TagResource\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:GetSecretValue\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"secretsmanager:ResourceTag\/SageMaker\": \"true\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"robomaker:CreateSimulationApplication\",\n                \"robomaker:DescribeSimulationApplication\",\n                \"robomaker:DeleteSimulationApplication\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"robomaker:CreateSimulationJob\",\n                \"robomaker:DescribeSimulationJob\",\n                \"robomaker:CancelSimulationJob\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::*SageMaker*\",\n                \"arn:aws:s3:::*Sagemaker*\",\n                \"arn:aws:s3:::*sagemaker*\",\n                \"arn:aws:s3:::*aws-glue*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:CreateBucket\",\n                \"s3:GetBucketLocation\",\n                \"s3:ListBucket\",\n                \"s3:ListAllMyBuckets\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEqualsIgnoreCase\": {\n                    \"s3:ExistingObjectTag\/SageMaker\": \"true\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"lambda:InvokeFunction\"\n            ],\n            \"Resource\": [\n                \"arn:aws:lambda:*:*:function:*SageMaker*\",\n                \"arn:aws:lambda:*:*:function:*sagemaker*\",\n                \"arn:aws:lambda:*:*:function:*Sagemaker*\",\n                \"arn:aws:lambda:*:*:function:*LabelingFunction*\"\n            ]\n        },\n        {\n            \"Action\": \"iam:CreateServiceLinkedRole\",\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:iam::*:role\/aws-service-role\/sagemaker.application-autoscaling.amazonaws.com\/AWSServiceRoleForApplicationAutoScaling_SageMakerEndpoint\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"iam:AWSServiceName\": \"sagemaker.application-autoscaling.amazonaws.com\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"iam:CreateServiceLinkedRole\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"iam:AWSServiceName\": \"robomaker.amazonaws.com\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:PassRole\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"iam:PassedToService\": [\n                        \"sagemaker.amazonaws.com\",\n                        \"glue.amazonaws.com\",\n                        \"robomaker.amazonaws.com\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>The Sagemaker instance is in us-east-1 same as the dataset.<\/p>\n\n<p>The dataset is <a href=\"https:\/\/registry.opendata.aws\/fast-ai-imageclas\/\" rel=\"nofollow noreferrer\">https:\/\/registry.opendata.aws\/fast-ai-imageclas\/<\/a><\/p>",
        "Challenge_closed_time":1549864663812,
        "Challenge_comment_count":2,
        "Challenge_created_time":1549841280153,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to read AWS open data datasets into Sagemaker and is receiving an Access Denied error. The user has the s3:getObjects * permission and the full Sagemaker policies, but is still unable to access the dataset. The Sagemaker instance and the dataset are both in us-east-1.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54622191",
        "Challenge_link_count":4,
        "Challenge_participation_count":3,
        "Challenge_readability":40.9,
        "Challenge_reading_time":86.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":6.4954608333,
        "Challenge_title":"Access Denied read open data into Sagemaker",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1192.0,
        "Challenge_word_count":309,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1298484007147,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"New York, NY, United States",
        "Poster_reputation_count":9271.0,
        "Poster_view_count":1819.0,
        "Solution_body":"<p>thanks to Matthew I looked into the permissions of the notebook itself, not just the user using Sagemaker.<\/p>\n\n<p>The policies on the notebook look like this and I can download from the aws open data datasets!<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/s0jwV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s0jwV.png\" alt=\"notebook settings\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/OqEHi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/OqEHi.png\" alt=\"notebook permissions\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":13.1,
        "Solution_reading_time":7.05,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":50.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":14.184865,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is there an option to export the Azure ML Designer to code so we can copy between workspaces?<\/p>",
        "Challenge_closed_time":1646202005287,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646150939773,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is looking for an option to export Azure ML Designer to code for copying between workspaces.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/755142\/azure-ml-designer-export-code",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.4,
        "Challenge_reading_time":1.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":14.184865,
        "Challenge_title":"Azure ML Designer: Export Code",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":22,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, this feature is currently not supported as mentioned on this <a href=\"https:\/\/stackoverflow.com\/questions\/60306240\/export-azure-ml-studio-designer-project-as-jupyter-notebook\">thread<\/a>. However, it's on the roadmap.  <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.4,
        "Solution_reading_time":3.1,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1305851487736,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5993.0,
        "Answerer_view_count":457.0,
        "Challenge_adjusted_solved_time":68.3225869444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>My team has a set up wherein we track datasets and models in DVC, and have a GitLab repository for tracking our code and DVC metadata files. We have a job in our dev GitLab pipeline (run on each push to a merge request) that has the goal of checking to be sure that the developer remembered to run <code>dvc push<\/code> to keep DVC remote storage up-to-date. Right now, the way we do this is by running <code>dvc pull<\/code> on the GitLab runner, which will fail with errors telling you which files (new files or latest versions of existing files) were not found.<\/p>\n<p>The downside to this approach is that we are loading the entirety of our data stored in DVC onto a GitLab runner, and we've run into out-of-memory issues, not to mention lengthy run time to download all that data. Since the path and md5 hash of the objects are stored in the DVC metadata files, I would think that's all the information that DVC would need to be able to answer the question &quot;is the remote storage system up-to-date&quot;.<\/p>\n<p>It seems like <code>dvc status<\/code> is similar to what I'm asking for, but compares the cache or workspace and remote storage. In other words, it requires the files to actually be present on whatever filesystem is making the call.<\/p>\n<p>Is there some way to achieve the goal I laid out above (&quot;inform the developer that they need to run <code>dvc push<\/code>&quot;) without pulling everything from DVC?<\/p>",
        "Challenge_closed_time":1622257759208,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622232629793,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user's team tracks datasets and models in DVC and has a GitLab repository for tracking code and DVC metadata files. They have a job in their dev GitLab pipeline that checks if the developer has run \"dvc push\" to keep DVC remote storage up-to-date. Currently, they run \"dvc pull\" on the GitLab runner, which loads the entirety of their data stored in DVC onto the runner, causing out-of-memory issues and lengthy run time. The user is looking for a way to check if the version of a file tracked by a DVC metadata file exists in remote storage without pulling the file.",
        "Challenge_last_edit_time":1622257491983,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67744934",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":19.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":6.9803930556,
        "Challenge_title":"Is it possible to check that the version of a file tracked by a DVC metadata file exists in remote storage without pulling the file?",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":488.0,
        "Challenge_word_count":272,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1618255062696,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":75.0,
        "Poster_view_count":2.0,
        "Solution_body":"<blockquote>\n<p>It seems like dvc status is similar to what I'm asking for<\/p>\n<\/blockquote>\n<p><code>dvc status --cloud<\/code> will give you a list of &quot;new&quot; files if they that haven't been pushed to the (default) remote. It won't error out though, so your CI script should fail depending on the stdout message.<\/p>\n<p>More info: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/status#options\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/status#options<\/a><\/p>\n<p>I'd also ask everyone to run <code>dvc install<\/code>, which will setup some Git hooks, including automatic <code>dvc push<\/code> with <code>git push<\/code>.<\/p>\n<p>See <a href=\"https:\/\/dvc.org\/doc\/command-reference\/install\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/install<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1622503453296,
        "Solution_link_count":4.0,
        "Solution_readability":13.8,
        "Solution_reading_time":10.5,
        "Solution_score_count":3.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":83.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1431525955023,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cherry Hill, NJ, United States",
        "Answerer_reputation_count":2069.0,
        "Answerer_view_count":145.0,
        "Challenge_adjusted_solved_time":15431.7114127778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is there a way to get log the descriptive stats of a dataset using MLflow? If any could you please share the details?<\/p>",
        "Challenge_closed_time":1557279162456,
        "Challenge_comment_count":0,
        "Challenge_created_time":1556081529530,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information on how to log descriptive statistics of a dataset using MLflow. They are requesting details on how to do so.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55822637",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":3.4,
        "Challenge_reading_time":2.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":332.6758127778,
        "Challenge_title":"Is there a way to get log the descriptive stats of a dataset using MLflow?",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":4592.0,
        "Challenge_word_count":37,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1411361217027,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":569.0,
        "Poster_view_count":123.0,
        "Solution_body":"<p>Generally speaking you can log arbitrary output from your code using the mlflow_log_artifact() function.  From <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_artifact\" rel=\"noreferrer\">the docs<\/a>:<\/p>\n<blockquote>\n<p><strong>mlflow.log_artifact(local_path, artifact_path=None)<\/strong>\nLog a local file or directory as an artifact of the currently active run.<\/p>\n<\/blockquote>\n<blockquote>\n<p><strong>Parameters:<\/strong><br \/>\n<em>local_path<\/em> \u2013 Path to the file to write.\n<em>artifact_path<\/em> \u2013 If provided, the directory in artifact_uri to write to.<\/p>\n<\/blockquote>\n<p>As an example, say you have your statistics in a pandas dataframe, <code>stat_df<\/code>.<\/p>\n<pre><code>## Write csv from stats dataframe\nstat_df.to_csv('dataset_statistics.csv')\n\n## Log CSV to MLflow\nmlflow.log_artifact('dataset_statistics.csv')\n<\/code><\/pre>\n<p>This will show up under the artifacts section of this MLflow run in the Tracking UI.  If you explore the docs further you'll see that you can also log an entire directory and the objects therein.  In general, MLflow provides you a lot of flexibility - anything you write to your file system you can track with MLflow.  Of course that doesn't mean you should. :)<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1611635690616,
        "Solution_link_count":1.0,
        "Solution_readability":10.1,
        "Solution_reading_time":15.92,
        "Solution_score_count":9.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":148.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":16.7097122222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm in classic Azure ML mode. I am working on my first ever experiment, so please be patient..    <\/p>\n<p>I cannot locate column selector for CSV data to filter out columns. I found this:    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/select-columns-in-dataset\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/select-columns-in-dataset<\/a>    <\/p>\n<p>And I'm following a tutorial (behind pay wall, from 2017) that shows it in the right hand side properties pane. It says in his example to add the &quot;Select columns in dataset&quot; and it shows the option of &quot;launch column selector&quot;.    <\/p>\n<p>I have browsed through every single choice in the left menu, but cannot locate it... I have no idea what I am missing.    <\/p>\n<p>I need to exclude columns from the data set. Then later I need to make some of the fields &quot;categorical&quot;. Input on that would be appreciated too, unless it becomes obvious from other information provided.    <\/p>\n<p>Please help me :) Thanks in advance for patience and\/or assistance.<\/p>",
        "Challenge_closed_time":1619488667407,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619428512443,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is a beginner in classic Azure ML mode and is unable to locate the column selector for CSV data to filter out columns. They have followed a tutorial that shows the option of \"launch column selector\" in the right-hand side properties pane, but they cannot find it. They need to exclude columns from the data set and make some fields \"categorical\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/371634\/beginner-question-cannot-locate-column-selector-fo",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":15.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":16.7097122222,
        "Challenge_title":"Beginner question - Cannot locate column selector for CSV data to filter out columns",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":162,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello,    <\/p>\n<p>First you need to navigate to Data Transformation  - &gt; Manipulation -&gt; Select columns in dataset, drag that into your process.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/91523-image.png?platform=QnA\" alt=\"91523-image.png\" \/>    <\/p>\n<p>Then, left click on the module and click launch column selector.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/91497-image.png?platform=QnA\" alt=\"91497-image.png\" \/>    <\/p>\n<p>And you can do you want now.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/91524-image.png?platform=QnA\" alt=\"91524-image.png\" \/>    <\/p>\n<p>Please accept the answer if you feel helpful, thanks.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.7,
        "Solution_reading_time":9.81,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":69.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1518706063680,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":95.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":866.1332491667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>To overcome <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-save-write-experiment-files#storage-limits-of-experiment-snapshots\" rel=\"nofollow noreferrer\">300MB snapshot size limit<\/a> I created an .amlignore file in the root of my repository:<\/p>\n\n<pre><code>\/*\n!\/root\n<\/code><\/pre>\n\n<p>The intention is to exclude everything except <code>\/root<\/code> directory where all python code is. The size of the <code>root<\/code> directory is less than 1MB, still I get an error of exceeding snapshot limit size of 300MB. What am I doing wrong?<\/p>",
        "Challenge_closed_time":1573932699567,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570814619870,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user created an .amlignore file to exclude everything except the \/root directory where all the Python code is located in order to overcome the 300MB snapshot size limit. However, even though the size of the \/root directory is less than 1MB, the user is still getting an error of exceeding the snapshot limit size of 300MB.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58345935",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":8.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":866.1332491667,
        "Challenge_title":"The amlignore file doesn't reduce the size of snapshot",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":522.0,
        "Challenge_word_count":72,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1518706063680,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>This is fixed in version <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/azure-machine-learning-release-notes#azure-machine-learning-sdk-for-python-v1074\" rel=\"nofollow noreferrer\">1.0.74 of azureml-sdk<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":28.3,
        "Solution_reading_time":3.35,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":11.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1513106638900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":276.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":12.3196019445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm attempting to use Tensorflows <code>tf.contrib.factorization.KMeansClustering<\/code> estimator with SageMaker but am having some trouble. The output of my SageMaker <code>predictor.predict()<\/code> call looks incorrect. The cluster values are too large as they should be integers from 0-7. (I have the number of clusters set to 8).<\/p>\n\n<p>I get a similar output on every run (where the last half of the array is <code>4L<\/code> or some other digit like <code>0L<\/code>). There are 40 values in the array because that s how many rows(users and their ratings I pass into the <code>predict()<\/code> function)<\/p>\n\n<p>Example:\n<code>{'outputs': {u'output': {'int64_val': [6L, 0L, 6L, 1L, 2L, 4L, 5L, 7L, 7L, 7L, 7L, 5L, 0L, 1L, 7L, 3L, 3L, 6L, 7L, 3L, 7L, 2L, 6L, 2L, 3L, 7L, 6L, 3L, 3L, 6L, 1L, 2L, 1L, 3L, 7L, 7L, 7L, 3L, 5L, 7L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L], 'dtype': 9, 'tensor_shape': {'dim': [{'size': 100L}]}}}, 'model_spec': {'signature_name': u'serving_default', 'version': {'value': 1534392971L}, 'name': u'generic_model'}}<\/code><\/p>\n\n<p>The data I'm working with is a sparse matrix of item ratings where <code>rows=users<\/code>, <code>cols=items<\/code>, and the cells contain floats bewteen 0.0 and 10. So my input data is a matrix instead of the typical array of features.<\/p>\n\n<p>I think the issue might be in the serving_input_fn function. Here is my SageMaker entry_point script:<\/p>\n\n<pre><code>def estimator_fn(run_config, params):\n    #feature_columns = [tf.feature_column.numeric_column('inputs', shape=list(params['input_shape']))]\n    return tf.contrib.factorization.KMeansClustering(num_clusters=NUM_CLUSTERS,\n                            distance_metric=tf.contrib.factorization.KMeansClustering.COSINE_DISTANCE,\n                            use_mini_batch=False,\n                            feature_columns=None,\n                            config=run_config)\n\ndef serving_input_fn(params):\n    tensor = tf.placeholder(tf.float32, shape=[None, None])\n    return tf.estimator.export.build_raw_serving_input_receiver_fn({'inputs': tensor})()\n\ndef train_input_fn(training_dir, params):\n    \"\"\" Returns input function that would feed the model during training \"\"\"\n    return generate_input_fn(training_dir, 'train.csv')\n\n\ndef eval_input_fn(training_dir, params):\n    \"\"\" Returns input function that would feed the model during evaluation \"\"\"\n    return generate_input_fn(training_dir, 'test.csv')\n\n\ndef generate_input_fn(training_dir, training_filename):\n    \"\"\" Generate all the input data needed to train and evaluate the model. \"\"\"\n    # Load train\/test data from s3 bucket\n    train = np.loadtxt(os.path.join(training_dir, training_filename), delimiter=\",\")\n    return tf.estimator.inputs.numpy_input_fn(\n        x={'inputs': np.array(train, dtype=np.float32)},\n        y=None,\n        num_epochs=1,\n        shuffle=False)()\n<\/code><\/pre>\n\n<p>In <code>generate_input_fn()<\/code>, <code>train<\/code> is the numpy ratings matrix.<\/p>\n\n<p>If it helps, here is my call to the <code>predict()<\/code> function, (<code>ratings_matrix<\/code> is a 40 x num_items numpy array):<\/p>\n\n<pre><code>mtx = tf.make_tensor_proto(values=ratings_matrix,\n                           shape=list(ratings_matrix.shape), dtype=tf.float32)\nresult = predictor.predict(mtx)\n<\/code><\/pre>\n\n<p>I feel like the issue is something simple I'm missing. This is the first ML algorithm I've written so any help would be appreciated.<\/p>",
        "Challenge_closed_time":1534550266343,
        "Challenge_comment_count":0,
        "Challenge_created_time":1534484294060,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble using Tensorflow's KMeansClustering estimator with SageMaker. The output of the predictor.predict() call looks incorrect, with cluster values that are too large. The data being worked with is a sparse matrix of item ratings, where rows represent users and columns represent items. The input data is a matrix instead of the typical array of features. The issue may be in the serving_input_fn function. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":1534505915776,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51888996",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.7,
        "Challenge_reading_time":44.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":18.3256341667,
        "Challenge_title":"How to write Tensorflow KMeans Estimator script for Sagemaker",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":321.0,
        "Challenge_word_count":415,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1516029540768,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":161.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>Thanks javadba for your answer!<\/p>\n\n<p>I am not very well adversed in Machine Learning or TensorFlow, so please correct me. However, it looks like you were able to integrate with SageMaker, but the predictions aren't what you are expecting.<\/p>\n\n<p>Ultimately, SageMaker runs your <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst#preparing-the-tensorflow-training-script\" rel=\"nofollow noreferrer\">EstimatorSpec<\/a> with <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/blob\/master\/src\/tf_container\/trainer.py#L73\" rel=\"nofollow noreferrer\">train_and_evaluate<\/a> for training and uses TensorFlow Serving for your predictions. It doesn't have any other hidden functionalities, so the results you get from your KMeans predictions using the TensorFlow estimator is going to be independent of SageMaker. It might be affected by how you define your serving_input_fn and output_fn however.<\/p>\n\n<p>When you run this same estimator outside of the SageMaker ecosystem using the same setup, do you get predictions in the format you're expecting?<\/p>\n\n<p>The SageMaker TensorFlow experience is open sourced here and shows what is possible and isn't as of now.\n<a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":15.6,
        "Solution_reading_time":17.95,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":147.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1491467888608,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":381.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":12.9035880556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I do not want to use wandb. I don't even have an account. I am simply following <a href=\"https:\/\/colab.research.google.com\/github\/huggingface\/notebooks\/blob\/master\/examples\/summarization.ipynb#scrollTo=UmvbnJ9JIrJd\" rel=\"nofollow noreferrer\">this notebook<\/a> for finetuning. I am not running the 2nd and 3 cells because I do not want to push the model to the hub.<\/p>\n<p>However, when I do trainer.train() I get the following error : <a href=\"https:\/\/i.stack.imgur.com\/VPIZm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/VPIZm.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I don't understand where wandb.log is being called.\nI even tried os.environ[&quot;WANDB_DISABLED&quot;]  = &quot;true&quot; but I still get the error.\nPlease help.<\/p>",
        "Challenge_closed_time":1649156563120,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649110110203,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue where wandb is getting logged without initiating it. The user does not want to use wandb and is following a notebook for finetuning. However, when the user runs trainer.train(), they get an error related to wandb.log being called. The user has tried disabling wandb but still encounters the error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71744288",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":9.5,
        "Challenge_reading_time":10.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":12.9035880556,
        "Challenge_title":"wandb getting logged without initiating",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":289.0,
        "Challenge_word_count":90,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1499867951607,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":307.0,
        "Poster_view_count":54.0,
        "Solution_body":"<p>posting the same message as <a href=\"https:\/\/github.com\/huggingface\/transformers\/issues\/16594\" rel=\"nofollow noreferrer\">over on <code>transformers<\/code><\/a>:<\/p>\n<hr \/>\n<p>You can turn off all external logger logging, including wandb logging by passing <code>report_to=&quot;none&quot;<\/code> in your <code>Seq2SeqTrainingArguments<\/code>.<\/p>\n<p>You might have noticed the following warning when setting up your TrainingArguments:<\/p>\n<pre><code>The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-)\n<\/code><\/pre>\n<p>Right now the default is to run all loggers that you have installed, so maybe you installed wandb on your machine since the last time you ran the script?<\/p>\n<p>If you would like to log with wandb, best practice would already be to start setting <code>report_to=&quot;wandb&quot;<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.1,
        "Solution_reading_time":13.1,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":133.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":70.57987,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I already post my problem <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/aff7df3f-afbc-4abc-8fb0-5597184fa6c1\/export-data-blob-storage-v2?forum=MachineLearning\" rel=\"nofollow noreferrer\">here<\/a> and they suggested me to post it here.\nI am trying to export data from Azure ML to Azure Storage but I have this error:<\/p>\n\n<p>Error writing to cloud storage: The remote server returned an error: (400) Bad Request.. Please check the url. . ( Error 0151 )<\/p>\n\n<p>My blob storage configuration is Storage v2 \/ Standard and  Require secure transfer set as enabled.<\/p>\n\n<p>If I set the Require secure transfer set as disabled, the export works fine.<\/p>\n\n<p><strong>How can I export data to my blob storage with the require secure transfer set as enabled ?<\/strong><\/p>",
        "Challenge_closed_time":1550477536900,
        "Challenge_comment_count":1,
        "Challenge_created_time":1550223149923,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while exporting data from Azure ML to Azure Storage V2. The error message \"Error writing to cloud storage: The remote server returned an error: (400) Bad Request\" is displayed when the user tries to export data with \"Require secure transfer\" enabled. However, the export works fine when \"Require secure transfer\" is disabled. The user is seeking a solution to export data with \"Require secure transfer\" enabled.",
        "Challenge_last_edit_time":1550223449368,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54706312",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.8,
        "Challenge_reading_time":10.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":70.6630491667,
        "Challenge_title":"Azure ML studio export data Azure Storage V2",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":819.0,
        "Challenge_word_count":109,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521189557296,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":160.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>According to the offical tutorial <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/export-to-azure-blob-storage\" rel=\"nofollow noreferrer\"><code>Export to Azure Blob Storage<\/code><\/a>, there are two authentication types for exporting data to Azure Blob Storage: SAS and Account. The description for them as below.<\/p>\n\n<blockquote>\n  <ol start=\"4\">\n  <li><p>For <strong>Authentication type<\/strong>, choose <strong>Public (SAS URL)<\/strong> if you know that the storage supports access via a SAS URL.<\/p>\n  \n  <p>A SAS URL is a special type of URL that can be generated by using an Azure storage utility, and is available for only a limited time. It contains all the information that is needed for authentication and download.<\/p>\n  \n  <p>For <strong>URI<\/strong>, type or paste the full URI that defines the account and the public blob.<\/p><\/li>\n  <li><p>For private accounts, choose <strong>Account<\/strong>, and provide the account name and the account key, so that the experiment can write to the storage account.<\/p>\n  \n  <ul>\n  <li><p><strong>Account name<\/strong>: Type or paste the name of the account where you want to save the data. For example, if the full URL of the storage account is <a href=\"http:\/\/myshared.blob.core.windows.net\" rel=\"nofollow noreferrer\">http:\/\/myshared.blob.core.windows.net<\/a>, you would type myshared.<\/p><\/li>\n  <li><p><strong>Account key<\/strong>: Paste the storage access key that is associated with the account.<\/p><\/li>\n  <\/ul><\/li>\n  <\/ol>\n<\/blockquote>\n\n<p>I try to use a simple module combination as the figure and Python code below to test the issue you got.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9XXPV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9XXPV.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<pre><code>import pandas as pd\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    dataframe1 = pd.DataFrame(data={'col1': [1, 2], 'col2': [3, 4]})\n    return dataframe1,\n<\/code><\/pre>\n\n<p>When I tried to use the authentication type <code>Account<\/code> of my Blob Storage V2 account, I got the same issue as yours which the error code is <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/errors\/error-0151\" rel=\"nofollow noreferrer\">Error 0151<\/a> as below via click the <code>View error log<\/code> Button under the link of <code>View output log<\/code>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/TFQgO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TFQgO.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<blockquote>\n  <p><strong>Error 0151<\/strong><\/p>\n  \n  <p>There was an error writing to cloud storage. Please check the URL.<\/p>\n  \n  <p>This error in Azure Machine Learning occurs when the module tries to write data to cloud storage but the URL is unavailable or invalid.<\/p>\n  \n  <p><strong>Resolution<\/strong>\n  Check the URL and verify that it is writable.<\/p>\n  \n  <p><strong>Exception Messages<\/strong><\/p>\n  \n  <ul>\n  <li>Error writing to cloud storage (possibly a bad url).<\/li>\n  <li>Error writing to cloud storage: {0}. Please check the url.<\/li>\n  <\/ul>\n<\/blockquote>\n\n<p>Based on the error description above, the error should be caused by the blob url with SAS incorrectly generated by the <code>Export Data<\/code> module code with account information. May I think the code is old and not compatible with the new V2 storage API or API version information. You can report it to <code>feedback.azure.com<\/code>.<\/p>\n\n<p>However, I switched to use <code>SAS<\/code> authentication type to type a blob url with a SAS query string of my container which I generated via <a href=\"https:\/\/azure.microsoft.com\/en-us\/features\/storage-explorer\/\" rel=\"nofollow noreferrer\">Azure Storage Explorer<\/a> tool as below, it works fine.<\/p>\n\n<p>Fig 1: Right click on the container of your Blob Storage account, and click the <code>Get Shared Access Signature<\/code><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/1fyFL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1fyFL.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 2: Enable the permission <code>Write<\/code> (recommended to use UTC timezone) and click <code>Create<\/code> button<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/IsbQQ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IsbQQ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 3: Copy the <code>Query string<\/code> value, and build a blob url with a container SAS query string like <code>https:\/\/&lt;account name&gt;.blob.core.windows.net\/&lt;container name&gt;\/&lt;blob name&gt;&lt;query string&gt;<\/code><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/RGnxn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RGnxn.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Note: The blob must be not exist in the container, otherwise an <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/errors\/error-0057\" rel=\"nofollow noreferrer\">Error 0057<\/a> will be caused.<\/em><\/strong><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":17.0,
        "Solution_readability":10.9,
        "Solution_reading_time":65.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":42.0,
        "Solution_word_count":563.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1589984605967,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":20.0,
        "Challenge_adjusted_solved_time":50.0349444445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to test Sagemaker Groundtruth's active learning capability, but cannot figure out how to get the auto-labeling part to work. I started a previous labeling job with an initial model that I had to create manually. This allowed me to retrieve the model's ARN as a starting point for the next job. I uploaded 1,758 dataset objects and labeled 40 of them. I assumed the auto-labeling would take it from here, but the job in Sagemaker just says \"complete\" and is only displaying the labels that I created. How do I make the auto-labeler work?<\/p>\n\n<p>Do I have to manually label 1,000 dataset objects before it can start working? I saw this post: <a href=\"https:\/\/stackoverflow.com\/questions\/57852690\/information-regarding-amazon-sagemaker-groundtruth\">Information regarding Amazon Sagemaker groundtruth<\/a>, where the representative said that some of the 1,000 objects can be auto-labeled, but how is that possible if it needs 1,000 objects to start auto-labeling? <\/p>\n\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1589986381867,
        "Challenge_comment_count":0,
        "Challenge_created_time":1589806256067,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is having trouble getting the auto-labeling feature to work in Amazon Sagemaker Groundtruth's active learning capability. They have labeled 40 out of 1,758 dataset objects and assumed the auto-labeling would take over, but it did not. The user is unsure if they need to manually label 1,000 dataset objects before the auto-labeler can work. They are seeking advice on how to make the auto-labeler work.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61870000",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":13.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":50.0349444445,
        "Challenge_title":"Amazon Sagemaker Groundtruth: Cannot get active learning to work",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":738.0,
        "Challenge_word_count":159,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1489377488790,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":437.0,
        "Poster_view_count":68.0,
        "Solution_body":"<p>I'm an engineer at AWS. In order to understand the \"active learning\"\/\"automated data labeling\" feature, it will be helpful to start with a broader recap of how SageMaker Ground Truth works.<\/p>\n\n<p>First, let's consider the workflow without the active learning feature. Recall that Ground Truth annotates data in batches [<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-batching.html]\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-batching.html]<\/a>. This means that your dataset is submitted for annotation in \"chunks.\" The size of these batches is controlled by the API parameter MaxConcurrentTaskCount [<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_HumanTaskConfig.html#sagemaker-Type-HumanTaskConfig-MaxConcurrentTaskCount]\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_HumanTaskConfig.html#sagemaker-Type-HumanTaskConfig-MaxConcurrentTaskCount]<\/a>. This parameter has a default value of 1,000. You cannot control this value when you use the AWS console, so the default value will be used unless you alter it by submitting your job via the API instead of the console.<\/p>\n\n<p>Now, let's consider how active learning fits into this workflow. Active learning runs <em>in between<\/em> your batches of manual annotation. Another important detail is that Ground Truth will partition your dataset into a validation set and an unlabeled set. For datasets smaller than 5,000 objects, the validation set will be 20% of your total dataset; for datasets largert than 5,000 objects, the validation set will be 10% of your total dataset. Once the validation set is collected, any data that is subsequently annotated manually consistutes the training set. The collection of the validation set and training set proceeds according to the batch-wise process described in the previous paragraph. A longer discussion of active learning is available in [<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html]\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html]<\/a>.<\/p>\n\n<p>That last paragraph was a bit of a mouthful, so I'll provide an example using the numbers you gave.<\/p>\n\n<h1>Example #1<\/h1>\n\n<ul>\n<li>Default MaxConcurrentTaskCount (\"batch size\") of 1,000<\/li>\n<li>Total dataset size: 1,758 objects<\/li>\n<li>Computed validation set size: 0.2 * 1758 = 351 objects<\/li>\n<\/ul>\n\n<p>Batch #<\/p>\n\n<ol>\n<li>Annotate 351 objects to populate the validation set (1407 remaining).<\/li>\n<li>Annotate 1,000 objects to populate the first iteration of the training set (407 remaining).<\/li>\n<li>Run active learning. This step may, depending on the accuracy of the model at this stage, result in the annotation of zero, some, or all of the remaining 407 objects.<\/li>\n<li>(Assume no objects were automatically labeled in step #3) Annotate 407 objects. End labeling job.<\/li>\n<\/ol>\n\n<h1>Example #2<\/h1>\n\n<ul>\n<li>Non-default MaxConcurrentTaskCount (\"batch size\") of 250<\/li>\n<li>Total dataset size: 1,758 objects<\/li>\n<li>Computed validation set size: 0.2 * 1758 = 351 objects<\/li>\n<\/ul>\n\n<p>Batch #<\/p>\n\n<ol>\n<li>Annotate 250 objects to begin populating the validation set (1508 remaining).<\/li>\n<li>Annotate 101 objects to finish populating the validation set (1407 remaining).<\/li>\n<li>Annotate 250 objects to populate the first iteration of the training set (1157 remaining).<\/li>\n<li>Run active learning. This step may, depending on the accuracy of the model at this stage, result in the annotation of zero, some, or all of the remaining 1157 objects. All else being equal, we would expect the model to be less accurate than the model in example #1 at this stage, because our training set is only 250 objects here.<\/li>\n<li>Repeat alternating steps of annotating batches of 250 objects and running active learning.<\/li>\n<\/ol>\n\n<p>Hopefully these examples illustrate the workflow and help you understand the process a little better. Since your dataset consists of 1,758 objects, the upper bound on the number of automated labels that can be supplied is 407 objects (assuming you use the default MaxConcurrentTaskCount).<\/p>\n\n<p>Ultimately, 1,758 objects is still a relatively small dataset. We typically recommend at least 5,000 objects to see meaningful results [<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html]\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html]<\/a>. Without knowing any other details of your labeling job, it's difficult to gauge why your job didn't result in more automated annotations. A useful starting point might be to inspect the annotations you received, and to determine the quality of the model that was trained during the Ground Truth labeling job.<\/p>\n\n<p>Best regards from AWS! <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":8.0,
        "Solution_readability":12.6,
        "Solution_reading_time":62.4,
        "Solution_score_count":4.0,
        "Solution_sentence_count":42.0,
        "Solution_word_count":618.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1305851487736,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5993.0,
        "Answerer_view_count":457.0,
        "Challenge_adjusted_solved_time":5.7160508333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a couple of projects that are using and updating the same data sources. I recently learned about <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries\" rel=\"nofollow noreferrer\">dvc's data registries<\/a>, which sound like a great way of versioning data across these different projects (e.g. scrapers, computational pipelines).<\/p>\n<p>I have put all of the relevant data into <code>data-registry<\/code> and then I imported the relevant files into the scraper project with:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ poetry run dvc import https:\/\/github.com\/username\/data-registry raw\n<\/code><\/pre>\n<p>where <code>raw<\/code> is a directory that stores the scraped data. This seems to have worked properly, but then when I went to build <a href=\"https:\/\/dvc.org\/doc\/start\/data-pipelines\" rel=\"nofollow noreferrer\">a dvc pipeline<\/a> that <em>outputted<\/em> data into a file that was already tracked by dvc, I got an error:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ dvc run -n menu_items -d src\/ -o raw\/menu_items\/restaurant.jsonl scrapy crawl restaurant\nERROR: Paths for outs:                                                \n'raw'('raw.dvc')\n'raw\/menu_items\/restaurant.jsonl'('menu_items')\noverlap. To avoid unpredictable behaviour, rerun command with non overlapping outs paths.\n<\/code><\/pre>\n<p>Can someone help me understand what is going on here? <strong>What is the best way to use data registries to share and update data across projects?<\/strong><\/p>\n<p>I would ideally like to update the data-registry with new data from the scraper project and then allow other dependent projects to update their data when they are ready to do so.<\/p>",
        "Challenge_closed_time":1614537291720,
        "Challenge_comment_count":1,
        "Challenge_created_time":1614516713937,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use dvc's data registries to version data across different projects. They imported relevant files into the scraper project, but when they tried to build a dvc pipeline that outputted data into a file already tracked by dvc, they received an error message. The user is seeking help to understand what is going on and the best way to use data registries to share and update data across projects.",
        "Challenge_last_edit_time":1614699218992,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66409283",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":21.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":5.7160508333,
        "Challenge_title":"updating data in dvc registry from other projects",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":388.0,
        "Challenge_word_count":217,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1294268936687,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Chicago, IL",
        "Poster_reputation_count":2893.0,
        "Poster_view_count":168.0,
        "Solution_body":"<p>When you <code>import<\/code> (or <code>add<\/code>) something into your project, a .dvc file is created with that lists that something (in this case the <code>raw\/<\/code> dir) as an &quot;output&quot;.<\/p>\n<p>DVC doesn't allow overlapping outputs among .dvc files or dvc.yaml stages, meaning that your &quot;menu_items&quot; stage shouldn't write to <code>raw\/<\/code> since it's already under the control of <code>raw.dvc<\/code>.<\/p>\n<p>Can you make a separate directory for the pipeline outputs? E.g. use <code>processed\/menu_items\/restaurant.jsonl<\/code><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1614698988012,
        "Solution_link_count":0.0,
        "Solution_readability":9.0,
        "Solution_reading_time":7.26,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":69.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1265234764768,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Denver, CO",
        "Answerer_reputation_count":30577.0,
        "Answerer_view_count":6460.0,
        "Challenge_adjusted_solved_time":19.0194533333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>If I have a column of data of type string in an incoming Azure ML dataset that contains HTML tags screwing up my results, how can I remove those tags?<\/p>",
        "Challenge_closed_time":1484610622880,
        "Challenge_comment_count":0,
        "Challenge_created_time":1484610622880,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in removing HTML tags from a string column in an incoming Azure ML dataset to avoid interference with the results.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41686871",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":2.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"How to strip HTML from a text column in Azure ML Execute Python Script step",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":325.0,
        "Challenge_word_count":44,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1265234764768,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Denver, CO",
        "Poster_reputation_count":30577.0,
        "Poster_view_count":6460.0,
        "Solution_body":"<p>Like this:<\/p>\n\n<pre><code>def azureml_main(dataframe1 = None, dataframe2 = None):\n  dataframe1[1] = dataframe1['text'].str.replace('&lt;[^&lt;]+?&gt;', ' ', case=False)\n  return dataframe1,\n<\/code><\/pre>\n\n<p>Remember to precede the <code>Execute Python Script<\/code> step with <code>Clean Missing Data<\/code> step and change the action to remove the entire row (if appropriate). This is important because the <code>Execute Python Script<\/code> step cannot return an empty <code>dataframe<\/code>. Only you know your data, in this case.<\/p>\n\n<p>Let me also point out that the <code>Preprocessing Text<\/code> step allows you to apply a Regular Expression. That is another alternative that might be right for your situation.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1484679092912,
        "Solution_link_count":0.0,
        "Solution_readability":10.1,
        "Solution_reading_time":9.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":87.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1221810788500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paderborn, North-Rhine-Westphalia, Germany",
        "Answerer_reputation_count":68522.0,
        "Answerer_view_count":7896.0,
        "Challenge_adjusted_solved_time":6278.0877944445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Python code generated from an ml software with mlflow to read a dataframe, perform some table operations and output a dataframe. I am able to run the code successfully and save the new dataframe as an artifact. However I am unable to log the model using log_model because it is not a lr or classifier model where we train and fit. I want to log a model for this so that it can be served with new data and deployed with a rest API<\/p>\n<pre><code>df = pd.read_csv(r&quot;\/home\/xxxx.csv&quot;)\n\n\nwith mlflow.start_run():\n    def getPrediction(row):\n        \n        perform_some_python_operaions \n\n        return [Status_prediction, Status_0_probability, Status_1_probability]\n    columnValues = []\n    for column in columns:\n        columnValues.append([])\n\n    for index, row in df.iterrows():\n        results = getPrediction(row)\n        for n in range(len(results)):\n            columnValues[n].append(results[n])\n\n    for n in range(len(columns)):\n        df[columns[n]] = columnValues[n]\n\n    df.to_csv('dataset_statistics.csv')\n    mlflow.log_artifact('dataset_statistics.csv')\n   \n<\/code><\/pre>",
        "Challenge_closed_time":1611592914947,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611586824463,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary_original":"The user is using mlflow to read a dataframe, perform some table operations and output a dataframe. They are able to run the code successfully and save the new dataframe as an artifact. However, they are unable to log the model using log_model because it is not a lr or classifier model where they train and fit. They want to log a model for this so that it can be served with new data and deployed with a rest API.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65887231",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.9,
        "Challenge_reading_time":13.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":1.6918011111,
        "Challenge_title":"Use mlflow to serve a custom python model for scoring",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":3026.0,
        "Challenge_word_count":134,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1573739890560,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>MLflow supports <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#custom-python-models\" rel=\"nofollow noreferrer\">custom models<\/a> of mlflow.pyfunc flavor.  You can create a custom  class  inherited from the <code>mlflow.pyfunc.PythonModel<\/code>, that needs to provide function <code>predict<\/code> for performing predictions, and optional <code>load_context<\/code> to load the necessary artifacts, like this (adopted from the docs):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class MyModel(mlflow.pyfunc.PythonModel):\n\n    def load_context(self, context):\n        # load your artifacts\n\n    def predict(self, context, model_input):\n        return my_predict(model_input.values)\n<\/code><\/pre>\n<p>You can log to MLflow whatever artifacts you need for your models, define Conda environment if necessary, etc.<br \/>\nThen you can use <code>save_model<\/code> with your class to save your implementation, that could be loaded with <code>load_model<\/code> and do the <code>predict<\/code> using your model:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.pyfunc.save_model(\n        path=mlflow_pyfunc_model_path, \n        python_model=MyModel(), \n        artifacts=artifacts)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(mlflow_pyfunc_model_path)\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1634187940523,
        "Solution_link_count":1.0,
        "Solution_readability":17.3,
        "Solution_reading_time":16.79,
        "Solution_score_count":9.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":118.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.9223880556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I encounter the following error :  <\/p>\n<p>Parameter &quot;Stopwords columns&quot; value should be less than or equal to parameter &quot;1&quot; value. . ( Error 0007 )  <br \/>\nwhen building a simple pipeline :  <\/p>\n<p>with a .csv Dataset followed by a &quot;Preprocessed Text&quot;.  <\/p>\n<p>No parameter 'Stopwords columns' is available in the &quot;Preprocessed Text&quot; properties !!!<\/p>",
        "Challenge_closed_time":1612878128547,
        "Challenge_comment_count":0,
        "Challenge_created_time":1612874807950,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error (Error 0007) while building a pipeline with a .csv dataset and \"Preprocessed Text\". The error message stated that the \"Stopwords columns\" parameter value should be less than or equal to the parameter \"1\" value, but the user did not find any such parameter in the \"Preprocessed Text\" properties.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/265397\/dataset-preprocessed-text-parameter-stopwords-colu",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":6.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.9223880556,
        "Challenge_title":"Dataset + Preprocessed Text : Parameter \"Stopwords columns\" value should be less than or equal to parameter \"1\" value. . ( Error 0007 )",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":70,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Solved.  <\/p>\n<p>There must be only one connection (left: Dataset) and not 2 connections (left : Dataset + right : Stopwords) from the &quot;Dataset&quot; to the &quot;Preprocessed Text&quot;<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":2.5,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1393852033260,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"India",
        "Answerer_reputation_count":33599.0,
        "Answerer_view_count":6250.0,
        "Challenge_adjusted_solved_time":15862.09938,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am new to Sagemaker and not sure how to classify the text input in AWS sagemaker, <\/p>\n\n<p>Suppose I have a Dataframe having two fields like 'Ticket' and 'Category', Both are text input, Now I want to split it test and training set and upload in Sagemaker training model. <\/p>\n\n<pre><code>X_train, X_test, y_train, y_test = model_selection.train_test_split(fewRecords['Ticket'],fewRecords['Category'])\n<\/code><\/pre>\n\n<p>Now as I want to perform TD-IDF feature extraction and then convert it to numeric value, so performing this operation<\/p>\n\n<pre><code>tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\ntfidf_vect.fit(fewRecords['Category'])\nxtrain_tfidf =  tfidf_vect.transform(X_train)\nxvalid_tfidf =  tfidf_vect.transform(X_test)\n<\/code><\/pre>\n\n<p>When I want to upload the model in Sagemaker so I can perform next operation like <\/p>\n\n<pre><code>buf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(buf, xtrain_tfidf, y_train)\nbuf.seek(0)\n<\/code><\/pre>\n\n<p>I am getting this error <\/p>\n\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-36-8055e6cdbf34&gt; in &lt;module&gt;()\n      1 buf = io.BytesIO()\n----&gt; 2 smac.write_numpy_to_dense_tensor(buf, xtrain_tfidf, y_train)\n      3 buf.seek(0)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/amazon\/common.py in write_numpy_to_dense_tensor(file, array, labels)\n     98             raise ValueError(\"Label shape {} not compatible with array shape {}\".format(\n     99                              labels.shape, array.shape))\n--&gt; 100         resolved_label_type = _resolve_type(labels.dtype)\n    101     resolved_type = _resolve_type(array.dtype)\n    102 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/amazon\/common.py in _resolve_type(dtype)\n    205     elif dtype == np.dtype('float32'):\n    206         return 'Float32'\n--&gt; 207     raise ValueError('Unsupported dtype {} on array'.format(dtype))\n\nValueError: Unsupported dtype object on array\n<\/code><\/pre>\n\n<p>Other than this exception, I am not clear if this is right way as TfidfVectorizer convert the series to Matrix.<\/p>\n\n<p>The code is predicting fine on my local machine but not sure how to do the same on Sagemaker, All the example mentioned there are too lengthy and not for the person who still reached to SciKit Learn <\/p>",
        "Challenge_closed_time":1535540801190,
        "Challenge_comment_count":0,
        "Challenge_created_time":1535524538620,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in classifying text input in AWS Sagemaker. They have a Dataframe with two text input fields, 'Ticket' and 'Category', and want to split it into test and training sets for TD-IDF feature extraction. However, when trying to upload the model in Sagemaker, they encounter a ValueError related to unsupported dtype object on array. The user is unsure if their approach is correct and is looking for guidance on how to perform the same operation on Sagemaker.",
        "Challenge_last_edit_time":1535540817292,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52070950",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":30.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":4.5173805556,
        "Challenge_title":"AWS Sagemaker | how to train text data | For ticket classification",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1519.0,
        "Challenge_word_count":255,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501403168107,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":1370.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>The output of <code>TfidfVectorizer<\/code> is a scipy sparse matrix, not a simple numpy array.<\/p>\n<p>So either use a different function like:<\/p>\n<blockquote>\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/amazon\/common.py#L113\" rel=\"nofollow noreferrer\">write_spmatrix_to_sparse_tensor<\/a><\/p>\n<p>&quot;&quot;&quot;Writes a scipy sparse matrix to a sparse tensor&quot;&quot;&quot;<\/p>\n<\/blockquote>\n<p>See <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/27\" rel=\"nofollow noreferrer\">this issue<\/a> for more details.<\/p>\n<p><strong>OR<\/strong> first convert the output of <code>TfidfVectorizer<\/code> to a dense numpy array and then use your above code<\/p>\n<pre><code>xtrain_tfidf =  tfidf_vect.transform(X_train).toarray()   \nbuf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(buf, xtrain_tfidf, y_train)\n...\n...\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1592644375060,
        "Solution_link_count":2.0,
        "Solution_readability":15.5,
        "Solution_reading_time":11.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":71.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4168277778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi, I am getting this error when scoring a model.   <\/p>\n<p>Seems like it is an out-of-memory issue or a segfault issue (no idea what that means).  <\/p>\n<p>I'm using Designer while my compute is Standard Dv2 Family vCPUs. Have made no changes to my storage account key.   <\/p>\n<p>Any advice on how to debug this one? Many thanks in advance  <\/p>\n<blockquote>\n<p>AzureMLCompute job failed.  <br \/>\nUserProcessKilledBySystemSignal: Job failed since the user script received system termination signal usually due to out-of-memory or segfault.  <br \/>\n Reason: Process Killed with either 6:aborted or 9:killed  or 11:segment fault. exit code here is from wrapping bash hence 128 + n  <br \/>\n Cause: killed  <br \/>\n TaskIndex:   <br \/>\n NodeIp: 10.0.0.5  <br \/>\n NodeId: tvmps_ee452edcf7395836bdf60c0e0cd5f3a6308fafbb41c860c50a47be1367393df6_d  <br \/>\n Reason: Job failed with non-zero exit Code  <\/p>\n<\/blockquote>",
        "Challenge_closed_time":1620235517220,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620234016640,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while scoring a model in AzureMLCompute, which failed due to the user script receiving a system termination signal, usually caused by out-of-memory or segfault issues. The user is seeking advice on how to debug the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/384152\/userprocesskilledbysystemsignal-job-failed-since-t",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.4,
        "Challenge_reading_time":12.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.4168277778,
        "Challenge_title":"UserProcessKilledBySystemSignal: Job failed since the user script received system termination signal usually due to out-of-memory or segfault.",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":143,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Seems like it was an out-of-memory problem. If I reduce the trainning set, I get no error.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.2,
        "Solution_reading_time":1.19,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1566583092316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":479.0,
        "Answerer_view_count":51.0,
        "Challenge_adjusted_solved_time":1.2175213889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a dataset(CSV) file that looks like this one column with an identifier and another with the URL of the image, and I need to download images to my storage in azure machine learning, maybe someone could help what is the core should be?<\/p>",
        "Challenge_closed_time":1607110542867,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607106159790,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user needs assistance in uploading images from a CSV file that contains columns with labels and URLs of images in Azure machine learning. They are seeking guidance on the necessary code to download the images to their storage.",
        "Challenge_last_edit_time":1607459547320,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65148768",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.6,
        "Challenge_reading_time":4.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":1.2175213889,
        "Challenge_title":"how to upload images from csv file that have column with labels and urls of images in Azure machine learning by code",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":127.0,
        "Challenge_word_count":66,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1547213373932,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":47.0,
        "Poster_view_count":59.0,
        "Solution_body":"<p>Please refer to the sample code provided in this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-bring-data\" rel=\"nofollow noreferrer\">tutorial<\/a>. Specifically, where it explains how to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-bring-data#upload\" rel=\"nofollow noreferrer\">upload data to Azure<\/a>.<\/p>\n<pre><code>from azureml.core import Workspace\nws = Workspace.from_config()\ndatastore = ws.get_default_datastore()\ndatastore.upload(src_dir='.\/data',\n                 target_path='datasets\/cifar10',\n                 overwrite=True)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1607110967612,
        "Solution_link_count":2.0,
        "Solution_readability":26.4,
        "Solution_reading_time":8.33,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":38.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1221810788500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paderborn, North-Rhine-Westphalia, Germany",
        "Answerer_reputation_count":68522.0,
        "Answerer_view_count":7896.0,
        "Challenge_adjusted_solved_time":10.5619811111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Mlflow for my project hosting it in an EC2 instance. I was wondering in MlFlow what is the difference between the backend_store_uri we set when we launch the server and the trarcking_uri ?<\/p>\n<p>Thanks,<\/p>",
        "Challenge_closed_time":1629269923972,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629231900840,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using MLFlow for their project hosted on an EC2 instance and is confused about the difference between the backend_store_uri and the tracking_uri in MLFlow. They are seeking clarification on this matter.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68823606",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.9,
        "Challenge_reading_time":3.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":10.5619811111,
        "Challenge_title":"Difference between tracking_uri and the backend store uri in MLFLOW",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":682.0,
        "Challenge_word_count":46,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1585131052300,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p><code>tracking_uri<\/code> is the URL of the MLflow server (remote, or built-in in Databricks) that will be used to log metadata &amp; model (see <a href=\"https:\/\/mlflow.org\/docs\/latest\/quickstart.html#launch-a-tracking-server-on-a-remote-machine\" rel=\"nofollow noreferrer\">doc<\/a>).  In your case, this will be the URL pointing to your EC2 instance that should be configured in programs that will log parameters into your server.<\/p>\n<p><code>backend_store_uri<\/code> - is used by MLflow server to configure where to store this data - on filesystem, in SQL-compatible database, etc. (see <a href=\"https:\/\/mlflow.org\/docs\/latest\/cli.html#cmdoption-mlflow-server-backend-store-uri\" rel=\"nofollow noreferrer\">doc<\/a>). If you use SQL database, then you also need to provide the <code>--default-artifact-root<\/code> option to point where to store generated artifacts (images, model files, etc.)<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.3,
        "Solution_reading_time":11.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":102.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1526489986910,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Philadelphia, PA, USA",
        "Answerer_reputation_count":926.0,
        "Answerer_view_count":48.0,
        "Challenge_adjusted_solved_time":9.0913125,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm new to the aws how to set path of my bucket and access file of that bucket?<\/p>\n\n<p>Is there anything i need to change with prefix ?<\/p>\n\n<pre><code>import os\nimport boto3\nimport re\nimport copy\nimport time\nfrom time import gmtime, strftime\nfrom sagemaker import get_execution_role\n\nrole = get_execution_role()\n\nregion = boto3.Session().region_name\n\nbucket='ltfs1' # Replace with your s3 bucket name\nprefix = 'sagemaker\/ltfs1' # Used as part of the path in the bucket where you store data\n# bucket_path = 'https:\/\/s3-{}.amazonaws.com\/{}'.format(region,bucket) # The URL to access the bucket\n<\/code><\/pre>\n\n<p>I'm using the above code but it's showing file not found error<\/p>",
        "Challenge_closed_time":1562166882852,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562134154127,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is new to AWS and is facing challenges in setting the path of their bucket in Amazon SageMaker Jupyter Notebook. They have shared a code snippet that they are using, but it is resulting in a \"file not found\" error. They are seeking guidance on how to set the path of their bucket and access files from it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56863907",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.0,
        "Challenge_reading_time":9.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":9.0913125,
        "Challenge_title":"how to set path of bucket in amazonsagemaker jupyter notebook?",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":852.0,
        "Challenge_word_count":103,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559907487263,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>If the file you are accessing is in the root directory of your s3 bucket, you can access the file like this:<\/p>\n\n<pre><code>import pandas as pd\n\nbucket='ltfs1'\ndata_key = 'data.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\ntraining_data = pd.read_csv(data_location)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.2,
        "Solution_reading_time":3.78,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":35.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":117.2994491667,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I know how to create a table with a data frame programmatically. However, I have two data frames, and they have different number of rows, so I cannot combine them into a single data frame. How do I upload two different tables to a Weights&amp;Biases project? Somehow, I suspect that the following is not the correct approach:<\/p>\n<pre><code class=\"lang-python\">    train_df = pd.DataFrame({\n        'tx':train_x,\n        'ty':train_y,\n    })\n    valid_df = pd.DataFrame({\n        'vx':valid_x,\n        'vy':valid_y\n    })\n\n    # How to add multiple tables\n\n    wandb.log({\"table\": train_df}, commit=False)\n    wandb.log({\"table\": valid_df}, commit=False)\n<\/code><\/pre>\n<p>Any help is greatly appreciated.<\/p>",
        "Challenge_closed_time":1660166829976,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659744551959,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to upload two different tables to a Weights&Biases project, but the tables have different numbers of rows and cannot be combined into a single data frame. They are seeking guidance on the correct approach to upload multiple tables.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/multiple-tables\/2856",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.6,
        "Challenge_reading_time":8.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":117.2994491667,
        "Challenge_title":"Multiple tables",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":236.0,
        "Challenge_word_count":85,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/erlebacher\">@erlebacher<\/a> , see <a href=\"https:\/\/docs.wandb.ai\/guides\/data-vis\/log-tables#create-tables\">this document<\/a> on how to create tables from dataframes and please let me know if you have any questions.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.7,
        "Solution_reading_time":3.33,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5105555556,
        "Challenge_answer_count":0,
        "Challenge_body":"```\r\ntests\/conftest.py:4: in <module>\r\n    from rikai.spark.sql import init\r\n..\/rikai\/python\/rikai\/__init__.py:19: in <module>\r\n    from rikai.spark.sql.codegen import mlflow_logger as mlflow\r\n..\/rikai\/python\/rikai\/spark\/sql\/codegen\/mlflow_logger.py:19: in <module>\r\n    import mlflow\r\nE   ModuleNotFoundError: No module named 'mlflow'\r\n```",
        "Challenge_closed_time":1617994925000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617993087000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an MLFlow API Request 409 Conflict error when deploying jobs with the --assets-only option. The error occurred while uploading a local file, and the response indicated that the file already exists and cannot be overwritten. The user updated a few jobs using the latest dbx version, and MLFlow was only used to define a specific experiment path. The user's environment includes dbx version 0.8.x, Databricks Runtime version 10.4 LTS (standard or ML), and Python version 3.8.11.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/eto-ai\/rikai\/issues\/207",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":10.0,
        "Challenge_reading_time":4.61,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":19.0,
        "Challenge_repo_issue_count":717.0,
        "Challenge_repo_star_count":131.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.5105555556,
        "Challenge_title":"Leaking mlflow dependency",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":30,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1443426419048,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation_count":842.0,
        "Answerer_view_count":219.0,
        "Challenge_adjusted_solved_time":235.30147,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to see a chart that I have produced within an R script module in Azure ML. It looks like this: <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/S74aN.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/S74aN.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Needless to say, this is unusably small. I am looking for something like <code>width<\/code>... is there anything available?<\/p>\n\n<p>Just in case, the script looks like this:<\/p>\n\n<pre><code>library(GGally)\ndf &lt;- dataset1\nnames(df) &lt;- gsub(\"[- ]\",\"x\",names(df))\nggpairs(df,  alpha=0.4)\n<\/code><\/pre>",
        "Challenge_closed_time":1472811239372,
        "Challenge_comment_count":0,
        "Challenge_created_time":1471964154080,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to view a chart produced within an R script module in Azure ML, but the chart is too small to be usable. They are looking for a way to increase the size of the chart, possibly through a \"width\" option. The R script being used includes the GGally library and is creating a scatterplot matrix using the ggpairs function.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/39104538",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":5.9,
        "Challenge_reading_time":8.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":235.30147,
        "Challenge_title":"Increase size of chart in Azure ML R script",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":124.0,
        "Challenge_word_count":78,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1381858437316,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":2720.0,
        "Poster_view_count":482.0,
        "Solution_body":"<p>One thing we can do is exporting the output plot to a pdf file and storing it to an Azure blob storage. for that you should create a blob storage in your azure storage. \nThen modify the script as follows.<\/p>\n\n<pre><code>d &lt;- maml.mapInputPort(1)\nlibrary(GGally)\nlibrary(caTools)\npdf()\ndf &lt;- d\nnames(df) &lt;- gsub(\"[- ]\",\"x\",names(df))\nd &lt;- ggpairs(df,  alpha=0.4)\nb64ePDF &lt;- function(filename) {\n                maxFileSizeInBytes &lt;- 5 * 1024 * 1024 # 5 MB\n                return(base64encode(readBin(filename, \"raw\", n = maxFileSizeInBytes)))\n}\nd2 &lt;- data.frame(pdf = b64ePDF(\"Rplots.pdf\"))\nmaml.mapOutputPort(\"d2\");\n<\/code><\/pre>\n\n<p>Set your azure blob storage destination. The plot will save as a pdf.\n<a href=\"https:\/\/i.stack.imgur.com\/mTgNH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mTgNH.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.3,
        "Solution_reading_time":11.02,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":99.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.8464063889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When using explanations for AutoML models or standalone model, the explanation dashboard has 2 tabs which displays same information.    <\/p>\n<p>I am using azureml-interpret to explain the models that are executed under azure context  and upload the explanations into Azure ML studio.    <br \/>\nI use global_explanation and local_explanation to explain the overall model performance and local model performance.    <\/p>\n<p>I guess this is creating 2 tabs if I am correct, but both of them seems to have same or duplicate information. I don't understand what is the need for that?    <\/p>\n<p>This seem to the case when I use AutoML models also, there is 2 tabs which has same information. Note, here I am not uploading anything,  it is by default uploading the model explanations and I am using azure-python-sdk-v1.    <\/p>\n<p>I have provided the accompanying screenshots with the information, please let me know if there is gap in my understanding or it is problem with the azure explanation?    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/264609-first-tab-information.png?platform=QnA\" alt=\"264609-first-tab-information.png\" \/>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/264610-second-tab-information.png?platform=QnA\" alt=\"264610-second-tab-information.png\" \/>    <\/p>",
        "Challenge_closed_time":1669634696120,
        "Challenge_comment_count":0,
        "Challenge_created_time":1669620849057,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue with the explanation dashboard in Azure ML Studio, where there are two tabs displaying duplicate information when using explanations for AutoML models or standalone models. The user is unsure of the need for two tabs and is seeking clarification on whether this is a problem with Azure's explanation or a gap in their understanding. Screenshots have been provided for reference.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1106407\/why-explanation-dashboard-is-showing-2-tabs-with-d",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":17.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":3.8464063889,
        "Challenge_title":"Why explanation dashboard is showing 2 tabs with duplicate information in Azure ML Studio?",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":181,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=b3da8189-5298-4acf-8e9a-e4e5f7b30c14\">@Bharath Kumar Loganathan  <\/a> I think the explanation ids are based on the raw and engineered datasets. Raw explanations are based on the features from the original dataset and engineered explanations are based on the features from the dataset with feature engineering applied. The documentation from these links provides a bit more information about the different explanation ids. If you expand the menu on the left this should confirm the same.    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-automated-ml-for-ml-models#model-explanations-preview\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-automated-ml-for-ml-models#model-explanations-preview<\/a>    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-machine-learning-interpretability-aml#visualizations\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-machine-learning-interpretability-aml#visualizations<\/a>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/264729-image.png?platform=QnA\" alt=\"264729-image.png\" \/>    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":21.7,
        "Solution_reading_time":20.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":109.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":555.5452777778,
        "Challenge_answer_count":4,
        "Challenge_body":"Hello,  \n  \nMy aim is to create a model for garden birds. I have 293 photos of birds that I have put through 2 custom labelling jobs in ground truth for training and validation. The issue I encountered was being able to have multiple labels on the bounding box which I managed to do via creating a custom labelling job with the following labels:\n\n```\n<crowd-bounding-box\r\n    name=\"annotatedResult\"\r\n    labels=\"['Blackbird', 'Blue tit', 'Coal tit', 'Dunnock', 'Great tit', 'Long-tailed tit', 'Nuthatch', 'Pigeon', 'Robin']\" .....\n```\n\nI now have 2 output manifest files with many lines of this:\n\n```\n{\"source-ref\":\"s3:\/\/XXXXX\/Blackbird_1.JPG\",\"BirdLabel\":{\"workerId\":\"privateXXXXX\",\"imageSource\":{\"s3Uri\":\"s3:\/\/XXXXX\/Blackbird_1.JPG\"},\"boxesInfo\":{\"annotatedResult\":{\"boundingBoxes\":[{\"width\":1619,\"top\":840,\"label\":\"Blackbird\",\"left\":1287,\"height\":753}],\"inputImageProperties\":{\"width\":3872,\"height\":2592}}}},\"BirdLabel-metadata\":{\"type\":\"groundtruth\/custom\",\"job-name\":\"birdlabel\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-01-10T15:41:52+0000\"}}\n```\n\nAfter this job was successful, I made an ml.p3.2xlarge instance, using the object_detection_augmented_manifest_training template.  \n  \nI have filled in the necessary sections, I then run it and received this error when I have the Content Type to _'application\/x-image'_ with _Record wrapper type:RecordIO_ : **'ClientError: train channel is not specified.'**  \n  \nI then changed the channel to train_annotation instead of train and I receive this error message: **\"ClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError)\\n\\nCaused by: u'train' is a required property**  \n  \nAdditional information can be provided if neccessary.  \nAny help would be much apreciated! Thank you.  \n  \nEdited by: LuciA on Jan 16, 2019 1:12 PM  \n  \nEdited by: LuciA on Jan 16, 2019 1:18 PM  \n  \nEdited by: LuciA on Jan 16, 2019 1:19 PM",
        "Challenge_closed_time":1549563627000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1547563664000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a model for garden birds using 293 photos that have been put through 2 custom labelling jobs in ground truth for training and validation. The user encountered an issue with multiple labels on the bounding box, which was resolved by creating a custom labelling job. However, when using the object_detection_augmented_manifest_training template, the user received the error message \"ClientError: train channel is not specified\" when the Content Type was set to 'application\/x-image' with Record wrapper type:RecordIO. Changing the channel to train_annotation resulted in the error message \"ClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError) Caused by: u'train' is",
        "Challenge_last_edit_time":1668624099052,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUjk2-AZ6VQl68Zdm1Owq-_A\/clienterror-object-detection-augmented-manifest-training-template",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":13.4,
        "Challenge_reading_time":25.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":555.5452777778,
        "Challenge_title":"ClientError: object_detection_augmented_manifest_training template",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":123.0,
        "Challenge_word_count":219,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi LuciA - I'm an engineer at AWS. Thanks for continuing to try the service in the face of some difficulties. Can you please cross-reference your augmented manifest against the samples shown in https:\/\/aws.amazon.com\/blogs\/aws\/amazon-sagemaker-ground-truth-build-highly-accurate-datasets-and-reduce-labeling-costs-by-up-to-70\/, https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/object_detection_augmented_manifest_training\/object_detection_augmented_manifest_training.ipynb, and https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/ground_truth_object_detection_tutorial\/object_detection_tutorial.ipynb?   \n  \nIt looks like your format is a little different, e.g., the algorithm expects to see keys called \"annotations\" and \"image_size\". Can you please check the syntax and let us know if your results change?",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1549563627000,
        "Solution_link_count":3.0,
        "Solution_readability":21.3,
        "Solution_reading_time":11.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1488575811772,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bothell, WA, United States",
        "Answerer_reputation_count":51.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":303.7152472222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a model in AzureML that scores incoming values from a csv.<\/p>\n\n<p>The flow is ...->(Score Model using one-class SVM)->(Normalize Data)->(Convert to CSV)->(Convert to Dataset)->(Web Service Output)<\/p>\n\n<p>When the experiment is run I can download the csv from the (Convert to CSV) module output and it will contain Scored Probabilities column.<\/p>\n\n<p>But when I'm using a streaming job I don't know how to access the Scored Probabilities column using Query SQL. How do I do it?<\/p>",
        "Challenge_closed_time":1488576068267,
        "Challenge_comment_count":0,
        "Challenge_created_time":1487482693377,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in accessing the Scored Probabilities column using Query SQL while using a streaming job in AzureML to score incoming values from a CSV. The user is seeking guidance on how to access the Scored Probabilities column.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42324035",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":6.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":303.7152472222,
        "Challenge_title":"How to select Scored Probabilities from azure prediction model",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":576.0,
        "Challenge_word_count":85,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1300047702248,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":586.0,
        "Poster_view_count":108.0,
        "Solution_body":"<p>You can access the response using the amlresult.[Scored Probabilities] notation, where amlresult is an alias for the return value from your AzureML call.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.0,
        "Solution_reading_time":2.03,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":23.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":263.7483333333,
        "Challenge_answer_count":0,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\n```\r\n.FFF.                                                                    [100%]\r\n=================================== FAILURES ===================================\r\n_____________________________ test_21_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_21_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\r\n            \"21_deployment_on_azure_container_instances\"\r\n        ]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:58: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:104: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour...end_time': '2019-09-12T10:19:40.699401', 'duration': 5.033488, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [2]\":\r\nE           ---------------------------------------------------------------------------\r\nE           SSLError                                  Traceback (most recent call last)\r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in do_open(self, http_class, req, **http_conn_args)\r\nE              1317                 h.request(req.get_method(), req.selector, req.data, headers,\r\nE           -> 1318                           encode_chunked=req.has_header('Transfer-encoding'))\r\nE              1319             except OSError as err: # timeout error\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in request(self, method, url, body, headers, encode_chunked)\r\nE              1238         \"\"\"Send a complete request to the server.\"\"\"\r\nE           -> 1239         self._send_request(method, url, body, headers, encode_chunked)\r\nE              1240 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in _send_request(self, method, url, body, headers, encode_chunked)\r\nE              1284             body = _encode(body, 'body')\r\nE           -> 1285         self.endheaders(body, encode_chunked=encode_chunked)\r\nE              1286 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in endheaders(self, message_body, encode_chunked)\r\nE              1233             raise CannotSendHeader()\r\nE           -> 1234         self._send_output(message_body, encode_chunked=encode_chunked)\r\nE              1235 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in _send_output(self, message_body, encode_chunked)\r\nE              1025         del self._buffer[:]\r\nE           -> 1026         self.send(msg)\r\nE              1027 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in send(self, data)\r\nE               963             if self.auto_open:\r\nE           --> 964                 self.connect()\r\nE               965             else:\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in connect(self)\r\nE              1399             self.sock = self._context.wrap_socket(self.sock,\r\nE           -> 1400                                                   server_hostname=server_hostname)\r\nE              1401             if not self._context.check_hostname and self._check_hostname:\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/ssl.py in wrap_socket(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\r\nE               406                          server_hostname=server_hostname,\r\nE           --> 407                          _context=self, _session=session)\r\nE               408 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/ssl.py in __init__(self, sock, keyfile, certfile, server_side, cert_reqs, ssl_version, ca_certs, do_handshake_on_connect, family, type, proto, fileno, suppress_ragged_eofs, npn_protocols, ciphers, server_hostname, _context, _session)\r\nE               816                         raise ValueError(\"do_handshake_on_connect should not be specified for non-blocking sockets\")\r\nE           --> 817                     self.do_handshake()\r\nE               818 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/ssl.py in do_handshake(self, block)\r\nE              1076                 self.settimeout(None)\r\nE           -> 1077             self._sslobj.do_handshake()\r\nE              1078         finally:\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/ssl.py in do_handshake(self)\r\nE               688         \"\"\"Start the SSL\/TLS handshake.\"\"\"\r\nE           --> 689         self._sslobj.do_handshake()\r\nE               690         if self.context.check_hostname:\r\nE           \r\nE           SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)\r\nE           \r\nE           During handling of the above exception, another exception occurred:\r\nE           \r\nE           URLError                                  Traceback (most recent call last)\r\nE           <ipython-input-2-2e2a8adec5e2> in <module>\r\nE           ----> 1 learn = model_to_learner(models.resnet18(pretrained=True), IMAGENET_IM_SIZE)\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/torchvision\/models\/resnet.py in resnet18(pretrained, progress, **kwargs)\r\nE               229     \"\"\"\r\nE               230     return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\r\nE           --> 231                    **kwargs)\r\nE               232 \r\nE               233 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/torchvision\/models\/resnet.py in _resnet(arch, block, layers, pretrained, progress, **kwargs)\r\nE               215     if pretrained:\r\nE               216         state_dict = load_state_dict_from_url(model_urls[arch],\r\nE           --> 217                                               progress=progress)\r\nE               218         model.load_state_dict(state_dict)\r\nE               219     return model\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/torch\/hub.py in load_state_dict_from_url(url, model_dir, map_location, progress)\r\nE               460         sys.stderr.write('Downloading: \"{}\" to {}\\n'.format(url, cached_file))\r\nE               461         hash_prefix = HASH_REGEX.search(filename).group(1)\r\nE           --> 462         _download_url_to_file(url, cached_file, hash_prefix, progress=progress)\r\nE               463     return torch.load(cached_file, map_location=map_location)\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/torch\/hub.py in _download_url_to_file(url, dst, hash_prefix, progress)\r\nE               370 def _download_url_to_file(url, dst, hash_prefix, progress):\r\nE               371     file_size = None\r\nE           --> 372     u = urlopen(url)\r\nE               373     meta = u.info()\r\nE               374     if hasattr(meta, 'getheaders'):\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in urlopen(url, data, timeout, cafile, capath, cadefault, context)\r\nE               221     else:\r\nE               222         opener = _opener\r\nE           --> 223     return opener.open(url, data, timeout)\r\nE               224 \r\nE               225 def install_opener(opener):\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in open(self, fullurl, data, timeout)\r\nE               524             req = meth(req)\r\nE               525 \r\nE           --> 526         response = self._open(req, data)\r\nE               527 \r\nE               528         # post-process response\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in _open(self, req, data)\r\nE               542         protocol = req.type\r\nE               543         result = self._call_chain(self.handle_open, protocol, protocol +\r\nE           --> 544                                   '_open', req)\r\nE               545         if result:\r\nE               546             return result\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in _call_chain(self, chain, kind, meth_name, *args)\r\nE               502         for handler in handlers:\r\nE               503             func = getattr(handler, meth_name)\r\nE           --> 504             result = func(*args)\r\nE               505             if result is not None:\r\nE               506                 return result\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in https_open(self, req)\r\nE              1359         def https_open(self, req):\r\nE              1360             return self.do_open(http.client.HTTPSConnection, req,\r\nE           -> 1361                 context=self._context, check_hostname=self._check_hostname)\r\nE              1362 \r\nE              1363         https_request = AbstractHTTPHandler.do_request_\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in do_open(self, http_class, req, **http_conn_args)\r\nE              1318                           encode_chunked=req.has_header('Transfer-encoding'))\r\nE              1319             except OSError as err: # timeout error\r\nE           -> 1320                 raise URLError(err)\r\nE              1321             r = h.getresponse()\r\nE              1322         except:\r\nE           \r\nE           URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)>\r\n\r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:188: PapermillExecutionError\r\n----------------------------- Captured stderr call -----------------------------\r\n\r\nExecuting:   0%|          | 0\/65 [00:00<?, ?cell\/s]\r\nExecuting:   2%|\u258f         | 1\/65 [00:00<00:56,  1.14cell\/s]\r\nExecuting:   5%|\u258d         | 3\/65 [00:01<00:39,  1.58cell\/s]\r\nExecuting:   8%|\u258a         | 5\/65 [00:01<00:27,  2.16cell\/s]\r\nExecuting:   9%|\u2589         | 6\/65 [00:03<01:00,  1.03s\/cell]\r\nExecuting:  12%|\u2588\u258f        | 8\/65 [00:04<00:47,  1.19cell\/s]\r\nExecuting:  12%|\u2588\u258f        | 8\/65 [00:05<00:35,  1.59cell\/s]\r\n_____________________________ test_22_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_22_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\r\n            \"22_deployment_on_azure_kubernetes_service\"\r\n        ]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:83: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:104: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour...end_time': '2019-09-12T10:19:46.959285', 'duration': 5.817276, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [6]\":\r\nE           ---------------------------------------------------------------------------\r\nE           KeyError                                  Traceback (most recent call last)\r\nE           <ipython-input-6-af5043783823> in <module>\r\nE           ----> 1 docker_image = ws.images[\"image-classif-resnet18-f48\"]\r\nE           \r\nE           KeyError: 'image-classif-resnet18-f48'\r\n\r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:188: PapermillExecutionError\r\n----------------------------- Captured stderr call -----------------------------\r\n\r\nExecuting:   0%|          | 0\/36 [00:00<?, ?cell\/s]\r\nExecuting:   3%|\u258e         | 1\/36 [00:00<00:30,  1.16cell\/s]\r\nExecuting:  11%|\u2588         | 4\/36 [00:02<00:24,  1.32cell\/s]\r\nExecuting:  19%|\u2588\u2589        | 7\/36 [00:02<00:15,  1.84cell\/s]\r\nExecuting:  25%|\u2588\u2588\u258c       | 9\/36 [00:02<00:10,  2.52cell\/s]\r\nExecuting:  31%|\u2588\u2588\u2588       | 11\/36 [00:03<00:10,  2.47cell\/s]\r\nExecuting:  33%|\u2588\u2588\u2588\u258e      | 12\/36 [00:04<00:16,  1.50cell\/s]\r\nExecuting:  39%|\u2588\u2588\u2588\u2589      | 14\/36 [00:05<00:12,  1.81cell\/s]\r\nExecuting:  39%|\u2588\u2588\u2588\u2589      | 14\/36 [00:05<00:09,  2.41cell\/s]\r\n_____________________________ test_23_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_23_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\"23_aci_aks_web_service_testing\"]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:106: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:104: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour...end_time': '2019-09-12T10:19:53.061402', 'duration': 6.023939, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [6]\":\r\nE           ---------------------------------------------------------------------------\r\nE           KeyError                                  Traceback (most recent call last)\r\nE           <ipython-input-6-883397ed965d> in <module>\r\nE                 1 # Retrieve the web services\r\nE           ----> 2 aci_service = ws.webservices['im-classif-websvc']\r\nE                 3 aks_service = ws.webservices['aks-cpu-image-classif-web-svc']\r\nE           \r\nE           KeyError: 'im-classif-websvc'\r\n```\r\n\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Windows\/Linux.  -->\r\n<!--- * CPU\/GPU.  -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a Linux Data Science Virtual Machine one Azure with V100 GPU -->\r\n<!--- * Run unit test `test_classification_data.py` -->\r\n<!--- * ... -->\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The test `test_is_data_multilabel` for GPU model training should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":1569234937000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1568285443000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a regression issue in the azure credentials module where the import statement caused the loading of azureml.core.authentication when it was not needed. The suggested solution is to add an import statement before InteractiveLoginAuthentication is called and remove the import statement from the top.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/320",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.1,
        "Challenge_reading_time":224.21,
        "Challenge_repo_contributor_count":40.0,
        "Challenge_repo_fork_count":1127.0,
        "Challenge_repo_issue_count":684.0,
        "Challenge_repo_star_count":8985.0,
        "Challenge_repo_watch_count":288.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":155,
        "Challenge_solved_time":263.7483333333,
        "Challenge_title":"[BUG] pipeline azureml-notebook-test-linux-cpu failing",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":1547,
        "Discussion_body":"fixed with new pipeline and test machines",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":69.1163888889,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nA customer is experimenting with SageMaker batch transform with parquet and is interested is some form of local development to speedup iteration. Does SageMaker Batch Transform support local mode?",
        "Challenge_closed_time":1571303926000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1571055107000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of using local mode for SageMaker Batch Transform to speed up iteration while experimenting with parquet.",
        "Challenge_last_edit_time":1668610004528,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUtNtH0LyFSLCXc0xCV4hYkw\/sagemaker-batch-transform-local-mode",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":3.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":69.1163888889,
        "Challenge_title":"SageMaker Batch Transform local mode?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":336.0,
        "Challenge_word_count":34,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You can do local testing by running the container in serve mode as a docker. Then using Curl\/Postman to send an HTTP request and inspecting the response.  \n\nThe request can be CSV\/JSON or binary (a parquet file in your case).  \n\nIf you're able to run the Pytorch model in serve mode locally, then this local testing provides a lot of coverage before running in Batch Transform itself.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925565480,
        "Solution_link_count":0.0,
        "Solution_readability":8.7,
        "Solution_reading_time":4.58,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":719.2222222222,
        "Challenge_answer_count":0,
        "Challenge_body":"I have the following problem running on ddp mode with cometlogger.\r\nWhen I detach the logger from the trainer (i.e deleting`logger=comet_logger`) the code runs.\r\n```\r\nException has occurred: AttributeError\r\nCan't pickle local object 'SummaryTopic.__init__.<locals>.default'\r\n  File \"\/path\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\n  File \"\/path\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/path\/multiprocessing\/popen_fork.py\", line 20, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/path\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/path\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/path\/multiprocessing\/process.py\", line 112, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/path\/site-packages\/torch\/multiprocessing\/spawn.py\", line 162, in spawn\r\n    process.start()\r\n  File \"\/path\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 751, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))\r\n  File \"\/repo_path\/train.py\", line 158, in main_train\r\n    trainer.fit(model)\r\n  File \"\/repo_path\/train.py\", line 72, in main\r\n    main_train(model_class_pointer, hyperparams, logger)\r\n  File \"\/repo_path\/train.py\", line 167, in <module>\r\n    main()\r\n  File \"\/path\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/path\/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"\/path\/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"\/path\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/path\/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n```",
        "Challenge_closed_time":1591023634000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1588434434000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where test metrics are no longer being pushed to Comet.ML and possibly other logging destinations after updating to PyTorch Lightning 0.7.2. The issue can be reproduced by running a fast-run of training. The user has provided the environment details for reference.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/1704",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.6,
        "Challenge_reading_time":23.8,
        "Challenge_repo_contributor_count":444.0,
        "Challenge_repo_fork_count":2922.0,
        "Challenge_repo_issue_count":15116.0,
        "Challenge_repo_star_count":23576.0,
        "Challenge_repo_watch_count":234.0,
        "Challenge_score_count":6.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":719.2222222222,
        "Challenge_title":"Error running on ddp (can't pickle local object 'SummaryTopic) with comet logger",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":171,
        "Discussion_body":"@ceyzaguirre4 pls ^^",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":258.9256763889,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>We need to set a dataset folder in S3 as an artifact.  The folder has many sub-directories (only one layer though).<br>\nWhen I use the a <code>add_reference()<\/code> command it only stores the directory names of the top-level.<br>\nOf course, I could loop across it, but I\u2019m wondering if there is a command option to make the operation recursive?<\/p>\n<pre><code class=\"lang-auto\">run  = wandb.init(project=WB_PROJECT)\nart = wandb.Artifact(WB_ENTITY, type=WB_DATASET)\nart.add_reference(s3_full, max_objects=WB_MAX_OBJECTS_TO_UPLOAD)\nrun.log_artifact(art)\nwandb.finish()\n<\/code><\/pre>\n<p>EDIT 1: I conclude that the all files are not being added because the <code>Num Files<\/code> in the Artifact Overview shows only <code>5<\/code>.  If I click on the directories, it seems I can see the files, but I assume they are not actually there because of the <code>5<\/code> being reported for the number of files.<\/p>",
        "Challenge_closed_time":1663759922548,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662827790113,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to set a dataset folder in S3 as an artifact using the `add_reference()` command in WandB. However, the command only stores the directory names of the top-level and not the files within the sub-directories. The user is looking for a command option to make the operation recursive. The `Num Files` in the Artifact Overview shows only 5, indicating that not all files are being added.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/add-reference-with-nested-folders\/3092",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.1,
        "Challenge_reading_time":11.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":258.9256763889,
        "Challenge_title":"Add_reference() with nested folders",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":450.0,
        "Challenge_word_count":127,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Kevin,<\/p>\n<p>Thanks for the detailed explanation! I see your issue, I will create a request for this feature, thanks for reporting it! May I help you with any other issue?<\/p>\n<p>Best,<br>\nLuis<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.2,
        "Solution_reading_time":2.53,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":33.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":43.9051222222,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi,<\/p>\n<p>I am having trouble accessing run data keys in several of my runs. Specifically, I have logged a metric in my code, the metric is tracked in the online wandb UI, but when I try accessing the data using the following code<\/p>\n<pre><code class=\"lang-auto\">import wandb\napi = wandb.Api()\nrun = api.run(\"xxxxxx\")\nrun.history()[['_step', 'metric_name']]\n<\/code><\/pre>\n<p>It throws a <code>KeyError: \"['metric_name'] not in index\"<\/code>.<\/p>\n<p>When I print out <code>run.history()<\/code> in table format, it does show \u2018metric_name\u2019 as one of the columns; \u2018metric_name\u2019 also appears as a key in <code>run.summary<\/code>. I wonder what is the issue here?<\/p>\n<p>Would appreciate any help. Thank you!<\/p>",
        "Challenge_closed_time":1670943419675,
        "Challenge_comment_count":0,
        "Challenge_created_time":1670785361235,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to access run data keys in several of their runs despite logging a metric in their code and tracking it in the online wandb UI. When attempting to access the data using the provided code, a KeyError is thrown. The metric_name appears as a column in the run history table and as a key in run.summary, leaving the user unsure of the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/cannot-access-run-data-via-run-history\/3530",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":7.8,
        "Challenge_reading_time":9.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":43.9051222222,
        "Challenge_title":"Cannot access run data via run.history()",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":131.0,
        "Challenge_word_count":104,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/toruowo\">@toruowo<\/a> thank you for writing in! Can you please change your last line to:<\/p>\n<pre><code class=\"lang-auto\">run.history(keys=['_step', 'metric_name'])\n<\/code><\/pre>\n<p>Would this work for you?  Please let me know if you have any further issues or questions. You may also find some more <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide#public-api-examples\">API examples here<\/a> if that helps.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.8,
        "Solution_reading_time":5.95,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":51.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.0441666667,
        "Challenge_answer_count":0,
        "Challenge_body":"When only `zn.Method` without `zn.params` is used in a Node the `dvc.yaml` will not depend on the `params.yaml`.\r\n",
        "Challenge_closed_time":1643235530000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643228171000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user needs to update dvc for dvc-bench to work with versions greater than 2.0.0, but ignoring lockfile is not allowed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/zincware\/ZnTrack\/issues\/211",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":2.8,
        "Challenge_reading_time":1.95,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":629.0,
        "Challenge_repo_star_count":35.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2.0441666667,
        "Challenge_title":"zn.Method does not add params to `dvc.yaml`",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":24,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1476987111723,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Canada",
        "Answerer_reputation_count":2021.0,
        "Answerer_view_count":134.0,
        "Challenge_adjusted_solved_time":1.4678261111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have 50TB of uncompressed data (images) that is in dozens of tar.gz files in S3. I'm training tensorflow models with a dozen of these tar.gz files at a time. I would like to use a Sagemaker training job to pull this data and unpack it before training. Is this possible? Do I have to change the way that the data is stored before running training?<\/p>",
        "Challenge_closed_time":1611858171047,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611852886873,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has 50TB of uncompressed image data stored in dozens of tar.gz files in S3 and wants to know if they can be used for training data in Sagemaker. They are training tensorflow models with a dozen of these files at a time and want to know if they need to change the way the data is stored before running training.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65941675",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":3.5,
        "Challenge_reading_time":4.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1.4678261111,
        "Challenge_title":"Can gzip tar files be used for training data in Sagemaker?",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":433.0,
        "Challenge_word_count":76,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1476987111723,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Canada",
        "Poster_reputation_count":2021.0,
        "Poster_view_count":134.0,
        "Solution_body":"<p><strong>Short Answer<\/strong> : No<\/p>\n<p><strong>Long Answer<\/strong>:\nThe recommended way to use Sagemaker with very large datasets is to use the Pipe API (as opposed to the File Api) which streams data to the training image rather than downloading the data. To take advantage of the Pipe API the data will need to be in one of the supported file types: <strong>text records, TFRecord or Protobuf<\/strong><\/p>\n<p>The benefits are<\/p>\n<ol>\n<li>reducing delay when the container is launched<\/li>\n<li>not needing to scale the instance storage to the size of the training data<\/li>\n<li>increasing throughput by moving most preprocessing before model training<\/li>\n<\/ol>\n<p>References:<\/p>\n<ol>\n<li><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/<\/a><\/li>\n<li><a href=\"https:\/\/julsimon.medium.com\/making-amazon-sagemaker-and-tensorflow-work-for-you-893365184233\" rel=\"nofollow noreferrer\">https:\/\/julsimon.medium.com\/making-amazon-sagemaker-and-tensorflow-work-for-you-893365184233<\/a> (This is a fantastic resource which answers a lot of questions regarding using Sagemaker on very large datasets)<\/li>\n<li><a href=\"https:\/\/julsimon.medium.com\/deep-dive-on-tensorflow-training-with-amazon-sagemaker-and-amazon-s3-12038828075c\" rel=\"nofollow noreferrer\">https:\/\/julsimon.medium.com\/deep-dive-on-tensorflow-training-with-amazon-sagemaker-and-amazon-s3-12038828075c<\/a><\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":20.1,
        "Solution_reading_time":21.09,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":132.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1403539373872,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Atlanta, GA",
        "Answerer_reputation_count":458.0,
        "Answerer_view_count":119.0,
        "Challenge_adjusted_solved_time":68.3956241667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm new to mlflow so I may misunderstand how things are supposed to work on a fundamental level.<\/p>\n<p>However when I try to do the following:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>TRACKING_URI = os.path.join(\n    &quot;hdfs:\/\/namenode\/user\/userid\/&quot;,\n    &quot;mlflow&quot;,\n    &quot;anomaly_detection&quot;,\n)\n        \nmlflow.set_tracking_uri(TRACKING_URI)\nclient = mlflow.tracking.MlflowClient(TRACKING_URI)\n<\/code><\/pre>\n<p>I get the following error:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>UnsupportedModelRegistryStoreURIException:  Model registry functionality is unavailable; got unsupported URI 'hdfs:\/\/nameservice1\/user\/rxb427\/mlflow\/anomaly_detection' for model registry data storage. Supported URI schemes are: ['', 'file', 'databricks', 'http', 'https', 'postgresql', 'mysql', 'sqlite', 'mssql']. See https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#storage for how to run an MLflow server against one of the supported backend storage locations.\n<\/code><\/pre>\n<p>Within the above link provided by the error it states that hdfs is supported. Bug or am I missing something?<\/p>",
        "Challenge_closed_time":1594661070470,
        "Challenge_comment_count":0,
        "Challenge_created_time":1594412490347,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to set the tracking URI in mlflow using an HDFS path, but is encountering an error stating that the URI is unsupported for model registry data storage. The user is unsure if this is a bug or if they are missing something.",
        "Challenge_last_edit_time":1594414846223,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62841756",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":12.6,
        "Challenge_reading_time":15.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":69.0500341667,
        "Challenge_title":"Can't use HDFS path to set_tracking_uri in mlflow within python",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":621.0,
        "Challenge_word_count":118,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403539373872,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Atlanta, GA",
        "Poster_reputation_count":458.0,
        "Poster_view_count":119.0,
        "Solution_body":"<p>Ok. So it looks like while the ARTIFACTS STORE does support hdfs, you have to use either file or a sql like for the BACKEND STORE.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.7,
        "Solution_reading_time":1.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":26.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3487.3783333333,
        "Challenge_answer_count":0,
        "Challenge_body":"I'm using clustering module of pycaret and the integration with mlflow but I have problems because I think it doesn't save all artifacs and the status is always failed.\r\n![image](https:\/\/user-images.githubusercontent.com\/12554263\/101971863-66f70d00-3c02-11eb-9710-01cf228fca1b.png)\r\n\r\nThis is my code:\r\n\r\n```python\r\nfrom pycaret.clustering import *\r\n\r\npostpaid_exp = setup(postpaid_sample,\r\n                     ignore_features=ignore_features,\r\n                     numeric_features=numeric_features,\r\n                     normalize=True,\r\n                     normalize_method='robust',\r\n                     remove_multicollinearity=True,\r\n                     multicollinearity_threshold=0.7,\r\n                     log_experiment=True,\r\n                     log_plots=True,\r\n                     log_profile=True,\r\n                     log_data=True,\r\n                     profile=False,\r\n                     experiment_name='pospatid_segmentation',\r\n                     session_id=123)\r\n\r\n# Create model with six clusters\r\nmodel_kmeans =  create_model(model='kmeans', num_clusters=6)\r\n```\r\nMy logs are the following\r\n\r\n```\r\n2020-12-11 22:39:07,118:INFO:PyCaret Supervised Module\r\n2020-12-11 22:39:07,118:INFO:ML Usecase: clustering\r\n2020-12-11 22:39:07,118:INFO:version 2.2.0\r\n2020-12-11 22:39:07,118:INFO:Initializing setup()\r\n2020-12-11 22:39:07,119:INFO:setup(target=None, ml_usecase=clustering, available_plots={'cluster': 'Cluster PCA Plot (2d)', 'tsne': 'Cluster TSnE (3d)', 'elbow': 'Elbow', 'silhouette': 'Silhouette', 'distance': 'Distance', 'distribution': 'Distribution'}, train_size=0.7, test_data=None, preprocess=True, imputation_type=simple, iterative_imputation_iters=5, categorical_features=None, categorical_imputation=mode, categorical_iterative_imputer=lightgbm, ordinal_features=None, high_cardinality_features=None, high_cardinality_method=frequency, numeric_features=['avg_dias_bancos_3m', 'avg_dias_app_pagos_3m', 'avg_dias_viajes_3m', 'avg_dias_compras_3m', 'avg_dias_mb_total_3m', 'avg_mb_total_3m', 'avg_q_apps_3m', 'ate_wh_sum_dias_3m', 'LEADs_tot_3m', 'tot_dias_appmov_movil_3m', 'avg_days_out_voice_tot_3m', 'meses_pagodig_3m'], numeric_imputation=mean, numeric_iterative_imputer=lightgbm, date_features=None, ignore_features=['periodo', 'telefono', 'anexo', 'tot_dias_appmov_fija_3m', 'avg_dias_vid_mus_3m'], normalize=True, normalize_method=robust, transformation=False, transformation_method=yeo-johnson, handle_unknown_categorical=True, unknown_categorical_method=least_frequent, pca=False, pca_method=linear, pca_components=None, ignore_low_variance=False, combine_rare_levels=False, rare_level_threshold=0.1, bin_numeric_features=None, remove_outliers=False, outliers_threshold=0.05, remove_multicollinearity=True, multicollinearity_threshold=0.7, remove_perfect_collinearity=False, create_clusters=False, cluster_iter=20, polynomial_features=False, polynomial_degree=2, trigonometry_features=False, polynomial_threshold=0.1, group_features=None, group_names=None, feature_selection=False, feature_selection_threshold=0.8, feature_selection_method=classic, feature_interaction=False, feature_ratio=False, interaction_threshold=0.01, fix_imbalance=False, fix_imbalance_method=None, transform_target=False, transform_target_method=box-cox, data_split_shuffle=False, data_split_stratify=False, fold_strategy=kfold, fold=10, fold_shuffle=False, fold_groups=None, n_jobs=-1, use_gpu=False, custom_pipeline=None, html=True, session_id=123, log_experiment=True, experiment_name=pospatid_segmentation, log_plots=['cluster', 'distribution', 'elbow'], log_profile=True, log_data=True, silent=False, verbose=True, profile=False, display=None)\r\n2020-12-11 22:39:07,119:INFO:Checking environment\r\n2020-12-11 22:39:07,119:INFO:python_version: 3.8.5\r\n2020-12-11 22:39:07,119:INFO:python_build: ('default', 'Aug  5 2020 09:44:06')\r\n2020-12-11 22:39:07,119:INFO:machine: AMD64\r\n2020-12-11 22:39:07,120:INFO:platform: Windows-10-10.0.18362-SP0\r\n2020-12-11 22:39:07,121:WARNING:cannot find psutil installation. memory not traceable. Install psutil using pip to enable memory logging.\r\n2020-12-11 22:39:07,122:INFO:Checking libraries\r\n2020-12-11 22:39:07,122:INFO:pd==1.1.4\r\n2020-12-11 22:39:07,122:INFO:numpy==1.19.4\r\n2020-12-11 22:39:07,122:INFO:sklearn==0.23.2\r\n2020-12-11 22:39:07,156:INFO:xgboost==1.2.0\r\n2020-12-11 22:39:07,156:INFO:lightgbm==3.0.0\r\n2020-12-11 22:39:07,170:INFO:catboost==0.24.1\r\n2020-12-11 22:39:07,901:INFO:mlflow==1.11.0\r\n2020-12-11 22:39:07,901:INFO:Checking Exceptions\r\n2020-12-11 22:39:07,901:INFO:Declaring global variables\r\n2020-12-11 22:39:07,901:INFO:USI: cd5c\r\n2020-12-11 22:39:07,901:INFO:pycaret_globals: {'_available_plots', 'master_model_container', 'display_container', 'imputation_classifier', 'logging_param', 'seed', 'transform_target_param', 'experiment__', 'transform_target_method_param', 'iterative_imputation_iters_param', 'fold_groups_param', 'fix_imbalance_param', 'prep_pipe', 'exp_name_log', '_all_metrics', 'html_param', '_ml_usecase', 'USI', 'imputation_regressor', 'stratify_param', 'fold_generator', 'fix_imbalance_method_param', '_all_models', 'gpu_param', 'target_param', '_gpu_n_jobs_param', 'log_plots_param', 'pycaret_globals', 'fold_shuffle_param', '_all_models_internal', 'fold_param', 'create_model_container', 'data_before_preprocess', '_internal_pipeline', 'X', 'n_jobs_param'}\r\n2020-12-11 22:39:07,901:INFO:Preparing display monitor\r\n2020-12-11 22:39:07,901:INFO:Preparing display monitor\r\n2020-12-11 22:39:07,914:INFO:Importing libraries\r\n2020-12-11 22:39:07,914:INFO:Copying data for preprocessing\r\n2020-12-11 22:39:07,927:INFO:Declaring preprocessing parameters\r\n2020-12-11 22:39:07,940:INFO:Creating preprocessing pipeline\r\n2020-12-11 22:39:08,059:INFO:Preprocessing pipeline created successfully\r\n2020-12-11 22:39:08,060:ERROR:(Process Exit): setup has been interupted with user command 'quit'. setup must rerun.\r\n2020-12-11 22:39:08,060:INFO:Creating global containers\r\n2020-12-11 22:39:08,061:INFO:Internal pipeline: Pipeline(memory=None, steps=[('empty_step', 'passthrough')], verbose=False)\r\n2020-12-11 22:39:10,064:INFO:Creating grid variables\r\n2020-12-11 22:39:10,101:INFO:Logging experiment in MLFlow\r\n2020-12-11 22:39:10,108:WARNING:Couldn't create mlflow experiment. Exception:\r\n2020-12-11 22:39:10,185:WARNING:Traceback (most recent call last):\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pycaret\\internal\\tabular.py\", line 1668, in setup\r\n    mlflow.create_experiment(exp_name_log)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\tracking\\fluent.py\", line 365, in create_experiment\r\n    return MlflowClient().create_experiment(name, artifact_location)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\tracking\\client.py\", line 184, in create_experiment\r\n    return self._tracking_client.create_experiment(name, artifact_location)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py\", line 142, in create_experiment\r\n    return self.store.create_experiment(name=name, artifact_location=artifact_location,)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 288, in create_experiment\r\n    self._validate_experiment_name(name)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 281, in _validate_experiment_name\r\n    raise MlflowException(\r\nmlflow.exceptions.MlflowException: Experiment 'pospatid_segmentation' already exists.\r\n\r\n2020-12-11 22:39:10,490:INFO:SubProcess save_model() called ==================================\r\n2020-12-11 22:39:10,501:INFO:Initializing save_model()\r\n2020-12-11 22:39:10,501:INFO:save_model(model=Pipeline(memory=None,\r\n         steps=[('dtypes',\r\n                 DataTypes_Auto_infer(categorical_features=[],\r\n                                      display_types=True,\r\n                                      features_todrop=['periodo', 'telefono',\r\n                                                       'anexo',\r\n                                                       'tot_dias_appmov_fija_3m',\r\n                                                       'avg_dias_vid_mus_3m'],\r\n                                      id_columns=[],\r\n                                      ml_usecase='classification',\r\n                                      numerical_features=['avg_dias_bancos_3m',\r\n                                                          'avg_dias_app_pagos_3m',\r\n                                                          'avg_dias_viajes_3m',\r\n                                                          'avg_dias_compras_3m',\r\n                                                          'av...\r\n                ('dummy', Dummify(target='UNSUPERVISED_DUMMY_TARGET')),\r\n                ('fix_perfect', 'passthrough'),\r\n                ('clean_names', Clean_Colum_Names()),\r\n                ('feature_select', 'passthrough'),\r\n                ('fix_multi',\r\n                 Fix_multicollinearity(correlation_with_target_preference=None,\r\n                                       correlation_with_target_threshold=0.0,\r\n                                       target_variable='UNSUPERVISED_DUMMY_TARGET',\r\n                                       threshold=0.7)),\r\n                ('dfs', 'passthrough'), ('pca', 'passthrough')],\r\n         verbose=False), model_name=Transformation Pipeline, prep_pipe_=Pipeline(memory=None,\r\n         steps=[('dtypes',\r\n                 DataTypes_Auto_infer(categorical_features=[],\r\n                                      display_types=True,\r\n                                      features_todrop=['periodo', 'telefono',\r\n                                                       'anexo',\r\n                                                       'tot_dias_appmov_fija_3m',\r\n                                                       'avg_dias_vid_mus_3m'],\r\n                                      id_columns=[],\r\n                                      ml_usecase='classification',\r\n                                      numerical_features=['avg_dias_bancos_3m',\r\n                                                          'avg_dias_app_pagos_3m',\r\n                                                          'avg_dias_viajes_3m',\r\n                                                          'avg_dias_compras_3m',\r\n                                                          'av...\r\n                ('dummy', Dummify(target='UNSUPERVISED_DUMMY_TARGET')),\r\n                ('fix_perfect', 'passthrough'),\r\n                ('clean_names', Clean_Colum_Names()),\r\n                ('feature_select', 'passthrough'),\r\n                ('fix_multi',\r\n                 Fix_multicollinearity(correlation_with_target_preference=None,\r\n                                       correlation_with_target_threshold=0.0,\r\n                                       target_variable='UNSUPERVISED_DUMMY_TARGET',\r\n                                       threshold=0.7)),\r\n                ('dfs', 'passthrough'), ('pca', 'passthrough')],\r\n         verbose=False), verbose=False)\r\n2020-12-11 22:39:10,501:INFO:Adding model into prep_pipe\r\n2020-12-11 22:39:10,506:WARNING:Only Model saved as it was a pipeline.\r\n2020-12-11 22:39:10,530:INFO:Transformation Pipeline.pkl saved in current working directory\r\n2020-12-11 22:39:10,535:INFO:Pipeline(memory=None,\r\n         steps=[('dtypes',\r\n                 DataTypes_Auto_infer(categorical_features=[],\r\n                                      display_types=True,\r\n                                      features_todrop=['periodo', 'telefono',\r\n                                                       'anexo',\r\n                                                       'tot_dias_appmov_fija_3m',\r\n                                                       'avg_dias_vid_mus_3m'],\r\n                                      id_columns=[],\r\n                                      ml_usecase='classification',\r\n                                      numerical_features=['avg_dias_bancos_3m',\r\n                                                          'avg_dias_app_pagos_3m',\r\n                                                          'avg_dias_viajes_3m',\r\n                                                          'avg_dias_compras_3m',\r\n                                                          'av...\r\n                ('dummy', Dummify(target='UNSUPERVISED_DUMMY_TARGET')),\r\n                ('fix_perfect', 'passthrough'),\r\n                ('clean_names', Clean_Colum_Names()),\r\n                ('feature_select', 'passthrough'),\r\n                ('fix_multi',\r\n                 Fix_multicollinearity(correlation_with_target_preference=None,\r\n                                       correlation_with_target_threshold=0.0,\r\n                                       target_variable='UNSUPERVISED_DUMMY_TARGET',\r\n                                       threshold=0.7)),\r\n                ('dfs', 'passthrough'), ('pca', 'passthrough')],\r\n         verbose=False)\r\n2020-12-11 22:39:10,535:INFO:save_model() succesfully completed......................................\r\n2020-12-11 22:39:10,536:INFO:SubProcess save_model() end ==================================\r\n2020-12-11 22:40:03,332:INFO:create_model_container: 0\r\n2020-12-11 22:40:03,332:INFO:master_model_container: 0\r\n2020-12-11 22:40:03,332:INFO:display_container: 0\r\n2020-12-11 22:40:03,336:INFO:Pipeline(memory=None,\r\n         steps=[('dtypes',\r\n                 DataTypes_Auto_infer(categorical_features=[],\r\n                                      display_types=True,\r\n                                      features_todrop=['periodo', 'telefono',\r\n                                                       'anexo',\r\n                                                       'tot_dias_appmov_fija_3m',\r\n                                                       'avg_dias_vid_mus_3m'],\r\n                                      id_columns=[],\r\n                                      ml_usecase='classification',\r\n                                      numerical_features=['avg_dias_bancos_3m',\r\n                                                          'avg_dias_app_pagos_3m',\r\n                                                          'avg_dias_viajes_3m',\r\n                                                          'avg_dias_compras_3m',\r\n                                                          'av...\r\n                ('dummy', Dummify(target='UNSUPERVISED_DUMMY_TARGET')),\r\n                ('fix_perfect', 'passthrough'),\r\n                ('clean_names', Clean_Colum_Names()),\r\n                ('feature_select', 'passthrough'),\r\n                ('fix_multi',\r\n                 Fix_multicollinearity(correlation_with_target_preference=None,\r\n                                       correlation_with_target_threshold=0.0,\r\n                                       target_variable='UNSUPERVISED_DUMMY_TARGET',\r\n                                       threshold=0.7)),\r\n                ('dfs', 'passthrough'), ('pca', 'passthrough')],\r\n         verbose=False)\r\n2020-12-11 22:40:03,336:INFO:setup() succesfully completed......................................\r\n2020-12-11 22:40:07,628:INFO:Initializing create_model()\r\n2020-12-11 22:40:07,628:INFO:create_model(estimator=kmeans, num_clusters=6, fraction=0.05, ground_truth=None, round=4, fit_kwargs=None, verbose=True, system=True, raise_num_clusters=False, display=None, kwargs={})\r\n2020-12-11 22:40:07,628:INFO:Checking exceptions\r\n2020-12-11 22:40:07,629:INFO:Preparing display monitor\r\n2020-12-11 22:40:07,645:INFO:Importing libraries\r\n2020-12-11 22:40:07,652:INFO:Importing untrained model\r\n2020-12-11 22:40:07,662:INFO:K-Means Clustering Imported succesfully\r\n2020-12-11 22:40:07,670:INFO:Fitting Model\r\n2020-12-11 22:42:30,467:INFO:KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0)\r\n2020-12-11 22:42:30,467:INFO:create_models() succesfully completed......................................\r\n2020-12-11 22:42:30,467:INFO:Creating MLFlow logs\r\n2020-12-11 22:42:30,481:INFO:Model: K-Means Clustering\r\n2020-12-11 22:42:30,518:INFO:logged params: {'algorithm': 'auto', 'copy_x': True, 'init': 'k-means++', 'max_iter': 300, 'n_clusters': 6, 'n_init': 10, 'n_jobs': -1, 'precompute_distances': 'deprecated', 'random_state': 123, 'tol': 0.0001, 'verbose': 0}\r\n2020-12-11 22:42:30,557:INFO:SubProcess plot_model() called ==================================\r\n2020-12-11 22:42:30,557:INFO:Initializing plot_model()\r\n2020-12-11 22:42:30,557:INFO:plot_model(plot=cluster, fold=None, verbose=False, display=None, estimator=KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0), feature_name=None, fit_kwargs=None, groups=None, label=False, save=True, scale=1, system=False)\r\n2020-12-11 22:42:30,557:INFO:Checking exceptions\r\n2020-12-11 22:42:30,558:INFO:Preloading libraries\r\n2020-12-11 22:42:30,558:INFO:Copying training dataset\r\n2020-12-11 22:42:30,560:INFO:Plot type: cluster\r\n2020-12-11 22:42:31,493:INFO:SubProcess assign_model() called ==================================\r\n2020-12-11 22:42:31,494:INFO:Initializing assign_model()\r\n2020-12-11 22:42:31,494:INFO:assign_model(model=Pipeline(memory=None,\r\n         steps=[('empty_step', 'passthrough'),\r\n                ('actual_estimator',\r\n                 KMeans(algorithm='auto', copy_x=True, init='k-means++',\r\n                        max_iter=300, n_clusters=6, n_init=10, n_jobs=-1,\r\n                        precompute_distances='deprecated', random_state=123,\r\n                        tol=0.0001, verbose=0))],\r\n         verbose=False), transformation=True, score=True, verbose=False)\r\n2020-12-11 22:42:31,494:INFO:Checking exceptions\r\n2020-12-11 22:42:31,495:INFO:Determining Trained Model\r\n2020-12-11 22:42:31,495:INFO:Trained Model : K-Means Clustering\r\n2020-12-11 22:42:31,495:INFO:Copying data\r\n2020-12-11 22:42:31,496:INFO:Transformation param set to True. Assigned clusters are attached on transformed dataset.\r\n2020-12-11 22:42:31,529:INFO:(90000, 12)\r\n2020-12-11 22:42:31,529:INFO:assign_model() succesfully completed......................................\r\n2020-12-11 22:42:31,530:INFO:SubProcess assign_model() end ==================================\r\n2020-12-11 22:42:31,541:INFO:Fitting PCA()\r\n2020-12-11 22:42:31,908:INFO:Sorting dataframe\r\n2020-12-11 22:42:31,974:INFO:Rendering Visual\r\n2020-12-11 22:42:41,765:INFO:Saving 'Cluster PCA Plot (2d).html' in current active directory\r\n2020-12-11 22:42:41,765:INFO:Visual Rendered Successfully\r\n2020-12-11 22:42:42,286:INFO:plot_model() succesfully completed......................................\r\n2020-12-11 22:42:42,739:INFO:Initializing plot_model()\r\n2020-12-11 22:42:42,739:INFO:plot_model(plot=distribution, fold=None, verbose=False, display=None, estimator=KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0), feature_name=None, fit_kwargs=None, groups=None, label=False, save=True, scale=1, system=False)\r\n2020-12-11 22:42:42,739:INFO:Checking exceptions\r\n2020-12-11 22:42:42,739:INFO:Preloading libraries\r\n2020-12-11 22:42:42,739:INFO:Copying training dataset\r\n2020-12-11 22:42:42,741:INFO:Plot type: distribution\r\n2020-12-11 22:42:42,741:INFO:SubProcess assign_model() called ==================================\r\n2020-12-11 22:42:42,742:INFO:Initializing assign_model()\r\n2020-12-11 22:42:42,742:INFO:assign_model(model=Pipeline(memory=None,\r\n         steps=[('empty_step', 'passthrough'),\r\n                ('actual_estimator',\r\n                 KMeans(algorithm='auto', copy_x=True, init='k-means++',\r\n                        max_iter=300, n_clusters=6, n_init=10, n_jobs=-1,\r\n                        precompute_distances='deprecated', random_state=123,\r\n                        tol=0.0001, verbose=0))],\r\n         verbose=False), transformation=False, score=True, verbose=False)\r\n2020-12-11 22:42:42,742:INFO:Checking exceptions\r\n2020-12-11 22:42:42,742:INFO:Determining Trained Model\r\n2020-12-11 22:42:42,742:INFO:Trained Model : K-Means Clustering\r\n2020-12-11 22:42:42,742:INFO:Copying data\r\n2020-12-11 22:42:42,793:INFO:(90000, 18)\r\n2020-12-11 22:42:42,793:INFO:assign_model() succesfully completed......................................\r\n2020-12-11 22:42:42,794:INFO:SubProcess assign_model() end ==================================\r\n2020-12-11 22:42:42,794:INFO:Sorting dataframe\r\n2020-12-11 22:42:42,925:INFO:Rendering Visual\r\n2020-12-11 22:42:48,837:INFO:Saving 'Distribution.html' in current active directory\r\n2020-12-11 22:42:48,837:INFO:Visual Rendered Successfully\r\n2020-12-11 22:42:48,979:INFO:plot_model() succesfully completed......................................\r\n2020-12-11 22:42:49,583:INFO:Initializing plot_model()\r\n2020-12-11 22:42:49,584:INFO:plot_model(plot=elbow, fold=None, verbose=False, display=None, estimator=KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0), feature_name=None, fit_kwargs=None, groups=None, label=False, save=True, scale=1, system=False)\r\n2020-12-11 22:42:49,584:INFO:Checking exceptions\r\n2020-12-11 22:42:49,584:INFO:Preloading libraries\r\n2020-12-11 22:42:49,584:INFO:Copying training dataset\r\n2020-12-11 22:42:49,586:INFO:Plot type: elbow\r\n2020-12-11 22:42:49,690:INFO:Fitting Model\r\n2020-12-11 22:43:12,604:INFO:Saving 'Elbow.png' in current active directory\r\n2020-12-11 22:43:13,207:INFO:Visual Rendered Successfully\r\n2020-12-11 22:43:13,325:INFO:plot_model() succesfully completed......................................\r\n2020-12-11 22:43:13,340:INFO:SubProcess plot_model() end ==================================\r\n2020-12-11 22:43:13,341:WARNING:Couldn't infer MLFlow signature.\r\n2020-12-11 22:43:13,352:ERROR:_mlflow_log_model() for KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0) raised an exception:\r\n2020-12-11 22:43:13,431:ERROR:Traceback (most recent call last):\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pycaret\\internal\\tabular.py\", line 2631, in create_model_unsupervised\r\n    _mlflow_log_model(\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pycaret\\internal\\tabular.py\", line 9942, in _mlflow_log_model\r\n    mlflow.sklearn.log_model(\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\sklearn\\__init__.py\", line 290, in log_model\r\n    return Model.log(\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\models\\model.py\", line 160, in log\r\n    flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\sklearn\\__init__.py\", line 171, in save_model\r\n    _save_example(mlflow_model, input_example, path)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\models\\utils.py\", line 131, in _save_example\r\n    example = _Example(input_example)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\models\\utils.py\", line 67, in __init__\r\n    input_example = pd.DataFrame.from_dict(input_example)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\frame.py\", line 1309, in from_dict\r\n    return cls(data, index=index, columns=columns, dtype=dtype)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\frame.py\", line 468, in __init__\r\n    mgr = init_dict(data, index, columns, dtype=dtype)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 283, in init_dict\r\n    return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 78, in arrays_to_mgr\r\n    index = extract_index(arrays)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 387, in extract_index\r\n    raise ValueError(\"If using all scalar values, you must pass an index\")\r\nValueError: If using all scalar values, you must pass an index\r\n\r\n2020-12-11 22:43:13,432:INFO:Uploading results into container\r\n2020-12-11 22:43:13,435:INFO:Uploading model into container now\r\n2020-12-11 22:43:13,440:INFO:create_model_container: 1\r\n2020-12-11 22:43:13,440:INFO:master_model_container: 1\r\n2020-12-11 22:43:13,440:INFO:display_container: 1\r\n2020-12-11 22:43:13,440:INFO:KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0)\r\n2020-12-11 22:43:13,440:INFO:create_model() succesfully completed......................................\r\n\r\n```\r\n\r\nI'm using Pycaret version : 2.2.0",
        "Challenge_closed_time":1620299767000,
        "Challenge_comment_count":5,
        "Challenge_created_time":1607745205000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while using the pytorch.log_model function in the CIFAR pytorch distributed example. Although the model training was completed and saved, the driver logs showed an unexpected error related to mlflow.utils.environment while inferring pip requirements. The user tried different environment variations but faced the same outcome.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/931",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":25.9,
        "Challenge_reading_time":288.86,
        "Challenge_repo_contributor_count":105.0,
        "Challenge_repo_fork_count":1603.0,
        "Challenge_repo_issue_count":2975.0,
        "Challenge_repo_star_count":7363.0,
        "Challenge_repo_watch_count":128.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":96,
        "Challenge_solved_time":3487.3783333333,
        "Challenge_title":"MLFlow doesn't save model artifact and some plots - Clustering",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":1212,
        "Discussion_body":"@DXcarlos I have tried to reproduce the error on `jewellery` dataset available on our GitHub repository and I couldn't reproduce this error.\r\n\r\nIs it possible for you to share the Notebook along with the dataset so we can reproduce the error and troubleshoot what is causing this?\r\n\r\nI am including @Yard1 in this thread to see if he can understand what's going on with the log file you shared above. Antoni, maybe something specific to the dataset.  @pycaret @Yard1 How can I share you privately? @DXcarlos You can private message on our Slack channel. If you are still not there, you can join using the following link:\r\n\r\nhttps:\/\/join.slack.com\/t\/pycaret\/shared_invite\/zt-kdoe7hee-yvNANPHXPM9VtK7R6Npx4Q\r\n\r\n Stale issue message @DXcarlos , we will close out this issue for now. Please feel free to reopen if you want.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1489593596310,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":162.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":0.2549580556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am retrieving a list of image filenames from DynamoDB and using those image filenames to replace the default <code>src=<\/code> image in a portion of a website.<\/p>\n\n<p>I'm a JS novice, so I'm certainly missing something, but my function is returning the list of filenames too late.<\/p>\n\n<p>My inline script is:<\/p>\n\n<pre><code>&lt;script&gt;\n        customElements.whenDefined( 'crowd-bounding-box' ).then( () =&gt; {  \n        var imgBox = document.getElementById('annotator');\n        newImg = imageslist();\n        console.log(\"Result of newImg is: \" + newImg);\n        imgBox.src = \"https:\/\/my-images-bucket.s3.amazonaws.com\/\" + newImg;\n    } )\n&lt;\/script&gt;\n<\/code><\/pre>\n\n<p>My JS function is:<\/p>\n\n<pre><code>async function imageslist() {\n    const username = \"sampleuser\";\n    const params = {\n        TableName: \"mytable\",\n        FilterExpression: \"attribute_not_exists(\" + username + \")\",\n        ReturnConsumedCapacity: \"NONE\"\n    };\n    try {\n        var data = await ddb.scan(params).promise()\n        var imglist = [];\n        for(let i = 0; i &lt; data.Items.length; i++) {\n            imglist.push(data.Items[i].img.S);\n        };\n        imglist.sort();\n        var firstimg = imglist[0];\n        console.log(firstimg);\n        return imglist\n    } catch(error) {\n        console.error(error);\n    }\n}\n<\/code><\/pre>\n\n<p>The console reports <code>Result of newImg is: [object Promise]<\/code> and shortly after that, it reports the expected filename.  After the page has been rendered, I can input <code>newImg<\/code> in the console and I receive the expected result.<\/p>\n\n<p>Am I using the <strong>await<\/strong> syntax improperly?<\/p>\n\n<p>Side note: This site uses the Crowd HTML Elements (for Ground Truth and Mechanical Turk), so I'm forced to have the <code>src=<\/code> attribute present in my <code>crowd-bounding-box<\/code> tag and it must be a non-zero value.  I'm loading a default image and replacing it with another image.<\/p>",
        "Challenge_closed_time":1588365818456,
        "Challenge_comment_count":1,
        "Challenge_created_time":1588364900607,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is retrieving a list of image filenames from DynamoDB and using them to replace the default image in a portion of a website. However, the function is returning the list of filenames too late, and the console reports \"Result of newImg is: [object Promise].\" The user is unsure if they are using the await syntax improperly.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61550297",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":10.4,
        "Challenge_reading_time":22.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":0.2549580556,
        "Challenge_title":"Await returns too soon",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":63.0,
        "Challenge_word_count":208,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1483444144907,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hoth",
        "Poster_reputation_count":312.0,
        "Poster_view_count":65.0,
        "Solution_body":"<p>Anytime you use the <code>await<\/code> keyword, you must use the <code>async<\/code> keyword before the function definition (which you have done). The thing is, any time an async function is called, it will always return a <code>Promise<\/code> object since it expects that some asynchronous task will take place within the function.<\/p>\n\n<p>Therefore,you'll need to <code>await<\/code> the result of the imageslist function and make the surrounding function <code>async<\/code>.<\/p>\n\n<pre class=\"lang-js prettyprint-override\"><code>&lt;script&gt;\n    customElements.whenDefined( 'crowd-bounding-box' ).then( async () =&gt; {  \n        var imgBox = document.getElementById('annotator');\n        newImg = await imageslist();\n        console.log(\"Result of newImg is: \" + newImg);\n        imgBox.src = \"https:\/\/my-images-bucket.s3.amazonaws.com\/\" + newImg;\n    } )\n&lt;\/script&gt;\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.1,
        "Solution_reading_time":10.99,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":90.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1443426419048,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation_count":842.0,
        "Answerer_view_count":219.0,
        "Challenge_adjusted_solved_time":133.5554527778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Azure ML support says to me that delimiter must be comma, this would cause too much hassle with data having semicolon as separator and with a lot of commas in the cell values. <\/p>\n\n<p>So how to process semicolon separated CSV files in Azure ML? <\/p>",
        "Challenge_closed_time":1485838771543,
        "Challenge_comment_count":1,
        "Challenge_created_time":1485357971913,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in processing semicolon separated CSV files in Azure ML as the platform only supports comma as a delimiter. The user is seeking a solution to this problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41855344",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":3.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":133.5554527778,
        "Challenge_title":"Azure ML: How to save and process CSV files with semicolon as delimiter?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2908.0,
        "Challenge_word_count":58,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1251372839052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":48616.0,
        "Poster_view_count":3348.0,
        "Solution_body":"<p>Azure ML only accepts the comma <code>,<\/code> separated CSV. Do a little work around.\nOpen your data file using a text editor. (Notepad will do the trick). Find and replace all semicolons with 'tab' (Make it a TSV) and the commas in data values may not occur a problem then. Make sure to define that the input is a TSV; not a CSV. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.9,
        "Solution_reading_time":4.05,
        "Solution_score_count":7.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1394078070848,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":913.0,
        "Answerer_view_count":88.0,
        "Challenge_adjusted_solved_time":23.4023241667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there anything in the Python API that lets you alter the artifact subdirectories? For example, I have a .json file stored here:<\/p>\n<p><code>s3:\/\/mlflow\/3\/1353808bf7324824b7343658882b1e45\/artifacts\/feature_importance_split.json<\/code><\/p>\n<p>MlFlow creates a <code>3\/<\/code> key in s3. Is there a way to change to modify this key to something else (a date or the name of the experiment)?<\/p>",
        "Challenge_closed_time":1626213927412,
        "Challenge_comment_count":1,
        "Challenge_created_time":1626152546053,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to modify the subdirectory of the MLflow artifact store using the Python API. They want to change the default key \"3\/\" to something else like a date or the name of the experiment.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68356746",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.0,
        "Challenge_reading_time":5.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":17.0503775,
        "Challenge_title":"Changing subdirectory of MLflow artifact store",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1493.0,
        "Challenge_word_count":57,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1394078070848,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":913.0,
        "Poster_view_count":88.0,
        "Solution_body":"<p>As I commented above, yes, <code>mlflow.create_experiment()<\/code> does allow you set the artifact location using the <code>artifact_location<\/code> parameter.<\/p>\n<p>However, sort of related, the problem with setting the <code>artifact_location<\/code> using the <code>create_experiment()<\/code> function is that once you create a experiment, MLflow will throw an error if you run the <code>create_experiment()<\/code> function again.<\/p>\n<p>I didn't see this in the docs but it's confirmed that if an experiment already exists in the backend-store, MlFlow will not allow you to run the same <code>create_experiment()<\/code> function again. And as of this post, MLfLow does not have <code>check_if_exists<\/code> flag or a <code>create_experiments_if_not_exists()<\/code> function.<\/p>\n<p>To make things more frustrating, you cannot set the <code>artifcact_location<\/code> in the <code>set_experiment()<\/code> function either.<\/p>\n<p>So here is a pretty easy work around, it also avoids the &quot;ERROR mlflow.utils.rest_utils...&quot; stdout logging as well.\n:<\/p>\n<pre><code>import os\nfrom random import random, randint\n\nfrom mlflow import mlflow,log_metric, log_param, log_artifacts\nfrom mlflow.exceptions import MlflowException\n\ntry:\n    experiment = mlflow.get_experiment_by_name('oof')\n    experiment_id = experiment.experiment_id\nexcept AttributeError:\n    experiment_id = mlflow.create_experiment('oof', artifact_location='s3:\/\/mlflow-minio\/sample\/')\n\nwith mlflow.start_run(experiment_id=experiment_id) as run:\n    mlflow.set_tracking_uri('http:\/\/localhost:5000')\n    print(&quot;Running mlflow_tracking.py&quot;)\n\n    log_param(&quot;param1&quot;, randint(0, 100))\n    \n    log_metric(&quot;foo&quot;, random())\n    log_metric(&quot;foo&quot;, random() + 1)\n    log_metric(&quot;foo&quot;, random() + 2)\n\n    if not os.path.exists(&quot;outputs&quot;):\n        os.makedirs(&quot;outputs&quot;)\n    with open(&quot;outputs\/test.txt&quot;, &quot;w&quot;) as f:\n        f.write(&quot;hello world!&quot;)\n\n    log_artifacts(&quot;outputs&quot;)\n<\/code><\/pre>\n<p>If it is the user's first time creating the experiment, the code will run into an AttributeError since <code>experiment_id<\/code> does not exist and the <code>except<\/code> code block gets executed creating the experiment.<\/p>\n<p>If it is the second, third, etc the code is run, it will only execute the code under the <code>try<\/code> statement since the experiment now exists. Mlflow will now create a 'sample' key in your s3 bucket. Not fully tested but it works for me at least.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1626236794420,
        "Solution_link_count":1.0,
        "Solution_readability":12.7,
        "Solution_reading_time":32.42,
        "Solution_score_count":1.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":267.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1577919980176,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hamburg, Germany",
        "Answerer_reputation_count":5588.0,
        "Answerer_view_count":398.0,
        "Challenge_adjusted_solved_time":9.1191208333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm a newbie in Sagemaker and i'm trying to load a pickle dataset into sagemaker notebook.\nI'm using the Python 3 (Data Science) kernel and ml.t3.medium instance.\nEither i load the pickle from S3 or I upload it directly from the studio like this:<\/p>\n<pre><code>import pickle5\nwith open('filename', 'rb') as f:\n    x = pickle.load(f)\n<\/code><\/pre>\n<p><strong>I get this Error:<\/strong><\/p>\n<pre><code>---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n\/opt\/conda\/lib\/python3.7\/site-packages\/IPython\/core\/formatters.py in __call__(self, obj)\n    700                 type_pprinters=self.type_printers,\n    701                 deferred_pprinters=self.deferred_printers)\n--&gt; 702             printer.pretty(obj)\n    703             printer.flush()\n    704             return stream.getvalue()\n\n..................... more errors here\n\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pandas\/core\/generic.py in __getattr__(self, name)\n   5268             or name in self._accessors\n   5269         ):\n-&gt; 5270             return object.__getattribute__(self, name)\n   5271         else:\n   5272             if self._info_axis._can_hold_identifiers_and_holds_name(name):\n\npandas\/_libs\/properties.pyx in pandas._libs.properties.AxisProperty.__get__()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pandas\/core\/generic.py in __getattr__(self, name)\n   5268             or name in self._accessors\n   5269         ):\n-&gt; 5270             return object.__getattribute__(self, name)\n   5271         else:\n   5272             if self._info_axis._can_hold_identifiers_and_holds_name(name):\n\nAttributeError: 'DataFrame' object has no attribute '_data'\n<\/code><\/pre>",
        "Challenge_closed_time":1620025682632,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619992853797,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to load a pickle dataset into Sagemaker notebook using Python 3 (Data Science) kernel and ml.t3.medium instance. The user is getting an AttributeError stating that the 'DataFrame' object has no attribute '_data'.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67361483",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":20.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":9.1191208333,
        "Challenge_title":"AWS Sagemaker Studio, cannot load pickle files",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":237.0,
        "Challenge_word_count":141,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1553704286212,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":27.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Can you check your Pandas versions? This error typically occurs when the pickled file was written in an old Pandas version. Your Sagemaker notebook probably runs Pandas &gt; 1.1 where as the Pandas in which the dataframe was pickled is probably &lt; 1.1<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.3,
        "Solution_reading_time":3.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":43.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1541802293200,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":163.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":109.1823416667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The short story is, when I try to submit an azure ML pipeline run (an <em>azure ML pipeline<\/em>, not an <em>Azure pipeline<\/em>) from a jupyter notebook, I get PermissionError: [Errno 13] Permission denied: '.\\NTUSER.DAT'.  More details:<\/p>\n\n<p>Relevant code:<\/p>\n\n<pre><code>from azureml.train.automl import AutoMLConfig\nfrom azureml.train.automl.runtime import AutoMLStep\nautoml_settings = {\n    \"iteration_timeout_minutes\": 20,\n    \"experiment_timeout_minutes\": 30,\n    \"n_cross_validations\": 3,\n    \"primary_metric\": 'r2_score',\n    \"preprocess\": True,\n    \"max_concurrent_iterations\": 3,\n    \"max_cores_per_iteration\": -1,\n    \"verbosity\": logging.INFO,\n    \"enable_early_stopping\": True,\n    'time_column_name': \"DateTime\"\n}\n\nautoml_config = AutoMLConfig(task = 'forecasting',\n                             debug_log = 'automl_errors.log',\n                             path = \".\",\n                             compute_target=compute_target,\n                             run_configuration=conda_run_config,                               \n                             training_data = financeforecast_dataset,\n                             label_column_name = 'TotalUSD',\n                             **automl_settings\n                            )\n\nautoml_step = AutoMLStep(\n    name='automl_module',\n    automl_config=automl_config,\n    allow_reuse=False)\n\ntraining_pipeline = Pipeline(\n    description=\"training_pipeline\",\n    workspace=ws,    \n    steps=[automl_step])\n\ntraining_pipeline_run = Experiment(ws, 'test').submit(training_pipeline)\n<\/code><\/pre>\n\n<p>The training_pipeline step runs for apx 20 seconds, and then I get a long trace, ending in:<\/p>\n\n<pre><code>~\\AppData\\Local\\Continuum\\anaconda2\\envs\\forecasting\\lib\\site- \npackages\\azureml\\pipeline\\core\\_module_builder.py in _hash_from_file_paths(hash_src)\n    100             hasher = hashlib.md5()\n    101             for f in hash_src:\n--&gt; 102                 with open(str(f), 'rb') as afile:\n    103                     buf = afile.read()\n    104                     hasher.update(buf)\n\nPermissionError: [Errno 13] Permission denied: '.\\\\NTUSER.DAT'\n<\/code><\/pre>\n\n<p>According to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-your-first-pipeline\" rel=\"nofollow noreferrer\">Azure's docs on this topic<\/a>, submitting a pipeline uploads a \"snapshot\" of the \"source directory\" you specified.  Initially, I hadn't specified a source directory, so, to test that out, I added: <\/p>\n\n<pre><code>default_source_directory=\"testing\",\n<\/code><\/pre>\n\n<p>as a parameter for the training_pipeline object, but saw the same behavior when I then tried to run it.  Not sure if that is the same source directory the documentation is referring to.  The docs also say that if no source directory is specified, the \"current local directory\" is uploaded.  I used print (os.getcwd()) to get the working directory and gave \"Everyone\" full control permissions on the directory (working in a windows env).<\/p>\n\n<p>All the preceding code works fine, and I can submit an experiment if I use a ScriptRunConfig and run it on attached compute rather than using a pipeline\/training cluster.  <\/p>\n\n<p>Any ideas?  Thanks in advance to anyone who tries to help.  P.S. There is no \"azure-machine-learning-pipelines\" tag, and I can't add one because I don't have enough reputation points.  Someone else could though!  <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-ml-pipelines\" rel=\"nofollow noreferrer\">General<\/a> info on what they are.<\/p>",
        "Challenge_closed_time":1577994637207,
        "Challenge_comment_count":0,
        "Challenge_created_time":1577601580777,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a PermissionError: [Errno 13] Permission denied: '.\\NTUSER.DAT' when trying to submit an Azure ML pipeline run from a Jupyter notebook. The error occurs when the pipeline tries to upload a \"snapshot\" of the \"source directory\" specified. The user has tried specifying a source directory and giving \"Everyone\" full control permissions on the working directory, but the issue persists. The code works fine when using a ScriptRunConfig and running it on attached compute rather than using a pipeline\/training cluster.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59517355",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.4,
        "Challenge_reading_time":41.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":109.1823416667,
        "Challenge_title":"Permission denied: '.\\NTUSER.DAT' when trying to run an Azure ML Pipeline",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":409.0,
        "Challenge_word_count":336,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1541802293200,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":163.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>I resolved this answer by setting the path and the data_script variables in the AutoMLConfig task object, like this (relevant code indicated by -->):<\/p>\n\n<pre><code>automl_config = AutoMLConfig(task = 'forecasting',\n                             debug_log = 'automl_errors.log',\n                             compute_target=compute_target,\n                             run_configuration=conda_run_config,\n                             --&gt;path = \"c:\\\\users\\\\me\",\n                             data_script =\"script.py\",&lt;--\n                             **automl_settings\n                            )\n<\/code><\/pre>\n\n<p>Setting the data_script variable to include the full path, as shown below, did not work.<\/p>\n\n<pre><code>automl_config = AutoMLConfig(task = 'forecasting',\n                             debug_log = 'automl_errors.log',\n                             path = \".\",\n                             --&gt;data_script = \"c:\\\\users\\\\me\\\\script.py\"&lt;--\n                             compute_target=compute_target,\n                             run_configuration=conda_run_config, \n                             **automl_settings\n                            )\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.7,
        "Solution_reading_time":10.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11.13646,
        "Challenge_answer_count":2,
        "Challenge_body":"to set up CatBoost Classifier as a built-in algorithm, aws in this [https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/catboost.html] suggested this notebook [https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/lightgbm_catboost_tabular\/Amazon_Tabular_Classification_LightGBM_CatBoost.ipynb] , my question is should I prepare inference file on top of the train.csv? if yes what is that and how it should be prepared?",
        "Challenge_closed_time":1658504084534,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658463993278,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to set up CatBoost Classifier as a built-in algorithm using AWS SageMaker and is following the instructions provided in the documentation and a suggested notebook. The user is unsure if they need to prepare an inference file on top of the train.csv and is seeking guidance on how to prepare it.",
        "Challenge_last_edit_time":1667993528404,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU-0PVSBTSR4GvFO3E5FusCQ\/input-and-output-interface-for-the-catboost-algorithm",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":18.8,
        "Challenge_reading_time":6.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":11.13646,
        "Challenge_title":"Input and Output interface for the CatBoost algorithm",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":124.0,
        "Challenge_word_count":48,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"According to the documentation,[https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/catboost.html] 'The CatBoost built-in algorithm runs in script mode, but the training script is provided for you and there is no need to replace it. If you have extensive experience using script mode to create a SageMaker training job, then you can incorporate your own CatBoost training scripts.' Is the same with the Inference script, all provided artifacts.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1658504084534,
        "Solution_link_count":1.0,
        "Solution_readability":11.6,
        "Solution_reading_time":5.58,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":61.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9316.4808333333,
        "Challenge_answer_count":0,
        "Challenge_body":"`ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden`\r\n\r\n`dvc pull` needs mantis creds so a reader will not be able to follow. we need to make the bucket public and read only.",
        "Challenge_closed_time":1668173479000,
        "Challenge_comment_count":4,
        "Challenge_created_time":1634634148000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a \"FileNotFoundError\" while trying to run the \"kedro mlflow ui\" command after running \"kedro mlflow init\" command. The error message indicates that the 'mlflow_tracking_uri' key in mlflow.yml is relative and is converted to a valid uri.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/MantisAI\/Rasa-MLOPs\/issues\/5",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":9.3,
        "Challenge_reading_time":3.57,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":1.0,
        "Challenge_repo_issue_count":14.0,
        "Challenge_repo_star_count":3.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":9316.4808333333,
        "Challenge_title":"Remote storage is not publicly accessible (dvc pull fails)",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":46,
        "Discussion_body":"So:\r\n1. You will need to have aws credentials set up with `aws configure`, having installed awscli (which is in the virtualenv)\r\n2. I'm having some issues getting the mantisnlp-public bucket to be accessible to dvc with a non mantis aws profile. I don't know if this is related but did you try `--acl public-read`? I had some problems with public buckets in grants tagger and for me it was resolved by adding this flag. example https:\/\/github.com\/wellcometrust\/grants_tagger\/blob\/970abbc63b448c4d14d7b70fa13ca29760a897ce\/Makefile#L94 I've done this at the bucket level, not at the individual object level, because they are added by dvc. It _should_ be working... btw this issue is probably badly named because:\r\n1. You only need to set `AWS_PROFILE` if you have more than one set of aws credentials\r\n2. You can also set `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` in the folder to the same effect, and users can decide how best to do this.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":103.2534313889,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I was training a yolov5 model, using the pre-configured wandb settings. But the weights weren\u2019t uploaded because the session was killed. I tried <code>wandb sync path\/to\/run<\/code> but the model file didn\u2019t get synced.<\/p>\n<p>I want to upload the resulting <code>best.pt<\/code> file to the artifacts regardless without messing up with the current summary and results of the finished run. I looked up in the documentation and tried multiple guides but couldn\u2019t manage to do that.<\/p>\n<p>TL;DR: I have a finished run and a weights file. I need to upload the weights file as a model artifact to that finished run using the run path.<\/p>",
        "Challenge_closed_time":1654587679840,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654215967487,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue while trying to upload model weights to the Artifacts of a finished run using pre-configured wandb settings. The session was killed and the weights were not uploaded. The user tried to sync the file but it didn't work. The user wants to upload the resulting best.pt file to the artifacts without affecting the current summary and results of the finished run. The user is seeking guidance on how to upload the weights file as a model artifact to the finished run using the run path.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/upload-model-weights-to-the-artifacts-of-a-finished-run\/2540",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":6.9,
        "Challenge_reading_time":8.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":103.2534313889,
        "Challenge_title":"Upload model weights to the Artifacts of a finished run",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":235.0,
        "Challenge_word_count":112,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey <a class=\"mention\" href=\"\/u\/alyetama\">@alyetama<\/a>, here is a code snippet you can use: <a href=\"https:\/\/docs.wandb.ai\/guides\/artifacts\/artifacts-faqs#how-do-i-log-an-artifact-to-an-existing-run\">https:\/\/docs.wandb.ai\/guides\/artifacts\/artifacts-faqs#how-do-i-log-an-artifact-to-an-existing-run<\/a><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":41.8,
        "Solution_reading_time":4.35,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":14.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1254957460063,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"North Carolina, USA",
        "Answerer_reputation_count":2484.0,
        "Answerer_view_count":362.0,
        "Challenge_adjusted_solved_time":21.4263258333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>My \"experiment\" is like this,<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/AgnGE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AgnGE.png\" alt=\"Experiment\"><\/a><\/p>\n\n<p>I have 10 rows (excluding header) in \"Dataset.csv\" and 3 rows (excluding header) in the CSV being imported by <em>Import Data<\/em>. The schema of both CSVs is same. I want <em>Add Rows<\/em> to <strong>append<\/strong> the 3 rows to Dataset.csv.<\/p>\n\n<p>The real \"Dataset.csv\" has more than 25,000 rows and is expected to grow. Hence, using <em>Export Data<\/em> to generate a merged dataset (as a new CSV) is not a feasible solution. Any way to implement <strong>append<\/strong> for this scenario?<\/p>\n\n<p>Thanks<\/p>\n\n<p>Update 1:\nDataset.csv is present in ML Studios <em>Dataset<\/em>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/LBimY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LBimY.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":1535538465923,
        "Challenge_comment_count":8,
        "Challenge_created_time":1535452910717,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to append 3 rows to a dataset in Azure Machine Learning Studio, but the dataset already has more than 25,000 rows and exporting the data to generate a merged dataset is not feasible. The user is looking for a way to implement append for this scenario.",
        "Challenge_last_edit_time":1535461331150,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52055933",
        "Challenge_link_count":4,
        "Challenge_participation_count":9,
        "Challenge_readability":8.5,
        "Challenge_reading_time":12.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":23.765335,
        "Challenge_title":"Azure Machine Learning Studio append rows to dataset",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":590.0,
        "Challenge_word_count":115,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1409841727700,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":805.0,
        "Poster_view_count":137.0,
        "Solution_body":"<p>So it turns out the <a href=\"https:\/\/github.com\/Azure\/Azure-MachineLearning-ClientLibrary-Python\" rel=\"nofollow noreferrer\">Python SDK<\/a> has an <code>update_from_dataframe<\/code> method on it that can be used to update a dataset that has been uploaded to Azure ML Studio. If you're unable to use a new CSV and need to update an existing data set, then this should do the trick.<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.1,
        "Solution_reading_time":4.89,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1436184843608,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":305.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":0.8062980556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is there any way I can make my dataset features in Azure ML into something else than what it already is? <\/p>\n\n<p>I found a dataset of the Titanic ship in the sample datasets which I would like to work with but all of my columns are either a numeric feature or string feature, but I would like to categorize these. Also is there any possibility to rename the columns within my model so it\u2019s more descriptive than what I initially got? I have no clue what SibSp means for instance.<\/p>",
        "Challenge_closed_time":1465484184003,
        "Challenge_comment_count":0,
        "Challenge_created_time":1465481281330,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to change the dataset features in Azure ML from numeric and string to categorized features. They also want to rename the columns within the model to make them more descriptive.",
        "Challenge_last_edit_time":1465977179120,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37728314",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.6,
        "Challenge_reading_time":6.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.8062980556,
        "Challenge_title":"Refactor columns and features in Azure Machine Learning",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":297.0,
        "Challenge_word_count":98,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1465326515432,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>What you are doing is essentially recreating this experiment made by Raja Iqbal for the Titanic dataset. I recommend you check that out here: <a href=\"http:\/\/gallery.cortanaintelligence.com\/Experiment\/Tutorial-Building-a-classification-model-in-Azure-ML-8?share=1\" rel=\"nofollow noreferrer\">http:\/\/gallery.cortanaintelligence.com\/Experiment\/Tutorial-Building-a-classification-model-in-Azure-ML-8?share=1<\/a><\/p>\n\n<p>To answer your question, the module you can drag to your canvas in order to make the features into categories; is the Edit Metadata module where you select the columns you want and change the \u201cunchanged\u201d into \u201cMake categorical\u201d within the Categorical-properties pane like in the image below:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/2NDht.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2NDht.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>You can also use the same module to make better sense from your columns by giving them a different column name. SibSp means SiblingSpouse like I have renamed it to in the image below:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Gm9Rr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Gm9Rr.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>And at last you can assign the targeted value (survived) and make the field into a label for ease of use.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/LyN0j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LyN0j.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":8.0,
        "Solution_readability":14.3,
        "Solution_reading_time":19.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":158.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":338.3591666667,
        "Challenge_answer_count":0,
        "Challenge_body":"https:\/\/github.com\/canonical\/mlflow-operator\/blob\/c856446074868d4735627c95878960d91555f4da\/charms\/mlflow-server\/src\/charm.py#L20\r\n\r\nThe name of the bucket for MLFlow is hardcoded. This is a big issue because this makes using Minio in Gateway mode + MLFlow impossible on AWS (S3 buckets are globally unique).\r\n\r\nIt's a good first issue :)",
        "Challenge_closed_time":1647350309000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646132216000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The mlflow chart has a bug where the newly added staticPrefix parameter under extraArgs breaks the chart when used because it tries to add an extra argument to the mlflow server command that doesn't exist. The user suggests a solution to handle the staticPrefix as a separate argument in the extraEnv when starting up the mlflow server to make it work smoother for the final user. The user is creating a pull request to address this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/canonical\/mlflow-operator\/issues\/24",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":9.1,
        "Challenge_reading_time":5.14,
        "Challenge_repo_contributor_count":17.0,
        "Challenge_repo_fork_count":8.0,
        "Challenge_repo_issue_count":148.0,
        "Challenge_repo_star_count":8.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":338.3591666667,
        "Challenge_title":"MLFlow hardcoded bucket name - impossible to use MLFlow with AWS S3",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":47,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1352429442632,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":740.0,
        "Answerer_view_count":63.0,
        "Challenge_adjusted_solved_time":35.1769344445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to log data points for the same metric across multiple runs (<code>wandb.init<\/code> is called repeatedly in between each data point) and I'm unsure how to avoid the behavior seen in the attached screenshot...<\/p>\n<p>Instead of getting a line chart with multiple points, I'm getting a single data point with associated statistics. In the attached e.g., the 1st data point was generated at step 1,470 and the 2nd at step 2,940...rather than seeing two points, I'm instead getting a single point that's the average and appears at step 2,205.\n<a href=\"https:\/\/i.stack.imgur.com\/98Lln.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/98Lln.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>My hunch is that using the <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/resuming\" rel=\"nofollow noreferrer\">resume run<\/a> feature may address my problem, but even testing out this hunch is proving to be cumbersome given the constraints of the system I'm working with...<\/p>\n<p>Before I invest more time in my hypothesized solution, could someone confirm that the behavior I'm seeing is, indeed, the result of logging data to the same metric across separate runs without using the resume feature?<\/p>\n<p>If this is the case, can you confirm or deny my conception of how to use resume?<\/p>\n<p>Initial run:<\/p>\n<ol>\n<li><code>run = wandb.init()<\/code><\/li>\n<li><code>wandb_id = run.id<\/code><\/li>\n<li>cache <code>wandb_id<\/code> for successive runs<\/li>\n<\/ol>\n<p>Successive run:<\/p>\n<ol>\n<li>retrieve <code>wandb_id<\/code> from cache<\/li>\n<li><code>wandb.init(id=wandb_id, resume=&quot;must&quot;)<\/code><\/li>\n<\/ol>\n<p>Is it also acceptable \/ preferable to replace <code>1.<\/code> and <code>2.<\/code> of the initial run with:<\/p>\n<ol>\n<li><code>wandb_id = wandb.util.generate_id()<\/code><\/li>\n<li><code>wandb.init(id=wandb_id)<\/code><\/li>\n<\/ol>",
        "Challenge_closed_time":1662569956687,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662443319723,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to log data points for the same metric across multiple runs using wandb.init, but instead of getting a line chart with multiple points, they are getting a single data point with associated statistics. The user suspects that using the resume run feature may address the problem, but they are unsure. They are seeking confirmation that the behavior they are seeing is the result of logging data to the same metric across separate runs without using the resume feature and guidance on how to use the resume feature.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73617230",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":9.7,
        "Challenge_reading_time":24.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":35.1769344445,
        "Challenge_title":"How to avoid data averaging when logging to metric across multiple runs?",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":35.0,
        "Challenge_word_count":243,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1352429442632,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":740.0,
        "Poster_view_count":63.0,
        "Solution_body":"<blockquote>\n<p>My hunch is that using the resume run feature may address my problem,<\/p>\n<\/blockquote>\n<p>Indeed, providing a cached <code>id<\/code> in combination with <code>resume=&quot;must&quot;<\/code> fixed the issue.\n<a href=\"https:\/\/i.stack.imgur.com\/KooZ9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KooZ9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Corresponding snippet:<\/p>\n<pre><code>import wandb\n\n# wandb run associated with evaluation after first N epochs of training.\nwandb_id = wandb.util.generate_id()\nwandb.init(id=wandb_id, project=&quot;alrichards&quot;, name=&quot;test-run-3\/job-1&quot;, group=&quot;test-run-3&quot;)\nwandb.log({&quot;mean_evaluate_loss_epoch&quot;: 20}, step=1)\nwandb.finish()\n\n# wandb run associated with evaluation after second N epochs of training.\nwandb.init(id=wandb_id, resume=&quot;must&quot;, project=&quot;alrichards&quot;, name=&quot;test-run-3\/job-2&quot;, group=&quot;test-run-3&quot;)\nwandb.log({&quot;mean_evaluate_loss_epoch&quot;: 10}, step=5)\nwandb.finish()\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.6,
        "Solution_reading_time":14.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":83.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1426675778223,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Coimbatore, Tamil Nadu, India",
        "Answerer_reputation_count":10189.0,
        "Answerer_view_count":1471.0,
        "Challenge_adjusted_solved_time":19.2195019445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following this <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_neo_compilation_jobs\/gluoncv_ssd_mobilenet\/gluoncv_ssd_mobilenet_neo.ipynb\" rel=\"nofollow noreferrer\">link<\/a> to train an object detection model. I am able to successfully deploy the model on EC2 instance. The accuracy was good. I complied the same model file for m edge Device Jetson Nano. My inference code looks like below,<\/p>\n<pre><code>from dlr import DLRModel\nimport json\nimport cv2\n\nmodel = DLRModel('model', 'gpu')\n\nimg = cv2.imread('test.jpg')\nimg = cv2.resize(img, (512, 512), interpolation=cv2.INTER_AREA)\nimg = np.expand_dims(img, 0)\n\noutputs = model.run(img)\nobjects=outputs[0][0]\nscores=outputs[1][0]\nbounding_boxes=outputs[2][0]\n<\/code><\/pre>\n<p>When I look the result, it's not at all matched with SageMaker Notebook instance result. Boudning boxes' values are sometime in ~70000. I couldn't understand the format of result produced by DLR.<\/p>\n<p>Sample result for an image.<\/p>\n<p>Classes:\n[[10.0], [14.0], [4.0], [10.0], [14.0], [-1.0], [-1.0], [-1.0], [7.0], [-1.0], [6.0], [11.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [2.0], [-1.0], [-1.0], [0.0], [-1.0], [17.0], [-1.0], [-1.0], [6.0], [18.0], [-1.0], [-1.0], [-1.0], [18.0], [-1.0], [12.0], [-1.0], [-1.0], [13.0], [-1.0], [-1.0], [-1.0], [1.0], [-1.0], [-1.0], [5.0], [-1.0], [0.0], [-1.0], [-1.0], [-1.0], [-1.0], [3.0], [8.0], [5.0], [-1.0], [-1.0], [15.0], [-1.0], [9.0], [3.0], [-1.0], [10.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [16.0], [8.0], [-1.0], [16.0], [19.0], [-1.0], [9.0], [-1.0], [4.0], [-1.0], [-1.0], [15.0], [10.0], [-1.0], [-1.0], [4.0], [-1.0], [8.0], [2.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0]]<\/p>\n<p>scores:\n[[0.9527158737182617], [0.910746157169342], [0.28013306856155396], [0.059000786393880844], [0.04898739233613014], [-1.0], [-1.0], [-1.0], [0.04864144325256348], [-1.0], [0.04847110062837601], [0.04843416064977646], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [0.04821664094924927], [-1.0], [-1.0], [0.04808368161320686], [-1.0], [0.0479729063808918], [-1.0], [-1.0], [0.04782549664378166], [0.04778601974248886], [-1.0], [-1.0], [-1.0], [0.0475776381790638], [-1.0], [0.047538403421640396], [-1.0], [-1.0], [0.047468967735767365], [-1.0], [-1.0], [-1.0], [0.04737424850463867], [-1.0], [-1.0], [0.047330085188150406], [-1.0], [0.04730956256389618], [-1.0], [-1.0], [-1.0], [-1.0], [0.04710235074162483], [0.04710135608911514], [0.047083333134651184], [-1.0], [-1.0], [0.047033149749040604], [-1.0], [0.0469636432826519], [0.046939317137002945], [-1.0], [0.04687687009572983], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [0.046708278357982635], [0.046680934727191925], [-1.0], [0.04660974070429802], [0.046597886830568314], [-1.0], [0.04656397923827171], [-1.0], [0.046513814479112625], [-1.0], [-1.0], [0.04647510126233101], [0.04644943028688431], [-1.0], [-1.0], [0.046245038509368896], [-1.0], [0.01647786796092987], [0.010514501482248306], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0]]<\/p>\n<p>Bouding boxes:\n[[523.2319946289062, -777751.875, 523.5722045898438, 778992.625], [425.0546875, -1141093.0, 429.6099853515625, 1142524.5], [680.2073364257812, 542.9566650390625, 680.2202758789062, 543.36376953125], [425.0546875, -1141093.0, 429.6099853515625, 1142524.5], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [591.7652587890625, -1121890.0, 592.9493408203125, 1123299.75], [523.2319946289062, -777751.875, 523.5722045898438, 778992.625], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0]]<\/p>\n<p>What causes this notorious result?\nIs there any issue while compiling model in Neo?\nAny issue in inference Code?<\/p>\n<p>Any hint would be appreciable.<\/p>",
        "Challenge_closed_time":1609912383447,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609843193240,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"the user is encountering challenges with their object detection model, as the results produced by the dlr model are not matching the results from the notebook instance, with bounding boxes values sometimes reaching ~70000.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65577286",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":143.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":19.2195019445,
        "Challenge_title":"Dlr model gives notorious result for object detection model",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":98.0,
        "Challenge_word_count":747,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1426675778223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Coimbatore, Tamil Nadu, India",
        "Poster_reputation_count":10189.0,
        "Poster_view_count":1471.0,
        "Solution_body":"<p>The reason behind the abnormal result is due to improper pre-processing method was applied. Here is the complete inference code for Mobilenet-ssd model.<\/p>\n<pre><code>def transform(img):\n    # Normalize\n    mean_vec = np.array([0.485, 0.456, 0.406])\n    stddev_vec = np.array([0.229, 0.224, 0.225])\n    image = (img \/ 255 - mean_vec) \/ stddev_vec\n\n    # Transpose\n    if len(image.shape) == 2:  # for greyscale image\n        image = np.expand_dims(image, axis=2)\n\n    image = np.rollaxis(image, axis=2, start=0)[np.newaxis, :]\n    return image\n\nmodel = DLRModel(model_dir, 'gpu')\n\n\nfor file_name in image_folder:\n    image = PIL.Image.open(file_name)\n    image = np.asarray(image.resize((512, 512)))\n    image = transform(image)\n    # flatten within a input array\n    input_data = {'data': image}\n    outputs = model.run(input_data)\n    objects = outputs[0]\n    scores = outputs[1]\n    bounding_boxes = outputs[2]\n    result = [objects.tolist(), scores.tolist(), bounding_boxes.tolist()]\n    print(json.dumps(result))\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.2,
        "Solution_reading_time":12.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":91.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":46.9975786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello , I'm new to azureML and I have a question about Azure ML designer , after creating  a workflow (drag and drop components , let's say problem of linear  regression ) can I export, download a file of the  metadata of the workflow that  contains the names of components used and its parameters.  <br \/>\nif it's possible can automate the process of creating a workflow in azureML designer by using already exsiting metadata file (json, xml , yaml ... ) similar to the one mentioned previously.   <\/p>\n<p>if there any other services in azure that's capable of solving this issue please feel free to mention it   <\/p>\n<p>thank youu.<\/p>",
        "Challenge_closed_time":1645441227963,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645272036680,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is new to AzureML and wants to know if it is possible to export a metadata file of the components and parameters used in a workflow created in Azure ML designer. They also want to know if it is possible to automate the process of creating a workflow using an existing metadata file. The user is open to suggestions for other Azure services that can solve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/742573\/export-metadata-file-of-componenets-and-its-parame",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":8.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":46.9975786111,
        "Challenge_title":"Export metadata file of componenets and its parameters  of trained and submitted model  in AzureML  designer",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":118,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=ca3ed354-350d-4b5d-be80-486f8db9ec5b\">@Achraf DRIDI  <\/a> Yes, you can export your designer experiment as a pipeline. The option to export is available from the designer from the top right hand corner. This is basically a cli command that helps you export the experiment in two ways.    <\/p>\n<ol>\n<li> Shallow     <\/li>\n<li> Deep    <\/li>\n<\/ol>\n<p><strong>UPDATE<\/strong>    <br \/>\nThe feature mentioned above is in private preview and not available to all users. Exact ETA not available at this point of time.    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.8,
        "Solution_reading_time":11.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":105.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":22.5241808334,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I used  <code>tempfile.mkdtemp() <\/code> to create a temporary directory for my runs (as I don\u2019t want a persistent folder with tons of runs)<\/p>\n<p>For training everything works fine but when resuming the run to do some validation \/ evaluation updates, and using <code>run.summary.update({\"key\": value})<\/code> I got a<\/p>\n<pre><code class=\"lang-auto\">wandb: WARNING Path \/tmp\/tmpq5uafy4d\/wandb\/ wasn't writable, using system temp directory\n<\/code><\/pre>\n<p>with obviously<\/p>\n<pre><code class=\"lang-auto\">File \"\/mnt\/Projets\/nlp\/.venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/internal\/sender.py\", line 855, in _update_summary\n    with open(summary_path, \"w\") as f:\nFileNotFoundError: [Errno 2] No such file or directory: '\/tmp\/tmpq5uafy4d\/wandb\/run-20211102_153311-37264m5k\/files\/wandb-summary.json'\n<\/code><\/pre>\n<p>As in the doc of <a href=\"https:\/\/docs.python.org\/3.9\/library\/tempfile.html#tempfile.mkdtemp\" rel=\"noopener nofollow ugc\"><code>mkdtemp<\/code><\/a> :<\/p>\n<pre><code class=\"lang-auto\"> The directory is readable, writable, and searchable only by the creating user ID.\n<\/code><\/pre>\n<p>So I guess WandB is not using the user ID and thus is not able to write in the directory for updating.<br>\nNote that this directory is different from the training one (as it\u2019s random at each init)<\/p>\n<p>Thanks in advance for any help.<br>\nHave a great day.<\/p>",
        "Challenge_closed_time":1635963680198,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635882593147,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user created a temporary directory using tempfile.mkdtemp() for their runs in WandB. While training, everything works fine, but when resuming the run for validation\/evaluation updates, WandB is not using the user ID and is unable to write in the directory for updating. The user received a warning message stating that the path was not writable and that the system temp directory would be used instead.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-not-using-user-pid-when-updating\/1204",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":12.0,
        "Challenge_reading_time":18.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":22.5241808334,
        "Challenge_title":"WandB not using user PID when updating",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":352.0,
        "Challenge_word_count":164,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>As of now, there isn\u2019t an option to enable that by default. One issue you should be aware of is that if you are currently logging a run when you call <code>wandb sync --clean<\/code> bad things will happen. We\u2019re working on improving the the robustness of these features and will likely support an automatic clean option in the future.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":4.16,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":59.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1528790837107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":610.0,
        "Answerer_view_count":203.0,
        "Challenge_adjusted_solved_time":152.7426680556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I\u2019m having some issues trying to load a dataset in Azure ML Studio, a dataset containing a column that looks like a DateTime, but is in fact a string. Azure ML Studio converts the values to DateTimes internally, and no amount of wrangling seems to convince it of the that they\u2019re in fact strings.<\/p>\n\n<p>This is an issue, because during conversion the values lose precision and start appearing as duplicates whereas in fact they are unique. Does anybody know if ML Studio can be configured not to infer data types for columns while importing a dataset?<\/p>\n\n<p>Now, for the long(er) story :)<\/p>\n\n<p>I\u2019m working here with a public dataset - specifically <a href=\"https:\/\/www.kaggle.com\/c\/new-york-city-taxi-fare-prediction\" rel=\"noreferrer\">Kaggle\u2019s New York City Fare Prediction<\/a> competition. I wanted to see if I could do a quick-and-dirty solution using Azure ML Studio, however the dataset\u2019s unique key values are of the form\n<code>\n    2015-01-27 13:08:24.0000003\n    2015-01-27 13:08:24.0000002\n    2011-10-06 12:10:20.0000001\n<\/code>\nand so on. <\/p>\n\n<p>When importing them in my experiment the key values get converted to DateTime, making them no longer unique, even though they\u2019re unique in the csv. Needless to say, this prevents me from submitting any solution to Kaggle, since I can\u2019t identify the rows uniquely :).<\/p>\n\n<p>I\u2019ve tried the following:<\/p>\n\n<ul>\n<li>edit the metadata of the dataset after it has been loaded and setting the data type of the column to string, but this doesn\u2019t do much as the precision has already been lost<\/li>\n<li>import the dataset from an Azure blob, convert it to csv and then loading it in Jupyter\/Python - this brings me the same (duplicated) keys. <\/li>\n<li>loading the dataset locally with pandas works, as expected.<\/li>\n<\/ul>\n\n<p>I\u2019ve reproduced this behavior with both the big, 5.5GB <code>train<\/code> dataset, but also with the more manageable <code>sample_submission<\/code> dataset. <\/p>\n\n<p>Curious to know if there is some sort of workaround to tell ML Studio not to try converting this column while loading the dataset. I'm looking here specifically for Azure ML Studio-only solutions, as I don't want to do any preprocessing on the dataset.<\/p>",
        "Challenge_closed_time":1534433513132,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533883639527,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing issues while loading a dataset in Azure ML Studio that contains a column with DateTime values, which are actually strings. Azure ML Studio is converting the values to DateTimes internally, causing the loss of precision and making the values appear as duplicates. The user has tried various methods to resolve the issue, but none of them have worked. The user is looking for a solution that can prevent Azure ML Studio from inferring data types for columns while importing a dataset.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51780562",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":28.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":152.7426680556,
        "Challenge_title":"How to prevent Azure ML Studio from converting a feature column to DateTime while importing a dataset",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":401.0,
        "Challenge_word_count":354,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1250158552416,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Romania",
        "Poster_reputation_count":7916.0,
        "Poster_view_count":801.0,
        "Solution_body":"<p>I have tried with you sample data and here is my quick and dirty solution:\n1) Add any symbol (I've added the '#') in front of each date\n2) Load it to AML Studio (it is now considered as a string feature)\n3) Add a Python\/R component to remove the '#' symbol and explicitly convert the column to string (as.string(columnname) or str(columnname))<\/p>\n\n<p>Hope this helps<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.2,
        "Solution_reading_time":4.54,
        "Solution_score_count":4.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":26.7248641667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have keras training script on my machine. I am experimenting to run my script on AWS sagemaker container. For that I have used below code.<\/p>\n<pre><code>from sagemaker.tensorflow import TensorFlow\nest = TensorFlow(\n    entry_point=&quot;caller.py&quot;,\n    source_dir=&quot;.\/&quot;,\n    role='role_arn',\n    framework_version=&quot;2.3.1&quot;,\n    py_version=&quot;py37&quot;,\n    instance_type='ml.m5.large',\n    instance_count=1,\n    hyperparameters={'batch': 8, 'epochs': 10},\n)\n\nest.fit()\n<\/code><\/pre>\n<p>here <code>caller.py<\/code> is my entry point. After executing the above code I am getting <code>keras is not installed<\/code>. Here is the stacktrace.<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;executor.py&quot;, line 14, in &lt;module&gt;\n    est.fit()\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/estimator.py&quot;, line 682, in fit\n    self.latest_training_job.wait(logs=logs)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/estimator.py&quot;, line 1625, in wait\n    self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/session.py&quot;, line 3681, in logs_for_job\n    self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/session.py&quot;, line 3240, in _check_job_status\n    raise exceptions.UnexpectedStatusException(\nsagemaker.exceptions.UnexpectedStatusException: Error for Training job tensorflow-training-2021-06-09-07-14-01-778: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/usr\/local\/bin\/python3.7 caller.py --batch 4 --epochs 10\n\nModuleNotFoundError: No module named 'keras'\n\n<\/code><\/pre>\n<ol>\n<li>Which instance has pre-installed keras?<\/li>\n<li>Is there any way I can install the python package to the AWS container? or any workaround for the issue?<\/li>\n<\/ol>\n<p>Note: I have tried with my own container uploading to ECR and successfully run my code. I am looking for AWS's existing container capability.<\/p>",
        "Challenge_closed_time":1623320398007,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623223568407,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to train a Keras model on AWS Sagemaker using a TensorFlow estimator. However, upon executing the code, the user encounters an error stating that Keras is not installed. The user is seeking a solution to either install Keras on the AWS container or find an alternative workaround.",
        "Challenge_last_edit_time":1623224188496,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67899421",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.0,
        "Challenge_reading_time":28.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":26.8971111111,
        "Challenge_title":"Training keras model in AWS Sagemaker",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":316.0,
        "Challenge_word_count":191,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1426675778223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Coimbatore, Tamil Nadu, India",
        "Poster_reputation_count":10189.0,
        "Poster_view_count":1471.0,
        "Solution_body":"<p>Keras is now part of tensorflow, so you can just reformat your code to use <code>tf.keras<\/code> instead of <code>keras<\/code>. Since version 2.3.0 of tensorflow they are in sync, so it should not be that difficult.\nYou container is <a href=\"https:\/\/aws.amazon.com\/releasenotes\/aws-deep-learning-containers-for-tensorflow-2-3-1-with-cuda-11-0\/\" rel=\"nofollow noreferrer\">this<\/a>, as you can see from the list of the packages, there is no <code>Keras<\/code>.\nIf you instead want to extend a pre-built container you can take a look <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/prebuilt-containers-extend.html\" rel=\"nofollow noreferrer\">here<\/a> but I don't recommend in this specific use-case, because also for future code maintainability you should go for <code>tf.keras<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.4,
        "Solution_reading_time":10.34,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":93.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":575.7216666667,
        "Challenge_answer_count":0,
        "Challenge_body":"When using `fds clone` for non-DVC repo it throws the following error:\r\n\r\n`ERROR: you are not inside of a DVC repository (checked up to mount point '\/')`\r\n\r\nCloning a non-DVC repo using FDS can be a common use case, e.g., cloning a DAGsHub repo containing many files, but none of them are tracked by DVC nur the repo contains DVC config files. \r\n\r\nI suggest that after cloning the Git server, FDS will check if the repo contains DVC files. \r\n\r\nif it contains DVC files:\r\n  - echo 'Starting DVC Clone...`\r\n  - FDS will start a wizard to set the user name and password for each remote storage in the local config. (consider checking if they are set in the global config file first?)\r\n  - FDS will pull all the files from the remotes and show a progress bar (might be reasonable to ask if the user wants to pull the files from each remote)\r\n \r\nIt doesn't contain DVC files:\r\n  - FDS will initialize DVC\r\n  \r\n    if the Git server URL is DAGsHub's:\r\n      - FDS will set DAGsHub storage as the remote using the Git URL (replacing`.git` with `.dvc`).\r\n      - FDS will start a wizard to set the remote user name, password, and name.\r\n      \r\n    else:\r\n       - FDS will start a wizard asking do you want to set a DVC remote\r\n       if yes:\r\n           - With the wizard, the user will set the remote URL, name, username, and password.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1630576282000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628503684000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue where the markdown in the dvc install prompt is not being rendered as markdown.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/87",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":8.8,
        "Challenge_reading_time":15.28,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":145.0,
        "Challenge_repo_star_count":369.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":575.7216666667,
        "Challenge_title":"fsd clone for non-DVC repos throws an error",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":232,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":233.3348527778,
        "Challenge_answer_count":7,
        "Challenge_body":"<p>I\u2019m using AWS Sagemaker to train a Keras model with the Wandb callback. In my Sagemaker script, I save checkpoints to <code>'\/opt\/ml\/checkpoints\/'<\/code> which it redirects to an s3 bucket continuously. After the model has finished training, I create my artifact and add a reference to that bucket.<\/p>\n<p>Later, if I try to download the model with:<\/p>\n<pre><code class=\"lang-auto\">model_path = run.use_artifact(...)\nmodel_path.download()\n<\/code><\/pre>\n<p>I get the following error:<\/p>\n<blockquote>\n<p>ValueError: Digest mismatch for object s3:\/\/\u2026\/variables\/variables.data-00000-of-00001: expected 4f8d37a52a3e87f1f0ee2d3101688848-3 but found 8ad5ef5242d547d7edaa76f620597b60-3<\/p>\n<\/blockquote>\n<p>My guess is that I\u2019ve added the reference to the artifact before Sagemaker has pushed the final model from the local directory to S3. I\u2019m not sure how to get around this, is there a better way to have my Artifacts be linked to an S3 bucket?<\/p>",
        "Challenge_closed_time":1666891898080,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666051892610,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a digest mismatch error while trying to download a model artifact from S3 in AWS Sagemaker. The error is caused by adding a reference to the artifact before the final model is pushed from the local directory to S3. The user is unsure how to resolve the issue and is seeking a better way to link Artifacts to an S3 bucket.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/digest-mismatch-error-when-trying-to-download-model-artifact-from-s3\/3269",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":9.9,
        "Challenge_reading_time":12.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":233.3348527778,
        "Challenge_title":"Digest mismatch error when trying to download model artifact from S3",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":308.0,
        "Challenge_word_count":136,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/dspectrum\">@dspectrum<\/a>,<\/p>\n<p>Looking at your error and tracing back through our code - looks like versioning is not enabled on your S3 bucket, which means the artifact is changing the file itself, leading to different hashes. I would suggest turning on versioning on your S3 bucket and letting me know if you still run into the same error.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.6,
        "Solution_reading_time":4.74,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":59.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1209.0019444444,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen an error is raised during training with `MLFlowLogger`, status of a `mlflow.entities.run_info.RunInfo` object should be updated to be 'FAILED', while it remains 'RUNNING'.\r\nDue to the problem, when you look at MLFlow Tracking Server screen, It seams as if training is still in progress even though it has been terminated with an error.\r\n\r\n### To Reproduce\r\n\r\n<!--\r\nPlease reproduce using the BoringModel!\r\n\r\nYou can use the following Colab link:\r\nhttps:\/\/colab.research.google.com\/drive\/1HvWVVTK8j2Nj52qU4Q4YCyzOm0_aLQF3?usp=sharing\r\nIMPORTANT: has to be public.\r\n\r\nor this simple template:\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n\r\nIf you could not reproduce using the BoringModel and still think there's a bug, please post here\r\nbut remember, bugs with code are fixed faster!\r\n-->\r\n```py\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.loggers import MLFlowLogger ##### added #####\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        raise Exception ##### added #####\r\n        return {\"loss\": loss}\r\n        \r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    \r\n    mlf_logger = MLFlowLogger() ##### added #####\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        # enable_model_summary=False,\r\n        logger=mlf_logger ##### added #####\r\n    )\r\n    try:\r\n        trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n        trainer.test(model, dataloaders=test_data)\r\n    finally:\r\n        print(trainer.logger.experiment.get_run(trainer.logger._run_id).info.status) # This should be 'FAILED'\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\nStatus of each MLFlow's run is correctly updated when `pl.Trainer.fit` failed.\r\n\r\n### Environment\r\n\r\n<!--\r\nPlease copy and paste the output from our environment collection script:\r\nhttps:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/requirements\/collect_env_details.py\r\n(For security purposes, please check the contents of the script before running it)\r\n\r\nYou can get the script and run it with:\r\n```bash\r\nwget https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/requirements\/collect_env_details.py\r\npython collect_env_details.py\r\n```\r\n\r\nYou can also fill out the list below manually.\r\n-->\r\n\r\n- PyTorch Lightning Version: 1.4.9\r\n- MLFlow Version: 1.12.0\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Challenge_closed_time":1640642583000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1636290176000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a bug where external MLFlow logging failures cause the training job to fail. When the Databricks updates, the user loses access to MLFlow for a brief period, causing logging to fail. This error not only causes logging to fail but also causes the entire training pipeline to fail, losing progress on a potentially long-running job with limited error handling options currently available. The user is requesting flexibility in PyTorch Lightning to allow users to handle logging errors such that it will not always kill the training job.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/10397",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":45.61,
        "Challenge_repo_contributor_count":444.0,
        "Challenge_repo_fork_count":2922.0,
        "Challenge_repo_issue_count":15116.0,
        "Challenge_repo_star_count":23576.0,
        "Challenge_repo_watch_count":234.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":1209.0019444444,
        "Challenge_title":"`MLFlowLogger` does not update its status when `trainer.fit` failed",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":338,
        "Discussion_body":"This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1643255215608,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3075.0,
        "Answerer_view_count":3907.0,
        "Challenge_adjusted_solved_time":1.0057008333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a folder called data with a bunch of csvs (about 80), file sizes are fairly small. This data is clean and has already been preprocessed. I want to upload this data folder and register as a datastore in azureml. Which would be best for this scenario data store created with file share or a data store created with blob storage?<\/p>",
        "Challenge_closed_time":1658317354420,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658313733897,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to upload a folder containing preprocessed CSV files to AzureML and register it as a datastore. They are unsure whether to create a data store using file share or blob storage.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73050203",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.3,
        "Challenge_reading_time":4.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.0057008333,
        "Challenge_title":"azureml register datastore file share or blob storage",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":125.0,
        "Challenge_word_count":69,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1606756004663,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":49.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p><strong>AFAIK<\/strong>, based on your scenario you can make use of <strong><code>Azure File Share<\/code><\/strong> to create data store.<\/p>\n<p>Please <strong>note<\/strong> that, Azure Blob storage is suitable for uploading large amount of unstructured data whereas <strong><code>Azure File Share<\/code><\/strong> is suitable for uploading and processing the structured files in chunks (more interaction with app to share files).<\/p>\n<blockquote>\n<p>I have a folder called data with a bunch of csvs (about 80), file sizes are fairly small. This data is clean and has already been preprocessed.<\/p>\n<\/blockquote>\n<p>As you mentioned <strong><code>CSV<\/code><\/strong> data is clean and preprocessed it comes under structured data. So, you can make you of <strong>Azure File Share<\/strong> to create data store.<\/p>\n<p>To register a data store with <strong>Azure File Share<\/strong> you can make use of this <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore.datastore?view=azure-ml-py#azureml-core-datastore-datastore-register-azure-file-share\" rel=\"nofollow noreferrer\"><strong>MsDoc<\/strong><\/a><\/p>\n<p>To know more about Azure File Share and Azure Blob storage, please find below <strong>links<\/strong>:<\/p>\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/67217164\/azure-blob-storage-or-azure-file-storage\">Azure Blob Storage or Azure File Storage by Mike<\/a><\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_storage_datastore.azurefiledatastore?view=azure-ml-py\" rel=\"nofollow noreferrer\">azureml.data.azure_storage_datastore.AzureFileDatastore class - Azure Machine Learning Python | Microsoft Docs<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.7,
        "Solution_reading_time":22.4,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":165.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":44.4,
        "Challenge_answer_count":3,
        "Challenge_body":"I have been using the the autoML regression pipeline templates for a while successfully.\u00a0 Three days ago overnight they broke.\u00a0 The same job that worked the night before hangs with the error message: \"Unable to create pipeline run due to the following error: Input parameter type mismatch. PipelineSpec.root.input_definitions.parameters['dataflow_use_public_ips'] is defined as BOOLEAN that parses BOOL_VALUE type, but the default value is provided as NUMBER_VALUE type.\"\n\nIf I want to re-run a clone of a previously successful training the same happens.\u00a0 I have tried to set public IP setting to different values -- no success.\u00a0 I have downloaded and edited the yaml according to the error-- no luck either!\n\nAnybody encountered the same?\u00a0 Is there a workaround?",
        "Challenge_closed_time":1683696000000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1683536160000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with VertexAI autoML regression pipeline templates. The error message \"Input parameter type mismatch\" is displayed while creating a pipeline run due to a mismatch in the default value provided for the parameter 'dataflow_use_public_ips'. The user has tried different settings and editing the yaml file but has not been successful in resolving the issue. The user is seeking help for a workaround.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/VertexAI-autoML-pipeline-template-error-tabular-regression\/m-p\/551367#M1828",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.2,
        "Challenge_reading_time":10.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":44.4,
        "Challenge_title":"VertexAI autoML pipeline template error (tabular-regression)",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":80.0,
        "Challenge_word_count":120,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"OK!\u00a0 The original Google provided template is now fixed and running!\n\nThanks for the help!\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.7,
        "Solution_reading_time":1.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":20.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":21.1952627778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am looking for a way to import my data according to my data blueprint. My data scheme is changing depending on our scenario. It finally will be import as Pandas but I want to define the way. Is this possible?<\/p>",
        "Challenge_closed_time":1682886733486,
        "Challenge_comment_count":0,
        "Challenge_created_time":1682810430540,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is looking for a way to import data according to their changing data blueprint in Azure machine learning. They want to define the way of importing the data, which will eventually be in Pandas format.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1268796\/azure-machine-learning-data-scheme-blue-print-supp",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.9,
        "Challenge_reading_time":3.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":21.1952627778,
        "Challenge_title":"Azure machine learning data scheme blue print support?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":48,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <\/p>\n<p>Thanks for reaching out to us, one choice you may want to consider is MLtable - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-mltable?view=azureml-api-2&amp;tabs=cli\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-mltable?view=azureml-api-2&amp;tabs=cli<\/a><\/p>\n<p>Azure Machine Learning supports a Table type (<code>mltable<\/code>). This allows for the creation of a <em>blueprint<\/em> that defines how to load data files into memory as a Pandas or Spark data frame.<\/p>\n<p>This is very similar to the scenario you described. <\/p>\n<p>Azure Machine Learning Tables (<code>mltable<\/code>) allow you to define how you want to <em>load<\/em> your data files into memory, as a Pandas and\/or Spark data frame. Tables have two key features:<\/p>\n<ol>\n<li> <strong>An MLTable file.<\/strong> A YAML-based file that defines the data loading <em>blueprint<\/em>. In the MLTable file, you can specify:<\/li>\n<\/ol>\n<ul>\n<li> The storage location(s) of the data - local, in the cloud, or on a public http(s) server.<\/li>\n<li> <em>Globbing<\/em> patterns over cloud storage. These locations can specify sets of filenames, with wildcard characters (<code>*<\/code>).<\/li>\n<li> <em>read transformation<\/em> - for example, the file format type (delimited text, Parquet, Delta, json), delimiters, headers, etc.<\/li>\n<li> Column type conversions (enforce schema).<\/li>\n<li> New column creation, using folder structure information - for example, creation of a year and month column, using the <code>{year}\/{month}<\/code> folder structure in the path.<\/li>\n<li> <em>Subsets of data<\/em> to load - for example, filter rows, keep\/drop columns, take random samples.<\/li>\n<\/ul>\n<ol>\n<li> <strong>A fast and efficient engine<\/strong> to load the data into a Pandas or Spark dataframe, according to the blueprint defined in the MLTable file. The engine relies on <a href=\"https:\/\/www.rust-lang.org\/\">Rust<\/a> for high speed and memory efficiency.<\/li>\n<\/ol>\n<p>Please take a look at above and let me know if  this is what you are looking for, thanks.<\/p>\n<p>Regards,<\/p>\n<p>Yutong<\/p>\n<p>-Please kindly accept the answer and vote yes if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.3,
        "Solution_reading_time":28.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":292.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1553088438367,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tehran, Tehran Province, Iran",
        "Answerer_reputation_count":415.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":61.4036480555,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to log the plot of a <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_estimator\" rel=\"nofollow noreferrer\">confusion matrix generated with scikit-learn<\/a> for a <em>test<\/em> set using <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.sklearn.html\" rel=\"nofollow noreferrer\">mlflow's support for scikit-learn<\/a>.<\/p>\n<p>For this, I tried something that resemble the code below (I'm using mlflow hosted on Databricks, and <code>sklearn==1.0.1<\/code>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sklearn.datasets\nimport pandas as pd\nimport numpy as np\nimport mlflow\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nmlflow.set_tracking_uri(&quot;databricks&quot;)\nmlflow.set_experiment(&quot;\/Users\/name.surname\/plotcm&quot;)\n\ndata = sklearn.datasets.fetch_20newsgroups(categories=['alt.atheism', 'sci.space'])\n\ndf = pd.DataFrame(data = np.c_[data['data'], data['target']])\\\n       .rename({0:'text', 1:'class'}, axis = 'columns')\n\ntrain, test = train_test_split(df)\n\nmy_pipeline = Pipeline([\n    ('vectorizer', TfidfVectorizer()),\n    ('classifier', SGDClassifier(loss='modified_huber')),\n])\n\nmlflow.sklearn.autolog()\n\nfrom sklearn.metrics import ConfusionMatrixDisplay # should I import this after the call to `.autolog()`?\n\nmy_pipeline.fit(train['text'].values, train['class'].values)\n\ncm = ConfusionMatrixDisplay.from_predictions(\n      y_true=test[&quot;class&quot;], y_pred=my_pipeline.predict(test[&quot;text&quot;])\n  )\n<\/code><\/pre>\n<p>while the confusion matrix for the training set is saved in my mlflow run, no png file is created in the mlflow frontend for the <code>test<\/code> set.<\/p>\n<p>If I try to add<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>cm.figure_.savefig('test_confusion_matrix.png')\nmlflow.log_artifact('test_confusion_matrix.png')\n<\/code><\/pre>\n<p>that does the job, but requires explicitly logging the artifact.<\/p>\n<p>Is there an idiomatic\/proper way to autolog the confusion matrix computed using a test set after <code>my_pipeline.fit()<\/code>?<\/p>",
        "Challenge_closed_time":1658304934100,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657892681357,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to log the plot of a confusion matrix generated with scikit-learn for a test set using mlflow's support for scikit-learn. Although the confusion matrix for the training set is saved in the mlflow run, no png file is created in the mlflow frontend for the test set. The user is looking for an idiomatic\/proper way to autolog the confusion matrix computed using a test set after my_pipeline.fit().",
        "Challenge_last_edit_time":1658083880967,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72994988",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.0,
        "Challenge_reading_time":30.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":114.5146508333,
        "Challenge_title":"How to mlflow-autolog a sklearn ConfusionMatrixDisplay?",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":157.0,
        "Challenge_word_count":188,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1415722650716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Verona, VR, Italy",
        "Poster_reputation_count":4811.0,
        "Poster_view_count":713.0,
        "Solution_body":"<p>The proper way to do this is to use <code>mlflow.log_figure<\/code> as a fluent API announced in <code>MLflow 1.13.0<\/code>. You can read the documentation <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_figure\" rel=\"nofollow noreferrer\">here<\/a>. This code will do the job.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.log_figure(cm.figure_, 'test_confusion_matrix.png')\n<\/code><\/pre>\n<p>This function implicitly store the image, and then calls <code>log_artifact<\/code> against that path, something like you did.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.9,
        "Solution_reading_time":7.49,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":55.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":234.8477777778,
        "Challenge_answer_count":0,
        "Challenge_body":"### Contact Details [Optional]\n\nfrancogbocci@gmail.com\n\n### System Information\n\nZenML version: 0.20.5\r\nInstall path: \/Users\/f.bocci\/Library\/Caches\/pypoetry\/virtualenvs\/banana-bMSm4ime-py3.9\/lib\/python3.9\/site-packages\/zenml\r\nPython version: 3.9.6\r\nPlatform information: {'os': 'mac', 'mac_version': '10.15.7'}\r\nEnvironment: native\r\nIntegrations: ['gcp', 'graphviz', 'kubeflow', 'kubernetes', 'scipy', 'sklearn']\n\n### What happened?\n\nTrying to follow the [guide to run a pipeline using Vertex AI](https:\/\/blog.zenml.io\/vertex-ai-blog\/), it fails because ZenML does not now have a `metadata-store` stack category.\r\n\r\n```shell\r\n$ zenml\r\nStack Components:\r\n      alerter                 Commands to interact with alerters.\r\n      annotator               Commands to interact with annotators.\r\n      artifact-store          Commands to interact with artifact stores.\r\n      container-registry      Commands to interact with container registries.\r\n      data-validator          Commands to interact with data validators.\r\n      experiment-tracker      Commands to interact with experiment trackers.\r\n      feature-store           Commands to interact with feature stores.\r\n      model-deployer          Commands to interact with model deployers.\r\n      orchestrator            Commands to interact with orchestrators.\r\n      secrets-manager         Commands to interact with secrets managers.\r\n      step-operator           Commands to interact with step operators.\r\n$ zenml metadata-store\r\nError: No such command 'metadata-store'.\r\n```\n\n### Reproduction steps\n\n1. zenml metadata-store\r\n\r\nIf I don't add it and run the Vertex AI pipeline, it fails.\r\n\n\n### Relevant log output\n\n_No response_\n\n### Code of Conduct\n\n- [X] I agree to follow this project's Code of Conduct",
        "Challenge_closed_time":1667472145000,
        "Challenge_comment_count":7,
        "Challenge_created_time":1666626693000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a bug where the confusion matrix appears in the w&b page without the values after running a model evaluation suite and exploring to wandb using \"to_wandb\" function. The expected behavior is for the confusion matrix in w&b to appear like the confusion matrix in the notebook which has its values shown. The user is using Linux OS, Python version 3.7.1, and Deepchecks version 0.7.2.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/zenml-io\/zenml\/issues\/1001",
        "Challenge_link_count":1,
        "Challenge_participation_count":7,
        "Challenge_readability":11.1,
        "Challenge_reading_time":20.77,
        "Challenge_repo_contributor_count":66.0,
        "Challenge_repo_fork_count":300.0,
        "Challenge_repo_issue_count":1578.0,
        "Challenge_repo_star_count":2887.0,
        "Challenge_repo_watch_count":40.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":234.8477777778,
        "Challenge_title":"[BUG]: Vertex AI blogpost is outdated after 0.20.0 release",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":186,
        "Discussion_body":"@francobocciDH Thanks for reporting the issue. We have recently undergone a [big architectural shift](https:\/\/blog.zenml.io\/zenml-revamped\/) and therefore a lot of the blog is a bit outdated! In particular, the metadata store is no longer a required stack component.\r\n\r\nIn order to make the vertex orchestrator work, I would suggest either taking a look at the [updated docs page](https:\/\/docs.zenml.io\/component-gallery\/orchestrators\/gcloud-vertexai), or taking a look at the [migration guide](https:\/\/docs.zenml.io\/guidelines\/migration-zero-twenty) that will help you update that blog's code to  the 0.20.5 world. Hey! Thanks for the quick reply. I followed the updated docs page. I checked the post as well to see if there is something different, but following the docs I'm still getting the error\r\n```\r\nMaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=8237): Max retries exceeded with url: \/api\/v1\/login (Caused by \r\nNewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f46e31ea0a0>: Failed to establish a new connection: [Errno 111] Connection refused'))\r\nConnectionError: HTTPConnectionPool(host='127.0.0.1', port=8237): Max retries exceeded with url: \/api\/v1\/login (Caused by \r\nNewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f46e31ea0a0>: Failed to establish a new connection: [Errno 111] Connection refused'))\r\n```\r\n\r\nFor what I saw following the traceback, it's something related to:\r\n`\/usr\/local\/lib\/python3.9\/site-packages\/zenml\/zen_stores\/base_zen_store.py:104`\r\nbut I haven't solved it yet.\r\n\r\nCould you follow the steps on the guide and make it work? I downloaded the image, got into the container and launch the entrypoint being used in Vertex AI:\r\n`python -m zenml.entrypoints.entrypoint --entrypoint_config_source zenml.integrations.gcp.orchestrators.vertex_entrypoint_configuration.VertexEntrypointConfiguration@zenml_0.20.5 --step_name importer --vertex_job_id test1234`\r\n\r\nAnd I got the same error. After that, I ran the ZenML Server (`zenml up`), and I got a different error (so apparently, something's missing?)\r\n\r\nThe error I'm getting now comes from `tfx` package and it's:\r\n```\r\nThe filesystem scheme 'gs:\/\/' is not available for use. For expanded filesystem scheme support, install the `tensorflow` package to enable additional filesystem plugins\r\n```\r\n\r\n I made it work locally. I had to:\r\n1) Register the `artifact-store` using GCS\r\n2) Set it as the artifact-store in the \"default\" stack\r\n3) Start zenml server\r\n\r\nShould this be done in some specific way by the user? @francobocciDH I think the main problem you are suffering from is that you have not deployed ZenML on Google before doing all this. Its our fault as I see that the Vertex orchestrator guide does not make this clear at all (only if you read the docs from the top, it does).\r\n\r\nPlease try [deploying ZenML](https:\/\/docs.zenml.io\/getting-started\/deploying-zenml) to google first. The easiest way to do it is to do:\r\n\r\n```\r\nzenml deploy\r\n```\r\n\r\nAfter you have done this, you can connect to the remote ZenML deployemnt, and re-register your stack as described in the Vertex AI docs, and then run your pipeline. It should work then! @francobocciDH Did this work out? Hey @htahir1 , yes, I deployed it and it worked. It could be clearer in the Vertex AI section of the docs, but it is clearly mentioned in other places of the documentation, so it's my fault for missing this. We can close this from my side. Let me know if there is anything I can help with \ud83d\udc4d ",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":193.1276480556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm very excited on the newly released Azure Machine Learning service (preview), which is a great step up from the previous (and deprecated) Machine Learning Workbench.<\/p>\n\n<p>However, I am thinking a lot about the best practice on structuring the folders and files in my project(s). I'll try to explain my thoughts.<\/p>\n\n<p>Looking at the documentation for the training of a model (e.g. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml#create-an-estimator\" rel=\"nofollow noreferrer\">Tutorial #1<\/a>), there seems to be good-practice to put all training scripts and necessary additional scripts inside a subfolder, so that it can be passed into the <code>Estimator<\/code> object without also passing all other files in the project. This is fine.<\/p>\n\n<p>But when working with the deployment of the service, specifically the deployment of the image, the documentation (e.g. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml#deploy-in-aci\" rel=\"nofollow noreferrer\">Tutorial #2<\/a>) seems to indicate that the scoring script need to be located in the root folder. If I try to refer to a script located in a subfolder, I get an error message saying<\/p>\n\n<p><code>WebserviceException: Unable to use a driver file not in current directory. Please navigate to the location of the driver file and try again.<\/code><\/p>\n\n<p>This may not be a big deal. Except, I have some additional scripts that I import both in the training script and in the scoring script, and I don't want to duplicate those additional scripts to be able to import them in both the training and the scoring scripts.<\/p>\n\n<p>I am working mainly in Jupyter Notebooks when executing the training and the deployment, and I could of course use some tricks to read the particular scripts from some other folder, save them to disk as a copy, execute the training or deployment while referring to the copies and finally delete the copies. This would be a decent workaround, but it seems to me that there should be a better way than just decent.<\/p>\n\n<p>What do you think?<\/p>",
        "Challenge_closed_time":1540309291403,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539614031870,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is seeking advice on the best practice for structuring folders and files in Azure Machine Learning service (preview) projects. While the documentation suggests putting all training scripts and necessary additional scripts inside a subfolder, the scoring script needs to be located in the root folder for deployment. The user is concerned about duplicating additional scripts to import them in both the training and scoring scripts and is looking for a better solution.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52819122",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":28.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":193.1276480556,
        "Challenge_title":"What is the best practice on folder structure for Azure Machine Learning service (preview) projects",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":782.0,
        "Challenge_word_count":326,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1463756509236,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Uppsala, Sverige",
        "Poster_reputation_count":400.0,
        "Poster_view_count":43.0,
        "Solution_body":"<p>Currently, the score.py needs to be in current working directory, but dependency scripts - the <em>dependencies<\/em> argument to  <em>ContainerImage.image_configuration<\/em> - can be in a subfolder.<\/p>\n\n<p>Therefore, you should be able to use folder structure like this:<\/p>\n\n<pre><code>.\/score.py \n.\/myscripts\/train.py \n.\/myscripts\/common.py\n<\/code><\/pre>\n\n<p>Note that the relative folder structure is preserved during web service deployment; if you reference the common file in subfolder from your score.py, that reference should be valid within deployed image.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.1,
        "Solution_reading_time":7.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":69.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1558539179612,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":16.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":4461.4114333334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'd like to (a) plot SHAP values out of the SageMaker (b) AutoML pipeline. To achieve (a), debugger shall be used according to: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/ml-explainability-with-amazon-sagemaker-debugger\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/ml-explainability-with-amazon-sagemaker-debugger\/<\/a>.<\/p>\n\n<p>But how to enable the debug model in the AutoPilot without hacking into the background?<\/p>",
        "Challenge_closed_time":1607117580710,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591056499550,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user wants to plot SHAP values out of the SageMaker AutoML pipeline and needs to enable the debugger to achieve this. They are seeking guidance on how to enable the debug model in the AutoPilot without hacking into the background.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62142825",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":17.8,
        "Challenge_reading_time":6.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":4461.4114333334,
        "Challenge_title":"How to Enable SageMaker Debugger in the SageMaker AutoPilot",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":215.0,
        "Challenge_word_count":50,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1392607100776,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sydney NSW, Australia",
        "Poster_reputation_count":133.0,
        "Poster_view_count":29.0,
        "Solution_body":"<p>SageMaker Autopilot doesn't support SageMaker Debugger out of the box currently (as of Dec 2020). You can hack the Hyperparameter Tuning job to pass in a debug parameter.<\/p>\n<p>However, there is a way to use SHAP with Autopilot models. Take a look at this blog post explaining how to use SHAP with SageMaker Autopilot: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/explaining-amazon-sagemaker-autopilot-models-with-shap\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/explaining-amazon-sagemaker-autopilot-models-with-shap\/<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.1,
        "Solution_reading_time":7.55,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":58.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":120.5566666667,
        "Challenge_answer_count":0,
        "Challenge_body":"Currently, it will display always display\r\n`========== Make your selection, Press \"h\" for help ==========`\r\neven if there is no selection to make since the list of files is empty\r\n\r\nhttps:\/\/github.com\/DAGsHub\/fds\/blob\/a8fea54f59131d3ddea4df5184adeee3ecc9998f\/fds\/services\/dvc_service.py#L119",
        "Challenge_closed_time":1622551859000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1622117855000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges with the current implementation of the rename action in DVC, which uses shutils' mv command to rename the base image. This approach is invalid as it creates two identical files with different names when a DVC pull is performed. The user suggests using the dvc rename command to rename the actual file, pointer, and update the path property for consistency.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/37",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":4.48,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":145.0,
        "Challenge_repo_star_count":369.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":120.5566666667,
        "Challenge_title":"Only display the DVC add prompt if there is anything to add",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":40,
        "Discussion_body":"Fixed in #46 ",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":703.9719444444,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nWhen testing a model with `Trainer.test` metrics are not logged to Comet if the model was previously trained using `Trainer.fit`. While training metrics are logged correctly.\r\n\r\n\r\n#### Code sample\r\n```\r\n    comet_logger = CometLogger()\r\n    trainer = Trainer(logger=comet_logger)\r\n    model = get_model()\r\n\r\n    trainer.fit(model) # Metrics are logged to Comet\r\n    trainer.test(model) # No metrics are logged to Comet\r\n```\r\n\r\n### Expected behavior\r\n\r\nTest metrics should also be logged in to Comet.\r\n\r\n### Environment\r\n\r\n```\r\n- PyTorch version: 1.3.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.168\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 418.67\r\ncuDNN version: \/usr\/local\/cuda-10.1\/targets\/x86_64-linux\/lib\/libcudnn.so.7.6.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.4\r\n[pip3] pytorch-lightning==0.6.0\r\n[pip3] torch==1.3.0\r\n[pip3] torchvision==0.4.1\r\n[conda] Could not collect\r\n```\r\n\r\n### Additional context\r\n\r\nI believe the issue is caused because at the [end of the training routine](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/deffbaba7ffb16ff57b56fe65f62df761f25fbd6\/pytorch_lightning\/trainer\/training_loop.py#L366), `logger.finalize(\"success\")` is called. This in turn calls `experiment.end()` inside the logger and the `Experiment` object doesn't expect to send more information after this.\r\n\r\nAn alternative is to create another `Trainer` object, with another logger but this means that the metrics will be logged into a different Comet experiment from the original. This issue can be solved using the `ExistingExperiment` object form the Comet SDK, but the solution seems a little hacky and the `CometLogger` currently doesn't support this kind of experiment.\r\n",
        "Challenge_closed_time":1582760093000,
        "Challenge_comment_count":10,
        "Challenge_created_time":1580225794000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a NotImplementedError when trying to create a CometLogger instance and passing it to Trainer using trainer(logger=my_comet_logger) because CometLogger does not implement the name() and version() class methods. This raises an error when the logger version is checked during training.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/760",
        "Challenge_link_count":1,
        "Challenge_participation_count":10,
        "Challenge_readability":8.6,
        "Challenge_reading_time":26.63,
        "Challenge_repo_contributor_count":444.0,
        "Challenge_repo_fork_count":2922.0,
        "Challenge_repo_issue_count":15116.0,
        "Challenge_repo_star_count":23576.0,
        "Challenge_repo_watch_count":234.0,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":703.9719444444,
        "Challenge_title":"Test metrics not logging to Comet after training",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":277,
        "Discussion_body":"Did you find a solution?\r\nMind submitting a PR?\r\n@fdelrio89  I did solve the issue but in a kind of hacky way. It's not that elegant but it works for me, and I haven't had the time to think of a better solution.\r\n\r\nI solved it by getting the experiment key and creating another logger and trainer with it.\r\n```\r\n    comet_logger = CometLogger()\r\n    trainer = Trainer(logger=comet_logger)\r\n    model = get_model()\r\n\r\n    trainer.fit(model)\r\n\r\n    experiment_key = comet_logger.experiment.get_key()\r\n    comet_logger = CometLogger(experiment_key=experiment_key)\r\n    trainer = Trainer(logger=comet_logger)\r\n\r\n    trainer.test(model)\r\n```\r\n\r\nFor this to work, I had to modify the `CometLogger` class to accept the `experiment_key` and create a `CometExistingExperiment` from the Comet SDK when this param is present.\r\n\r\n```\r\nclass CometLogger(LightningLoggerBase):\r\n     ...\r\n\r\n    @property\r\n    def experiment(self):\r\n        ...\r\n\r\n        if self.mode == \"online\":\r\n            if self.experiment_key is None:\r\n                self._experiment = CometExperiment(\r\n                    api_key=self.api_key,\r\n                    workspace=self.workspace,\r\n                    project_name=self.project_name,\r\n                    **self._kwargs\r\n                )\r\n            else:\r\n                self._experiment = CometExistingExperiment(\r\n                    api_key=self.api_key,\r\n                    workspace=self.workspace,\r\n                    project_name=self.project_name,\r\n                    previous_experiment=self.experiment_key,\r\n                    **self._kwargs\r\n                )\r\n        else:\r\n            ...\r\n\r\n        return self._experiment\r\n```\r\n\r\nI can happily do the PR if this solution is acceptable for you guys, but I think a better solution can be achieved I haven't had the time to think about it @williamFalcon. @williamFalcon Any progress on this Issue? I am facing the same problem.\r\n @fdelrio89 Since the logger object is available for the lifetime of the trainer, maybe you can refactor to store the `experiment_key` directly in the logger object itself, instead of having to re-instantiate the logger.  @xssChauhan good idea, I just submitted a PR (https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/pull\/892) considering this. Thanks!\r\n I assume that it was fixed by #892\r\n if you have some other problems feel free to reopen or create a new... :robot:  Actually I'm still facing the problem. @dvirginz are you using the latest master? may you provide a minimal example? > @dvirginz are you using the latest master? may you provide a minimal example?\r\n\r\nYou are right, sorry. \r\nAfter building from source it works.  I should probably open a new issue, but it happens with Weights & Biases logger too. I haven't had the time to delve deep into it yet.",
        "Discussion_score_count":4.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":1334762714136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation_count":6557.0,
        "Answerer_view_count":2005.0,
        "Challenge_adjusted_solved_time":15.7899352778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When I upload dataset with more then 100 columns I can see only part of them in the visualisation block. Can I see stats for all columns from dataset? Thanks<\/p>",
        "Challenge_closed_time":1459517016507,
        "Challenge_comment_count":0,
        "Challenge_created_time":1459460172740,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge with Azure machine learning as they are unable to view all columns of a dataset with more than 100 columns in the visualization block. They are seeking a solution to view statistics for all columns in the dataset.",
        "Challenge_last_edit_time":1459517035743,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36344278",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":3.5,
        "Challenge_reading_time":2.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":15.7899352778,
        "Challenge_title":"Azure machine learning. How can I see all columns",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":736.0,
        "Challenge_word_count":38,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459459387887,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>If you are an owner in the workspace, you can open your dataset in Python inside of a Jupyter Notebook. By the visualize should be an open in notebook button. Then just execute the code that is provided for you, and it should print your dataset. You can then also select specific columns to visualize as well.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":3.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":57.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":42.8398436111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What's the best way to preserve Azure ML workspace so that it can be restored at a later point? I was hoping to find some automatic way to take a snapshot of artifacts &amp; code and dump it into Azure storage, but haven't been able to find anything relevant in the online documentation. <\/p>",
        "Challenge_closed_time":1669596362440,
        "Challenge_comment_count":0,
        "Challenge_created_time":1669442139003,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is looking for the best way to preserve their Azure ML workspace so that it can be restored later. They are seeking an automatic way to take a snapshot of artifacts and code and store it in Azure storage, but have not found any relevant information in the online documentation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1105130\/whats-the-best-way-to-preserve-azure-ml-workspace",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":4.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":42.8398436111,
        "Challenge_title":"What's the best way to preserve Azure ML workspace so that it can be restored",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":68,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@VaraPrasad-1740 Thanks for the question. I would recommend you can have a git repository that backs your project.  For some details about this approach you can check <a href=\"https:\/\/santiagof.medium.com\/structure-your-machine-learning-project-source-code-like-a-pro-44815cac8652\">https:\/\/santiagof.medium.com\/structure-your-machine-learning-project-source-code-like-a-pro-44815cac8652<\/a><\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.7,
        "Solution_reading_time":5.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":29.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1467035387036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":1277.0,
        "Answerer_view_count":42.0,
        "Challenge_adjusted_solved_time":191.1962875,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to create an Azure DEVOPS ML Pipeline. The following code works 100% fine on Jupyter Notebooks, but when I run it in Azure Devops I get this error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;src\/my_custom_package\/data.py&quot;, line 26, in &lt;module&gt;\n    ws = Workspace.from_config()\n  File &quot;\/opt\/hostedtoolcache\/Python\/3.8.7\/x64\/lib\/python3.8\/site-packages\/azureml\/core\/workspace.py&quot;, line 258, in from_config\n    raise UserErrorException('We could not find config.json in: {} or in its parent directories. '\nazureml.exceptions._azureml_exception.UserErrorException: UserErrorException:\n    Message: We could not find config.json in: \/home\/vsts\/work\/1\/s or in its parent directories. Please provide the full path to the config file or ensure that config.json exists in the parent directories.\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;message&quot;: &quot;We could not find config.json in: \/home\/vsts\/work\/1\/s or in its parent directories. Please provide the full path to the config file or ensure that config.json exists in the parent directories.&quot;\n    }\n}\n<\/code><\/pre>\n<p>The code is:<\/p>\n<pre><code>#import\nfrom sklearn.model_selection import train_test_split\nfrom azureml.core.workspace import Workspace\nfrom azureml.train.automl import AutoMLConfig\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\nfrom azureml.core.experiment import Experiment\nfrom datetime import date\nfrom azureml.core import Workspace, Dataset\n\n\n\nimport pandas as pd\nimport numpy as np\nimport logging\n\n#getdata\nsubscription_id = 'mysubid'\nresource_group = 'myrg'\nworkspace_name = 'mlplayground'\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\ndataset = Dataset.get_by_name(workspace, name='correctData')\n\n\n#auto ml\nws = Workspace.from_config()\n\n\nautoml_settings = {\n    &quot;iteration_timeout_minutes&quot;: 2880,\n    &quot;experiment_timeout_hours&quot;: 48,\n    &quot;enable_early_stopping&quot;: True,\n    &quot;primary_metric&quot;: 'spearman_correlation',\n    &quot;featurization&quot;: 'auto',\n    &quot;verbosity&quot;: logging.INFO,\n    &quot;n_cross_validations&quot;: 5,\n    &quot;max_concurrent_iterations&quot;: 4,\n    &quot;max_cores_per_iteration&quot;: -1,\n}\n\n\n\ncpu_cluster_name = &quot;computecluster&quot;\ncompute_target = ComputeTarget(workspace=ws, name=cpu_cluster_name)\nprint(compute_target)\nautoml_config = AutoMLConfig(task='regression',\n                             compute_target = compute_target,\n                             debug_log='automated_ml_errors.log',\n                             training_data = dataset,\n                             label_column_name=&quot;paidInDays&quot;,\n                             **automl_settings)\n\ntoday = date.today()\nd4 = today.strftime(&quot;%b-%d-%Y&quot;)\n\nexperiment = Experiment(ws, &quot;myexperiment&quot;+d4)\nremote_run = experiment.submit(automl_config, show_output = True)\n\nfrom azureml.widgets import RunDetails\nRunDetails(remote_run).show()\n\nremote_run.wait_for_completion()\n<\/code><\/pre>",
        "Challenge_closed_time":1612865840128,
        "Challenge_comment_count":0,
        "Challenge_created_time":1612177533493,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while creating an Azure DevOps ML Pipeline. The error message indicates that the config.json file cannot be found in the specified directory or its parent directories. The code works fine on Jupyter Notebooks but not on Azure DevOps. The user has provided the code used for the pipeline.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65991587",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":16.0,
        "Challenge_reading_time":40.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":191.1962875,
        "Challenge_title":"AzureDevOPS ML Error: We could not find config.json in: \/home\/vsts\/work\/1\/s or in its parent directories",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2339.0,
        "Challenge_word_count":265,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1302030303092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brussels, B\u00e9lgica",
        "Poster_reputation_count":30340.0,
        "Poster_view_count":2937.0,
        "Solution_body":"<p>There is something weird happening on your code, you are getting the data from a first workspace (<code>workspace = Workspace(subscription_id, resource_group, workspace_name)<\/code>), then using the resources from a second one (<code>ws = Workspace.from_config()<\/code>). I would suggest avoiding having code relying on two different workspaces, especially when you know that an underlying datasource can be registered (linked) to multiple workspaces (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#create-and-register-datastores\" rel=\"nofollow noreferrer\">documentation<\/a>).<\/p>\n<p>In general using a <code>config.json<\/code> file when instantiating a <code>Workspace<\/code> object will result in an interactive authentication. When your code will be processed and you will have a log asking you to reach a specific URL and enter a code. This will use your Microsoft account to verify that you are authorized to access the Azure resource (in this case your <code>Workspace('mysubid', 'myrg', 'mlplayground')<\/code>). This has its limitations when you start deploying the code onto virtual machines or agents, you will not always manually check the logs, access the URL and authenticate yourself.<\/p>\n<p>For this matter it is strongly recommended setting up more advanced authentication methods and personally I would suggest using the service principal one since it is simple, convinient and secure if done properly.\nYou can follow Azure's official documentation <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-setup-authentication#configure-a-service-principal\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.8,
        "Solution_reading_time":21.59,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":197.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1124.7483333333,
        "Challenge_answer_count":0,
        "Challenge_body":"The artifact folder by default is not reemplacing the `$ARTIFACTS_BUCKET` env var",
        "Challenge_closed_time":1623230636000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1619181542000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a \"PermissionError\" while trying to log models to mlflow on their Mac. The error occurred when the program attempted to create a directory \"\/var\/lib\/mlflow\" and was denied permission. The user is running mlflow version 1.2 on macOS 12.1.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/380",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":1.51,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":933.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":1124.7483333333,
        "Challenge_title":"Bad MLflow artifact folder by default",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":17,
        "Discussion_body":"This problem has been solved adding the variable of `$ARTIFACTS_BUCKET` between `()` like this `$(ARTIFACTS_BUCKET)` in the deployment.yaml of the project-operator.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1353403873547,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":4909.0,
        "Answerer_view_count":583.0,
        "Challenge_adjusted_solved_time":370.54581,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Optuna's FAQ has a <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/faq.html#id10\" rel=\"nofollow noreferrer\">clear answer<\/a> when it comes to dynamically adjusting the range of parameter during a study: it poses no problem since each sampler is defined individually.<\/p>\n<p>But what about adding and\/or removing parameters? Is Optuna able to handle such adjustments?<\/p>\n<p>One thing I noticed when doing this is that in the results dataframe these parameters get <code>nan<\/code> entries for other trials. Would there be any benefit to being able to set these <code>nan<\/code>s to their (default) value that they had when not being sampled? Is the study still sound with all these unknown values?<\/p>",
        "Challenge_closed_time":1609649865303,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608315900387,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is questioning whether Optuna can handle adding and\/or removing parameters dynamically during a study. They have noticed that when doing so, the results dataframe shows \"nan\" entries for other trials. The user is wondering if it would be beneficial to set these \"nan\" values to their default value and if the study is still valid with these unknown values.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65362133",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":9.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":370.54581,
        "Challenge_title":"What happens when I add\/remove parameters dynamically during an Optuna study?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":227.0,
        "Challenge_word_count":110,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1353403873547,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":4909.0,
        "Poster_view_count":583.0,
        "Solution_body":"<p>Question was answered <a href=\"https:\/\/github.com\/optuna\/optuna\/issues\/2141\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<blockquote>\n<p>Thanks for the question. Optuna internally supports two types of sampling: <code>optuna.samplers.BaseSampler.sample_independent<\/code> and <code>optuna.samplers.BaseSampler.sample_relative<\/code>.<\/p>\n<p>The former <code>optuna.samplers.BaseSampler.sample_independent<\/code> is a method that samples independently on each parameter, and is not affected by the addition or removal of parameters. The added parameters are taken into account from the timing when they are added.<\/p>\n<p>The latter <code>optuna.samplers.BaseSampler.sample_relative<\/code> is a method that samples by considering the correlation of parameters and is affected by the addition or removal of parameters. Optuna's default search space for correlation is the product set of the domains of the parameters that exist from the beginning of the hyperparameter tuning to the present. Developers who implement samplers can implement their own search space calculation method <code>optuna.samplers.BaseSampler.infer_relative_search_space<\/code>. This may allow correlations to be considered for hyperparameters that have been added or removed, but this depends on the sampling algorithm, so there is no API for normal users to modify.<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.3,
        "Solution_reading_time":17.6,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":157.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1412713062067,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Munich, Germany",
        "Answerer_reputation_count":3603.0,
        "Answerer_view_count":228.0,
        "Challenge_adjusted_solved_time":5092.1609733333,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I want to know if it's possible to get an Amazon ECR container URI for a specific image programmatically (using AWS CLI or Python). For example, if I need the URL for the latest <code>linear-learner<\/code> (built-in model) image for the <code>eu-central-1<\/code> region.<\/p>\n<p>Expected result:<\/p>\n<pre><code>664544806723.dkr.ecr.eu-central-1.amazonaws.com\/linear-learner:latest\n<\/code><\/pre>\n<p>EDIT: I have found the solution with <code>get_image_uri<\/code>. It looks like this function will be depreceated and I don't know how to use <code>ImageURIProvider<\/code> instead.<\/p>",
        "Challenge_closed_time":1617813746592,
        "Challenge_comment_count":3,
        "Challenge_created_time":1599474994217,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking a way to programmatically obtain an Amazon ECR container URI for a specific image, such as the latest linear-learner built-in model image for the eu-central-1 region, using AWS CLI or Python. The user has found a solution using get_image_uri but is unsure how to use ImageURIProvider instead, as get_image_uri is being deprecated.",
        "Challenge_last_edit_time":1599481967088,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63775893",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":11.2,
        "Challenge_reading_time":8.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":5094.0978819444,
        "Challenge_title":"How to get an Amazon ECR container URI for a specific model image in Sagemaker?",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":3017.0,
        "Challenge_word_count":85,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1511210305768,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bad Orb, Germany",
        "Poster_reputation_count":12908.0,
        "Poster_view_count":1267.0,
        "Solution_body":"<p>The newer versions of SageMaker SDK have a more centralized API for getting the URIs:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sagemaker \nsagemaker.image_uris.retrieve(&quot;linear-learner&quot;, &quot;eu-central-1&quot;)\n<\/code><\/pre>\n<p>which gives the expected result:<\/p>\n<pre><code>664544806723.dkr.ecr.eu-central-1.amazonaws.com\/linear-learner:1\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":20.7,
        "Solution_reading_time":5.24,
        "Solution_score_count":4.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":29.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":46.0753738889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello:  <\/p>\n<p>I want to know that if it is possible automate copy file from azure storage to Azure ML folder.  <\/p>\n<p>I understand that it is duplication of data, but I want to know if yes, how I can do that.  <\/p>\n<p>Any pointer is greatly appreciated.  <\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1626436211016,
        "Challenge_comment_count":2,
        "Challenge_created_time":1626270339670,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information on how to automate the process of copying files from Azure storage to Azure ML folder, despite acknowledging that it involves duplication of data. They are requesting any guidance on how to achieve this.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/475768\/azure-ml-datastoredatasets",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.2,
        "Challenge_reading_time":3.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":46.0753738889,
        "Challenge_title":"Azure ML Datastore\\Datasets",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":52,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Depending on the frequency at which you would like to move data you can create scripts that could run on crontab to move the data between source storage account to your workspace blob store. For example, use <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/storage\/common\/storage-use-azcopy-blobs-copy?toc=\/azure\/storage\/blobs\/toc.json\">azcopy<\/a> to perform this activity.    <\/p>\n<p>A very comprehensive method to move storage between storage accounts is available as a <a href=\"https:\/\/learn.microsoft.com\/en-us\/learn\/modules\/copy-blobs-from-command-line-and-code\/\">Microsoft learn module<\/a> that you could take to understand the possibilities and attain this from code to automate in your application.     <\/p>\n<p> I would ideally assume that you would like to pull data when your experiment kicks off because you cannot move data to an experiments run id folder unless the experiment has started, In this case you could use the first option to place the data in your workspace blob store and then use it in your experiment without moving it to any other storage. I hope this helps.     <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.9,
        "Solution_reading_time":13.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":151.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1546882781360,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":106.0,
        "Answerer_view_count":41.0,
        "Challenge_adjusted_solved_time":248.7106977778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>What is the minimum number of text rows needed for ground truth to do auto-labelling ? I have text file which contains 1000 rows, is this good enough to get started with auto-labelling by sagemaker ground truth ?<\/p>",
        "Challenge_closed_time":1554582491992,
        "Challenge_comment_count":0,
        "Challenge_created_time":1553687133480,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the minimum number of text rows required for Amazon Sagemaker ground truth to perform auto-labeling. They have a text file with 1000 rows and are wondering if this is sufficient to begin auto-labeling.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55376406",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":3.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":248.7106977778,
        "Challenge_title":"Auto labeling for Text Data with Amazon Sagemaker ground truth",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":374.0,
        "Challenge_word_count":46,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1482399248096,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":61.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>I'm a product manager on the Amazon SageMaker Ground Truth team, and I'm happy to help you with this question. The minimum system requirement is 1,000 objects. In practice with text classification, we typically see meaningful results (% of data auto-labeled) only once you have 2,000 to 3,000 text objects. Remember performance is variable and depends on your dataset and the complexity of your task.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.5,
        "Solution_reading_time":5.04,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1262067470272,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Los Angeles, CA, United States",
        "Answerer_reputation_count":11653.0,
        "Answerer_view_count":727.0,
        "Challenge_adjusted_solved_time":162.6529575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>So I'm trying to use <code>tf.contrib.learn.preprocessing.VocabularyProcessor.restore()<\/code> to restore a vocabulary file from an S3 bucket. First, I tried to get the path name to the bucket to use in <code>.restore()<\/code> and I kept getting 'object doesn't exist' error. Afterwards, upon further research, I found a method people use to load text files and JSON files and applied the same method here:<\/p>\n\n<pre><code>obj = s3.Object(BUCKET_NAME, KEY).get()['Body'].read()\nvocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor.restore(obj)\n<\/code><\/pre>\n\n<p>This worked for a while until the contents of file increased and eventually got a 'File name too long' error. Is there a better way to load and restore a file from an S3 bucket? <\/p>\n\n<p>By the way, I tested this out locally on my machine and it works perfectly fine since there it just needs to take the path to the file, not the entire contents of the file. <\/p>",
        "Challenge_closed_time":1521485629848,
        "Challenge_comment_count":0,
        "Challenge_created_time":1521471311783,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to restore a vocabulary file from an S3 bucket using tf.contrib.learn.preprocessing.VocabularyProcessor.restore(). Initially, they faced an 'object doesn't exist' error while trying to get the path name to the bucket. Later, they used a method to load text and JSON files, which worked until they encountered a 'File name too long' error. The user is seeking a better way to load and restore a file from an S3 bucket.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49365900",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":12.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":3.9772402778,
        "Challenge_title":"What's a better way to load a file using boto? (getting filename too long error)",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":532.0,
        "Challenge_word_count":153,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521470035783,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":210.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>It looks like you\u2019re passing in the actual contents of the file as the file name?<\/p>\n\n<p>I think you\u2019ll need to download the object from S3 to a tmp file and pass the path to that file into restore.<\/p>\n\n<p>Try using the method here: <a href=\"http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/s3.html#S3.Object.download_file\" rel=\"nofollow noreferrer\">http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/s3.html#S3.Object.download_file<\/a><\/p>\n\n<p>Update:\nI went through the code here: <a href=\"https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/contrib\/learn\/python\/learn\/preprocessing\/text.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/contrib\/learn\/python\/learn\/preprocessing\/text.py<\/a> and it looks like this just saves a pickle so you can really easily just import pickle and call the following:<\/p>\n\n<pre><code>import pickle\nobj = s3.Object(BUCKET_NAME, KEY).get()['Body']\nvocab_processor = pickle.loads(obj.read())\n<\/code><\/pre>\n\n<p>Hopefully that works?<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1522056862430,
        "Solution_link_count":4.0,
        "Solution_readability":16.7,
        "Solution_reading_time":13.9,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":91.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1421238326280,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Melrose, Johannesburg, Gauteng, South Africa",
        "Answerer_reputation_count":1951.0,
        "Answerer_view_count":217.0,
        "Challenge_adjusted_solved_time":0.5754191667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Let's say I have some text data that has already been labeled in SageMaker. This data could have either been labeled by humans or an ner model. Then let's say I want to have a human go back over the dataset, either to label new entity class or correct existing labels. How would I set up a labeling job to allow this? I tried using an output manifest from another labeling job, but all of the documents that were already labeled cannot be accessed by workers to re-label.<\/p>",
        "Challenge_closed_time":1609890680276,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609888608767,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in using pre-labeled data in AWS SageMaker Ground Truth NER. They want to have a human go back over the dataset to label new entity class or correct existing labels, but are unable to access the documents that were already labeled. They are seeking guidance on how to set up a labeling job to allow this.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65587939",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":6.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.5754191667,
        "Challenge_title":"Can I use pre-labeled data in AWS SageMaker Ground Truth NER?",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":367.0,
        "Challenge_word_count":98,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1588952728736,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Yes, this is possible you are looking for <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates.html\" rel=\"nofollow noreferrer\">Custom Labelling worklflows<\/a> you can also apply either Majority Voting (MV) or MDS to evaluate the accuracy of the job<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.2,
        "Solution_reading_time":3.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1393524211332,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":745.0,
        "Answerer_view_count":71.0,
        "Challenge_adjusted_solved_time":355.1771858333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm training in SageMaker using TensorFlow + Script Mode and currently using 'File' input mode for my data.<\/p>\n\n<p>Has anyone figured out how to stream data using 'Pipe' data format in conjunction with Script Mode training?<\/p>",
        "Challenge_closed_time":1551201776032,
        "Challenge_comment_count":1,
        "Challenge_created_time":1549915827960,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge while training in SageMaker using TensorFlow + Script Mode and wants to know if anyone has figured out how to stream data using 'Pipe' data format in conjunction with Script Mode training.",
        "Challenge_last_edit_time":1549923138163,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54638364",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.4,
        "Challenge_reading_time":3.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":357.2077977778,
        "Challenge_title":"SageMaker Script Mode + Pipe Mode",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1385.0,
        "Challenge_word_count":39,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>You can import <code>sagemaker_tensorflow<\/code> from the training script as follows:<\/p>\n\n<pre><code>from sagemaker_tensorflow import PipeModeDataset\nfrom tensorflow.contrib.data import map_and_batch\n\nchannel = 'my-pipe-channel-name'\n\nds = PipeModeDataset(channel)\nds = ds.repeat(EPOCHS)\nds = ds.prefetch(PREFETCH_SIZE)\nds = ds.apply(map_and_batch(parse, batch_size=BATCH_SIZE,\n                            num_parallel_batches=NUM_PARALLEL_BATCHES))\n<\/code><\/pre>\n\n<p>You can find the full example here: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_pipemode_example\/pipemode.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_pipemode_example\/pipemode.py<\/a><\/p>\n\n<p>You can find documentation about sagemaker_tensorflow here <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions#using-the-pipemodedataset\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions#using-the-pipemodedataset<\/a><\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":34.2,
        "Solution_reading_time":14.43,
        "Solution_score_count":4.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":53.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":14.603815,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I posted a similar question last week and didn't get a response to that yet so I'm posting another one now.  <\/p>\n<p>The code below is what I use to pull data into the compute instance from the Datastore. I transfer data from a Datastore to the compute instance and then save the data to my directory as a csv. The data originates from a SCOPE script and is transferred from Cosmos to the Datastore via Azure Data Factory.   <\/p>\n<p>Once the data is in the directory as a csv, I then utilize R to pull in the data into an RStudio session and then I run various tasks that create new data sets. I also save these new data sets to the compute instance directory as csv's. These new data sets are the ones I'd like to push back to the Datastore so they can be transferred elsewhere via Azure Data Factory and later consumed by a PowerBI app we're looking to create.  <\/p>\n<p>I tried using Designer and it ran for 4 days without completing before I cancelled the job and started looking for an alternative route. I don't know if it would have completed or if it ran into memory issues and simply didn't fail. When I pull data into the compute instance from the datastore it takes less than a few minutes to complete so I'm not sure why it would take Designer multiple days to attempt to do the reverse operation.  <\/p>\n<p>I've looked through a bunch of documentation and I am not able to find anything that tells us how we can transfer data from the compute instance back to the Datastore aside from Designer which is too slow or unable to handle.  <\/p>\n<p>This task seems like one that should be obvious for use and a major selling point of Azure Machine Learning so I'm a bit dumbfounded to see that this is a challenge figuring out how to do and that the documentation doesn't clearly show users how to achieve this task, assuming it's even possible. If it's not possible then I need to figure out a whole new system to use to get my work done. If it's not possible, the Azure Machine Learning team should enable this functionality as soon as possible.   <\/p>\n<pre><code># Azure management\nfrom azureml.core import Workspace, Dataset\n\n# MetaData\nsubscription_id = '09b5fdb3-165d-4e2b-8ca0-34f998d176d5'\nresource_group = 'xCloudData'\nworkspace_name = 'xCloudML'\n\n# Create workspace \nworkspace = Workspace(subscription_id, resource_group, workspace_name)\n\n# 1. Retention_Engagement_CombinedData\ndataset = Dataset.get_by_name(workspace, name='retention-engagement-combineddata')\n\n# Save data to file\ndf = dataset.to_pandas_dataframe()\ndf.to_csv('\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/v-aantico1\/code\/RetentionEngagement_CombinedData.csv')\n\n# 2. TitleNameJoin\ndataset = Dataset.get_by_name(workspace, name='TitleForJoiningInR')\n\n# Save data to file\ndf = dataset.to_pandas_dataframe()\ndf.to_csv('\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/v-aantico1\/code\/TitleNameJoin.csv')\n<\/code><\/pre>",
        "Challenge_closed_time":1632211214827,
        "Challenge_comment_count":0,
        "Challenge_created_time":1632158641093,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to transfer a CSV file from an Azure Machine Learning compute instance directory back to the Datastore. They have tried using Designer, but it took too long to complete. The user is looking for an alternative route to transfer the data and is unable to find any documentation on how to achieve this task. They have provided a code snippet that they use to pull data into the compute instance from the Datastore.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/559227\/how-can-i-transfer-a-csv-file-on-an-azure-machine",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":37.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":14.603815,
        "Challenge_title":"How can I transfer a csv file on an Azure Machine Learning compute instance directory back to the Datastore?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":447,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=1e7638a3-d560-4c0d-85b3-9061fd2bc218\">@Adrian Antico (TEKsystems, Inc.)  <\/a> Have you tried the following to upload data to your datastore?    <\/p>\n<pre><code>from azureml.core import Workspace  \nws = Workspace.from_config()  \ndatastore = ws.get_default_datastore()  \n  \ndatastore.upload(src_dir='.\/data',  \n                 target_path='datasets\/',  \n                 overwrite=True)  \n<\/code><\/pre>\n<p>I think <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_storage_datastore.azureblobdatastore?view=azure-ml-py#upload-src-dir--target-path-none--overwrite-false--show-progress-true-\">datastore.upload()<\/a> should work for you to upload the required datafiles from your compute instance to datastore.    <\/p>\n",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":19.0,
        "Solution_reading_time":9.87,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":50.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1345114008840,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Lyon, France",
        "Answerer_reputation_count":4233.0,
        "Answerer_view_count":151.0,
        "Challenge_adjusted_solved_time":58.2732816667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Lately i've been testing Azure Machine Learning, and i like it. However, when i try to transform my dataset, there's a step that i can't perform easily : replacing a specific value in a column by another one.<\/p>\n\n<p>The <code>Missing Values Scrubber<\/code> module allows me to deal with undefined values, but in my case i need to change a specific value, or remove rows where that value appears. I don't see which module meets my requirement.<\/p>\n\n<p>Do you have any suggestion about this issue ? <\/p>",
        "Challenge_closed_time":1412637208267,
        "Challenge_comment_count":0,
        "Challenge_created_time":1412427424453,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having difficulty replacing a specific value in a column of their dataset using Azure Machine Learning. They have tried using the \"Missing Values Scrubber\" module but it only deals with undefined values. They are seeking suggestions for a module that can help them replace or remove rows with a specific value.",
        "Challenge_last_edit_time":1446191259743,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/26193051",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":6.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":58.2732816667,
        "Challenge_title":"Replacing specific values in dataset with Azure ML",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":3772.0,
        "Challenge_word_count":91,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1345114008840,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lyon, France",
        "Poster_reputation_count":4233.0,
        "Poster_view_count":151.0,
        "Solution_body":"<p>I found a solution <a href=\"http:\/\/social.msdn.microsoft.com\/Forums\/en-US\/bf8f76c7-f976-4552-8553-8e54133ff2c6\/replacing-specific-values-in-dataset-with-azure-ml?forum=MachineLearning\" rel=\"nofollow\">there<\/a>, by using a <code>Convert to Dataset<\/code> module.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":29.6,
        "Solution_reading_time":3.75,
        "Solution_score_count":4.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":14.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":62.5413425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Azure machine learning designer :  <\/p>\n<p>I have a dataset on the designer connected to a Normalize data module but it keeps loading when i try to Edit columns on Normalize data module with no result or errors.  <br \/>\nThe same thing happens with Select columns in dataset module.  <\/p>\n<p>I have tried to recreate and restart and even deleted the whole resource group but no luck.  <br \/>\nI tried on both mac and windows with different browsers but still getting stuck on the same place.  <\/p>\n<p>any idea on how to solve this issue?   <\/p>\n<p>Thanks!  <\/p>\n<p>Screenshot:  <br \/>\n<a href=\"https:\/\/i.imgur.com\/P0oWrGR.png\">https:\/\/i.imgur.com\/P0oWrGR.png<\/a>  <\/p>",
        "Challenge_closed_time":1617711247023,
        "Challenge_comment_count":12,
        "Challenge_created_time":1617486098190,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with Azure machine learning designer where the Edit columns on Normalize data module and Select columns in dataset module are stuck on loading with no errors. The user has tried recreating, restarting, and deleting the resource group but the issue persists on both Mac and Windows with different browsers. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/343332\/azure-machine-learning-designer-edit-columns-stuck",
        "Challenge_link_count":1,
        "Challenge_participation_count":13,
        "Challenge_readability":8.2,
        "Challenge_reading_time":8.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":62.5413425,
        "Challenge_title":"Azure machine learning designer - edit columns stuck on loading",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":110,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=7246d026-6219-48cf-ab02-a9826f9174a5\">@Ayush Bhardwaj  <\/a> <a href=\"\/users\/na\/?userid=1f1ee936-54c7-4809-839f-2bd5ff84ec7b\">@yazeen jasim  <\/a> <a href=\"\/users\/na\/?userid=491da3e9-5349-4967-9851-b193f28abcac\">@Anshul Sharma  <\/a> This issue is now fixed in all regions and it does not require an additional parameter to be added to the URL. Please try and let us know if it works fine.<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":5.49,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":44.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1342628508448,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"United States",
        "Answerer_reputation_count":5147.0,
        "Answerer_view_count":1739.0,
        "Challenge_adjusted_solved_time":13.2526622222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to do some basic multi-label classification in Azure ML. I have some basic data in the following format:<\/p>\n\n<pre><code>value_x value_y label\nx1      y1      label1\nx2      y2      label1\nx3      y3      label2\n.....\n<\/code><\/pre>\n\n<p>My problem is that in my data certain labels (out of a total of five) are overrepresented, as about 40% of the data is label1, about 20% is label 2 and the rest around 10%. <\/p>\n\n<p>I would like to get a sampling out of these to train my model, so that each label is represented in equal amounts. <\/p>\n\n<p>Tried the stratification option in the Sampling module on the labels column, but that just gives me a sampling with the same distribution of labels as in the initial dataset.<\/p>\n\n<p>Any idea how I could do this with a module?<\/p>",
        "Challenge_closed_time":1456458689160,
        "Challenge_comment_count":2,
        "Challenge_created_time":1456404822827,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in performing multi-label classification in Azure ML due to over-representation of certain labels in the data. They are looking for a way to get a sampling of the data where each label is represented in equal amounts, but the stratification option in the Sampling module did not work. The user is seeking suggestions for a module that can help them achieve their goal.",
        "Challenge_last_edit_time":1456410979576,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35627916",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.8,
        "Challenge_reading_time":9.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":14.9628702778,
        "Challenge_title":"Azure machine learning even sampling",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":539.0,
        "Challenge_word_count":137,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1381405661540,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":185.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>I was able to do this using a combination of <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/70530644-c97a-4ab6-85f7-88bf30a8be5f\" rel=\"nofollow\">Split Data<\/a>, <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/a8726e34-1b3e-4515-b59a-3e4a475654b8\" rel=\"nofollow\">Partition and Sample<\/a>, and <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/b2ebdabd-217d-4915-86cc-5b05972f7270\" rel=\"nofollow\">Add Rows<\/a> modules.  There may be an easier way to do it, but I did confirm it works.  :)  I published my work at <a href=\"http:\/\/gallery.azureml.net\/Details\/1245147fd7004e91bc7a3683cda19cc7\" rel=\"nofollow\">http:\/\/gallery.azureml.net\/Details\/1245147fd7004e91bc7a3683cda19cc7<\/a> so you can grab it directly from there, and run to confirm it does what you expect.  <\/p>\n\n<p>Since you said you wanted a sampling of the data, I just reduced each of the labels to 10% to have all labels represented equally.  Since you have a good understanding of the distribution in your dataset, leave label 3, 4, and 5 all at about 10%, and reduce label 1 by 1\/4 and label 2 by 1\/2 to get about 10% of them as well.  <\/p>\n\n<p>To explain what I did in the workspace linked above:<\/p>\n\n<ul>\n<li>I used some \"Split Data\" modules to filter out the label1 and label2 data.  In the Split Data module, change the Splitting mode to \"Regular Expression\" and set the regular expression to <strong>\\\"Label\" ^label1<\/strong> (to get the label1 data, for example).  <\/li>\n<li>Then I used some \"Partition and Sample\" modules to reduce the size of the label1 and label2 data appropriately.  <\/li>\n<li>Finally, I used some \"Add Rows\" modules to join all of the data back together again.  <\/li>\n<\/ul>\n\n<p>Finally, I didn't include this in my work, but you can also look at the <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/9f3fe1c4-520e-49ac-a152-2e104169912a\" rel=\"nofollow\">SMOTE<\/a> module.  It will increase the number of low-occurring samples using synthetic minority oversampling.  <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":9.5,
        "Solution_reading_time":24.9,
        "Solution_score_count":3.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":256.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1428454496052,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":35.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":185.6452458333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to build a training set for Sagemaker using the Linear Learner algorithm. This algorithm supports recordIO wrapped protobuf and csv as format for the training data. As the training data is generated using spark I am having issues to generate a csv file from a dataframe (this seem broken for now), so I am trying to use protobuf. <\/p>\n\n<p>I managed to create a binary file for the training dataset using Protostuff which is a library that allows to generate protobuf messages from POJO objects. The problem is when triggering the training job I receive that message from SageMaker:\nClientError: No training data processed. Either the training channel is empty or the mini-batch size is too high. Verify that training data contains non-empty files and the mini-batch size is less than the number of records per training host.<\/p>\n\n<p>The training file is certainly not null. I suspect the way I generate the training data to be incorrect as I am able to train models using the libsvm format. Is there a way to generate IOrecord using the Sagemaker java client ?<\/p>",
        "Challenge_closed_time":1526653073368,
        "Challenge_comment_count":0,
        "Challenge_created_time":1525984750483,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in generating a training set for Sagemaker using the Linear Learner algorithm. They are having issues generating a CSV file from a dataframe and are trying to use protobuf instead. They have managed to create a binary file for the training dataset using Protostuff, but when triggering the training job, they receive an error message stating that no training data has been processed. The user suspects that the way they are generating the training data is incorrect and is seeking a way to generate IOrecord using the Sagemaker java client.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50281188",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.8,
        "Challenge_reading_time":13.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":185.6452458333,
        "Challenge_title":"Sagemaker Java client generate IOrecord",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":222.0,
        "Challenge_word_count":188,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1428454496052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Answering my own question. It was an issue in the algorithm configuration. I reduced mini batch size and it worked fine.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":1.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":21.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1259808393296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, Canada",
        "Answerer_reputation_count":44706.0,
        "Answerer_view_count":4356.0,
        "Challenge_adjusted_solved_time":4257.2200897222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create a Sklearn processing job in Amazon Sagemekar to perform some data transformation of my input data before I do model training.<\/p>\n<p>I wrote a custom python script <code>preprocessing.py<\/code> which does the needful. I use some python package in this script. <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.ipynb\" rel=\"nofollow noreferrer\">Here is the Sagemaker example I followed<\/a>.<\/p>\n<p>When I try to submit the Processing Job I get an error -<\/p>\n<pre><code>............................Traceback (most recent call last):\n  File &quot;\/opt\/ml\/processing\/input\/code\/preprocessing.py&quot;, line 6, in &lt;module&gt;\n    import snowflake.connector\nModuleNotFoundError: No module named 'snowflake.connector'\n<\/code><\/pre>\n<p>I understand that my processing job is unable to find this package and I need to install it. My question is how can I accomplish this using Sagemaker Processing Job API? Ideally there should be a way to define a <code>requirements.txt<\/code> in the API call, but I don't see such functionality in the docs.<\/p>\n<p>I know I can <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/processing-container-run-scripts.html\" rel=\"nofollow noreferrer\">create a custom Image with relevant packages<\/a> and later use this image in the Processing Job, but this seems too much work for something that should be built-in?<\/p>\n<p>Is there an easier\/elegant way to install packages needed in Sagemaker Processing Job ?<\/p>",
        "Challenge_closed_time":1638909512683,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638871133180,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to create a Sklearn processing job in Amazon Sagemaker to perform data transformation using a custom python script that requires a specific python package. However, when trying to submit the processing job, the user encounters an error indicating that the package is not found. The user is seeking an easier and more elegant way to install the required package within the Sagemaker Processing Job API.",
        "Challenge_last_edit_time":1638872002100,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70258080",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":21.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":10.6609730556,
        "Challenge_title":"How to install python packages within Amazon Sagemaker Processing Job?",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2087.0,
        "Challenge_word_count":199,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1410888611067,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":435.0,
        "Poster_view_count":32.0,
        "Solution_body":"<p>One way would be to <a href=\"https:\/\/stackoverflow.com\/questions\/12332975\/installing-python-module-within-code\">call pip from Python<\/a>:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>subprocess.check_call([sys.executable, &quot;-m&quot;, &quot;pip&quot;, &quot;install&quot;, package])\n<\/code><\/pre>\n<p>Another way would be to use an <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/sagemaker.sklearn.html\" rel=\"nofollow noreferrer\">SKLearn Estimator<\/a> (training job) instead, to do the same thing. You can provide the <code>source_dir<\/code>, which can include a <code>requirements.txt<\/code> file, and these requirements will be installed for you<\/p>\n<pre class=\"lang-python prettyprint-override\"><code>estimator = SKLearn(\n    entry_point=&quot;foo.py&quot;,\n    source_dir=&quot;.\/foo&quot;, # no trailing slash! put requirements.txt here\n    framework_version=&quot;0.23-1&quot;,\n    role = ...,\n    instance_count = 1,\n    instance_type = &quot;ml.m5.large&quot;\n)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1654197994423,
        "Solution_link_count":2.0,
        "Solution_readability":14.8,
        "Solution_reading_time":13.46,
        "Solution_score_count":4.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":76.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":8.8217069444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following this <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_mscoco_multi_label\/Image-classification-multilabel-lst.ipynb\" rel=\"nofollow noreferrer\">tutorial<\/a> with my custom data and my custom S3 buckets where train and validation data are. I am getting the following error:<\/p>\n<pre><code>Customer Error: imread read blank (None) image for file: \/opt\/ml\/input\/data\/train\/s3:\/\/image-classification\/image_classification_model_data\/train\/img-001.png\n<\/code><\/pre>\n<p>I have all my training data are in one folder named '<code>train<\/code>' I have set up my <code>lst<\/code> file like this suggested by <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">doc<\/a>,<\/p>\n<pre><code>22  1   s3:\/\/image-classification\/image_classification_model_data\/train\/img-001.png\n86  0   s3:\/\/image-classification\/image_classification_model_data\/train\/img-002.png\n...\n<\/code><\/pre>\n<p>My other configurations:<\/p>\n<pre><code>s3_bucket = 'image-classification'\nprefix =  'image_classification_model_data'\n\n\ns3train = 's3:\/\/{}\/{}\/train\/'.format(s3_bucket, prefix)\ns3validation = 's3:\/\/{}\/{}\/validation\/'.format(s3_bucket, prefix)\n\ns3train_lst = 's3:\/\/{}\/{}\/train_lst\/'.format(s3_bucket, prefix)\ns3validation_lst = 's3:\/\/{}\/{}\/validation_lst\/'.format(s3_bucket, prefix)\n\n\n\ntrain_data = sagemaker.inputs.TrainingInput(s3train, distribution='FullyReplicated', \n                        content_type='application\/x-image', s3_data_type='S3Prefix')\n\nvalidation_data = sagemaker.inputs.TrainingInput(s3validation, distribution='FullyReplicated', \n                             content_type='application\/x-image', s3_data_type='S3Prefix')\n\ntrain_data_lst = sagemaker.inputs.TrainingInput(s3train_lst, distribution='FullyReplicated', \n                        content_type='application\/x-image', s3_data_type='S3Prefix')\n\nvalidation_data_lst = sagemaker.inputs.TrainingInput(s3validation_lst, distribution='FullyReplicated', \n                             content_type='application\/x-image', s3_data_type='S3Prefix')\n\n\ndata_channels = {'train': train_data, 'validation': validation_data, 'train_lst': train_data_lst, \n                 'validation_lst': validation_data_lst}\n<\/code><\/pre>\n<p>I checked the images downloaded and checked physically, I see the image. Now sure what this error gets thrown out as <code>blank<\/code>. Any suggestion would be great.<\/p>",
        "Challenge_closed_time":1616684272012,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616651745117,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while following an image classification tutorial on Amazon Sagemaker with custom data and S3 buckets. The error message states that the image read is blank (None) for a specific file. The user has set up the lst file and other configurations as suggested by the documentation. The user has physically checked the downloaded images and is unsure why the error is being thrown.",
        "Challenge_last_edit_time":1616652513867,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66793845",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":30.4,
        "Challenge_reading_time":32.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":9.0352486111,
        "Challenge_title":"Customer Error: imread read blank (None) image for file- Sagemaker AWS",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":164.0,
        "Challenge_word_count":160,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1519936486960,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Minneapolis, MN, USA",
        "Poster_reputation_count":1113.0,
        "Poster_view_count":122.0,
        "Solution_body":"<p>Sagemaker copies the input data you specify in <code>s3train<\/code> into the instance in <code>\/opt\/ml\/input\/data\/train\/<\/code> and that's why you have an error, because as you can see from the error message is trying to concatenate the filename in the <code>lst<\/code> file with the path where it expect the image to be. So just put only the filenames in your <code>lst<\/code>and should be fine (remove the s3 path).<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.0,
        "Solution_reading_time":5.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":326.7363888889,
        "Challenge_answer_count":0,
        "Challenge_body":"When I tried to run benchmark on sagemaker with anubis, it showed processing benchmark submission request and then cannot execute the requested benchmark. \r\n<img width=\"1038\" alt=\"smmrcnn\" src=\"https:\/\/user-images.githubusercontent.com\/54413235\/66169329-e0ee3800-e5f4-11e9-887f-8e6fce87a917.png\">\r\n\r\nI also tried to run the sample for sagemaker https:\/\/github.com\/MXNetEdge\/benchmark-ai\/blob\/master\/sample-benchmarks\/sagemaker\/horovod.toml   and it showed with the same error\r\n<img width=\"1018\" alt=\"smsample\" src=\"https:\/\/user-images.githubusercontent.com\/54413235\/66169407-201c8900-e5f5-11e9-9de7-b46a7e9501a4.png\">\r\n\r\n\r\nBTW, when we wanna run with sagemaker, besides specify  execution_engine = \"aws.sagemaker\" and framework , is there anything else we need to specify or change?\r\n",
        "Challenge_closed_time":1571326528000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1570150277000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue with the data path inside a Sagemaker notebook as the bucket of processed data does not exist, resulting in a NoSuchBucket error when calling the ListObjectsV2 operation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/benchmark-ai\/issues\/907",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":10.68,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":6.0,
        "Challenge_repo_issue_count":1054.0,
        "Challenge_repo_star_count":14.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":326.7363888889,
        "Challenge_title":"Cannot run benchmark for sagemaker",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":75,
        "Discussion_body":"3 tactics were used to address this issue (by @perdasilva): \r\nStop gap, watcher, error reporting in the status.\r\n@haohanchen-yagao - Please confirm and the close this issue.",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1498598028756,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":2103.0,
        "Answerer_view_count":129.0,
        "Challenge_adjusted_solved_time":0.4859305556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm getting this error while calling drop.duplicate function:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;train.py&quot;, line 159, in &lt;module&gt;\n    orders_dfx = preprocess_orders(orders_df)\n  File &quot;train.py&quot;, line 20, in preprocess_orders\n    ao = ao.drop_duplicates(subset=['order_id'], keep='last')\nAttributeError: 'TabularDataset' object has no attribute 'drop_duplicates'\n<\/code><\/pre>\n<p>Here is a part of <code>train.py<\/code> code<\/p>\n<pre><code>def preprocess_orders(ao):\n  ao = ao.drop_duplicates(subset=['order_id'], keep='last')\n  ao['order_id'] = ao['order_id'].astype('str')\n  ao['class'] = ao['class'].astype('int')\n  ao['age'] = ao['age'].astype('float').fillna(ao['age'].mean()).round(2)\n  return ao\n\norders_df = Dataset.get_by_name(ws, name='class_cancelled_orders')\norders_df.to_pandas_dataframe()\n# Doing processing\norders_dfx = preprocess_orders(orders_df)\n<\/code><\/pre>\n<p>I'm getting the data from the datasets in azureml studio. The job.py file is used for running experiment as:<\/p>\n<pre><code># submit job\nrun = Experiment(ws, experiment_name).submit(src)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>",
        "Challenge_closed_time":1615031651630,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615028731157,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to remove duplicates in a Python AzureML classification problem. The error message indicates that the 'TabularDataset' object has no attribute 'drop_duplicates'. The user has provided a part of the 'train.py' code where the error is occurring. The user is getting the data from the datasets in AzureML studio and is using the 'job.py' file for running the experiment.",
        "Challenge_last_edit_time":1615029902280,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66504979",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.5,
        "Challenge_reading_time":16.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":0.8112425,
        "Challenge_title":"Getting the error while removing the duplicates in python AzureML classification problem",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":37.0,
        "Challenge_word_count":105,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1598018883048,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":77.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>The <code>to_pandas_dataframe()<\/code>method <em>returns<\/em> a pandas DataFrame, so you need to assign it back your variable:<\/p>\n<pre><code>orders_df = orders_df.to_pandas_dataframe()\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.0,
        "Solution_reading_time":2.7,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":18.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":43.4361111111,
        "Challenge_answer_count":0,
        "Challenge_body":"### System Info\r\n\r\n```shell\r\n- mlflow==1.25.1\r\n- `transformers` version: 4.19.0.dev0\r\n- Platform: Linux-5.10.76-linuxkit-aarch64-with-glibc2.31\r\n- Python version: 3.9.7\r\n- Huggingface_hub version: 0.2.1\r\n- PyTorch version (GPU?): 1.10.2 (False)\r\n```\r\n\r\n\r\n### Who can help?\r\n\r\nShould be fixed by #17067\r\n\r\n### Information\r\n\r\n- [X] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [X] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nSteps to reproduce:\r\n1. Follow Training tutorial as per https:\/\/huggingface.co\/docs\/transformers\/training\r\n2. Change the training arguments to use `TrainingArguments(output_dir=\"test_trainer\", report_to=['mlflow'], run_name=\"run0\")`\r\n3. On `trainer.train()` the MLFlow UI should report a run with a Run Name of `run0` which is not currently the case.\r\n\r\nCause of the Issue:\r\n```\r\n>> import mlflow\r\n>> print(mlflow.active_run is None, mlflow.active_run() is None)\r\nFalse True\r\n```\r\n\r\nIn `src\/transformers\/integrations.py` the line `if self._ml_flow.active_run is None:` need to be replaced by `if self._ml_flow.active_run() is None:`\r\n\r\n### Expected behavior\r\n\r\nPR #14894 introduce support for run_name in the MLflowCallback. Though, this does not work as expected since the active run is checked using a method reference that always returns true. Bug introduced by #16131.\r\n",
        "Challenge_closed_time":1651758596000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651602226000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where the supplementary information about the run, such as git commit sha, user, and filename, is not logged when using `MLFlowLogger` as a logger in pytorch_lightning. The user suggests adding tags internally to resolve the issue seamlessly and expects the pytorch_lightning to manage the mlflow's run.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17066",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":6.6,
        "Challenge_reading_time":18.14,
        "Challenge_repo_contributor_count":437.0,
        "Challenge_repo_fork_count":20818.0,
        "Challenge_repo_issue_count":23746.0,
        "Challenge_repo_star_count":102902.0,
        "Challenge_repo_watch_count":1019.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":43.4361111111,
        "Challenge_title":"Incorrect check for MLFlow active run in MLflowCallback",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":177,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":0.4832136111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to classify ~1m+ documents and have a Version Control System for in- and Output of the corresponding model. <\/p>\n\n<p>The data changes over time:<\/p>\n\n<ul>\n<li>sample size increases over time<\/li>\n<li>new Features might appear<\/li>\n<li>anonymization procedure might Change over time<\/li>\n<\/ul>\n\n<p>So basically \"everything\" might change: amount of observations, Features and the values.\nWe are interested in making the ml model Building reproducible without using 10\/100+ GB \nof disk volume, because we save all updated versions of Input data. Currently the volume size of the data is ~700mb.<\/p>\n\n<p>The most promising tool i found is: <a href=\"https:\/\/github.com\/iterative\/dvc\" rel=\"noreferrer\">https:\/\/github.com\/iterative\/dvc<\/a>. Currently the data\nis stored in a database in loaded in R\/Python from there.<\/p>\n\n<p><strong>Question:<\/strong><\/p>\n\n<p>How much disk volume can be (very approx.) saved by using dvc? <\/p>\n\n<p>If one can roughly estimate that. I tried to find out if only the \"diffs\" of the data are saved. I didnt find much info by reading through: <a href=\"https:\/\/github.com\/iterative\/dvc#how-dvc-works\" rel=\"noreferrer\">https:\/\/github.com\/iterative\/dvc#how-dvc-works<\/a> or other documentation. <\/p>\n\n<p><strong>I am aware that this is a very vague question. And it will highly depend on the dataset. However, i would still be interested in getting a very approximate idea.<\/strong><\/p>",
        "Challenge_closed_time":1582487867856,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582482701247,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary_original":"The user wants to classify over 1 million documents and have a version control system for input and output of the corresponding model. They are interested in making the ML model building reproducible without using a large amount of disk volume. The user is seeking an estimate of how much disk volume can be saved by using dvc, a tool they found promising, but they are aware that the answer will depend on the dataset.",
        "Challenge_last_edit_time":1582486128287,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60365473",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":18.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":1.4351691667,
        "Challenge_title":"By how much can i approx. reduce disk volume by using dvc?",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":689.0,
        "Challenge_word_count":204,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1504097190907,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1365.0,
        "Poster_view_count":193.0,
        "Solution_body":"<p>Let me try to summarize how does DVC store data and I hope you'll be able to figure our from this how much space will be saved\/consumed in your specific scenario.<\/p>\n\n<p><strong>DVC is storing and deduplicating data on the individual <em>file level<\/em>.<\/strong> So, what does it usually mean from a practical perspective.<\/p>\n\n<p>I will use <code>dvc add<\/code> as an example, but the same logic applies to all commands that save data files or directories into DVC cache - <code>dvc add<\/code>, <code>dvc run<\/code>, etc.<\/p>\n\n<h2>Scenario 1: Modifying file<\/h2>\n\n<p>Let's imagine I have a single 1GB XML file. I start tracking it with DVC:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ dvc add data.xml\n<\/code><\/pre>\n\n<p>On the modern file system (or if <code>hardlinks<\/code>, <code>symlinks<\/code> are enabled, see <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\" rel=\"noreferrer\">this<\/a> for more details) after this command we still consume 1GB (even though file is moved into DVC cache and is still present in the workspace).<\/p>\n\n<p>Now, let's change it a bit and save it again:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ echo \"&lt;test\/&gt;\" &gt;&gt; data.xml\n$ dvc add data.xml\n<\/code><\/pre>\n\n<p>In this case we will have 2GB consumed. <strong>DVC does not do diff between two versions of the same file<\/strong>, neither it splits files into chunks or blocks to understand that only small portion of data has changed.<\/p>\n\n<blockquote>\n  <p>To be precise, it calculates <code>md5<\/code> of each file and save it in the content addressable key-value storage. <code>md5<\/code> of the files serves as a key (path of the file in cache) and value is the file itself:<\/p>\n  \n  <pre class=\"lang-sh prettyprint-override\"><code>(.env) [ivan@ivan ~\/Projects\/test]$ md5 data.xml\n0c12dce03223117e423606e92650192c\n\n(.env) [ivan@ivan ~\/Projects\/test]$ tree .dvc\/cache\n.dvc\/cache\n\u2514\u2500\u2500 0c\n   \u2514\u2500\u2500 12dce03223117e423606e92650192c\n\n1 directory, 1 file\n\n(.env) [ivan@ivan ~\/Projects\/test]$ ls -lh data.xml\ndata.xml ----&gt; .dvc\/cache\/0c\/12dce03223117e423606e92650192c (some type of link)\n<\/code><\/pre>\n<\/blockquote>\n\n<h2>Scenario 2: Modifying directory<\/h2>\n\n<p>Let's now imagine we have a single large 1GB directory <code>images<\/code> with a lot of files:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ du -hs images\n1GB\n\n$ ls -l images | wc -l\n1001\n\n$ dvc add images\n<\/code><\/pre>\n\n<p>At this point we still consume 1GB. Nothing has changed. But if we modify the directory by adding more files (or removing some of them):<\/p>\n\n<pre><code>$ cp \/tmp\/new-image.png images\n\n$ ls -l images | wc -l\n1002\n\n$ dvc add images\n<\/code><\/pre>\n\n<p>In this case, after saving the new version we <strong>still close to 1GB<\/strong> consumption. <strong>DVC calculates diff on the directory level.<\/strong> It won't be saving all the files that were existing before in the directory.<\/p>\n\n<p>The same logic applies to all commands that save data files or directories into DVC cache - <code>dvc add<\/code>, <code>dvc run<\/code>, etc.<\/p>\n\n<p>Please, let me know if it's clear or we need to add more details, clarifications.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.5,
        "Solution_reading_time":39.4,
        "Solution_score_count":12.0,
        "Solution_sentence_count":26.0,
        "Solution_word_count":428.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1431575832387,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":550.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":71.7747852778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Background: There seems to be a way to parameterize <code>DataPath<\/code> with <code>PipelineParameter<\/code>\n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb<\/a><\/p>\n<p>But I'd like to parameterize my SQL query with PipelineParameter, for example, with this query<\/p>\n<pre><code>sql_query = &quot;&quot;&quot;\nSELECT id, foo, bar FROM baz\nWHERE baz.id BETWEEN 10 AND 20\n&quot;&quot;&quot;\ndataset = Dataset.Tabular.from_sql_query((sql_datastore, sql_query))\n<\/code><\/pre>\n<p>I'd like to use PipelineParameter to parameterize <code>10<\/code> and <code>20<\/code> as <code>param_1<\/code> and <code>param_2<\/code>. Is this possible?<\/p>",
        "Challenge_closed_time":1603727871847,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603497171040,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to parameterize a SQL query in Azure ML using PipelineParameter to replace the values of two parameters in the query. They have successfully parameterized DataPath but are unsure if it is possible to do the same with a SQL query.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64508625",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":22.6,
        "Challenge_reading_time":14.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":64.0835575,
        "Challenge_title":"Parameterized SQL query in Azure ML",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":188.0,
        "Challenge_word_count":72,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1431575832387,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":550.0,
        "Poster_view_count":40.0,
        "Solution_body":"<p>Found a way to do this:<\/p>\n<p>Pass your params to PythonScriptStep<\/p>\n<pre><code>param_1 = PipelineParameter(name='min_id', default_value=5)\nparam_2 = PipelineParameter(name='max_id', default_value=10)\nsql_datastore = &quot;sql_datastore&quot;\nstep = PythonScriptStep(script_name='script.py', arguments=[param_1, param_2, \nsql_datastore])\n<\/code><\/pre>\n<p>In script.py<\/p>\n<pre><code>min_id_param = sys.argv[1]\nmax_id_param = sys.argv[2]\nsql_datastore_name = sys.argv[3]\nquery = &quot;&quot;&quot;\nSELECT id, foo, bar FROM baz\nWHERE baz.id BETWEEN {} AND {}\n&quot;&quot;&quot;.format(min_id_param, max_id_param)\nrun = Run.get_context()\nsql_datastore = Datastore.get(run.experiment.workspace, sql_datastore_name)\ndataset = Dataset.Tabular.from_sql_query((sql_datastore, query))\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1603755560267,
        "Solution_link_count":0.0,
        "Solution_readability":15.9,
        "Solution_reading_time":10.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":56.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2627777778,
        "Challenge_answer_count":1,
        "Challenge_body":"Is it possible to deploy an existing model artifact from SageMaker to Redshift ML? \n\nFor example, with an **Aurora ML** you can reference a SageMaker endpoint and then use it as a UDF in a `SELECT` statement. \n**Redshift ML** works a bit differently - when you call `CREATE MODEL` - the model is trained with **SageMaker Autopilot** and then deployed to the **Redshift Cluster**. \n\nWhat if I already have a trained model, can i deploy it to a Redshift Cluster and then use a UDF for Inference?",
        "Challenge_closed_time":1609955532000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609954586000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is asking if it is possible to deploy an existing model artifact from SageMaker to Redshift ML and use it as a UDF for inference. Redshift ML works differently from Aurora ML, as the model is trained with SageMaker Autopilot and then deployed to the Redshift Cluster when `CREATE MODEL` is called. The user wants to know if they can deploy a pre-trained model to Redshift Cluster and use it as a UDF for inference.",
        "Challenge_last_edit_time":1668536452452,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUCMYCx28qRe-MOCIfj91Y2g\/redshift-ml-sagemaker-deploy-an-existing-model-artifact-to-a-redshift-cluster",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":6.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.2627777778,
        "Challenge_title":"Redshift ML \/ SageMaker - Deploy an existing model artifact to a Redshift Cluster",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":148.0,
        "Challenge_word_count":96,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"As of January 30 2021, you can't deploy an existing model artifact from SageMaker to Redshift ML directly with currently announced Redshift ML preview features. But you can reference  sagemaker endpoint through a lambda function and use that lambda function as an user defined function in Redshift.\n\nBelow would be the steps:\n\n1. Train and deploy your SageMaker model in a SageMaker Endpoint. \n2. Use Lambda function to [reference sagemaker endpoint](https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/). \n3. Create a [Redshift Lambda UDF](https:\/\/aws.amazon.com\/blogs\/big-data\/accessing-external-components-using-amazon-redshift-lambda-udfs\/) referring above lambda function to run predictions.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1612026491936,
        "Solution_link_count":2.0,
        "Solution_readability":15.1,
        "Solution_reading_time":10.02,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":84.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":28.7265325,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to get instance used time by instance type and day, in EC2 and Sagemaker service,<\/p>\n<p>But in AWSCUR it seems no value of instance running time(hour\/minute\/second),<\/p>\n<p>How can I get the instance actual used time?<\/p>",
        "Challenge_closed_time":1635436765040,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635333349523,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to obtain the actual used time of AWS EC2\/Sagemaker instances by instance type and day, but is unable to find this information in AWSCUR. They are seeking guidance on how to obtain this data.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69737649",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.5,
        "Challenge_reading_time":3.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":28.7265325,
        "Challenge_title":"How to get AWS EC2\/Sagemaker instance used time by instance type and day?",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":61.0,
        "Challenge_word_count":49,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458116093247,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1803.0,
        "Poster_view_count":75.0,
        "Solution_body":"<p>you can see daily (or even hourly if you opt-in) cost and usage by instance type with:<br \/>\n<code>aws ce get-cost-and-usage --time-period Start=2021-10-26,End=2021-10-27 --granularity DAILY --metrics &quot;UsageQuantity&quot; &quot;BlendedCost&quot; --group-by Type=DIMENSION,Key=INSTANCE_TYPE<\/code><br \/>\nNote that SageMaker instance types names starts with: <code>ml.*<\/code><\/p>\n<p>To view things in the finest resolution you'll need to produce detailed billing reports (DBR): <a href=\"https:\/\/docs.aws.amazon.com\/cur\/latest\/userguide\/detailed-billing.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cur\/latest\/userguide\/detailed-billing.html<\/a><br \/>\nIt will generates CSV reports in S3, which you could query with Athena using SQL: <a href=\"https:\/\/docs.aws.amazon.com\/cur\/latest\/userguide\/cur-query-athena.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cur\/latest\/userguide\/cur-query-athena.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":18.7,
        "Solution_reading_time":12.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":76.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":755.505,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug \r\n\r\nThe Comet logger cannot be pickled after an experiment (at least an OfflineExperiment) has been created.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\ninitialize the logger object (works fine)\r\n```\r\nfrom pytorch_lightning.loggers import CometLogger\r\nimport tests.base.utils as tutils\r\nfrom pytorch_lightning import Trainer\r\nimport pickle\r\n\r\nmodel, _ = tutils.get_default_model()\r\nlogger = CometLogger(save_dir='test')\r\npickle.dumps(logger)\r\n```\r\n\r\ninitialize a Trainer object with the logger (works fine)\r\n```\r\ntrainer = Trainer(\r\n    max_epochs=1,\r\n    logger=logger\r\n)\r\npickle.dumps(logger)\r\npickle.dumps(trainer)\r\n```\r\n\r\naccess the `experiment` attribute which creates the OfflineExperiment object (fails)\r\n```\r\nlogger.experiment\r\npickle.dumps(logger)\r\n>> TypeError: can't pickle _thread.lock objects\r\n```\r\n\r\n### Expected behavior\r\n\r\nWe should be able to pickle loggers for distributed training.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n        - available:         False\r\n        - version:           None\r\n* Packages:\r\n        - numpy:             1.18.1\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.4.0\r\n        - pytorch-lightning: 0.7.5\r\n        - tensorboard:       2.1.0\r\n        - tqdm:              4.42.0\r\n* System:\r\n        - OS:                Darwin\r\n        - architecture:\r\n                - 64bit\r\n                - \r\n        - processor:         i386\r\n        - python:            3.7.6\r\n        - version:           Darwin Kernel Version 19.3.0: Thu Jan  9 20:58:23 PST 2020; root:xnu-6153.81.5~1\/RELEASE_X86_64\r\n\r\n",
        "Challenge_closed_time":1591023635000,
        "Challenge_comment_count":15,
        "Challenge_created_time":1588303817000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where test metrics are not being logged to Comet after training a model using `Trainer.fit` and then testing it using `Trainer.test`. The metrics are logged correctly during training but not during testing. The user suspects that the issue is caused by `logger.finalize(\"success\")` being called at the end of the training routine, which in turn calls `experiment.end()` inside the logger, causing the `Experiment` object to not expect any more information. The user suggests creating another `Trainer` object with another logger as a workaround, but this would log the metrics into a different Comet experiment from the original.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/1682",
        "Challenge_link_count":0,
        "Challenge_participation_count":15,
        "Challenge_readability":11.1,
        "Challenge_reading_time":16.81,
        "Challenge_repo_contributor_count":444.0,
        "Challenge_repo_fork_count":2922.0,
        "Challenge_repo_issue_count":15116.0,
        "Challenge_repo_star_count":23576.0,
        "Challenge_repo_watch_count":234.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":755.505,
        "Challenge_title":"Comet logger cannot be pickled after creating an experiment",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":144,
        "Discussion_body":"@ceyzaguirre4 pls ^^ I don't know if it can help or if it is the right place, but a similar error occurswhen running in ddp mode with the WandB logger.\r\n\r\nWandB uses a lambda function at some point.\r\n\r\nDoes the logger have to pickled ? Couldn't it log only on rank 0 at epoch_end ?\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"..\/train.py\", line 140, in <module>\r\n    main(args.gpus, args.nodes, args.fast_dev_run, args.mixed_precision, project_config, hparams)\r\n  File \"..\/train.py\", line 117, in main\r\n    trainer.fit(model)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 751, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/site-packages\/torch\/multiprocessing\/spawn.py\", line 200, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/site-packages\/torch\/multiprocessing\/spawn.py\", line 149, in start_processes\r\n    process.start()\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/process.py\", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/context.py\", line 283, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object 'TorchHistory.add_log_hooks_to_pytorch_module.<locals>.<lambda>'\r\n```\r\n\r\nalso related: \r\n#1704 I had the same error as @jeremyjordan  `can't pickle _thread.lock objects`. This happened when I added the  `logger` and additional `callbacks` in `from_argparse_args`, as explained here https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/hyperparameters.html\r\n\r\n```\r\ntrainer = pl.Trainer.from_argparse_args(hparams, logger=logger, callbacks=[PrinterCallback(), ])\r\n```\r\nI could make the problem go away by directly overwriting the members of `Trainer`\r\n\r\n```\r\ntrainer = pl.Trainer.from_argparse_args(hparams)\r\ntrainer.logger = logger\r\ntrainer.callbacks.append(PrinterCallback())\r\n``` Same issue as @F-Barto using a wandb logger across 2 nodes with `ddp`. same issue when using wandb logger with ddp same here.. @joseluisvaz your workaround doesn't solve the callback issue.. when I try to add a callback like this it is simply being ignored :\/ but adding it the Trainer init call normally works.. so I'm pretty sure the error is thrown by the logger (I'm using TB) not the callbacks. Same issue, using wandb logger with 8 gpus in an AWS p2.8xlarge machine  With CometLogger, I get this error only when the experiment name is declared. If it is not declared, I get no issue. I still have this error with 1.5.10 on macOS\r\n\r\n```\r\nError executing job with overrides: ['train.pl_trainer.fast_dev_run=False', 'train.pl_trainer.gpus=0', 'train.pl_trainer.precision=32', 'logging.wandb_arg.mode=offline']\r\nTraceback (most recent call last):\r\n  File \"\/Users\/ric\/Documents\/PhD\/Projects\/ed-experiments\/src\/train.py\", line 78, in main\r\n    train(conf)\r\n  File \"\/Users\/ric\/Documents\/PhD\/Projects\/ed-experiments\/src\/train.py\", line 70, in train\r\n    trainer.fit(pl_module, datamodule=pl_data_module)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 740, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 685, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 777, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1199, in _run\r\n    self._dispatch()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1279, in _dispatch\r\n    self.training_type_plugin.start_training(self)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/plugins\/training_type\/training_type_plugin.py\", line 202, in start_training\r\n    self._results = trainer.run_stage()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1289, in run_stage\r\n    return self._run_train()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1311, in _run_train\r\n    self._run_sanity_check(self.lightning_module)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1375, in _run_sanity_check\r\n    self._evaluation_loop.run()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/loops\/base.py\", line 145, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/loops\/dataloader\/evaluation_loop.py\", line 110, in advance\r\n    dl_outputs = self.epoch_loop.run(dataloader, dataloader_idx, dl_max_batches, self.num_dataloaders)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/loops\/base.py\", line 140, in run\r\n    self.on_run_start(*args, **kwargs)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/loops\/epoch\/evaluation_epoch_loop.py\", line 86, in on_run_start\r\n    self._dataloader_iter = _update_dataloader_iter(data_fetcher, self.batch_progress.current.ready)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/loops\/utilities.py\", line 121, in _update_dataloader_iter\r\n    dataloader_iter = enumerate(data_fetcher, batch_idx)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/fetching.py\", line 198, in __iter__\r\n    self._apply_patch()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/fetching.py\", line 133, in _apply_patch\r\n    apply_to_collections(self.loaders, self.loader_iters, (Iterator, DataLoader), _apply_patch_fn)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/fetching.py\", line 181, in loader_iters\r\n    loader_iters = self.dataloader_iter.loader_iters\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/supporters.py\", line 537, in loader_iters\r\n    self._loader_iters = self.create_loader_iters(self.loaders)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/supporters.py\", line 577, in create_loader_iters\r\n    return apply_to_collection(loaders, Iterable, iter, wrong_dtype=(Sequence, Mapping))\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/apply_func.py\", line 104, in apply_to_collection\r\n    v = apply_to_collection(\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/apply_func.py\", line 96, in apply_to_collection\r\n    return function(data, *args, **kwargs)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/supporters.py\", line 177, in __iter__\r\n    self._loader_iter = iter(self.loader)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/torch\/utils\/data\/dataloader.py\", line 359, in __iter__\r\n    return self._get_iterator()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/torch\/utils\/data\/dataloader.py\", line 305, in _get_iterator\r\n    return _MultiProcessingDataLoaderIter(self)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/torch\/utils\/data\/dataloader.py\", line 918, in __init__\r\n    w.start()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/process.py\", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/context.py\", line 224, in _Popen\r\n    return _default_context.get_context().Process._Popen(process_obj)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object 'TorchHistory.add_log_hooks_to_pytorch_module.<locals>.<lambda>'\r\n``` I still see this bug as well with WandB logger. Currently having this issue with wandbLogger. Having same issue with wandb @ebalogun01 Were you able to solve this issue? I'm also seeing the same issue with WandbLogger > @ebalogun01 Were you able to solve this issue? I'm also seeing the same issue with WandbLogger\r\n\r\nWhat version are you using?  @Borda I'm using Lightning 2.1.0.post0 version. Another detail I'd like to add is that, I find that `wandblogger` is \"unpickable\" only when wandb is disabled from the terminal using: `wandb disabled`",
        "Discussion_score_count":26.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":18.8050408334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,  <\/p>\n<p>We are currently annotating images in a data labeling instance segmentation (polygon) project. Our images are rather blueish, which makes it difficult to use the polygon &quot;draw polygon region&quot; tool, which draws the polygon in blue.  <\/p>\n<p>Is it possible to change the color to, for example, black?  <\/p>\n<p>Thanks and BR,  <br \/>\nMaite<\/p>",
        "Challenge_closed_time":1647596267230,
        "Challenge_comment_count":1,
        "Challenge_created_time":1647528569083,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing difficulty in annotating images in a data labeling instance segmentation project on Azure ML due to the blue color of the polygon tool. They are seeking a solution to change the color of the tool to black.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/776555\/azure-ml-data-labeling-change-polygon-color",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":5.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":18.8050408334,
        "Challenge_title":"Azure ML data labeling change polygon color",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":62,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=54dc3768-e4dc-41b2-8290-e72ad5c207f3\">@Maite  <\/a>     <\/p>\n<p>Thanks for reaching out to us, I am sorry we are using only blue for the polygon color. I will forward your feedback to product team for future release.     <\/p>\n<p>One workaround may help with your scenario is, you can change the brightness to &quot;-100&quot; when you draw and revert the brightness back when you done as below screenshot. This will help to make things clear.     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/184541-image.png?platform=QnA\" alt=\"184541-image.png\" \/>    <\/p>\n<p>Hope this helps and thanks for the feedback again.     <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p><em>-Please kindly accept the answer if you feel helpful, thanks.<\/em>    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.7,
        "Solution_reading_time":9.77,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":100.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1094444444,
        "Challenge_answer_count":0,
        "Challenge_body":"Example job: job-7acb5d09-e580-46a2-aa11-03ce72ddc0f0\r\n\r\nAt the end of the job run, we upload the artifact, where `set-output` happens, and terminate the job.\r\nHowever, we have:\r\n```\r\n...\r\nINFO:wabucketref.api:Uploading artifact from '\/tmp\/tmpqkuqrluh' to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1 ...\r\nINFO:wabucketref.api:Artifact uploaded to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\r\nINFO:botocore.credentials:Found credentials in shared credentials file: \/var\/secrets\/aws\/credentials-pca-pipeline\r\nwandb: Generating checksum for up to 100000 objects with prefix \"dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\"... Done. 0.0s\r\n::set-output name=artifact_name::texture-maps\r\n::set-output name=artifact_type::dataset\r\nwandb: Waiting for W&B process to finish, PID 75\r\n...\r\n```\r\n\r\nWhile it should be:\r\n```\r\nINFO:wabucketref.api:Uploading artifact from '\/tmp\/tmpqkuqrluh' to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1 ...\r\nINFO:wabucketref.api:Artifact uploaded to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\r\nINFO:botocore.credentials:Found credentials in shared credentials file: \/var\/secrets\/aws\/credentials-pca-pipeline\r\nwandb: Generating checksum for up to 100000 objects with prefix \"dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\"... Done. 0.0s\r\n::set-output name=artifact_name::texture-maps\r\n::set-output name=artifact_type::dataset\r\n::set-output name=artifact_alias::8154311a-ab2e-45cd-adeb-f7e5270122c1\r\nwandb: Waiting for W&B process to finish, PID 75\r\nwandb: Program ended successfully.\r\nwandb:                                                                                \r\n```\r\n\r\nOne line was overwritten by the `wandb: Waiting for W&B process to finish, PID 75`, which, apparently is running in a separate process (`wandb.Settings(start_method=\"fork\")`). ",
        "Challenge_closed_time":1625736868000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625736474000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The issue is that the output of WandB overwrites the output of wabucketref during artifact upload, causing one line to be missing in the output.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/neuro-inc\/mlops-wandb-bucket-ref\/issues\/16",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":15.0,
        "Challenge_reading_time":25.31,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":1.0,
        "Challenge_repo_issue_count":139.0,
        "Challenge_repo_star_count":0.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":0.1094444444,
        "Challenge_title":"WandB output overwrites wabucketref's output in case of artifact upload",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":154,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.1954663889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to write the output of batch scoring into datalake:<\/p>\n<pre><code>    parallel_step_name = &quot;batchscoring-&quot; + datetime.now().strftime(&quot;%Y%m%d%H%M&quot;)\n    \n    output_dir = PipelineData(name=&quot;scores&quot;, \n                              datastore=def_ADL_store,\n                              output_mode=&quot;upload&quot;,\n                              output_path_on_compute=&quot;path in data lake&quot;)\n\nparallel_run_config = ParallelRunConfig(\n    environment=curated_environment,\n    entry_script=&quot;use_model.py&quot;,\n    source_directory=&quot;.\/&quot;,\n    output_action=&quot;append_row&quot;,\n    mini_batch_size=&quot;20&quot;,\n    error_threshold=1,\n    compute_target=compute_target,\n    process_count_per_node=2,\n    node_count=2\n)\n    \n    batch_score_step = ParallelRunStep(\n        name=parallel_step_name,\n        inputs=[test_data.as_named_input(&quot;test_data&quot;)],\n        output=output_dir,\n        parallel_run_config=parallel_run_config,\n        allow_reuse=False\n    )\n<\/code><\/pre>\n<p>However I meet the error: &quot;code&quot;: &quot;UserError&quot;,\n&quot;message&quot;: &quot;User program failed with Exception: Missing argument --output or its value is empty.&quot;<\/p>\n<p>How can I write results of batch score to data lake?<\/p>",
        "Challenge_closed_time":1596781453976,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596780750297,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to write the output of batch scoring into a data lake using Azure machine learning, but encounters an error message stating that the argument for output is missing or empty. The user is seeking guidance on how to write the results of batch scoring to the data lake.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63296185",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":21.8,
        "Challenge_reading_time":16.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.1954663889,
        "Challenge_title":"How to write Azure machine learning batch scoring results to data lake?",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":277.0,
        "Challenge_word_count":85,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1513841518107,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"China",
        "Poster_reputation_count":71.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>I don\u2019t think ADLS is supported for <code>PipelineData<\/code>. My suggestion is to use the workspace\u2019s default blob store for the <code>PipelineData<\/code>, then use a <code>DataTransferStep<\/code> for after the <code>ParallelRunStep<\/code> is completed.<\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.9,
        "Solution_reading_time":3.39,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":31.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7267.8997222222,
        "Challenge_answer_count":0,
        "Challenge_body":"Checklist\r\n- [x] I've prepended issue tag with type of change: [bug]\r\n- [ ] (If applicable) I've attached the script to reproduce the bug\r\n- [ ] (If applicable) I've documented below the DLC image\/dockerfile this relates to\r\n- [ ] (If applicable) I've documented below the tests I've run on the DLC image\r\n- [ ] I'm using an existing DLC image listed here: https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/deep-learning-containers-images.html\r\n- [ ] I've built my own container based off DLC (and I've attached the code used to build my own image)\r\n\r\n*Concise Description:*\r\nSM Remote Test log doesn't get reported correctly.\r\n\r\nObserved in 2 commits of the PR: https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\r\n\r\n- https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\/commits\/5dd2de96fb6f88707a030fca111ca6585534dbb8\r\n- https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\/commits\/867d3946aabd6e30accde84337e1f76c40211730\r\n\r\n*DLC image\/dockerfile:*\r\nMX 1.6 DLC\r\n\r\n*Current behavior:*\r\nGithub shows \"pending\" status.\r\nCodeBuild logs show \"Failed\" status.\r\nHowever, actual codebuild logs doesn't bear Failure log. It terminates abruptly.\r\n\r\n```\r\n\r\n============================= test session starts ==============================\r\nplatform linux -- Python 3.8.0, pytest-5.3.5, py-1.9.0, pluggy-0.13.1\r\nrootdir: \/codebuild\/output\/src687836801\/src\/github.com\/aws\/deep-learning-containers\/test\/dlc_tests\r\nplugins: rerunfailures-9.0, forked-1.3.0, xdist-1.31.0, timeout-1.4.2\r\ngw0 I \/ gw1 I \/ gw2 I \/ gw3 I \/ gw4 I \/ gw5 I \/ gw6 I \/ gw7 I\r\ngw0 [3] \/ gw1 [3] \/ gw2 [3] \/ gw3 [3] \/ gw4 [3] \/ gw5 [3] \/ gw6 [3] \/ gw7 [3]\r\n```\r\n\r\nSM-Cloudwatch log\r\nNavigating to the appropriate SM training log shows that the job ran for 2 hours and ended successfully. It says: \r\n`mx-tr-bench-gpu-4-node-py3-867d394-2020-09-11-21-28-30\/algo-1-1599859900`\r\n```\r\n2020-09-11 23:31:37,755 sagemaker-training-toolkit INFO     Reporting training SUCCESS\r\n```\r\n\r\n*Expected behavior:*\r\n\r\n1. PR commit status should say Failed if CodeBuild log says Failed\r\n2. CodeBuild log should not abruptly hang. It should print out the error. Currently it just terminates after printing some logs post session start.\r\n\r\n*Additional context:*\r\n",
        "Challenge_closed_time":1626207887000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1600043448000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an apt-get error in sagemaker-local-test builds due to an active and running apt-get process, resulting in the inability to acquire the dpkg frontend lock.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/deep-learning-containers\/issues\/589",
        "Challenge_link_count":4,
        "Challenge_participation_count":3,
        "Challenge_readability":12.1,
        "Challenge_reading_time":28.38,
        "Challenge_repo_contributor_count":112.0,
        "Challenge_repo_fork_count":384.0,
        "Challenge_repo_issue_count":3057.0,
        "Challenge_repo_star_count":725.0,
        "Challenge_repo_watch_count":45.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":7267.8997222222,
        "Challenge_title":"[bug] Sagemaker Remote Test reporting issues",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":244,
        "Discussion_body":"@saimidu mentioned that codebuild runs have a timeout of 90min. However, \r\n- codebuild should have shown status as timed out instead of Failed\r\n- PR commit status should have been failed instead of pending.\r\nSo that's still an open issue. Depends on #444 It appears this issue has been resolved by the PR mentioned above. Closing this ticket out.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1459541800380,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pune, Maharashtra, India",
        "Answerer_reputation_count":125.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":3.1766472222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I know that there are a similar question but it is more general and not specific of this package. I am saving a pandas dataframe within a Sagemaker Jupyter notebook into a csv in S3 as follow:<\/p>\n\n<pre><code>df.to_csv('s3:\/\/bucket\/key\/file.csv', index=False)\n<\/code><\/pre>\n\n<p>However I am getting the following error:<\/p>\n\n<pre><code>NotImplementedError: Text mode not supported, use mode='wb' and manage bytes\n<\/code><\/pre>\n\n<p>The code more or less is that I read a csv from S3, make some preprocessing on it and then saves it to S3. I can read csv from S3 successfully with:<\/p>\n\n<pre><code>df.read_csv('s3:\/\/bucket\/key\/file.csv')\n<\/code><\/pre>\n\n<p>The object that I am trying to save to S3 is indeed a <em>pandas.core.frame.DataFrame<\/em><\/p>\n\n<p>In the notebook I can see using <code>!pip show package<\/code> that I have pandas 0.24.2 and s3fs 0.1.5. <\/p>\n\n<p>What could be the problem? <\/p>",
        "Challenge_closed_time":1580493486723,
        "Challenge_comment_count":5,
        "Challenge_created_time":1580482050793,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to save a pandas dataframe into a CSV file in S3 using a Sagemaker Jupyter notebook, but is encountering an error \"NotImplementedError: Text mode not supported, use mode='wb' and manage bytes\". The user is able to read CSV from S3 successfully but is unable to save the dataframe to S3. The user is using pandas 0.24.2 and s3fs 0.1.5.",
        "Challenge_last_edit_time":1581088182972,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60006106",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":8.8,
        "Challenge_reading_time":12.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":3.1766472222,
        "Challenge_title":"NotImplementedError: Text mode not supported, use mode='wb' and manage bytes in s3fs",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1738.0,
        "Challenge_word_count":142,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1523298968403,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1754.0,
        "Poster_view_count":197.0,
        "Solution_body":"<p>Can you Please try<\/p>\n\n<pre><code>df.to_csv(\"s3:\/\/bucket\/key\/file.csv\", index=False, mode='wb')\n<\/code><\/pre>\n\n<p>It should fix your error. The default mode is <strong>w<\/strong> which writes in the file system as text and not bytes. Where as s3 expects the data to be bytes. hence you have to specify mode as <strong>wb<\/strong>(write bytes) while writing the dataframe as csv to the filesystem.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.3,
        "Solution_reading_time":5.1,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":18.5230555556,
        "Challenge_answer_count":0,
        "Challenge_body":"This is more like a suggestion than a bug. The `config` parameter to the WandBLogger is supposed to be of type `args.namespace`. Therefore it converts it to a dictionary inside its `arge_parse` function using `vars(.)`. This might be restrictive in some cases if someone wants to pass configs directly as a dictionary (for example when hyperparameters are loaded from a YAML file). Wouldn't it be better to do the conversion outside the logger to make it more general in terms of config input?\r\n\r\nThanks :)",
        "Challenge_closed_time":1635948747000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1635882064000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a bug where the wandb value is not updating, except for the learning rate (lr). The issue seems to be caused by mistakenly indexing a continuously updating list with list[0].",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ContinualAI\/avalanche\/issues\/797",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.9,
        "Challenge_reading_time":6.51,
        "Challenge_repo_contributor_count":61.0,
        "Challenge_repo_fork_count":229.0,
        "Challenge_repo_issue_count":1184.0,
        "Challenge_repo_star_count":1364.0,
        "Challenge_repo_watch_count":30.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":18.5230555556,
        "Challenge_title":"Config type in WandBLogger",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":87,
        "Discussion_body":"I agree, we can easily add support for plain dictionary. @digantamisra98 are you still working on the logger right? Can you take care of this? I've made a simple fix to it by removing the conversion inside WandBLogger. It works with plain dictionaries now. I also made a PR just in case.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1577919980176,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hamburg, Germany",
        "Answerer_reputation_count":5588.0,
        "Answerer_view_count":398.0,
        "Challenge_adjusted_solved_time":0.5756725,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to use more than 50 labels with AWS Ground Truth?<\/p>\n<p>For example <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-bounding-box.html\" rel=\"nofollow noreferrer\">here<\/a> are 3 labels:<\/p>\n<ul>\n<li>bird<\/li>\n<li>plane<\/li>\n<li>kite<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/GII4X.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/GII4X.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It shows that only 50 labels can be created. Is it possible to create more than 50 labels via AWS-CLI or any other API?<\/p>",
        "Challenge_closed_time":1606832097008,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606830024587,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is asking if it is possible to use more than 50 labels with AWS Ground Truth, as the platform currently only allows for up to 50 labels to be created. They are seeking information on whether it is possible to create more than 50 labels through AWS-CLI or any other API.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65091602",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":8.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.5756725,
        "Challenge_title":"Is it possible to use more than 50 Labels in AWS Ground Truth",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":149.0,
        "Challenge_word_count":73,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1510064331503,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"M\u00fcnchen, Deutschland",
        "Poster_reputation_count":5537.0,
        "Poster_view_count":215.0,
        "Solution_body":"<p>No, according to the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-text-classification-multilabel.html\" rel=\"nofollow noreferrer\">documentation<\/a> the maximum is 50.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":31.6,
        "Solution_reading_time":2.6,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":12.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":14.9545102778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In Advanced Scoring Scripting for AzureML webservice, to automatically generate a schema for our web service, we provide a sample of the input and\/or output in the constructor for one of the defined type objects. The type and sample are used to automatically create the schema.\nTo use schema generation, we include the open-source inference-schema package version 1.1.0 or above. The types that I can find include Numpy Type, Pandas Type, Abstract Parameter type.\nHow do we define the schema for a Nested Dictionary of (generalized) format:<\/p>\n<pre><code>{    &quot;top_level_key&quot;: [\n                         {&quot;nested_key_1&quot;: &quot;string_1&quot;,\n                          &quot;nested_key_2&quot;: &lt;float_number&gt;, \n                          &quot;nested_key_3&quot;: &lt;True\/False&gt;}\n                      ]\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1622006064007,
        "Challenge_comment_count":0,
        "Challenge_created_time":1621952227770,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to generate an inference schema for a nested dictionary with a specific format using the Azure InferenceSchema package. They are using Advanced Scoring Scripting for AzureML webservice and need to provide a sample of the input and\/or output in the constructor for one of the defined type objects to automatically create the schema. The user is looking for information on how to define the schema for a nested dictionary with a specific format.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67689868",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.6,
        "Challenge_reading_time":10.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":14.9545102778,
        "Challenge_title":"How to generate Inference Schema for Dictionary with nested structure using Azure InferenceSchema package?",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":172.0,
        "Challenge_word_count":109,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>we don\u2019t have a good way to extend the handling for generic Python class objects. However, we are planning to add support for that, basically by providing more information on the necessary hooks, and allowing users to extend a base class to implement the hook to match the desired class structure.\nThese types are currently supported:<\/p>\n<p>pandas\nnumpy\npyspark\nStandard Python object<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script#automatically-generate-a-swagger-schema\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script#automatically-generate-a-swagger-schema<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.0,
        "Solution_reading_time":9.37,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1477057589223,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Columbus, OH, United States",
        "Answerer_reputation_count":547.0,
        "Answerer_view_count":45.0,
        "Challenge_adjusted_solved_time":0.0,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We are currently working on making an Azure MachineLearning Studio experiment operational.<\/p>\n\n<p>Our most recent iteration has a webjob that accepts a queue message, gets some data to train the model, and consumes the ML Experiment webservice to put a trained model in a blob location.<\/p>\n\n<p>A second webjob accepts a queue message, pulls the data to be used in the predictive experiment, gets the location path of the trained .ilearner model, and then consumes THAT ML Experiment webservice.<\/p>\n\n<p>The data used to make the predictions is passed in as an input parameter, and the storage account name, key, and .ilearner path are all passed in as global parameters--Dictionary objects defined according to what the data scientist provided.<\/p>\n\n<p>Everything <em>appears<\/em> to work correctly--except in some cases, the predictive experiment fails, and the error message makes it clear the wrong .ilearner file is being used.<\/p>\n\n<p>When a non-existent blob path is passed to the experiment webservice, the error message reflects there is no such blob, so it's clear the webservice is at least validating the .ilearner's existence. <\/p>\n\n<p>The data scientist can run it locally, but has to change the name of the .ilearner file when he exports it locally through PowerShell. Ensuring each trained model has a unique file name did not resolve this issue.<\/p>\n\n<p>All files, when I view them in the Azure Storage Explorer, appear to be getting updated as expected based on last-modified dates. It's almost like there's a cached version of the .ilearner somewhere that isn't being overridden properly.<\/p>",
        "Challenge_closed_time":1535139575903,
        "Challenge_comment_count":0,
        "Challenge_created_time":1535139575903,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with Azure MachineLearning WebService where the wrong .ilearner file is being used in some cases, despite passing the correct file path as a global parameter. The experiment webservice is validating the .ilearner's existence, but the error message indicates the wrong file is being used. The user has tried ensuring each trained model has a unique file name, but the issue persists. It seems like there may be a cached version of the .ilearner file that isn't being overridden properly.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52010761",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":20.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":0.0,
        "Challenge_title":"Azure MachineLearning WebService Not Using Passed .ilearner Model",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":65.0,
        "Challenge_word_count":260,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1477057589223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Columbus, OH, United States",
        "Poster_reputation_count":547.0,
        "Poster_view_count":45.0,
        "Solution_body":"<p>After ruling out all possibility of passing in the wrong file, our data scientist took a closer look at the experiment itself. He discovered that it was defaulting to one hardcoded .ilearner path he had been using in development.<\/p>\n\n<p>At one point in time, he had created webservice parameters to override this value (hence why I had them defined in my webservice call), but they had been removed during one of the redesigns of the experiment with anyone noticing, because the webservice will apparently accept superfluous arguments.<\/p>\n\n<p><strong>The webservice was accepting my global parameters<\/strong>, and apparently even validating them. But since they weren't wired to anything inside <strong>the experiment the passed .ilearner file info was never applied to anything<\/strong>--the hardcoded .ilearner was being applied no matter what.<\/p>\n\n<p>We were all very surprised there was no exception thrown about passing in parameters to the webservice that weren't actually defined. Had <em>that<\/em> happened, we would have gotten to the bottom of it much more quickly.<\/p>\n\n<p>tl\/dr: The experiment wasn't properly configured to accept an .ilearner file path (or Account Name, or Account Key) as a parameter, and the webservice was happily accepting and ignoring the parameter arguments without raising any alarm since it had the hardcoded value to run with.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.2,
        "Solution_reading_time":17.11,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":208.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3836.5522222222,
        "Challenge_answer_count":0,
        "Challenge_body":"Currently the `.drone.yaml` is referencing the k8s secret `mlflow-server-secret` which doesn't exist by default.\r\n\r\nWe have noticed that `{{ .ProjectID }}-mlflow-secret` secret is created when a `kdlProject` resource is created.\r\n\r\nTo solve this issue the name of the `mlflow-server-secret` must be changed into `{{ .ProjectID }}-mlflow-secret`",
        "Challenge_closed_time":1664791991000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650980403000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where the default MLflow artifact folder is not replacing the `$ARTIFACTS_BUCKET` environment variable.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/810",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":8.3,
        "Challenge_reading_time":4.82,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":933.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":3836.5522222222,
        "Challenge_title":"Project template mlflow secret bad name",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":49,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1476195722390,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":399.0,
        "Answerer_view_count":75.0,
        "Challenge_adjusted_solved_time":12.5487494444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using the generated code from huggingface, Task: <code>Zero-Shot Classification<\/code>, Configuration: <code>AWS<\/code> and running it in Sagemaker's jupyterlab<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.huggingface import HuggingFaceModel\nimport sagemaker\n\nrole = sagemaker.get_execution_role()\n# Hub Model configuration. https:\/\/huggingface.co\/models\nhub = {\n    'HF_MODEL_ID':'facebook\/bart-large-mnli',\n    'HF_TASK':'zero-shot-classification'\n}\n\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n    transformers_version='4.6.1',\n    pytorch_version='1.7.1',\n    py_version='py36',\n    env=hub,\n    role=role, \n)\n\n# deploy model to SageMaker Inference\npredictor = huggingface_model.deploy(\n    initial_instance_count=1, # number of instances\n    instance_type='ml.m5.xlarge' # ec2 instance type\n)\n\npredictor.predict({\n    'inputs': &quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;\n})\n<\/code><\/pre>\n<p>The following error returned:<\/p>\n<blockquote>\n<p>ModelError: An error occurred (ModelError) when calling the\nInvokeEndpoint operation: Received client error (400) from primary\nwith message &quot;{   &quot;code&quot;: 400,   &quot;type&quot;: &quot;InternalServerException&quot;,<br \/>\n&quot;message&quot;: &quot;<strong>call<\/strong>() missing 1 required positional argument:\n\\u0027candidate_labels\\u0027&quot; } &quot;. See ...\nin account **** for more information.<\/p>\n<\/blockquote>\n<p>I tried running them differently such as this,<\/p>\n<pre><code>predictor.predict({\n    'inputs': &quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;,\n    'candidate_labels': ['science', 'life']\n})\n<\/code><\/pre>\n<p>but still don't work. How should I run it?<\/p>",
        "Challenge_closed_time":1635482654648,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635437479150,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while deploying huggingface zero-shot classification in Sagemaker using a template. The error message states that a positional argument 'candidate_labels' is missing. The user has tried running the code with the 'candidate_labels' argument, but it still does not work. The user is seeking guidance on how to resolve the issue.",
        "Challenge_last_edit_time":1635487565192,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69757539",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":16.5,
        "Challenge_reading_time":25.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":12.5487494444,
        "Challenge_title":"Deploying huggingface zero-shot classification in Sagemaker using template returns error, missing positional argument 'candidate_labels'",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":287.0,
        "Challenge_word_count":191,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1476195722390,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":399.0,
        "Poster_view_count":75.0,
        "Solution_body":"<p>The schema of request body for a zero-shot classification model is defined in this <a href=\"https:\/\/github.com\/huggingface\/notebooks\/blob\/772ddf7140bccc443da265c90c95eda99e69c564\/sagemaker\/10_deploy_model_from_s3\/deploy_transformer_model_from_s3.ipynb\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>\n<pre><code>{\n    &quot;inputs&quot;: &quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;,\n    &quot;parameters&quot;: {\n        &quot;candidate_labels&quot;: [\n            &quot;refund&quot;,\n            &quot;legal&quot;,\n            &quot;faq&quot;\n        ]\n    }\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.2,
        "Solution_reading_time":7.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":49.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1423224680703,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Italy",
        "Answerer_reputation_count":1034.0,
        "Answerer_view_count":207.0,
        "Challenge_adjusted_solved_time":0.2080508334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've been running some optimization with optuna, and I'd like to produce plots with the same scale on both axes, but so far I was unable to find out how.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>study = optuna.create_study(study_name=study_name,\n                            storage=f&quot;sqlite:\/\/\/{results_folder}\/{study_name}.db&quot;,\n                            directions=[&quot;maximize&quot;, &quot;maximize&quot;],\n                            load_if_exists=True)\n# I tried either\nfig_pareto = optuna.visualization.plot_pareto_front(study, target_names=['precision', 'recall'])\nfig_pareto.show()\n\n# or\nfig, ax = plt.subplots()\noptuna.visualization.matplotlib.plot_pareto_front(study, target_names=['precision', 'recall']) \nax.axis(&quot;equal&quot;)\nax.set_xlim(0.7, 1)\nax.set_ylim(0.7, 1)\ntarget_names=['precision', 'recall'])\nplt.savefig(&quot;some_name.png&quot;)\n\n<\/code><\/pre>\n<p>but without success.\nThis is what the saved plot looks like:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ejZ2f.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ejZ2f.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>With the first method, the pictures open in an interactive view in the browser and I can resize them, but there's no option to precisely make them square.<\/p>\n<p>When using the second way, it looks like calling:<\/p>\n<pre><code>optuna.visualization.matplotlib.plot_pareto_front(study, target_names=['precision', 'recall']) \n<\/code><\/pre>\n<p>is not linking the produced plot to the ax object?\nIf I do:<\/p>\n<pre><code>pareto_plot = optuna.visualization.matplotlib.plot_pareto_front(study, target_names=['precision', 'recall'])  \n<\/code><\/pre>\n<p>the pareto_plot is an AxesSubplot object, can I manually load it in the axes?<\/p>",
        "Challenge_closed_time":1660906320883,
        "Challenge_comment_count":2,
        "Challenge_created_time":1660901645950,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to produce plots with the same scale on both axes using Optuna, but has been unsuccessful so far. They have tried two methods, but neither has worked. The saved plot does not have a square shape, and when using the second method, the produced plot is not linked to the ax object. The user is wondering if they can manually load the produced plot in the axes.",
        "Challenge_last_edit_time":1660905571900,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73414713",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":15.3,
        "Challenge_reading_time":22.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":1.2985925,
        "Challenge_title":"Optuna, change axes ratio in plots",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":56.0,
        "Challenge_word_count":161,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423224680703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Italy",
        "Poster_reputation_count":1034.0,
        "Poster_view_count":207.0,
        "Solution_body":"<p>Sorry, I had the answer right there.\nI had to do:<\/p>\n<pre><code>pareto = optuna.visualization.matplotlib.plot_pareto_front(study, target_names=['precision', 'recall'])\npareto.axis('equal')\nplt.savefig(&quot;pareto_plot.png&quot;)\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":19.8,
        "Solution_reading_time":3.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":18.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.6055555556,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nIf I create a PipelineML objects  and I return it in the `hooks.py`:\r\n\r\n\r\n```python\r\nclass ProjectHooks:\r\n    @hook_impl\r\n    def register_pipelines(self) -> Dict[str, Pipeline]:\r\n        \"\"\"Register the project's pipeline.\r\n        Returns:\r\n            A mapping from a pipeline name to a ``Pipeline`` object.\r\n        \"\"\"\r\n       ml_pipeline=create_ml_pipeline()\r\n        training_pipeline = pipeline_ml_factory(training=ml_pipeline.only_nodes_with_tags(\"training\"), inference=ml_pipeline.only_nodes_with_tags(\"inference\"), input_name=\"instances\")\r\n\r\n        return {\r\n            \"training\": training_pipeline,\r\n            \"__default__\": other_pipeline\r\n        }\r\n````\r\n\r\n`kedro run` command works fine, but `kedro viz` and `kedro pipeline list` fail.\r\n\r\n## Context\r\n\r\nI was trying to visualise a pipeline with kedro-viz==3.7.0 (I also tried 3.4.0 and 3.0.0), and kedro==0.16.6\r\n\r\n## Steps to Reproduce\r\n\r\n1. Create a PipelineMl object with pipeline_ml_factory in `hooks;py`\r\n2. Launch `kedro viz` in terminal\r\n\r\n## Expected Result\r\nKedro viz should be launched on localhost:5000\r\n\r\n## Actual Result\r\nTell us what happens instead.\r\n\r\n```\r\n-- If you received an error, place it here.\r\n```\r\n\r\n```\r\n-- Separate them if you have more than one.\r\n```\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`):\r\n* Python version used (`python -V`):\r\n* Operating system and version:\r\n\r\n*Note: everything works fine with the older template (`kedro<=0.16.4`) and the `pipeline.py` file instead of `hooks.py`*\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n## Potential solution: \r\n\r\nIt seems the `__add__` method of the `PipelineML` class must be implemented.",
        "Challenge_closed_time":1605720463000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1605718283000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing issues with the introduction of namespaces in kedro 0.17.7, which are not properly handled in kfp artifacts. This causes errors when running or updating the pipeline, and can be resolved by disabling the function to create kfp artifacts in the `kubeflow.yaml` config.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/119",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.5,
        "Challenge_reading_time":22.28,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":21.0,
        "Challenge_repo_issue_count":414.0,
        "Challenge_repo_star_count":145.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":0.6055555556,
        "Challenge_title":"PipelineML objects in `hooks.py` breaks all kedro-viz versions with kedro template>=0.16.5",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":218,
        "Discussion_body":"The issue is not confirmed and was due to adding a Pipeline and a PipelineML object.\r\nI close it.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":384.0669444444,
        "Challenge_answer_count":0,
        "Challenge_body":"### System Specs\r\n**Operating System:** Windows 10\r\n**Python Version:** 3.9.5 64-bit\r\n\r\nWhen I run the command:\r\n\r\n```terminal\r\npip install azureml-core\r\n```\r\n\r\nI get an error during the installation, specifically on the `ruamel.yaml` package. I guess the first question I have is there any reason we are restricted to that specific version of `ruamel.yaml`? I was able to install the latest version **(0.17.10)** no problem, so if we could use a later version that would be the easiest fix.\r\n\r\n### Partial Log\r\n```terminal\r\nAttempting uninstall: ruamel.yaml\r\nFound existing installation: ruamel.yaml 0.17.10\r\nUninstalling ruamel.yaml-0.17.10:\r\nSuccessfully uninstalled ruamel.yaml-0.17.10\r\nRunning setup.py install for ruamel.yaml ... error\r\nERROR: Command errored out with exit status 1:\r\n```\r\n\r\n### Full Log\r\n[Error Log From Installation Run](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/files\/6913613\/error.log)",
        "Challenge_closed_time":1629244160000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1627861519000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an AzureMLException while trying to download a registered model from the AMLS workspace. The file is being created in the target directory but with 0 bytes, indicating that no data is being transferred into it. The traceback shows a FileNotFoundError and an AzureMLException.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1564",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.2,
        "Challenge_reading_time":12.03,
        "Challenge_repo_contributor_count":58.0,
        "Challenge_repo_fork_count":2387.0,
        "Challenge_repo_issue_count":1906.0,
        "Challenge_repo_star_count":3704.0,
        "Challenge_repo_watch_count":2001.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":384.0669444444,
        "Challenge_title":"pip install `azureml-core` fails on `ruamel.yaml`",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":119,
        "Discussion_body":"0.17.5 introduced a breaking change hence there is an upperbound Thank you @vizhur! @areed1192, I'm closing this issue. Please reopen if you still have questions.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.4287777778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>hello new to ML studio. We have some trained model already but I want to use the studio for my next step. How should I import my model and retrain them? <\/p>",
        "Challenge_closed_time":1664410354943,
        "Challenge_comment_count":1,
        "Challenge_created_time":1664405211343,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is new to ML studio and needs guidance on how to import and retrain their already trained models in the studio.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1027872\/need-guidance-to-use-train-models-in-ml-studio",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":3.7,
        "Challenge_reading_time":2.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.4287777778,
        "Challenge_title":"need guidance to use train models in ML studio",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":40,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=94bb880d-285d-4901-b0c8-d07117e001d8\">@Manuel  <\/a>     <\/p>\n<p>Thanks for using Microsoft Q&amp;A platform. Yes you can import your trained model to Azure Machine Learning Studio - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-models?tabs=use-local\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-models?tabs=use-local<\/a>    <\/p>\n<p>You can learn how to register a model from different locations, and how to use the Azure Machine Learning SDK, the user interface (UI), and the Azure Machine Learning CLI to manage your models.    <\/p>\n<p>Please check on above article to see how to register your model. I hope it helps.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":10.89,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":99.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1436818579270,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Eugene, OR, USA",
        "Answerer_reputation_count":474.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":239.6215952778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using the Score Matchbox Recommender set to recommend items from unrated items. This module will run for over 3 hours (I haven't tried longer) and not finish. It will work fine when I'm recommending from rated items to evaluate the recommender, but as soon as I switch to unrated it will run indefinitely. I'm currently using the split data module on already split data to get an even smaller sample of about 20,000 rows. Is this too much for this module to handle? <\/p>\n\n<p>If I try to take the sample even smaller using the partition and sample module (yes I know it's not a recommender split), I immediately get an Exception 0000: Internal system error. <\/p>\n\n<p>Any idea why it's taking so long\/how to fix it?<\/p>",
        "Challenge_closed_time":1530555170483,
        "Challenge_comment_count":2,
        "Challenge_created_time":1529692532740,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing issues with the Score Matchbox Recommender module, which is taking over 3 hours to recommend items from unrated items and not finishing. The module works fine when recommending from rated items, but not with unrated items. The user is using the split data module on already split data to get a sample of about 20,000 rows, but it is still not working. The user is also encountering an internal system error when trying to use the partition and sample module to reduce the sample size. The user is seeking help to understand why it is taking so long and how to fix it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50993858",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.7,
        "Challenge_reading_time":9.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":239.6215952778,
        "Challenge_title":"Score Matchbox Recommender Stuck or Throwing an Error",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":215.0,
        "Challenge_word_count":135,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436818579270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Eugene, OR, USA",
        "Poster_reputation_count":474.0,
        "Poster_view_count":28.0,
        "Solution_body":"<p>Using the \"Filter Based Feature Selection\" module and then removing all columns besides those found significant and the identifiers seemed to fix the issue. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.0,
        "Solution_reading_time":2.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1468179475927,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pasadena, CA, United States",
        "Answerer_reputation_count":795.0,
        "Answerer_view_count":210.0,
        "Challenge_adjusted_solved_time":340.7236255556,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hi i am started to learning the azure data lake and azure machine learning ,i need to use the azure data lake storage as a azure machine learning studio input data .There have a any options are there, i gone through the azure data lake and machine learning documentation but i can't reach that,finally i got one solution on this \n<a href=\"https:\/\/stackoverflow.com\/questions\/36127510\/how-to-use-azure-data-lake-store-as-an-input-data-set-for-azure-ml\">link<\/a> but they are mentioning there is no option for it,but this post is old one,so might be the Microsoft people added the future  on it if it's please let me know, let me know Thank you. <\/p>",
        "Challenge_closed_time":1490126381092,
        "Challenge_comment_count":0,
        "Challenge_created_time":1488899776040,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in connecting Azure Data lake storage to Azure ML for using it as input data. Despite going through the documentation, the user was unable to find a solution. The user found an old post on Stack Overflow that mentioned there was no option for it, but is unsure if Microsoft has added the feature since then.",
        "Challenge_last_edit_time":1495535398003,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42651900",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":12.6,
        "Challenge_reading_time":8.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":340.7236255556,
        "Challenge_title":"How to connect Azure Data lake storage to Azure ML?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":4966.0,
        "Challenge_word_count":108,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1487413134923,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Gurugram, Haryana, India",
        "Poster_reputation_count":321.0,
        "Poster_view_count":65.0,
        "Solution_body":"<p>I recommend the following:<\/p>\n\n<ul>\n<li>Get a tenant ID, client ID, and client secret for your ADLS using the tutorial <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/data-lake-store\/data-lake-store-authenticate-using-active-directory#step-2-get-client-id-client-secret-and-tenant-id\" rel=\"nofollow noreferrer\">here<\/a>.<\/li>\n<li>Install the <a href=\"https:\/\/github.com\/Azure\/azure-data-lake-store-python\" rel=\"nofollow noreferrer\"><code>azure-datalake-store<\/code><\/a> Python package on AML Studio by attaching it as a Script Bundle to an Execute Python Script module.<\/li>\n<li>In the Execute Python Script module, import the <code>azure-datalake-store<\/code> package and connect to the ADLS with your tenant ID, client ID, and client secret.<\/li>\n<li>Download the data you need from ADLS and convert it into a dataframe within the Python Script module; return that dataframe to make the data available in the rest of AML Studio.<\/li>\n<\/ul>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.7,
        "Solution_reading_time":12.34,
        "Solution_score_count":4.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":105.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":55.1218747222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hello MS team,<\/p>\n<p>I am using Azure devOps pipeline to submit a control script to the Azure-ML workspace. This control script in turn kicks off the Azure-ML pipeline containing pythonscriptsteps and hyperdrive step.<\/p>\n<p><strong>My directory structure:<\/strong><\/p>\n<p>.  <br \/>\n\u251c\u2500\u2500\u2500.vscode  <br \/>\n\u251c\u2500\u2500\u2500Automation  <br \/>\n\u251c\u2500\u2500\u2500Build  <br \/>\n\u2514\u2500\u2500\u2500Source  <br \/>\n\u251c\u2500\u2500\u2500.azureml  <br \/>\n\u251c\u2500\u2500\u2500.vscode  <br \/>\n\u251c\u2500\u2500\u2500amlcode  <br \/>\n\u2502 \u251c\u2500\u2500\u2500projectcode  <br \/>\n\u2502 \u2514\u2500\u2500\u2500<strong>pycache<\/strong>  <br \/>\n\u251c\u2500\u2500\u2500config  <br \/>\n\u251c\u2500\u2500\u2500Data  <br \/>\n\u251c\u2500\u2500\u2500setup  <br \/>\n\u251c\u2500\u2500\u2500tests  <br \/>\n\u2502 \u251c\u2500\u2500\u2500.pytest_cache  <br \/>\n\u2502 \u2502 \u2514\u2500\u2500\u2500v  <br \/>\n\u2502 \u2502 \u2514\u2500\u2500\u2500cache  <br \/>\n\u2502 \u2514\u2500\u2500\u2500<strong>pycache<\/strong>  <br \/>\n\u2514\u2500\u2500\u2500<strong>pycache<\/strong><\/p>\n<p>So here one of the azure cli task in Azure DevOps pipeline uses:<\/p>\n<p><em>az ml folder attach -w $(azureml.workspaceName) -g $(azureml.resourceGroup)<\/em><\/p>\n<p><strong>This command attaches my whole directory to the AML workspace and automatically creates &quot;.amlignore&quot; and &quot;.azureml&quot; is automatically added to that.<\/strong><\/p>\n<p>So it is throwing an authentication error as the <em>config.json()<\/em> is not found because it is generally put in the path <em>\/.azureml<\/em>.<\/p>\n<p>Where to put the config.json() then? What is the best practice?<\/p>",
        "Challenge_closed_time":1643613417496,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643414978747,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue with the Azure CLI command \"az ml attach folder\" which is adding the .azureml directory to .amlignore, causing an authentication error as the config.json file is not found. The user is seeking advice on where to put the config.json file when using Azure DevOps pipeline to submit a script to AML workspace.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/714713\/the-azure-cli-command-az-ml-attach-folder-is-direc",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.2,
        "Challenge_reading_time":18.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":55.1218747222,
        "Challenge_title":"The azure cli command \"az ml attach folder\" is directly adding .azureml directory to .amlignore , so where to put config.json when using Azure devops pipeline to submit script to aml workspace?",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":178,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=6755dac2-30f1-48a2-9d0d-4d2c96edc5d4\">@Shivapriya Katta  <\/a> The command az ml folder attach will create the directories and add the config file to .azureml to ensure the workspace resources are easily accessible. You can lookup the note section of the command for <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/reference-azure-machine-learning-cli\">reference<\/a>.    <\/p>\n<blockquote>\n<p>This command creates a .azureml subdirectory that contains example runconfig and conda environment files. It also contains a config.json file that is used to communicate with your Azure Machine Learning workspace.    <\/p>\n<\/blockquote>\n<p>The authentication error in your case could be because <code>az login<\/code> command might have been missed which allows the cli to authenticate interactively or service principal or MI and then run rest of the commands. You can try to run <a href=\"https:\/\/learn.microsoft.com\/en-us\/cli\/azure\/authenticate-azure-cli\">this<\/a> and check if the attach works successfully.     <\/p>\n<p>Also, with the devops pipeline I am not sure if <code>az devops login<\/code>  is required to be run but if the above command fails even after <code>az login<\/code> authentication you can try <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/devops\/cli\/?view=azure-devops\">az devops login<\/a>.    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":11.7,
        "Solution_reading_time":22.31,
        "Solution_score_count":1.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":189.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1639972620503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1653.0,
        "Answerer_view_count":1212.0,
        "Challenge_adjusted_solved_time":500.2513305555,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>While the artifact repository was successfully creating, running a docker push to push the image to the google artifact registry fails with a permissions error even after granting all artifact permissions to the accounting I am using on gcloud cli.<\/p>\n<p><strong>Command used to push image:<\/strong><\/p>\n<pre><code>docker push us-central1-docker.pkg.dev\/project-id\/repo-name:v2\n<\/code><\/pre>\n<p><strong>Error message:<\/strong><\/p>\n<pre><code>The push refers to repository [us-central1-docker.pkg.dev\/project-id\/repo-name]\n6f6f4a472f31: Preparing\nbc096d7549c4: Preparing\n5f70bf18a086: Preparing\n20bed28d4def: Preparing\n2a3255c6d9fb: Preparing\n3f5d38b4936d: Waiting\n7be8268e2fb0: Waiting\nb889a93a79dd: Waiting\n9d4550089a93: Waiting\na7934564e6b9: Waiting\n1b7cceb6a07c: Waiting\nb274e8788e0c: Waiting\n78658088978a: Waiting\ndenied: Permission &quot;artifactregistry.repositories.downloadArtifacts&quot; denied on resource &quot;projects\/project-id\/locations\/us-central1\/repositories\/repo-name&quot; (or it may not exist)\n\n\n<\/code><\/pre>",
        "Challenge_closed_time":1652683203067,
        "Challenge_comment_count":3,
        "Challenge_created_time":1652644857243,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered a permissions error while trying to push an image to the Google Artifact Registry using the \"docker push\" command. The error message indicates that the permission \"artifactregistry.repositories.downloadArtifacts\" was denied on the resource \"projects\/project-id\/locations\/us-central1\/repositories\/repo-name\". The user had already granted all artifact permissions to the accounting being used on gcloud cli.",
        "Challenge_last_edit_time":1652667678670,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72251787",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":19.2,
        "Challenge_reading_time":14.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":12.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":10.6516177778,
        "Challenge_title":"Permission \"artifactregistry.repositories.downloadArtifacts\" denied on resource",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":5722.0,
        "Challenge_word_count":100,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1426920929352,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangkok",
        "Poster_reputation_count":194.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>I was able to recreate your use case. This happens when you are trying to push an image on a <code>repository<\/code> in which its specific hostname (associated with it's repository location) is not yet  added to the credential helper configuration for authentication. You may refer to this <a href=\"https:\/\/cloud.google.com\/artifact-registry\/docs\/docker\/authentication\" rel=\"noreferrer\">Setting up authentication for Docker <\/a> as also provided by @DazWilkin in the comments for more details.<\/p>\n<p>In my example, I was trying to push an image on a repository that has a location of <code>us-east1<\/code> and got the same error since it is not yet added to the credential helper configuration.\n<a href=\"https:\/\/i.stack.imgur.com\/NQeIf.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NQeIf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>And after I ran the authentication using below command (specifically for us-east1 since it is the <code>location<\/code> of my repository), the image was successfully pushed:<\/p>\n<pre><code>gcloud auth configure-docker us-east1-docker.pkg.dev\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q2Q9x.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q2Q9x.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><em><strong>QUICK TIP<\/strong><\/em>: You may  get your authentication command specific for your repository when you open your desired repository in the <a href=\"https:\/\/console.cloud.google.com\/artifacts\" rel=\"noreferrer\">console<\/a>, and then click on the <code>SETUP INSTRUCTIONS<\/code>.\n<a href=\"https:\/\/i.stack.imgur.com\/KBjqa.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KBjqa.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1654468583460,
        "Solution_link_count":8.0,
        "Solution_readability":14.8,
        "Solution_reading_time":22.51,
        "Solution_score_count":23.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":188.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1577919980176,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hamburg, Germany",
        "Answerer_reputation_count":5588.0,
        "Answerer_view_count":398.0,
        "Challenge_adjusted_solved_time":1.1658544444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I open AWS Notebook Instance-&gt; Jupyter Notebook. It gives me a storage (probably called an S3 bucket). I created a folder there and tried to upload 1000s of data. However, it asks me to manually click on the upload button next to every single file. Is it possible to upload that data much easier way?<\/p>",
        "Challenge_closed_time":1599632498016,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599628300940,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing difficulty in uploading thousands of files to their AWS Notebook Instance's S3 bucket as they have to manually click on the upload button for each file. They are looking for a more efficient way to upload the data.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63805114",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.2,
        "Challenge_reading_time":4.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.1658544444,
        "Challenge_title":"Uploading 1000s of files to AWS Notebook Instance",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":382.0,
        "Challenge_word_count":63,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369539919747,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":311.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>You could use the <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/userguide\/cli-services-s3-commands.html#using-s3-commands-managing-objects-move\" rel=\"nofollow noreferrer\">AWS-CLI<\/a> or the <a href=\"https:\/\/docs.aws.amazon.com\/AWSJavaScriptSDK\/latest\/AWS\/S3.html\" rel=\"nofollow noreferrer\">AWS-S3 SDK<\/a> (JS in this example).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":27.0,
        "Solution_reading_time":4.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":13.0572022222,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I need to import MatPlotLib images into WandB.  On the surface, this seems simple, since the documentation clearly shows how to ingest a <code>plt<\/code> or <code>fig<\/code> object.  However, WandB is making a mess of the plots and I don\u2019t want to recode them in plotly.<br>\nSo I next want to use MatPlotLib to save a PNG and ingest that.  Again seems easy, but I would prefer to do it using an in-memory buffer object (this avoids messing with local paths and temp directories on various instances).  Apparently I\u2019m not the first one to do this either (<a href=\"https:\/\/stackoverflow.com\/questions\/35999020\/convert-pyplot-figure-into-wand-image-image\" rel=\"noopener nofollow ugc\">link<\/a>). The instructions are clear and show someone has already done this.  But it fails when I try it:<\/p>\n<pre><code class=\"lang-auto\">fig, (ax1, ax2) = plt.subplots(2, 1, dpi=300, figsize=(10, 5))\n...\nbuf = io.BytesIO()\nplt.savefig(buf, format='png')\nbuf.seek(0)\nwandb.log(({\"chart\": wandb.Image(file=buf)}))\n<\/code><\/pre>\n<p>The error seems to be with <code>wandb.Image()<\/code>.  It returns:<br>\n<code>{TypeError}__init__() got an unexpected keyword argument 'file'<\/code><\/p>\n<p>I can remove the <code>file=<\/code> parameter so that the command is:<\/p>\n<pre><code class=\"lang-auto\">wandb.Image(buf)\n<\/code><\/pre>\n<p>And I get: <code>{AttributeError}'_io.BytesIO' object has no attribute 'ndim'<\/code><\/p>\n<p>Any recommendations?<\/p>",
        "Challenge_closed_time":1664817995860,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664770989932,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to import MatPlotLib images into WandB, but WandB is not displaying the plots correctly. The user then tries to save the plot as a PNG and ingest it using an in-memory buffer object, but encounters an error with the WandB Image function. The error message suggests that the 'file' parameter is unexpected, and removing it results in another error stating that the '_io.BytesIO' object has no attribute 'ndim'. The user is seeking recommendations to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/matplotlib-into-wandb\/3212",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":10.0,
        "Challenge_reading_time":18.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":13.0572022222,
        "Challenge_title":"MatPlotLib into WandB",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":817.0,
        "Challenge_word_count":179,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/kevinashaw\">@kevinashaw<\/a>!<\/p>\n<p>The <code>BytesIO<\/code> type is not supported by <code>wandb.Image<\/code> which is why you are running into this issue. Here are a few options that would work instead:<\/p>\n<ul>\n<li><code>wandb.log({ 'chart' : wandb.Image(Image.open(buf)) })<\/code><\/li>\n<li><code>wandb.log({ 'chart' : wandb.Image(fig) })<\/code><\/li>\n<li>\n<code>wandb.log({ 'chart' : fig })<\/code> (Please note that this does not actually save an image but an interactable Plotly chart on your workspace<\/li>\n<\/ul>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":7.48,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":65.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":32.7057008333,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>I am new to the Azure ML Studio and just deployed the bike-rental regression model. When I tried to test it using the built in test tool in the studio, I am getting the attached error. Similar results running the Python code as well. Can someone please help me?    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/176918-mlerror.png?platform=QnA\" alt=\"176918-mlerror.png\" \/>    <\/p>",
        "Challenge_closed_time":1645695329740,
        "Challenge_comment_count":4,
        "Challenge_created_time":1645577589217,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while testing a bike-rental regression model in Azure ML Studio's built-in test tool and Python code. The error message indicates a \"list index out of range\" issue. The user is seeking assistance to resolve the problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/746784\/azure-ml-studio-error-while-testing-real-time-endp",
        "Challenge_link_count":1,
        "Challenge_participation_count":9,
        "Challenge_readability":8.8,
        "Challenge_reading_time":6.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":32.7057008333,
        "Challenge_title":"Azure ML Studio error while testing real-time endpoint -  list index out of range",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":66,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=b7844017-59f9-4d2e-a021-76c2270e06ca\">@Kumar, Priya  <\/a> Thanks for the question. It's known issue and the product team working on the fix to change in the UI.    <\/p>\n<p>Workaround: As shown below please set the GlobalParameters flag to 1.0 or a float number or remove it.     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/177485-image.png?platform=QnA\" alt=\"177485-image.png\" \/>    <\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.4,
        "Solution_reading_time":5.69,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":48.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1468179475927,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pasadena, CA, United States",
        "Answerer_reputation_count":795.0,
        "Answerer_view_count":210.0,
        "Challenge_adjusted_solved_time":155.9679077778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have 4 csv files that are inputs to the python script in azure ML, but the widget has only 2 inputs for dataframes and the third for a zip file. I tried to put the csv files in a zipped folder and connect it to the third input for the script but that also did not work :\n<a href=\"https:\/\/i.stack.imgur.com\/FkxRE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/FkxRE.png\" alt=\"Image of workspace\"><\/a><\/p>\n\n<p>I would like to know how to read multiple csv files in the python script.<\/p>",
        "Challenge_closed_time":1500412322168,
        "Challenge_comment_count":1,
        "Challenge_created_time":1499844354733,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has 4 CSV files as inputs to a Python script in Azure ML, but the widget only allows for 2 inputs for dataframes and 1 for a zip file. The user attempted to put the CSV files in a zipped folder and connect it to the third input, but it did not work. The user is seeking guidance on how to read multiple CSV files in the Python script.",
        "Challenge_last_edit_time":1499850837700,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45051055",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":6.7,
        "Challenge_reading_time":6.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":157.7687319444,
        "Challenge_title":"Read multiple CSV files in Azure ML Python Script",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":944.0,
        "Challenge_word_count":89,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1470376815796,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":596.0,
        "Poster_view_count":57.0,
        "Solution_body":"<p>Here's some more detail on the approach others have outlined above. Try replacing the code currently in the \"Execute Python Script\" module with the following:<\/p>\n\n<pre><code>import pandas as pd\nimport os\ndef azureml_main(dataframe1=None, dataframe2=None):\n    print(os.listdir('.'))\n    return(pd.DataFrame([]))\n<\/code><\/pre>\n\n<p>After running the experiment, click on the module. There should be a \"View output log\" link now in the right-hand bar. I get something like the following:<\/p>\n\n<pre><code>[Information]         Started in [C:\\temp]\n[Information]         Running in [C:\\temp]\n[Information]         Executing 4af67c05ba02417a980f6a16e84e61dc with inputs [] and generating outputs ['.maml.oport1']\n[Information]         Extracting Script Bundle.zip to .\\Script Bundle\n[Information]         File Name                                             Modified             Size\n[Information]         temp.csv                                       2016-05-06 13:16:56           52\n[Information]         [ READING ] 0:00:00\n[Information]         ['4af67c05ba02417a980f6a16e84e61dc.py', 'Script Bundle', 'Script Bundle.zip']\n<\/code><\/pre>\n\n<p>This tells me that the contents of my zip file have been extracted to the <code>C:\\temp\\Script Bundle<\/code> folder. In my case the zip file contained just one CSV file, <code>temp.csv<\/code>: your output would probably have four files. You may also have zipped a folder containing your four files, in which case the filepath would be one layer deeper. You can use the <code>os.listdir()<\/code> to explore your directory structure further if necessary.<\/p>\n\n<p>Once you think you know the full filepaths for your CSV files, edit your Execute Python Script module's code to load them, e.g.:<\/p>\n\n<pre><code>import pandas as pd\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    df = pd.read_csv('C:\/temp\/Script Bundle\/temp.csv')\n    # ...load other files and merge into a single dataframe...\n    return(df)\n<\/code><\/pre>\n\n<p>Hope that helps!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":23.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":228.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0066666667,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nThere used to be a switch to show\/hide all active runs (multiple states at once) under the flags dropdown, I can't find it anymore. Was it removed from the UI?",
        "Challenge_closed_time":1649332897000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649332873000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to find the switch to show\/hide all active runs under the flags dropdown and is wondering if it has been removed from the UI.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1479",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":6.4,
        "Challenge_reading_time":2.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.0066666667,
        "Challenge_title":"How to show all active runs without selecting the status manually",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":42,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"It's directly under statuses, we consolidated some flags\/config options:",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.9,
        "Solution_reading_time":0.94,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":9.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.5714147222,
        "Challenge_answer_count":10,
        "Challenge_body":"<p>It would be great to plot hparams without doing sweeps, most of the time I\u2019m doing experiments and I would love the plot to be across runs and not as a sweep. It might be complex to make this feature automated, but I\u2019m fine if it\u2019s within one run, would be great to have something like <code>wandb.plots.ParallelCoordinates<\/code><\/p>",
        "Challenge_closed_time":1671425374288,
        "Challenge_comment_count":0,
        "Challenge_created_time":1671394517195,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to create a Parallel Coordinates plot without doing sweeps, as they mostly conduct experiments and would like the plot to be across runs. They suggest having a feature like \"wandb.plots.ParallelCoordinates\" within one run.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-create-parallel-coordinates-plot-without-sweeps\/3566",
        "Challenge_link_count":0,
        "Challenge_participation_count":10,
        "Challenge_readability":10.4,
        "Challenge_reading_time":4.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":8.5714147222,
        "Challenge_title":"How to create Parallel Coordinates plot without sweeps",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":294.0,
        "Challenge_word_count":64,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Faris!<\/p>\n<p>You absolutely can use Parallel coordinates plots without sweeps. The web UI has an option to add additional plots on the top right of the graph section which contains the Parallel Coordinates Plot.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":3.1,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":37.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":78.9828208333,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Hey Guys, i just started using wandb and so far everything is working pretty well, however I noticed that there seems to be a problem with strings as parameters in parallel coordinates chart<\/p>\n<p>Attached is a screenshot to illustrate the problem<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/f0054aff74ba40b8cb33e86b18b172e51cd527c7.jpeg\" data-download-href=\"\/uploads\/short-url\/yfjVQxyNGXrfluwDks1707BnsO3.jpeg?dl=1\" title=\"wandb_string_problem\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f0054aff74ba40b8cb33e86b18b172e51cd527c7_2_690x374.jpeg\" alt=\"wandb_string_problem\" data-base62-sha1=\"yfjVQxyNGXrfluwDks1707BnsO3\" width=\"690\" height=\"374\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f0054aff74ba40b8cb33e86b18b172e51cd527c7_2_690x374.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f0054aff74ba40b8cb33e86b18b172e51cd527c7_2_1035x561.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f0054aff74ba40b8cb33e86b18b172e51cd527c7_2_1380x748.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f0054aff74ba40b8cb33e86b18b172e51cd527c7_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">wandb_string_problem<\/span><span class=\"informations\">1502\u00d7816 332 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>So now I wonder if I made a mistake or if I have to wait for a fix from you.<\/p>\n<p>Kind regards<br>\nChris<\/p>",
        "Challenge_closed_time":1641515228648,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641230890493,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered a problem with strings as parameters in the parallel coordinates chart while using wandb. The issue is illustrated in a screenshot attached to the post, and the user is unsure if they made a mistake or if they need to wait for a fix from wandb.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/string-bug-in-the-parallel-coordinates-chart\/1674",
        "Challenge_link_count":6,
        "Challenge_participation_count":5,
        "Challenge_readability":22.4,
        "Challenge_reading_time":25.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":78.9828208333,
        "Challenge_title":"String bug in the parallel coordinates chart",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":186.0,
        "Challenge_word_count":109,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/chrismartin\">@chrismartin<\/a>,<\/p>\n<p>This issue has been fixed. Parallel Coordinate charts  with strings should behave as expected now.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":2.61,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":21.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.7009416667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I used this code to create a confusion matrix:<\/p>\n<pre><code class=\"lang-auto\"># confusion matrix\n        wandb.log({\"confusion-matrix-test\": wandb.plot.confusion_matrix(\n            probs=None,\n            y_true=all_gt, preds=all_pre,\n            class_names=classes_names)})\n<\/code><\/pre>\n<p>However, Wanda\u2019s website only shows a table instead of the confusion matrix. This is a screenshot from the issue:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046.png\" data-download-href=\"\/uploads\/short-url\/8UGiFwpsOZ6Pivp7qmFXgL1OuvI.png?dl=1\" title=\"Screenshot from 2022-01-09 20-58-35\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_690x249.png\" alt=\"Screenshot from 2022-01-09 20-58-35\" data-base62-sha1=\"8UGiFwpsOZ6Pivp7qmFXgL1OuvI\" width=\"690\" height=\"249\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_690x249.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_1035x373.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_1380x498.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot from 2022-01-09 20-58-35<\/span><span class=\"informations\">1741\u00d7629 29.6 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Challenge_closed_time":1641809553312,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641781829922,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with Wandb.plot.confusion_matrix() function as it only displays a table instead of a confusion matrix on Wanda's website. The user has shared a screenshot of the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-plot-confusion-matrix-just-show-a-table\/1744",
        "Challenge_link_count":6,
        "Challenge_participation_count":2,
        "Challenge_readability":27.3,
        "Challenge_reading_time":26.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":7.7009416667,
        "Challenge_title":"Wandb.plot.confusion_matrix() just show a Table!",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":689.0,
        "Challenge_word_count":92,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<aside class=\"quote no-group\" data-username=\"fdaliran\" data-post=\"1\" data-topic=\"1744\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/f\/73ab20\/40.png\" class=\"avatar\"> fdaliran:<\/div>\n<blockquote>\n<pre><code class=\"lang-auto\">        wandb.log({\"confusion-matrix-test\": wandb.plot.confusion_matrix(\n            probs=None,\n            y_true=all_gt, preds=all_pre,\n            class_names=classes_names)})\n<\/code><\/pre>\n<\/blockquote>\n<\/aside>\n<p>If you click the section called \u201cCustom Charts\u201d above the Table, it\u2019ll show the line plot that you\u2019ve logged.<\/p>\n<p>Logging the Table also is expected behaviour because this will allow users to interactively explore the logged data in a W&amp;B Table after logging it.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.1,
        "Solution_reading_time":10.22,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":73.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1618467374027,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":522.1835613889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a Tabular Dataset using Azure ML python API. Data under question is a bunch of parquet files (~10K parquet files each of size of 330 KB) residing in Azure Data Lake Gen 2 spread across multiple partitions. When I try to load the dataset using the API <code>TabularDataset.to_pandas_dataframe()<\/code>, it continues forever (hangs), if there are empty parquet files included in the Dataset. If the tabular dataset doesn't include those empty parquet files, <code>TabularDataset.to_pandas_dataframe()<\/code> completes within few minutes.<\/p>\n<p>By empty parquet file, I mean that the if I read the individual parquet file using pandas (pd.read_parquet()), it results in an empty DF (df.empty == True).<\/p>\n<p>I discovered the root cause while working on another issue mentioned <code>[here][1]<\/code>.<\/p>\n<p><strong>My question is how can make <code>TabularDataset.to_pandas_dataframe()<\/code> work even when there are empty parquet files?<\/strong><\/p>\n<p><strong>Update<\/strong>\nThe issue has been fixed in the following version:<\/p>\n<ul>\n<li>azureml-dataprep : 3.0.1<\/li>\n<li>azureml-core :  1.40.0<\/li>\n<\/ul>",
        "Challenge_closed_time":1646432724768,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644552863947,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with AzureML's TabularDataset API when trying to load a dataset containing empty parquet files. The API hangs indefinitely when attempting to load the dataset using the to_pandas_dataframe() function. The user is seeking a solution to make the function work even when empty parquet files are included in the dataset. The issue has been resolved in the latest versions of azureml-dataprep and azureml-core.",
        "Challenge_last_edit_time":1648643447772,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71075255",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.8,
        "Challenge_reading_time":15.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":522.1835613889,
        "Challenge_title":"AzureML: TabularDataset.to_pandas_dataframe() hangs when parquet file is empty",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":300.0,
        "Challenge_word_count":156,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1280505139752,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, India",
        "Poster_reputation_count":4265.0,
        "Poster_view_count":403.0,
        "Solution_body":"<p>Thanks for reporting it.\nThis is a bug in handling of the parquet files with columns but empty row set. This has been fixed already and will be included in next release.<\/p>\n<p>I could not repro the hang on multiple files, though, so if you could provide more info on that would be nice.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.0,
        "Solution_reading_time":3.54,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.0585394444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi is there a way for Azure Machine Learning to be able to perform analytics using data from an on premise SQL Server?    <\/p>\n<p>Only found the below article which is for Azure Machine Learning Studio (classic):    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/studio\/use-data-from-an-on-premises-sql-server\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/studio\/use-data-from-an-on-premises-sql-server<\/a>    <\/p>\n<p>Thanks.    <\/p>",
        "Challenge_closed_time":1592900268572,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592874857830,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information on whether Azure Machine Learning can perform analytics using data from an on-premise SQL Server. They have only found an article related to Azure Machine Learning Studio (classic) and are looking for further guidance.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/38894\/azure-machine-learning-with-on-premise-sql-server",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":11.7,
        "Challenge_reading_time":6.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":7.0585394444,
        "Challenge_title":"Azure Machine Learning with on premise SQL Server",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":50,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a>@conrad<\/a> Here is the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-register-datasets#access-datasets-in-your-script\">link<\/a> to connect with the Azure SQL server.  <br \/>\n<a href=\"https:\/\/stackoverflow.com\/questions\/61806350\/database-communication-link-error-occurded-on-azure-ml-service-used-azure-sql-s\/61950481#61950481\">https:\/\/stackoverflow.com\/questions\/61806350\/database-communication-link-error-occurded-on-azure-ml-service-used-azure-sql-s\/61950481#61950481<\/a><\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":43.1,
        "Solution_reading_time":7.36,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1429402428283,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":76.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":36.4623708333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>\u203b I used google translation, if you have any question, let me know!<\/p>\n\n<p>I am trying to run python script with huge 4 data, using sagemaker processing. And my current situation are as follows:<\/p>\n\n<ul>\n<li>can run this script with 3 data<\/li>\n<li>can't run the script with only 1 data (the biggest, the same structure with others)<\/li>\n<li>as for all of 4 data, the script has finished (so, I suspected this error in S3, ie. when copying sagemaker result to S3)<\/li>\n<\/ul>\n\n<p>The error I got is this InternalServerError.<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"sagemaker_train_and_predict.py\", line 56, in &lt;module&gt;\n    outputs=outputs\n  File \"{xxx}\/sagemaker_constructor.py\", line 39, in run\n    outputs=outputs\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/processing.py\", line 408, in run\n    self.latest_job.wait(logs=logs)\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/processing.py\", line 723, in wait\n    self.sagemaker_session.logs_for_processing_job(self.job_name, wait=True)\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/session.py\", line 3111, in logs_for_processing_job\n    self._check_job_status(job_name, description, \"ProcessingJobStatus\")\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/session.py\", line 2615, in _check_job_status\n    actual_status=status,\nsagemaker.exceptions.UnexpectedStatusException: Error for Processing job sagemaker-vm-train-and-predict-2020-04-12-04-15-40-655: Failed. Reason: InternalServerError: We encountered an internal error.  Please try again.\n<\/code><\/pre>",
        "Challenge_closed_time":1586886613192,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586755348657,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an InternalServerError while trying to save the result on S3 from sagemaker processing. The user is trying to run a python script with 4 data, but can only run it with 3 data and can't run it with only 1 data. The error occurred while copying sagemaker result to S3.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61181955",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":22.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":36.4623708333,
        "Challenge_title":"Is there any limits of saving result on S3 from sagemaker Processing?",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":95.0,
        "Challenge_word_count":168,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1586754432800,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>There may be some issue transferring the output data to S3 if the output is generated at a high rate and size is too large. <\/p>\n\n<p>You can 1) try to slow down writing the output a bit or 2) call S3 from your algorithm container to upload the output directly using boto client (<a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html<\/a>).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.7,
        "Solution_reading_time":6.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":58.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":51.0787569444,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hey everyone,<br>\nLet\u2019s say that I have a dataset with 50000 samples and I am training my model for 10 epochs. Now, in each epoch, I am recording the <em>per sample loss<\/em> (i.e. loss of each sample - Not the average loss of all samples). This means that there are 50000 loss values per epoch. I want to log these values for <em>each epoch<\/em>, so that I can later perform some analysis on how the loss values for the samples change as training progresses (And, if possible, observe the loss values of a particular sample across epochs). For reference, <a href=\"https:\/\/proceedings.neurips.cc\/paper\/2020\/file\/62000dee5a05a6a71de3a6127a68778a-Paper.pdf\" rel=\"noopener nofollow ugc\">this<\/a> paper tracks such  statistics. Here are two ways I can think of doing this -<\/p>\n<ul>\n<li>A simple way to do this is to update the values in a 50000x10 array, then log the array as a table at the end of training (I would obviously need to track which indices belong to which samples). However, I need to wait for the training to end in this scenario.<\/li>\n<li>I can also log each sample\u2019s statistic with wandb.log (Maybe put them under \u201csample_statistics\/\u201d to pull them more easily). This ensures that the metrics are logged as and when they are observed, however, I am not sure if this is the most optimal solution.<\/li>\n<\/ul>\n<p>Is there any other way in which I can do this so that I can analyse the resulting data effectively? Open to all suggestions!<br>\nThank you!<\/p>",
        "Challenge_closed_time":1656718364004,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656534480479,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to record per sample loss for each epoch while training a model on a dataset with 50,000 samples for 10 epochs. They are considering two options - updating values in a 50,000x10 array and logging it as a table at the end of training or logging each sample's statistic with wandb.log. The user is open to suggestions for an optimal solution to analyze the resulting data effectively.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/logging-metrics-for-each-sample-per-epoch\/2678",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":7.0,
        "Challenge_reading_time":18.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":51.0787569444,
        "Challenge_title":"Logging Metrics for each sample per epoch",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":267.0,
        "Challenge_word_count":248,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/tataganesh\">@tataganesh<\/a> ,<\/p>\n<p>Thank you for writing in with your question.   Ideally what would be best in your case here is to create an Empty table and log per sample loss values per epoch and be able to see your data live in the UI. However, we currently,  don\u2019t support adding new rows to existing tables that you\u2019ve already logged. We are working on adding this functionality.<\/p>\n<p>In the meantime here are two approaches<\/p>\n<ol>\n<li>Keep the wandb.Table locally holding all the data in memory and logging it once.<\/li>\n<li>Keep logging the same table at each step, and just add new rows to it. The final table you log will have all the rows you want, and you\u2019ll be able to see the latest table logged in the UI. This would be risky if you have large table sizes.<\/li>\n<\/ol>\n<p>Please Note: If you were to look through our docs and come across the<a href=\"https:\/\/docs.wandb.ai\/guides\/data-vis\/log-tables#add-data\"> Add Data Incrementally<\/a> to  Tables doc, this functionality is currently broken and we are working on an active fix.  There is github issues thread <a href=\"https:\/\/github.com\/wandb\/client\/issues\/2981\" rel=\"noopener nofollow ugc\">here<\/a> where community members have posted workarounds for this, you may find it helpful.<\/p>\n<p>Please let me know if you have any questions.<\/p>\n<p>Regards,<\/p>\n<p>Mohammad<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.6,
        "Solution_reading_time":17.08,
        "Solution_score_count":null,
        "Solution_sentence_count":15.0,
        "Solution_word_count":210.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1398891777052,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA",
        "Answerer_reputation_count":1359.0,
        "Answerer_view_count":228.0,
        "Challenge_adjusted_solved_time":20.2441541667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Duplicating: <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/6560c2d6-9836-41a1-8076-caf0d514222a\/azure-machine-learning-reader-table-storage?forum=MachineLearning\" rel=\"nofollow\">https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/6560c2d6-9836-41a1-8076-caf0d514222a\/azure-machine-learning-reader-table-storage?forum=MachineLearning<\/a><\/p>\n\n<p>I currently have a table storage setup which is constantly performing insertions. There is approximately 260 million rows in the table storage. <\/p>\n\n<p>I have set up two machine learning experiments to use a 'Reader' to read the data from the 'Azure Table'. <\/p>\n\n<p>Experiment 1 is set to read all the rows to train the model.<\/p>\n\n<p>Experiment 2 is set to read only the top 1,000 rows to train the model.<\/p>\n\n<p>Experiment 1 has been running for over 5 hours with no results.<\/p>\n\n<p>Experiment 2 has been running for over 1 hour with no results.<\/p>\n\n<p>It is stuck on the 'Reader' process.<\/p>\n\n<p>I do not understand why experiment 2 is taking so long. I know I have set this up right as I tested the 'Reader's with another table storage. Thanks in advance for any help\/suggestions.<\/p>",
        "Challenge_closed_time":1455137464832,
        "Challenge_comment_count":0,
        "Challenge_created_time":1455064585877,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing issues with Azure Machine Learning Reader and Table Storage. They have set up two experiments to read data from Azure Table, but Experiment 1 has been running for over 5 hours and Experiment 2 has been running for over 1 hour with no results. The user is unsure why Experiment 2 is taking so long and is seeking help and suggestions.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35304901",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":15.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":20.2441541667,
        "Challenge_title":"Azure Machine Learning Reader + Table Storage",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":104.0,
        "Challenge_word_count":146,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1373050450247,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":498.0,
        "Poster_view_count":47.0,
        "Solution_body":"<p>A lot of this will probably depend on the design of your tables. Table Storage is a key \/ value store (think of it as a dictionary). It has some capabilities for scanning within a partition and across partitions - but the latencies will differ greatly. Ideally if you want to query 1000 rows they should be localized within a partition. See <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/storage-table-design-guide\/\" rel=\"nofollow\">Table Design Guide<\/a> and <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/storage-performance-checklist\/\" rel=\"nofollow\">Perf and Scalability Checklist<\/a> for full details.  <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.3,
        "Solution_reading_time":8.43,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":76.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.2541636111,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I have a few questions regarding the hyperparameter sweeps from Python.<br>\nI am wanting to essentially start a few tmux sessions on my server, and connect them all to the same sweep agent, but no keyword in the sweep_config (that i have found) allow me to connect to a specific sweep ID, and rather just a sweep name that doesnt connect to the same sweep, but just makes multiple sweeps of the same name.  If this possible or strongly advised against due to computational usage or similar?<\/p>\n<p>Furthermore, sweeps take up a great deal of storage requirements due to saving all the models, is it possible to store the model file from the best model only, while keeping the statistics from all the models for plots and interpretation? This would allow me to keep the great information gathered from sweeps, while not taking up 100+ GB from a single sweep.<\/p>\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1641593349991,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641567235002,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in connecting to a specific sweep ID from Python and is only able to connect to a sweep name that creates multiple sweeps of the same name. They are also concerned about the storage requirements of sweeps and want to know if it is possible to store only the model file from the best model while keeping the statistics from all the models for plots and interpretation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/connecting-to-existing-sweep-from-python\/1721",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":12.3,
        "Challenge_reading_time":11.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":7.2541636111,
        "Challenge_title":"Connecting to existing sweep from Python",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":253.0,
        "Challenge_word_count":156,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I found the issue, i was trying to create a new wandb.sweep(config, project, entity) and pass the ID into the config dictionary, but instead i just needed to take the ID directly, and just do sweep_id = sweep_id_string which worked.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.7,
        "Solution_reading_time":2.94,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":39.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1386311998172,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Remote, OR, USA",
        "Answerer_reputation_count":2153.0,
        "Answerer_view_count":205.0,
        "Challenge_adjusted_solved_time":122.6327544444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to read a <code>.csv<\/code> file stored in an s3 bucket, and I'm getting errors. I'm following the instructions <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/r_kernel\/using_r_with_amazon_sagemaker.ipynb\" rel=\"nofollow noreferrer\">here<\/a>, but either it does not work or I am making a mistake and I'm not getting what I'm doing wrong.<\/p>\n\n<p>Here's what I'm trying to do:<\/p>\n\n<pre><code># I'm working on a SageMaker notebook instance\nlibrary(reticulate)\nlibrary(tidyverse)\n\nsagemaker &lt;- import('sagemaker')\nsagemaker.session &lt;- sagemaker$Session()\n\nregion &lt;- sagemaker.session$boto_region_name\nbucket &lt;- \"my-bucket\"\nprefix &lt;- \"data\/staging\"\nbucket.path &lt;- sprintf(\"https:\/\/s3-%s.amazonaws.com\/%s\", region, bucket)\nrole &lt;- sagemaker$get_execution_role()\n\nclient &lt;- sagemaker.session$boto_session$client('s3')\nkey &lt;- sprintf(\"%s\/%s\", prefix, 'my_file.csv')\n\nmy.obj &lt;- client$get_object(Bucket=bucket, Key=key)\n\nmy.df &lt;- read_csv(my.obj$Body) # This is where it all breaks down:\n## \n## Error: `file` must be a string, raw vector or a connection.\n## Traceback:\n## \n## 1. read_csv(my.obj$Body)\n## 2. read_delimited(file, tokenizer, col_names = col_names, col_types = col_types, \n##  .     locale = locale, skip = skip, skip_empty_rows = skip_empty_rows, \n##  .     comment = comment, n_max = n_max, guess_max = guess_max, \n##  .     progress = progress)\n## 3. col_spec_standardise(data, skip = skip, skip_empty_rows = skip_empty_rows, \n##  .     comment = comment, guess_max = guess_max, col_names = col_names, \n##  .     col_types = col_types, tokenizer = tokenizer, locale = locale)\n## 4. datasource(file, skip = skip, skip_empty_rows = skip_empty_rows, \n##  .     comment = comment)\n## 5. stop(\"`file` must be a string, raw vector or a connection.\", \n##  .     call. = FALSE)\n<\/code><\/pre>\n\n<p>When working with Python, I can read a CSV file using someting like this:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>import pandas as pd\n# ... Lots of boilerplate code\nmy_data = pd.read_csv(client.get_object(Bucket=bucket, Key=key)['Body'])\n<\/code><\/pre>\n\n<p>This is very similar to what I'm trying to do in R, and it works with Python... so why does it not work on R?<\/p>\n\n<p>Can you point me in the right path?<\/p>\n\n<p><strong>Note:<\/strong> Although I could use a Python kernel for this, I'd like to stick to R, because I'm more fluent with it than with Python, at least when it comes to dataframe crunching.<\/p>",
        "Challenge_closed_time":1586562277483,
        "Challenge_comment_count":1,
        "Challenge_created_time":1586299117960,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to read a .csv file stored in an s3 bucket using tidyverse in R, following the instructions provided in a GitHub repository. However, the user is encountering an error while reading the file, and the error message suggests that the file must be a string, raw vector, or a connection. The user is seeking guidance to resolve the issue and wants to stick to R for dataframe crunching.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61090530",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":32.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":73.0998675,
        "Challenge_title":"Using tidyverse to read data from s3 bucket",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1735.0,
        "Challenge_word_count":281,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1275433277096,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Mexico",
        "Poster_reputation_count":20017.0,
        "Poster_view_count":2754.0,
        "Solution_body":"<p>I'd recommend trying the <code>aws.s3<\/code> package instead:<\/p>\n\n<p><a href=\"https:\/\/github.com\/cloudyr\/aws.s3\" rel=\"nofollow noreferrer\">https:\/\/github.com\/cloudyr\/aws.s3<\/a><\/p>\n\n<p>Pretty simple - set your env variables:<\/p>\n\n<pre><code>Sys.setenv(\"AWS_ACCESS_KEY_ID\" = \"mykey\",\n           \"AWS_SECRET_ACCESS_KEY\" = \"mysecretkey\",\n           \"AWS_DEFAULT_REGION\" = \"us-east-1\",\n           \"AWS_SESSION_TOKEN\" = \"mytoken\")\n<\/code><\/pre>\n\n<p>and then once that is out of the way:<\/p>\n\n<p><code>aws.s3::s3read_using(read.csv, object = \"s3:\/\/bucket\/folder\/data.csv\")<\/code><\/p>\n\n<p>Update: I see you're also already familiar with boto and trying to use reticulate so leaving this easy wrapper for that here:\n<a href=\"https:\/\/github.com\/cloudyr\/roto.s3\" rel=\"nofollow noreferrer\">https:\/\/github.com\/cloudyr\/roto.s3<\/a><\/p>\n\n<p>Looks like it has a great api for example the variable layout you're aiming to use:<\/p>\n\n<pre><code>download_file(\n  bucket = \"is.rud.test\", \n  key = \"mtcars.csv\", \n  filename = \"\/tmp\/mtcars-again.csv\", \n  profile_name = \"personal\"\n)\n\nread_csv(\"\/tmp\/mtcars-again.csv\")\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1586740595876,
        "Solution_link_count":4.0,
        "Solution_readability":12.2,
        "Solution_reading_time":14.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":91.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1436818579270,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Eugene, OR, USA",
        "Answerer_reputation_count":474.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":19.74615,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using a \"Split Data\" module set to recommender split to split data for training and testing a matchbox recommender. The input data is a valid user-item-rating tuple (for example, 575978 - 157381 - 3) and I've left the parameters for the recommender split as default (0s for everything), besides changing it to a .75 and .25 split. However, when this module finishes, it returns the complete, unsplit dataset for dataset1 and a completely empty (but labelled) dataset for dataset2. This also happens when doing a stratified split using the \"Split Rows\" mode. Any idea what's going on?<\/p>\n\n<p>Thanks.<\/p>\n\n<p>Edit: Including a sample of my data.<\/p>\n\n<pre><code>UserID  ItemID  Rating\n835793  165937  3\n154738  11214   3\n938459  748288  3\n819375  789768  6\n738571  98987   3\n847509  153777  3\n991757  124458  3\n968685  288070  2\n236349  8337    3\n127299  545885  3\n<\/code><\/pre>",
        "Challenge_closed_time":1529598877083,
        "Challenge_comment_count":4,
        "Challenge_created_time":1529519087767,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with the \"Split Data\" module set to recommender split, which is returning an empty dataset for dataset2 and the complete unsplit dataset for dataset1, despite changing the parameters to a .75 and .25 split. The same issue is also happening when doing a stratified split using the \"Split Rows\" mode. The user has provided a sample of their data.",
        "Challenge_last_edit_time":1529527790943,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50954802",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":7.9,
        "Challenge_reading_time":11.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":22.1636988889,
        "Challenge_title":"Recommender Split Returning Empty Dataset",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":88.0,
        "Challenge_word_count":142,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436818579270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Eugene, OR, USA",
        "Poster_reputation_count":474.0,
        "Poster_view_count":28.0,
        "Solution_body":"<p>Figured it out. In my \"Remove Duplicate Rows\" module up the chain a bit I was only removing duplicates by UserID instead of UserID <em>and<\/em> ItemID. This still left quite a bit of rows but I'm assuming it messed with the stratification. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":3.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1601920258310,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":123.8963886111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to create a Time series dataset from a folder that contains parquet files this way:<\/p>\n<ul>\n<li>timestamp=2018-01-06<\/li>\n<li>timestamp=2018-01-07<\/li>\n<\/ul>\n<p>How can I make Azure Dataset, through the GUI, recognises the timestamp partition as a date and mark my dataset as a time series dataset?<\/p>\n<p>It is supposed to be automatic, but it doesn't work.<\/p>",
        "Challenge_closed_time":1601920685456,
        "Challenge_comment_count":1,
        "Challenge_created_time":1601474658457,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a time series dataset from a folder containing parquet files with date partitions, but is having trouble getting Azure Dataset to recognize the timestamp partition as a date and mark the dataset as a time series dataset. The user is seeking guidance on how to do this through the GUI.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64139290",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.8,
        "Challenge_reading_time":6.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":123.8963886111,
        "Challenge_title":"How can I mark an Azure Dataset as a time series dataset reading from a parquet folder with date partitions?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":109.0,
        "Challenge_word_count":76,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423640080283,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lyon, France",
        "Poster_reputation_count":457.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>Thanks for reaching out to us.<\/p>\n<p>In Azure Machine Learning Studio, you would need to setup partition format similar to python SDK, as follows, assuming your data path is &quot;timeseries\/timestamp=2020-01-01\/data.parquet&quot;:\n<a href=\"https:\/\/i.stack.imgur.com\/HwYfF.png\" rel=\"nofollow noreferrer\">Set up partition format when creating time series dataset<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.1,
        "Solution_reading_time":4.88,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":42.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":1.2687819444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Suppose I have a server where many users run different experiments, possibly with different Trains Servers.<\/p>\n<p>I know about the <code>TRAINS_CONFIG_FILE<\/code> environment variable, but I wonder if this can be made more flexible in one of the following ways:<\/p>\n<ol>\n<li>Specifying the Trains config file dynamically, i.e. during runtime of the training script?<\/li>\n<li>Storing a config file in each of the training repos and specifying its path relatively to the running script path (instead of relatively to <code>~\/<\/code>)?<\/li>\n<\/ol>",
        "Challenge_closed_time":1592997291032,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592992723417,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is wondering if the Trains config file can be specified dynamically during runtime of the training script or if it can be stored in each of the training repositories and specified relatively to the running script path instead of relatively to ~\/ directory.",
        "Challenge_last_edit_time":1609531417760,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62552414",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":7.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.2687819444,
        "Challenge_title":"Can Trains config file be specified dynamically or relative to the running script path?",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":129.0,
        "Challenge_word_count":92,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1311330349880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tel Aviv",
        "Poster_reputation_count":3784.0,
        "Poster_view_count":342.0,
        "Solution_body":"<p>Disclaimer: I'm a member of Allegro Trains team<\/p>\n<ol>\n<li>Loading of the configuration is done at import time. This means that if you set the os environment before importing the package, you should be fine:<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>os.environ['TRAINS_CONFIG_FILE']='~\/repo\/trains.conf'\nfrom trains import Task\n<\/code><\/pre>\n<ol start=\"2\">\n<li>The configuration file is loaded based on the current working directory, this means that if you have <code>os.environ['TRAINS_CONFIG_FILE']='trains.conf'<\/code> the trains.conf file will be loaded from the running directory at the time the import happens (which usually is the folder where your script is executed from). This means you can have it as part of the repository, and always set the <code>TRAINS_CONFIG_FILE<\/code> to point to it.<\/li>\n<\/ol>\n<p>A few notes:<\/p>\n<ul>\n<li>What is the use case for different configuration files ?<\/li>\n<li>Notice that when running with <a href=\"https:\/\/github.com\/allegroai\/trains-agent\" rel=\"nofollow noreferrer\">trains-agent<\/a> , this method will override the configuration that the <em>trains-agent<\/em> passes to the code.<\/li>\n<\/ul>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.1,
        "Solution_reading_time":14.9,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":149.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.0180180555,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>azureml-sdk version: <code>1.0.85<\/code><\/p>\n\n<p>Calling below (as given in the Dataset UI), I get this<\/p>\n\n<pre><code>ds_split = Dataset.get_by_name(workspace, name='ret- holdout-split')\nds_split.download(target_path=dir_outputs, overwrite=True)\n<\/code><\/pre>\n\n<pre><code>UnexpectedError:\n{'errorCode': 'Microsoft.DataPrep.ErrorCodes.Unknown', 'message':\n    'The client could not finish the operation within specified timeout.',\n    'errorData': {}}\n<\/code><\/pre>\n\n<p>The <code>FileDataset<\/code> 1GB pickled file stored in blob.\n<a href=\"https:\/\/gist.github.com\/swanderz\/c608ced5f2c6a2802b7553bc9ead0762\" rel=\"nofollow noreferrer\">Here's a gist with the full traceback<\/a><\/p>",
        "Challenge_closed_time":1581436892352,
        "Challenge_comment_count":0,
        "Challenge_created_time":1581384771000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a timeout error while using the Dataset.download() function in azureml-sdk version 1.0.85. The error message indicates that the client could not finish the operation within the specified timeout. The user is trying to download a 1GB pickled file stored in blob using the FileDataset.",
        "Challenge_last_edit_time":1581436827487,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60160773",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":14.5,
        "Challenge_reading_time":9.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":14.4781533333,
        "Challenge_title":"Workaround for timeout error in Dataset.download()",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":217.0,
        "Challenge_word_count":60,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1405457120427,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seattle, WA, USA",
        "Poster_reputation_count":3359.0,
        "Poster_view_count":555.0,
        "Solution_body":"<p>Tried again this AM and it worked. let's file this under \"transient error\"<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":1.1,
        "Solution_reading_time":1.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1604747085276,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":24.4361597222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working on a CNN project and I would like to log the model.summary to neptune.ai. The intention of that is to have an idea about the model parameters while comparing different models. Any help\/tips would be much appreciated!<\/p>",
        "Challenge_closed_time":1604748950752,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604660980577,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is working on a CNN project and wants to log the model.summary to neptune.ai to compare different models and have an idea about the model parameters. They are seeking help or tips to achieve this.",
        "Challenge_last_edit_time":1660057709880,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64713492",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.1,
        "Challenge_reading_time":3.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":24.4361597222,
        "Challenge_title":"Is there a way to log the keras model summary to neptune?",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":285.0,
        "Challenge_word_count":51,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604146329127,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>You can log <code>model.summary<\/code> (assuming it's keras), like this:<\/p>\n<pre><code>neptune.init('workspace\/project')\nneptune.create_experiment()\n\nmodel = keras.Sequential(...)\nmodel.summary(print_fn=lambda x: neptune.log_text('model_summary', x))\n<\/code><\/pre>\n<p>This will log entire summary as lines of text. You can later browse it in the <em>Logs<\/em> section of the experiment. Look for tile: &quot;model_summary&quot; in this <a href=\"https:\/\/ui.neptune.ai\/o\/USERNAME\/org\/example-project\/e\/HELLO-325\/logs\" rel=\"nofollow noreferrer\">example<\/a>.<\/p>\n<p>Another option - for easier compare - is to log hyper-parameters at experiment creation, like this:<\/p>\n<pre><code># Define parameters as Python dict\nPARAMS = {'batch_size': 64,\n          'n_epochs': 100,\n          'shuffle': True,\n          'activation': 'elu'}\n\n# Pass PARAMS dict to params at experiment creation\nneptune.create_experiment(params=PARAMS)\n<\/code><\/pre>\n<p>You will have them in <em>Parameters<\/em> tab of the experiment, like in this <a href=\"https:\/\/ui.neptune.ai\/o\/USERNAME\/org\/example-project\/e\/HELLO-44\/parameters\" rel=\"nofollow noreferrer\">example<\/a>. You will be able to add each parameter as a column to the dashboard for quick compare. Look for greenish columns in this <a href=\"https:\/\/ui.neptune.ai\/o\/USERNAME\/org\/example-project\/experiments?viewId=d7f80ebe-5bfe-4d12-97c1-2b1e6184a2ed\" rel=\"nofollow noreferrer\">dashboard<\/a>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":14.0,
        "Solution_reading_time":18.48,
        "Solution_score_count":2.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":132.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.6372222222,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nWhat parquet data loading logic is known to work well to train with SageMaker on parquet? ml-io? pyarrow? any examples? That would be to train a classifier, either logistic regression, XGBoost or custom TF.",
        "Challenge_closed_time":1588843302000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1588841008000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking advice on the best parquet data loading logic to use with SageMaker for training a classifier, specifically logistic regression, XGBoost, or custom TF. They are asking for examples and recommendations for ml-io and pyarrow.",
        "Challenge_last_edit_time":1668588105088,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUCqvDUq4hSQqRT97tBUvE8Q\/training-a-classifier-on-parquet-with-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.8,
        "Challenge_reading_time":3.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.6372222222,
        "Challenge_title":"Training a classifier on parquet with SageMaker ?",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":424.0,
        "Challenge_word_count":42,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"XGBoost as a framework container (v0.90+) can read parquet for training (see example [notebook][1]).  \nThe full list of valid content types are CSV, LIBSVM, PARQUET, RECORDIO_PROTOBUF (see [source][2]) \n\nAdditionally:  \n[Uber Petastorm][3] for reading parquet into Tensorflow, Pytorch, and PySpark inputs.   \nAs XGBoost accepts numpy, you can convert from PySpark to numpy\/pandas using the mentioned PyArrow.\n\n\n  [1]: https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/caf9363c0242d0da2de7f5765e7318fd843ce4c3\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_parquet_input_training.ipynb\n  [2]: https:\/\/github.com\/aws\/sagemaker-xgboost-container\/blob\/5e778770e009ce989e288e7bbc1255556129e75b\/src\/sagemaker_xgboost_container\/data_utils.py#L40\n  [3]: https:\/\/github.com\/uber\/petastorm",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925592848,
        "Solution_link_count":3.0,
        "Solution_readability":19.1,
        "Solution_reading_time":10.59,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":61.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1425965839876,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Santa Cruz, CA",
        "Answerer_reputation_count":3256.0,
        "Answerer_view_count":164.0,
        "Challenge_adjusted_solved_time":0.8799466667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>MLflow provides a very cool tracking server, however, this server does not provide authentication or RBAC which is required for my needs.<\/p>\n<p>I would like to add my own authentication and RBAC functionality. I think one way to accomplish this is to import the MLflow WSGI application object and add some middleware layers to perform authentication \/ authorization before passing requests through to the tracking server, essentially proxying requests through my custom middleware stack.<\/p>\n<p>How do I go about doing this? I can see from <a href=\"https:\/\/fastapi.tiangolo.com\/advanced\/wsgi\/\" rel=\"nofollow noreferrer\">these docs<\/a> that I can use FastAPI to import another WSGI application and add custom middleware, but I'm not sure of a few things<\/p>\n<ol>\n<li>Where do I find the MLflow tracking server WSGI app (where can it be imported from)?<\/li>\n<li>How do I pass through the relevant arguments to the MLflow tracking server? I.e. the tracking server expects params to configure the backend storage layer, host, and port. If I just import the application object, how do I pass those parameters to it?<\/li>\n<\/ol>\n<p>edit - it looks like the Flask application can be found here <a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/server\/__init__.py#L28\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/server\/__init__.py#L28<\/a><\/p>",
        "Challenge_closed_time":1648708838088,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648703157780,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to add authentication and RBAC functionality to the MLflow tracking server, but it does not provide these features. They plan to import the MLflow WSGI application object and add middleware layers to perform authentication\/authorization before passing requests through to the tracking server. However, they are unsure of where to find the MLflow tracking server WSGI app and how to pass relevant arguments to it. The user has found the Flask application on GitHub.",
        "Challenge_last_edit_time":1648705670280,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71687131",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":10.9,
        "Challenge_reading_time":18.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":1.5778633333,
        "Challenge_title":"How to import MLflow tracking server WSGI application via Flask or FastAPI?",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":394.0,
        "Challenge_word_count":198,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1425965839876,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Santa Cruz, CA",
        "Poster_reputation_count":3256.0,
        "Poster_view_count":164.0,
        "Solution_body":"<p>This was actually very simple, below is an example using FastAPI to import and mount the MLflow WSGI application.<\/p>\n<pre><code>import os\nimport subprocess\nfrom fastapi import FastAPI\nfrom fastapi.middleware.wsgi import WSGIMiddleware\n\nfrom mlflow.server import app as mlflow_app\n\napp = FastAPI()\napp.mount(&quot;\/&quot;, WSGIMiddleware(mlflow_app))\n\nBACKEND_STORE_URI_ENV_VAR = &quot;_MLFLOW_SERVER_FILE_STORE&quot;\nARTIFACT_ROOT_ENV_VAR = &quot;_MLFLOW_SERVER_ARTIFACT_ROOT&quot;\nARTIFACTS_DESTINATION_ENV_VAR = &quot;_MLFLOW_SERVER_ARTIFACT_DESTINATION&quot;\nPROMETHEUS_EXPORTER_ENV_VAR = &quot;prometheus_multiproc_dir&quot;\nSERVE_ARTIFACTS_ENV_VAR = &quot;_MLFLOW_SERVER_SERVE_ARTIFACTS&quot;\nARTIFACTS_ONLY_ENV_VAR = &quot;_MLFLOW_SERVER_ARTIFACTS_ONLY&quot;\n\ndef parse_args():\n    a = argparse.ArgumentParser()\n    a.add_argument(&quot;--host&quot;, type=str, default=&quot;0.0.0.0&quot;)\n    a.add_argument(&quot;--port&quot;, type=str, default=&quot;5000&quot;)\n    a.add_argument(&quot;--backend-store-uri&quot;, type=str, default=&quot;sqlite:\/\/\/mlflow.db&quot;)\n    a.add_argument(&quot;--serve-artifacts&quot;, action=&quot;store_true&quot;, default=False)\n    a.add_argument(&quot;--artifacts-destination&quot;, type=str)\n    a.add_argument(&quot;--default-artifact-root&quot;, type=str)\n    a.add_argument(&quot;--gunicorn-opts&quot;, type=str, default=&quot;&quot;)\n    a.add_argument(&quot;--n-workers&quot;, type=str, default=1)\n    return a.parse_args()\n\ndef run_command(cmd, env, cwd=None):\n    cmd_env = os.environ.copy()\n    if cmd_env:\n        cmd_env.update(env)\n    child = subprocess.Popen(\n        cmd, env=cmd_env, cwd=cwd, text=True, stdin=subprocess.PIPE\n    )\n    child.communicate()\n    exit_code = child.wait()\n    if exit_code != 0:\n        raise Exception(&quot;Non-zero exitcode: %s&quot; % (exit_code))\n    return exit_code\n\ndef run_server(args):\n    env_map = dict()\n    if args.backend_store_uri:\n        env_map[BACKEND_STORE_URI_ENV_VAR] = args.backend_store_uri\n    if args.serve_artifacts:\n        env_map[SERVE_ARTIFACTS_ENV_VAR] = &quot;true&quot;\n    if args.artifacts_destination:\n        env_map[ARTIFACTS_DESTINATION_ENV_VAR] = args.artifacts_destination\n    if args.default_artifact_root:\n        env_map[ARTIFACT_ROOT_ENV_VAR] = args.default_artifact_root\n\n    print(f&quot;Envmap: {env_map}&quot;)\n\n    #opts = args.gunicorn_opts.split(&quot; &quot;) if args.gunicorn_opts else []\n    opts = args.gunicorn_opts if args.gunicorn_opts else &quot;&quot;\n\n    cmd = [\n        &quot;gunicorn&quot;, &quot;-b&quot;, f&quot;{args.host}:{args.port}&quot;, &quot;-w&quot;, f&quot;{args.n_workers}&quot;, &quot;-k&quot;, &quot;uvicorn.workers.UvicornWorker&quot;, &quot;server:app&quot;\n    ]\n    run_command(cmd, env_map)\n\ndef main():\n    args = parse_args()\n    run_server(args)\n\nif __name__ == &quot;__main__&quot;:\n    main()\n<\/code><\/pre>\n<p>Run like<\/p>\n<pre><code>python server.py --artifacts-destination s3:\/\/mlflow-mr --default-artifact-root s3:\/\/mlflow-mr --serve-artifacts\n<\/code><\/pre>\n<p>Then navigate to your browser and see the tracking server running! This allows you to insert custom FastAPI middleware in front of the tracking server<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":19.6,
        "Solution_reading_time":40.72,
        "Solution_score_count":2.0,
        "Solution_sentence_count":36.0,
        "Solution_word_count":200.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1250158552416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Challenge_adjusted_solved_time":45.0161775,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>How do I replace the values in a specific column with a particular value based on a condition in Azure ML Studio. I can do this using pandas in python as foolows:<\/p>\n\n<pre><code>df.loc[df['col_name'] &gt; 1990, 'col_name'] = 1\n<\/code><\/pre>\n\n<p>I'm trying to find a Module in Azure Machine Learning Studio that does the equivalent of this. <\/p>\n\n<p>I understand there is a replace option under the ConverToDataset module and a Replace Discrete Values module. But neither of these seems to do what I want. Is there an option to replace the values in just one column to a specific value based on a condition?<\/p>",
        "Challenge_closed_time":1557391929596,
        "Challenge_comment_count":0,
        "Challenge_created_time":1557229871357,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in replacing values in a specific column based on a condition in Azure ML Studio. They are looking for a module that can perform the equivalent of a pandas command in Python, but the available options in Azure ML Studio do not seem to meet their requirements.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56021977",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":8.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":45.0161775,
        "Challenge_title":"Replace values in a column based on a condition in Azure ML Studio",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":564.0,
        "Challenge_word_count":115,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1450260166772,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1587.0,
        "Poster_view_count":540.0,
        "Solution_body":"<p>You can use either the more general <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/apply-sql-transformation\" rel=\"nofollow noreferrer\">Apply SQL Transformation<\/a>, or the dedicated <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/clip-values\" rel=\"nofollow noreferrer\">Clip Values<\/a> module. If all else fails, there's also <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/execute-python-script\" rel=\"nofollow noreferrer\">Execute Python Script<\/a>.<\/p>\n\n<p>Personally, for your example I'd use <code>Clip Values<\/code> with <code>Clip Peaks<\/code> and <code>Upper Threshold<\/code> set. For more complex rules I'd use either <code>Apply SQL Transformation<\/code> or <code>Execute Python Script<\/code>, depending on the rules but favouring SQL :).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":17.8,
        "Solution_reading_time":11.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":71.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.6476944444,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi, \nI'm trying to run the SageMaker XGBoost Parquet example [linked here](https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_parquet_input_training.html). I followed the exact same steps but using my own data. I uploaded my data, converted it to a pandas df. The train_df shape is (15279798, 32) while the test_df shape is (150848, 32). I then converted it to parquet files and uploaded it to an S3 bucket - per example instructions. \n\nMy error is as follows:\n\n```\nFailure reason\nAlgorithmError: framework error: Traceback (most recent call last): File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_xgboost_container\/data_utils.py\", line 422, in _get_parquet_dmatrix_pipe_mode data = np.vstack(examples) File \"<__array_function__ internals>\", line 6, in vstack File \"\/miniconda3\/lib\/python3.7\/site-packages\/numpy\/core\/shape_base.py\", line 283, in vstack return _nx.concatenate(arrs, 0) File \"<__array_function__ internals>\", line 6, in concatenate ValueError: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 32 and the array at index 1 has size 9 During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_trainer.py\", line 84, in train entrypoint() File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_xgboost_container\/training.py\", line 94, in main train(framework.tr\n\n```\nBut I'm confused because the train and test are the same shape and I added no extra code. My code below:\n\n\n```\n# requires PyArrow installed\ntrain.to_parquet(\"Xgb_train.parquet\")\ntest.to_parquet(\"Xgb_test.parquet\")\n\n%%time\nsagemaker.Session().upload_data(\n    \"Xgb_train.parquet\", bucket=bucket, key_prefix=prefix + \"\/\" + \"Ptrain\"\n)\n\nsagemaker.Session().upload_data(\n    \"Xgb_test.parquet\", bucket=bucket, key_prefix=prefix + \"\/\" + \"Ptest\"\n)\n\ncontainer = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-2\")\n\n%%time\nimport time\nfrom time import gmtime, strftime\n\njob_name = \"xgboost-parquet-example-training-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nprint(\"Training job\", job_name)\n\n# Ensure that the training and validation data folders generated above are reflected in the \"InputDataConfig\" parameter below.\n\ncreate_training_params = {\n    \"AlgorithmSpecification\": {\"TrainingImage\": container, \"TrainingInputMode\": \"Pipe\"},\n    \"RoleArn\": role,\n    \"OutputDataConfig\": {\"S3OutputPath\": bucket_path + \"\/\" + prefix + \"\/single-xgboost\"},\n    \"ResourceConfig\": {\"InstanceCount\": 1, \"InstanceType\": \"ml.m5.2xlarge\", \"VolumeSizeInGB\": 20},\n    \"TrainingJobName\": job_name,\n    \"HyperParameters\": {\n        \"max_depth\": \"5\",\n        \"eta\": \"0.2\",\n        \"gamma\": \"4\",\n        \"min_child_weight\": \"6\",\n        \"subsample\": \"0.7\",\n        \"objective\": \"reg:linear\",\n        \"num_round\": \"10\",\n        \"verbosity\": \"2\",\n    },\n    \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 3600},\n    \"InputDataConfig\": [\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": bucket_path + \"\/\" + prefix + \"\/Ptrain\",\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                }\n            },\n            \"ContentType\": \"application\/x-parquet\",\n            \"CompressionType\": \"None\",\n        },\n        {\n            \"ChannelName\": \"validation\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": bucket_path + \"\/\" + prefix + \"\/Ptest\",\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                }\n            },\n            \"ContentType\": \"application\/x-parquet\",\n            \"CompressionType\": \"None\",\n        },\n    ],\n}\n\n\nclient = boto3.client(\"sagemaker\", region_name=region)\nclient.create_training_job(**create_training_params)\nprint(client)\nstatus = client.describe_training_job(TrainingJobName=job_name)[\"TrainingJobStatus\"]\nprint(status)\nwhile status != \"Completed\" and status != \"Failed\":\n    time.sleep(60)\n    status = client.describe_training_job(TrainingJobName=job_name)[\"TrainingJobStatus\"]\n    print(status)\n```",
        "Challenge_closed_time":1648149098276,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648146766576,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while running the SageMaker XGBoost Parquet example code. The error message indicates a concatenation issue due to mismatched input array dimensions. The user has followed the example instructions and uploaded their own data, which has the same shape for both train and test data. The user is unsure why the error is occurring and has provided their code for reference.",
        "Challenge_last_edit_time":1668604200244,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUqqbIbodsT42efRxxi1FLzw\/sagemaker-xgboost-parquet-example-code-fails-and-errors-out-bug",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":16.2,
        "Challenge_reading_time":51.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":29,
        "Challenge_solved_time":0.6476944444,
        "Challenge_title":"SageMaker XGBoost Parquet Example Code Fails and Errors out. Bug?",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":306.0,
        "Challenge_word_count":346,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I just changed my bucket name and file names. It worked now.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1648149098276,
        "Solution_link_count":0.0,
        "Solution_readability":-0.4,
        "Solution_reading_time":0.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":12.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1613062428296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":141.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":1.9716805556,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am looking at Kedro Library as my team are looking into using it for our data pipeline.<\/p>\n<p>While going to the offical tutorial - Spaceflight.<\/p>\n<p>I came across this function:<\/p>\n<pre><code>def preprocess_companies(companies: pd.DataFrame) -&gt; pd.DataFrame:\n&quot;&quot;&quot;Preprocess the data for companies.\n\n    Args:\n        companies: Source data.\n    Returns:\n        Preprocessed data.\n\n&quot;&quot;&quot;\n\ncompanies[&quot;iata_approved&quot;] = companies[&quot;iata_approved&quot;].apply(_is_true)\n\ncompanies[&quot;company_rating&quot;] = companies[&quot;company_rating&quot;].apply(_parse_percentage)\n\nreturn companies\n<\/code><\/pre>\n<ul>\n<li>companies is the name of the csv file containing the data<\/li>\n<\/ul>\n<p>Looking at the function, my assumption is that <code>(companies: pd.Dafarame)<\/code> is the shorthand to read the &quot;companies&quot; dataset as a dataframe. If so, I do not understand what does <code>-&gt; pd.Dataframe<\/code> at the end means<\/p>\n<p>I tried looking at python documentation regarding such style of code but I did not managed to find any<\/p>\n<p>Much help is appreciated to assist me in understanding this.<\/p>\n<p>Thank you<\/p>",
        "Challenge_closed_time":1613062868172,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613060367020,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to understand the meaning of the function signature in Kedro Tutorial, specifically the part that reads \"companies: pd.DataFrame -> pd.DataFrame\". They assume that \"companies\" is the name of the CSV file containing the data and that the code reads the dataset as a dataframe, but they do not understand the \"-> pd.DataFrame\" part. They are seeking assistance in understanding this.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66158536",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.4,
        "Challenge_reading_time":15.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":0.6947644444,
        "Challenge_title":"What does this python function signature means in Kedro Tutorial?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":135.0,
        "Challenge_word_count":143,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1504515330836,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":5.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>The <code>-&gt;<\/code> notation is <a href=\"https:\/\/docs.python.org\/3\/library\/typing.html\" rel=\"nofollow noreferrer\">type hinting<\/a>, as is the <code>:<\/code> part in the <code>companies: pd.DataFrame<\/code> function definition. This is not essential to do in Python but many people like to include it. The function definition would work exactly the same if it didn't contain this but instead read:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def preprocess_companies(companies):\n<\/code><\/pre>\n<p>This is a general Python thing rather than anything kedro-specific.<\/p>\n<p>The way that kedro registers <code>companies<\/code> as a kedro dataset is completely separate from this function definition and is done through the catalog.yml file:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>companies:\n  type: pandas.CSVDataSet\n  filepath: data\/01_raw\/companies.csv\n<\/code><\/pre>\n<p>There will then a <em>node<\/em> defined (in pipeline.py) to specify that the <code>preprocess_companies<\/code> function should take as input the kedro dataset <code>companies<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>node(\n    func=preprocess_companies,\n    inputs=&quot;companies&quot;,  # THIS LINE REFERS TO THE DATASET NAME\n    outputs=&quot;preprocessed_companies&quot;,\n    name=&quot;preprocessing_companies&quot;,\n),\n<\/code><\/pre>\n<p>In theory the name of the parameter in the function itself could be completely different, e.g.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def preprocess_companies(anything_you_want):\n<\/code><\/pre>\n<p>... although it is very common to give it the same name as the dataset.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1613067465070,
        "Solution_link_count":1.0,
        "Solution_readability":13.6,
        "Solution_reading_time":21.26,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":171.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1538757797860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Dallas, TX, USA",
        "Answerer_reputation_count":5671.0,
        "Answerer_view_count":629.0,
        "Challenge_adjusted_solved_time":4.7034825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm reading a file from my S3 bucket in a notebook in sagemaker studio (same account) using the following code:<\/p>\n<pre><code>dataset_path_in_h5=&quot;\/Mode1\/SingleFault\/SimulationCompleted\/IDV2\/Mode1_IDVInfo_2_100\/Run1\/processdata&quot;\ns3 = s3fs.S3FileSystem()\nh5_file = h5py.File(s3.open(s3url,'rb'), 'r')\ndata = h5_file.get(dataset_path_in_h5)\n<\/code><\/pre>\n<p>But I don't know what actually append behind the scene, does the whole h5 file is being transferred  ? that's seems unlikely as the code is executed quite fast while the whole file is 20GB. Or is just the dataset in dataset_path_in_h5 is transferred ?\nI suppose that if the whole file is transferred at each call it could cost me a lot.<\/p>",
        "Challenge_closed_time":1662041978820,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662025046283,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to read an HDF5 file from their S3 bucket in a Sagemaker Studio notebook using Python code. They are unsure if the entire 20GB file is being transferred or just the specific dataset they are accessing. The user is concerned about the potential cost of transferring the entire file each time the code is executed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73567221",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":9.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":4.7034825,
        "Challenge_title":"reading hdf5 file from s3 to sagemaker, is the whole file transferred?",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":18.0,
        "Challenge_word_count":101,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1576136255052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":795.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p>When you open the file, a file object is created. It has a tiny memory footprint. The dataset values aren't read into memory until you access them.<\/p>\n<p>You are returning <code>data<\/code> as a NumPy array. That loads the entire dataset into memory. (NOTE: the <code>.get()<\/code> method you are using is deprecated. Current syntax is provided in the example.)<\/p>\n<p>As an alternative to returning an array, you can create a dataset object (which also has a small memory foorprint). When you do, the data is read into memory as you need it. Dataset objects behave like NumPy arrays. (Use of a dataset object vs NumPy array depends on downstream usage. Frequently you don't need an array, but sometimes they are required.) Also, if chunked I\/O was enabled when the dataset was created, datasets are read in chunks.<\/p>\n<p>Differences shown below. Note, I used Python's file context manager to open the file. It avoids problems if the file isn't closed properly (you forget or the program exits prematurely).<\/p>\n<pre><code>dataset_path_in_h5=&quot;\/Mode1\/SingleFault\/SimulationCompleted\/IDV2\/Mode1_IDVInfo_2_100\/Run1\/processdata&quot;\ns3 = s3fs.S3FileSystem()\nwith h5py.File(s3.open(s3url,'rb'), 'r') as h5_file:\n     # your way to get a numpy array -- .get() is depreciated:\n     data = h5_file.get(dataset_path_in_h5)\n     # this is the preferred syntax to return an array:\n     data_arr = h5_file[dataset_path_in_h5][()]\n     # this returns a h5py dataset object:\n     data_ds = h5_file[dataset_path_in_h5]  # deleted [()] \n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":19.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":207.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":525.8622483334,
        "Challenge_answer_count":7,
        "Challenge_body":"<p>I have a guild operation <code>main<\/code> that runs 3 steps which are other operations: <code>impute<\/code>, <code>evaluate<\/code>, and <code>predict<\/code>. The latter two require on the <code>impute<\/code> operation (specifically a model checkpoint and some data output).<br>\n(<a href=\"https:\/\/github.com\/davzaman\/autopopulus\/blob\/dev\/guild.yml\" rel=\"noopener nofollow ugc\">guild.yml<\/a> file for reference)<\/p>\n<ol>\n<li>When one of the steps fails (e.g. <code>evaluate<\/code>), the <code>main<\/code> op shows error and so does <code>evaluate<\/code>. If I fix the error in the code and restart the run with something like <code>for hash in $(guild select --operation evaluate --error --all); do guild run -y --background --restart $hash --force-sourcecode; done<\/code>,  then the <code>evaluate<\/code> op fixes to completed, but the <code>main<\/code> operation does not. It doesn\u2019t seem very possible to update it, but it is slightly unclean and annoying to keep track of what broke and what is fixed. I end up with something like:<\/li>\n<\/ol>\n<pre><code class=\"lang-plaintext\">[71:ec03c916]   evaluate  2023-02-20 14:43:57  completed  dvae myexperiment\n[72:957ecb30]   evaluate  2023-02-20 14:43:56  completed  dvae myexperiment\n[73:19493e6b]   evaluate  2023-02-20 14:43:56  completed  dvae myexperiment\n...\n[127:fe72a7ff]  predict   2023-02-18 20:58:56  completed  dvae \n[128:617bc8fd]  impute    2023-02-18 20:26:16  completed  dvae myexperiment\n[129:2b155ff0]  main      2023-02-18 20:26:14  error      dvae \n[130:39125144]  predict   2023-02-18 20:21:08  completed  dvae \n[131:5c4ed46a]  impute    2023-02-18 19:45:25  completed  dvae myexperiment\n[132:c542fcbe]  main      2023-02-18 19:45:24  error      dvae \n<\/code><\/pre>\n<p>It said <code>error<\/code> for <code>main<\/code> but it\u2019s really been fixed sine the <code>evaluate<\/code> op was fixed.<br>\nAnother issue is also what files are stored under each op which leads me to the next point, where ill use <code>run 132<\/code> as an example:<\/p>\n<ol start=\"2\">\n<li>If I look at what is stored under the <code>main<\/code> op I see:<\/li>\n<\/ol>\n<pre><code class=\"lang-shell\">me@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/c542fcbe24ab4a86b1ea0e33fabd839a$ ls\nevaluate  impute  options.yml  predict\n<\/code><\/pre>\n<p>If I drill into the directories I see:<\/p>\n<pre><code class=\"lang-plaintext\">me@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/c542fcbe24ab4a86b1ea0e33fabd839a$ cd evaluate\nme@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/c542fcbe24ab4a86b1ea0e33fabd839a\/evaluate$ ls\nF.O.  options.yml  serialized_models\n<\/code><\/pre>\n<p>If <code>evaluate<\/code> fails and I rerun it, does that mean that the <code>evaluate<\/code>folder will be updated too (is it a symlink)? There seems to be some redundancy too which leads me to:<\/p>\n<ol start=\"3\">\n<li>If I look at the output of the substeps <code>impute<\/code> and <code>predict<\/code> I see:<\/li>\n<\/ol>\n<pre><code class=\"lang-plaintext\"># impute op\nme@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/5c4ed46a12e145158b2351621ee81345\/serialized_models$ ls\nAEDitto_STATIC.pt  imputed_data.pkl  STATIC_test_dataloader.pt\n# predict op\nme@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/391251446fe041048d93d80deda6ac8a\/serialized_models$ ls\nAEDitto_STATIC.pt  imputed_data.pkl  STATIC_test_dataloader.pt\n<\/code><\/pre>\n<p>I also see<\/p>\n<pre><code class=\"lang-plaintext\"># impute op\nme@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/5c4ed46a12e145158b2351621ee81345$ ls F.O.\/0.33\/MNAR\\(G\\)\/dvae\/lightning_logs\/version_0\/\nevents.out.tfevents.1676780435.lambda2.6521.0\nevents.out.tfevents.1676780445.lambda2.6521.1\nevents.out.tfevents.1676780453.lambda2.6521.2\nhparams.yaml\n# predict op\nme@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/391251446fe041048d93d80deda6ac8a$ ls F.O.\/0.33\/MNAR\\(G\\)\/dvae\/lightning_logs\/version_0\/\nevents.out.tfevents.1676780435.lambda2.6521.0\nevents.out.tfevents.1676780445.lambda2.6521.1\nevents.out.tfevents.1676780453.lambda2.6521.2\nhparams.yaml\n<\/code><\/pre>\n<p>It looks like it copies over everything from the <code>impute<\/code> op top the parents: <code>main<\/code>, and dependent steps: <code>predict<\/code>, and <code>evaluate<\/code>. This is a lot of redundancy especially for expensive\/large models and artifacts. This is making me run out of space on my machine.<\/p>\n<p>My questions are<br>\na) How do I avoid redundancy in stored artifacts between parent and child steps like <code>main<\/code> having substeps.<br>\nb) How do I avoid redundancy amongst sibling runs where one may be dependent on another? While <code>evaluate<\/code> relies on the artifacts from <code>impute<\/code> I don\u2019t want it to store all the artifacts all over again (including the model checkpoints, data, and the logging files), I just want <code>evaluate<\/code> to use the checkpointed data and model. <a href=\"https:\/\/my.guild.ai\/t\/guild-file-cheatsheet\/192#required-operation-files-14\">I know there\u2019s a <code>select:<\/code> option<\/a> but it seems to be regex, making it complicated to select the checkpointed model AND data. Also even if that solves excluding the logged files, I don\u2019t want to copy over the files it relies on to the final logged artifacts.<\/p>",
        "Challenge_closed_time":1678831800198,
        "Challenge_comment_count":0,
        "Challenge_created_time":1676938696104,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing confusion regarding multistep operations, restarting substeps, and copied files. When one of the steps fails, the main operation shows an error and fixing the error in the code and restarting the run does not update the main operation. The user is also facing redundancy in stored artifacts between parent and child steps and amongst sibling runs, which is making them run out of space on their machine. They are seeking solutions to avoid redundancy in stored artifacts and to avoid copying over files that the final logged artifacts rely on.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/my.guild.ai\/t\/confusion-on-multistep-operations-restarting-substeps-and-copied-files\/998",
        "Challenge_link_count":2,
        "Challenge_participation_count":7,
        "Challenge_readability":13.3,
        "Challenge_reading_time":67.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":525.8622483334,
        "Challenge_title":"Confusion on multistep operations, restarting substeps, and copied files?",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":128.0,
        "Challenge_word_count":550,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a class=\"mention\" href=\"\/u\/davzaman\">@davzaman<\/a> It looks like there was a regression and Guild is indeed <em>copying<\/em> resolved operation dependency files. This is not the intended behavior and we\u2019ll fix that ASAP.<\/p>\n<p>As a workaround, avoid copying files by adding <code>target-type<\/code> to your dependency def like this:<\/p>\n<pre><code class=\"lang-yaml\">upstream: {}\n\ndownstream:\n  requires:\n    - operation: upstream\n      target-type: link  # tells Guild to link to the resolved files, not copy\n<\/code><\/pre>\n<p>The <code>downstream<\/code> operation is any operation that requires an upstream run.<\/p>\n<p>Sorry about that! This will make a big difference in disk space for you. We\u2019ll post here when the fix is applied, after which you can remove the explicit <code>target-type<\/code> in your dependencies.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.5,
        "Solution_reading_time":10.34,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":108.0,
        "Tool":"Guild AI"
    },
    {
        "Answerer_created_time":1348082104976,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":9564.0,
        "Answerer_view_count":894.0,
        "Challenge_adjusted_solved_time":3234.1247702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have hundreds of CSV files that I want to process similarly. For simplicity, we can assume that they are all in <code>.\/data\/01_raw\/<\/code> (like <code>.\/data\/01_raw\/1.csv<\/code>, <code>.\/data\/02_raw\/2.csv<\/code>) etc. I would much rather not give each file a different name and keep track of them individually when building my pipeline. I would like to know if there is any way to read all of them in bulk by specifying something in the <code>catalog.yml<\/code> file?<\/p>",
        "Challenge_closed_time":1588804881827,
        "Challenge_comment_count":0,
        "Challenge_created_time":1588799145203,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user wants to know if there is a way to add hundreds of CSV files to the catalog in Kedro without giving each file a different name and keeping track of them individually when building the pipeline. They are looking for a way to read all the files in bulk by specifying something in the catalog.yml file.",
        "Challenge_last_edit_time":1588803043230,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61645397",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.3,
        "Challenge_reading_time":6.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.5935066667,
        "Challenge_title":"How do I add many CSV files to the catalog in Kedro?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":806.0,
        "Challenge_word_count":83,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1453233461910,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":299.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>You are looking for <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/02_kedro_io.html#partitioned-dataset\" rel=\"nofollow noreferrer\">PartitionedDataSet<\/a>. In your example, the <code>catalog.yml<\/code> might look like this:<\/p>\n<pre><code>my_partitioned_dataset:\n  type: &quot;PartitionedDataSet&quot;\n  path: &quot;data\/01_raw&quot;\n  dataset: &quot;pandas.CSVDataSet&quot;\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1600445892403,
        "Solution_link_count":1.0,
        "Solution_readability":21.1,
        "Solution_reading_time":5.42,
        "Solution_score_count":8.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":25.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":20.7202602778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hello, I am using Azure machine learning studio, which has been changed since last year.    <\/p>\n<p>Previously, the Azure Machine Learning designer function of the Classic version could be applied to Excel by importing the App function to Excel and downloading it. Like the picture below!    <\/p>\n<p>Has the function that can be linked to Excel be lost in this Azure Machine Learning Studio? it's very difficult....    <\/p>\n<p>If there is a function, can you tell me how to do it?    <\/p>\n<p>And I wonder if there are any lectures that explain the new azure machine learning designer features.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/178194-azure2.png?platform=QnA\" alt=\"178194-azure2.png\" \/>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/178202-azure1.png?platform=QnA\" alt=\"178202-azure1.png\" \/>    <\/p>",
        "Challenge_closed_time":1646044643350,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645970050413,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in using Azure machine learning studio as the function that could be linked to Excel seems to have been lost in the new version. They are seeking guidance on how to connect the designer function with Excel and also looking for lectures that explain the new features of Azure machine learning designer.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/752248\/azure-machine-learning-studio-for-designer-functio",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":9.0,
        "Challenge_reading_time":11.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":20.7202602778,
        "Challenge_title":"Azure machine learning studio for designer function connected with excel?",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":117,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=3b41fd96-f090-42a6-b421-e5af3d214f5f\">@Robin Jang  <\/a> The designer studio does not have an add-in for excel. This is only available with the classic version of Azure Machine Learning.     <br \/>\nIf you are new to Azure machine learning designer I would recommend to start with the tutorials from Microsoft Learn available <a href=\"https:\/\/learn.microsoft.com\/en-us\/learn\/browse\/?filter-products=machine&amp;products=azure-machine-learning\">here<\/a>.    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":14.2,
        "Solution_reading_time":11.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":77.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1351154914716,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2564.0,
        "Answerer_view_count":451.0,
        "Challenge_adjusted_solved_time":50.1200916667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created a Vetex AI dataset in <code>us-central1<\/code> and confirm it exists using:<\/p>\n<pre><code>vertex_ai.TabularDataset.list()\n<\/code><\/pre>\n<p>When I look at the UI I don't see any datsets, but I see a region drop-down, but no <code>us-central1<\/code>. Why is that? (The project is the correct one).<\/p>",
        "Challenge_closed_time":1652357677267,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652177244937,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user created a Vertex AI dataset in us-central1 and confirmed its existence using vertex_ai.TabularDataset.list(), but it is not displayed in the UI. The user noticed a region drop-down but did not see us-central1. The user is seeking an explanation for this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72184371",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.1,
        "Challenge_reading_time":4.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":50.1200916667,
        "Challenge_title":"Why is my Vertex AI dataset not displayed?",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":89.0,
        "Challenge_word_count":51,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>It <strong>is<\/strong> there but at the beginning of the list, not with the other US ones.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":1.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":16.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1444418094503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":762.0,
        "Answerer_view_count":26.0,
        "Challenge_adjusted_solved_time":0.1511591667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a folder with predicted masks on AWS Sagemaker. ( It has 4 folders inside it and lot of files inside those folders. ) I want to download the entire folder to my laptop. \nThis might sound so simple and easy, but I could not find a way to do it. Appreciate any help.<\/p>\n\n<p>Thanks<\/p>",
        "Challenge_closed_time":1551375926143,
        "Challenge_comment_count":0,
        "Challenge_created_time":1551375381970,
        "Challenge_favorite_count":8.0,
        "Challenge_gpt_summary_original":"The user is having difficulty downloading an entire folder containing multiple subfolders and files from AWS Sagemaker to their laptop. They are seeking assistance in finding a solution.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54931270",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":4.9,
        "Challenge_reading_time":4.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":17.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.1511591667,
        "Challenge_title":"Download an entire folder from AWS sagemaker to laptop",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":13015.0,
        "Challenge_word_count":62,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1366530725212,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"California, USA",
        "Poster_reputation_count":440.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>You can do that by opening a terminal on sagemaker. Navigate to the path where your folder is. Run the command to zip it<\/p>\n\n<pre><code>zip -r -X archive_name.zip folder_to_compress\n<\/code><\/pre>\n\n<p>You will find the zipped folder. You can then select it and download it.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.5,
        "Solution_reading_time":3.45,
        "Solution_score_count":35.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":44.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1639972620503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1653.0,
        "Answerer_view_count":1212.0,
        "Challenge_adjusted_solved_time":24.1372647222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am passing the values of lables as below to create a featurestore with labels. But after creation of the featurestore, I do not see the featurestore created with labels. Is it still not supported in VertexAI<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>    fs = aiplatform.Featurestore.create(\n        featurestore_id=featurestore_id,\n        labels=dict(project='retail', env='prod'),\n        online_store_fixed_node_count=online_store_fixed_node_count,\n        sync=sync\n    )\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/viOSu.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/viOSu.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1651709813300,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651616413553,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to create a feature store in VertexAI using labels. They have passed the values of labels to create a feature store, but after creation, they are unable to see the feature store created with labels. The user is unsure if this feature is supported in VertexAI.",
        "Challenge_last_edit_time":1651623411107,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72106030",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":9.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":25.9443741667,
        "Challenge_title":"I am not able to create a feature store in vertexAI using labels",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":83.0,
        "Challenge_word_count":70,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1530457174832,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1043.0,
        "Poster_view_count":212.0,
        "Solution_body":"<p>As mentioned in this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/managing-featurestores\" rel=\"nofollow noreferrer\">featurestore documentation<\/a>:<\/p>\n<blockquote>\n<p>A <strong>featurestore<\/strong> is a top-level container for entity types, features,\nand feature values.<\/p>\n<\/blockquote>\n<p>With this, the GCP console UI &quot;labels&quot; are the &quot;labels&quot; at the <strong>Feature<\/strong> level.<\/p>\n<p>Once a <strong>featurestore<\/strong> is created, you will need to create an <strong>entity<\/strong> and then create a <strong>Feature<\/strong> that has the <em>labels<\/em> parameter as shown on the below sample python code.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from google.cloud import aiplatform\n\ntest_label = {'key1' : 'value1'}\n\ndef create_feature_sample(\n    project: str,\n    location: str,\n    feature_id: str,\n    value_type: str,\n    entity_type_id: str,\n    featurestore_id: str,\n):\n\n    aiplatform.init(project=project, location=location)\n\n    my_feature = aiplatform.Feature.create(\n        feature_id=feature_id,\n        value_type=value_type,\n        entity_type_name=entity_type_id,\n        featurestore_id=featurestore_id,\n        labels=test_label,\n    )\n\n    my_feature.wait()\n\n    return my_feature\n\ncreate_feature_sample('your-project','us-central1','test_feature3','STRING','test_entity3','test_fs3')\n<\/code><\/pre>\n<p>Below is the screenshot of the GCP console which shows that <em>labels<\/em> for <strong>test_feature3<\/strong> feature has the values defined in the above sample python code.\n<a href=\"https:\/\/i.stack.imgur.com\/7S2oa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7S2oa.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>You may refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/managing-features#create-feature\" rel=\"nofollow noreferrer\">creation of feature documentation<\/a> using python for more details.<\/p>\n<p>On the other hand, you may still view the <em>labels<\/em> you defined for your featurestore using the REST API as shown on the below sample.<\/p>\n<pre><code>curl -X GET \\\n-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print-access-token) \\\n&quot;https:\/\/&lt;your-location&gt;-aiplatform.googleapis.com\/v1\/projects\/&lt;your-project&gt;\/locations\/&lt;your-location&gt;\/featurestores&quot;\n<\/code><\/pre>\n<p>Below is the result of the REST API which also shows the value of the <em>labels<\/em> I defined for my &quot;test_fs3&quot; featurestore.\n<a href=\"https:\/\/i.stack.imgur.com\/gW45X.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gW45X.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1651710305260,
        "Solution_link_count":7.0,
        "Solution_readability":16.7,
        "Solution_reading_time":34.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":226.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":27.1724516667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,  <\/p>\n<p>I am trying to export data from Azure ML to an Azure SQL Database using the 'Export Data' module but the log file contains the following messages and no data is exported to the database.  <\/p>\n<p>&quot;Not exporting to run RunHistory as the exporter is either stopped or there is no data&quot;  <\/p>\n<p>&quot;Process exiting with code: 0  <\/p>\n<p>There is definitely data flowing to the 'Export Data' module from an 'Execute R Script' module as I have checked the Result dataset.  <\/p>\n<p>Would appreciate some assistance.  <\/p>\n<p>Thank you.<\/p>",
        "Challenge_closed_time":1629106747876,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629008927050,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue while exporting data from Azure ML to an Azure SQL Database using the 'Export Data' module. The log file shows messages stating that no data is being exported, even though data is flowing to the module from an 'Execute R Script' module. The user is seeking assistance to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/514067\/no-data-being-exported-from-export-data-module-in",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":7.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":27.1724516667,
        "Challenge_title":"No Data being exported from 'Export Data' module in Azure ML",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":102,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi,   <\/p>\n<p>I have resolved this issue. I had set the export table to be dbo.TestTable rather than just TestTable. As the table dbo.TestTable did not exist the 'Export module' created it in the dbo schema so the table name effectively became dbo.dbo.TestTable.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.5,
        "Solution_reading_time":3.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.8091666667,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi, I'm confused with the automated labelling feature of SM. Usually when people label things it is to train their own models afterwards. Is the goal of this feature to replace the downstream ML model that would use the labelled dataset? some sort of code free computer vision system?",
        "Challenge_closed_time":1544632081000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544611168000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is confused about the purpose of SageMaker Ground Truth's automated labelling feature. They are unsure if it is meant to replace downstream ML models or if it is a code-free computer vision system.",
        "Challenge_last_edit_time":1667965191088,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUp6wm80kUT0GZJp8meQtgTg\/why-sagemaker-ground-truth-automated-labelling",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.3,
        "Challenge_reading_time":4.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":5.8091666667,
        "Challenge_title":"why SageMaker Ground Truth automated labelling?",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":108.0,
        "Challenge_word_count":54,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Automated data labeling is labeling of data using machine learning. Amazon SageMaker Ground Truth will first select a random sample of data and send it to humans to be labeled. The results are then used to train a labeling model that attempts to label a new sample of raw data automatically. The labels are committed when the model can label the data with a confidence score that meets or exceeds a high threshold. Where the confidence score falls below this threshold, the data is sent to human labelers. Some of the data labeled by humans is used to generate a new training dataset for the labeling model, and the model is automatically retrained to improve its accuracy. This process repeats with each sample of raw data to be labeled. The labeling model becomes more capable of automatically labeling raw data with each iteration, and less data is routed to humans.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925583503,
        "Solution_link_count":0.0,
        "Solution_readability":10.6,
        "Solution_reading_time":10.58,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":150.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":30.4174394444,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hi all,<\/p>\n<p>I\u2019m trying to figure out how does the caching  of artifacts work. Let\u2019s say I want to download a model artifact to run some evaluation on. I don\u2019t need the file on disk to persist rather I just want to load it into memory. What I do right now in my evaluation script is:<\/p>\n<pre><code class=\"lang-auto\">import tempfile\nimport wandb\n\nartifact = wandb.use_artifact(model_weights_uri)\nwith tempfile.TemporaryDirectory() as tmpdirname:\n    artifact.download(tmpdirname)\n    model_weights = load_pickle(os.path.join(tmpdirname, \"model_weights.pickle\"))\n<\/code><\/pre>\n<p>And from that point on I use the <code>model_weights<\/code> as it was loaded into memory.<\/p>\n<p>My first question is: if I run the code twice (on the same machine), <strong>will the model-weights be downloaded again<\/strong> or are they cached somewhere? assuming the logged artifact wasn\u2019t changed of course. And if they are cached, where are they cached?<br>\nI\u2019m also not clear about the <code>artifact<\/code> directory (which is used if I run <code>artifact.download()<\/code> without any argument). Does that directory serve as cache? if so, what does the <code>.cache<\/code> directory used for?<\/p>\n<p>I would appreciate answers to my questions and perhaps a  general explanation of the artifact caching mechanism &amp; best practices.<\/p>\n<p>Thanks!<br>\nRan<\/p>",
        "Challenge_closed_time":1650312955392,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650203452610,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to understand how the caching of artifacts works in their evaluation script. They want to know if the model-weights will be downloaded again if they run the code twice on the same machine and where the cached files are stored. They also have questions about the artifact directory and the .cache directory. The user is seeking answers and best practices for artifact caching.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/artifacts-local-caching-how-does-it-really-work\/2255",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.4,
        "Challenge_reading_time":17.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":30.4174394444,
        "Challenge_title":"Artifacts (local) caching - how does it really work?",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":861.0,
        "Challenge_word_count":191,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ranshadmi-nexite\">@ranshadmi-nexite<\/a>,<\/p>\n<p>Thank you for your question. You are right, all Artifacts are cached on your system under <code>~\/.cache\/wandb\/artifacts<\/code> and organized by their checksum. So if you try to download a file with checksum <code>x<\/code> and that file has been logged in an Artifact from your machine or downloaded to your machine as part of an artifact before, we just pull it from the cache by checking if there is a cached Artifact file with checksum <code>x<\/code>.<\/p>\n<p>So, if you run the same code twice, assuming the version of the artifact you are trying to download has not changed, the artifact can simply be pickked up from your cache directory.<\/p>\n<p>Also, when calling <code>artifact.download()<\/code> without any arguments, the artifact is saved in the directory in which the code is running. This, however,  is not the directory that serves as a cache, that still remains <code>.cache<\/code> which acts as a central location to look for artifacts before fetching it.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":13.49,
        "Solution_score_count":null,
        "Solution_sentence_count":10.0,
        "Solution_word_count":162.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":157.7512475,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I log model scores by steps and at every step I have metric value, confidence interval lower bound, confidence interval upper bound. Is it possible to log confidence intervals (on one graph) and show the confidence interval using different color?<\/p>",
        "Challenge_closed_time":1674027872136,
        "Challenge_comment_count":0,
        "Challenge_created_time":1673459967645,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to know if it is possible to log confidence intervals on one graph and show them using different colors. They currently log model scores by steps and have metric value, confidence interval lower bound, and confidence interval upper bound at every step.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/is-it-possible-to-log-confidence-intervals\/3684",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":11.6,
        "Challenge_reading_time":3.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":157.7512475,
        "Challenge_title":"Is it possible to log confidence intervals?",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":223.0,
        "Challenge_word_count":46,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Thank you so much for the example! This helps a whole lot. Currently this isn\u2019t a feature we have in our product, but I\u2019ll create a feature request for this and our team will reach out to you once there are any updates on this ticket.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":2.88,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":46.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":90.4033952778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi Team,  <\/p>\n<p>I tried connecting to Azure table storage in Azure ML Studio. It shows connection successful after updating all credentials but after hitting run, import is landing to internal system error.  <br \/>\nBelow is the message :  <br \/>\n[Critical]     Error: Sorry, it seems that you have encountered an internal system error. Please contact amlforum@microsoft.com with the full URL in the browser and the time you experienced the failure. We can locate this error with your help and investigate further. Thank you.  <\/p>\n<p>Requesting you to please assist in this case.  <\/p>\n<p>Regards,  <br \/>\nSachin<\/p>",
        "Challenge_closed_time":1616395982956,
        "Challenge_comment_count":6,
        "Challenge_created_time":1616070530733,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue while importing data from Azure table storage to Azure ML Studio. The connection is successful but the import is resulting in an internal system error. The user has requested assistance in resolving the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/320696\/data-import-error-for-azure-table-storage-to-azure",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":7.9,
        "Challenge_reading_time":8.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":90.4033952778,
        "Challenge_title":"Data Import error for Azure table storage to Azure ML studio ?",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":106,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello,    <\/p>\n<p>There is a known issue that Azure ML Studio only supports \u201chttp\u201d protocol when connecting with Azure Storage Account. You might hit this issue when using the Import Data module.    <\/p>\n<p>Here is a quick work around:    <br \/>\nPlease check the \u201cConfiguration\u201d of your Storage Account, and make sure the \u201cSecure transfer required\u201d is disabled (see the figure below).    <\/p>\n<p>If still encountering error after taking these steps, please double check and make sure the account key is correct.    <\/p>\n<p><a href=\"\/users\/na\/?userid=520e72bc-f33a-4fa2-84f8-4795fd5f44af\">@Sachin Gaikwad  <\/a> Please accept the answer if you feel the work around works. Thank you!    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/80028-image.png?platform=QnA\" alt=\"80028-image.png\" \/>    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.6,
        "Solution_reading_time":10.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":108.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":314.4053583334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a use case wherein I need to refer to the input dataset in the ACI\/AKS which is in a blob (same used for training model). I'm not able to find related resources in the Microsoft official documentation. If anyone suggests to me how to do it, that will be very helpful.<\/p>",
        "Challenge_closed_time":1627304144467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1626172285177,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in referring to an input dataset in ACI\/AKS that is stored in a blob, which is the same dataset used for training the model. The user is seeking guidance on how to resolve this issue as there are no related resources available in Microsoft's official documentation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68360738",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.7,
        "Challenge_reading_time":3.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":314.4053583334,
        "Challenge_title":"AKS an ACI Deployment with blob mount",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":57.0,
        "Challenge_word_count":59,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1448994884167,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":265.0,
        "Poster_view_count":156.0,
        "Solution_body":"<p>It will be supported in the near future, Running Python scripts on Azure with Azure Container Instances to connect the blob.\n<a href=\"https:\/\/kohera.be\/tutorials-2\/running-python-scripts-on-azure-with-azure-container-instances\/\" rel=\"nofollow noreferrer\">https:\/\/kohera.be\/tutorials-2\/running-python-scripts-on-azure-with-azure-container-instances\/<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":25.9,
        "Solution_reading_time":4.92,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1588674524307,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":208.0,
        "Answerer_view_count":29.0,
        "Challenge_adjusted_solved_time":0.8600380556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I use Amazon Sagemaker for model training and prediction. I have a problem with the returned data with predictions.  I am trying to convert prediction data to pandas dataframe format.<\/p>\n<p>After the model is deployed:<\/p>\n<pre><code>from sagemaker.serializers import CSVSerializer\n\nxgb_predictor=estimator.deploy(\n    initial_instance_count=1,\n    instance_type='ml.g4dn.xlarge',\n    serializer=CSVSerializer()\n)\n\n<\/code><\/pre>\n<p>I made a prediction on the test data:<\/p>\n<pre><code>predictions=xgb_predictor.predict(first_day.to_numpy())\n\n<\/code><\/pre>\n<p>The returned prediction results are in a binary file<\/p>\n<pre><code>predictions\n<\/code><\/pre>\n<pre><code>b'2.092024326324463\\n10.584211349487305\\n18.23127555847168\\n2.092024326324463\\n8.308058738708496\\n32.35516357421875\\n4.129155158996582\\n7.429899215698242\\n55.65376281738281\\n116.5504379272461\\n1.0734045505523682\\n5.29403018951416\\n1.0924320220947266\\n1.9484598636627197\\n5.29403018951416\\n2.190509080886841\\n2.085641860961914\\n2.092024326324463\\n7.674410343170166\\n2.1198673248291016\\n5.293967247009277\\n7.088096618652344\\n2.092024326324463\\n10.410735130310059\\n10.36008358001709\\n2.092024326324463\\n10.565692901611328\\n15.495997428894043\\n15.61841106414795\\n1.0533703565597534\\n6.262670993804932\\n31.02411460876465\\n10.43086051940918\\n3.116995096206665\\n3.2846100330352783\\n108.82835388183594\\n26.210166931152344\\n1.0658172369003296\\n10.55643367767334\\n6.245237350463867\\n15.951444625854492\\n10.195240020751953\\n1.0734045505523682\\n48.720497131347656\\n2.119992256164551\\n9.41071605682373\\n2.241959810256958\\n3.1907501220703125\\n10.415051460266113\\n1.2154537439346313\\n2.13691782951355\\n31.1861515045166\\n3.0827555656433105\\n6.261478424072266\\n5.279026985168457\\n15.897627830505371\\n20.483125686645508\\n20.874958038330078\\n53.2086296081543\\n10.731611251831055\\n2.115110397338867\\n13.79739761352539\\n2.1198673248291016\\n26.628803253173828\\n10.030998229980469\\n15.897627830505371\\n5.278475284576416\\n45.371158599853516\\n2.2791690826416016\\n15.58777141571045\\n15.947166442871094\\n30.88138771057129\\n10.388553619384766\\n48.22294235229492\\n10.565692901611328\\n20.808977127075195\\n10.388553619384766\\n15.910200119018555\\n8.252408981323242\\n1.109586238861084\\n15.58777141571045\\n13.718815803527832\\n3.1227424144744873\\n32.171592712402344\\n10.524396896362305\\n15.897627830505371\\n2.092024326324463\\n14.52088737487793\\n5.293967247009277\\n57.61208724975586\\n21.161712646484375\\n14.173937797546387\\n5.230247974395752\\n16.257652282714844\n\n<\/code><\/pre>\n<p>How can I convert prediction data to pandas dataframe?<\/p>",
        "Challenge_closed_time":1659449084300,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659445988163,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in converting binary file data to pandas dataframe format after making a prediction on test data using Amazon Sagemaker for model training and prediction. The returned prediction results are in binary file format and the user is seeking guidance on how to convert this data to pandas dataframe.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73208208",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.6,
        "Challenge_reading_time":36.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.8600380556,
        "Challenge_title":"how to convert binary file to pandas dataframe",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":31.0,
        "Challenge_word_count":83,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1414361702887,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>you mean this:<\/p>\n<pre><code>import pandas as pd\n\na = a.decode(encoding=&quot;utf-8&quot;).split(&quot;\\n&quot;)\n\ndf = pd.DataFrame(data=a)\ndf.head()\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.7,
        "Solution_reading_time":2.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":57.6558561111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hi there! I was wondering, how do I deal with having multiple variables to log, but one of those variables I only want to log every 100 timesteps? The wandb docs seem to suggest that I need to collect all my metrics into one log function call, but in my scenario above where I want to track one variable every step and another variable every 100 steps, I would need multiple log calls. I saw the docs for the define metrics function, but I\u2019m not quite sure if that\u2019s the way to handle this. How do I approach this in PyTorch? Thanks!<\/p>\n<p>As an example, I currently have this Tensorboard logging that I\u2019m trying to convert to wandb:<\/p>\n<pre><code class=\"lang-auto\">print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\nwriter.add_scalar(\"charts\/episodic_return\", info[\"episode\"][\"r\"], global_step)\n\nif global_step % 100 == 0:\n    writer.add_scalar(\n        \"losses\/qf1_values\", qf1_a_values.mean().item(), global_step\n    )\n<\/code><\/pre>",
        "Challenge_closed_time":1673569740548,
        "Challenge_comment_count":0,
        "Challenge_created_time":1673362179466,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to log multiple variables in PyTorch using wandb, but wants to log one variable every step and another variable every 100 steps. The user is unsure if the define metrics function is the way to handle this and is seeking guidance on how to approach this issue. The user provided an example of Tensorboard logging that they are trying to convert to wandb.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-log-two-variables-at-different-increments-of-timesteps\/3674",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.5,
        "Challenge_reading_time":12.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":57.6558561111,
        "Challenge_title":"How to log two variables at different increments of timesteps?",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":150.0,
        "Challenge_word_count":142,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/chulabhaya\">@chulabhaya<\/a> , happy to help. The approach you are considering is correct. You can set a check in place and log a dictionary with the values you want and set the step value.<\/p>\n<pre><code class=\"lang-auto\">for i in range (300):\n    if i%100==0:\n        wandb.log({\"value\": i, \"value\": 100}, step =i)\n    else:\n        wandb.log({\"value\": 100})\n<\/code><\/pre>\n<p>The <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log#customize-axes-and-summaries-with-define_metric\">defined metric<\/a> function allows you to have more control over the representation of your x axis and also how that axes is incremented. There are a few examples listed in the linked doc on how it functions. Please let me know if you have any questions.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.8,
        "Solution_reading_time":9.48,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":101.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":92.1736986111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have some data with very particular format (e.g., tdms files generated by NI systems) and I stored them in a S3 bucket. Typically, for reading this data in python if the data was stored in my local computer, I would use npTDMS package. But, how should is read this tdms files when they are stored in a S3 bucket? One solution is to download the data for instance to the EC2 instance and then use npTDMS package for reading the data into python. But it does not seem to be a perfect solution. Is there any way that I can read the data similar to reading CSV files from S3? <\/p>",
        "Challenge_closed_time":1577181091056,
        "Challenge_comment_count":3,
        "Challenge_created_time":1576870865157,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has data stored in a specific format (tdms files) in an AWS S3 bucket and is looking for a way to read the data in Python without having to download it to an EC2 instance. They are wondering if there is a way to read the data similar to reading CSV files from S3.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59430560",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":6.6,
        "Challenge_reading_time":7.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":86.1738608333,
        "Challenge_title":"Reading Data from AWS S3",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":4393.0,
        "Challenge_word_count":116,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1534965197292,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seattle, WA",
        "Poster_reputation_count":320.0,
        "Poster_view_count":48.0,
        "Solution_body":"<p>Some Python packages (such as Pandas) support reading data directly from S3, as it is the most popular location for data. See <a href=\"https:\/\/stackoverflow.com\/questions\/37703634\/how-to-import-a-text-file-on-aws-s3-into-pandas-without-writing-to-disk\">this question<\/a> for example on the way to do that with Pandas.<\/p>\n\n<p>If the package (npTDMS) doesn't support reading directly from S3, you should copy the data to the local disk of the notebook instance.<\/p>\n\n<p>The simplest way to copy is to run the AWS CLI in a cell in your notebook<\/p>\n\n<pre><code>!aws s3 cp s3:\/\/bucket_name\/path_to_your_data\/ data\/\n<\/code><\/pre>\n\n<p>This command will copy all the files under the \"folder\" in S3 to the local folder <code>data<\/code><\/p>\n\n<p>You can use more fine-grained copy using the filtering of the files and other specific requirements using the boto3 rich capabilities. For example:<\/p>\n\n<pre><code>s3 = boto3.resource('s3')\nbucket = s3.Bucket('my-bucket')\nobjs = bucket.objects.filter(Prefix='myprefix')\nfor obj in objs:\n   obj.download_file(obj.key)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1577202690472,
        "Solution_link_count":1.0,
        "Solution_readability":10.8,
        "Solution_reading_time":13.62,
        "Solution_score_count":3.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":133.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":69.685,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nWhen you try to specify an artifact path and a run_id in an ``MlflowArtifactDataSet``, you get an error. \r\n\r\nThis works:\r\n```python\r\nmlflow_csv_dataset = MlflowArtifactDataSet(\r\n    data_set=dict(type=CSVDataSet, filepath=\"path\/to\/df.csv\"),\r\n    artifact_path=None,\r\n    run_id=\"1234\",\r\n)\r\nmlflow_csv_dataset .load()\r\n```\r\n\r\nwhile this :\r\n```python\r\nmlflow_csv_dataset = MlflowArtifactDataSet(\r\n    data_set=dict(type=CSVDataSet, filepath=\"path\/to\/df.csv\"),\r\n    artifact_path=\"folder\", # this is the difference\r\n    run_id=\"1234\",\r\n)\r\nmlflow_csv_dataset .load()\r\n```\r\nraises the following error: ``unsupported operand type(s) for \/: 'str' and 'str'``:\r\n",
        "Challenge_closed_time":1665079955000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1664829089000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The issue is related to the failure of MlflowArtifactDataset.load() when both artifact_path and run_id are specified. An error is encountered when the artifact_path is not None and run_id is specified.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/362",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.6,
        "Challenge_reading_time":9.28,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":21.0,
        "Challenge_repo_issue_count":414.0,
        "Challenge_repo_star_count":145.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":69.685,
        "Challenge_title":"MlflowArtifactDataset.load() fails if artifact_path is not None and run_id is specified",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":67,
        "Discussion_body":"The issue is still here when there is nested artifact_path: if the file does not exists, it is downloaded to ``self._filepath \/artifact_path\/filename.pkl`` and cannot be loaded (due to the ``artifact_path`` suffix)",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1501163272143,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":118.0,
        "Challenge_adjusted_solved_time":3.5842077778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Assume I have an Execute R Script that calculates multiple variables, say X and Y.\nIs it possible to save X as a dataset ds_X and Y as a dataset ds_Y?<\/p>\n\n<p>The problem is that there is only 1 output port available that needs to be mapped to a data.frame. Am I missing an option to add more output ports?\nSame problem for input ports. I may connect 2 of the \"Enter Data Manually\" modules to it, but what if I need 3? The current workaround is to put CSV files in a ZIP file and connect that. Are there easier solution?<\/p>\n\n<p><strong>Example of what i tried:<\/strong><\/p>\n\n<p>I tried adding ds_X and ds_Y to a list. The idea is to pass this list to multiple \"Execute R Script\" modules and use the required list elements there.\nMapping a list to an output port does not seem to work though:<\/p>\n\n<pre><code># Calculate lots of stuff - results are ds_X and ds_Y\nds_X &lt;- mtcars\nds_Y &lt;- cars\nout &lt;- list(ds_X, ds_Y)\n\nmaml.mapOutputPort(\"out\")\n<\/code><\/pre>\n\n<p>results in an error:<\/p>\n\n<pre><code>Error: Mapped variable must be of class type data.frame at this time.\n<\/code><\/pre>",
        "Challenge_closed_time":1506004266756,
        "Challenge_comment_count":4,
        "Challenge_created_time":1505987948480,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge with Execute R Script in Azure ML Studio, as there is only one output port available that needs to be mapped to a data.frame. The user is also unable to add more input ports and is currently using a workaround by putting CSV files in a ZIP file and connecting that. The user tried adding ds_X and ds_Y to a list but mapping a list to an output port does not seem to work.",
        "Challenge_last_edit_time":1505991363608,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46340959",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":6.3,
        "Challenge_reading_time":13.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":4.5328544444,
        "Challenge_title":"Multiple Inputs\/Outputs from Execute R Script",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":754.0,
        "Challenge_word_count":193,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1389795136836,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":625.0,
        "Poster_view_count":36.0,
        "Solution_body":"<p>You can author custom R Modules. <\/p>\n\n<p>Here is some documentation: \n<a href=\"https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/04\/23\/build-your-own-r-modules-in-azure-ml\/\" rel=\"nofollow noreferrer\">https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/04\/23\/build-your-own-r-modules-in-azure-ml\/<\/a>\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-custom-r-modules\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-custom-r-modules<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":47.3,
        "Solution_reading_time":7.62,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":70.6579766667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created a file dataset from a data lake folder on Azure ML Studio,  at the moment I\u00b4m able to download the data from the dataset to the compute instance with this code:<\/p>\n<pre><code>subscription_id = 'xxx'\nresource_group = 'luisdatapipelinetest'\nworkspace_name = 'ml-pipelines'\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\ndataset = Dataset.get_by_name(workspace, name='files_test')\npath = &quot;\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/demo1231\/code\/Users\/luis.rramirez\/test\/&quot;\ndataset.download(target_path=path, overwrite=True)\n<\/code><\/pre>\n<p>With that I'm able to access the files from the notebook.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/8q8y2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8q8y2.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But copying the data from the data lake to the compute instance is not efficient, how can I mount the data lake directory in the vm instead of copying the data each time?<\/p>",
        "Challenge_closed_time":1630298993132,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630008530263,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a file dataset from a data lake folder on Azure ML Studio and is able to download the data from the dataset to the compute instance. However, copying the data from the data lake to the compute instance is not efficient, and the user is seeking a way to mount the data lake directory in the VM instead of copying the data each time.",
        "Challenge_last_edit_time":1630044624416,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68944750",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":13.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":80.6841302778,
        "Challenge_title":"Mount a datalake storage in azure ML studio",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":258.0,
        "Challenge_word_count":112,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423439611840,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":8349.0,
        "Poster_view_count":949.0,
        "Solution_body":"<p>MOUNTING ADLS2 to AML so you can save files into your mountPoint directly. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#azure-data-lake-storage-generation-2\" rel=\"nofollow noreferrer\">Here<\/a> is the example of registering the storage and <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.file_dataset.filedataset?view=azure-ml-py#mount-mount-point-none----kwargs-\" rel=\"nofollow noreferrer\">here<\/a> shows how to mount your registered datastore.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.0,
        "Solution_reading_time":7.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":36.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1499171495843,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bhubaneswar, Odisha, India",
        "Answerer_reputation_count":521.0,
        "Answerer_view_count":77.0,
        "Challenge_adjusted_solved_time":6.5169719444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am creating a Question Answering model using <a href=\"https:\/\/simpletransformers.ai\/docs\/qa-specifics\/\" rel=\"nofollow noreferrer\">simpletransformers<\/a>. I would also like to use wandb to track model artifacts. As I understand from <a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/other\/simpletransformers\" rel=\"nofollow noreferrer\">wandb docs<\/a>, there is an integration touchpoint for simpletransformers but there is no mention of logging artifacts.<\/p>\n<p>I would like to log artifacts generated at the train, validation, and test phase such as train.json, eval.json, test.json, output\/nbest_predictions_test.json and best performing model.<\/p>",
        "Challenge_closed_time":1634729204572,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634705743473,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use wandb to track model artifacts while creating a Question Answering model using simpletransformers, but is unable to find any mention of logging artifacts in the wandb documentation for simpletransformers integration. The user wants to log artifacts generated during the train, validation, and test phases.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69640534",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.1,
        "Challenge_reading_time":9.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":6.5169719444,
        "Challenge_title":"How to log artifacts in wandb while using saimpletransformers?",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":53.0,
        "Challenge_word_count":79,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1528765704783,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Ahmedabad, Gujarat, India",
        "Poster_reputation_count":13.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>Currently simpleTransformers doesn't support logging artifacts within the training\/testing scripts. But you can do it manually:<\/p>\n<pre><code>import os \n\nwith wandb.init(id=model.wandb_run_id, resume=&quot;allow&quot;, project=wandb_project) as training_run:\n    for dir in sorted(os.listdir(&quot;outputs&quot;)):\n        if &quot;checkpoint&quot; in dir:\n            artifact = wandb.Artifact(&quot;model-checkpoints&quot;, type=&quot;checkpoints&quot;)\n            artifact.add_dir(&quot;outputs&quot; + &quot;\/&quot; + dir)\n            training_run.log_artifact(artifact)\n<\/code><\/pre>\n<p>For more info, you can follow along with the W&amp;B notebook in the SimpleTransofrmer's README.md<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.6,
        "Solution_reading_time":8.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":55.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.8761111111,
        "Challenge_answer_count":1,
        "Challenge_body":"I launched an Autopilot job in SageMaker Studio, and now I'm trying to figure out how to compare autoML iterations. Is there a way to list them, see their metrics, and see the configuration of the best job?",
        "Challenge_closed_time":1601339581000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601332827000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user launched an Autopilot job in Amazon SageMaker Studio and is now looking for a way to compare autoML iterations, view their metrics, and see the configuration of the best job.",
        "Challenge_last_edit_time":1667925765776,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU7p9B6zcpSIeTG4MbzrSjKA\/how-do-you-analyze-autopilot-results-in-amazon-sagemaker-studio",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":3.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1.8761111111,
        "Challenge_title":"How do you analyze Autopilot results in Amazon SageMaker Studio?",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":91.0,
        "Challenge_word_count":47,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Watch the [Choose and deploy the best model](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/autopilot-videos.html#autopilot-video-choose-and-deploy-the-best-model) video tutorial in the SageMaker developer guide. The video shows how to use SageMaker Autopilot to visualize and compare model metrics.\n\nFor more SageMaker Autopilot tutorials, see [Videos: Use Autopilot to automate and explore the machine learning process](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/autopilot-videos.html).",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925562588,
        "Solution_link_count":2.0,
        "Solution_readability":18.4,
        "Solution_reading_time":6.58,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":46.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":457.0916797222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I need to train a custom OCR in vertex AI. My data with have folder of cropped image, each image is a line, and a csv file with 2 columns: image name and text in image.\nBut when I tried to import it into a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-managed-datasets\" rel=\"nofollow noreferrer\">dataset<\/a> in vertex AI, I see that image dataset only support for classification, segmentation, object detection. All of dataset have fixed number of label, but my data have a infinite number of labels(if we view text in image as label), so all types doesn't match with my requirement. Can I use vertex AI for training, and how to do that ?<\/p>",
        "Challenge_closed_time":1652091441187,
        "Challenge_comment_count":2,
        "Challenge_created_time":1650445911140,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while trying to train a custom OCR in Vertex AI using a custom data format that includes a folder of cropped images and a CSV file with two columns. The user is unable to import the data into a dataset in Vertex AI as the image dataset only supports classification, segmentation, and object detection, which do not match the user's requirements. The user is seeking guidance on whether Vertex AI can be used for training and how to proceed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71937033",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":8.7,
        "Challenge_reading_time":8.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":457.0916797222,
        "Challenge_title":"Google Cloud Platform - Vertex AI training with custom data format",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":303.0,
        "Challenge_word_count":118,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1412860343896,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hanoi, Vietnam",
        "Poster_reputation_count":803.0,
        "Poster_view_count":114.0,
        "Solution_body":"<p>Since Vertex AI managed datasets do not support OCR applications, you can train and deploy a custom model using Vertex AI\u2019s training and prediction services.<\/p>\n<p>I found a good <a href=\"https:\/\/medium.com\/geekculture\/building-a-complete-ocr-engine-from-scratch-in-python-be1fd184753b\" rel=\"nofollow noreferrer\">article<\/a> on building an OCR system from scratch. This OCR system is implemented in 2 steps<\/p>\n<ol>\n<li>Text detection<\/li>\n<li>Text recognition<\/li>\n<\/ol>\n<p>Please note that this article is not officially supported by Google Cloud.<\/p>\n<p>Once you have tested the model locally, you can train the same on Vertex AI using the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/custom-training\" rel=\"nofollow noreferrer\">custom model training service<\/a>. Please follow this <a href=\"https:\/\/codelabs.developers.google.com\/vertex_custom_training_prediction\" rel=\"nofollow noreferrer\">codelab<\/a> for step-by-step instructions on training and deploying a custom model.<\/p>\n<p>Once the training is complete, the model can be deployed for inference using a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/pre-built-containers\" rel=\"nofollow noreferrer\">pre-built container<\/a> offered by Vertex AI or a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements\" rel=\"nofollow noreferrer\">custom container<\/a> based on your requirements. You can also choose between batch predictions for synchronous requests and online predictions for asynchronous requests.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":15.5,
        "Solution_reading_time":20.2,
        "Solution_score_count":2.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":157.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2728.7016666667,
        "Challenge_answer_count":0,
        "Challenge_body":"",
        "Challenge_closed_time":1652740320000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1642916994000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is experiencing issues with the `WandbFileSystem.ls` function when trying to access nested directories.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/allenai\/tango\/issues\/151",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":2.9,
        "Challenge_reading_time":1.03,
        "Challenge_repo_contributor_count":16.0,
        "Challenge_repo_fork_count":35.0,
        "Challenge_repo_issue_count":570.0,
        "Challenge_repo_star_count":376.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":1,
        "Challenge_solved_time":2728.7016666667,
        "Challenge_title":"WandB callback changes the train step's unique ID, but does not change the results",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":14,
        "Discussion_body":"Actually, callbacks can change the result. So we'll close this.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1538816771612,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Berlin",
        "Answerer_reputation_count":647.0,
        "Answerer_view_count":61.0,
        "Challenge_adjusted_solved_time":17.7895613889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using MLflow to track my experiments. I am using an S3 bucket as an artifact store. For acessing it, I want to use <em>proxied artifact access<\/em>, as described in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">docs<\/a>, however this does not work for me, since it locally looks for credentials (but the server should handle this).<\/p>\n<h2>Expected Behaviour<\/h2>\n<p>As described in the docs, I would expect that locally, I do not need to specify my AWS credentials, since the server handles this for me. From <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n<blockquote>\n<p>This eliminates the need to allow end users to have direct path access to a remote object store (e.g., s3, adls, gcs, hdfs) for artifact handling and eliminates the need for an end-user to provide access credentials to interact with an underlying object store.<\/p>\n<\/blockquote>\n<h2>Actual Behaviour \/ Error<\/h2>\n<p>Whenever I run an experiment on my machine, I am running into the following error:<\/p>\n<p><code>botocore.exceptions.NoCredentialsError: Unable to locate credentials<\/code><\/p>\n<p>So the error is local. However, this should not happen since the server should handle the auth instead of me needing to store my credentials locally. Also, I would expect that I would not even need library <code>boto3<\/code> locally.<\/p>\n<h2>Solutions Tried<\/h2>\n<p>I am aware that I need to create a new experiment, because existing experiments might still use a different artifact location which is proposed in <a href=\"https:\/\/stackoverflow.com\/a\/71417933\/10465165\">this SO answer<\/a> as well as in the note in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">docs<\/a>. Creating a new experiment did not solve the error for me. Whenever I run the experiment, I get an explicit log in the console validating this:<\/p>\n<p><code>INFO mlflow.tracking.fluent: Experiment with name 'test' does not exist. Creating a new experiment.<\/code><\/p>\n<p>Related Questions (<a href=\"https:\/\/stackoverflow.com\/questions\/72206086\/cant-log-mlflow-artifacts-to-s3-with-docker-based-tracking-server\">#1<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/72236258\/mlflow-unable-to-store-artifacts-to-s3\/72261826#comment128726676_72261826\">#2<\/a>) refer to a different scenario, which is also <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-4-mlflow-with-remote-tracking-server-backend-and-artifact-stores\" rel=\"nofollow noreferrer\">described in the docs<\/a><\/p>\n<h2>Server Config<\/h2>\n<p>The server runs on a kubernetes pod with the following config:<\/p>\n<pre><code>mlflow server \\\n    --host 0.0.0.0 \\\n    --port 5000 \\\n    --backend-store-uri postgresql:\/\/user:pw@endpoint \\\n    --artifacts-destination s3:\/\/my_bucket\/artifacts \\\n    --serve-artifacts \\\n    --default-artifact-root s3:\/\/my_bucket\/artifacts \\\n<\/code><\/pre>\n<p>I would expect my config to be correct, looking at doc <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">page 1<\/a> and <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#using-the-tracking-server-for-proxied-artifact-access\" rel=\"nofollow noreferrer\">page 2<\/a><\/p>\n<p>I am able to see the mlflow UI if I forward the port to my local machine. I also see the experiment runs as failed, because of the error I sent above.<\/p>\n<h2>My Code<\/h2>\n<p>The relevant part of my code which fails is the logging of the model:<\/p>\n<pre><code>mlflow.set_tracking_uri(&quot;http:\/\/localhost:5000&quot;)\nmlflow.set_experiment(&quot;test2)\n\n...\n\n# this works\nmlflow.log_params(hyperparameters)\n                        \nmodel = self._train(model_name, hyperparameters, X_train, y_train)\ny_pred = model.predict(X_test)\nself._evaluate(y_test, y_pred)\n\n# this fails with the error from above\nmlflow.sklearn.log_model(model, &quot;artifacts&quot;)\n\n<\/code><\/pre>\n<h2>Question<\/h2>\n<p>I am probably overlooking something. Is there a need to locally indicate that I want to use proxied artified access? If yes, how do I do this? Is there something I have missed?<\/p>\n<h2>Full Traceback<\/h2>\n<pre><code>  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/models\/model.py&quot;, line 295, in log\n    mlflow.tracking.fluent.log_artifacts(local_path, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/tracking\/fluent.py&quot;, line 726, in log_artifacts\n    MlflowClient().log_artifacts(run_id, local_dir, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/tracking\/client.py&quot;, line 1001, in log_artifacts\n    self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py&quot;, line 346, in log_artifacts\n    self._get_artifact_repo(run_id).log_artifacts(local_dir, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/store\/artifact\/s3_artifact_repo.py&quot;, line 141, in log_artifacts\n    self._upload_file(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/store\/artifact\/s3_artifact_repo.py&quot;, line 117, in _upload_file\n    s3_client.upload_file(Filename=local_file, Bucket=bucket, Key=key, ExtraArgs=extra_args)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/boto3\/s3\/inject.py&quot;, line 143, in upload_file\n    return transfer.upload_file(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/boto3\/s3\/transfer.py&quot;, line 288, in upload_file\n    future.result()\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/futures.py&quot;, line 103, in result\n    return self._coordinator.result()\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/futures.py&quot;, line 266, in result\n    raise self._exception\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/tasks.py&quot;, line 139, in __call__\n    return self._execute_main(kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/tasks.py&quot;, line 162, in _execute_main\n    return_value = self._main(**kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/upload.py&quot;, line 758, in _main\n    client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/client.py&quot;, line 508, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/client.py&quot;, line 898, in _make_api_call\n    http, parsed_response = self._make_request(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/client.py&quot;, line 921, in _make_request\n    return self._endpoint.make_request(operation_model, request_dict)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/endpoint.py&quot;, line 119, in make_request\n    return self._send_request(request_dict, operation_model)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/endpoint.py&quot;, line 198, in _send_request\n    request = self.create_request(request_dict, operation_model)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/endpoint.py&quot;, line 134, in create_request\n    self._event_emitter.emit(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/hooks.py&quot;, line 412, in emit\n    return self._emitter.emit(aliased_event_name, **kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/hooks.py&quot;, line 256, in emit\n    return self._emit(event_name, kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/hooks.py&quot;, line 239, in _emit\n    response = handler(**kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/signers.py&quot;, line 103, in handler\n    return self.sign(operation_name, request)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/signers.py&quot;, line 187, in sign\n    auth.add_auth(request)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/auth.py&quot;, line 407, in add_auth\n    raise NoCredentialsError()\nbotocore.exceptions.NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>",
        "Challenge_closed_time":1657186814368,
        "Challenge_comment_count":1,
        "Challenge_created_time":1657122030593,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with MLflow's proxied artifact access while using an S3 bucket as an artifact store. The user is expecting the server to handle the authentication instead of needing to store credentials locally, but is receiving a \"NoCredentialsError\" when running an experiment on their machine. The user has tried creating a new experiment and ensuring their server configuration is correct, but the issue persists. The relevant code failing is the logging of the model. The user is seeking assistance in resolving the issue.",
        "Challenge_last_edit_time":1657122771947,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72886409",
        "Challenge_link_count":10,
        "Challenge_participation_count":2,
        "Challenge_readability":14.6,
        "Challenge_reading_time":104.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":87,
        "Challenge_solved_time":17.9954930556,
        "Challenge_title":"MLflow proxied artifact access: Unable to locate credentials",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":237.0,
        "Challenge_word_count":681,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1538816771612,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berlin",
        "Poster_reputation_count":647.0,
        "Poster_view_count":61.0,
        "Solution_body":"<p>The problem is that the server is running on wrong run parameters, the <code>--default-artifact-root<\/code> needs to either be removed or set to <code>mlflow-artifacts:\/<\/code>.<\/p>\n<p>From <code>mlflow server --help<\/code>:<\/p>\n<pre><code>  --default-artifact-root URI  Directory in which to store artifacts for any\n                               new experiments created. For tracking server\n                               backends that rely on SQL, this option is\n                               required in order to store artifacts. Note that\n                               this flag does not impact already-created\n                               experiments with any previous configuration of\n                               an MLflow server instance. By default, data\n                               will be logged to the mlflow-artifacts:\/ uri\n                               proxy if the --serve-artifacts option is\n                               enabled. Otherwise, the default location will\n                               be .\/mlruns.\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.0,
        "Solution_reading_time":9.46,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":101.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1448964835883,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Z\u00fcrich, Schweiz",
        "Answerer_reputation_count":269.0,
        "Answerer_view_count":42.0,
        "Challenge_adjusted_solved_time":0.3934,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying a machine learning model and logging metrics using mlflow. But I am getting <code>TypeError: 'numpy.float32' object is not iterable<\/code>. I have tried using <code>.tolist()<\/code> and <code>dict()<\/code> but nothing seems to work.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def train(max_epochs, model, optimizer, scheduler, train_loader, valid_loader, project_name):\n    best_val_loss = 100\n    for epoch in range(max_epochs):\n        model.train()\n        running_loss = []\n        tq_loader = tqdm(train_loader)\n        o = {}\n        for samples in tq_loader:\n            optimizer.zero_grad()\n            outputs, interaction_map = model(\n                [samples[0].to(device), samples[1].to(device), torch.tensor(samples[2]).to(device),\n                 torch.tensor(samples[3]).to(device)])\n            l1_norm = torch.norm(interaction_map, p=2) * 1e-4\n            loss = loss_fn(outputs, torch.tensor(samples[4]).to(device).float()) + l1_norm\n            loss.backward()\n            optimizer.step()\n            loss = loss - l1_norm\n            running_loss.append(loss.cpu().detach())\n            tq_loader.set_description(\n                &quot;Epoch: &quot; + str(epoch + 1) + &quot;  Training loss: &quot; + str(np.mean(np.array(running_loss))))\n        model.eval()\n        val_loss, mae_loss = get_metrics(model, valid_loader)\n        scheduler.step(val_loss)\n        \n        #metrics mlflow\n        mlflow.log_metrics('train_loss',(np.mean(np.array(running_loss))).tolist())\n        mlflow.log_metrics('validation_loss',(val_loss).tolist())\n        mlflow.log_metrics('MAE Val_loss', (mae_loss).tolist())\n\n        print(&quot;Epoch: &quot; + str(epoch + 1) + &quot;  train_loss &quot; + str(np.mean(np.array(running_loss))) + &quot; Val_loss &quot; + str(\n            val_loss) + &quot; MAE Val_loss &quot; + str(mae_loss))\n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), &quot;.\/runs\/run-&quot; + str(project_name) + &quot;\/models\/best_model.tar&quot;)\n\nmlflow.set_experiment('CIGIN_V2')\nmlflow.start_run(nested=True)\ntrain(max_epochs, model, optimizer, scheduler, train_loader, valid_loader, project_name)\nmlflow.end_run()\n<\/code><\/pre>\n<p>Error<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>Epoch: 1  Training loss: 6770.575: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:04&lt;00:00,  4.35s\/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:03&lt;00:00,  3.86s\/it]\n\n---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\n&lt;ipython-input-96-8c3a6eb822c3&gt; in &lt;module&gt;()\n      1 mlflow.set_experiment('CIGIN_V2')\n      2 mlflow.start_run(nested=True)\n----&gt; 3 train(max_epochs, model, optimizer, scheduler, train_loader, valid_loader, project_name)\n      4 mlflow.end_run()\n\n&lt;ipython-input-95-ab0a6c80b65b&gt; in train(max_epochs, model, optimizer, scheduler, train_loader, valid_loader, project_name)\n     55 \n     56         #metrics mlflow\n---&gt; 57         mlflow.log_metrics('train_loss',dict(np.mean(np.array(running_loss))).tolist())\n     58         mlflow.log_metrics('validation_loss',dict(val_loss).tolist())\n     59         mlflow.log_metrics('MAE Val_loss', dict(mae_loss).tolist())\n\nTypeError: 'numpy.float32' object is not iterable\n<\/code><\/pre>",
        "Challenge_closed_time":1653903536587,
        "Challenge_comment_count":1,
        "Challenge_created_time":1653902310097,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a TypeError while logging metrics using mlflow in a machine learning model. The error message states that 'numpy.float32' object is not iterable. The user has tried using .tolist() and dict() but the issue persists.",
        "Challenge_last_edit_time":1653902478312,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72431938",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.0,
        "Challenge_reading_time":40.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":0.3406916667,
        "Challenge_title":"TypeError: 'numpy.float32' object is not iterable when logging in mlflow",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":51.0,
        "Challenge_word_count":221,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651898762636,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":41.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>Youre logging a single value into log_metrics and i dont think thats correct based on the implementation of log_metric and log_metrics in the documentation:<\/p>\n<p><a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_metric\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_metric<\/a> and\n<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_metrics\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_metrics<\/a><\/p>\n<p>So i would suggest to maybe change the &quot;log_metrics&quot; to &quot;log_metric&quot; and leave the tolist out<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1653903894552,
        "Solution_link_count":4.0,
        "Solution_readability":16.6,
        "Solution_reading_time":9.3,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":49.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":42.1575872222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When a sagemaker studio domain is created. An EFS storage is associated with the domain. As the assigned users log into Sagemaker studio, a corresponding home directory is created.<\/p>\n<p>Using a separate EC2 instance, I mounted the EFS storage that was created to try to see whether is it possible to look at each of the individual home domains. I noticed that each of these home directories are shown in terms of numbers (e.g 200000, 200005). Is there a specific rule on how this folders are named? Is it possible to trace the folders back to a particular user or whether this is done by design?<\/p>\n<p>(currently doing exploration on my personal aws account)<\/p>",
        "Challenge_closed_time":1645133944567,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644982177253,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to identify individual user home directories in AWS Sagemaker Studio's EFS storage, which are shown as numbers instead of usernames. They are wondering if there is a specific naming convention or if it is by design, and if it is possible to trace the folders back to a particular user.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71136057",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":8.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":42.1575872222,
        "Challenge_title":"Identifying user from AWS Sagemaker Studio generated EFS storage",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":302.0,
        "Challenge_word_count":122,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1644981356940,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Yes, if you <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/list-user-profiles.html\" rel=\"nofollow noreferrer\">list<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/describe-user-profile.html\" rel=\"nofollow noreferrer\">describe<\/a> the domain users, you'll get back the user's <code>HomeEfsFileSystemUid<\/code> value.<br \/>\nHere's a CLI example:<\/p>\n<pre><code>aws sagemaker describe-user-profile --domain-id d-lcn1vbt47yku --user-profile-name default-1588670743757\n{\n    ...\n    &quot;UserProfileName&quot;: &quot;default-1588670743757&quot;,\n    &quot;HomeEfsFileSystemUid&quot;: &quot;200005&quot;,\n    ...\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":24.0,
        "Solution_reading_time":9.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":38.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":142.3802777778,
        "Challenge_answer_count":0,
        "Challenge_body":"This [notebook](https:\/\/github.com\/Microsoft\/Recommenders\/blob\/master\/notebooks\/04_operationalize\/als_movie_o16n.ipynb) contains a reference to Azure ML SDK preview private index. \r\n\r\n    # Required packages for AzureML execution, history, and data preparation.\r\n    - --extra-index-url https:\/\/azuremlsdktestpypi.azureedge.net\/sdk-release\/Preview\/E7501C02541B433786111FE8E140CAA1\r\n\r\nGiven that Azure ML SDK is now available though regular PyPi as a GA product, and preview versions are unsupported, the extra-index-url should be removed.\r\n",
        "Challenge_closed_time":1548948415000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1548435846000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to open the R package locfit in Azure Machine Learning. They have followed the steps of downloading the package, creating a zip file, and uploading it to AML as a dataset. However, when executing the code, an error message is returned stating that there is no package called 'locfit_package'.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/451",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":18.3,
        "Challenge_reading_time":7.92,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2749.0,
        "Challenge_repo_issue_count":1927.0,
        "Challenge_repo_star_count":15795.0,
        "Challenge_repo_watch_count":264.0,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":142.3802777778,
        "Challenge_title":"Remove azureml sdk preview private PyPi index from operationalize notebook",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":57,
        "Discussion_body":"hey @rastala thanks for the pointer, we are working on updating that notebook to a newer version of databricks and spark. @jreynolds01 is looking at this based on this issue https:\/\/github.com\/Microsoft\/Recommenders\/issues\/427 yes, this should be fixed with my PR. fixed with #438 ",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":30.0886975,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Im trying to learn Azure, with little luck (yet). All the tutorials show using PipelineData just as a file, when configured in &quot;upload&quot; mode. However, im getting &quot;FileNotFoundError: [Errno 2] No such file or directory: ''&quot; error. I would love to ask a more specific question, but i just can't see what im doing wrong.<\/p>\n<pre><code>from azureml.core import Workspace, Datastore,Dataset,Environment\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\nfrom azureml.core.runconfig import RunConfiguration\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.pipeline.steps import PythonScriptStep\nfrom azureml.pipeline.core import Pipeline, PipelineData\nimport os\n\nws = Workspace.from_config()\ndatastore = ws.get_default_datastore()\n\ncompute_name = &quot;cpucluster&quot;\ncompute_target = ComputeTarget(workspace=ws, name=compute_name)\naml_run_config = RunConfiguration()\naml_run_config.target = compute_target\naml_run_config.environment.python.user_managed_dependencies = False\naml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n    conda_packages=['pandas','scikit-learn'], \n    pip_packages=['azureml-sdk', 'azureml-dataprep[fuse,pandas]'], \n    pin_sdk_version=False)\n\noutput1 = PipelineData(&quot;processed_data1&quot;,datastore=datastore, output_mode=&quot;upload&quot;)\nprep_step = PythonScriptStep(\n    name=&quot;dataprep&quot;,\n    script_name=&quot;dataprep.py&quot;,\n    source_directory=os.path.join(os.getcwd(),'dataprep'),\n    arguments=[&quot;--output&quot;, output1],\n    outputs = [output1],\n    compute_target=compute_target,\n    runconfig=aml_run_config,\n    allow_reuse=True\n)\n<\/code><\/pre>\n<p>In the dataprep.py i hve the following:<\/p>\n<pre><code>import numpy, argparse, pandas\nfrom azureml.core import Run\nrun = Run.get_context()\nparser = argparse.ArgumentParser()\nparser.add_argument('--output', dest='output', required=True)\nargs = parser.parse_args()\ndf = pandas.DataFrame(numpy.random.rand(100,3))\ndf.iloc[:, 2] = df.iloc[:,0] + df.iloc[:,1]\nprint(df.iloc[:5,:])\ndf.to_csv(args.output)\n\n<\/code><\/pre>\n<p>So, yeah. pd is supposed to write to the output, but my compute cluster says the following:<\/p>\n<pre><code>&quot;User program failed with FileNotFoundError: [Errno 2] No such file or directory: ''\\&quot;.\n<\/code><\/pre>\n<p>When i dont include the to_csv() function, the cluster does not complain<\/p>",
        "Challenge_closed_time":1626667519372,
        "Challenge_comment_count":2,
        "Challenge_created_time":1626541888290,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to write to Azure PipelineData. They are following tutorials that show using PipelineData as a file in \"upload\" mode, but they are getting a \"FileNotFoundError\" error. The user has shared their code and the error message they are receiving. The error occurs when they try to use the \"to_csv()\" function.",
        "Challenge_last_edit_time":1626559748576,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68422680",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":15.6,
        "Challenge_reading_time":32.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":34.8975227778,
        "Challenge_title":"how to write to Azure PipelineData properly?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":404.0,
        "Challenge_word_count":207,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1588424911652,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":59.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>Here is an <a href=\"https:\/\/github.com\/james-tn\/highperformance_python_in_azure\/blob\/master\/parallel_python_processing\/pipeline_definition.ipynb\" rel=\"nofollow noreferrer\">example<\/a> for PRS.\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.pipelinedata?view=azure-ml-py\" rel=\"nofollow noreferrer\">PipelineData<\/a> was intended to represent &quot;transient&quot; data from one step to the next one, while OutputDatasetConfig was intended for capturing the final state of a dataset (and hence why you see features like lineage, ADLS support, etc). PipelineData always outputs data in a folder structure like {run_id}{output_name}. OutputDatasetConfig allows to decouple the data from the run and hence it allows you to control where to land the data (although by default it will produce similar folder structure). The OutputDatasetConfig allows even to register the output as a Dataset, where getting rid of such folder structure makes sense. From the docs itself: &quot;Represent how to copy the output of a run and be promoted as a FileDataset. The OutputFileDatasetConfig allows you to specify how you want a particular local path on the compute target to be uploaded to the specified destination&quot;.<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-pipeline-batch-scoring-classification#create-dataset-objects\" rel=\"nofollow noreferrer\">OutFileDatasetConfig<\/a> is a control plane concept to pass data between pipeline steps.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1626668067887,
        "Solution_link_count":3.0,
        "Solution_readability":14.5,
        "Solution_reading_time":19.88,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":167.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1378136257732,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Budapest, Hungary",
        "Answerer_reputation_count":8162.0,
        "Answerer_view_count":283.0,
        "Challenge_adjusted_solved_time":0.0478905556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working on Azure ML implementation on text analytics with NLTK, the following execution is throwing <\/p>\n\n<pre><code>AssertionError: 1 columns passed, passed data had 2 columns\\r\\nProcess returned with non-zero exit code 1\n<\/code><\/pre>\n\n<p>Below is the code <\/p>\n\n<pre><code># The script MUST include the following function,\n# which is the entry point for this module:\n# Param&lt;dataframe1&gt;: a pandas.DataFrame\n# Param&lt;dataframe2&gt;: a pandas.DataFrame\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    # import required packages\n    import pandas as pd\n    import nltk\n    import numpy as np\n    # tokenize the review text and store the word corpus\n    word_dict = {}\n    token_list = []\n    nltk.download(info_or_id='punkt', download_dir='C:\/users\/client\/nltk_data')\n    nltk.download(info_or_id='maxent_treebank_pos_tagger', download_dir='C:\/users\/client\/nltk_data')\n    for text in dataframe1[\"tweet_text\"]:\n        tokens = nltk.word_tokenize(text.decode('utf8'))\n        tagged = nltk.pos_tag(tokens)\n\n\n      # convert feature vector to dataframe object\n    dataframe_output = pd.DataFrame(tagged, columns=['Output'])\n    return [dataframe_output]\n<\/code><\/pre>\n\n<p>Error is throwing here <\/p>\n\n<pre><code> dataframe_output = pd.DataFrame(tagged, columns=['Output'])\n<\/code><\/pre>\n\n<p>I suspect this to be the tagged data type passed to dataframe, can some one let me know the right approach to add this to dataframe.<\/p>",
        "Challenge_closed_time":1471040769603,
        "Challenge_comment_count":0,
        "Challenge_created_time":1471040597197,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary_original":"The user is encountering an AssertionError while working on Azure ML implementation on text analytics with NLTK. The error message states that 1 column was passed, but the passed data had 2 columns. The error is being thrown at the line where the user is trying to convert a feature vector to a dataframe object. The user suspects that the issue is with the tagged data type passed to the dataframe.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38927230",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":18.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.0478905556,
        "Challenge_title":"Panda AssertionError columns passed, passed data had 2 columns",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":48200.0,
        "Challenge_word_count":158,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1370924418390,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":1748.0,
        "Poster_view_count":339.0,
        "Solution_body":"<p>Try this:<\/p>\n\n<pre><code>dataframe_output = pd.DataFrame(tagged, columns=['Output', 'temp'])\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.4,
        "Solution_reading_time":1.5,
        "Solution_score_count":13.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":7.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1431288135943,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"England",
        "Answerer_reputation_count":121.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":269.4953261111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run a mlflow tracking server that is installed inside of a virtualenv as a systemd service on Ubuntu 20.04 but I am getting an error indicating that it is unable to find gunicorn. Here is my journal<\/p>\n<pre><code>nov 27 10:37:17 Atrium-Power mlflow[81375]: Traceback (most recent call last):\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/bin\/mlflow&quot;, line 8, in &lt;module&gt;\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     sys.exit(cli())\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 829, in __call__\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     return self.main(*args, **kwargs)\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 782, in main\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     rv = self.invoke(ctx)\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 1259, in invoke\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     return _process_result(sub_ctx.command.invoke(sub_ctx))\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 1066, in invoke\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     return ctx.invoke(self.callback, **ctx.params)\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 610, in invoke\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     return callback(*args, **kwargs)\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/mlflow\/cli.py&quot;, line 392, in server\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     _run_server(\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/mlflow\/server\/__init__.py&quot;, line 138, in _run_server\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     exec_cmd(full_command, env=env_map, stream_output=True)\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/mlflow\/utils\/process.py&quot;, line 34, in exec_cmd\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     child = subprocess.Popen(\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/subprocess.py&quot;, line 947, in __init__\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     self._execute_child(args, executable, preexec_fn, close_fds,\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/subprocess.py&quot;, line 1819, in _execute_child\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     raise child_exception_type(errno_num, err_msg, err_filename)\nnov 27 10:37:17 Atrium-Power mlflow[81375]: FileNotFoundError: [Errno 2] No such file or directory: 'gunicorn'\n<\/code><\/pre>\n<p>and my systemd is this:<\/p>\n<pre><code>[Unit]\nStartLimitBurst=5\nStartLimitIntervalSec=33\n\n[Service]\nUser=praxasense\nWorkingDirectory=\/home\/praxasense\nRestart=always\nRestartSec=5\nExecStart=\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/bin\/mlflow server --port 3569 --backend-store-uri .mlruns\n\n[Install]\nWantedBy=multi-user.target\n<\/code><\/pre>\n<p>The strange thing is that if I run the command from <code>ExecStart<\/code> in my terminal it works fine in fish shell, but not in bash, <em>but<\/em> if I do <code>conda activate mlflow-server<\/code> and then do <code>mlflow ...<\/code> it <em>does<\/em> work. As far as I understood the Python interpreter should be aware of it's virtual environment and so it should work as I tried it, but apparently I am missing something that makes it not able to find the gunicon package, which is obviously there.<\/p>\n<p>Any ideas?<\/p>",
        "Challenge_closed_time":1607442042567,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606471859393,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to run a mlflow tracking server as a systemd service on Ubuntu 20.04. The error indicates that gunicorn is not found. The user has provided their journal and systemd files, and has noted that running the command from ExecStart in the terminal works fine in fish shell but not in bash. The user is unsure why the Python interpreter is not able to find the gunicorn package, which is present in the virtual environment.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65035488",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":55.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":269.4953261111,
        "Challenge_title":"Running mlflow as a systemd service - gunicorn not found",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1018.0,
        "Challenge_word_count":386,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1314532759332,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Rotterdam, Netherlands",
        "Poster_reputation_count":2732.0,
        "Poster_view_count":609.0,
        "Solution_body":"<p>Try adding the venv's bin path to the environment that systemd runs in:<\/p>\n<pre><code>[Service]\n...\nEnvironment=&quot;PATH=\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/bin&quot;\n...\n<\/code><\/pre>\n<p>I also recommend setting <code>KillMode=mixed<\/code>, since MLFlow will spawn gunicorn instances that won't be terminated if you terminate the service otherwise. <code>mixed<\/code> means that child processes will also be terminated.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.0,
        "Solution_reading_time":5.8,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":46.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1338385871600,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":118.0,
        "Answerer_view_count":52.0,
        "Challenge_adjusted_solved_time":49.5372508333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using the Azure ML model available at <a href=\"https:\/\/gallery.azure.ai\/Experiment\/Weather-prediction-model-1\" rel=\"nofollow noreferrer\">https:\/\/gallery.azure.ai\/Experiment\/Weather-prediction-model-1<\/a> to design a prediction mechanism based on temperature and humidity. I haven't done any changes to the existing model and feeding in data from a simulator. The prediction output is stuck at 0.489944100379944. I have taken over 17k samples and still, the prediction is constant at this value. <\/p>\n\n<p>Any help will be highly appreciated.<\/p>\n\n<p><em>N.B. - This is my first stint with ML<\/em><\/p>",
        "Challenge_closed_time":1525107860376,
        "Challenge_comment_count":10,
        "Challenge_created_time":1524929526273,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with the Azure ML model available at https:\/\/gallery.azure.ai\/Experiment\/Weather-prediction-model-1. The prediction output is stuck at 0.489944100379944 despite feeding in over 17k samples. The user seeks help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50078161",
        "Challenge_link_count":2,
        "Challenge_participation_count":11,
        "Challenge_readability":10.3,
        "Challenge_reading_time":8.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":49.5372508333,
        "Challenge_title":"Azure ML Prediction Is Constant",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":163.0,
        "Challenge_word_count":80,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1338385871600,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":118.0,
        "Poster_view_count":52.0,
        "Solution_body":"<p>This was caused by the training dataset. The dataset had characters in the humidity and temperature columns. This led to the model expecting characters but operating on floating point numbers. I cleaned the dataset and ensured that there are only floats in the temperature and humidity columns. Then I used this training data for the model and phew!!!! Everything's working now. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.3,
        "Solution_reading_time":4.77,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":62.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1576813179640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":39.3623194445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a data set with about 7999 attributes and 39 labels, with 3339 total observations (resulting in 3339x8038 data set), and I'm trying to upload id to Azure ML platform.\nI've selected the 'type' as 'tabular', encoding as 'utf-8', no row skipping, and use header from first file.\nThe problem is, that the headers are still not included and the data is interpreted as string with 0s, 1s, and commas (see pic <a href=\"https:\/\/imgur.com\/a\/QdQNt1y\" rel=\"nofollow noreferrer\">https:\/\/imgur.com\/a\/QdQNt1y<\/a>)<\/p>\n\n<p>Am I missing something? For smaller data sets it seemed to work. My headers are A1, ... A7999 for the attributes, and L1, ... L39 for the labels.<\/p>\n\n<p>Thanks for help in advance.<\/p>",
        "Challenge_closed_time":1576813812910,
        "Challenge_comment_count":0,
        "Challenge_created_time":1576672108560,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with the Azure ML platform where the uploaded dataset with 7999 attributes and 39 labels is being wrongly interpreted as a string with 0s, 1s, and commas, despite selecting the 'type' as 'tabular', encoding as 'utf-8', no row skipping, and using the header from the first file. The headers are not included in the interpreted data. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59392060",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":6.2,
        "Challenge_reading_time":9.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":39.3623194445,
        "Challenge_title":"Azure ML platform wrongly interprets uploaded dataset",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":49.0,
        "Challenge_word_count":114,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1527091507808,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Stockholm, Sweden",
        "Poster_reputation_count":235.0,
        "Poster_view_count":43.0,
        "Solution_body":"<p>our system does our best guess over file settings when you try to create a dataset, but cannot guarantee perfect guesses in all cases. <\/p>\n\n<p>In such scenarios, you should be able to adjust the settings. We had a bug with the ability to change those settings, but rolled out a fix. Can you try to change those now?  <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":3.88,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":60.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1491467888608,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":381.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":20.1632008333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working with weight and bias(wandb).<br \/>\nHowever, it logs by step. And that makes plot disturbing when comparing runs.<br \/>\nFor example, I have a run A and run B(assume that they run with same dataset).<br \/>\nrun A: 30epochs, 4 batch, 200step\/epoch<br \/>\nrun B: 30epochs, 8 batch, 100step\/epoch<\/p>\n<p>then, the plot of run A gets longer(double, in this case) in axis x when it shows with run B.<\/p>\n<p>How can I scale x axis depend to runs AFTER training?<\/p>",
        "Challenge_closed_time":1633345375676,
        "Challenge_comment_count":0,
        "Challenge_created_time":1633272788153,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge with weight and bias (wandb) where the plots are disturbing when comparing runs because it logs by step. The user wants to know if there is any way to scale the x-axis of plots in wandb depending on the runs after training.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69425994",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.7,
        "Challenge_reading_time":6.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":20.1632008333,
        "Challenge_title":"is there any way to scale axis of plots in wandb?",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":604.0,
        "Challenge_word_count":89,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1603378831587,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":161.0,
        "Poster_view_count":29.0,
        "Solution_body":"<p>You can change the x-axis used via the chart settings by clicking on the pencil icon and then selecting a different x-axis. E.g. in your case you could select &quot;epoch&quot; instead of &quot;steps&quot;. Just make sure to log &quot;epoch&quot; to your charts, something like:<\/p>\n<pre><code>steps_per_epoch = n_samples \/ batch_size\nepoch = current_step \/ steps_per_epoch\nwandb.log({&quot;epoch&quot;:epoch, ...})\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.9,
        "Solution_reading_time":5.52,
        "Solution_score_count":4.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":52.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":94.4830555556,
        "Challenge_answer_count":0,
        "Challenge_body":"```Azure ML SDK Version:  1.11.0```\r\n\r\nIn a ```PythonScriptStep``` I'm getting a crash error that: \"\r\n```\r\nazureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\r\n```\r\n\r\nHere is my RunConfiguration:\r\n```\r\ncompute_target = ComputeTarget(workspace=f.ws, name=compute_name)\r\n\r\ncd = CondaDependencies.create(\r\n    pip_packages=[\"pandas\", \"numpy\",\r\n                  \"azureml-defaults\", \"azureml-sdk[explain,automl]\", \"azureml-train-automl-runtime\"],\r\n    conda_packages=[\"xlrd\", \"scikit-learn\", \"numpy\", \"pyyaml\", \"pip\"])\r\namlcompute_run_config = RunConfiguration(conda_dependencies=cd)\r\namlcompute_run_config.environment.docker.enabled = True\r\n```\r\n\r\nhere is the step:\r\n```\r\nadd_vendor_sets = PythonScriptStep(\r\n    name='Add Vendor set',\r\n    script_name='add_vendor_set.py',\r\n    arguments=['--respondent_dir', level_respondent,\r\n                '--my_dir', my_raw,\r\n                '--output_dir', factset_processed],\r\n    compute_target=compute_target,\r\n    inputs=[level_respondent, my_raw],\r\n    outputs=[my_processed],\r\n    runconfig=amlcompute_run_config,\r\n    source_directory=os.path.join(os.getcwd(), 'pipes\/add_vendor_set'),\r\n    allow_reuse=True\r\n)\r\n```\r\n\r\nThe environment is obviously included, but also definitely missing.  I'm stuck and now none of my pipelines, that were running in previous version, will work. \r\n\r\n",
        "Challenge_closed_time":1598387113000,
        "Challenge_comment_count":5,
        "Challenge_created_time":1598046974000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where version history is not maintained when pulling data from an Azure SQL DB or DW into Azure ML datasets. Only the first version is refreshed every time new data is pulled. The user has provided a reproducible example to explain the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1111",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":18.0,
        "Challenge_reading_time":18.26,
        "Challenge_repo_contributor_count":58.0,
        "Challenge_repo_fork_count":2387.0,
        "Challenge_repo_issue_count":1906.0,
        "Challenge_repo_star_count":3704.0,
        "Challenge_repo_watch_count":2001.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":94.4830555556,
        "Challenge_title":"error: azureml-train-automl-runtime is required however it is included",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":115,
        "Discussion_body":"can you share the full stacktrace? and is the error happening when you submit the pipeline script? or is it happening in the logs of the `PythonScriptStep`? ```\r\n\"error\": {\r\n        \"code\": \"UserError\",\r\n        \"message\": \"azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\",\r\n        \"detailsUri\": \"https:\/\/aka.ms\/azureml-known-errors\",\r\n        \"details\": [],\r\n        \"debugInfo\": {\r\n            \"type\": \"UserScriptException\",\r\n            \"message\": \"UserScriptException:\\n\\tMessage: azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\\n\\tInnerException OptionalDependencyMissingException:\\n\\tMessage: azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\\n\\tInnerException: None\\n\\tErrorResponse \\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"inner_error\\\": {\\n            \\\"code\\\": \\\"ValidationError\\\",\\n            \\\"inner_error\\\": {\\n                \\\"code\\\": \\\"ScenarioNotSuported\\\",\\n                \\\"inner_error\\\": {\\n                    \\\"code\\\": \\\"OptionalDependencyMissing\\\"\\n                }\\n            }\\n        },\\n        \\\"message\\\": \\\"azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\\\"\\n    }\\n}\\n\\tErrorResponse \\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\\\"\\n    }\\n}\",\r\n            \"stackTrace\": \"  File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/pjx-d-cu1-mlw-models\/azureml\/d881de2a-9dba-4e74-805e-d3c6eaab2076\/mounts\/workspaceblobstore\/azureml\/d881de2a-9dba-4e74-805e-d3c6eaab2076\/azureml-setup\/context_manager_injector.py\\\", line 197, in execute_with_context\\n    raise UserScriptException(baseEx).with_traceback(exceptionInfo[2])\\n  File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/pjx-d-cu1-mlw-models\/azureml\/d881de2a-9dba-4e74-805e-d3c6eaab2076\/mounts\/workspaceblobstore\/azureml\/d881de2a-9dba-4e74-805e-d3c6eaab2076\/azureml-setup\/context_manager_injector.py\\\", line 166, in execute_with_context\\n    runpy.run_path(sys.argv[0], globals(), run_name=\\\"__main__\\\")\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/runpy.py\\\", line 263, in run_path\\n    pkg_name=pkg_name, script_name=fname)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/runpy.py\\\", line 96, in _run_module_code\\n    mod_name, mod_spec, pkg_name, script_name)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/runpy.py\\\", line 85, in _run_code\\n    exec(code, run_globals)\\n  File \\\"run_models.py\\\", line 286, in \\n    main()\\n  File \\\"run_models.py\\\", line 197, in main\\n    run = experiment.submit(config=automl_config, tags=tags)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/core\/experiment.py\\\", line 211, in submit\\n    run = submit_func(config, self.workspace, self.name, **kwargs)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\\\", line 97, in _automl_static_submit\\n    show_output)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\\\", line 255, in _start_execution\\n    automl_run = _default_execution(experiment, settings_obj, fit_params, True, show_output, parent_run_id)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\\\", line 121, in _default_execution\\n    return automl_estimator.fit(**fit_params)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/_azureautomlclient.py\\\", line 349, in fit\\n    \\\"azureml-train-automl-runtime must be installed in the current environment to run local in \\\"\\n\"\r\n        },\r\n        \"messageFormat\": \"azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\",\r\n        \"messageParameters\": {}\r\n    },\r\n    \"time\": \"0001-01-01T00:00:00.000Z\"\r\n}\r\n``` Here is my stack trace from the 70_driver_log.txt:\r\n```\r\nTraceback (most recent call last):\r\n  File \"run_models.py\", line 286, in <module>\r\n    main()\r\n  File \"run_models.py\", line 197, in main\r\n    run = experiment.submit(config=automl_config, tags=tags)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/core\/experiment.py\", line 211, in submit\r\n    run = submit_func(config, self.workspace, self.name, **kwargs)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\", line 97, in _automl_static_submit\r\n    show_output)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\", line 255, in _start_execution\r\n    automl_run = _default_execution(experiment, settings_obj, fit_params, True, show_output, parent_run_id)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\", line 121, in _default_execution\r\n    return automl_estimator.fit(**fit_params)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/_azureautomlclient.py\", line 349, in fit\r\n    \"azureml-train-automl-runtime must be installed in the current environment to run local in \"\r\nUserScriptException: UserScriptException:\r\n\tMessage: azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\r\n``` @swatig007 this is an error, @BillmanH is experiencing when submitting an AutoML run from within a `PythonScriptStep` rather than using an `AutoMLStep`. This approach worked for over a year, but is now throwing an error about `azureml-train-automl-runtime` not being installed. upgraded to 1.12.0, which solved this problem and opened other issues. ",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1280505139752,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bangalore, India",
        "Answerer_reputation_count":4265.0,
        "Answerer_view_count":403.0,
        "Challenge_adjusted_solved_time":0.0257813889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Looks like AzureML Python SDK has two Dataset packages exposed over API:<\/p>\n<ol>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.tabulardataset?view=azure-ml-py\" rel=\"nofollow noreferrer\">azureml.contrib.dataset<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-contrib-dataset\/azureml.contrib.dataset.tabulardataset?view=azure-ml-py\" rel=\"nofollow noreferrer\">azureml.data<\/a><\/li>\n<\/ol>\n<p>The documentation doesn't clearly mention the difference or when should we use which one? But, it creates confusion for sure. For example, There are two Tabular Dataset classes exposed over API. And they have different APIs for different functions:<\/p>\n<ol>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.tabulardataset?view=azure-ml-py\" rel=\"nofollow noreferrer\">azureml.data.TabularDataset<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-contrib-dataset\/azureml.contrib.dataset.tabulardataset?view=azure-ml-py\" rel=\"nofollow noreferrer\">azureml.contrib.dataset.TabularDataset<\/a><\/li>\n<\/ol>\n<p>Any suggestion about when should I use which package will be helpful.<\/p>",
        "Challenge_closed_time":1645168074896,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645165311677,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is confused about the difference between two Dataset packages, azureml.contrib.dataset and azureml.data, in the AzureML Python SDK. The documentation does not clearly explain when to use which package, and there are two Tabular Dataset classes with different APIs for different functions. The user is seeking suggestions on when to use which package.",
        "Challenge_last_edit_time":1645167982083,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71169178",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":17.6,
        "Challenge_reading_time":16.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":0.7675608333,
        "Challenge_title":"azureml.contrib.dataset vs azureml.data",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":24.0,
        "Challenge_word_count":85,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1280505139752,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, India",
        "Poster_reputation_count":4265.0,
        "Poster_view_count":403.0,
        "Solution_body":"<p>As per the <a href=\"https:\/\/pypi.org\/project\/azureml-contrib-dataset\/\" rel=\"nofollow noreferrer\">PyPi<\/a>, <code>azureml.contrib.dataset<\/code> has been deprecated and <code>azureml.data<\/code> should be used instead:<\/p>\n<blockquote>\n<p>The azureml-contrib-dataset package has been deprecated and might not\nreceive future updates and removed from the distribution altogether.\nPlease use azureml-core instead.<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.6,
        "Solution_reading_time":5.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":41.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1259808393296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, Canada",
        "Answerer_reputation_count":44706.0,
        "Answerer_view_count":4356.0,
        "Challenge_adjusted_solved_time":120.6901825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In sagemaker, the docs talk about inference scripts requiring to have 4 specific functions. When we get a prediction, the python SDK sends a request to the endpoint.<\/p>\n<p>Then the inference script runs. But I cannot find where in the SDK the inference script is run.<\/p>\n<p>When I navigate through the sdk code the <code>Predictor.predict()<\/code> method calls the sagemaker session to post a request to the endpoint and get a response. That is the final step in the sdk. Sagemaker is obviously doing something when it receives that request.<\/p>\n<p>What is the code that it runs?<\/p>",
        "Challenge_closed_time":1646329084320,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646326739290,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to understand the process of getting a prediction from a Sagemaker endpoint. They are aware of the 4 specific functions required in the inference script and have noticed that the <code>Predictor.predict()<\/code> method calls the Sagemaker session to post a request to the endpoint. However, they are unable to find where the inference script is run and are curious about the code that Sagemaker runs when it receives the request.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71340893",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.1,
        "Challenge_reading_time":8.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.6513972222,
        "Challenge_title":"When I get a prediction from sagemaker endpoint, what does the endpoint do?",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":405.0,
        "Challenge_word_count":108,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1578932319743,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Ireland",
        "Poster_reputation_count":1012.0,
        "Poster_view_count":66.0,
        "Solution_body":"<p>The endpoint is essentially a Flask web server running in a Docker container<\/p>\n<p>If it's a scikit-learn image, when you invoke the endpoint, it loads your script from S3, then...<\/p>\n<p>It calls <code>input_fn(request_body: bytearray, content_type) -&gt; np.ndarray<\/code> to parse the <code>request_body<\/code> into a numpy array<\/p>\n<p>Then it calls your <code>model_fn(model_dir: str) -&gt; object<\/code> function to load the model from <code>model_dir<\/code> and return the model<\/p>\n<p>Then it calls <code>predict_fn(input_object: np.ndarray, model: object) -&gt; np.array<\/code>, which calls your <code>model.predict()<\/code> function and returns the prediction<\/p>\n<p>Then it calls <code>output_fn(prediction: np.array, accept: str)<\/code> to take the result from <code>predict_fn<\/code> and encode it to the <code>accept<\/code> type<\/p>\n<p>You don't need to implement all of these functions yourself, as there are defaults<\/p>\n<p>You <strong>do<\/strong> need to implement <code>model_fn<\/code><\/p>\n<p>You only need to implement <code>input_fn<\/code> if you have non numeric data<\/p>\n<p>You only need to implement <code>predict_fn<\/code> if your model uses something other than <code>.predict()<\/code><\/p>\n<p>You can see how the default function implementations work <a href=\"https:\/\/github.com\/aws\/sagemaker-scikit-learn-container\/blob\/master\/src\/sagemaker_sklearn_container\/serving.py\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1646761223947,
        "Solution_link_count":1.0,
        "Solution_readability":13.8,
        "Solution_reading_time":18.79,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":161.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1250158552416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Challenge_adjusted_solved_time":17.1400166667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to know what is the difference between <code>feature numeric<\/code> and <code>numeric<\/code> columns in Azure Machine Learning Studio.<\/p>\n\n<p>The <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/edit-metadata\" rel=\"nofollow noreferrer\">documentation site<\/a> states: <\/p>\n\n<blockquote>\n  <p>Because all columns are initially treated as features, for modules\n  that perform mathematical operations, you might need to use this\n  option to prevent numeric columns from being treated as variables.<\/p>\n<\/blockquote>\n\n<p>But nothing more. Not what a feature is, in which modules you need features. Nothing. <\/p>\n\n<p>I specifically would like to understand if the <code>clear feature<\/code> dropdown option in the <code>fields<\/code> in the <code>edit metadata<\/code>-module has any effect. Can somebody give me a szenario where this <code>clear feature<\/code>-operation changes the ML outcome? Thank you<\/p>\n\n<p>According to the documentation in ought to have an effect:<\/p>\n\n<blockquote>\n  <p>Use the Fields option if you want to change the way that Azure Machine\n  Learning uses the data in a model.<\/p>\n<\/blockquote>\n\n<p>But what can this effect be? Any example might help<\/p>",
        "Challenge_closed_time":1538116098500,
        "Challenge_comment_count":0,
        "Challenge_created_time":1538054010173,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on the difference between \"feature numeric\" and \"numeric\" columns in Azure Machine Learning Studio. They are also unsure about the purpose of the \"clear feature\" dropdown option in the \"edit metadata\" module and how it affects the ML outcome. The documentation provides limited information on these topics, and the user is looking for specific scenarios or examples.",
        "Challenge_last_edit_time":1538054394440,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52537861",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.3,
        "Challenge_reading_time":16.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":17.2467575,
        "Challenge_title":"What is the role of feature type in AzureML?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":70.0,
        "Challenge_word_count":164,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1368739128832,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Riga, Latvia",
        "Poster_reputation_count":1763.0,
        "Poster_view_count":380.0,
        "Solution_body":"<p>As you suspect, setting a column as <code>feature<\/code> does have an effect, and it's actually quite important - when training a model, the algorithms will only take into account columns with the <code>feature<\/code> flag, effectively ignoring the others. <\/p>\n\n<p>For example, if you have a dataset with columns <code>Feature1<\/code>, <code>Feature2<\/code>, and <code>Label<\/code> and you want to try out just <code>Feature1<\/code>, you would apply <code>clear feature<\/code> to the <code>Feature2<\/code> column (while making sure that <code>Feature1<\/code> has the <code>feature<\/code> label set, of course).<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":21.3,
        "Solution_reading_time":7.89,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":80.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2392.2144444444,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nIt is not possible to store a ``PartitionedDataSet`` as an mlflow artifact with the ``MlflowArtifactDataSet``.\r\n\r\n## Context\r\n\r\nI had a use case where I need to save a dict with many small result tables to mlflow, and I tried to use ``PartitionedDataSet`` for this.\r\n\r\n## Steps to Reproduce\r\n\r\n```yaml\r\n# catalog.yml\r\n\r\nmy_dataset:\r\n    type: kedro_mlflow.io.artifacts.MlflowArtifactDataSet\r\n    data_set:\r\n        type: PartitionedDataSet  # or any valid kedro DataSet\r\n        path: \/path\/to\/a\/local\/folder # the attribute is \"path\", and not \"filepath\"!\r\n        dataset: \"pandas.CSVDataSet\"\r\n```\r\n\r\nthen save a dict using this dataset:\r\n\r\n```\r\ncatalog.save(\"my_dataset\", dict(\"a\": pd.DataFrame(data=[1,2,3], columns=[\"a\"], \"b\": pd.DataFrame(data=[1,2,3], columns=[\"b\"])\r\n```\r\n## Expected Result\r\n\r\nThe 2 Dataframes should be logged as artifacts in the current mlflow run.\r\n\r\n## Actual Result\r\n\r\nAn error ``dataset has not attribute \"_filepath\"`` is raised.\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Potential solution\r\n\r\nThe error comes from this line:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/904207ad505b71391d78d8088aaed151ca6a011d\/kedro_mlflow\/io\/artifacts\/mlflow_artifact_dataset.py#L53\r\n\r\nmaybe we can add a better condition here to default to \"path\" if there is no \"filepath\" attribute.",
        "Challenge_closed_time":1644674290000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636062318000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to store a PartitionedDataSet as an mlflow artifact with the MlflowArtifactDataSet. The user tried to save a dict with many small result tables to mlflow using PartitionedDataSet, but an error \"dataset has not attribute '_filepath'\" was raised. The bug also happens with the last version on master. A potential solution is to add a better condition to default to \"path\" if there is no \"filepath\" attribute.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/258",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":7.6,
        "Challenge_reading_time":17.08,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":21.0,
        "Challenge_repo_issue_count":414.0,
        "Challenge_repo_star_count":145.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":2392.2144444444,
        "Challenge_title":"MlflowArtifactDataSet does not work with PartitionedDataSet",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":156,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":31.5833333333,
        "Challenge_answer_count":2,
        "Challenge_body":"Under the Vertex AI - a dataset failed to create due to a constraint applied to the organization. It does not allow for the deletion of the dataset, I attempted using python (Delete a dataset \u00a0|\u00a0 Vertex AI \u00a0|\u00a0 Google Cloud) and the response was -\u00a0\"...is in failure state and cannot be deleted. It will be deleted automatically after a few days.\"\u00a0\u00a0but it didn't delete. There is not a gcloud command to correct. Short of a support request..how can the dataset be removed as I foresee this occuring as others attempt experiments. I have addressed the issue with the constraint.",
        "Challenge_closed_time":1677681300000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1677567600000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to delete a failed dataset in Vertex AI due to a constraint applied to the organization. The user attempted to delete the dataset using Python and received a response that it will be deleted automatically after a few days, but it did not delete. The user is seeking a solution to remove the dataset without a support request.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Deleting-a-failed-dataset\/m-p\/527077#M1360",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":4.9,
        "Challenge_reading_time":7.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":31.5833333333,
        "Challenge_title":"Deleting a failed dataset",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":96.0,
        "Challenge_word_count":101,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I tried running the same\u00a0code you used and I was able to delete a dataset that was successfully created. I suspect in your case, the failure state of the dataset is the problem. Also, there is indeed no gcloud command to manually delete it. I would still suggest you file a\u00a0ticket here so\u00a0Google Cloud's engineering team can further investigate.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.2,
        "Solution_reading_time":4.55,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":67.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1336422648776,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Barcelona, Spain",
        "Answerer_reputation_count":171.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":949.2684408333,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I'd used MLflow and logged parameters using the function below (from pydataberlin).<\/p>\n<pre><code>def train(alpha=0.5, l1_ratio=0.5):\n    # train a model with given parameters\n    warnings.filterwarnings(&quot;ignore&quot;)\n    np.random.seed(40)\n\n    # Read the wine-quality csv file (make sure you're running this from the root of MLflow!)\n    data_path = &quot;data\/wine-quality.csv&quot;\n    train_x, train_y, test_x, test_y = load_data(data_path)\n\n    # Useful for multiple runs (only doing one run in this sample notebook)    \n    with mlflow.start_run():\n        # Execute ElasticNet\n        lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n        lr.fit(train_x, train_y)\n\n        # Evaluate Metrics\n        predicted_qualities = lr.predict(test_x)\n        (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n\n        # Print out metrics\n        print(&quot;Elasticnet model (alpha=%f, l1_ratio=%f):&quot; % (alpha, l1_ratio))\n        print(&quot;  RMSE: %s&quot; % rmse)\n        print(&quot;  MAE: %s&quot; % mae)\n        print(&quot;  R2: %s&quot; % r2)\n\n        # Log parameter, metrics, and model to MLflow\n        mlflow.log_param(key=&quot;alpha&quot;, value=alpha)\n        mlflow.log_param(key=&quot;l1_ratio&quot;, value=l1_ratio)\n        mlflow.log_metric(key=&quot;rmse&quot;, value=rmse)\n        mlflow.log_metrics({&quot;mae&quot;: mae, &quot;r2&quot;: r2})\n        mlflow.log_artifact(data_path)\n        print(&quot;Save to: {}&quot;.format(mlflow.get_artifact_uri()))\n        \n        mlflow.sklearn.log_model(lr, &quot;model&quot;)\n<\/code><\/pre>\n<p>Once I run <code>train()<\/code> with its parameters, in UI I cannot see Artifacts, but I can see models and its parameters and Metric.<\/p>\n<p>In artifact tab it's written <code>No Artifacts Recorded Use the log artifact APIs to store file outputs from MLflow runs.<\/code> But in finder in models folders all Artifacts existe with models Pickle.<\/p>\n<p>help<\/p>",
        "Challenge_closed_time":1593697638320,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590280271933,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has encountered an issue with MLflow where Artifacts are not showing up in the UI even though they have been logged using the log artifact API. However, the Artifacts can be found in the models folder as Pickle files. The user is seeking help to fix this issue.",
        "Challenge_last_edit_time":1656334439607,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61980244",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":11.1,
        "Challenge_reading_time":23.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":949.2684408333,
        "Challenge_title":"How to fix Artifacts not showing in MLflow UI",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":8044.0,
        "Challenge_word_count":185,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1500490643012,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"France",
        "Poster_reputation_count":722.0,
        "Poster_view_count":290.0,
        "Solution_body":"<p>Had a similar issue. In my case, I solved it by running <code>mlflow ui<\/code> inside the <code>mlruns<\/code> directory of your experiment.<\/p>\n<p>See the full discussion on Github <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/3030\" rel=\"nofollow noreferrer\">here<\/a><\/p>\n<p>Hope it helps!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1593982070672,
        "Solution_link_count":1.0,
        "Solution_readability":7.8,
        "Solution_reading_time":3.91,
        "Solution_score_count":4.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":34.0,
        "Tool":"MLflow"
    }
]