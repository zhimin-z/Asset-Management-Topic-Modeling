[
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":12402.8375,
        "Challenge_answer_count":0,
        "Challenge_body":"**Environment**:\r\n- NNI version: 2.0\r\n- NNI mode (local|remote|pai): remote\r\n- Client OS: Windows 10\r\n- Server OS (for remote mode only): Linux\r\n- Python version: 3.6.12\r\n- PyTorch\/TensorFlow version:  PyTorch1.7.1\r\n- Is conda\/virtualenv\/venv used?: conda\r\n- Is running in Docker?: No\r\n\r\n**Log message**:\r\n - nnimanager.log: \r\n [2021-04-07 15:24:48] INFO [ 'Datastore initialization done' ]\r\n[2021-04-07 15:24:48] INFO [ 'RestServer start' ]\r\n[2021-04-07 15:24:48] INFO [ 'RestServer base port is 8086' ]\r\n[2021-04-07 15:24:48] INFO [ 'Rest server listening on: http:\/\/0.0.0.0:8086' ]\r\n[2021-04-07 15:24:51] INFO [ 'NNIManager setClusterMetadata, key: aml_config, value: {\"subscriptionId\":\"xxxxxxxxxxxx\",\"resourceGroup\":\"xxxxxxxxxxxxxxx\",\"workspaceName\":\"xxxxxxxxxxxxxx\",\"computeTarget\":\"xxxxxxxxxxxxxxxx\"}' ]\r\n[2021-04-07 15:24:53] INFO [ 'NNIManager setClusterMetadata, key: nni_manager_ip, value: {\"nniManagerIp\":\"10.194.188.18\"}' ]\r\n[2021-04-07 15:24:55] INFO [ 'NNIManager setClusterMetadata, key: trial_config, value: {\"command\":\"python3 mnist.py\",\"codeDir\":\"C:\\\\\\\\Users\\\\\\\\yanmi\\\\\\\\nni\\\\\\\\examples\\\\\\\\trials\\\\\\\\mnist-pytorch\\\\\\\\.\",\"image\":\"msranni\/nni\"}' ]\r\n[2021-04-07 15:24:57] INFO [ 'Starting experiment: fy8bAx3K' ]\r\n[2021-04-07 15:24:57] INFO [ 'Change NNIManager status from: INITIALIZED to: RUNNING' ]\r\n[2021-04-07 15:24:57] INFO [ 'Add event listeners' ]\r\n[2021-04-07 15:24:57] INFO [ 'TrialDispatcher: started channel: AMLCommandChannel' ]\r\n[2021-04-07 15:24:57] INFO [ 'TrialDispatcher: copying code and settings.' ]\r\n[2021-04-07 15:25:06] INFO [ 'NNIManager received command from dispatcher: ID, ' ]\r\n[2021-04-07 15:25:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 0, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 128, \"lr\": 0.1, \"momentum\": 0.754420685055723}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:25:07] INFO [ 'Initialize environments total number: 0' ]\r\n[2021-04-07 15:25:07] INFO [ 'TrialDispatcher: run loop started.' ]\r\n[2021-04-07 15:25:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":0,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 0, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.754420685055723}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:25:12] INFO [ 'Assign environment service aml to environment XlEgg' ]\r\n[2021-04-07 15:25:24] INFO [ 'requested environment nni_exp_fy8bAx3K_1617834318_1a1683cd and job id is nni_exp_fy8bAx3K_env_XlEgg.' ]\r\n[2021-04-07 15:25:24] INFO [ 'requested new environment, live trials: 1, live environments: 0, neededEnvironmentCount: 1, requestedCount: 1' ]\r\n[2021-04-07 15:25:42] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from UNKNOWN to WAITING.' ]\r\n[2021-04-07 15:28:27] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from WAITING to RUNNING.' ]\r\n[2021-04-07 15:29:35] INFO [ 'TrialDispatcher: env nni_exp_fy8bAx3K_1617834318_1a1683cd received initialized message and runner is ready, env status: RUNNING.' ]\r\n[2021-04-07 15:29:35] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial KH7Ph.' ]\r\n[2021-04-07 15:29:36] INFO [ 'Trial job KH7Ph status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:34:06] INFO [ 'Trial job KH7Ph status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:34:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 1, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 128, \"lr\": 0.001, \"momentum\": 0.48989819362825704}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:34:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":1,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 1, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.001, \\\\\"momentum\\\\\": 0.48989819362825704}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:34:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial Uh6jK.' ]\r\n[2021-04-07 15:34:16] INFO [ 'Trial job Uh6jK status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:37:26] INFO [ 'Trial job Uh6jK status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:37:26] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 2, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 256, \"lr\": 0.01, \"momentum\": 0.7009004965885264}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:37:31] INFO [ 'submitTrialJob: form: {\"sequenceId\":2,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 2, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 16, \\\\\"hidden_size\\\\\": 256, \\\\\"lr\\\\\": 0.01, \\\\\"momentum\\\\\": 0.7009004965885264}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:37:32] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial JqjWi.' ]\r\n[2021-04-07 15:37:36] INFO [ 'Trial job JqjWi status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:41:26] INFO [ 'Trial job JqjWi status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:41:26] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 3, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 32, \"hidden_size\": 512, \"lr\": 0.1, \"momentum\": 0.6258856288476062}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:41:31] INFO [ 'submitTrialJob: form: {\"sequenceId\":3,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 3, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 32, \\\\\"hidden_size\\\\\": 512, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.6258856288476062}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:41:32] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial ijhph.' ]\r\n[2021-04-07 15:41:36] INFO [ 'Trial job ijhph status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:46:31] INFO [ 'Trial job ijhph status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:46:31] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 4, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 1024, \"lr\": 0.1, \"momentum\": 0.30905289366545063}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:46:36] INFO [ 'submitTrialJob: form: {\"sequenceId\":4,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 4, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.30905289366545063}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:46:38] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial bElKu.' ]\r\n[2021-04-07 15:46:41] INFO [ 'Trial job bElKu status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:52:06] INFO [ 'Trial job bElKu status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:52:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 5, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 1024, \"lr\": 0.0001, \"momentum\": 0.0003307910747289977}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:52:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":5,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 5, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.0001, \\\\\"momentum\\\\\": 0.0003307910747289977}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:52:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial upDtw.' ]\r\n[2021-04-07 15:52:16] INFO [ 'Trial job upDtw status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:56:07] INFO [ 'Trial job upDtw status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:56:07] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 6, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 64, \"hidden_size\": 128, \"lr\": 0.01, \"momentum\": 0.876381947693324}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:56:12] INFO [ 'submitTrialJob: form: {\"sequenceId\":6,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 6, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 64, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.01, \\\\\"momentum\\\\\": 0.876381947693324}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:56:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial Zgo5Q.' ]\r\n[2021-04-07 15:56:17] INFO [ 'Trial job Zgo5Q status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:59:32] INFO [ 'Trial job Zgo5Q status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:59:32] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 7, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 512, \"lr\": 0.1, \"momentum\": 0.2948365715286464}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:59:37] INFO [ 'submitTrialJob: form: {\"sequenceId\":7,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 7, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 512, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.2948365715286464}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:59:38] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial T92cL.' ]\r\n[2021-04-07 15:59:42] INFO [ 'Trial job T92cL status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:02:49] INFO [ 'Trial job T92cL status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:02:49] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 8, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 128, \"lr\": 0.001, \"momentum\": 0.5108633717497612}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:02:54] INFO [ 'submitTrialJob: form: {\"sequenceId\":8,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 8, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 16, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.001, \\\\\"momentum\\\\\": 0.5108633717497612}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 16:02:54] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial RoHBk.' ]\r\n[2021-04-07 16:02:59] INFO [ 'Trial job RoHBk status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:06:58] INFO [ 'Trial job RoHBk status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:06:58] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 9, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 32, \"hidden_size\": 1024, \"lr\": 0.1, \"momentum\": 0.1371728116640185}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:07:03] INFO [ 'submitTrialJob: form: {\"sequenceId\":9,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 9, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 32, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.1371728116640185}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 16:07:06] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial UURlR.' ]\r\n[2021-04-07 16:07:08] INFO [ 'Trial job UURlR status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:07:08] INFO [ 'Change NNIManager status from: RUNNING to: NO_MORE_TRIAL' ]\r\n[2021-04-07 16:10:36] INFO [ 'Trial job UURlR status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:10:36] INFO [ 'Change NNIManager status from: NO_MORE_TRIAL to: DONE' ]\r\n[2021-04-07 16:10:36] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 10, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 128, \"lr\": 0.01, \"momentum\": 0.5296207133227185}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:10:36] INFO [ 'Experiment done.' ]\r\n[2021-04-07 16:20:40] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from RUNNING to UNKNOWN.' ]\r\n[2021-04-07 16:21:10] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from UNKNOWN to SUCCEEDED.' ]\r\n\r\n - dispatcher.log:\r\n [2021-04-07 15:24:58] INFO (nni.runtime.msg_dispatcher_base\/MainThread) Dispatcher started\r\n[2021-04-07 15:25:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001968 seconds\r\n[2021-04-07 15:25:06] INFO (hyperopt.tpe\/Thread-1) TPE using 0 trials\r\n[2021-04-07 15:34:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 15:34:06] INFO (hyperopt.tpe\/Thread-1) TPE using 1\/1 trials with best loss -98.950000\r\n[2021-04-07 15:37:26] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001003 seconds\r\n[2021-04-07 15:37:26] INFO (hyperopt.tpe\/Thread-1) TPE using 2\/2 trials with best loss -98.950000\r\n[2021-04-07 15:41:26] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001019 seconds\r\n[2021-04-07 15:41:26] INFO (hyperopt.tpe\/Thread-1) TPE using 3\/3 trials with best loss -99.220000\r\n[2021-04-07 15:46:31] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001025 seconds\r\n[2021-04-07 15:46:31] INFO (hyperopt.tpe\/Thread-1) TPE using 4\/4 trials with best loss -99.220000\r\n[2021-04-07 15:52:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000998 seconds\r\n[2021-04-07 15:52:06] INFO (hyperopt.tpe\/Thread-1) TPE using 5\/5 trials with best loss -99.300000\r\n[2021-04-07 15:56:07] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000969 seconds\r\n[2021-04-07 15:56:07] INFO (hyperopt.tpe\/Thread-1) TPE using 6\/6 trials with best loss -99.300000\r\n[2021-04-07 15:59:32] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001000 seconds\r\n[2021-04-07 15:59:32] INFO (hyperopt.tpe\/Thread-1) TPE using 7\/7 trials with best loss -99.300000\r\n[2021-04-07 16:02:49] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001994 seconds\r\n[2021-04-07 16:02:49] INFO (hyperopt.tpe\/Thread-1) TPE using 8\/8 trials with best loss -99.300000\r\n[2021-04-07 16:06:58] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 16:06:58] INFO (hyperopt.tpe\/Thread-1) TPE using 9\/9 trials with best loss -99.300000\r\n[2021-04-07 16:10:36] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 16:10:36] INFO (hyperopt.tpe\/Thread-1) TPE using 10\/10 trials with best loss -99.340000\r\n\r\n - nnictl stdout and stderr:\r\n\r\n-----------------------------------------------------------------------\r\n                Experiment start time 2021-04-07 15:24:42\r\n-----------------------------------------------------------------------\r\n\r\n-----------------------------------------------------------------------\r\n                Experiment start time 2021-04-07 15:24:42\r\n-----------------------------------------------------------------------\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 message listeners added. Use emitter.setMaxListeners() to increase limit\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 error listeners added. Use emitter.setMaxListeners() to increase limit\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 close listeners added. Use emitter.setMaxListeners() to increase limit\r\n\r\n<!-- Where can you find the log files: [log](https:\/\/github.com\/microsoft\/nni\/blob\/master\/docs\/en_US\/Tutorial\/HowToDebug.md#experiment-root-director), [stdout\/stderr](https:\/\/github.com\/microsoft\/nni\/blob\/master\/docs\/en_US\/Tutorial\/Nnictl.md#nnictl%20log%20stdout) -->\r\n\r\n**What issue meet, what's expected?**:\r\nThe mnist_pytorch example training with Azure ML is unreasonably slow, each trial take about 3 to 5 mins. The entire experiment took nearly 50 mins. I was expecting it to be much faster given that it's using STANDARD_NC6 with GPU - 1 x NVIDIA Tesla K80.\r\n\r\n**How to reproduce it?**: \r\nFollow this doc https:\/\/nni.readthedocs.io\/en\/latest\/TrainingService\/AMLMode.html\r\n\r\n**Additional information**:\r\nTried adding gpuNum: 1 and useActiveGpu: true in config file, only made it even slower with trials spending more time in waiting status, also instead of doing all 10 trials in 1 run, each trial take 1 run.",
        "Challenge_closed_time":1662517763000,
        "Challenge_comment_count":7,
        "Challenge_created_time":1617867548000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the `silnlp.nmt.translate` script, which always creates a ClearML task. The user wants this to be optional and execute locally by default.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/nni\/issues\/3518",
        "Challenge_link_count":4,
        "Challenge_participation_count":7,
        "Challenge_readability":12.4,
        "Challenge_reading_time":208.42,
        "Challenge_repo_contributor_count":171.0,
        "Challenge_repo_fork_count":1727.0,
        "Challenge_repo_issue_count":5102.0,
        "Challenge_repo_star_count":12323.0,
        "Challenge_repo_watch_count":282.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":130,
        "Challenge_solved_time":12402.8375,
        "Challenge_title":"Training extremely slow with Azure Machine Learning",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":1467,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@yangmingwanli Each run only start one trial job, and then start new run? @SparkSnail After adding gpuNum: 1 and useActiveGpu: true, yes each run only start one trial job, and then start new run.\r\nWithout making these changes, it will finish all trials in one run, just very slowly. I reproduced this issue, and this seems to be a bug, will fix it ASAP. @SparkSnail , does it look like going to be a hard to fix bug? Is there any workaround before fix is released? Thanks!  Have you tried setting gpuNum:0, and resubmit the job? Just tried that, after setting gpuNum:0, training is still extremely slow, didn't start new run for new trial, but failed after two trials due to \"Converting circular structure to JSON\" error. @SparkSnail is it a bug that needs to be fixed? \r\n\r\n> \"Converting circular structure to JSON\" error.\r\n   \r\nthis error had been fixed in NNI v2.3.\r\n\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.1,
        "Solution_reading_time":10.34,
        "Solution_score_count":null,
        "Solution_sentence_count":11.0,
        "Solution_word_count":150.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":69.685,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nWhen you try to specify an artifact path and a run_id in an ``MlflowArtifactDataSet``, you get an error. \r\n\r\nThis works:\r\n```python\r\nmlflow_csv_dataset = MlflowArtifactDataSet(\r\n    data_set=dict(type=CSVDataSet, filepath=\"path\/to\/df.csv\"),\r\n    artifact_path=None,\r\n    run_id=\"1234\",\r\n)\r\nmlflow_csv_dataset .load()\r\n```\r\n\r\nwhile this :\r\n```python\r\nmlflow_csv_dataset = MlflowArtifactDataSet(\r\n    data_set=dict(type=CSVDataSet, filepath=\"path\/to\/df.csv\"),\r\n    artifact_path=\"folder\", # this is the difference\r\n    run_id=\"1234\",\r\n)\r\nmlflow_csv_dataset .load()\r\n```\r\nraises the following error: ``unsupported operand type(s) for \/: 'str' and 'str'``:\r\n",
        "Challenge_closed_time":1665079955000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1664829089000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The issue is related to the failure of MlflowArtifactDataset.load() when both artifact_path and run_id are specified. An error is encountered when the artifact_path is not None and run_id is specified.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/362",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.6,
        "Challenge_reading_time":9.28,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":69.685,
        "Challenge_title":"MlflowArtifactDataset.load() fails if artifact_path is not None and run_id is specified",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":67,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The issue is still here when there is nested artifact_path: if the file does not exists, it is downloaded to ``self._filepath \/artifact_path\/filename.pkl`` and cannot be loaded (due to the ``artifact_path`` suffix)",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.7,
        "Solution_reading_time":2.7,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":31.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":142.5483333333,
        "Challenge_answer_count":0,
        "Challenge_body":"## Instructions \r\nPage 133 of chapter 7 requires the reader to navigate to the following directory and enter the commands below:\r\n\r\n`cd Chapter07\/psystock-data-features-main`\r\n `mlflow run . --experiement-name=psystock_data_pipelines`\r\n\r\n## Problem\r\n\r\nThe following error message appears when running line of code specified above:\r\n``` \r\nTraceback (most recent call last):\r\n  File \"feature_set_generation.py\", line 30, in <module>\r\n    raise Exception('x should not exceed 5. The value of x was: {}'.format(x))\r\nNameError: name 'x' is not defined\r\n```\r\n\r\n## Solution\r\nResolve this by deleting line 30 below in `feature_set_generation.py`\r\n\r\n`30         raise Exception('x should not exceed 5. The value of x was: {}'.format(x))`\r\nThe stray `raise` statement is referencing an undefined variable `x`.\r\n\r\nRemoving this line of code removed the reference to this point and lead to the successful deployment of the experiment. I would consider adding such assertions in the `check_verify_data.py` file instead.\r\n\r\n",
        "Challenge_closed_time":1637063606000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1636550432000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The MLFlowLogger fails to update the status of a `mlflow.entities.run_info.RunInfo` object to 'FAILED' when an error is raised during training, causing the MLFlow Tracking Server screen to show that the training is still in progress even though it has been terminated with an error. The user has provided a code snippet to reproduce the issue and expects the status of each MLFlow's run to be correctly updated when `pl.Trainer.fit` fails. The PyTorch Lightning version used is 1.4.9, and the MLFlow version is 1.12.0.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/PacktPublishing\/Machine-Learning-Engineering-with-MLflow\/issues\/7",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":13.22,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":59.0,
        "Challenge_repo_issue_count":18.0,
        "Challenge_repo_star_count":82.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":142.5483333333,
        "Challenge_title":"Chapter 7 `mlflow run . --experiement-name=psystock_data_pipelines` fails - BUGFIX",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":138,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks, invaluable contributions. We will add this to the Errata!!!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":0.85,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":10.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":27.1724516667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,  <\/p>\n<p>I am trying to export data from Azure ML to an Azure SQL Database using the 'Export Data' module but the log file contains the following messages and no data is exported to the database.  <\/p>\n<p>&quot;Not exporting to run RunHistory as the exporter is either stopped or there is no data&quot;  <\/p>\n<p>&quot;Process exiting with code: 0  <\/p>\n<p>There is definitely data flowing to the 'Export Data' module from an 'Execute R Script' module as I have checked the Result dataset.  <\/p>\n<p>Would appreciate some assistance.  <\/p>\n<p>Thank you.<\/p>",
        "Challenge_closed_time":1629106747876,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629008927050,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue while exporting data from Azure ML to an Azure SQL Database using the 'Export Data' module. The log file shows messages stating that no data is being exported, even though data is flowing to the module from an 'Execute R Script' module. The user is seeking assistance to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/514067\/no-data-being-exported-from-export-data-module-in",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":7.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":27.1724516667,
        "Challenge_title":"No Data being exported from 'Export Data' module in Azure ML",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":102,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi,   <\/p>\n<p>I have resolved this issue. I had set the export table to be dbo.TestTable rather than just TestTable. As the table dbo.TestTable did not exist the 'Export module' created it in the dbo schema so the table name effectively became dbo.dbo.TestTable.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.5,
        "Solution_reading_time":3.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1553088438367,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tehran, Tehran Province, Iran",
        "Answerer_reputation_count":415.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":61.4036480555,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to log the plot of a <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_estimator\" rel=\"nofollow noreferrer\">confusion matrix generated with scikit-learn<\/a> for a <em>test<\/em> set using <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.sklearn.html\" rel=\"nofollow noreferrer\">mlflow's support for scikit-learn<\/a>.<\/p>\n<p>For this, I tried something that resemble the code below (I'm using mlflow hosted on Databricks, and <code>sklearn==1.0.1<\/code>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sklearn.datasets\nimport pandas as pd\nimport numpy as np\nimport mlflow\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nmlflow.set_tracking_uri(&quot;databricks&quot;)\nmlflow.set_experiment(&quot;\/Users\/name.surname\/plotcm&quot;)\n\ndata = sklearn.datasets.fetch_20newsgroups(categories=['alt.atheism', 'sci.space'])\n\ndf = pd.DataFrame(data = np.c_[data['data'], data['target']])\\\n       .rename({0:'text', 1:'class'}, axis = 'columns')\n\ntrain, test = train_test_split(df)\n\nmy_pipeline = Pipeline([\n    ('vectorizer', TfidfVectorizer()),\n    ('classifier', SGDClassifier(loss='modified_huber')),\n])\n\nmlflow.sklearn.autolog()\n\nfrom sklearn.metrics import ConfusionMatrixDisplay # should I import this after the call to `.autolog()`?\n\nmy_pipeline.fit(train['text'].values, train['class'].values)\n\ncm = ConfusionMatrixDisplay.from_predictions(\n      y_true=test[&quot;class&quot;], y_pred=my_pipeline.predict(test[&quot;text&quot;])\n  )\n<\/code><\/pre>\n<p>while the confusion matrix for the training set is saved in my mlflow run, no png file is created in the mlflow frontend for the <code>test<\/code> set.<\/p>\n<p>If I try to add<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>cm.figure_.savefig('test_confusion_matrix.png')\nmlflow.log_artifact('test_confusion_matrix.png')\n<\/code><\/pre>\n<p>that does the job, but requires explicitly logging the artifact.<\/p>\n<p>Is there an idiomatic\/proper way to autolog the confusion matrix computed using a test set after <code>my_pipeline.fit()<\/code>?<\/p>",
        "Challenge_closed_time":1658304934100,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657892681357,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to log the plot of a confusion matrix generated with scikit-learn for a test set using mlflow's support for scikit-learn. Although the confusion matrix for the training set is saved in the mlflow run, no png file is created in the mlflow frontend for the test set. The user is looking for an idiomatic\/proper way to autolog the confusion matrix computed using a test set after my_pipeline.fit().",
        "Challenge_last_edit_time":1658083880967,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72994988",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.0,
        "Challenge_reading_time":30.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":114.5146508333,
        "Challenge_title":"How to mlflow-autolog a sklearn ConfusionMatrixDisplay?",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":157.0,
        "Challenge_word_count":188,
        "Platform":"Stack Overflow",
        "Poster_created_time":1415722650716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Verona, VR, Italy",
        "Poster_reputation_count":4811.0,
        "Poster_view_count":713.0,
        "Solution_body":"<p>The proper way to do this is to use <code>mlflow.log_figure<\/code> as a fluent API announced in <code>MLflow 1.13.0<\/code>. You can read the documentation <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_figure\" rel=\"nofollow noreferrer\">here<\/a>. This code will do the job.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.log_figure(cm.figure_, 'test_confusion_matrix.png')\n<\/code><\/pre>\n<p>This function implicitly store the image, and then calls <code>log_artifact<\/code> against that path, something like you did.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.9,
        "Solution_reading_time":7.49,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":55.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1554424491243,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":51.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":18.6288588889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>From <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform.html#google.cloud.aiplatform.AutoMLTabularTrainingJob.run\" rel=\"nofollow noreferrer\">the docs<\/a> it says that<\/p>\n<blockquote>\n<p>The value of the key values of the key (the values in the column) must be in RFC 3339 date-time format, where time-offset = \u201cZ\u201d (e.g. 1985-04-12T23:20:50.52Z)<\/p>\n<\/blockquote>\n<p>The dataset that I'm pointing to is a CSV in cloud storage, where the data is in the format suggested by the docs:<\/p>\n<pre><code>$ gsutil cat gs:\/\/my-data.csv | head | xsv select TS_SPLIT_COL\nTS_SPLIT_COL\n2021-01-18T00:00:00.00Z\n2021-01-18T00:00:00.00Z\n2021-01-04T00:00:00.00Z\n2021-03-06T00:00:00.00Z\n2021-01-15T00:00:00.00Z\n2021-02-11T00:00:00.00Z\n2021-02-05T00:00:00.00Z\n2021-05-20T00:00:00.00Z\n2021-01-05T00:00:00.00Z\n<\/code><\/pre>\n<p>But I receive a <code>Training pipeline failed with error message: The timestamp column must have valid timestamp entries.<\/code> error when I try to run a training job<\/p>\n<p>EDIT: this should hopefully make it more reproducible<\/p>\n<p>data: <a href=\"https:\/\/pastebin.com\/qEDqvzX6\" rel=\"nofollow noreferrer\">https:\/\/pastebin.com\/qEDqvzX6<\/a><\/p>\n<p>Code I'm running:<\/p>\n<pre><code>from google.cloud import aiplatform\n\nPROJECT = &quot;my-project&quot;\nDATASET_ID = &quot;dataset-id&quot;  # points to CSV \n\naiplatform.init(project=PROJECT)\n\ndataset = aiplatform.TabularDataset(DATASET_ID)\n\njob = aiplatform.AutoMLTabularTrainingJob(\n    display_name=&quot;so-58454722&quot;,\n    optimization_prediction_type=&quot;classification&quot;,\n    optimization_objective=&quot;maximize-au-roc&quot;,\n)\n\nmodel = job.run(\n    dataset=dataset,\n    model_display_name=&quot;so-58454722&quot;,\n    target_column=&quot;Y&quot;,\n    training_fraction_split=0.8,\n    validation_fraction_split=0.1,\n    test_fraction_split=0.1,\n    timestamp_split_column_name=&quot;TS_SPLIT_COL&quot;,\n)\n<\/code><\/pre>",
        "Challenge_closed_time":1647602028112,
        "Challenge_comment_count":3,
        "Challenge_created_time":1647473614997,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a \"The timestamp column must have valid timestamp entries.\" error when using the `timestamp_split_column_name` argument in `AutoMLTabularTrainingJob.run`. The user's dataset is in the correct format suggested by the documentation, but the error still occurs.",
        "Challenge_last_edit_time":1647534964220,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71505415",
        "Challenge_link_count":3,
        "Challenge_participation_count":4,
        "Challenge_readability":13.2,
        "Challenge_reading_time":26.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":35.6703097222,
        "Challenge_title":"\"The timestamp column must have valid timestamp entries.\" error when using `timestamp_split_column_name` arg in `AutoMLTabularTrainingJob.run`",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":157.0,
        "Challenge_word_count":165,
        "Platform":"Stack Overflow",
        "Poster_created_time":1519933306780,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3576.0,
        "Poster_view_count":203.0,
        "Solution_body":"<p>Try this timestamp format instead:<\/p>\n<p><code>2022-03-18T01:23:45.123456+00:00<\/code><\/p>\n<p>It uses <code>+00:00<\/code> instead of <code>Z<\/code> to specify timezone.<\/p>\n<p>This change will eliminate the &quot;The timestamp column must have valid timestamp entries.&quot; error<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":3.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":29.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":306.3980480556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,  <\/p>\n<p>I have create a compute instance through the Azure ML Studio. Using the &quot;Applications&quot; list, I can access Jupyter, RStudio, a Terminal and connect via VS Code. But how can I just connect via SSH? I have added my SSH key to the instance while creating it, but cannot connect via the public IP address listed in the node details. I tried this as <code>ssh azureuser@&lt;compute-ip&gt;<\/code>  <\/p>\n<p>In the output of the <code>azure ml compute show<\/code> command in the cloud shell I noticed the values <code>&quot;enable_public_ip&quot;: false<\/code> and <code>&quot;ssh_public_access&quot;: &quot;False&quot;<\/code>, but couldn't find anything regarding how to change these settings. Or is there any way to connect to them without a public ip?  <\/p>",
        "Challenge_closed_time":1627973477260,
        "Challenge_comment_count":1,
        "Challenge_created_time":1626870444287,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is having trouble connecting to a compute instance via SSH in Azure ML Studio, despite adding their SSH key to the instance during creation. The output of the \"azure ml compute show\" command shows that \"enable_public_ip\" and \"ssh_public_access\" are both false, and the user is unsure how to change these settings or if there is another way to connect without a public IP.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/484265\/connect-to-compute-instance-via-ssh",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.9,
        "Challenge_reading_time":10.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":306.3980480556,
        "Challenge_title":"Connect to compute instance via ssh",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":121,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey Yutong,  <br \/>\nI was just asking about compute instances created though the Azure ML Studio in general. Like the ones you create when you use the ML Studio, then select <em>Compute<\/em> in the sidebar and under <em>Compute Instances<\/em> create a new compute node. During creation, I selected the &quot;Enable SSH access&quot; option.  <\/p>\n<p>I did figure out a solution to my problem of connecting to such an instance a bit later, and will leave it here for others with the same problem. As long as you select &quot;Enable SSH access&quot;, the SSH access does work even though the <code>azure ml compute show<\/code> says <code>&quot;ssh_public_access&quot;: &quot;False&quot;<\/code>. My problem just was that I did not know the user name I had to log in as and also did not know which port I had to connect to. It turns out that the user name is always set to <code>azureuser<\/code> and the port seems to be either <code>50000<\/code> or <code>50001<\/code>. You should therefore be able to connect via <code>ssh  azureuser@&lt;PUBLIC_IP&gt; -p 50000<\/code> or with <code>-p 50001<\/code>. You can find the public IP on the details page of the compute instance. If you want to know for sure which port it is, you can query the azure CLI with   <\/p>\n<pre><code>az resource show --ids \/subscriptions\/{subscriptionId}\/resourceGroups\/{resourceGroupName}\/providers\/Microsoft.MachineLearningServices\/workspaces\/{workspaceName}\/computes\/{computeName}\n<\/code><\/pre>\n<p>and in the JSON response look under <em>properties.sshSettings<\/em> for <em>adminUserName<\/em> and <em>sshPort<\/em>.  <\/p>\n<p>I just wished that this was documented somewhere in the Azure ML docs. I had to read through the source code of how the Azure ML VSCode extension connects to a compute instance to find this out.  <\/p>\n<p>Is there any easier way to accomplish this? I needed the SSH access to forward specific ports from the compute instance to local and also to set up a Python SSH remote interpreter in IntelliJ<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.4,
        "Solution_reading_time":24.8,
        "Solution_score_count":1.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":299.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1254957460063,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"North Carolina, USA",
        "Answerer_reputation_count":2484.0,
        "Answerer_view_count":362.0,
        "Challenge_adjusted_solved_time":7.0602616667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In Azure Machine Learning studio I need to convert a column of data that has three categorical values 'yes', 'no' and 'maybe', and wish to combine the 'no' and 'maybe' values as just 'no'. <\/p>\n\n<p>I can do this easily using SQL, R, or Python but for these purposes I need to show if it is possible to do this without using these languages. I can't seem to find a way to do this. <\/p>\n\n<p>Does anyone have any ideas? I'm fine if the answer is no but I don't want to say it's not possible if it is. <\/p>",
        "Challenge_closed_time":1527597469392,
        "Challenge_comment_count":0,
        "Challenge_created_time":1527572052450,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to replace values in a dataset within Azure Machine Learning Studio. They need to combine the 'no' and 'maybe' values as just 'no' in a column of data that has three categorical values 'yes', 'no' and 'maybe'. The user is looking for a way to do this without using SQL, R, or Python.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50576929",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":6.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":7.0602616667,
        "Challenge_title":"Replacing values in dataset within Azure Machine Learning Studio",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":484.0,
        "Challenge_word_count":108,
        "Platform":"Stack Overflow",
        "Poster_created_time":1367782461047,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1647.0,
        "Poster_view_count":321.0,
        "Solution_body":"<p>It can be done! :)<\/p>\n\n<p>You would just use the \"Group Categorical Values\" module. Choose the column that has the data you want to group, and you can set the values like the following:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/UGhrR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/UGhrR.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>What's going on here is that the default, which will get used if the other levels aren't caught, is set to \"yes\". Then when any values are \"no\", or \"maybe\", it gets grouped into a category of \"no\".<\/p>\n\n<p>However, this will error unless you make that column a categorical type, so you would need to use the \"Edit Metadata\" module to do that.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/45s2Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/45s2Q.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The example I used is <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Replace-Values-in-Dataset\" rel=\"nofollow noreferrer\">published to the gallery<\/a>, if you need to reference it.<\/p>\n\n<p>If you need more info, just let me know.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":9.1,
        "Solution_reading_time":14.41,
        "Solution_score_count":3.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":142.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426093220648,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bengaluru, India",
        "Answerer_reputation_count":1861.0,
        "Answerer_view_count":294.0,
        "Challenge_adjusted_solved_time":7.1692591667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using PartitionedDataSet to load multiple csv files from azure blob storage. I defined my data set in the datacatalog as below.<\/p>\n<pre><code>my_partitioned_data_set:\n          type: PartitionedDataSet\n          path: my\/azure\/folder\/path\n          credentials: my credentials\n          dataset: pandas.CSVDataSet\n          load_args:\n                sep: &quot;;&quot;\n                encoding: latin1\n<\/code><\/pre>\n<p>I also defined a node to combine all the partitions. But while loading each file as a CSVDataSet kedro is not considering the load_args, so I am getting the below error.<\/p>\n<pre><code>Failed while loading data from data set CSVDataSet(filepath=my\/azure\/folder\/path, load_args={}, protocol=abfs, save_args={'index': False}).\n'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte \n<\/code><\/pre>\n<p>The error shows that while loading the CSVDataSet kedro is not considering the load_args defined in the PartitionedDataSet. And passing an empty dict as a load_args parameter to CSVDataSet.\nI am following the documentation\n<code>https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/02_kedro_io.html#partitioned-dataset<\/code>\nI am not getting where I am doing mistakes.<\/p>",
        "Challenge_closed_time":1638684474152,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638659712850,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a DataSetError while loading PartitionedDataSet to load multiple CSV files from Azure Blob Storage. The load_args defined in the PartitionedDataSet are not being considered by kedro while loading each file as a CSVDataSet, resulting in an error. The user is following the documentation but is unsure where they are making a mistake.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70230262",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":15.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":6.8781394444,
        "Challenge_title":"kedro DataSetError while loading PartitionedDataSet",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":381.0,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_created_time":1495105930728,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>Move <code>load_args<\/code> inside dataset<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>my_partitioned_data_set:\n  type: PartitionedDataSet\n  path: my\/azure\/folder\/path\n  credentials: my credentials\n  dataset:\n    type: pandas.CSVDataSet\n    load_args:\n      sep: &quot;;&quot;\n      encoding: latin1\n<\/code><\/pre>\n<ul>\n<li><p><code>load_args<\/code> mentioned outside dataset is passed into <code>find()<\/code> method of the corresponding filesystem implementation<\/p>\n<\/li>\n<li><p>To pass granular configuration to underlying dataset put it inside <code>dataset<\/code> as above.<\/p>\n<\/li>\n<\/ul>\n<p>You can check out the details in the docs<\/p>\n<p><a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/02_kedro_io.html?highlight=partitoned%20dataset#partitioned-dataset-definition\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/02_kedro_io.html?highlight=partitoned%20dataset#partitioned-dataset-definition<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1638685522183,
        "Solution_link_count":2.0,
        "Solution_readability":22.8,
        "Solution_reading_time":12.65,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":67.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.6135575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Currently! I'm experimenting with the azure data labelling tool in the machine learning workspace for image classification, what I found was azure shows only the unlabelled data to each user i.e if a user has already labelled an image, other users won't be shown the same image again.   <br \/>\nIs there any setting that exists, which can be enabled or disabled so that we can let more than one labeller label the same data?   <\/p>",
        "Challenge_closed_time":1625073229547,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625056620740,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is experimenting with Azure's data labelling tool for image classification and has found that each user is only shown unlabelled data, meaning that if one user has already labelled an image, other users won't be shown the same image again. The user is asking if there is a setting that can be enabled or disabled to allow multiple labelers to label the same data and reach a consensus.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/458004\/azure-machine-learning-data-labelling-is-it-possib",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.7,
        "Challenge_reading_time":6.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":4.6135575,
        "Challenge_title":"Azure machine learning data labelling- Is it possible to assign different labelers to label same data in a single project to reach a consensus?",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":98,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Thanks for reaching to us. This capability is currently in development, and expected to release soon.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":1.37,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1637398374483,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":176.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":296.2662186111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>i esttablished a function of optuna to find out best model of gbm and xgboost for my data but i was wondering if i can take the best model and apply it directly into my notebook(extracting best model as an object to reuse it later)\nhere is my objective function:<\/p>\n<pre><code>import lightgbm as lgb \nimport optuna\nimport sklearn.metrics\nfrom xgboost import XGBRegressor\nfrom optuna.integration import XGBoostPruningCallback\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nbest_booster = None\ngbm = None\ndef objective(trial,random_state=22,n_jobs=1,early_stopping_rounds=50):\n    \n    regrosser_name = trial.suggest_categorical(&quot;regressor&quot;, [&quot;XGBoost&quot;, &quot;lightgbm&quot;])\n    train_x, valid_x, train_y, valid_y = train_test_split(X_train, y_train, test_size=0.25)\n    dtrain = lgb.Dataset(train_x, label=train_y)\n    # Step 2. Setup values for the hyperparameters:\n    if regrosser_name == 'XGBoost':\n        params = {\n        &quot;verbosity&quot;: 0,  # 0 (silent) - 3 (debug)\n        &quot;objective&quot;: &quot;reg:squarederror&quot;,\n        &quot;n_estimators&quot;: 10000,\n        &quot;max_depth&quot;: trial.suggest_int(&quot;max_depth&quot;, 4, 12),\n        &quot;learning_rate&quot;: trial.suggest_loguniform(&quot;learning_rate&quot;, 0.005, 0.05),\n        &quot;colsample_bytree&quot;: trial.suggest_loguniform(&quot;colsample_bytree&quot;, 0.2, 0.6),\n        &quot;subsample&quot;: trial.suggest_loguniform(&quot;subsample&quot;, 0.4, 0.8),\n        &quot;alpha&quot;: trial.suggest_loguniform(&quot;alpha&quot;, 0.01, 10.0),\n        &quot;lambda&quot;: trial.suggest_loguniform(&quot;lambda&quot;, 1e-8, 10.0),\n        &quot;gamma&quot;: trial.suggest_loguniform(&quot;lambda&quot;, 1e-8, 10.0),\n        &quot;min_child_weight&quot;: trial.suggest_loguniform(&quot;min_child_weight&quot;, 10, 1000),\n        &quot;seed&quot;: random_state,\n        &quot;n_jobs&quot;: n_jobs,\n        }\n        model = XGBRegressor(**params)\n        model.fit(train_x, train_y)\n        y_pred = model.predict(X_val)\n        accuracy_rf = sklearn.metrics.mean_absolute_error(valid_y, y_pred)\n        return accuracy_rf\n    \n        print(rf_max_depth)\n        print(rf_n_estimators)\n        \n    else:\n        param = {\n        &quot;objective&quot;: &quot;binary&quot;,\n        &quot;metric&quot;: &quot;binary_logloss&quot;,\n        &quot;verbosity&quot;: -1,\n        &quot;boosting_type&quot;: &quot;gbdt&quot;,\n        &quot;lambda_l1&quot;: trial.suggest_float(&quot;lambda_l1&quot;, 1e-8, 10.0, log=True),\n        &quot;lambda_l2&quot;: trial.suggest_float(&quot;lambda_l2&quot;, 1e-8, 10.0, log=True),\n        &quot;num_leaves&quot;: trial.suggest_int(&quot;num_leaves&quot;, 2, 256),\n        &quot;feature_fraction&quot;: trial.suggest_float(&quot;feature_fraction&quot;, 0.4, 1.0),\n        &quot;bagging_fraction&quot;: trial.suggest_float(&quot;bagging_fraction&quot;, 0.4, 1.0),\n        &quot;bagging_freq&quot;: trial.suggest_int(&quot;bagging_freq&quot;, 1, 7),\n        &quot;min_child_samples&quot;: trial.suggest_int(&quot;min_child_samples&quot;, 5, 100),\n        }\n        gbm = lgb.train(param, dtrain)\n        preds_gbm = gbm.predict(valid_x)\n        pred_labels_gbm = np.rint(preds_gbm)\n        accuracy_gbm = sklearn.metrics.mean_absolute_error(valid_y, pred_labels_gbm)\n        return accuracy_gbm\n<\/code><\/pre>\n<p>and here is how i tried to solve this issue:<\/p>\n<pre><code>def callback(study, trial):\n    global best_booster\n    if study.best_trial == trial:\n        best_booster = gbm\nif __name__ == &quot;__main__&quot;:\n    study = optuna.create_study(direction=&quot;maximize&quot;)\n    study.optimize(objective, n_trials=100, callbacks=[callback])\n<\/code><\/pre>\n<p>i think its about importing somthing, and if there is any tips on my optuna function please state it<\/p>",
        "Challenge_closed_time":1649608540120,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648541981733,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has established an Optuna function to find the best model of GBM and XGBoost for their data and wants to know if they can take the best model and apply it directly in their notebook. They have tried to solve the issue by creating a callback function to extract the best booster, but they are unsure if they need to import something and are open to any tips on their Optuna function.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71658687",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.8,
        "Challenge_reading_time":50.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":296.2662186111,
        "Challenge_title":"can i take best parameters and best model of optuna function and apply this model directly in my notebook?",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":585.0,
        "Challenge_word_count":295,
        "Platform":"Stack Overflow",
        "Poster_created_time":1548633918320,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":79.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>If I understood your question correctly, then yes, that's what models are for.<\/p>\n<p>Like bring your saved model to your notebook, feed it data that has the same structure  as what you used to train it, and it should serve its purpose.  Or use it in a pipeline.<\/p>\n<p>Even 1 line of the same structure as an np array can be used.  For example, my model predicts whether a loan should be approved or not.<\/p>\n<p>For example, a bank customer wants a loan and submits his information.  The bank officer inputs this info in the system.  The system transforms this information into a single np array with the same structure as the dataset used to train the model.<\/p>\n<p>The model is then used by the system to predict whether the loan should be approved or not.<\/p>\n<p>I save my optuna xgb models as json, e.g.<\/p>\n<p>my_model.get_booster().save_model(f'{savepath}my_model.json')<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":10.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":146.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":169.8337822222,
        "Challenge_answer_count":1,
        "Challenge_body":"I have setup the pdf labelling task in Sagemaker groud truth following this link - https:\/\/github.com\/aws-samples\/amazon-comprehend-semi-structured-documents-annotation-tools\n\nAfter sometime, the job is failed saying  \"**Complete with labeling errors**\". I found the below log in cloudwatch logs\n\n```\n{\n    \"event-name\": \"HUMAN_TASK_FAILED\",\n    \"event-log-message\": \"ERROR: Human task failed for line 694.\",\n    \"labeling-job-name\": \"resume-labeling-job-20221201T182336\"\n}\n```\n\nNot sure what happened. Does anyone have any way to identify the root cause behind this failure ?",
        "Challenge_closed_time":1671877894556,
        "Challenge_comment_count":0,
        "Challenge_created_time":1670920092769,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered a problem with Sagemaker ground truth labelling jobs. The job failed with a message \"**Complete with labeling errors**\" and the user found an error log in cloudwatch logs. The user is seeking help to identify the root cause of the failure.",
        "Challenge_last_edit_time":1671266492940,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUirlDbmZPS8SCqnmC2RFA6A\/human-task-failed-sagemaker-ground-truth-labelling-jobs",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":7.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":266.0560519444,
        "Challenge_title":"Human Task Failed - Sagemaker ground truth labelling jobs",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":91.0,
        "Challenge_word_count":70,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Since I didn't get answer here, I reached out to tech support to seek guidance. It looks like the job duration is elapsed. \n\n\"failure-reason\": \"ClientError: Annotation tasks expired. \n\nReasons are TaskAvailabilityLifetimeInSeconds parameter is too small. \n\nYou can validate this configuration by running the following AWS CLI command from your environment:\naws sagemaker describe-labeling-job --labeling-job-name resume-labeling-job-20221212T094103\n\n\u2014References\u2014\n[1] https:\/\/docs.aws.amazon.com\/sagemaker\/\/latest\/APIReference\/API_HumanTaskConfig.html#API_HumanTaskConfig_Contents",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1671877894556,
        "Solution_link_count":1.0,
        "Solution_readability":14.9,
        "Solution_reading_time":7.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":58.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":18.8050408334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,  <\/p>\n<p>We are currently annotating images in a data labeling instance segmentation (polygon) project. Our images are rather blueish, which makes it difficult to use the polygon &quot;draw polygon region&quot; tool, which draws the polygon in blue.  <\/p>\n<p>Is it possible to change the color to, for example, black?  <\/p>\n<p>Thanks and BR,  <br \/>\nMaite<\/p>",
        "Challenge_closed_time":1647596267230,
        "Challenge_comment_count":1,
        "Challenge_created_time":1647528569083,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing difficulty in annotating images in a data labeling instance segmentation project on Azure ML due to the blue color of the polygon tool. They are seeking a solution to change the color of the tool to black.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/776555\/azure-ml-data-labeling-change-polygon-color",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":5.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":18.8050408334,
        "Challenge_title":"Azure ML data labeling change polygon color",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":62,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=54dc3768-e4dc-41b2-8290-e72ad5c207f3\">@Maite  <\/a>     <\/p>\n<p>Thanks for reaching out to us, I am sorry we are using only blue for the polygon color. I will forward your feedback to product team for future release.     <\/p>\n<p>One workaround may help with your scenario is, you can change the brightness to &quot;-100&quot; when you draw and revert the brightness back when you done as below screenshot. This will help to make things clear.     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/184541-image.png?platform=QnA\" alt=\"184541-image.png\" \/>    <\/p>\n<p>Hope this helps and thanks for the feedback again.     <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p><em>-Please kindly accept the answer if you feel helpful, thanks.<\/em>    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.7,
        "Solution_reading_time":9.77,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":100.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":69.1163888889,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nA customer is experimenting with SageMaker batch transform with parquet and is interested is some form of local development to speedup iteration. Does SageMaker Batch Transform support local mode?",
        "Challenge_closed_time":1571303926000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1571055107000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of using local mode for SageMaker Batch Transform to speed up iteration while experimenting with parquet.",
        "Challenge_last_edit_time":1668610004528,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUtNtH0LyFSLCXc0xCV4hYkw\/sagemaker-batch-transform-local-mode",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":3.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":69.1163888889,
        "Challenge_title":"SageMaker Batch Transform local mode?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":336.0,
        "Challenge_word_count":34,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You can do local testing by running the container in serve mode as a docker. Then using Curl\/Postman to send an HTTP request and inspecting the response.  \n\nThe request can be CSV\/JSON or binary (a parquet file in your case).  \n\nIf you're able to run the Pytorch model in serve mode locally, then this local testing provides a lot of coverage before running in Batch Transform itself.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925565480,
        "Solution_link_count":0.0,
        "Solution_readability":8.7,
        "Solution_reading_time":4.58,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1431575832387,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":550.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":71.7747852778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Background: There seems to be a way to parameterize <code>DataPath<\/code> with <code>PipelineParameter<\/code>\n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb<\/a><\/p>\n<p>But I'd like to parameterize my SQL query with PipelineParameter, for example, with this query<\/p>\n<pre><code>sql_query = &quot;&quot;&quot;\nSELECT id, foo, bar FROM baz\nWHERE baz.id BETWEEN 10 AND 20\n&quot;&quot;&quot;\ndataset = Dataset.Tabular.from_sql_query((sql_datastore, sql_query))\n<\/code><\/pre>\n<p>I'd like to use PipelineParameter to parameterize <code>10<\/code> and <code>20<\/code> as <code>param_1<\/code> and <code>param_2<\/code>. Is this possible?<\/p>",
        "Challenge_closed_time":1603727871847,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603497171040,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to parameterize a SQL query in Azure ML using PipelineParameter to replace the values of two parameters in the query. They have successfully parameterized DataPath but are unsure if it is possible to do the same with a SQL query.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64508625",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":22.6,
        "Challenge_reading_time":14.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":64.0835575,
        "Challenge_title":"Parameterized SQL query in Azure ML",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":188.0,
        "Challenge_word_count":72,
        "Platform":"Stack Overflow",
        "Poster_created_time":1431575832387,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":550.0,
        "Poster_view_count":40.0,
        "Solution_body":"<p>Found a way to do this:<\/p>\n<p>Pass your params to PythonScriptStep<\/p>\n<pre><code>param_1 = PipelineParameter(name='min_id', default_value=5)\nparam_2 = PipelineParameter(name='max_id', default_value=10)\nsql_datastore = &quot;sql_datastore&quot;\nstep = PythonScriptStep(script_name='script.py', arguments=[param_1, param_2, \nsql_datastore])\n<\/code><\/pre>\n<p>In script.py<\/p>\n<pre><code>min_id_param = sys.argv[1]\nmax_id_param = sys.argv[2]\nsql_datastore_name = sys.argv[3]\nquery = &quot;&quot;&quot;\nSELECT id, foo, bar FROM baz\nWHERE baz.id BETWEEN {} AND {}\n&quot;&quot;&quot;.format(min_id_param, max_id_param)\nrun = Run.get_context()\nsql_datastore = Datastore.get(run.experiment.workspace, sql_datastore_name)\ndataset = Dataset.Tabular.from_sql_query((sql_datastore, query))\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1603755560267,
        "Solution_link_count":0.0,
        "Solution_readability":15.9,
        "Solution_reading_time":10.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":56.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1393852033260,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"India",
        "Answerer_reputation_count":33599.0,
        "Answerer_view_count":6250.0,
        "Challenge_adjusted_solved_time":15862.09938,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am new to Sagemaker and not sure how to classify the text input in AWS sagemaker, <\/p>\n\n<p>Suppose I have a Dataframe having two fields like 'Ticket' and 'Category', Both are text input, Now I want to split it test and training set and upload in Sagemaker training model. <\/p>\n\n<pre><code>X_train, X_test, y_train, y_test = model_selection.train_test_split(fewRecords['Ticket'],fewRecords['Category'])\n<\/code><\/pre>\n\n<p>Now as I want to perform TD-IDF feature extraction and then convert it to numeric value, so performing this operation<\/p>\n\n<pre><code>tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\ntfidf_vect.fit(fewRecords['Category'])\nxtrain_tfidf =  tfidf_vect.transform(X_train)\nxvalid_tfidf =  tfidf_vect.transform(X_test)\n<\/code><\/pre>\n\n<p>When I want to upload the model in Sagemaker so I can perform next operation like <\/p>\n\n<pre><code>buf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(buf, xtrain_tfidf, y_train)\nbuf.seek(0)\n<\/code><\/pre>\n\n<p>I am getting this error <\/p>\n\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-36-8055e6cdbf34&gt; in &lt;module&gt;()\n      1 buf = io.BytesIO()\n----&gt; 2 smac.write_numpy_to_dense_tensor(buf, xtrain_tfidf, y_train)\n      3 buf.seek(0)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/amazon\/common.py in write_numpy_to_dense_tensor(file, array, labels)\n     98             raise ValueError(\"Label shape {} not compatible with array shape {}\".format(\n     99                              labels.shape, array.shape))\n--&gt; 100         resolved_label_type = _resolve_type(labels.dtype)\n    101     resolved_type = _resolve_type(array.dtype)\n    102 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/amazon\/common.py in _resolve_type(dtype)\n    205     elif dtype == np.dtype('float32'):\n    206         return 'Float32'\n--&gt; 207     raise ValueError('Unsupported dtype {} on array'.format(dtype))\n\nValueError: Unsupported dtype object on array\n<\/code><\/pre>\n\n<p>Other than this exception, I am not clear if this is right way as TfidfVectorizer convert the series to Matrix.<\/p>\n\n<p>The code is predicting fine on my local machine but not sure how to do the same on Sagemaker, All the example mentioned there are too lengthy and not for the person who still reached to SciKit Learn <\/p>",
        "Challenge_closed_time":1535540801190,
        "Challenge_comment_count":0,
        "Challenge_created_time":1535524538620,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in classifying text input in AWS Sagemaker. They have a Dataframe with two text input fields, 'Ticket' and 'Category', and want to split it into test and training sets for TD-IDF feature extraction. However, when trying to upload the model in Sagemaker, they encounter a ValueError related to unsupported dtype object on array. The user is unsure if their approach is correct and is looking for guidance on how to perform the same operation on Sagemaker.",
        "Challenge_last_edit_time":1535540817292,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52070950",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":30.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":4.5173805556,
        "Challenge_title":"AWS Sagemaker | how to train text data | For ticket classification",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1519.0,
        "Challenge_word_count":255,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501403168107,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":1370.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>The output of <code>TfidfVectorizer<\/code> is a scipy sparse matrix, not a simple numpy array.<\/p>\n<p>So either use a different function like:<\/p>\n<blockquote>\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/amazon\/common.py#L113\" rel=\"nofollow noreferrer\">write_spmatrix_to_sparse_tensor<\/a><\/p>\n<p>&quot;&quot;&quot;Writes a scipy sparse matrix to a sparse tensor&quot;&quot;&quot;<\/p>\n<\/blockquote>\n<p>See <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/27\" rel=\"nofollow noreferrer\">this issue<\/a> for more details.<\/p>\n<p><strong>OR<\/strong> first convert the output of <code>TfidfVectorizer<\/code> to a dense numpy array and then use your above code<\/p>\n<pre><code>xtrain_tfidf =  tfidf_vect.transform(X_train).toarray()   \nbuf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(buf, xtrain_tfidf, y_train)\n...\n...\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1592644375060,
        "Solution_link_count":2.0,
        "Solution_readability":15.5,
        "Solution_reading_time":11.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":71.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.3731275,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/151940-test-img.png?platform=QnA\" alt=\"151940-test-img.png\" \/>  <br \/>\nHow can I edit the existing tags and it will update to the tags in labelled images.  <br \/>\nFor example :  <br \/>\nEdit the tags['test1'] to new tag['Breeze'] and the tags['test1] in the image will replace with the new tag ['Breeze']  <br \/>\nHow can I do it in the azure UI or by using python<\/p>",
        "Challenge_closed_time":1637743807056,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637728063797,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to update labelled tags in Azure Machine Learning. They want to know how to edit existing tags and have them updated in labelled images, either through the Azure UI or using Python.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/638862\/how-to-update-the-labelled-tags-in-the-azure-machi",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":6.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":4.3731275,
        "Challenge_title":"How to update the labelled tags in the azure machine learning ?",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":69,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=2dae0229-1f81-4758-967b-90d1919f4e0f\">@Zi Xiang Yan  <\/a> You can edit the tags using the Details tab -&gt; Label Classes screen of your project from the portal. If the project is in paused state you can add new labels and choose the required option to continue or start over by keeping existing labels or removing all labels and relabel.     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/152182-image.png?platform=QnA\" alt=\"152182-image.png\" \/>    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.7,
        "Solution_reading_time":11.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":87.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":12.23786,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've been trying to upload some images to azure blob and then using <strong>ImageReader<\/strong> in <strong>Azure ML studio<\/strong> to read them from the blob. The problem is that ImageReader takes a lot of time to load images and I need it in real time. <br>\nI also tried making a <strong>csv<\/strong> of <strong>4 images (four rows)<\/strong> containing 800x600 pixels as columns <strong>(500,000 cols. approx)<\/strong> and tried simple <strong>Reader<\/strong>. Reader took <strong>31 mins<\/strong> to read the file from the blob.<br>\nI want to know the alternate methods of loading and reading images in Azure ML studio. If anyone know any other method or can share a helpful and relevant link.<br>\nPlease share if i can speed up ImageReader by any means.\nThanks<\/p>",
        "Challenge_closed_time":1441739836496,
        "Challenge_comment_count":0,
        "Challenge_created_time":1441695780200,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges with loading images quickly from Azure Blob using ImageReader in Azure ML studio. They have tried creating a csv file with 4 images and using a simple Reader, but it took 31 minutes to read the file from the blob. The user is seeking alternate methods to load and read images faster and is asking for suggestions to speed up ImageReader.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/32451243",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.3,
        "Challenge_reading_time":10.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":12.23786,
        "Challenge_title":"How to load images faster from Azure Blob?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":803.0,
        "Challenge_word_count":128,
        "Platform":"Stack Overflow",
        "Poster_created_time":1387426285030,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lahore, Pakistan",
        "Poster_reputation_count":2128.0,
        "Poster_view_count":211.0,
        "Solution_body":"<p>Look at the Azure CDN <a href=\"http:\/\/azure.microsoft.com\/en-us\/services\/cdn\/\" rel=\"nofollow\">http:\/\/azure.microsoft.com\/en-us\/services\/cdn\/<\/a> , after which the blobs will get an alternative url. My blob downloads became about 4 times faster after switching.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.0,
        "Solution_reading_time":3.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":27.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1341842709088,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jo\u00e3o Pessoa - PB, Brasil",
        "Answerer_reputation_count":314.0,
        "Answerer_view_count":43.0,
        "Challenge_adjusted_solved_time":38.6383375,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to build an autoencoder, which I'm sure I'm doing something wrong. I tried separating the creation of the model from the actual training but this is not really working out for me and is giving me the following error.<\/p>\n<pre><code>AssertionError: Could not compute output KerasTensor(type_spec=TensorSpec(shape=(None, 310), dtype=tf.float32, name=None), name='dense_7\/Sigmoid:0', description=&quot;created by layer 'dense_7'&quot;)\n<\/code><\/pre>\n<p>I'm doing this all using the Kedro framework. I have a pipeline.py file with the pipeline definition and a nodes.py with the functions that I want to use. So far, this is my project structure:<\/p>\n<p>pipelines.py:<\/p>\n<pre><code>from kedro.pipeline import Pipeline, node\nfrom .nodes.autoencoder_nodes import *\n\ndef train_autoencoder_pipeline():\n    return Pipeline([\n        # Build neural network\n        node(\n            build_models, \n            inputs=[\n                &quot;train_x&quot;, \n                &quot;params:autoencoder_n_hidden_layers&quot;,\n                &quot;params:autoencoder_latent_space_size&quot;,\n                &quot;params:autoencoder_regularization_strength&quot;,\n                &quot;params:seed&quot;\n                ],\n            outputs=dict(\n                pre_train_autoencoder=&quot;pre_train_autoencoder&quot;,\n                pre_train_encoder=&quot;pre_train_encoder&quot;,\n                pre_train_decoder=&quot;pre_train_decoder&quot;\n            ), name=&quot;autoencoder-create-models&quot;\n        ),\n        # Scale features\n        node(fit_scaler, inputs=&quot;train_x&quot;, outputs=&quot;autoencoder_scaler&quot;, name=&quot;autoencoder-fit-scaler&quot;),\n        node(tranform_scaler, inputs=[&quot;autoencoder_scaler&quot;, &quot;train_x&quot;], outputs=&quot;autoencoder_scaled_train_x&quot;, name=&quot;autoencoder-scale-train&quot;),\n        node(tranform_scaler, inputs=[&quot;autoencoder_scaler&quot;, &quot;test_x&quot;], outputs=&quot;autoencoder_scaled_test_x&quot;, name=&quot;autoencoder-scale-test&quot;),\n\n        # Train autoencoder\n        node(\n            train_autoencoder, \n            inputs=[\n                &quot;autoencoder_scaled_train_x&quot;,\n                &quot;autoencoder_scaled_test_x&quot;,\n                &quot;pre_train_autoencoder&quot;, \n                &quot;pre_train_encoder&quot;, \n                &quot;pre_train_decoder&quot;,\n                &quot;params:autoencoder_epochs&quot;,\n                &quot;params:autoencoder_batch_size&quot;,\n                &quot;params:seed&quot;\n            ],\n            outputs= dict(\n                autoencoder=&quot;autoencoder&quot;,\n                encoder=&quot;encoder&quot;,\n                decoder=&quot;decoder&quot;,\n                autoencoder_history=&quot;autoencoder_history&quot;,\n            ),\n            name=&quot;autoencoder-train-model&quot;\n        )])\n<\/code><\/pre>\n<p>nodes.py:<\/p>\n<pre><code>from sklearn.preprocessing import MinMaxScaler\nfrom tensorflow import keras\nimport tensorflow as tf\n\nfrom typing import Dict, Any, Tuple\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport logging\n\n\ndef build_models(data: pd.DataFrame, n_hidden_layers: int, latent_space_size: int, retularization_stregth: float, seed: int) -&gt; Tuple[keras.Model, keras.Model, keras.Model]:\n    assert n_hidden_layers &gt;= 1, &quot;There must be at least 1 hidden layer for the autoencoder&quot;\n    \n    n_features = data.shape[1]\n    tf.random.set_seed(seed)\n    input_layer = keras.Input(shape=(n_features,))\n    \n    hidden = keras.layers.Dense(n_features, kernel_regularizer=keras.regularizers.l1(retularization_stregth))(input_layer)\n    hidden = keras.layers.LeakyReLU()(hidden)\n    \n    for _ in range(n_hidden_layers - 1):\n        hidden = keras.layers.Dense(n_features, kernel_regularizer=keras.regularizers.l1(retularization_stregth))(hidden)\n        hidden = keras.layers.LeakyReLU()(hidden)\n    \n    encoded = keras.layers.Dense(latent_space_size, activation=&quot;sigmoid&quot;)(hidden)\n\n    hidden = keras.layers.Dense(n_features, kernel_regularizer=keras.regularizers.l1(retularization_stregth))(encoded)\n    hidden = keras.layers.LeakyReLU()(hidden)\n    \n    for _ in range(n_hidden_layers - 1):\n        hidden = keras.layers.Dense(n_features, kernel_regularizer=keras.regularizers.l1(retularization_stregth))(hidden)\n        hidden = keras.layers.LeakyReLU()(hidden)\n    \n\n    decoded = keras.layers.Dense(n_features, activation=&quot;sigmoid&quot;)(hidden)\n\n    # Defines the neural networks\n    autoencoder = keras.models.Model(inputs=input_layer, outputs=decoded)\n    encoder = keras.models.Model(inputs=input_layer, outputs=encoded)\n    decoder = keras.models.Model(inputs=input_layer, outputs=decoded)\n    autoencoder.compile(optimizer=&quot;adam&quot;, loss=&quot;mean_absolute_error&quot;)\n\n    return dict(\n        pre_train_autoencoder=autoencoder,\n        pre_train_encoder=encoder,\n        pre_train_decoder=decoder\n    )\n\ndef fit_scaler(data: pd.DataFrame) -&gt; MinMaxScaler:\n    scaler = MinMaxScaler()\n    scaler.fit(data)\n    return scaler\n\ndef tranform_scaler(scaler: MinMaxScaler, data: pd.DataFrame) -&gt; np.array:\n    return scaler.transform(data)\n\ndef train_autoencoder(\n    train_x: pd.DataFrame, test_x: pd.DataFrame, \n    autoencoder: keras.Model, encoder: keras.Model, decoder: keras.Model, \n    epochs: int, batch_size: int, seed: int) -&gt; Dict[str, Any]:\n\n    tf.random.set_seed(seed)\n    callbacks = [\n        keras.callbacks.History(),\n        keras.callbacks.EarlyStopping(patience=3)\n    ]\n    logging.info(train_x.shape)\n    logging.info(test_x.shape)\n\n    history = autoencoder.fit(\n        train_x, train_x,\n        validation_data=(test_x, test_x),\n        callbacks=callbacks, \n        epochs=epochs,\n        batch_size=batch_size\n    )\n\n    return dict(\n        autoencoder=autoencoder,\n        encoder=encoder,\n        decoder=decoder,\n        autoencoder_history=history,\n    )\n<\/code><\/pre>\n<p>catalog.yaml:<\/p>\n<pre><code>autoencoder_scaler:\n  type: pickle.PickleDataSet\n  filepath: data\/06_models\/autoencoder_scaler.pkl\n\nautoencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/autoencoder.h5\n\nencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/encoder.h5\n\ndecoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/decoder.h5\n\nautoencoder_train_x:\n  type: pandas.CSVDataSet\n  filepath: data\/04_feature\/autoencoder_train_x.csv\n\nautoencoder_test_x:\n  type: pandas.CSVDataSet\n  filepath: data\/04_feature\/autoencoder_test_x.csv\n<\/code><\/pre>\n<p>And finally parameters.yaml:<\/p>\n<pre><code>seed: 200\n# Autoencoder\nautoencoder_n_hidden_layers: 3\nautoencoder_latent_space_size: 15\nautoencoder_epochs: 100\nautoencoder_batch_size: 32\nautoencoder_regularization_strength: 0.001\n<\/code><\/pre>\n<p>I believe that Keras is not seeing the whole graph since they will be out of the scope for the buld_models function, but I'm not sure whether this is the case, or how to fix it. Any help would be appreciated.<\/p>",
        "Challenge_closed_time":1629132605447,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629078707297,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while building an autoencoder using Keras and Kedro framework. They have defined the pipeline and functions in separate files and are receiving an AssertionError related to the output of a KerasTensor. The user suspects that Keras is not seeing the whole graph due to the scope of the build_models function. They are seeking help to fix the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68796641",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":19.0,
        "Challenge_reading_time":85.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":62,
        "Challenge_solved_time":14.9717083334,
        "Challenge_title":"Building an autoencoder with Keras and Kedro",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":245.0,
        "Challenge_word_count":439,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423164285360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Itabira, Brazil",
        "Poster_reputation_count":856.0,
        "Poster_view_count":106.0,
        "Solution_body":"<p>I was able to set up your project locally and reproduce the error. To fix it, I had to add the <code>pre_train_*<\/code> outputs to the catalog as well. Therefore, it's my <code>catalog.yaml<\/code> file:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>autoencoder_scaler:\n  type: pickle.PickleDataSet\n  filepath: data\/06_models\/autoencoder_scaler.pkl\n\npre_train_autoencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/pre_train_autoencoder.h5\n\npre_train_encoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/pre_train_encoder.h5\n\npre_train_decoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/pre_train_decoder.h5\n\nautoencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/autoencoder.h5\n\nencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/encoder.h5\n\ndecoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/decoder.h5\n<\/code><\/pre>\n<p>Also, I changed the return of <code>train_autoencoder<\/code> node to:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>return dict(\n    autoencoder=autoencoder,\n    autoencoder_history=history.history,\n)\n<\/code><\/pre>\n<p>Note that I changed the <code>autoencoder_history<\/code> to return <code>history.history<\/code> since <code>MemoryDataset<\/code> can't pickle the object <code>history<\/code> by itself. The <code>history.history<\/code> is a dictionary with losses of train and validation sets.<\/p>\n<p>You can find the complete code <a href=\"https:\/\/gist.github.com\/arnaldog12\/a3f7801fe3910c02f4bcea8be61b910c\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1629217805312,
        "Solution_link_count":1.0,
        "Solution_readability":19.6,
        "Solution_reading_time":23.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":127.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1017.9547222222,
        "Challenge_answer_count":0,
        "Challenge_body":"In **Moon_Classification_Solution.ipynb**, the original code below would cause an error `ValueError: framework_version or py_version was None, yet image_uri was also None. Either specify both framework_version and py_version, or specify image_uri.` So I specified `py_version='py3'`, cause the framework version only supports `py2` and `py3`, which fixed the problem. Or I guess just add `!pip install sagemaker==1.72.0` like notebooks in another [**repo**](https:\/\/github.com\/udacity\/sagemaker-deployment\/blob\/master\/Mini-Projects\/IMDB%20Sentiment%20Analysis%20-%20XGBoost%20(Batch%20Transform)%20-%20Solution.ipynb) would also solve the issue.\r\n\r\n```\r\n# import a PyTorch wrapper\r\nfrom sagemaker.pytorch import PyTorch\r\n\r\n# specify an output path\r\n# prefix is specified above\r\noutput_path = 's3:\/\/{}\/{}'.format(bucket, prefix)\r\n\r\n# instantiate a pytorch estimator\r\nestimator = PyTorch(entry_point='train.py',\r\n                    source_dir='source_solution', # this should be just \"source\" for your code\r\n                    role=role,\r\n                    framework_version='1.0',\r\n                    py_version='py3', ### <------------------------ added a line here\r\n                    train_instance_count=1,\r\n                    train_instance_type='ml.c4.xlarge',\r\n                    output_path=output_path,\r\n                    sagemaker_session=sagemaker_session,\r\n                    hyperparameters={\r\n                        'input_dim': 2,  # num of features\r\n                        'hidden_dim': 20,\r\n                        'output_dim': 1,\r\n                        'epochs': 80 # could change to higher\r\n                    })\r\n```",
        "Challenge_closed_time":1623053107000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1619388470000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a bug in the Kubeflow pipeline Sagemaker component where if a node scales up or down, the component tries to create the same job which fails because Sagemaker does not allow the creation of the same name job. The user expected the job to resume from the previous state, but instead, it hangs or fails. The component controller should be able to detect this and resume the job from the existing state. The environment used was kfp-1.6.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/udacity\/ML_SageMaker_Studies\/issues\/15",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":18.94,
        "Challenge_repo_contributor_count":8.0,
        "Challenge_repo_fork_count":428.0,
        "Challenge_repo_issue_count":16.0,
        "Challenge_repo_star_count":350.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":1017.9547222222,
        "Challenge_title":"With \"sagemaker 2.31.1\", \"sagemaker.pytorch.PyTorch\" needs to specify both \"framework_version\" and \"py_version\"",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":136,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Resolved by fixing Sagemaker's version to 1.72.0.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":0.63,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":7.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1359884693920,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":9637.0,
        "Answerer_view_count":609.0,
        "Challenge_adjusted_solved_time":2068.9864736111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>im trying to setup <a href=\"https:\/\/www.comet.ml\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml<\/a> to log my experiment details <\/p>\n\n<p>getting strange error:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"train.py\", line 7, in &lt;module&gt;\n    from comet_ml import Experiment\nImportError: No module named comet_ml\n<\/code><\/pre>\n\n<p>trying in python 2 and python3<\/p>",
        "Challenge_closed_time":1506066553020,
        "Challenge_comment_count":0,
        "Challenge_created_time":1506066265147,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to set up comet.ml to log experiment details but is encountering an error stating \"No module named comet_ml\" when trying to import Experiment from comet_ml in their train.py file. They have tried using both Python 2 and Python 3.",
        "Challenge_last_edit_time":1506066568087,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46359436",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":5.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.0799647222,
        "Challenge_title":"How to configure comet (comet.ml) to track Keras?",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1208.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_created_time":1505841491572,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>it seems like comet isn't installed on your machine.<\/p>\n\n<p>you can use :<\/p>\n\n<pre><code>pip3 install comet_ml\npip install comet_ml\n<\/code><\/pre>\n\n<p>take a look at the example projects at: <\/p>\n\n<p><a href=\"https:\/\/github.com\/comet-ml\/comet-quickstart-guide\" rel=\"nofollow noreferrer\">https:\/\/github.com\/comet-ml\/comet-quickstart-guide<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1513514919392,
        "Solution_link_count":2.0,
        "Solution_readability":9.9,
        "Solution_reading_time":4.6,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":33.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":1402536093248,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":817703.0,
        "Answerer_view_count":106500.0,
        "Challenge_adjusted_solved_time":0.2592397222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have dataframe with columns <\/p>\n\n<pre><code>date    open    high    low     close   adjclose    volume\n<\/code><\/pre>\n\n<p>I want to add one more column named \"result\"(1 if close > open, 0 if close &lt; open)<\/p>\n\n<p>I do<\/p>\n\n<pre><code># Map 1-based optional input ports to variables\ndata &lt;- maml.mapInputPort(1) # class: data.frame\n\n\n\n# calculate pass\/fail\ndata$result &lt;- as.factor(sapply(data$close,function(res) \n    if (res - data$open &gt;= 0) '1' else '0'))\n\n# Select data.frame to be sent to the output Dataset port\nmaml.mapOutputPort(\"data\");\n<\/code><\/pre>\n\n<p>But I have only 1 in result. Where is the problem?<\/p>",
        "Challenge_closed_time":1554572339496,
        "Challenge_comment_count":1,
        "Challenge_created_time":1554571406233,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to add a new column named \"result\" to a dataframe using Azure ML and R scripts. The column should contain 1 if the \"close\" value is greater than the \"open\" value, and 0 if it is less. However, the user is encountering an issue where only 1 is being added to the column.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55551617",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.3,
        "Challenge_reading_time":7.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.2592397222,
        "Challenge_title":"Azure ML and r scripts",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":77.0,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1553239567403,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":67.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>The <code>if\/else<\/code> can return only a single TRUE\/FALSE and is not vectorized for length > 1.  It may be suitable to use <code>ifelse<\/code> (but that is also not required and would be less efficient compared to direct coersion of logical vector to binary (<code>as.integer<\/code>).   In the OP's code, the 'close' column elements are looped  (<code>sapply<\/code>) and subtracted from the whole 'open' column.  The intention might be to do elementwise subtraction.  In that case, <code>-<\/code> between the columns is much cleaner and efficient (as these operations are vectorized)<\/p>\n\n<pre><code>data$result &lt;- with(data, factor(as.integer((close - open) &gt;= 0)))\n<\/code><\/pre>\n\n<p>In the above, we get the difference between the columns ('close', 'open'), check if it is greater than or equal to 0 (returns logical vector), convert it to binary (<code>as.integer<\/code> - TRUE -> 1, FALSE -> 0) and then change it to <code>factor<\/code> type (if needed)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.9,
        "Solution_reading_time":12.1,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":137.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":24.1505972222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi, I am getting the error from the subject line when i try to inner join a dataset of 850K rows and 3 columns (parquet data file of around 4mb) with another with 300K rows and 10 columns (parquet data file is about 1mb). I'm using Azure ML Studio Designer  <\/p>\n<p>My compute is Standard Dv2 Family vCPUs (20% of utilization).  <\/p>\n<p>I was surprised by this hitting a limit. Any idea on how i should proceed?<\/p>",
        "Challenge_closed_time":1619444233763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619357291613,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a ModuleExceptionMessage error stating that the memory has been exhausted while trying to inner join a dataset of 850K rows and 3 columns with another dataset of 300K rows and 10 columns in Azure ML Studio Designer. The user's compute is Standard Dv2 Family vCPUs with 20% utilization. The user is seeking advice on how to proceed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/370636\/moduleexceptionmessage-moduleoutofmemory-memory-ha",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":6.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":24.1505972222,
        "Challenge_title":"ModuleExceptionMessage:ModuleOutOfMemory: Memory has been exhausted, unable to complete running of module.",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":87,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>i manage to do this by trainning the model in a subset of records (using the Sample model).    <\/p>\n<p>Also noted that the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/apply-sql-transformation\">documentation<\/a> implies that an out of memory error is dependant on the RAM of the client \/ Designer user machine not the compute selected (or at least that is my understanding of the note at the beginning of the doc)    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.0,
        "Solution_reading_time":5.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":7.9073555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I trained multiple models with different configuration for a custom hyperparameter search. I use pytorch_lightning and its logging (TensorboardLogger).\nWhen running my training script after Task.init() ClearML auto-creates a Task and connects the logger output to the server.<\/p>\n<p>I log for each straining stage <code>train<\/code>, <code>val<\/code> and <code>test<\/code> the following scalars at each epoch: <code>loss<\/code>, <code>acc<\/code> and <code>iou<\/code><\/p>\n<p>When I have multiple configuration, e.g. <code>networkA<\/code> and <code>networkB<\/code> the first training log its values to <code>loss<\/code>, <code>acc<\/code> and <code>iou<\/code>, but the second to <code>networkB:loss<\/code>, <code>networkB:acc<\/code> and <code>networkB:iou<\/code>. This makes values umcomparable.<\/p>\n<p>My training loop with Task initalization looks like this:<\/p>\n<pre><code>names = ['networkA', networkB']\nfor name in names:\n     task = Task.init(project_name=&quot;NetworkProject&quot;, task_name=name)\n     pl_train(name)\n     task.close()\n<\/code><\/pre>\n<p>method pl_train is a wrapper for whole training with Pytorch Ligtning. No ClearML code is inside this method.<\/p>\n<p>Do you have any hint, how to properly use the usage of a loop in a script using completly separated tasks?<\/p>\n<hr \/>\n<p>Edit: ClearML version was 0.17.4. Issue is fixed in main branch.<\/p>",
        "Challenge_closed_time":1613773903383,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613745436903,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user trained multiple models with different configurations for a custom hyperparameter search using pytorch_lightning and TensorboardLogger for logging. When running the training script, ClearML auto-creates a Task and connects the logger output to the server. However, when there are multiple configurations, the logged values have different names, making them incomparable. The user is seeking advice on how to properly use a loop in a script using completely separated tasks.",
        "Challenge_last_edit_time":1614004159640,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66279581",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":18.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":7.9073555556,
        "Challenge_title":"ClearML multiple tasks in single script changes logged value names",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":279.0,
        "Challenge_word_count":172,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604391794420,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":89.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Disclaimer I'm part of the ClearML (formerly Trains) team.<\/p>\n<p><code>pytorch_lightning<\/code> is creating a new Tensorboard for each experiment. When ClearML logs the TB scalars, and it captures the same scalar being re-sent again, it adds a prefix so if you are reporting the same metric it will not overwrite the previous one. A good example would be reporting <code>loss<\/code> scalar in the training phase vs validation phase (producing &quot;loss&quot; and &quot;validation:loss&quot;). It might be the <code>task.close()<\/code> call does not clear the previous logs, so it &quot;thinks&quot; this is the same experiment, hence adding the prefix <code>networkB<\/code> to the <code>loss<\/code>. As long as you are closing the Task after training is completed you should have all experiments log with the same metric\/variant (title\/series). I suggest opening a GitHub issue, this should probably be considered a bug.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":11.68,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":135.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1250158552416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Challenge_adjusted_solved_time":3708.3794758333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The input data in the model includes column ControlNo.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/eqULH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eqULH.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But I don't want this column being part of learning process so I'm using <code>Select Columns in Dataset<\/code> to exclude <code>ControlNo<\/code> column.<\/p>\n<p>But as a output I want those columns:<\/p>\n<pre><code>ControlNo, Score Label, Score Probability\n<\/code><\/pre>\n<p>So basically I need NOT to include column <code>ControlNo<\/code> into learning process,\nbut have it as output along with <code>Score Label<\/code> column.<\/p>\n<p>How can I do that?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/iPlpa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/iPlpa.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1533111465720,
        "Challenge_comment_count":3,
        "Challenge_created_time":1519761299607,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to exclude the \"ControlNo\" column from the learning process in Azure ML but include it as output along with \"Score Label\" column. They are using \"Select Columns in Dataset\" to exclude the \"ControlNo\" column but need guidance on how to include it as output.",
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49016896",
        "Challenge_link_count":4,
        "Challenge_participation_count":4,
        "Challenge_readability":11.1,
        "Challenge_reading_time":12.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":3708.3794758333,
        "Challenge_title":"How to bypass ID column without being used in the training model but have it as output - Azure ML",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":582.0,
        "Challenge_word_count":110,
        "Platform":"Stack Overflow",
        "Poster_created_time":1457596845392,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Diego, CA, United States",
        "Poster_reputation_count":4046.0,
        "Poster_view_count":825.0,
        "Solution_body":"<p>Instead of removing the ControlNo column from the dataset, you can use the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/edit-metadata\" rel=\"nofollow noreferrer\">Edit Metadata<\/a> module to clear its \"Feature\" flag - just select the column and set <strong>Fields<\/strong> to <strong>Clear feature<\/strong>. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/EUy9A.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EUy9A.png\" alt=\"Edit Metadata settings\"><\/a><\/p>\n\n<p>This will cause the Azure ML Studio algorithms to ignore it during training, and you'll be able to return it as part of your output. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.2,
        "Solution_reading_time":8.65,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":69.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":367.1658333333,
        "Challenge_answer_count":0,
        "Challenge_body":"Hi!\r\n\r\nI have installed all required packages by `pip install -r requrements.txt` and tried to run hyperparametric search using the [file](https:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/main\/configs\/hparams_search\/mnist_optuna.yaml):\r\n```\r\ntrain.py -m hparams_search=mnist_optuna experiment=example\r\n``` \r\nI faced 2 problems:\r\n\r\n# 1. hydra-optuna-sweeper problem\r\n\r\nI got the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\utils.py\", line 213, in run_and_report\r\n    return func()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\utils.py\", line 461, in <lambda>\r\n    lambda: hydra.multirun(\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\hydra.py\", line 162, in multirun\r\n    ret = sweeper.sweep(arguments=task_overrides)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra_plugins\\hydra_optuna_sweeper\\optuna_sweeper.py\", line 52, in sweep\r\n    return self.sweeper.sweep(arguments)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra_plugins\\hydra_optuna_sweeper\\_impl.py\", line 289, in sweep\r\n    assert self.search_space is None\r\nAssertionError\r\n```\r\nThe same error was reported in [this issue](https:\/\/github.com\/facebookresearch\/hydra\/issues\/2253).\r\n\r\nFile [requrements.txt](https:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/main\/requirements.txt) contains the following versions for hydra-optuna-sweeper:\r\n```\r\n# --------- hydra --------- #\r\nhydra-core>=1.1.0\r\nhydra-colorlog>=1.1.0\r\nhydra-optuna-sweeper>=1.1.0\r\n```\r\nBut the latest versions of the packages are installing:\r\n```\r\nhydra-colorlog==1.2.0\r\nhydra-core==1.2.0\r\nhydra-optuna-sweeper==1.2.0\r\n```\r\n\r\nIf I understand correctly, optuna sweeper's syntax has changed in hydra since version 1.2.0. When I change the syntax to the new version (as it was in mentioned above [issue](https:\/\/github.com\/facebookresearch\/hydra\/issues\/2253)):\r\n```yaml\r\nhydra:\r\n  sweeper:\r\n    ...\r\n    params:\r\n      datamodule.batch_size: choice(32,64,128)\r\n      model.lr: interval(0.0001, 0.2)\r\n      model.net.lin1_size: choice(32, 64, 128, 256, 512)\r\n      model.net.lin2_size: choice(32, 64, 128, 256, 512)\r\n      model.net.lin3_size: choice(32, 64, 128, 256, 512)\r\n```\r\neverything works without errors.\r\n\r\n# 2. wandb problem\r\nAfter the command `pip install -r requrements.txt` wandb==0.12.20 was installed.\r\nWhen running the training process with this logger:\r\n```\r\ntrain.py -m hparams_search=mnist_optuna experiment=example logger=wandb\r\n```\r\nThe first run with the certian parameters combination finished successfully, the second run had the error:\r\n\r\n```\r\nException in thread StreamThr:\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\threading.py\", line 973, in _bootstrap_inner\r\n    self.run()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 40, in run\r\n    self._target(**self._kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\internal\\internal.py\", line 85, in wandb_internal\r\n    configure_logging(_settings.log_internal, _settings._log_level)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\internal\\internal.py\", line 189, in configure_logging\r\n    log_handler = logging.FileHandler(log_fname)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\logging\\__init__.py\", line 1146, in __init__\r\n    StreamHandler.__init__(self, self._open())\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\logging\\__init__.py\", line 1175, in _open\r\n    return open(self.baseFilename, self.mode, encoding=self.encoding,\r\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\yusip\\\\Desktop\\\\lightning-hydra-template-main\\\\logs\\\\experiments\\\\multiruns\\\\simple_dense_net\\\\2022-06-30_14-36-03\\\\0\\\\wandb\\\\run-2022\r\n0630_143648-2vxuij78\\\\logs\\\\debug-internal.log'\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\__main__.py\", line 3, in <module>\r\n    cli.cli(prog_name=\"python -m wandb\")\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\cli\\cli.py\", line 96, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\cli\\cli.py\", line 285, in service\r\n    server.serve()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\server.py\", line 140, in serve\r\n    mux.loop()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 332, in loop\r\n    raise e\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 330, in loop\r\n    self._loop()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 323, in _loop\r\n    self._process_action(action)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 288, in _process_action\r\n    self._process_add(action)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 208, in _process_add\r\n    stream.start_thread(thread)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 68, in start_thread\r\n    self._wait_thread_active()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 73, in _wait_thread_active\r\n    assert result\r\nAssertionError\r\nProblem at: C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py 357 experiment\r\nwandb: ERROR Error communicating with wandb process\r\nwandb: ERROR try: wandb.init(settings=wandb.Settings(start_method='fork'))\r\nwandb: ERROR or:  wandb.init(settings=wandb.Settings(start_method='thread'))\r\nwandb: ERROR For more info see: https:\/\/docs.wandb.ai\/library\/init#init-start-error\r\nError executing job with overrides: ['datamodule.batch_size=32', 'model.lr=0.09357304154313738', 'model.net.lin1_size=256', 'model.net.lin2_size=512', 'model.net.lin3_size=256', 'hparams_search=mnist_op\r\ntuna', 'experiment=example', 'logger=wandb']\r\nError in call to target 'pytorch_lightning.loggers.wandb.WandbLogger':\r\nUsageError(\"Error communicating with wandb process\\ntry: wandb.init(settings=wandb.Settings(start_method='fork'))\\nor:  wandb.init(settings=wandb.Settings(start_method='thread'))\\nFor more info see: htt\r\nps:\/\/docs.wandb.ai\/library\/init#init-start-error\")\r\nfull_key: logger.wandb\r\n```\r\nIt is not clear, which parameters should be passed to pytorch lighting wrapper when initializinig this logger, to avoid this error.\r\n\r\n\r\n",
        "Challenge_closed_time":1657912525000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1656590728000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered an issue with the requirements.txt file not having the complete dependency tree defined while working with Sorts in Sagemaker. The possible fix is to modify the requirements.txt file to include the complete tree of dependencies and their respective versions.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/362",
        "Challenge_link_count":5,
        "Challenge_participation_count":2,
        "Challenge_readability":17.7,
        "Challenge_reading_time":99.61,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":341.0,
        "Challenge_repo_issue_count":412.0,
        "Challenge_repo_star_count":2043.0,
        "Challenge_repo_watch_count":19.0,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":79,
        "Challenge_solved_time":367.1658333333,
        "Challenge_title":"hydra-optuna-sweeper and wandb versions conflict",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":523,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @GillianGrayson \r\n\r\nFor **1. hydra-optuna-sweeper problem**, it has been modified in release_1.4. You can find [here](https:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/7e67c4692590550e7b703655845e59508eb071bb\/configs\/hparams_search\/mnist_optuna.yaml#L49)\r\n\r\n @GillianGrayson ty for reporting, the problems have been fixed on the current `main` branch.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.1,
        "Solution_reading_time":4.76,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":30.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1250158552416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Challenge_adjusted_solved_time":45.0161775,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>How do I replace the values in a specific column with a particular value based on a condition in Azure ML Studio. I can do this using pandas in python as foolows:<\/p>\n\n<pre><code>df.loc[df['col_name'] &gt; 1990, 'col_name'] = 1\n<\/code><\/pre>\n\n<p>I'm trying to find a Module in Azure Machine Learning Studio that does the equivalent of this. <\/p>\n\n<p>I understand there is a replace option under the ConverToDataset module and a Replace Discrete Values module. But neither of these seems to do what I want. Is there an option to replace the values in just one column to a specific value based on a condition?<\/p>",
        "Challenge_closed_time":1557391929596,
        "Challenge_comment_count":0,
        "Challenge_created_time":1557229871357,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in replacing values in a specific column based on a condition in Azure ML Studio. They are looking for a module that can perform the equivalent of a pandas command in Python, but the available options in Azure ML Studio do not seem to meet their requirements.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56021977",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":8.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":45.0161775,
        "Challenge_title":"Replace values in a column based on a condition in Azure ML Studio",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":564.0,
        "Challenge_word_count":115,
        "Platform":"Stack Overflow",
        "Poster_created_time":1450260166772,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1587.0,
        "Poster_view_count":540.0,
        "Solution_body":"<p>You can use either the more general <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/apply-sql-transformation\" rel=\"nofollow noreferrer\">Apply SQL Transformation<\/a>, or the dedicated <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/clip-values\" rel=\"nofollow noreferrer\">Clip Values<\/a> module. If all else fails, there's also <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/execute-python-script\" rel=\"nofollow noreferrer\">Execute Python Script<\/a>.<\/p>\n\n<p>Personally, for your example I'd use <code>Clip Values<\/code> with <code>Clip Peaks<\/code> and <code>Upper Threshold<\/code> set. For more complex rules I'd use either <code>Apply SQL Transformation<\/code> or <code>Execute Python Script<\/code>, depending on the rules but favouring SQL :).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":17.8,
        "Solution_reading_time":11.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":71.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1305851487736,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5993.0,
        "Answerer_view_count":457.0,
        "Challenge_adjusted_solved_time":5.7160508333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a couple of projects that are using and updating the same data sources. I recently learned about <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries\" rel=\"nofollow noreferrer\">dvc's data registries<\/a>, which sound like a great way of versioning data across these different projects (e.g. scrapers, computational pipelines).<\/p>\n<p>I have put all of the relevant data into <code>data-registry<\/code> and then I imported the relevant files into the scraper project with:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ poetry run dvc import https:\/\/github.com\/username\/data-registry raw\n<\/code><\/pre>\n<p>where <code>raw<\/code> is a directory that stores the scraped data. This seems to have worked properly, but then when I went to build <a href=\"https:\/\/dvc.org\/doc\/start\/data-pipelines\" rel=\"nofollow noreferrer\">a dvc pipeline<\/a> that <em>outputted<\/em> data into a file that was already tracked by dvc, I got an error:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ dvc run -n menu_items -d src\/ -o raw\/menu_items\/restaurant.jsonl scrapy crawl restaurant\nERROR: Paths for outs:                                                \n'raw'('raw.dvc')\n'raw\/menu_items\/restaurant.jsonl'('menu_items')\noverlap. To avoid unpredictable behaviour, rerun command with non overlapping outs paths.\n<\/code><\/pre>\n<p>Can someone help me understand what is going on here? <strong>What is the best way to use data registries to share and update data across projects?<\/strong><\/p>\n<p>I would ideally like to update the data-registry with new data from the scraper project and then allow other dependent projects to update their data when they are ready to do so.<\/p>",
        "Challenge_closed_time":1614537291720,
        "Challenge_comment_count":1,
        "Challenge_created_time":1614516713937,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use dvc's data registries to version data across different projects. They imported relevant files into the scraper project, but when they tried to build a dvc pipeline that outputted data into a file already tracked by dvc, they received an error message. The user is seeking help to understand what is going on and the best way to use data registries to share and update data across projects.",
        "Challenge_last_edit_time":1614699218992,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66409283",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":21.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":5.7160508333,
        "Challenge_title":"updating data in dvc registry from other projects",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":388.0,
        "Challenge_word_count":217,
        "Platform":"Stack Overflow",
        "Poster_created_time":1294268936687,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Chicago, IL",
        "Poster_reputation_count":2893.0,
        "Poster_view_count":168.0,
        "Solution_body":"<p>When you <code>import<\/code> (or <code>add<\/code>) something into your project, a .dvc file is created with that lists that something (in this case the <code>raw\/<\/code> dir) as an &quot;output&quot;.<\/p>\n<p>DVC doesn't allow overlapping outputs among .dvc files or dvc.yaml stages, meaning that your &quot;menu_items&quot; stage shouldn't write to <code>raw\/<\/code> since it's already under the control of <code>raw.dvc<\/code>.<\/p>\n<p>Can you make a separate directory for the pipeline outputs? E.g. use <code>processed\/menu_items\/restaurant.jsonl<\/code><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1614698988012,
        "Solution_link_count":0.0,
        "Solution_readability":9.0,
        "Solution_reading_time":7.26,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":69.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1460437080990,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":386.0,
        "Answerer_view_count":42.0,
        "Challenge_adjusted_solved_time":1.4118238889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How do you rename an Azure ML Experiment?\u00a0I cannot see any property field you can set. All I can find is Save\u00a0As something else, then delete the\u00a0original\u00a0experiment. <\/p>\n\n<p>When I\u00a0Save for the first time, it doesn't ask me for a name, it just saves it with a standard date. <\/p>\n\n<p>Am I missing something simple and obvious? <\/p>",
        "Challenge_closed_time":1485656495463,
        "Challenge_comment_count":0,
        "Challenge_created_time":1485651412897,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble renaming an Azure ML experiment as there is no visible property field to set a new name. The only option found is to save the experiment as something else and delete the original. The user is seeking a simpler solution.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41916570",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.3,
        "Challenge_reading_time":4.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.4118238889,
        "Challenge_title":"Rename an Azure ML experiment",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":753.0,
        "Challenge_word_count":67,
        "Platform":"Stack Overflow",
        "Poster_created_time":1407201889907,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Redmond, WA",
        "Poster_reputation_count":2674.0,
        "Poster_view_count":355.0,
        "Solution_body":"<p>put the cursor on the name text when an experiment is open, and edit away.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":0.98,
        "Solution_score_count":3.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":15.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426685176247,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Athens, Greece",
        "Answerer_reputation_count":54268.0,
        "Answerer_view_count":22884.0,
        "Challenge_adjusted_solved_time":0.4555380556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Looks like I have 672 mission values, according to statistics. \nThere are NULL value in QuotedPremium column.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/p9Z3X.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/p9Z3X.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I implemented Clean Missing Data module where it should substitute missing values with 0, but for some reason I'm still seeing NULL values as QuotedPremium, but...it says that missing values are = 0<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/PDg97.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PDg97.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/BEdHD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BEdHD.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Here you see it tells me that missing values = 0, but there are still NULLs <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/WKlGZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WKlGZ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>So what really happened after I ran Clean Missing Data module? Why it ran succesfully but there are still NULL values, even though it tells that number of missing values are 0. <\/p>",
        "Challenge_closed_time":1515030180700,
        "Challenge_comment_count":2,
        "Challenge_created_time":1515028540763,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has encountered an issue with missing values in Azure Machine Learning Studio. Despite implementing the Clean Missing Data module to substitute missing values with 0, there are still NULL values in the QuotedPremium column. The module indicates that missing values are equal to 0, but the user is still seeing NULLs. The user is seeking clarification on what happened after running the Clean Missing Data module.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48087407",
        "Challenge_link_count":8,
        "Challenge_participation_count":4,
        "Challenge_readability":10.8,
        "Challenge_reading_time":17.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":0.4555380556,
        "Challenge_title":"How to deal with missing values in Azure Machine Learning Studio",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1556.0,
        "Challenge_word_count":144,
        "Platform":"Stack Overflow",
        "Poster_created_time":1457596845392,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Diego, CA, United States",
        "Poster_reputation_count":4046.0,
        "Poster_view_count":825.0,
        "Solution_body":"<p><code>NULL<\/code> is indeed a value; entries containing NULLs are <em>not<\/em> missing, hence they are neither cleaned with the 'Clean Missing Data' operator nor reported as missing.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.6,
        "Solution_reading_time":2.41,
        "Solution_score_count":2.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":26.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1408370821672,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Berlin, Germany",
        "Answerer_reputation_count":2521.0,
        "Answerer_view_count":197.0,
        "Challenge_adjusted_solved_time":6.5096966667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I run mlflow as following:<\/p>\n\n<p><code>Dockerfile<\/code> contains the the following CMD command<\/p>\n\n<pre><code>CMD mlflow server \\\n    --host 0.0.0.0 \\\n    --backend-store-uri \"${BACKEND_STORE_URI}\" \\\n    --default-artifact-root \"${DEFAULT_ARTIFACT_ROOT}\"\n<\/code><\/pre>\n\n<p>after <code>docker run --rm --name mlflow -p 5000:5000 -e BACKEND_STORE_URI=mssql+pymssql:\/\/user:pass@mybackendstoreuri\/mlflow mlflow<\/code><\/p>\n\n<p>it shows <\/p>\n\n<pre><code>INFO  [alembic.runtime.migration] Context impl MSSQLImpl.\nINFO  [alembic.runtime.migration] Will assume transactional DDL.\nINFO  [alembic.runtime.migration] Context impl MSSQLImpl.\nINFO  [alembic.runtime.migration] Will assume transactional DDL.\n<\/code><\/pre>\n\n<p>but then, the container exits without starting the server. <\/p>\n\n<p>Without specifying <code>backend store uri<\/code>, I can see the logs related to binding to host, and the container does not exist<\/p>\n\n<p>How to run mlflow tracking server and use backend store uri ?<\/p>",
        "Challenge_closed_time":1569316457008,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569314315910,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue where the mlflow tracking server does not start after specifying the backend-store-uri in the Dockerfile CMD command. The container exits without starting the server, but without specifying the backend store uri, the logs related to binding to the host are visible and the container does not exit. The user is seeking a solution to run the mlflow tracking server and use the backend store uri.",
        "Challenge_last_edit_time":1569315366808,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58076263",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":13.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.5947494444,
        "Challenge_title":"mlflow tracking server does not start after specifying backend-store-uri",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":3168.0,
        "Challenge_word_count":110,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408370821672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berlin, Germany",
        "Poster_reputation_count":2521.0,
        "Poster_view_count":197.0,
        "Solution_body":"<p>The root cause is <\/p>\n\n<pre><code>MLflow UI and client code expects a default experiment with ID 0.\nThis method uses SQL insert statement to create the default experiment as a hack, since\nexperiment table uses 'experiment_id' column is a PK and is also set to auto increment.\nMySQL and other implementation do not allow value '0' for such cases.\n<\/code><\/pre>\n\n<p>ref: <a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/v1.2.0\/mlflow\/store\/sqlalchemy_store.py#L171\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/blob\/v1.2.0\/mlflow\/store\/sqlalchemy_store.py#L171<\/a> <\/p>\n\n<p>No error is raised during migration, so no error shows up, and alembic version is the newest when fail silently. \nref:<a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/v1.2.0\/mlflow\/store\/db_migrations\/env.py#L71\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/blob\/v1.2.0\/mlflow\/store\/db_migrations\/env.py#L71<\/a><\/p>\n\n<p>If using the same idea as the MySQL test(<a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/v1.2.0\/mlflow\/store\/sqlalchemy_store.py#L171\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/blob\/v1.2.0\/mlflow\/store\/sqlalchemy_store.py#L171<\/a>), the exception is raised - <code>Cannot insert explicit value for identity column in table 'experiment' when IDENTITY_INSERT is set to OFF.<\/code><\/p>\n\n<p>Test snippet:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>class TestSqlAlchemyStoreMssqlDb(unittest.TestCase):\n    \"\"\"\n    Run tests against a MSSQL database\n    \"\"\"\n    def setUp(self):\n        db_username = \"test\"\n        db_password = \"test\"\n        host = \"test\"\n        db_name = \"TEST_DB\"\n\n        db_server_url = \"mssql+pymssql:\/\/%s:%s@%s\" % (db_username, db_password, host)\n        self._engine = sqlalchemy.create_engine(db_server_url)\n\n        self._db_url = \"%s\/%s\" % (db_server_url, db_name)\n        print(\"Connect to %s\" % self._db_url)\n\n    def test_store(self):\n        self.store = SqlAlchemyStore(db_uri=self._db_url, default_artifact_root=ARTIFACT_URI)\n<\/code><\/pre>\n\n<p>Using postgres server completes the migration as the log shows.<\/p>\n\n<pre><code>mlflow_1    | 2019\/09\/24 09:03:55 INFO mlflow.store.sqlalchemy_store: Creating initial MLflow database tables...\nmlflow_1    | 2019\/09\/24 09:03:55 INFO mlflow.store.db.utils: Updating database tables at postgresql:\/\/postgres:postgres@postgres:5432\/postgres\nmlflow_1    | INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\nmlflow_1    | INFO  [alembic.runtime.migration] Will assume transactional DDL.\nmlflow_1    | INFO  [alembic.runtime.migration] Running upgrade  -&gt; 451aebb31d03, add metric step\nmlflow_1    | INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -&gt; 90e64c465722, migrate user column to tags\nmlflow_1    | INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -&gt; 181f10493468, allow nulls for metric values\nmlflow_1    | INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -&gt; df50e92ffc5e, Add Experiment Tags Table\nmlflow_1    | INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -&gt; 7ac759974ad8, Update run tags with larger limit\nmlflow_1    | INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\nmlflow_1    | INFO  [alembic.runtime.migration] Will assume transactional DDL.\nmlflow_1    | [2019-09-24 09:03:55 +0000] [15] [INFO] Starting gunicorn 19.9.0\nmlflow_1    | [2019-09-24 09:03:55 +0000] [15] [INFO] Listening at: http:\/\/0.0.0.0:5000 (15)\nmlflow_1    | [2019-09-24 09:03:55 +0000] [15] [INFO] Using worker: sync\nmlflow_1    | [2019-09-24 09:03:55 +0000] [18] [INFO] Booting worker with pid: 18\nmlflow_1    | [2019-09-24 09:03:56 +0000] [22] [INFO] Booting worker with pid: 22\nmlflow_1    | [2019-09-24 09:03:56 +0000] [26] [INFO] Booting worker with pid: 26\nmlflow_1    | [2019-09-24 09:03:56 +0000] [27] [INFO] Booting worker with pid: 27\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1569338801716,
        "Solution_link_count":7.0,
        "Solution_readability":12.1,
        "Solution_reading_time":48.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":34.0,
        "Solution_word_count":356.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1618.0322222222,
        "Challenge_answer_count":0,
        "Challenge_body":"Experience is broken since every DVC command changes `.gitignore` now - makes it very annoying to jump between branches.",
        "Challenge_closed_time":1588739140000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1582914224000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error when using `fds clone` for a non-DVC repository. The error message states that the user is not inside a DVC repository. The user suggests that after cloning the Git server, FDS should check if the repo contains DVC files. If it does, FDS will start a wizard to set the user name and password for each remote storage in the local config and pull all the files from the remotes. If it doesn't contain DVC files, FDS will initialize DVC and start a wizard to set the remote user name, password, and name.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/iterative\/example-repos-dev\/issues\/12",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.9,
        "Challenge_reading_time":2.2,
        "Challenge_repo_contributor_count":17.0,
        "Challenge_repo_fork_count":11.0,
        "Challenge_repo_issue_count":154.0,
        "Challenge_repo_star_count":15.0,
        "Challenge_repo_watch_count":14.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1618.0322222222,
        "Challenge_title":"need to rebuild get-started with the latest DVC version",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":26,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This will be done as part of iterative\/dvc.org\/issues\/599. Shall we close here? Yep.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.6,
        "Solution_reading_time":1.06,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":62.5413425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Azure machine learning designer :  <\/p>\n<p>I have a dataset on the designer connected to a Normalize data module but it keeps loading when i try to Edit columns on Normalize data module with no result or errors.  <br \/>\nThe same thing happens with Select columns in dataset module.  <\/p>\n<p>I have tried to recreate and restart and even deleted the whole resource group but no luck.  <br \/>\nI tried on both mac and windows with different browsers but still getting stuck on the same place.  <\/p>\n<p>any idea on how to solve this issue?   <\/p>\n<p>Thanks!  <\/p>\n<p>Screenshot:  <br \/>\n<a href=\"https:\/\/i.imgur.com\/P0oWrGR.png\">https:\/\/i.imgur.com\/P0oWrGR.png<\/a>  <\/p>",
        "Challenge_closed_time":1617711247023,
        "Challenge_comment_count":12,
        "Challenge_created_time":1617486098190,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with Azure machine learning designer where the Edit columns on Normalize data module and Select columns in dataset module are stuck on loading with no errors. The user has tried recreating, restarting, and deleting the resource group but the issue persists on both Mac and Windows with different browsers. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/343332\/azure-machine-learning-designer-edit-columns-stuck",
        "Challenge_link_count":1,
        "Challenge_participation_count":13,
        "Challenge_readability":8.2,
        "Challenge_reading_time":8.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":62.5413425,
        "Challenge_title":"Azure machine learning designer - edit columns stuck on loading",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":110,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=7246d026-6219-48cf-ab02-a9826f9174a5\">@Ayush Bhardwaj  <\/a> <a href=\"\/users\/na\/?userid=1f1ee936-54c7-4809-839f-2bd5ff84ec7b\">@yazeen jasim  <\/a> <a href=\"\/users\/na\/?userid=491da3e9-5349-4967-9851-b193f28abcac\">@Anshul Sharma  <\/a> This issue is now fixed in all regions and it does not require an additional parameter to be added to the URL. Please try and let us know if it works fine.<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":5.49,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":44.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1576813179640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":39.3623194445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a data set with about 7999 attributes and 39 labels, with 3339 total observations (resulting in 3339x8038 data set), and I'm trying to upload id to Azure ML platform.\nI've selected the 'type' as 'tabular', encoding as 'utf-8', no row skipping, and use header from first file.\nThe problem is, that the headers are still not included and the data is interpreted as string with 0s, 1s, and commas (see pic <a href=\"https:\/\/imgur.com\/a\/QdQNt1y\" rel=\"nofollow noreferrer\">https:\/\/imgur.com\/a\/QdQNt1y<\/a>)<\/p>\n\n<p>Am I missing something? For smaller data sets it seemed to work. My headers are A1, ... A7999 for the attributes, and L1, ... L39 for the labels.<\/p>\n\n<p>Thanks for help in advance.<\/p>",
        "Challenge_closed_time":1576813812910,
        "Challenge_comment_count":0,
        "Challenge_created_time":1576672108560,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with the Azure ML platform where the uploaded dataset with 7999 attributes and 39 labels is being wrongly interpreted as a string with 0s, 1s, and commas, despite selecting the 'type' as 'tabular', encoding as 'utf-8', no row skipping, and using the header from the first file. The headers are not included in the interpreted data. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59392060",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":6.2,
        "Challenge_reading_time":9.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":39.3623194445,
        "Challenge_title":"Azure ML platform wrongly interprets uploaded dataset",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":49.0,
        "Challenge_word_count":114,
        "Platform":"Stack Overflow",
        "Poster_created_time":1527091507808,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Stockholm, Sweden",
        "Poster_reputation_count":235.0,
        "Poster_view_count":43.0,
        "Solution_body":"<p>our system does our best guess over file settings when you try to create a dataset, but cannot guarantee perfect guesses in all cases. <\/p>\n\n<p>In such scenarios, you should be able to adjust the settings. We had a bug with the ability to change those settings, but rolled out a fix. Can you try to change those now?  <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":3.88,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":60.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.8091666667,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi, I'm confused with the automated labelling feature of SM. Usually when people label things it is to train their own models afterwards. Is the goal of this feature to replace the downstream ML model that would use the labelled dataset? some sort of code free computer vision system?",
        "Challenge_closed_time":1544632081000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544611168000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is confused about the purpose of SageMaker Ground Truth's automated labelling feature. They are unsure if it is meant to replace downstream ML models or if it is a code-free computer vision system.",
        "Challenge_last_edit_time":1667965191088,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUp6wm80kUT0GZJp8meQtgTg\/why-sagemaker-ground-truth-automated-labelling",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.3,
        "Challenge_reading_time":4.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":5.8091666667,
        "Challenge_title":"why SageMaker Ground Truth automated labelling?",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":108.0,
        "Challenge_word_count":54,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Automated data labeling is labeling of data using machine learning. Amazon SageMaker Ground Truth will first select a random sample of data and send it to humans to be labeled. The results are then used to train a labeling model that attempts to label a new sample of raw data automatically. The labels are committed when the model can label the data with a confidence score that meets or exceeds a high threshold. Where the confidence score falls below this threshold, the data is sent to human labelers. Some of the data labeled by humans is used to generate a new training dataset for the labeling model, and the model is automatically retrained to improve its accuracy. This process repeats with each sample of raw data to be labeled. The labeling model becomes more capable of automatically labeling raw data with each iteration, and less data is routed to humans.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925583503,
        "Solution_link_count":0.0,
        "Solution_readability":10.6,
        "Solution_reading_time":10.58,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":150.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.7363491667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,  <\/p>\n<p>I am trying to figure out the folder structure of Azure ML workspace in my storage account.  <br \/>\nI want to be able to delete old pipeline runs and experiments that have piled up in my workspace directly from Azure Storage Explorer without breaking the system.  <br \/>\nMy datastores and folder structure are as follows:  <\/p>\n<p>Datastore: workspaceartifactstore  <br \/>\nBlob container: azureml  <br \/>\nFolder structure:  <br \/>\n\u251c\u2500 ComputeRecord  <br \/>\n\u251c\u2500 Dataset  <br \/>\n\u251c\u2500 ExperimentRun  <br \/>\n\u251c\u2500 LocalUpload  <\/p>\n<p>Datastore: workspaceblobstore (Default)  <br \/>\nBlob container: azureml-blobstore-<em>(a series of numbers)<\/em>  <br \/>\nFolder structure:  <br \/>\n\u251c\u2500 azureml  <br \/>\n\u2502   \u251c\u2500\u2500 <em>(a series of numbers)<\/em>-setup  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 _tracer.py  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 azureml_globals.py  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 context_managers.py  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 job_prep.py  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 log_history_status.py  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 request_utilities.py  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 run_token_provider.py  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 utility_context_managers.py  <br \/>\n\u2502   \u251c\u2500\u2500 <em>(another series of numbers)<\/em>-setup  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 <em>sames files as above<\/em>  <\/p>\n<p>It would help if I understood what does each of these containers actually store.  <br \/>\nI already tried to delete all blobs stored in 'workspaceblobstore', but it didn't remove any pipeline or experiment from ML Studio.  <br \/>\nI have a few datasets registered in my workspace, and I don't want to delete them (nor unregister them).  <\/p>\n<p>Can I set a data retention policy on both containers in order to delete old blobs?  <br \/>\nCan I safely delete the blobs (folders) stored in 'workspaceartifactstore' too? Will they be recreated automatically when I run a new experiment?  <br \/>\nWhy are there two separate 'azureml' and 'azureml-blobstore-<em>(a series of numbers)<\/em>' containers? Is it possible to merge them?  <\/p>\n<p>Thanks.  <\/p>\n<p>Thank you.<\/p>",
        "Challenge_closed_time":1647528776467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647500925610,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to understand the folder structure of Azure ML workspace in their storage account and wants to delete old pipeline runs and experiments without breaking the system. They have two datastores with different blob containers and folder structures and are unsure of what each container stores. The user has tried deleting all blobs in one container but it did not remove any pipeline or experiment from ML Studio. They are also unsure if they can safely delete the blobs stored in 'workspaceartifactstore' and if they can set a data retention policy on both containers. The user is also curious about the purpose of the two separate 'azureml' and 'azureml-blobstore-(a series of numbers)' containers and if they can be merged.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/775834\/azure-ml-workspace-blob-structure-can-i-safely-del",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":23.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":7.7363491667,
        "Challenge_title":"Azure ML workspace blob structure \/ Can I safely delete these blobs?",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":253,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. I've worked on a <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/issues\/60501\">similar inquiry<\/a> and the advise is to not delete data stored in default datastore to avoid weird errors. The option to easily delete experiment runs is on the roadmap. Here's a similar <a href=\"https:\/\/stackoverflow.com\/questions\/57497332\/how-to-delete-an-experiment-from-an-azure-machine-learning-workspace\">thread<\/a>. Feel free to raise and track feature request on <a href=\"https:\/\/feedback.azure.com\/d365community\/forum\/b9a0c624-ad25-ec11-b6e6-000d3a4f09d0\">ideas portal<\/a>.    <\/p>\n<blockquote>\n<p>According to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#prerequisites\">documentation<\/a>, when you create a workspace, an Azure blob container and an Azure file share are automatically registered as datastores to the workspace. They're named workspaceblobstore and workspacefilestore, respectively. The workspaceblobstore is used to store workspace artifacts and your machine learning experiment logs. It's also set as the default datastore and can't be deleted from the workspace. The workspacefilestore is used to store notebooks and R scripts authorized via compute instance.    <\/p>\n<\/blockquote>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.2,
        "Solution_reading_time":16.56,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":134.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1456986606312,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":757.0,
        "Answerer_view_count":80.0,
        "Challenge_adjusted_solved_time":2717.5608175,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I've got some data on S3 bucket that I want to work with. <\/p>\n\n<p>I've imported it using:<\/p>\n\n<pre><code>import boto3\nimport dask.dataframe as dd\n\ndef import_df(key):\n        s3 = boto3.client('s3')\n        df = dd.read_csv('s3:\/\/...\/' + key ,encoding='latin1')\n        return df\n\nkey = 'Churn\/CLEANED_data\/file.csv'\ntrain = import_df(key)\n<\/code><\/pre>\n\n<p>I can see that the data has been imported correctly using:<\/p>\n\n<pre><code>train.head()\n<\/code><\/pre>\n\n<p>but when I try simple operation (<a href=\"https:\/\/docs.dask.org\/en\/latest\/dataframe.html\" rel=\"nofollow noreferrer\">taken from this dask doc<\/a>):<\/p>\n\n<pre><code>train_churn = train[train['CON_CHURN_DECLARATION'] == 1]\ntrain_churn.compute()\n<\/code><\/pre>\n\n<p>I've got Error:<\/p>\n\n<blockquote>\n  <p>AttributeError                            Traceback (most recent call\n  last)  in ()<\/p>\n  \n  <p>1 train_churn = train[train['CON_CHURN_DECLARATION'] == 1]<\/p>\n  \n  <p>----> 2 train_churn.compute()<\/p>\n  \n  <p>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/dask\/base.py in\n  compute(self, **kwargs)\n      152         dask.base.compute\n      153         \"\"\"\n  --> 154         (result,) = compute(self, traverse=False, **kwargs)\n      155         return result\n      156<\/p>\n  \n  <p>AttributeError: 'DataFrame' object has no attribute '_getitem_array'<\/p>\n<\/blockquote>\n\n<p>Full error here: <a href=\"https:\/\/textuploader.com\/11lg7\" rel=\"nofollow noreferrer\">Error Upload<\/a><\/p>",
        "Challenge_closed_time":1574121568172,
        "Challenge_comment_count":1,
        "Challenge_created_time":1564579246737,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to work with data from an S3 bucket using Dask. They have imported the data correctly but are encountering an error when trying to perform a simple operation on the data. The error message states that the 'DataFrame' object has no attribute '_getitem_array'.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57291797",
        "Challenge_link_count":2,
        "Challenge_participation_count":5,
        "Challenge_readability":10.7,
        "Challenge_reading_time":18.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":2650.6448430556,
        "Challenge_title":"Dask: AttributeError: 'DataFrame' object has no attribute '_getitem_array'",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2742.0,
        "Challenge_word_count":129,
        "Platform":"Stack Overflow",
        "Poster_created_time":1429630461500,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Warszawa, Polska",
        "Poster_reputation_count":938.0,
        "Poster_view_count":137.0,
        "Solution_body":"<p>I was facing a similar issue when trying to read from s3 files, ultimately solved by updating dask to most recent version (I think the one sagemaker instances start with by default is deprecated)<\/p>\n\n<h2>Install\/Upgrade packages and dependencies (from notebook)<\/h2>\n\n<pre><code>! python -m pip install --upgrade dask\n! python -m pip install fsspec\n! python -m pip install --upgrade s3fs\n<\/code><\/pre>\n\n<p>Hope this helps!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1574362465680,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":5.35,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.9223880556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I encounter the following error :  <\/p>\n<p>Parameter &quot;Stopwords columns&quot; value should be less than or equal to parameter &quot;1&quot; value. . ( Error 0007 )  <br \/>\nwhen building a simple pipeline :  <\/p>\n<p>with a .csv Dataset followed by a &quot;Preprocessed Text&quot;.  <\/p>\n<p>No parameter 'Stopwords columns' is available in the &quot;Preprocessed Text&quot; properties !!!<\/p>",
        "Challenge_closed_time":1612878128547,
        "Challenge_comment_count":0,
        "Challenge_created_time":1612874807950,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error (Error 0007) while building a pipeline with a .csv dataset and \"Preprocessed Text\". The error message stated that the \"Stopwords columns\" parameter value should be less than or equal to the parameter \"1\" value, but the user did not find any such parameter in the \"Preprocessed Text\" properties.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/265397\/dataset-preprocessed-text-parameter-stopwords-colu",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":6.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.9223880556,
        "Challenge_title":"Dataset + Preprocessed Text : Parameter \"Stopwords columns\" value should be less than or equal to parameter \"1\" value. . ( Error 0007 )",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":70,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Solved.  <\/p>\n<p>There must be only one connection (left: Dataset) and not 2 connections (left : Dataset + right : Stopwords) from the &quot;Dataset&quot; to the &quot;Preprocessed Text&quot;<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":2.5,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1620426047396,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":66.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":386.5795511111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using the library weights and  biases. My model outputs a curve (a time series). I'd like to see how this curve changes throughout training. So, I'd need some kind of slider where I can select epoch and it shows me the curve for that epoch. It could be something very similar to what it's done with histograms (it shows an image of the histograms across epochs and when you hover it display the histogram corresponding to that epoch). Is there a way to do this or something similar using <code>wandb<\/code>?<\/p>\n<p>Currently my code looks like this:<\/p>\n<pre><code>for epoch in range(epochs):\n   output = model(input)\n   #output is shape (37,40) (lenght 40 and I have 37 samples)\n   #it's enough to plot the first sample\n   xs = torch.arange(40).unsqueeze(dim=1)\n   ys = output[0,:].unsqueeze(dim=1)\n   wandb.log({&quot;line&quot;: wandb.plot.line_series(xs=xs, ys=ys,title=&quot;Out&quot;)}, step=epoch)\n<\/code><\/pre>\n<p>I'd appreciate any help! Thanks!<\/p>",
        "Challenge_closed_time":1620426900467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619035214083,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using the library weights and biases to visualize a time series curve output by their model during training. They are looking for a way to create a slider that allows them to select an epoch and view the corresponding curve. They currently have code that logs the curve for each epoch using wandb.plot.line_series.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67202711",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":12.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":386.5795511111,
        "Challenge_title":"How to get multiple lines exported to wandb",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":840.0,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_created_time":1577734207070,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":422.0,
        "Poster_view_count":66.0,
        "Solution_body":"<p>You can use <code>wandb.log()<\/code> with matplotlib. Create your plot using matplotlib:<\/p>\n<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nx = np.linspace(0, 50)\nfor i in range(1, 4):\n    fig, ax = plt.subplots()\n    y = x ** i\n    ax.plot(x, y)\n    wandb.log({'chart': ax})\n<\/code><\/pre>\n<p>Then when you look on your wandb dashboard for the run, you will see the plot rendered as plotly plot. Click the gear in the upper left hand corner to see a slider that lets you slide over training steps and see the plot at each step.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.4,
        "Solution_reading_time":6.54,
        "Solution_score_count":3.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":85.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1491467888608,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":381.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":167.5534802778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using the <code>wandb<\/code> with my HuggingFace code. I would like to log the loss and other metrics. Now I have two questions<\/p>\n<ul>\n<li>How does <code>wandb<\/code> decide when to log the loss? Is this decided by <code>logging_steps<\/code> in <code>TrainingArguments(...)<\/code>\uff1f<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>training_args = TrainingArguments(output_dir=&quot;test&quot;, \n                                  learning_rate=lr,\n                                  num_train_epochs=n_epoch,\n                                  seed=seed,\n                                  per_device_train_batch_size=2,\n                                  per_device_eval_batch_size=2,\n                                  logging_strategy=&quot;steps&quot;,\n                                  logging_steps=5,\n                                  report_to=&quot;wandb&quot;)\n<\/code><\/pre>\n<ul>\n<li>How do I make sure <code>wandb<\/code> log other metrics (for example, adding validation metrics after each epoch)? Does this happen automatically?<\/li>\n<\/ul>",
        "Challenge_closed_time":1620162973716,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619559781187,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using wandb with HuggingFace code and wants to log loss and other metrics. They have two questions: how wandb decides when to log the loss and how to ensure wandb logs other metrics like validation metrics after each epoch. They are using logging_steps in TrainingArguments to control the logging frequency.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67291062",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":11.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":167.5534802778,
        "Challenge_title":"Control the logging frequency and contents when using wandb with HuggingFace",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":385.0,
        "Challenge_word_count":88,
        "Platform":"Stack Overflow",
        "Poster_created_time":1490778676136,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":322.0,
        "Poster_view_count":109.0,
        "Solution_body":"<p>Correct, it is dictated by the <code>on_log<\/code> event from the Trainer, you can see it <a href=\"https:\/\/github.com\/huggingface\/transformers\/blob\/6b241e0e3bda24546d15835e5e0b48b8d1e4732c\/src\/transformers\/integrations.py#L747\" rel=\"nofollow noreferrer\">here<\/a> in WandbCallback<\/p>\n<p>Your validation metrics should be logged to W&amp;B automatically every time you validate. How often Trainer does evaluation depends on what setting is used for <code>evaluation_strategy<\/code> (and potentially <code>eval_steps<\/code> if <code>evaluation_strategy == &quot;steps&quot;<\/code>)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":19.6,
        "Solution_reading_time":7.83,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":53.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0055555556,
        "Challenge_answer_count":1,
        "Challenge_body":"After successfully uploading CSV files from S3 to SageMaker notebook instance, I am stuck on doing the reverse.  \n  \nI have a dataframe and want to upload that to S3 Bucket as CSV or JSON. The code that I have is below:  \n  \nbucket='bucketname'  \ndata_key = 'test.csv'  \ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)  \ndf.to_csv(data_location)  \nI assumed since I successfully used pd.read_csv() while loading, using df.to_csv() would also work but it didn't. Probably it is generating error because this way I cannot pick the privacy options while uploading a file manually to S3. Is there a way to upload the data to S3 from SageMaker?",
        "Challenge_closed_time":1562042063000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562042043000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in uploading a dataframe to an AWS S3 bucket from SageMaker. They have tried using the df.to_csv() method but it did not work. The user suspects that the error may be due to the inability to pick privacy options while uploading the file manually to S3. The user is seeking a solution to upload the data to S3 from SageMaker.",
        "Challenge_last_edit_time":1668613629012,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUfoMiB7A8SFOpr5uklZZuNg\/uploading-a-dataframe-to-aws-s3-bucket-from-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":8.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.0055555556,
        "Challenge_title":"Uploading a Dataframe to AWS S3 Bucket from SageMaker",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2074.0,
        "Challenge_word_count":106,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"One way to solve this would be to save the CSV to the local storage on the SageMaker notebook instance, and then use the S3 API's via boto3 to upload the file as an s3 object. S3 docs for upload_file() available here.  \n  \nNote, you'll need to ensure that your SageMaker hosted notebook instance has proper ReadWrite permissions in its IAM role, otherwise you'll receive a permissions error.  \n  \n# code you already have, saving the file locally to whatever directory you wish  \nfile_name = \"mydata.csv\"   \ndf.to_csv(file_name)  \n# instantiate S3 client and upload to s3  \nimport boto3  \n  \ns3 = boto3.resource('s3')  \ns3.meta.client.upload_file(file_name, 'YOUR_S3_BUCKET_NAME', 'DESIRED_S3_OBJECT_NAME')  \nAlternatively, upload_fileobj() may help for parallelizing as a multi-part upload.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1562042063000,
        "Solution_link_count":0.0,
        "Solution_readability":11.4,
        "Solution_reading_time":9.58,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":107.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":117.2994491667,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I know how to create a table with a data frame programmatically. However, I have two data frames, and they have different number of rows, so I cannot combine them into a single data frame. How do I upload two different tables to a Weights&amp;Biases project? Somehow, I suspect that the following is not the correct approach:<\/p>\n<pre><code class=\"lang-python\">    train_df = pd.DataFrame({\n        'tx':train_x,\n        'ty':train_y,\n    })\n    valid_df = pd.DataFrame({\n        'vx':valid_x,\n        'vy':valid_y\n    })\n\n    # How to add multiple tables\n\n    wandb.log({\"table\": train_df}, commit=False)\n    wandb.log({\"table\": valid_df}, commit=False)\n<\/code><\/pre>\n<p>Any help is greatly appreciated.<\/p>",
        "Challenge_closed_time":1660166829976,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659744551959,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to upload two different tables to a Weights&Biases project, but the tables have different numbers of rows and cannot be combined into a single data frame. They are seeking guidance on the correct approach to upload multiple tables.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/multiple-tables\/2856",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.6,
        "Challenge_reading_time":8.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":117.2994491667,
        "Challenge_title":"Multiple tables",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":236.0,
        "Challenge_word_count":85,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/erlebacher\">@erlebacher<\/a> , see <a href=\"https:\/\/docs.wandb.ai\/guides\/data-vis\/log-tables#create-tables\">this document<\/a> on how to create tables from dataframes and please let me know if you have any questions.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.7,
        "Solution_reading_time":3.33,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":70.2958,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>I\u2019m trying to plot the figure as in [W&amp;B Smoothing Features], but it didn\u2019t provide any code:<\/p>\n<p>                    <a href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/9b0793db422eac619667bd11a7c56351d9d69149.png\" target=\"_blank\" rel=\"noopener nofollow ugc\" class=\"onebox\">\n            <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/9b0793db422eac619667bd11a7c56351d9d69149.png\" width=\"690\" height=\"301\">\n          <\/a>\n\n<\/p>\n<p>Tutorials I could find by searching <code>wandb multiline in one plot<\/code> is [Custom Multi-Line Plots] which introduces <code>wandb.plot.line_series()<\/code>.  So I tried the code following<\/p>\n<pre><code class=\"lang-python\">import wandb\nimport numpy as np\n\n\nwandb.init(project=\"test\", entity=\"xxxx\")\n\nwandb.log({\"my_custom_id\":\n           wandb.plot.line_series(\n               xs=range(100),\n               ys=[range(100), np.random.randint(100, size=100)],\n               keys=[\"y1\", \"y2\"],\n               title=\"Multiline\",\n               xname=\"steps\"\n           )})\n<\/code><\/pre>\n<p>It gives me the following pic after choosing <code>Edit panel<\/code><\/p>\n<p>                    <a href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/4e32b0d8ec1fb96df36dd2a7609f129875cb9e8e.png\" target=\"_blank\" rel=\"noopener nofollow ugc\" class=\"onebox\">\n            <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/4e32b0d8ec1fb96df36dd2a7609f129875cb9e8e.png\" width=\"690\" height=\"351\">\n          <\/a>\n\n<\/p>\n<p>Unlike the first picture:<\/p>\n<ol>\n<li>It <strong>doesn\u2019t<\/strong> have <code>Data<\/code>, <code>Group<\/code> etc tabs.<\/li>\n<li>There are <strong>two types<\/strong> of legend <code>name<\/code> and <code>lineKey<\/code> rather than one type.<\/li>\n<\/ol>\n<p>My question is how to plot exactly the same as the first picture with same function supported in wandb web?<\/p>",
        "Challenge_closed_time":1639528370760,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639275305880,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to plot a multiline graph with smoothing features using wandb.plot.line_series() function. However, the resulting graph does not match the desired output as it lacks certain features such as Data and Group tabs and has two types of legends. The user is seeking guidance on how to plot the graph with the same function supported in wandb web.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-plot-multiline-in-one-plot-with-smoothing-features\/1512",
        "Challenge_link_count":4,
        "Challenge_participation_count":5,
        "Challenge_readability":14.2,
        "Challenge_reading_time":24.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":70.2958,
        "Challenge_title":"How to plot multiline in one plot with smoothing features?",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":310.0,
        "Challenge_word_count":154,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>First I need to log the data I want<\/p>\n<pre><code class=\"lang-python\">import random\nimport wandb\n\nwandb.init(project=\"test\", entity=\"xxxx\")\nfor i in range(100):\n    wandb.log({\"y1\": random.random(), \"y2\": random.random(), \"x\": i})\n<\/code><\/pre>\n<p>Then, I need to mannually choosing y1 and y2 on Y Axis.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/1794a161b9d71b6745d9a5c7f4beff76003a4d5c.jpeg\" data-download-href=\"\/uploads\/short-url\/3mBq6eTgdFLO5BAFCPEQmOUHYoQ.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1794a161b9d71b6745d9a5c7f4beff76003a4d5c_2_690x262.jpeg\" alt=\"image\" data-base62-sha1=\"3mBq6eTgdFLO5BAFCPEQmOUHYoQ\" width=\"690\" height=\"262\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1794a161b9d71b6745d9a5c7f4beff76003a4d5c_2_690x262.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1794a161b9d71b6745d9a5c7f4beff76003a4d5c_2_1035x393.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1794a161b9d71b6745d9a5c7f4beff76003a4d5c_2_1380x524.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1794a161b9d71b6745d9a5c7f4beff76003a4d5c_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1807\u00d7687 214 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":27.9,
        "Solution_reading_time":23.97,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":78.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":29.82268,
        "Challenge_answer_count":3,
        "Challenge_body":"The SageMaker Data Wrangler UI in SageMaker Studio doesn't seem to support all the features that the API does.  When will the UI support:\n* Loading all s3 objects under a prefix?  https:\/\/aws-data-wrangler.readthedocs.io\/en\/stable\/stubs\/awswrangler.s3.read_csv.html#awswrangler.s3.read_csv\n* Loading JSON objects in addition to CSV and Parquet files?  https:\/\/aws-data-wrangler.readthedocs.io\/en\/stable\/stubs\/awswrangler.s3.read_json.html#awswrangler.s3.read_json",
        "Challenge_closed_time":1642051347464,
        "Challenge_comment_count":1,
        "Challenge_created_time":1641943985816,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing challenges with the SageMaker Data Wrangler UI in SageMaker Studio as it does not support all the features that the API does, specifically loading all s3 objects under a prefix and loading JSON objects in addition to CSV and Parquet files. The user is seeking information on when these features will be supported in the UI.",
        "Challenge_last_edit_time":1667957958272,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUcsIt78jnSTW8Ta9__kUm-w\/sagemaker-data-wrangler-ui-features",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":16.2,
        "Challenge_reading_time":6.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":29.82268,
        "Challenge_title":"SageMaker Data Wrangler UI Features",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":126.0,
        "Challenge_word_count":47,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"As mentioned by Tulio Alberto in comments, Amazon SageMaker Data Wrangler (the graphical data preparation feature inside Amazon SageMaker) is separate from AWS Data Wrangler (an open-source data prep utility published by AWS Labs): The two tools are based on different technologies and don't necessarily aim for full feature parity - they just happen to share similar names.\n\nTo my knowledge there's no committed timeline we can share at the moment for when these particular features will make it to SageMaker Data Wrangler, but I think as feature requests they make sense and the reasoning for both is pretty clear: I'm aware that both have been discussed to some extent internally already, and I'd personally like to see them launch too!\n\nThanks for sharing the feedback, and apologies for the naming confusion!",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1642051347467,
        "Solution_link_count":0.0,
        "Solution_readability":20.3,
        "Solution_reading_time":9.99,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":131.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1443426419048,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation_count":842.0,
        "Answerer_view_count":219.0,
        "Challenge_adjusted_solved_time":57.4111722222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>a friend have sent me a python3 notebook with his dataset to validate his notebook.<\/p>\n\n<p>but when i try to use his dataset on my azureml workspace i have an error saying that the dataset does not exist<\/p>\n\n<p>he sent me his datset code :<\/p>\n\n<pre><code>from azureml import Workspace\n\nws = Workspace(\n    workspace_id='toto',\n    authorization_token='titi',\n    endpoint='https:\/\/studioapi.azureml.net'\n)\nds = ws.datasets['mini.csv00']\nframe = ds.to_dataframe()\n\nframe\n<\/code><\/pre>\n\n<p>when i try to use it i have a :<\/p>\n\n<pre><code>ndexError                                Traceback (most recent call last)\n&lt;ipython-input-7-5f41120e38e4&gt; in &lt;module&gt;()\n----&gt; 1 ds = ws.datasets['mini.csv00']\n      2 frame = ds.to_dataframe()\n      3 \n      4 frame\n\n\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/azureml\/__init__.py in __getitem__(self, index)\n    461                     return self._create_dataset(dataset)\n    462 \n--&gt; 463         raise IndexError('A data set named \"{}\" does not exist'.format(index))\n    464 \n    465     def add_from_dataframe(self, dataframe, data_type_id, name, description):\n\nIndexError: A data set named \"mini.csv00\" does not exist\n<\/code><\/pre>\n\n<p>error ...<\/p>\n\n<p>But when i try it on my computer jupyter it works.\nAny ideas ?<\/p>\n\n<p>Thanks and regards<\/p>",
        "Challenge_closed_time":1486875192387,
        "Challenge_comment_count":1,
        "Challenge_created_time":1486668512167,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use a dataset shared by a friend on their AzureML workspace but is encountering an error stating that the dataset does not exist. The friend has shared the dataset code, which works on their computer's Jupyter notebook. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42145256",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":7.4,
        "Challenge_reading_time":16.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":57.4111722222,
        "Challenge_title":"Share dataset between two azureml environnement",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":108.0,
        "Challenge_word_count":149,
        "Platform":"Stack Overflow",
        "Poster_created_time":1280999248528,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1048.0,
        "Poster_view_count":602.0,
        "Solution_body":"<p>I guess you are using Jupyter notebook on AzureML to do the experiment. In that case the <code>'mini.csv00'<\/code> should be in your experiments with <code>workspace_id='toto'<\/code>. <\/p>\n\n<p>Create a new experiment in your workspace named toto and put the dataset into it first. Then open the dataset using 'open in a new Notebook'. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ztIw0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ztIw0.png\" alt=\"enter image description here\"><\/a> <\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.6,
        "Solution_reading_time":6.55,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1263294862568,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":183045.0,
        "Answerer_view_count":13691.0,
        "Challenge_adjusted_solved_time":0.3139213889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>following the answers to this question <a href=\"https:\/\/stackoverflow.com\/questions\/48264656\/load-s3-data-into-aws-sagemaker-notebook\">Load S3 Data into AWS SageMaker Notebook<\/a> I tried to load data from S3 bucket to SageMaker Jupyter Notebook.<\/p>\n<p>I used this code:<\/p>\n<pre><code>import pandas as pd\n\nbucket='my-bucket'\ndata_key = 'train.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\n\npd.read_csv(data_location)\n<\/code><\/pre>\n<p>I replaced <code>'my-bucket'<\/code> by the ARN (Amazon Ressource name) of my S3 bucket (e.g. &quot;<code>arn:aws:s3:::name-of-bucket<\/code>&quot;) and replaced <code>'train.csv'<\/code> by the csv-filename which is stored in the S3 bucket. Regarding the rest I did not change anything at all. What I got was this <code>ValueError<\/code>:<\/p>\n<pre><code>ValueError: Failed to head path 'arn:aws:s3:::name-of-bucket\/name_of_file_V1.csv': Parameter validation failed:\nInvalid bucket name &quot;arn:aws:s3:::name-of-bucket&quot;: Bucket name must match the regex &quot;^[a-zA-Z0-9.\\-_]{1,255}$&quot; or be an ARN matching the regex &quot;^arn:(aws).*:s3:[a-z\\-0-9]+:[0-9]{12}:accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-zA-Z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$&quot;\n<\/code><\/pre>\n<p>What did I do wrong? Do I have to modify the name of my S3 bucket?<\/p>",
        "Challenge_closed_time":1613558457267,
        "Challenge_comment_count":1,
        "Challenge_created_time":1613557327150,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to load data from an S3 bucket to a SageMaker Jupyter Notebook using Python code. However, they are encountering a ValueError that states the bucket name is invalid. The user replaced 'my-bucket' with the ARN of their S3 bucket and the csv-filename stored in the bucket. The error message suggests that the bucket name must match a specific regex pattern. The user is seeking guidance on what they did wrong and if they need to modify the name of their S3 bucket.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66239966",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":19.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":0.3139213889,
        "Challenge_title":"Loading data from S3 bucket to SageMaker Jupyter Notebook - ValueError - Invalid bucket name",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":345.0,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559131080072,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1166.0,
        "Poster_view_count":248.0,
        "Solution_body":"<p>The path should be:<\/p>\n<pre><code>data_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\n<\/code><\/pre>\n<p>where <code>bucket<\/code> is <code>&lt;bucket-name&gt;<\/code> <strong>not ARN<\/strong>. For example <code>bucket=my-bucket-333222<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.1,
        "Solution_reading_time":3.42,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":17.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1429402428283,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":76.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":36.4623708333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>\u203b I used google translation, if you have any question, let me know!<\/p>\n\n<p>I am trying to run python script with huge 4 data, using sagemaker processing. And my current situation are as follows:<\/p>\n\n<ul>\n<li>can run this script with 3 data<\/li>\n<li>can't run the script with only 1 data (the biggest, the same structure with others)<\/li>\n<li>as for all of 4 data, the script has finished (so, I suspected this error in S3, ie. when copying sagemaker result to S3)<\/li>\n<\/ul>\n\n<p>The error I got is this InternalServerError.<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"sagemaker_train_and_predict.py\", line 56, in &lt;module&gt;\n    outputs=outputs\n  File \"{xxx}\/sagemaker_constructor.py\", line 39, in run\n    outputs=outputs\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/processing.py\", line 408, in run\n    self.latest_job.wait(logs=logs)\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/processing.py\", line 723, in wait\n    self.sagemaker_session.logs_for_processing_job(self.job_name, wait=True)\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/session.py\", line 3111, in logs_for_processing_job\n    self._check_job_status(job_name, description, \"ProcessingJobStatus\")\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/session.py\", line 2615, in _check_job_status\n    actual_status=status,\nsagemaker.exceptions.UnexpectedStatusException: Error for Processing job sagemaker-vm-train-and-predict-2020-04-12-04-15-40-655: Failed. Reason: InternalServerError: We encountered an internal error.  Please try again.\n<\/code><\/pre>",
        "Challenge_closed_time":1586886613192,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586755348657,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an InternalServerError while trying to save the result on S3 from sagemaker processing. The user is trying to run a python script with 4 data, but can only run it with 3 data and can't run it with only 1 data. The error occurred while copying sagemaker result to S3.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61181955",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":22.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":36.4623708333,
        "Challenge_title":"Is there any limits of saving result on S3 from sagemaker Processing?",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":95.0,
        "Challenge_word_count":168,
        "Platform":"Stack Overflow",
        "Poster_created_time":1586754432800,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>There may be some issue transferring the output data to S3 if the output is generated at a high rate and size is too large. <\/p>\n\n<p>You can 1) try to slow down writing the output a bit or 2) call S3 from your algorithm container to upload the output directly using boto client (<a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html<\/a>).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.7,
        "Solution_reading_time":6.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":58.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":42.8398436111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What's the best way to preserve Azure ML workspace so that it can be restored at a later point? I was hoping to find some automatic way to take a snapshot of artifacts &amp; code and dump it into Azure storage, but haven't been able to find anything relevant in the online documentation. <\/p>",
        "Challenge_closed_time":1669596362440,
        "Challenge_comment_count":0,
        "Challenge_created_time":1669442139003,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is looking for the best way to preserve their Azure ML workspace so that it can be restored later. They are seeking an automatic way to take a snapshot of artifacts and code and store it in Azure storage, but have not found any relevant information in the online documentation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1105130\/whats-the-best-way-to-preserve-azure-ml-workspace",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":4.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":42.8398436111,
        "Challenge_title":"What's the best way to preserve Azure ML workspace so that it can be restored",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":68,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@VaraPrasad-1740 Thanks for the question. I would recommend you can have a git repository that backs your project.  For some details about this approach you can check <a href=\"https:\/\/santiagof.medium.com\/structure-your-machine-learning-project-source-code-like-a-pro-44815cac8652\">https:\/\/santiagof.medium.com\/structure-your-machine-learning-project-source-code-like-a-pro-44815cac8652<\/a><\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.7,
        "Solution_reading_time":5.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":29.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1353403873547,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":4909.0,
        "Answerer_view_count":583.0,
        "Challenge_adjusted_solved_time":370.54581,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Optuna's FAQ has a <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/faq.html#id10\" rel=\"nofollow noreferrer\">clear answer<\/a> when it comes to dynamically adjusting the range of parameter during a study: it poses no problem since each sampler is defined individually.<\/p>\n<p>But what about adding and\/or removing parameters? Is Optuna able to handle such adjustments?<\/p>\n<p>One thing I noticed when doing this is that in the results dataframe these parameters get <code>nan<\/code> entries for other trials. Would there be any benefit to being able to set these <code>nan<\/code>s to their (default) value that they had when not being sampled? Is the study still sound with all these unknown values?<\/p>",
        "Challenge_closed_time":1609649865303,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608315900387,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is questioning whether Optuna can handle adding and\/or removing parameters dynamically during a study. They have noticed that when doing so, the results dataframe shows \"nan\" entries for other trials. The user is wondering if it would be beneficial to set these \"nan\" values to their default value and if the study is still valid with these unknown values.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65362133",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":9.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":370.54581,
        "Challenge_title":"What happens when I add\/remove parameters dynamically during an Optuna study?",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":227.0,
        "Challenge_word_count":110,
        "Platform":"Stack Overflow",
        "Poster_created_time":1353403873547,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":4909.0,
        "Poster_view_count":583.0,
        "Solution_body":"<p>Question was answered <a href=\"https:\/\/github.com\/optuna\/optuna\/issues\/2141\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<blockquote>\n<p>Thanks for the question. Optuna internally supports two types of sampling: <code>optuna.samplers.BaseSampler.sample_independent<\/code> and <code>optuna.samplers.BaseSampler.sample_relative<\/code>.<\/p>\n<p>The former <code>optuna.samplers.BaseSampler.sample_independent<\/code> is a method that samples independently on each parameter, and is not affected by the addition or removal of parameters. The added parameters are taken into account from the timing when they are added.<\/p>\n<p>The latter <code>optuna.samplers.BaseSampler.sample_relative<\/code> is a method that samples by considering the correlation of parameters and is affected by the addition or removal of parameters. Optuna's default search space for correlation is the product set of the domains of the parameters that exist from the beginning of the hyperparameter tuning to the present. Developers who implement samplers can implement their own search space calculation method <code>optuna.samplers.BaseSampler.infer_relative_search_space<\/code>. This may allow correlations to be considered for hyperparameters that have been added or removed, but this depends on the sampling algorithm, so there is no API for normal users to modify.<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.3,
        "Solution_reading_time":17.6,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":157.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":14.2856144445,
        "Challenge_answer_count":1,
        "Challenge_body":"**Error message:**\n\"FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/processing\/output\/profile_case.html'\"\n\n**Background:**\nI am working in Sagemaker using python trying to profile a dataframe that is saved in a S3 bucket with pandas profiling. The data is very large so instead of spinning up a large EC2 instance, I am using a SKLearn processor.\n\nEverything runs fine but when the job finishes it does not save the pandas profile (a .html file) in a S3 bucket or back in the instance Sagemaker is running in.\n\nWhen I try to export the .html file that is created from the pandas profile, I keep getting errors saying that the file cannot be found.\n\nDoes anyone know of a way to export the .html file out of the temporary 24xl instance that the SKLearn processor is running in to S3? Below is the exact code I am using:\n\n\n```\nimport os\nimport sys\nimport subprocess\ndef install(package):\n    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])\ninstall('awswrangler')\ninstall('tqdm')\ninstall('pandas')\ninstall('botocore==1.19.4')\ninstall('ruamel.yaml')\ninstall('pandas-profiling==2.13.0')\nimport awswrangler as wr\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom dateutil.relativedelta import relativedelta\nfrom string import Template\nimport gc\nimport boto3\n\nfrom pandas_profiling import ProfileReport\n\nclient = boto3.client('s3')\nsession = boto3.Session(region_name=\"eu-west-2\")\n```\n\n```\n%%writefile casetableprofile.py\n\nimport os\nimport sys\nimport subprocess\ndef install(package):\n    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])\ninstall('awswrangler')\ninstall('tqdm')\ninstall('pandas')\ninstall('botocore')\ninstall('ruamel.yaml')\ninstall('pandas-profiling')\nimport awswrangler as wr\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom dateutil.relativedelta import relativedelta\nfrom string import Template\nimport gc\nimport boto3\n\nfrom pandas_profiling import ProfileReport\n\nclient = boto3.client('s3')\nsession = boto3.Session(region_name=\"eu-west-2\")\n\n\n\n\ndef run_profile():\n\n\n\n    query = \"\"\"\n    SELECT  * FROM \"healthcloud-refined\".\"case\"\n    ;\n    \"\"\"\n    tableforprofile = wr.athena.read_sql_query(query,\n                                            database=\"healthcloud-refined\",\n                                            boto3_session=session,\n                                            ctas_approach=False,\n                                            workgroup='DataScientists')\n    print(\"read in the table queried above\")\n\n    print(\"got rid of missing and added a new index\")\n\n    profile_tblforprofile = ProfileReport(tableforprofile, \n                                  title=\"Pandas Profiling Report\", \n                                  minimal=True)\n\n    print(\"Generated carerequest profile\")\n                                      \n    return profile_tblforprofile\n\n\nif __name__ == '__main__':\n\n    profile_tblforprofile = run_profile()\n    \n    print(\"Generated outputs\")\n\n    output_path_tblforprofile = ('profile_case.html')\n    print(output_path_tblforprofile)\n    \n    profile_tblforprofile.to_file(output_path_tblforprofile)\n\n    \n    #Below is the only part where I am getting errors\nimport boto3\nimport os   \ns3 = boto3.resource('s3')\ns3.meta.client.upload_file('\/opt\/ml\/processing\/output\/profile_case.html', 'intl-euro-uk-datascientist-prod','Mark\/healthclouddataprofiles\/{}'.format(output_path_tblforprofile))  \n```\n\n```\nimport sagemaker\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsession = boto3.Session(region_name=\"eu-west-2\")\n\nbucket = 'intl-euro-uk-datascientist-prod'\n\nprefix = 'Mark'\n\nsm_session = sagemaker.Session(boto_session=session, default_bucket=bucket)\nsm_session.upload_data(path='.\/casetableprofile.py',\n                                bucket=bucket,\n                                key_prefix=f'{prefix}\/source')\n```\n\n\n\n```\nimport boto3\n#import sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.sklearn.processing import SKLearnProcessor\n\nregion = boto3.session.Session().region_name\n\n\nS3_ROOT_PATH = \"s3:\/\/{}\/{}\".format(bucket, prefix)\n\nrole = get_execution_role()\nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role=role,\n                                     sagemaker_session=sm_session,\n                                     instance_type='ml.m5.24xlarge',\n                                     instance_count=1)\n```\n\n```\nsklearn_processor.run(code='s3:\/\/{}\/{}\/source\/casetableprofile.py'.format(bucket, prefix),\n                      inputs=[],\n                      outputs=[ProcessingOutput(output_name='output',\n                                                source='\/opt\/ml\/processing\/output',\n                                                destination='s3:\/\/intl-euro-uk-datascientist-prod\/Mark\/')])\n```\n\n\nThank you in advance!!!",
        "Challenge_closed_time":1660704602950,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660653174738,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to save a .html file to S3 that is created in a Sagemaker processing container. They are using pandas profiling to profile a large dataframe and are running the code in a SKLearn processor. However, when the job finishes, the pandas profile is not saved in S3 or in the instance Sagemaker is running in. The user is encountering an error message stating that the file cannot be found when trying to export the .html file. They are seeking a way to export the .html file out of the temporary 24xl instance that the SKLearn processor is running in to S3.",
        "Challenge_last_edit_time":1668605561608,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU5abOieUyQZSFvyRwfApRVA\/how-to-save-a-html-file-to-s3-that-is-created-in-a-sagemaker-processing-container",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.6,
        "Challenge_reading_time":55.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":38,
        "Challenge_solved_time":14.2856144445,
        "Challenge_title":"How to save a .html file to S3 that is created in a Sagemaker processing container",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":443.0,
        "Challenge_word_count":395,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi,\n\nFirstly, you should not (usually) need to directly interact with S3 from your processing script: The fact that you've configured your `ProcessingOutput` means that any files your script saves in `\/opt\/ml\/processing\/output` should automatically get uploaded to your `s3:\/\/...` destination URL. Of course there might be particular special cases where you want to directly access S3 from your script, but in general the processing job inputs and outputs should do it for you, to keep your code nice and simple.\n\nI'm no Pandas Profiler expert, but I *think* the error might be coming from here:\n\n```python\n    output_path_tblforprofile = ('profile_case.html')\n    print(output_path_tblforprofile)\n    \n    profile_tblforprofile.to_file(output_path_tblforprofile)\n```\n\nDoesn't this just save the report to `profile_case.html` in your current working directory? That's not the `\/opt\/ml\/processing\/output` directory: It's usually the folder where the script is downloaded to the container I believe. The FileNotFound error is telling you that the HTML file is not getting created in the folder you expect, I think.\n\nSo I would suggest to make your output path explicit e.g. `\/opt\/ml\/processing\/output\/profile_case.html`, and also remove the boto3\/s3 section at the end - hope that helps!",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1660704602951,
        "Solution_link_count":0.0,
        "Solution_readability":11.4,
        "Solution_reading_time":15.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":177.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.8825055556,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hey,<br>\nI\u2019m trying to get the version of an artifact directly after logging my model (encoder) as an artifact to WandB.<\/p>\n<p><strong>Code:<\/strong><\/p>\n<pre><code class=\"lang-auto\">artifact = wandb.Artifact('my_artifact_name', type='model')\nartifact.add_file('\/home\/dezzardhd\/encoder.pth')\nwandb.log_artifact(artifact)\nversion = artifact.version\n<\/code><\/pre>\n<p>Logging works so far, but\u2026<br>\nwhen trying to access the version of the artifact I get an error.<br>\n<strong>Error:<\/strong><\/p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\n  File \"\/home\/moritz\/PycharmProjects\/bachelorarbeit\/main.py\", line 48, in &lt;module&gt;\n    train_setups.start_training_sessions(project=project)\n  File \"\/home\/moritz\/PycharmProjects\/bachelorarbeit\/train_setups.py\", line 18, in start_training_sessions\n    model_pipeline(config, project=project)\n  File \"\/home\/moritz\/PycharmProjects\/bachelorarbeit\/learning.py\", line 84, in model_pipeline\n    save_model(model_ae=model, model_encoder=model_encoder, model_decoder=model_decoder)\n  File \"\/home\/moritz\/PycharmProjects\/bachelorarbeit\/learning.py\", line 124, in save_model\n    version = artifact_enc.version\n  File \"\/home\/moritz\/anaconda3\/envs\/bachelorarbeit\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_artifacts.py\", line 191, in version\n    return self._logged_artifact.version\n  File \"\/home\/moritz\/anaconda3\/envs\/bachelorarbeit\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 2899, in version\n    return self._assert_instance().version\n  File \"\/home\/moritz\/anaconda3\/envs\/bachelorarbeit\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 2871, in _assert_instance\n    raise ValueError(\nValueError: Must call wait() before accessing logged artifact properties\n<\/code><\/pre>\n<p>What should I do now?<\/p>\n<p>For context:<br>\nI want to print out the version number with some other parameters so that I can easier start my evaluation process for certain runs.<\/p>\n<p>Best regards<br>\nDezzardHD<\/p>",
        "Challenge_closed_time":1646696888232,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646690111212,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to get the version of an artifact after logging their model as an artifact to WandB. Although logging works, the user encounters an error when trying to access the version of the artifact. The error message suggests that the user must call wait() before accessing logged artifact properties. The user is seeking advice on what to do next as they want to print out the version number with some other parameters to start their evaluation process for certain runs.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-do-i-get-the-version-of-an-artifact\/2035",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":18.3,
        "Challenge_reading_time":26.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":1.8825055556,
        "Challenge_title":"How do I get the version of an artifact?",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":411.0,
        "Challenge_word_count":164,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/dezzardhd\">@dezzardhd<\/a>,<\/p>\n<p>Could you try running your code as the following?<\/p>\n<pre><code class=\"lang-python\">artifact = wandb.Artifact('my_artifact_name', type='model')\nartifact.add_file('\/home\/dezzardhd\/encoder.pth')\nwandb.log_artifact(artifact).wait()\nversion = artifact.version\n<\/code><\/pre>\n<p>Calling <code>wait()<\/code> after <code>log_artifact()<\/code> should resolve this for you.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":19.6,
        "Solution_reading_time":6.27,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":33.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":22.3175444444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi all,    <\/p>\n<p>I've registered in Azure Machine Learning a Data Lake Gen2 datastore that point to a container with a hierarchy of folders that contain avro files and on top of it I registered a folder_uri dataset (ML v2).    <\/p>\n<p>Now I want to access to these folders from a notebook, convert them in a pandas dataframe in order to do some data exploration.    <\/p>\n<p>I search on the documentation, and I only found examples that run job and using this type of dataset as input, but I need to be able to explore it using notebook.     <\/p>\n<p>Is it possible? How can I do it?    <\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1666874705140,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666794361980,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has registered a Data Lake Gen2 datastore in Azure Machine Learning that contains avro files in a hierarchy of folders. They have also registered a folder_uri dataset (ML v2) and want to access these folders from a notebook to convert them into a pandas dataframe for data exploration. The user is seeking guidance on how to explore the dataset using a notebook instead of running a job.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1063867\/azure-machine-learning-access-uri-folder-dataset-(",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":8.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":22.3175444444,
        "Challenge_title":"Azure Machine Learning - Access uri_folder dataset (ML v2) from notebook (not job)",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":120,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=c637b1ab-bffd-0006-0000-000000000000\">@G Cocci  <\/a> Thanks for the question. Currently it's not supported to access the avro files. Here is the document for accessing the datastore using folder_uri dataset.    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/migrate-to-v2-resource-datastore\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/migrate-to-v2-resource-datastore<\/a>    <\/p>\n<p>Mapping Data Flow supports AVRO as a source type <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/data-factory\/data-flow-source#supported-sources\">https:\/\/learn.microsoft.com\/en-us\/azure\/data-factory\/data-flow-source#supported-sources<\/a><\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":20.8,
        "Solution_reading_time":9.48,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1250158552416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Challenge_adjusted_solved_time":17.1400166667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to know what is the difference between <code>feature numeric<\/code> and <code>numeric<\/code> columns in Azure Machine Learning Studio.<\/p>\n\n<p>The <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/edit-metadata\" rel=\"nofollow noreferrer\">documentation site<\/a> states: <\/p>\n\n<blockquote>\n  <p>Because all columns are initially treated as features, for modules\n  that perform mathematical operations, you might need to use this\n  option to prevent numeric columns from being treated as variables.<\/p>\n<\/blockquote>\n\n<p>But nothing more. Not what a feature is, in which modules you need features. Nothing. <\/p>\n\n<p>I specifically would like to understand if the <code>clear feature<\/code> dropdown option in the <code>fields<\/code> in the <code>edit metadata<\/code>-module has any effect. Can somebody give me a szenario where this <code>clear feature<\/code>-operation changes the ML outcome? Thank you<\/p>\n\n<p>According to the documentation in ought to have an effect:<\/p>\n\n<blockquote>\n  <p>Use the Fields option if you want to change the way that Azure Machine\n  Learning uses the data in a model.<\/p>\n<\/blockquote>\n\n<p>But what can this effect be? Any example might help<\/p>",
        "Challenge_closed_time":1538116098500,
        "Challenge_comment_count":0,
        "Challenge_created_time":1538054010173,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on the difference between \"feature numeric\" and \"numeric\" columns in Azure Machine Learning Studio. They are also unsure about the purpose of the \"clear feature\" dropdown option in the \"edit metadata\" module and how it affects the ML outcome. The documentation provides limited information on these topics, and the user is looking for specific scenarios or examples.",
        "Challenge_last_edit_time":1538054394440,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52537861",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.3,
        "Challenge_reading_time":16.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":17.2467575,
        "Challenge_title":"What is the role of feature type in AzureML?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":70.0,
        "Challenge_word_count":164,
        "Platform":"Stack Overflow",
        "Poster_created_time":1368739128832,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Riga, Latvia",
        "Poster_reputation_count":1763.0,
        "Poster_view_count":380.0,
        "Solution_body":"<p>As you suspect, setting a column as <code>feature<\/code> does have an effect, and it's actually quite important - when training a model, the algorithms will only take into account columns with the <code>feature<\/code> flag, effectively ignoring the others. <\/p>\n\n<p>For example, if you have a dataset with columns <code>Feature1<\/code>, <code>Feature2<\/code>, and <code>Label<\/code> and you want to try out just <code>Feature1<\/code>, you would apply <code>clear feature<\/code> to the <code>Feature2<\/code> column (while making sure that <code>Feature1<\/code> has the <code>feature<\/code> label set, of course).<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":21.3,
        "Solution_reading_time":7.89,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":80.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":28.6503836111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm doing a pipeline in Azure ML SDK. After I had run the pipeline for some amount of times it reported I had reached the Snapshot limit of 300MB. I followed some of the fixes that was proposed:<\/p>\n<ul>\n<li>   Each step script is moved to a separate subfolder<\/li>\n<li>   I added a datastore to the pipeline<\/li>\n<li>   This line was added: <code>azureml._restclient.snapshots_client.SNAPSHOT_MAX_SIZE_BYTES = 1000<\/code><\/li>\n<\/ul>\n<p>But then a new Snapshot error occurred after I submitted my pipeline:<\/p>\n<pre><code>pipeline1 = Pipeline(default_source_directory=&quot;.&quot;, default_datastore=def_blob_store, workspace=ws, steps=[prep_step, hd_step, register_model_step])\n<\/code><\/pre>\n<p>THE ERROR MESSAGE:<\/p>\n<pre><code>WARNING:root:If 'script' has been provided here and a script file name has been specified in 'run_config', 'script' provided in ScriptRunConfig initialization will take precedence.\n---------------------------------------------------------------------------\nSnapshotException                         Traceback (most recent call last)\n&lt;ipython-input-14-05c5aa4991aa&gt; in &lt;module&gt;\n----&gt; 1 pipeline1 = Pipeline(default_source_directory=&quot;.&quot;, default_datastore=def_blob_store, workspace=ws, steps=[prep_step, hd_step, register_model_step])\n      2 pipeline1.validate()\n      3 pipeline_run = Experiment(ws, 'health_insuarance').submit(pipeline1, regenerate_outputs=False)\n      4 RunDetails(pipeline_run).show()\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/_experiment_method.py in wrapper(self, *args, **kwargs)\n     95             &quot;&quot;&quot;\n     96             ExperimentSubmitRegistrar.register_submit_function(self.__class__, submit_function)\n---&gt; 97             return init_func(self, *args, **kwargs)\n     98         return wrapper\n     99     return real_decorator\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/pipeline.py in __init__(self, workspace, steps, description, default_datastore, default_source_directory, resolve_closure, _workflow_provider, _service_endpoint, **kwargs)\n    175                 raise ValueError('parameter %s is not recognized for Pipeline ' % key)\n    176         self._enable_email_notification = enable_email_notification\n--&gt; 177         self._graph = self._graph_builder.build(self._name, steps, finalize=False)\n    178 \n    179     def _set_experiment_name(self, name):\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/builder.py in build(self, name, steps, finalize, regenerate_outputs)\n   1479                 pass\n   1480 \n-&gt; 1481         graph = self.construct(name, steps)\n   1482         if finalize:\n   1483             graph.finalize(regenerate_outputs=regenerate_outputs)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/builder.py in construct(self, name, steps)\n   1501         self._graph = Graph(name, self._context)\n   1502         self._nodeStack.append([])\n-&gt; 1503         self.process_collection(steps)\n   1504         for builder in self._builderStack[::-1]:\n   1505             builder.apply_rules()\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/builder.py in process_collection(self, collection)\n   1537         self._nodeStack.append([])\n   1538         self._builderStack.append(builder)\n-&gt; 1539         builder.process_collection(collection)\n   1540         added_nodes = self._nodeStack.pop()\n   1541         self._nodeStack[-1].extend(added_nodes)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/builder.py in process_collection(self, collection)\n   1828         &quot;&quot;&quot;\n   1829         for item in collection:\n-&gt; 1830             self._base_builder.process_collection(item)\n   1831 \n   1832     def apply_rules(self):\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/builder.py in process_collection(self, collection)\n   1531         # just a step?\n   1532         if isinstance(collection, PipelineStep):\n-&gt; 1533             return self.process_step(collection)\n   1534 \n   1535         # delegate to correct builder\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/builder.py in process_step(self, step)\n   1575             return self._step2node[step]\n   1576 \n-&gt; 1577         node = step.create_node(self._graph, self._default_datastore, self._context)\n   1578         self.assert_node_valid(step, self._graph, node)\n   1579 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/steps\/hyper_drive_step.py in create_node(self, graph, default_datastore, context)\n    247         &quot;&quot;&quot;\n    248         hyperdrive_config, reuse_hashable_config = self._get_hyperdrive_config(context._workspace,\n--&gt; 249                                                                                context._experiment_name)\n    250         self._params[HyperDriveStep._run_config_param_name] = json.dumps(hyperdrive_config)\n    251         self._params[HyperDriveStep._run_reuse_hashable_config] = json.dumps(reuse_hashable_config)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/steps\/hyper_drive_step.py in _get_hyperdrive_config(self, workspace, experiment_name)\n    323 \n    324         hyperdrive_dto = _search._create_experiment_dto(self._hyperdrive_config, workspace,\n--&gt; 325                                                         experiment_name, telemetry_values)\n    326 \n    327         hyperdrive_config = hyperdrive_dto.as_dict()\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/train\/hyperdrive\/_search.py in _create_experiment_dto(hyperdrive_config, workspace, experiment_name, telemetry_values, activity_logger, **kwargs)\n     41     if hyperdrive_config.source_directory is not None:\n     42         snapshot_client = SnapshotsClient(workspace.service_context)\n---&gt; 43         snapshot_id = snapshot_client.create_snapshot(hyperdrive_config.source_directory)\n     44 \n     45         if activity_logger is not None:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_restclient\/snapshots_client.py in create_snapshot(self, file_or_folder_path, retry_on_failure, raise_on_validation_failure)\n     83         exclude_function = ignore_file.is_file_excluded\n     84 \n---&gt; 85         self._validate_snapshot_size(file_or_folder_path, exclude_function, raise_on_validation_failure)\n     86 \n     87         # Get the previous snapshot for this project\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_restclient\/snapshots_client.py in _validate_snapshot_size(self, file_or_folder_path, exclude_function, raise_on_validation_failure)\n     61                             &quot;\\n&quot;.format(file_or_folder_path, SNAPSHOT_MAX_SIZE_BYTES \/ ONE_MB)\n     62             if raise_on_validation_failure:\n---&gt; 63                 raise SnapshotException(error_message)\n     64             else:\n     65                 self._logger.warning(error_message)\n\nSnapshotException: SnapshotException:\n    Message: ====================================================================\n\nWhile attempting to take snapshot of .\/train\/\nYour total snapshot size exceeds the limit of 0.00095367431640625 MB.\nPlease see http:\/\/aka.ms\/aml-largefiles on how to work with large files.\n\n====================================================================\n\n\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;====================================================================\\n\\nWhile attempting to take snapshot of .\/train\/\\nYour total snapshot size exceeds the limit of 0.00095367431640625 MB.\\nPlease see http:\/\/aka.ms\/aml-largefiles on how to work with large files.\\n\\n====================================================================\\n\\n&quot;\n    }\n}\n\u200b\n<\/code><\/pre>\n<p>Any idea how I fix this?<\/p>",
        "Challenge_closed_time":1612529086248,
        "Challenge_comment_count":3,
        "Challenge_created_time":1612425944867,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a Snapshot Exception while running a pipeline in Azure ML SDK. They had already tried some fixes like moving each step script to a separate subfolder, adding a datastore to the pipeline, and increasing the snapshot size limit. However, a new Snapshot error occurred after submitting the pipeline. The error message indicates that the total snapshot size exceeds the limit of 0.00095367431640625 MB. The user is seeking help to fix this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/258581\/how-do-i-fix-this-snapshot-exception",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":18.3,
        "Challenge_reading_time":96.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":61,
        "Challenge_solved_time":28.6503836111,
        "Challenge_title":"How do I fix this Snapshot Exception?",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":498,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Alright, so I found the fix.  <\/p>\n<p>I changed this line by adding a number equvilant to 1GB: <code>azureml._restclient.snapshots_client.SNAPSHOT_MAX_SIZE_BYTES = 1000000000<\/code>  <\/p>\n<p>For some reason, you have to define the size in BYTES and not megabytes even though the default is 300 MB. Not especially intuitive.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.3,
        "Solution_reading_time":4.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":46.0753738889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello:  <\/p>\n<p>I want to know that if it is possible automate copy file from azure storage to Azure ML folder.  <\/p>\n<p>I understand that it is duplication of data, but I want to know if yes, how I can do that.  <\/p>\n<p>Any pointer is greatly appreciated.  <\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1626436211016,
        "Challenge_comment_count":2,
        "Challenge_created_time":1626270339670,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information on how to automate the process of copying files from Azure storage to Azure ML folder, despite acknowledging that it involves duplication of data. They are requesting any guidance on how to achieve this.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/475768\/azure-ml-datastoredatasets",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.2,
        "Challenge_reading_time":3.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":46.0753738889,
        "Challenge_title":"Azure ML Datastore\\Datasets",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":52,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Depending on the frequency at which you would like to move data you can create scripts that could run on crontab to move the data between source storage account to your workspace blob store. For example, use <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/storage\/common\/storage-use-azcopy-blobs-copy?toc=\/azure\/storage\/blobs\/toc.json\">azcopy<\/a> to perform this activity.    <\/p>\n<p>A very comprehensive method to move storage between storage accounts is available as a <a href=\"https:\/\/learn.microsoft.com\/en-us\/learn\/modules\/copy-blobs-from-command-line-and-code\/\">Microsoft learn module<\/a> that you could take to understand the possibilities and attain this from code to automate in your application.     <\/p>\n<p> I would ideally assume that you would like to pull data when your experiment kicks off because you cannot move data to an experiments run id folder unless the experiment has started, In this case you could use the first option to place the data in your workspace blob store and then use it in your experiment without moving it to any other storage. I hope this helps.     <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.9,
        "Solution_reading_time":13.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":151.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1317676233236,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, United States",
        "Answerer_reputation_count":2348.0,
        "Answerer_view_count":206.0,
        "Challenge_adjusted_solved_time":276.3017388889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created an S3 bucket 'testshivaproject' and uploaded an image in it. When I try to access it in sagemaker notebook, it throws an error 'No such file or directory'.<\/p>\n\n<pre><code># import libraries\nimport boto3, re, sys, math, json, os, sagemaker, urllib.request\nfrom sagemaker import get_execution_role\nimport numpy as np                                   \n\n# Define IAM role\nrole = get_execution_role()\n\nmy_region = boto3.session.Session().region_name # set the region of the instance\n\nprint(\"success :\"+my_region)\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> success :us-east-2<\/p>\n\n<pre><code>role\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> 'arn:aws:iam::847047967498:role\/service-role\/AmazonSageMaker-ExecutionRole-20190825T121483'<\/p>\n\n<pre><code>bucket = 'testprojectshiva2' \ndata_key = 'ext_image6.jpg' \ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key) \nprint(data_location)\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> s3:\/\/testprojectshiva2\/ext_image6.jpg<\/p>\n\n<pre><code>test = load_img(data_location)\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> No such file or directory<\/p>\n\n<p>There are similar questions raised (<a href=\"https:\/\/stackoverflow.com\/questions\/48264656\/load-s3-data-into-aws-sagemaker-notebook\">Load S3 Data into AWS SageMaker Notebook<\/a>) but did not find any solution?<\/p>",
        "Challenge_closed_time":1567829268830,
        "Challenge_comment_count":0,
        "Challenge_created_time":1566834582570,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created an S3 bucket and uploaded an image in it. However, when trying to access it in Sagemaker notebook, an error 'No such file or directory' is thrown. The user has tried to load the image using the load_img() function but it did not work. The user is seeking a solution to this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57661142",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":15.1,
        "Challenge_reading_time":17.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":276.3017388889,
        "Challenge_title":"AWS S3 and Sagemaker: No such file or directory",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":4622.0,
        "Challenge_word_count":121,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442731325232,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hyderabad",
        "Poster_reputation_count":187.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>Thanks for using Amazon SageMaker!<\/p>\n\n<p>I sort of guessed from your description, but are you trying to use the Keras load_img function to load images directly from your S3 bucket?<\/p>\n\n<p>Unfortunately, <a href=\"https:\/\/github.com\/keras-team\/keras\/issues\/11684\" rel=\"nofollow noreferrer\">the load_img function is designed to only load files from disk<\/a>, so passing an s3:\/\/ URL to that function will always return a <code>FileNotFoundError<\/code>.<\/p>\n\n<p>It's common to first download images from S3 before using them, so you can use boto3 or the AWS CLI to download the file before calling load_img.<\/p>\n\n<p><strong>Alternatively<\/strong>, since the load_img function simply creates a <a href=\"https:\/\/en.wikipedia.org\/wiki\/Python_Imaging_Library\" rel=\"nofollow noreferrer\">PIL Image<\/a> object, you can create the PIL object directly from the data in S3 using boto3, and not use the load_img function at all.<\/p>\n\n<p>In other words, you could do something like this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from PIL import Image\n\ns3 = boto3.client('s3')\ntest = Image.open(BytesIO(\n    s3.get_object(Bucket=bucket, Key=data_key)['Body'].read()\n    ))\n<\/code><\/pre>\n\n<p>Hope this helps you out in your project!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.4,
        "Solution_reading_time":15.67,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":151.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":22.5241808334,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I used  <code>tempfile.mkdtemp() <\/code> to create a temporary directory for my runs (as I don\u2019t want a persistent folder with tons of runs)<\/p>\n<p>For training everything works fine but when resuming the run to do some validation \/ evaluation updates, and using <code>run.summary.update({\"key\": value})<\/code> I got a<\/p>\n<pre><code class=\"lang-auto\">wandb: WARNING Path \/tmp\/tmpq5uafy4d\/wandb\/ wasn't writable, using system temp directory\n<\/code><\/pre>\n<p>with obviously<\/p>\n<pre><code class=\"lang-auto\">File \"\/mnt\/Projets\/nlp\/.venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/internal\/sender.py\", line 855, in _update_summary\n    with open(summary_path, \"w\") as f:\nFileNotFoundError: [Errno 2] No such file or directory: '\/tmp\/tmpq5uafy4d\/wandb\/run-20211102_153311-37264m5k\/files\/wandb-summary.json'\n<\/code><\/pre>\n<p>As in the doc of <a href=\"https:\/\/docs.python.org\/3.9\/library\/tempfile.html#tempfile.mkdtemp\" rel=\"noopener nofollow ugc\"><code>mkdtemp<\/code><\/a> :<\/p>\n<pre><code class=\"lang-auto\"> The directory is readable, writable, and searchable only by the creating user ID.\n<\/code><\/pre>\n<p>So I guess WandB is not using the user ID and thus is not able to write in the directory for updating.<br>\nNote that this directory is different from the training one (as it\u2019s random at each init)<\/p>\n<p>Thanks in advance for any help.<br>\nHave a great day.<\/p>",
        "Challenge_closed_time":1635963680198,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635882593147,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user created a temporary directory using tempfile.mkdtemp() for their runs in WandB. While training, everything works fine, but when resuming the run for validation\/evaluation updates, WandB is not using the user ID and is unable to write in the directory for updating. The user received a warning message stating that the path was not writable and that the system temp directory would be used instead.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-not-using-user-pid-when-updating\/1204",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":12.0,
        "Challenge_reading_time":18.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":22.5241808334,
        "Challenge_title":"WandB not using user PID when updating",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":352.0,
        "Challenge_word_count":164,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>As of now, there isn\u2019t an option to enable that by default. One issue you should be aware of is that if you are currently logging a run when you call <code>wandb sync --clean<\/code> bad things will happen. We\u2019re working on improving the the robustness of these features and will likely support an automatic clean option in the future.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":4.16,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":59.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1352429442632,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":740.0,
        "Answerer_view_count":63.0,
        "Challenge_adjusted_solved_time":35.1769344445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to log data points for the same metric across multiple runs (<code>wandb.init<\/code> is called repeatedly in between each data point) and I'm unsure how to avoid the behavior seen in the attached screenshot...<\/p>\n<p>Instead of getting a line chart with multiple points, I'm getting a single data point with associated statistics. In the attached e.g., the 1st data point was generated at step 1,470 and the 2nd at step 2,940...rather than seeing two points, I'm instead getting a single point that's the average and appears at step 2,205.\n<a href=\"https:\/\/i.stack.imgur.com\/98Lln.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/98Lln.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>My hunch is that using the <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/resuming\" rel=\"nofollow noreferrer\">resume run<\/a> feature may address my problem, but even testing out this hunch is proving to be cumbersome given the constraints of the system I'm working with...<\/p>\n<p>Before I invest more time in my hypothesized solution, could someone confirm that the behavior I'm seeing is, indeed, the result of logging data to the same metric across separate runs without using the resume feature?<\/p>\n<p>If this is the case, can you confirm or deny my conception of how to use resume?<\/p>\n<p>Initial run:<\/p>\n<ol>\n<li><code>run = wandb.init()<\/code><\/li>\n<li><code>wandb_id = run.id<\/code><\/li>\n<li>cache <code>wandb_id<\/code> for successive runs<\/li>\n<\/ol>\n<p>Successive run:<\/p>\n<ol>\n<li>retrieve <code>wandb_id<\/code> from cache<\/li>\n<li><code>wandb.init(id=wandb_id, resume=&quot;must&quot;)<\/code><\/li>\n<\/ol>\n<p>Is it also acceptable \/ preferable to replace <code>1.<\/code> and <code>2.<\/code> of the initial run with:<\/p>\n<ol>\n<li><code>wandb_id = wandb.util.generate_id()<\/code><\/li>\n<li><code>wandb.init(id=wandb_id)<\/code><\/li>\n<\/ol>",
        "Challenge_closed_time":1662569956687,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662443319723,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to log data points for the same metric across multiple runs using wandb.init, but instead of getting a line chart with multiple points, they are getting a single data point with associated statistics. The user suspects that using the resume run feature may address the problem, but they are unsure. They are seeking confirmation that the behavior they are seeing is the result of logging data to the same metric across separate runs without using the resume feature and guidance on how to use the resume feature.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73617230",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":9.7,
        "Challenge_reading_time":24.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":35.1769344445,
        "Challenge_title":"How to avoid data averaging when logging to metric across multiple runs?",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":35.0,
        "Challenge_word_count":243,
        "Platform":"Stack Overflow",
        "Poster_created_time":1352429442632,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":740.0,
        "Poster_view_count":63.0,
        "Solution_body":"<blockquote>\n<p>My hunch is that using the resume run feature may address my problem,<\/p>\n<\/blockquote>\n<p>Indeed, providing a cached <code>id<\/code> in combination with <code>resume=&quot;must&quot;<\/code> fixed the issue.\n<a href=\"https:\/\/i.stack.imgur.com\/KooZ9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KooZ9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Corresponding snippet:<\/p>\n<pre><code>import wandb\n\n# wandb run associated with evaluation after first N epochs of training.\nwandb_id = wandb.util.generate_id()\nwandb.init(id=wandb_id, project=&quot;alrichards&quot;, name=&quot;test-run-3\/job-1&quot;, group=&quot;test-run-3&quot;)\nwandb.log({&quot;mean_evaluate_loss_epoch&quot;: 20}, step=1)\nwandb.finish()\n\n# wandb run associated with evaluation after second N epochs of training.\nwandb.init(id=wandb_id, resume=&quot;must&quot;, project=&quot;alrichards&quot;, name=&quot;test-run-3\/job-2&quot;, group=&quot;test-run-3&quot;)\nwandb.log({&quot;mean_evaluate_loss_epoch&quot;: 10}, step=5)\nwandb.finish()\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.6,
        "Solution_reading_time":14.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":83.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":147.8475,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nWhen I launch `kedro run` and the run fails, the `on_pipeline_error` closes all the mlflow runs (to avoid interactions with further runs)\r\n\r\n## Context\r\n\r\nI cannot distinguish failed runs from sucessful ones in the mlflow ui.\r\n\r\n## Steps to Reproduce\r\n\r\nLaunch a failing pipeline with kedro run.\r\n\r\n## Expected Result\r\n\r\nThe mlflow ui should display the run with a red cross\r\n\r\n## Actual Result\r\n\r\nThe mlflow ui displays the run with a green tick\r\n\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes.\r\n\r\n## Potential solution: \r\n\r\nReplace these lines:\r\n\r\n`https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L193-L194`\r\n\r\nwith \r\n\r\n```python\r\nwhile mlflow.active_run():\r\n    mlflow.end_run(mlflow.entities.RunStatus.FAILED)\r\n```\r\nor even better, retrieve current run status from mlflow?\r\n",
        "Challenge_closed_time":1606515096000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1605982845000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with MlflowDataSet failing to log on remote storage when the underlying dataset filepath is converted as a PurePosixPath. The error occurs when the local path is Linux and the `mlflow_tracking_uri` is an Azure blob storage. The issue can be fixed by replacing `self._filepath` by `self._filepath.as_posix()` in two locations.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/121",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.8,
        "Challenge_reading_time":11.93,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":147.8475,
        "Challenge_title":"RunStatus of mlflow run is \"FINISHED\" instead of \"FAILED\" when the kedro run fails",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":117,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Good catch ! \r\nSince we catch the Error and manually end the run, mlflow do not receive the \"error code 1\" of the current process. If we no longer end run manually, mlflow will tag the run as FAILED. But since we want to control the pipeline error, we can apply your suggestion (specifiying the status as failed) Yes, but we need to terminate the run manually when it failed and one use it interactively (in CLI, tis makes no difference because it gets the error code as you say) to avoid further interference.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.0,
        "Solution_reading_time":6.1,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":93.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.8364655556,
        "Challenge_answer_count":1,
        "Challenge_body":"Which Sagemaker Domain is used when using Redshift ML CREATE MODEL",
        "Challenge_closed_time":1683298611292,
        "Challenge_comment_count":0,
        "Challenge_created_time":1683292000016,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information on which SageMaker domain to use when creating a model with Redshift ML.",
        "Challenge_last_edit_time":1683639542431,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUHf0f_0SuS5KOSSctyn37xA\/redshift-ml-sagemaker-domain",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":1.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":1.8364655556,
        "Challenge_title":"Redshift ML SageMaker Domain",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":47.0,
        "Challenge_word_count":14,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @jkrice, Redshift ML does not use SageMaker Studio (domains). When you run a CREATE MODEL statement, it calls SageMaker to create an Autopilot job (https:\/\/aws.amazon.com\/sagemaker\/autopilot\/) to train a model. Autopilot usually does preprocessing, and trains on a variety of suitable models for your use case and finally returns the best model, that's then deployed for inference in Redshift. You can see more information here - https:\/\/docs.aws.amazon.com\/redshift\/latest\/dg\/machine_learning.html",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1683298611292,
        "Solution_link_count":2.0,
        "Solution_readability":10.8,
        "Solution_reading_time":6.39,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1350031215710,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":18744.0,
        "Answerer_view_count":1274.0,
        "Challenge_adjusted_solved_time":31.0469397222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to use mlflow to track the development of a TensorFlow model. How do I log the loss at each epoch? I have written the following code:<\/p>\n<pre><code>mlflow.set_tracking_uri(tracking_uri)\n\nmlflow.set_experiment(&quot;\/deep_learning&quot;)\nwith mlflow.start_run():\n    mlflow.log_param(&quot;batch_size&quot;, batch_size)\n    mlflow.log_param(&quot;learning_rate&quot;, learning_rate)\n    mlflow.log_param(&quot;epochs&quot;, epochs)\n    mlflow.log_param(&quot;Optimizer&quot;, opt)\n    mlflow.log_metric(&quot;train_loss&quot;, train_loss)\n    mlflow.log_metric(&quot;val_loss&quot;, val_loss)\n    mlflow.log_metric(&quot;test_loss&quot;, test_loss)\n    mlflow.log_metric(&quot;test_mse&quot;, test_mse)\n    mlflow.log_artifacts(&quot;.\/model&quot;)\n<\/code><\/pre>\n<p>If I change the train_loss and val_loss to<\/p>\n<pre><code>train_loss = history.history['loss']\nval_loss = history.history['val_loss']\n<\/code><\/pre>\n<p>I get the following error:<\/p>\n<pre><code>mlflow.exceptions.MlflowException: Got invalid value [12.041399002075195] for metric 'train_loss' (timestamp=1649783654667). Please specify value as a valid double (64-bit floating point)\n<\/code><\/pre>\n<p>How to I save the the loss and the val_loss at all epochs, so I can visualise a learning curve within mlflow?<\/p>",
        "Challenge_closed_time":1649809588952,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649783963393,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to track the development of a TensorFlow model using mlflow and is trying to log the loss at each epoch. However, when attempting to change the train_loss and val_loss to history.history['loss'] and history.history['val_loss'], respectively, an error occurs. The user is seeking guidance on how to save the loss and val_loss at all epochs to visualize a learning curve within mlflow.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71846804",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":17.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":7.1182108333,
        "Challenge_title":"How to I track loss at epoch using mlflow\/tensorflow?",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":320.0,
        "Challenge_word_count":118,
        "Platform":"Stack Overflow",
        "Poster_created_time":1507645492476,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Liverpool, UK",
        "Poster_reputation_count":77.0,
        "Poster_view_count":28.0,
        "Solution_body":"<p>As you can read <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.keras.html#module-mlflow.keras\" rel=\"nofollow noreferrer\">here<\/a>. You can use <code>mlflow.tensorflow.autolog()<\/code> and this, (from doc):<\/p>\n<blockquote>\n<p>Enables (or disables) and configures autologging from Keras to MLflow. Autologging captures the following information:<\/p>\n<blockquote>\n<p>fit() or fit_generator() parameters; optimizer name; learning rate; epsilon\n...<\/p>\n<\/blockquote>\n<\/blockquote>\n<p>For example:<\/p>\n<pre><code># !pip install mlflow\nimport tensorflow as tf\nimport mlflow\nimport numpy as np\n\n\nX_train = np.random.rand(100,100)\ny_train = np.random.randint(0,10,100)\n    \n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.Input(100,))\nmodel.add(tf.keras.layers.Dense(256, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(rate=.4))\nmodel.add(tf.keras.layers.Dense(10, activation='sigmoid'))        \nmodel.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n              optimizer='Adam', \n              metrics=['accuracy'])\nmodel.summary()\n\n\nmlflow.tensorflow.autolog()\nhistory = model.fit(X_train, y_train, epochs=100, batch_size=50)\n<\/code><\/pre>\n<p>Or as you mention in the comment you can use <code>mlflow.set_tracking_uri()<\/code> like below:<\/p>\n<pre><code>mlflow.set_tracking_uri('http:\/\/127.0.0.1:5000')\ntracking_uri = mlflow.get_tracking_uri()\nwith mlflow.start_run(run_name='PARENT_RUN') as parent_run:\n    batch_size=50\n    history = model.fit(X_train, y_train, epochs=2, batch_size=batch_size)\n    mlflow.log_param(&quot;batch_size&quot;, batch_size)  \n<\/code><\/pre>\n<p>For getting results:<\/p>\n<pre><code>!mlflow ui\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>[....] [...] [INFO] Starting gunicorn 20.1.0\n[....] [...] [INFO] Listening at: http:\/\/127.0.0.1:5000 (****)\n[....] [...] [INFO] Using worker: sync\n[....] [...] [INFO] Booting worker with pid: ****\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/9XXoi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9XXoi.png\" alt=\"enter image description here\" \/><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/V2tvM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/V2tvM.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1649895732376,
        "Solution_link_count":7.0,
        "Solution_readability":16.1,
        "Solution_reading_time":29.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":160.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1529340706892,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":146.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":9.9577358333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm working on a project that, because of the company's compliance rules, the data has to stay in a shared directory, that is synchronized among the programmers. The project's code on the other hand cannot be on that shared directory otherwise we wouldn't be able to version it and work together since it's all synchronized. The path to the shared folder is pretty much the same <code>C:\\Users\\&lt;employee name&gt;\\&lt;path to data&gt;<\/code>, is there a way that I can setup <code>C:\\Users\\&lt;employee name&gt;<\/code> as a base path for my data catalog in Kedro?<\/p>\n<p>I tried creating a <code>catalog.py<\/code> file that has the following code:<\/p>\n<pre><code>from kedro.io import DataCatalog\nfrom kedro.extras.datasets.pandas import (\n    CSVDataSet,\n    ExcelDataSet,\n)\nfrom pathlib import Path\n\nDEFAULT_DATA_PATH = Path.expanduser(\n    Path(\n        &quot;~&quot;, \n        &quot;Path to Data&quot;\n    )\n)\n\nDATA_CATALOG = DataCatalog(\n    {\n        &quot;data&quot;: ExcelDataSet(\n            filepath=Path(EXTERNAL_DATA_PATH, &quot;data.xlsx&quot;).as_uri()\n        )\n            \n    }\n)\n<\/code><\/pre>\n<p>And then on the <code>setting.py<\/code> I've added this:<\/p>\n<pre><code>from .catalog import DATA_CATALOG\nDATA_CATALOG_CLASS = DATA_CATALOG\n<\/code><\/pre>\n<p>but then I get the following error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;...\\Miniconda3\\Scripts\\kedro-script.py&quot;, line 9, in &lt;module&gt;\n    sys.exit(main())\n  File &quot;...\\Miniconda3\\lib\\site-packages\\kedro\\framework\\cli\\cli.py&quot;, line 205, in main \n    cli_collection = KedroCLI(project_path=Path.cwd())\n  File &quot;...\\Miniconda3\\lib\\site-packages\\kedro\\framework\\cli\\cli.py&quot;, line 114, in __init__\n    self._metadata = bootstrap_project(project_path)\n  File &quot;...\\Miniconda3\\lib\\site-packages\\kedro\\framework\\startup.py&quot;, line 155, in bootstrap_project\n    configure_project(metadata.package_name)\n  File &quot;...\\Miniconda3\\lib\\site-packages\\kedro\\framework\\project\\__init__.py&quot;, line 166, in configure_project\n    settings.configure(settings_module)\n  File &quot;...\\Miniconda3\\lib\\site-packages\\dynaconf\\base.py&quot;, line 223, in configure      \n    self._wrapped = Settings(settings_module=settings_module, **kwargs)\n  File &quot;...\\Miniconda3\\lib\\site-packages\\dynaconf\\base.py&quot;, line 271, in __init__       \n    self.validators.validate()\n  File &quot;...\\Miniconda3\\lib\\site-packages\\dynaconf\\validator.py&quot;, line 318, in validate  \n    validator.validate(self.settings)\n  File &quot;...\\Miniconda3\\lib\\site-packages\\kedro\\framework\\project\\__init__.py&quot;, line 34, \nin validate\n    if not issubclass(setting_value, default_class):\nTypeError: issubclass() arg 1 must be a class\n<\/code><\/pre>",
        "Challenge_closed_time":1651566013012,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651530165163,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to set up a base directory for the Data Catalog in Kedro, but the data has to stay in a shared directory that is synchronized among the programmers, while the project's code cannot be on that shared directory. The user tried creating a catalog.py file and adding it to the settings.py file, but encountered an error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72093004",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.6,
        "Challenge_reading_time":35.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":29,
        "Challenge_solved_time":9.9577358333,
        "Challenge_title":"Setup a base dir for the Data Catalog in Kedro",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":86.0,
        "Challenge_word_count":243,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423164285360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Itabira, Brazil",
        "Poster_reputation_count":856.0,
        "Poster_view_count":106.0,
        "Solution_body":"<p><code>DATA_CATALOG_CLASS<\/code> is expecting a class while you are providing an instance of data catalog, thus the error.<\/p>\n<p>I think the way to go here to use <code>TemplatedConfigLoader<\/code>, and pass the share directory as a variable. You would supply this <code>SHARE_DIR<\/code> either through a <code>global.yml<\/code> or just a variable.<\/p>\n<p>In your <code>catalog.yml<\/code>\nsome_data:\ntype: pandas.CSVDataSet<\/p>\n<p>See more documentation here.\n<a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.config.TemplatedConfigLoader.html\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.config.TemplatedConfigLoader.html<\/a>\npath: ${SHARE_DIR}\/file_name<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.3,
        "Solution_reading_time":9.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":64.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":51.0787569444,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hey everyone,<br>\nLet\u2019s say that I have a dataset with 50000 samples and I am training my model for 10 epochs. Now, in each epoch, I am recording the <em>per sample loss<\/em> (i.e. loss of each sample - Not the average loss of all samples). This means that there are 50000 loss values per epoch. I want to log these values for <em>each epoch<\/em>, so that I can later perform some analysis on how the loss values for the samples change as training progresses (And, if possible, observe the loss values of a particular sample across epochs). For reference, <a href=\"https:\/\/proceedings.neurips.cc\/paper\/2020\/file\/62000dee5a05a6a71de3a6127a68778a-Paper.pdf\" rel=\"noopener nofollow ugc\">this<\/a> paper tracks such  statistics. Here are two ways I can think of doing this -<\/p>\n<ul>\n<li>A simple way to do this is to update the values in a 50000x10 array, then log the array as a table at the end of training (I would obviously need to track which indices belong to which samples). However, I need to wait for the training to end in this scenario.<\/li>\n<li>I can also log each sample\u2019s statistic with wandb.log (Maybe put them under \u201csample_statistics\/\u201d to pull them more easily). This ensures that the metrics are logged as and when they are observed, however, I am not sure if this is the most optimal solution.<\/li>\n<\/ul>\n<p>Is there any other way in which I can do this so that I can analyse the resulting data effectively? Open to all suggestions!<br>\nThank you!<\/p>",
        "Challenge_closed_time":1656718364004,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656534480479,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to record per sample loss for each epoch while training a model on a dataset with 50,000 samples for 10 epochs. They are considering two options - updating values in a 50,000x10 array and logging it as a table at the end of training or logging each sample's statistic with wandb.log. The user is open to suggestions for an optimal solution to analyze the resulting data effectively.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/logging-metrics-for-each-sample-per-epoch\/2678",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":7.0,
        "Challenge_reading_time":18.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":51.0787569444,
        "Challenge_title":"Logging Metrics for each sample per epoch",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":267.0,
        "Challenge_word_count":248,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/tataganesh\">@tataganesh<\/a> ,<\/p>\n<p>Thank you for writing in with your question.   Ideally what would be best in your case here is to create an Empty table and log per sample loss values per epoch and be able to see your data live in the UI. However, we currently,  don\u2019t support adding new rows to existing tables that you\u2019ve already logged. We are working on adding this functionality.<\/p>\n<p>In the meantime here are two approaches<\/p>\n<ol>\n<li>Keep the wandb.Table locally holding all the data in memory and logging it once.<\/li>\n<li>Keep logging the same table at each step, and just add new rows to it. The final table you log will have all the rows you want, and you\u2019ll be able to see the latest table logged in the UI. This would be risky if you have large table sizes.<\/li>\n<\/ol>\n<p>Please Note: If you were to look through our docs and come across the<a href=\"https:\/\/docs.wandb.ai\/guides\/data-vis\/log-tables#add-data\"> Add Data Incrementally<\/a> to  Tables doc, this functionality is currently broken and we are working on an active fix.  There is github issues thread <a href=\"https:\/\/github.com\/wandb\/client\/issues\/2981\" rel=\"noopener nofollow ugc\">here<\/a> where community members have posted workarounds for this, you may find it helpful.<\/p>\n<p>Please let me know if you have any questions.<\/p>\n<p>Regards,<\/p>\n<p>Mohammad<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.6,
        "Solution_reading_time":17.08,
        "Solution_score_count":null,
        "Solution_sentence_count":15.0,
        "Solution_word_count":210.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.8413888889,
        "Challenge_answer_count":0,
        "Challenge_body":"<!-- \r\n### Common bugs:\r\n1. Tensorboard not showing in Jupyter-notebook see [issue 79](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/79).    \r\n2. PyTorch 1.1.0 vs 1.2.0 support [see FAQ](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning#faq)    \r\n-->\r\n\r\n## \ud83d\udc1b Bug\r\n\r\nCometmllogger with api key and  without save dir results in error.\r\nThis happens due to this if https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/comet.py#L135\r\n_save_dir is not set and later train loop tries to read it and fails.\r\nThis can be fixed by setting _save_dir to None. I will supply PR in a moment\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n```\r\n    model = LightningModel({})\r\n    comet_logger = CometLogger(\r\n        api_key=KEY,\r\n        workspace=\"workspace\"\r\n    )\r\n\r\n    trainer = Trainer(logger=comet_logger)\r\n    trainer.fit(model)\r\n```\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n\r\nTraceback (most recent call last):\r\ntrainer.fit(model)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/states.py\", line 48, in wrapped_fn\r\nresult = fn(self, *args, **kwargs)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1073, in fit\r\nresults = self.accelerator_backend.train(model)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py\", line 51, in train\r\nresults = self.trainer.run_pretrain_routine(model)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1239, in run_pretrain_routine\r\nself.train()\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/training_loop.py\", line 363, in train\r\nself.on_train_start()\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/callback_hook.py\", line 111, in on_train_start\r\ncallback.on_train_start(self, self.get_model())\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 27, in wrapped_fn\r\nreturn fn(*args, **kwargs)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/callbacks\/model_checkpoint.py\", line 296, in on_train_start\r\nsave_dir = trainer.logger.save_dir or trainer.default_root_dir\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/loggers\/comet.py\", line 253, in save_dir\r\nreturn self._save_dir\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Challenge_closed_time":1599658056000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1599655027000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue where the Comet logger cannot be pickled after an experiment has been created. Initializing the logger object and Trainer object with the logger works fine, but accessing the experiment attribute which creates the OfflineExperiment object fails. The expected behavior is to be able to pickle loggers for distributed training. The user is using pytorch-lightning version 0.7.5 and Python version 3.7.6 on a Darwin system.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3417",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":31.44,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":0.8413888889,
        "Challenge_title":"CometLogger failing without save_dir",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":206,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi! thanks for your contribution!, great first issue!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.7,
        "Solution_reading_time":0.68,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":8.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":555.5452777778,
        "Challenge_answer_count":4,
        "Challenge_body":"Hello,  \n  \nMy aim is to create a model for garden birds. I have 293 photos of birds that I have put through 2 custom labelling jobs in ground truth for training and validation. The issue I encountered was being able to have multiple labels on the bounding box which I managed to do via creating a custom labelling job with the following labels:\n\n```\n<crowd-bounding-box\r\n    name=\"annotatedResult\"\r\n    labels=\"['Blackbird', 'Blue tit', 'Coal tit', 'Dunnock', 'Great tit', 'Long-tailed tit', 'Nuthatch', 'Pigeon', 'Robin']\" .....\n```\n\nI now have 2 output manifest files with many lines of this:\n\n```\n{\"source-ref\":\"s3:\/\/XXXXX\/Blackbird_1.JPG\",\"BirdLabel\":{\"workerId\":\"privateXXXXX\",\"imageSource\":{\"s3Uri\":\"s3:\/\/XXXXX\/Blackbird_1.JPG\"},\"boxesInfo\":{\"annotatedResult\":{\"boundingBoxes\":[{\"width\":1619,\"top\":840,\"label\":\"Blackbird\",\"left\":1287,\"height\":753}],\"inputImageProperties\":{\"width\":3872,\"height\":2592}}}},\"BirdLabel-metadata\":{\"type\":\"groundtruth\/custom\",\"job-name\":\"birdlabel\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-01-10T15:41:52+0000\"}}\n```\n\nAfter this job was successful, I made an ml.p3.2xlarge instance, using the object_detection_augmented_manifest_training template.  \n  \nI have filled in the necessary sections, I then run it and received this error when I have the Content Type to _'application\/x-image'_ with _Record wrapper type:RecordIO_ : **'ClientError: train channel is not specified.'**  \n  \nI then changed the channel to train_annotation instead of train and I receive this error message: **\"ClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError)\\n\\nCaused by: u'train' is a required property**  \n  \nAdditional information can be provided if neccessary.  \nAny help would be much apreciated! Thank you.  \n  \nEdited by: LuciA on Jan 16, 2019 1:12 PM  \n  \nEdited by: LuciA on Jan 16, 2019 1:18 PM  \n  \nEdited by: LuciA on Jan 16, 2019 1:19 PM",
        "Challenge_closed_time":1549563627000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1547563664000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a model for garden birds using 293 photos that have been put through 2 custom labelling jobs in ground truth for training and validation. The user encountered an issue with multiple labels on the bounding box, which was resolved by creating a custom labelling job. However, when using the object_detection_augmented_manifest_training template, the user received the error message \"ClientError: train channel is not specified\" when the Content Type was set to 'application\/x-image' with Record wrapper type:RecordIO. Changing the channel to train_annotation resulted in the error message \"ClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError) Caused by: u'train' is",
        "Challenge_last_edit_time":1668624099052,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUjk2-AZ6VQl68Zdm1Owq-_A\/clienterror-object-detection-augmented-manifest-training-template",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":13.4,
        "Challenge_reading_time":25.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":555.5452777778,
        "Challenge_title":"ClientError: object_detection_augmented_manifest_training template",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":123.0,
        "Challenge_word_count":219,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi LuciA - I'm an engineer at AWS. Thanks for continuing to try the service in the face of some difficulties. Can you please cross-reference your augmented manifest against the samples shown in https:\/\/aws.amazon.com\/blogs\/aws\/amazon-sagemaker-ground-truth-build-highly-accurate-datasets-and-reduce-labeling-costs-by-up-to-70\/, https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/object_detection_augmented_manifest_training\/object_detection_augmented_manifest_training.ipynb, and https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/ground_truth_object_detection_tutorial\/object_detection_tutorial.ipynb?   \n  \nIt looks like your format is a little different, e.g., the algorithm expects to see keys called \"annotations\" and \"image_size\". Can you please check the syntax and let us know if your results change?",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1549563627000,
        "Solution_link_count":3.0,
        "Solution_readability":21.3,
        "Solution_reading_time":11.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.0441666667,
        "Challenge_answer_count":0,
        "Challenge_body":"When only `zn.Method` without `zn.params` is used in a Node the `dvc.yaml` will not depend on the `params.yaml`.\r\n",
        "Challenge_closed_time":1643235530000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643228171000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user needs to update dvc for dvc-bench to work with versions greater than 2.0.0, but ignoring lockfile is not allowed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/zincware\/ZnTrack\/issues\/211",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":2.8,
        "Challenge_reading_time":1.95,
        "Challenge_repo_contributor_count":3.0,
        "Challenge_repo_fork_count":4.0,
        "Challenge_repo_issue_count":459.0,
        "Challenge_repo_star_count":32.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2.0441666667,
        "Challenge_title":"zn.Method does not add params to `dvc.yaml`",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":24,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":51.3389041667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to transfer a generated csv file <code>test_df.csv<\/code> from my Azure ML notebook folder which has a path <code>\/Users\/Ankit19.Gupta\/test_df.csv<\/code> to a datastore which has a web path <code>https:\/\/abc.blob.core.windows.net\/azureml\/LocalUpload\/f3db18b6<\/code>. I have written the python code as<\/p>\n<pre><code>from azureml.core import Workspace\nws = Workspace.from_config()\ndatastore = ws.get_default_datastore()\n    \ndatastore.upload_files('\/Users\/Ankit19.Gupta\/test_df.csv',\n                  target_path='https:\/\/abc.blob.core.windows.net\/azureml\/LocalUpload\/f3db18b6',\n                  overwrite=True)\n<\/code><\/pre>\n<p>But it is showing the following error message:<\/p>\n<pre><code>UserErrorException: UserErrorException:\n    Message: '\/' does not point to a file. Please upload the file to cloud first if running in a cloud notebook.\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;message&quot;: &quot;'\/' does not point to a file. Please upload the file to cloud first if running in a cloud notebook.&quot;\n    }\n}\n<\/code><\/pre>\n<p>I have tried <a href=\"https:\/\/stackoverflow.com\/questions\/67897947\/how-to-transfer-data-from-azure-ml-notebooks-to-a-storage-container\">this<\/a> but it is not working for me. Can anyone please help me to resolve this issue. Any help would be appreciated.<\/p>",
        "Challenge_closed_time":1663658453172,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663473633117,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to transfer a generated csv file from their Azure ML notebook folder to a datastore using Python code. However, they are encountering an error message stating that '\/' does not point to a file and to upload the file to the cloud first if running in a cloud notebook. The user has tried a solution from Stack Overflow but it did not work. They are seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73760033",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":10.5,
        "Challenge_reading_time":18.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":51.3389041667,
        "Challenge_title":"How to transfer a csv file from notebook folder to a datastore",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":43.0,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540154634483,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":237.0,
        "Poster_view_count":173.0,
        "Solution_body":"<p>The way the path was mentioned is not accurate. The datastore path will be different manner.\nReplace the below code for the small change in the calling path.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4o1WU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4o1WU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<pre><code>from azureml.core import Workspace\nws = Workspace.from_config()\ndatastore = ws.get_default_datastore()\n    \ndatastore.upload_files('.\/Users\/foldername\/filename.csv',\n                  target_path=\u2019your targetfolder',\n                  overwrite=True)\n<\/code><\/pre>\n<p>We need to call all the parent folders before the folder.  <strong><code>\u201c.\/\u201d<\/code><\/strong> is the way we can call the dataset from datastore.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.9,
        "Solution_reading_time":9.46,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":73.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.0877777778,
        "Challenge_answer_count":1,
        "Challenge_body":"What are effective working patterns and tools to ensure we can easily reproduce the model artifacts deployed in production? (Customer is using DVC and Github to get version control on all key aspects: data, training scripts, model specification, hyper parameters, etc.) I'd like to share AWS best practices and recommendations with them.",
        "Challenge_closed_time":1619814328000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619803212000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking effective working patterns and tools to easily reproduce model artifacts used in machine learning deployed in production. They are currently using DVC and Github for version control on all key aspects, and are looking for AWS best practices and recommendations.",
        "Challenge_last_edit_time":1668544599315,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUp5EvOvmhTkqTM-NYhYAJZA\/tracking-model-artifacts-used-in-machine-learning",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":4.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":3.0877777778,
        "Challenge_title":"Tracking model artifacts used in machine learning",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":349.0,
        "Challenge_word_count":58,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"There are a few good practices that would ensure a robust model governance and tracking strategies in place. This is usually required by regulators especially in the financial industry and defined by frameworks like SR 11-7:\n\n**\u2022 Identify models, owners, and associated usage:** This is usually controlled by having a controlled landing zone for data scientists with clear authentication and authorization strategies to keep track of user activities and model owners.\n\n**\u2022 Cover all aspects of the model life cycle and MLOps:** By using an experimentation, proper documentation, feature tagging, testing, deployment environment and pipelining so that models can be independently validated by model validators without having to get back to model developers.\n\n**\u2022 maintain a centralized model inventory, and track the current validation status:** by keeping track of different model versions along with its associated risk and validation processes.\n\n**On-going monitoring for production models:** by implementing mechanisms to continuously assess accuracy, drift, building constraints on data feed used for inference and outcome analysis for different model versions.\n\nNow saying that, there are many tools out there to help in implementing and achieving all the above, which are scattered and can become challenge in implementation and integration. However, SageMaker have different modules to cover model of the practices mentioned above.\n\n- SageMaker Experiments: tracks all different steps of ML lifecycle in a construct called trial component. A bunch of trial components can form a trial and a trial belongs to an experiment. \n\n- SageMaker Pipelines: Help in building a re-producible experiment. Each stage of the ML Lifecycle fits in the pipeline and can automatically be tracked as Trial Components and tracked by its execution history.\n\n- SageMaker Model Registry: Builds a centralized catalog for models to manage different model versions, associate meta data with different version of models and manage approvals to enforce ownership.\n\n- SageMaker Model Monitoring: for managing data feed constraints, model performance analysis and drift detection.\n\n- SageMaker Feature Store: with proper tagging for feature groups and why certain features have been engineered by who is also a must.\n\n- SageMaker ML Lineage: which - from my point of view - is the most important component for model tracking, auditing and governance. SM ML Lineage is the glue that builds a graph to trace a certain model back to its origins. It can tell what Artifact contributed to which Trial Component and what data produced which model.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925575540,
        "Solution_link_count":0.0,
        "Solution_readability":14.6,
        "Solution_reading_time":32.44,
        "Solution_score_count":0.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":389.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":120.5566666667,
        "Challenge_answer_count":0,
        "Challenge_body":"Currently, it will display always display\r\n`========== Make your selection, Press \"h\" for help ==========`\r\neven if there is no selection to make since the list of files is empty\r\n\r\nhttps:\/\/github.com\/DAGsHub\/fds\/blob\/a8fea54f59131d3ddea4df5184adeee3ecc9998f\/fds\/services\/dvc_service.py#L119",
        "Challenge_closed_time":1622551859000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1622117855000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges with the current implementation of the rename action in DVC, which uses shutils' mv command to rename the base image. This approach is invalid as it creates two identical files with different names when a DVC pull is performed. The user suggests using the dvc rename command to rename the actual file, pointer, and update the path property for consistency.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/37",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":4.48,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":139.0,
        "Challenge_repo_star_count":357.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":120.5566666667,
        "Challenge_title":"Only display the DVC add prompt if there is anything to add",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":40,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Fixed in #46 ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-2.7,
        "Solution_reading_time":0.15,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1327481639092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Poznan, Poland",
        "Answerer_reputation_count":2923.0,
        "Answerer_view_count":838.0,
        "Challenge_adjusted_solved_time":2025.0750175,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Within AzureML, I have a CSV file which contains <code>2 columns<\/code> of data with <code>thousands of rows<\/code>. I'm looking to run this entire file as training, and find a pattern between these 2 sets of numbers, for example:<\/p>\n\n<pre><code>x -&gt; y\n\n... 10k x\n<\/code><\/pre>\n\n<p>And after all that training, I'd want to give this one line as the score model, so It'd look like:\nx -> ? (Predict answer from training)\n-- Note, the question mark here wouldn't need to be an exact match, as long as it is somewhat around what that actual number would turn out to be like.<\/p>\n\n<p>Is their a ML method (Inside <code>Azure ML<\/code>) that does such thing? Any points would be great.<\/p>\n\n<p>tl;dr: <code>Finding any type of pattern between 2 numbers (w\/ intense training).<\/code><\/p>",
        "Challenge_closed_time":1448906765136,
        "Challenge_comment_count":0,
        "Challenge_created_time":1441616495073,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking advice on the best ML method to use within AzureML to find a pattern between two sets of numbers in a CSV file with thousands of rows. The user wants to use the entire file for training and then use one line as a score model to predict an answer.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/32434805",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":10.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":2025.0750175,
        "Challenge_title":"What would be the best ML method for this use case?",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":32.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416688064183,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":347.0,
        "Poster_view_count":51.0,
        "Solution_body":"<p>Read about <code>linear regression<\/code>. This is answer for your question. And here is the link to Azure ML tutorial <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-create-experiment\/\" rel=\"nofollow\">link<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.8,
        "Solution_reading_time":3.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":22.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":0.4277797222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to set up a DVC repository for machine learning data with different tagged versions of the dataset. I do this with something like:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ cd \/raid\/ml_data  # folder on a data drive\n$ git init\n$ dvc init\n$ [add data]\n$ [commit to dvc, git]\n$ git tag -a 1.0.0\n$ [add or change data]\n$ [commit to dvc, git]\n$ git tag -a 1.1.0\n<\/code><\/pre>\n<p>I have multiple projects that each need to reference some version of this dataset. The problem is I can't figure out how to set up those projects to reference a specific version. I'm able to track the <code>HEAD<\/code> of the repo with something like:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ cd ~\/my_proj  # different drive than the remote\n$ mkdir data\n$ git init\n$ dvc init\n$ dvc remote add -d local \/raid\/ml_data  # add the remote on my data drive\n$ dvc cache dir \/raid\/ml_data\/.dvc\/cache  # tell DVC to use the remote cache\n$ dvc checkout\n$ dvc run --external -d \/raid\/ml_data -o data\/ cp -r \/raid\/ml_data data\n<\/code><\/pre>\n<p>This gets me the latest version of the dataset, symlinked into my <code>data<\/code> folder, but what if I want some projects to use the <code>1.0.0<\/code> version and some to use the <code>1.1.0<\/code> version, or another version? Or for that matter, if I update the dataset to <code>2.0.0<\/code> but don't want my existing projects to necessarily track <code>HEAD<\/code> and instead keep the version with which they were set up?<\/p>\n<p>It's important to me to not create a ton of local copies of my dataset as the <code>\/home<\/code> drive is much smaller than the <code>\/raid<\/code> drive and some of these datasets are huge.<\/p>",
        "Challenge_closed_time":1604351561432,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604349754297,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to set up a DVC repository for machine learning data with different tagged versions of the dataset. However, they are unable to figure out how to set up multiple projects to reference a specific version of the dataset. They are currently able to track the latest version of the dataset but want to be able to use different versions for different projects without creating multiple local copies of the dataset.",
        "Challenge_last_edit_time":1604361023336,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64653042",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":21.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":0.5019819444,
        "Challenge_title":"Control tracked version of external dependency",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":139.0,
        "Challenge_word_count":267,
        "Platform":"Stack Overflow",
        "Poster_created_time":1370629593700,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Colorado Springs, CO",
        "Poster_reputation_count":11685.0,
        "Poster_view_count":1329.0,
        "Solution_body":"<p>I think you are looking for the <a href=\"https:\/\/dvc.org\/doc\/start\/data-access\" rel=\"nofollow noreferrer\">data access<\/a> set of commands.<\/p>\n<p>In your particular case, <code>dvc import<\/code> makes sense:<\/p>\n<pre><code>$ dvc import \/raid\/ml_data data\n<\/code><\/pre>\n<p>if you want to get the most recent version (HEAD). Then you will be able to update it with the <code>dvc update<\/code> command (if 2.0.0 is released, for example).<\/p>\n<pre><code>$ dvc import \/raid\/ml_data data --rev 1.0.0\n<\/code><\/pre>\n<p>if you'd like to &quot;fix&quot; it to the specific version.<\/p>\n<h3>Avoiding copies<\/h3>\n<p>Make sure also, that <code>symlinks<\/code> are set for the second project, as described in the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\" rel=\"nofollow noreferrer\">Large Dataset Optimization<\/a>:<\/p>\n<pre><code>$ dvc config cache.type reflink,hardlink,symlink,copy\n<\/code><\/pre>\n<p>(there are config modifiers <code>--global<\/code>, <code>--local<\/code>, <code>--system<\/code> to set this setting for everyone at once, or just for one project, etc)<\/p>\n<p>Check the details instruction <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization#configuring-dvc-cache-file-link-type\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<hr \/>\n<p>Overall, it's a great setup, and looks like you got pretty much everything right. Please, don't hesitate to follow up and\/or create other questions here- we'll help you with this.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1604362563343,
        "Solution_link_count":3.0,
        "Solution_readability":10.6,
        "Solution_reading_time":18.98,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":165.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1094444444,
        "Challenge_answer_count":0,
        "Challenge_body":"Example job: job-7acb5d09-e580-46a2-aa11-03ce72ddc0f0\r\n\r\nAt the end of the job run, we upload the artifact, where `set-output` happens, and terminate the job.\r\nHowever, we have:\r\n```\r\n...\r\nINFO:wabucketref.api:Uploading artifact from '\/tmp\/tmpqkuqrluh' to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1 ...\r\nINFO:wabucketref.api:Artifact uploaded to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\r\nINFO:botocore.credentials:Found credentials in shared credentials file: \/var\/secrets\/aws\/credentials-pca-pipeline\r\nwandb: Generating checksum for up to 100000 objects with prefix \"dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\"... Done. 0.0s\r\n::set-output name=artifact_name::texture-maps\r\n::set-output name=artifact_type::dataset\r\nwandb: Waiting for W&B process to finish, PID 75\r\n...\r\n```\r\n\r\nWhile it should be:\r\n```\r\nINFO:wabucketref.api:Uploading artifact from '\/tmp\/tmpqkuqrluh' to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1 ...\r\nINFO:wabucketref.api:Artifact uploaded to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\r\nINFO:botocore.credentials:Found credentials in shared credentials file: \/var\/secrets\/aws\/credentials-pca-pipeline\r\nwandb: Generating checksum for up to 100000 objects with prefix \"dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\"... Done. 0.0s\r\n::set-output name=artifact_name::texture-maps\r\n::set-output name=artifact_type::dataset\r\n::set-output name=artifact_alias::8154311a-ab2e-45cd-adeb-f7e5270122c1\r\nwandb: Waiting for W&B process to finish, PID 75\r\nwandb: Program ended successfully.\r\nwandb:                                                                                \r\n```\r\n\r\nOne line was overwritten by the `wandb: Waiting for W&B process to finish, PID 75`, which, apparently is running in a separate process (`wandb.Settings(start_method=\"fork\")`). ",
        "Challenge_closed_time":1625736868000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625736474000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The issue is that the output of WandB overwrites the output of wabucketref during artifact upload, causing one line to be missing in the output.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/neuro-inc\/mlops-wandb-bucket-ref\/issues\/16",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":15.0,
        "Challenge_reading_time":25.31,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":1.0,
        "Challenge_repo_issue_count":119.0,
        "Challenge_repo_star_count":0.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":0.1094444444,
        "Challenge_title":"WandB output overwrites wabucketref's output in case of artifact upload",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":154,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1508797229168,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":1115.0,
        "Answerer_view_count":94.0,
        "Challenge_adjusted_solved_time":102.7725527778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to deploy a stacked model to Azure Machine Learning Service. The architecture of the solution consists of three models and one meta-model.\nData is a time-series data. <\/p>\n\n<p>I'd like the model to automatically re-train based on some schedule. I'd also like to re-tune hyperparameters during each re-training. <\/p>\n\n<p>AML Service offers <code>HyperDriveStep<\/code> class that can be used in the pipeline for automatic hyperparameter optimization. <\/p>\n\n<p>Is it possible - and if so, how to do it - to use <code>HyperDriveStep<\/code> with time-series CV?<\/p>\n\n<p>I checked the documentation, but haven't found a satisfying answer.<\/p>",
        "Challenge_closed_time":1565728003107,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565358021917,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to deploy a stacked model to Azure Machine Learning Service consisting of three models and one meta-model. The data is time-series and the user wants the model to automatically re-train and re-tune hyperparameters during each re-training. The user is wondering if it is possible to use HyperDriveStep with time-series cross-validation and is seeking guidance on how to do it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57431340",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":8.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":102.7725527778,
        "Challenge_title":"Is it possible to use HyperDriveStep with time-series cross-validation?",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":189.0,
        "Challenge_word_count":102,
        "Platform":"Stack Overflow",
        "Poster_created_time":1490275561927,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tel Aviv",
        "Poster_reputation_count":83.0,
        "Poster_view_count":56.0,
        "Solution_body":"<p>AzureML HyperDrive is a black box optimizer, meaning that it will just run your code with different parameter combinations based on the configuration you chose. At the same time, it supports Random and Bayesian sampling and has different policies for early stopping (see here for relevant <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-tune-hyperparameters\" rel=\"nofollow noreferrer\">docs<\/a> and here for an <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training-with-deep-learning\/train-hyperparameter-tune-deploy-with-tensorflow\/train-hyperparameter-tune-deploy-with-tensorflow.ipynb\" rel=\"nofollow noreferrer\">example<\/a> -- HyperDrive is towards the end of the notebook).<\/p>\n\n<p>The only thing that your model\/script\/training needs to adhere to is to be launched from a script that takes <code>--param<\/code> style parameters. As long as that holds you could optimize the parameters for each of your models individually and then tune the meta-model, or you could tune them all in one run. It will mainly depend on the size of the parameter space and the amount of compute you want to use (or pay for).<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.8,
        "Solution_reading_time":15.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":141.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1555475748808,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pennsylvania, USA",
        "Answerer_reputation_count":351.0,
        "Answerer_view_count":57.0,
        "Challenge_adjusted_solved_time":2227.3103758333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to save a file to S3 bucket from sagemaker instance. and below line throws an error!<\/p>\n<pre><code>df.to_csv(&quot;s3:\/\/informatri\/Drug_Data_Cleaned.csv&quot;), index = False)\n<\/code><\/pre>\n<pre><code>error - \nTypeErrorTraceback (most recent call last)\n&lt;ipython-input-28-d33896172c11&gt; in &lt;module&gt;()\n      1 \n----&gt; 2 a.to_csv(&quot;s3:\/\/informatri\/{}&quot;.format('Drug_Data_Cleaned.csv'), index = False)\n\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p27\/lib\/python2.7\/site-packages\/pandas\/core\/generic.pyc in to_csv(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\n   3018                                  doublequote=doublequote,\n   3019                                  escapechar=escapechar, decimal=decimal)\n-&gt; 3020         formatter.save()\n   3021 \n   3022         if path_or_buf is None:\n\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p27\/lib\/python2.7\/site-packages\/pandas\/io\/formats\/csvs.pyc in save(self)\n    170                 self.writer = UnicodeWriter(f, **writer_kwargs)\n    171 \n--&gt; 172             self._save()\n    173 \n    174         finally:\n\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p27\/lib\/python2.7\/site-packages\/pandas\/io\/formats\/csvs.pyc in _save(self)\n    272     def _save(self):\n    273 \n--&gt; 274         self._save_header()\n    275 \n    276         nrows = len(self.data_index)\n\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p27\/lib\/python2.7\/site-packages\/pandas\/io\/formats\/csvs.pyc in _save_header(self)\n    240         if not has_mi_columns or has_aliases:\n    241             encoded_labels += list(write_cols)\n--&gt; 242             writer.writerow(encoded_labels)\n    243         else:\n    244             # write out the mi\n\nTypeError: write() argument 1 must be unicode, not str\n<\/code><\/pre>\n<p>I tried the following:<\/p>\n<pre><code>df.to_csv(&quot;s3:\/\/informatri\/Drug_Data_Cleaned.csv&quot;), index = False, encoding = 'utf-8', sep = '\\t')\n<\/code><\/pre>\n<p>I still get the same error. If I do only:<\/p>\n<pre><code>df.to_csv(&quot;Drug_Data_Cleaned.csv&quot;), index = False) \n<\/code><\/pre>\n<p>It gets saved locally all fine. So not a problem with dataframe or the name etc. It has to do something with saving to S3 bucket.\nI have used similar ways to save to s3 bucket many times in the past and it has worked perfectly fine. Hence, I was wondering why the error?<\/p>",
        "Challenge_closed_time":1612607749436,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604589432083,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to save a file to an S3 bucket from a Sagemaker instance using the df.to_csv() function. The error message states that the write() argument 1 must be unicode, not str. The user has tried using encoding and separator parameters but still gets the same error. However, saving the file locally works fine, indicating that the issue is with saving to the S3 bucket. The user has used similar methods to save to S3 in the past without any issues.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64700093",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":30.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":2227.3103758333,
        "Challenge_title":"AWS Sagemaker - df.to_csv error write() argument 1 must be unicode, not str",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":275.0,
        "Challenge_word_count":226,
        "Platform":"Stack Overflow",
        "Poster_created_time":1555475748808,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pennsylvania, USA",
        "Poster_reputation_count":351.0,
        "Poster_view_count":57.0,
        "Solution_body":"<p>I fixed this problem.<\/p>\n<p>The error was that the Sagemaker ipynb notebook was opened in conda_python2.7 or so. Just re-wrote the script in conda_python3 and then everything worked fine :)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.7,
        "Solution_reading_time":2.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":30.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":27.7321336111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Good morning,     <br \/>\nI am on a text classification project and I test the text classification services on Microsoft Aeure.     <br \/>\nI would like to know the difference between Microsoft Azure Machine Learning Studio Data Labeling and Text Classifition de cognitive service (Langage Studio)?     <br \/>\nDoes one service perform better than the other?     <\/p>\n<p>Thank you in advance for your help     <\/p>\n<p>Cordially     <br \/>\nLysa <\/p>",
        "Challenge_closed_time":1671545106008,
        "Challenge_comment_count":0,
        "Challenge_created_time":1671445270327,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking clarification on the difference between Microsoft Azure Machine Learning Studio Data Labeling and Text Classification cognitive service (Language Studio) for a text classification project and whether one service performs better than the other.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1134081\/what-is-the-difference-between-data-labeling-text",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":6.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":27.7321336111,
        "Challenge_title":"What is the difference between Data Labeling Text Azure Machine Learning Studio and cognitive service Text Classifition (Langage Studio) ?",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":85,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=2d96c782-8477-4987-b5a4-5ea497b49eb9\">@AMROUN Lysa  <\/a> Thanks for the question. In Azure ML you will be able to train and customize text classification. In Language studio base line model provided to do the classification.    <\/p>\n<p>With Custom text classification, you can build custom AI models to classify text into pre-defined custom classes. By creating a custom text classification project, you can iteratively label data, train, evaluate, and improve model performance before deploying your model and making it available for consumption.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.7,
        "Solution_reading_time":7.35,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":78.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1313736279736,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA",
        "Answerer_reputation_count":207794.0,
        "Answerer_view_count":16864.0,
        "Challenge_adjusted_solved_time":4.9399880556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Trying to figure out whether this behaviour on IPython (v7.12.0, on Amazon SageMaker) is a bug or I'm missing some proper way \/ documented constraint...<\/p>\n<p>Say I have some Python variables like:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>NODE_VER = &quot;v16.14.2&quot;\nNODE_DISTRO = &quot;linux-x64&quot;\n<\/code><\/pre>\n<p>These commands both work fine in a notebook:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>!echo $PATH\n# Shows **contents of system path**\n!echo \/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:\n# Shows \/usr\/local\/lib\/nodejs\/node-v16.14.2-linux-x64\/bin\n<\/code><\/pre>\n<p>...But this does not:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>!echo \/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:$PATH\n# Shows:\n# \/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:**contents of system path**\n<\/code><\/pre>\n<p>I've tried a couple of combinations of e.g. using <code>$NODE_VER<\/code> syntax instead (which produces <code>node--\/<\/code> instead of <code>node-{NODE_VER}-{NODE_DISTRO}\/<\/code>, but seems like any combination using both shell variables (PATH) and Python variables (NODE_VER\/NODE_DISTRO) fails.<\/p>\n<p>Can anybody help me understand why and how to work around it?<\/p>\n<p>My end goal, as you might have guessed already, is to actually add this folder to the PATH rather than just echoing it - something like:<\/p>\n<pre><code>!export PATH=\/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:$PATH\n<\/code><\/pre>",
        "Challenge_closed_time":1648184137556,
        "Challenge_comment_count":1,
        "Challenge_created_time":1648174324543,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while trying to mix shell variables and Python variables in IPython's '!command'. While the commands using only shell variables or only Python variables work fine, the combination of both fails. The user is seeking help to understand why this is happening and how to work around it to add a folder to the PATH.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71611419",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.2,
        "Challenge_reading_time":20.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":2.7258369444,
        "Challenge_title":"Mixing shell variables and python variables in IPython '!command'",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":160.0,
        "Challenge_word_count":155,
        "Platform":"Stack Overflow",
        "Poster_created_time":1587281590603,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":473.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p><a href=\"https:\/\/stackoverflow.com\/questions\/69194172\/how-to-reference-both-a-python-and-environment-variable-in-jupyter-bash-magic\">How to reference both a python and environment variable in jupyter bash magic?<\/a><\/p>\n<p>Try<\/p>\n<pre><code>!echo \/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:$$PATH\n<\/code><\/pre>\n<p><code>$$PATH<\/code> forces it to use the system variable rather than try to find a Python\/local one.<\/p>\n<p>Various examples:<\/p>\n<pre><code>In [130]: foo = 'foo*.txt'\nIn [131]: HOME = 'myvar'\nIn [132]: !echo $foo\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt\nIn [133]: !echo $foo $HOME\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt myvar\nIn [134]: !echo $foo $$HOME\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt \/home\/paul\nIn [135]: !echo $foo $PWD\n\/home\/paul\/mypy\nIn [136]: !echo $foo $$PWD\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt \/home\/paul\/mypy\nIn [137]: !echo {foo} $PWD\n{foo} \/home\/paul\/mypy\nIn [138]: !echo {foo} $$PWD\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt \/home\/paul\/mypy\n<\/code><\/pre>\n<p>Any variable not locally defined forces the behavior you see:<\/p>\n<pre><code>In [139]: !echo $abc\n\nIn [140]: !echo {foo} $abc\n{foo}\n<\/code><\/pre>\n<p>It may put the substitution in a <code>try\/except<\/code> block, and &quot;give up&quot; if there's any <code>NameError<\/code>.<\/p>\n<p>This substitution can occur in most of the magics, not just <code>!<\/code>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1648192108500,
        "Solution_link_count":1.0,
        "Solution_readability":7.6,
        "Solution_reading_time":18.76,
        "Solution_score_count":3.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":160.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":16.7097122222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm in classic Azure ML mode. I am working on my first ever experiment, so please be patient..    <\/p>\n<p>I cannot locate column selector for CSV data to filter out columns. I found this:    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/select-columns-in-dataset\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/select-columns-in-dataset<\/a>    <\/p>\n<p>And I'm following a tutorial (behind pay wall, from 2017) that shows it in the right hand side properties pane. It says in his example to add the &quot;Select columns in dataset&quot; and it shows the option of &quot;launch column selector&quot;.    <\/p>\n<p>I have browsed through every single choice in the left menu, but cannot locate it... I have no idea what I am missing.    <\/p>\n<p>I need to exclude columns from the data set. Then later I need to make some of the fields &quot;categorical&quot;. Input on that would be appreciated too, unless it becomes obvious from other information provided.    <\/p>\n<p>Please help me :) Thanks in advance for patience and\/or assistance.<\/p>",
        "Challenge_closed_time":1619488667407,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619428512443,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is a beginner in classic Azure ML mode and is unable to locate the column selector for CSV data to filter out columns. They have followed a tutorial that shows the option of \"launch column selector\" in the right-hand side properties pane, but they cannot find it. They need to exclude columns from the data set and make some fields \"categorical\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/371634\/beginner-question-cannot-locate-column-selector-fo",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":15.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":16.7097122222,
        "Challenge_title":"Beginner question - Cannot locate column selector for CSV data to filter out columns",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":162,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello,    <\/p>\n<p>First you need to navigate to Data Transformation  - &gt; Manipulation -&gt; Select columns in dataset, drag that into your process.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/91523-image.png?platform=QnA\" alt=\"91523-image.png\" \/>    <\/p>\n<p>Then, left click on the module and click launch column selector.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/91497-image.png?platform=QnA\" alt=\"91497-image.png\" \/>    <\/p>\n<p>And you can do you want now.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/91524-image.png?platform=QnA\" alt=\"91524-image.png\" \/>    <\/p>\n<p>Please accept the answer if you feel helpful, thanks.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.7,
        "Solution_reading_time":9.81,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":69.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2627777778,
        "Challenge_answer_count":1,
        "Challenge_body":"Is it possible to deploy an existing model artifact from SageMaker to Redshift ML? \n\nFor example, with an **Aurora ML** you can reference a SageMaker endpoint and then use it as a UDF in a `SELECT` statement. \n**Redshift ML** works a bit differently - when you call `CREATE MODEL` - the model is trained with **SageMaker Autopilot** and then deployed to the **Redshift Cluster**. \n\nWhat if I already have a trained model, can i deploy it to a Redshift Cluster and then use a UDF for Inference?",
        "Challenge_closed_time":1609955532000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609954586000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is asking if it is possible to deploy an existing model artifact from SageMaker to Redshift ML and use it as a UDF for inference. Redshift ML works differently from Aurora ML, as the model is trained with SageMaker Autopilot and then deployed to the Redshift Cluster when `CREATE MODEL` is called. The user wants to know if they can deploy a pre-trained model to Redshift Cluster and use it as a UDF for inference.",
        "Challenge_last_edit_time":1668536452452,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUCMYCx28qRe-MOCIfj91Y2g\/redshift-ml-sagemaker-deploy-an-existing-model-artifact-to-a-redshift-cluster",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":6.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.2627777778,
        "Challenge_title":"Redshift ML \/ SageMaker - Deploy an existing model artifact to a Redshift Cluster",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":148.0,
        "Challenge_word_count":96,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"As of January 30 2021, you can't deploy an existing model artifact from SageMaker to Redshift ML directly with currently announced Redshift ML preview features. But you can reference  sagemaker endpoint through a lambda function and use that lambda function as an user defined function in Redshift.\n\nBelow would be the steps:\n\n1. Train and deploy your SageMaker model in a SageMaker Endpoint. \n2. Use Lambda function to [reference sagemaker endpoint](https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/). \n3. Create a [Redshift Lambda UDF](https:\/\/aws.amazon.com\/blogs\/big-data\/accessing-external-components-using-amazon-redshift-lambda-udfs\/) referring above lambda function to run predictions.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1612026491936,
        "Solution_link_count":2.0,
        "Solution_readability":15.1,
        "Solution_reading_time":10.02,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":84.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1124.7483333333,
        "Challenge_answer_count":0,
        "Challenge_body":"The artifact folder by default is not reemplacing the `$ARTIFACTS_BUCKET` env var",
        "Challenge_closed_time":1623230636000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1619181542000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a \"PermissionError\" while trying to log models to mlflow on their Mac. The error occurred when the program attempted to create a directory \"\/var\/lib\/mlflow\" and was denied permission. The user is running mlflow version 1.2 on macOS 12.1.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/380",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":1.51,
        "Challenge_repo_contributor_count":16.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":909.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":1124.7483333333,
        "Challenge_title":"Bad MLflow artifact folder by default",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":17,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This problem has been solved adding the variable of `$ARTIFACTS_BUCKET` between `()` like this `$(ARTIFACTS_BUCKET)` in the deployment.yaml of the project-operator.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.9,
        "Solution_reading_time":2.12,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":20.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":17.6541272222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>AzureML's Python Script module requires to return a Pandas DataFrame. I want to return only a value and I do this:<\/p>\n\n<pre><code>result=7\ndataframe1=pd.DataFrame(numpy.zeros(1))\ndataframe1[0][0]=result\n<\/code><\/pre>\n\n<p>by which I am able to return just a single value in Azure ML's Python Script module. <\/p>\n\n<p><strong>What is a proper way to create a pandas DataFrame with a single value?<\/strong><\/p>",
        "Challenge_closed_time":1496892281608,
        "Challenge_comment_count":4,
        "Challenge_created_time":1496828726750,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in creating a Pandas DataFrame with only a single value to be returned in AzureML's Python Script module. They have tried a workaround by assigning the value to a DataFrame, but they are seeking a proper way to create a DataFrame with a single value.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44409066",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":6.0,
        "Challenge_reading_time":5.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":17.6541272222,
        "Challenge_title":"Python Pandas DataFrame with only a single number stored?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1701.0,
        "Challenge_word_count":65,
        "Platform":"Stack Overflow",
        "Poster_created_time":1251372839052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":48616.0,
        "Poster_view_count":3348.0,
        "Solution_body":"<p>Following code should work:<\/p>\n\n<pre><code>import pandas as pd\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    result = pd.DataFrame({'mycol': [123]})\n    return result,\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":2.48,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1601920258310,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":123.8963886111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to create a Time series dataset from a folder that contains parquet files this way:<\/p>\n<ul>\n<li>timestamp=2018-01-06<\/li>\n<li>timestamp=2018-01-07<\/li>\n<\/ul>\n<p>How can I make Azure Dataset, through the GUI, recognises the timestamp partition as a date and mark my dataset as a time series dataset?<\/p>\n<p>It is supposed to be automatic, but it doesn't work.<\/p>",
        "Challenge_closed_time":1601920685456,
        "Challenge_comment_count":1,
        "Challenge_created_time":1601474658457,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a time series dataset from a folder containing parquet files with date partitions, but is having trouble getting Azure Dataset to recognize the timestamp partition as a date and mark the dataset as a time series dataset. The user is seeking guidance on how to do this through the GUI.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64139290",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.8,
        "Challenge_reading_time":6.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":123.8963886111,
        "Challenge_title":"How can I mark an Azure Dataset as a time series dataset reading from a parquet folder with date partitions?",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":109.0,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423640080283,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lyon, France",
        "Poster_reputation_count":457.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>Thanks for reaching out to us.<\/p>\n<p>In Azure Machine Learning Studio, you would need to setup partition format similar to python SDK, as follows, assuming your data path is &quot;timeseries\/timestamp=2020-01-01\/data.parquet&quot;:\n<a href=\"https:\/\/i.stack.imgur.com\/HwYfF.png\" rel=\"nofollow noreferrer\">Set up partition format when creating time series dataset<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.1,
        "Solution_reading_time":4.88,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":42.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1488575811772,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bothell, WA, United States",
        "Answerer_reputation_count":51.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":303.7152472222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a model in AzureML that scores incoming values from a csv.<\/p>\n\n<p>The flow is ...->(Score Model using one-class SVM)->(Normalize Data)->(Convert to CSV)->(Convert to Dataset)->(Web Service Output)<\/p>\n\n<p>When the experiment is run I can download the csv from the (Convert to CSV) module output and it will contain Scored Probabilities column.<\/p>\n\n<p>But when I'm using a streaming job I don't know how to access the Scored Probabilities column using Query SQL. How do I do it?<\/p>",
        "Challenge_closed_time":1488576068267,
        "Challenge_comment_count":0,
        "Challenge_created_time":1487482693377,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in accessing the Scored Probabilities column using Query SQL while using a streaming job in AzureML to score incoming values from a CSV. The user is seeking guidance on how to access the Scored Probabilities column.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42324035",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":6.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":303.7152472222,
        "Challenge_title":"How to select Scored Probabilities from azure prediction model",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":576.0,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Poster_created_time":1300047702248,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":586.0,
        "Poster_view_count":108.0,
        "Solution_body":"<p>You can access the response using the amlresult.[Scored Probabilities] notation, where amlresult is an alias for the return value from your AzureML call.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.0,
        "Solution_reading_time":2.03,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":23.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":162.0608333333,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n\r\n<!-- Please paste your BoringModel colab link here. -->\r\n\r\n### To Reproduce\r\n\r\nLog anything  parameters longer than 250 characters\r\n\r\n\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\n\r\nMlflowLogger not sending parameters longer than 250 characters to mlflow and log warning to user\r\n\r\n### Environment\r\n\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux): \r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA\/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\nMlflow only allow paramters to be at most 500 bytes (250 unicode characters), their limit in database is 250 characters:\r\nhttps:\/\/www.mlflow.org\/docs\/latest\/rest-api.html#log-param\r\nhttps:\/\/github.com\/mlflow\/mlflow\/issues\/1976\r\nhttps:\/\/github.com\/mlflow\/mlflow\/issues\/3931\r\n\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Challenge_closed_time":1613510526000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1612927107000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The MLFlow logger is causing latency and slowing down the training loop, even when logging metrics only per epoch. The issue seems to be caused by the logger communicating with the MLFlow server on each training step. The expected behavior is to avoid the logger from communicating with the server in each training loop. The solution involves modifying the codebase to avoid calling the MLFlow client on each step and storing the experiment within the logger.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/5892",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":14.59,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":162.0608333333,
        "Challenge_title":"MlflowLogger fail when logging long parameters",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":144,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Dear @ducthienbui97,\n\nThanks for opening a PR.\n\nBest,\nT.C",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.3,
        "Solution_reading_time":0.69,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":9.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":0.3772533333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In Azure ML studio, how to import images dataset, for image recognition algorithms. As zip file?<\/p>",
        "Challenge_closed_time":1460058230132,
        "Challenge_comment_count":0,
        "Challenge_created_time":1460056088943,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to import a dataset of images in Azure ML Studio for use in image recognition algorithms, specifically as a zip file.",
        "Challenge_last_edit_time":1460056872020,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36485084",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.1,
        "Challenge_reading_time":1.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.5947747222,
        "Challenge_title":"Azure ML Studio: How to import images dataset?",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2420.0,
        "Challenge_word_count":23,
        "Platform":"Stack Overflow",
        "Poster_created_time":1282073536110,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":4529.0,
        "Poster_view_count":1142.0,
        "Solution_body":"<p>You can use \"<strong>import images<\/strong>\" module in Azure ML Studio that can read images from Azure blob storage - <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Face-detection-2\" rel=\"nofollow\">here<\/a> is the sample experiment <\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.1,
        "Solution_reading_time":3.32,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":26.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.1856155556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I created a notebook in the workspace and when I sent the experiment for training I received error message <strong>undefined symbol: XGDMatrixSetDenseInfo<\/strong> for algorithm <strong>Xgboost<\/strong>. Do you know how to fix the problem?<\/p>\n<p><strong>Azure ML Version:<\/strong> 1.22.0  <br \/>\n<strong>Compute Instance:<\/strong> Standard_DS3_v2<\/p>\n<ul>\n<li> Code:  import logging  <br \/>\n  from azureml.train.automl import AutoMLConfig  <br \/>\n  from azureml.core.experiment import Experiment  automl_settings = {  <br \/>\n  &quot;iteration_timeout_minutes&quot;: 10,  <br \/>\n  &quot;experiment_timeout_hours&quot;: 0.3,  <br \/>\n  &quot;enable_early_stopping&quot;: True,  <br \/>\n  &quot;primary_metric&quot;: 'normalized_root_mean_squared_error',  <br \/>\n  &quot;featurization&quot;: 'auto',  <br \/>\n  &quot;verbosity&quot;: logging.INFO,  <br \/>\n  &quot;n_cross_validations&quot;: 5  <br \/>\n  }  automl_config = AutoMLConfig(task='regression',  <br \/>\n  debug_log='automated_ml_errors.log',  <br \/>\n  training_data=x_train,  <br \/>\n  label_column_name=&quot;production_time&quot;,  <br \/>\n  **automl_settings)  experiment = Experiment(ws, &quot;train-model&quot;)  <br \/>\n  local_run = experiment.submit(automl_config, show_output=True)<\/li>\n<li> Full Error Message:  ERROR: FitException:  <br \/>\n  Message: \/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo  <br \/>\n  InnerException: AttributeError: \/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo  <br \/>\n  ErrorResponse  <br \/>\n  {  <br \/>\n  &quot;error&quot;: {  <br \/>\n  &quot;code&quot;: &quot;SystemError&quot;,  <br \/>\n  &quot;message&quot;: &quot;Encountered an internal AutoML error. Error Message\/Code: FitException. Additional Info: FitException:\\n\\tMessage: \/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo\\n\\tInnerException: None\\n\\tErrorResponse \\n{\\n \\&quot;error\\&quot;: {\\n \\&quot;message\\&quot;: \\&quot;\/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo\\&quot;,\\n \\&quot;target\\&quot;: \\&quot;Xgboost\\&quot;,\\n \\&quot;reference_code\\&quot;: \\&quot;Xgboost\\&quot;\\n }\\n}&quot;,  <br \/>\n  &quot;details_uri&quot;: &quot;https:\/\/learn.microsoft.com\/azure\/machine-learning\/resource-known-issues#automated-machine-learning&quot;,  <br \/>\n  &quot;target&quot;: &quot;Xgboost&quot;,  <br \/>\n  &quot;inner_error&quot;: {  <br \/>\n  &quot;code&quot;: &quot;ClientError&quot;,  <br \/>\n  &quot;inner_error&quot;: {  <br \/>\n  &quot;code&quot;: &quot;AutoMLInternal&quot;  <br \/>\n  }  <br \/>\n  },  <br \/>\n  &quot;reference_code&quot;: &quot;Xgboost&quot;  <br \/>\n  }  <br \/>\n  }<\/li>\n<\/ul>\n<p>Best regards,  <br \/>\nCristina<\/p>\n<p><a href=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/79658-packages.txt?platform=QnA\">79658-packages.txt<\/a><\/p>",
        "Challenge_closed_time":1616196599996,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616170731780,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while running an experiment in Azure Machine Learning Services AutoML. The error message stated \"undefined symbol: XGDMatrixSetDenseInfo\" for the Xgboost algorithm. The user is seeking help to fix the problem. The Azure ML version used was 1.22.0 and the compute instance was Standard_DS3_v2. The error message and code used in the experiment are provided in the post.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/322886\/azure-machine-learning-services-automl-error-runni",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":23.9,
        "Challenge_reading_time":39.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":7.1856155556,
        "Challenge_title":"Azure Machine Learning Services - AutoMl - Error running experiment.submit: \"\/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo\"",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":197,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, can you try uninstalling and reinstalling Xgboost (try versions &lt;= 0.90 if you continue to get errors).<\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":1.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1392607100776,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sydney NSW, Australia",
        "Answerer_reputation_count":133.0,
        "Answerer_view_count":29.0,
        "Challenge_adjusted_solved_time":1338.1595822222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We have images of single size 400 MB to 800 MB.<\/p>\n<p>Not sure if SageMaker GroundTruth can handle it.<\/p>",
        "Challenge_closed_time":1605061301483,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600243584947,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unsure if SageMaker GroundTruth can handle images of size 400 MB to 800 MB and is seeking information on the largest allowed image size for labelling jobs.",
        "Challenge_last_edit_time":1600243926987,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63915711",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.7,
        "Challenge_reading_time":2.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1338.2545933334,
        "Challenge_title":"What's the largest allowed image size for SageMaker GroundTruth Labelling Job?",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":110.0,
        "Challenge_word_count":29,
        "Platform":"Stack Overflow",
        "Poster_created_time":1392607100776,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sydney NSW, Australia",
        "Poster_reputation_count":133.0,
        "Poster_view_count":29.0,
        "Solution_body":"<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/input-data-limits.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/input-data-limits.html<\/a><\/p>\n<p>In case someone is also looking for the answer of this one.<\/p>\n<p>In short:<\/p>\n<blockquote>\n<p>40 MB<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.5,
        "Solution_reading_time":4.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":22.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":23.538575,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Azure fails to connect with the Dataset citing 403 inspite of SAS token  <br \/>\nThis appears when we try to load the data as a pandas dataframe . dataset = Dataset.get_by_name() works<\/p>\n<p>Error message:<\/p>\n<p>{  <br \/>\n&quot;error&quot;: {  <br \/>\n&quot;code&quot;: &quot;UserError&quot;,  <br \/>\n&quot;message&quot;: &quot;Execution failed in operation 'to_pandas_dataframe' for Dataset(id='data id', name='dataset name', error_code=ScriptExecution.StreamAccess.Authentication,error_message=ScriptExecutionException was caused by StreamAccessException.\\r\\n StreamAccessException was caused by AuthenticationException.\\r\\n Authentication failed for 'AzureBlob GetReference' operation at '[REDACTED]' with '403: AuthenticationFailed'. Please make sure the SAS token or the account key is correct.\\r\\n Failed due to inner exception of type: StorageException\\r\\n| session_id=session_id) ErrorCode: ScriptExecution.StreamAccess.Authentication&quot;  <br \/>\n}  <br \/>\n}<\/p>",
        "Challenge_closed_time":1638278963500,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638194224630,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a ScriptExecution.StreamAccess.Authentication error while trying to load data as a pandas dataframe using dataset.to_pandas_dataframe() in Azure. The error message indicates that the authentication failed for 'AzureBlob GetReference' operation with '403: AuthenticationFailed', despite providing the correct SAS token.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/644562\/dataset-to-pandas-dataframe()-throws-a-scriptexecu",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":17.1,
        "Challenge_reading_time":13.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":23.538575,
        "Challenge_title":"dataset.to_pandas_dataframe() throws a ScriptExecution.StreamAccess.Authentication error",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":101,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The problem was solved by updating the account keys in the workspace.  <br \/>\naz ml workspace sync-keys -w mlw-kundenscore -g rg-datascience<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.2,
        "Solution_reading_time":1.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":21.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":1.5416555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have around 10000 images in my S3 bucket. I need to cut each of these images to 12 smaller images and save them in another folder in the S3 bucket. I want to do this through the AWS Sagemaker. I am not able to read the image from the S3 bucket from my Sagemaker Jupter notebook. I have the code for cutting the images. <\/p>\n\n<p>Need help in reading images and storing them back into S3 from Sagemaker.Is it possible to do this, and also efficiently?<\/p>",
        "Challenge_closed_time":1559476844132,
        "Challenge_comment_count":2,
        "Challenge_created_time":1559410433010,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user needs to cut 10000 images in their S3 bucket into 12 smaller images and save them in another folder in the same bucket using AWS Sagemaker. However, they are facing challenges in reading the images from S3 in their Sagemaker Jupyter notebook and require assistance in efficiently storing them back into S3.",
        "Challenge_last_edit_time":1559471294172,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56408976",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":5.6,
        "Challenge_reading_time":6.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":18.4475338889,
        "Challenge_title":"How to read AWS S3 images from Sagemaker for processing",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":641.0,
        "Challenge_word_count":96,
        "Platform":"Stack Overflow",
        "Poster_created_time":1495175078600,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Coimbatore, Tamil Nadu, India",
        "Poster_reputation_count":126.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>You can bring images to a local repo of your SageMaker instance (eg \/home\/ec2-user\/SageMaker\/Pics\/ with the following command:<\/p>\n\n<pre><code>aws s3 sync s3:\/\/pic_folder_in_s3 \/home\/ec2-user\/SageMaker\/Pics\n<\/code><\/pre>\n\n<p>or in python:<\/p>\n\n<pre><code>import subprocess as sb\n\nsb.call('aws s3 sync s3:\/\/pic_folder_in_s3 \/home\/ec2-user\/SageMaker\/Pics'.split())\n<\/code><\/pre>\n\n<p>Note that in order for the transfer to happen, the role carried by your SageMaker instance must have the right to read from this S3 location<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.8,
        "Solution_reading_time":6.77,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":63.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":85.1185105556,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi! During training, my script crashed unexpectedly and did not save the latest epoch information.  I restarted training without being aware of it, and now my epochs are offset by a large number.<\/p>\n<p>Is it possible to edit the epoch number (index) and add a certain value to each entry? I have tried opening the \u201crun_name.wandb\u201d file and I can already see the \u2018_step\u2019 variable for each entry, but I was wondering if there is a cleaner way to perform such an update.<\/p>\n<p>Thank you in advance for your help!<\/p>",
        "Challenge_closed_time":1659047787071,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658741360433,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user's script crashed during training and did not save the latest epoch information, resulting in a large offset in epoch numbers. The user is seeking a way to edit the epoch number and add a certain value to each entry, and is wondering if there is a cleaner way to perform such an update.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/update-offline-run-before-syncing\/2794",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":6.2,
        "Challenge_reading_time":6.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":85.1185105556,
        "Challenge_title":"Update offline run before syncing",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":149.0,
        "Challenge_word_count":94,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/vandrew\">@vandrew<\/a> , I understand what you are attempting to achieve now. At this time our API doesn\u2019t support offline mode to access local log files. We do have this planned as a future feature but I can\u2019t speak to a specific timeline. At this time you will have to sync your runs first in online mode, then update metrics using the API.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":4.63,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":64.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":146.1394444444,
        "Challenge_answer_count":0,
        "Challenge_body":"Users are reporting issues with mlflow 1.19\r\n\r\nCreating an issue here to track. Details will be added as we investigate further",
        "Challenge_closed_time":1634667593000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634141491000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered an issue where the default value for projectOperator.mlflow.image.tag is set to \"latest\" instead of \"v0.13.2\" on chart release v0.13.2. The user is advised to check the values.yml file to ensure that the correct tag is set.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/338",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":5.7,
        "Challenge_reading_time":2.09,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":86.0,
        "Challenge_repo_issue_count":1012.0,
        "Challenge_repo_star_count":1924.0,
        "Challenge_repo_watch_count":28.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":146.1394444444,
        "Challenge_title":"[Bug\/Feature Request] Support mlflow 1.19",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":25,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1373651649052,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"M\u00fcnchen, Deutschland",
        "Answerer_reputation_count":1066.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":1490.0125933333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to create a dataset in Azure ML where the data source are multiple files (eg images) in a Blob Storage. How do you do that correctly?<\/p>\n<h3>Here is the error I get following the documented approach in the UI<\/h3>\n<p>When I create the dataset in the UI and select the blob storage and directory with either just <code>dirname<\/code> or <code>dirname\/**<\/code> then the files can not be found in the explorer tab with the error <code>ScriptExecution.StreamAccess.NotFound: The provided path is not valid or the files could not be accessed.<\/code> When I try to download the data with the code snippet in the consume tab then I get the error:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace, Dataset\n\n# set variables \n\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\ndataset = Dataset.get_by_name(workspace, name='teststar')\ndataset.download(target_path='.', overwrite=False)\n<\/code><\/pre>\n<pre><code>Error Message: ScriptExecutionException was caused by StreamAccessException.\n  StreamAccessException was caused by NotFoundException.\n    Found no resources for the input provided: 'https:\/\/mystoragename.blob.core.windows.net\/data\/testdata\/**'\n\n<\/code><\/pre>\n<p>When I just select one of the files instead of <code>dirname<\/code> or <code>dirname\/**<\/code> then everything works. Does AzureML actually support Datasets consisting of multiple files?<\/p>\n<h3>Here is my setup:<\/h3>\n<p>I have a Data Storage with one container <code>data<\/code>. In there is a directory <code>testdata<\/code> containing <code>testfile1.txt<\/code> and <code>testfile2.txt<\/code>.<\/p>\n<p>In AzureML I created a datastore <code>testdatastore<\/code> and there I select the <code>data<\/code> container in my data storage.<\/p>\n<p>Then in Azure ML I create a Dataset from datastore, select file dataset and the datastore above. Then I can browse the files, select a folder and select that files in subdirectories should be included. This then creates the path <code>testdata\/**<\/code> which does not work as described above.<\/p>\n<p>I got the same issue when creating the dataset and datastore in python:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import azureml.core\nfrom azureml.core import Workspace, Datastore, Dataset\n\nws = Workspace.from_config()\n\ndatastore = Datastore(ws, &quot;mydatastore&quot;)\n\ndatastore_paths = [(datastore, 'testdata')]\ntest_ds = Dataset.File.from_files(path=datastore_paths)\ntest_ds.register(ws, &quot;testpython&quot;)\n<\/code><\/pre>",
        "Challenge_closed_time":1618851944876,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613487159093,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a dataset in Azure ML using multiple files from a Blob Storage, but encounters an error message stating that the provided path is not valid or the files could not be accessed. The user has tried selecting the directory with either just dirname or dirname\/**, but the files cannot be found. The user is wondering if Azure ML supports datasets consisting of multiple files. The user has also encountered the same issue when creating the dataset and datastore in Python.",
        "Challenge_last_edit_time":1613487899540,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66226685",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":33.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":1490.2182730556,
        "Challenge_title":"AzureML create dataset from datastore with multiple files - path not valid",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2337.0,
        "Challenge_word_count":311,
        "Platform":"Stack Overflow",
        "Poster_created_time":1373651649052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"M\u00fcnchen, Deutschland",
        "Poster_reputation_count":1066.0,
        "Poster_view_count":60.0,
        "Solution_body":"<p>I uploaded and registered the files with this script and everything works as expected.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Datastore, Dataset, Workspace\n\nimport logging\n\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(\n    level=logging.INFO,\n    format=&quot;%(asctime)s.%(msecs)03d %(levelname)s %(module)s - %(funcName)s: %(message)s&quot;,\n    datefmt=&quot;%Y-%m-%d %H:%M:%S&quot;,\n)\n\ndatastore_name = &quot;mydatastore&quot;\ndataset_path_on_disk = &quot;.\/data\/images_greyscale&quot;\ndataset_path_in_datastore = &quot;images_greyscale&quot;\n\nazure_dataset_name = &quot;images_grayscale&quot;\nazure_dataset_description = &quot;dataset transformed into the coco format and into grayscale images&quot;\n\n\nworkspace = Workspace.from_config()\ndatastore = Datastore.get(workspace, datastore_name=datastore_name)\n\nlogger.info(&quot;Uploading data...&quot;)\ndatastore.upload(\n    src_dir=dataset_path_on_disk, target_path=dataset_path_in_datastore, overwrite=False\n)\nlogger.info(&quot;Uploading data done.&quot;)\n\nlogger.info(&quot;Registering dataset...&quot;)\ndatastore_path = [(datastore, dataset_path_in_datastore)]\ndataset = Dataset.File.from_files(path=datastore_path)\ndataset.register(\n    workspace=workspace,\n    name=azure_dataset_name,\n    description=azure_dataset_description,\n    create_new_version=True,\n)\nlogger.info(&quot;Registering dataset done.&quot;)\n\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":23.7,
        "Solution_reading_time":19.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":84.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":60.5666666667,
        "Challenge_answer_count":0,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [X] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nAs started runs in MlflowLogger are never ended, all runs shown in MLflow dashboard seem to be nested recursively.\r\nMLflow 1.28.0 fixed the display of deeply nested runs correctly, so the bug is now problematic.\n\n### Reproducible Example\n\n```python\nimport mlflow\r\nfrom pycaret.classification import *\r\nfrom pycaret.datasets import get_data\r\n\r\nmlflow.set_tracking_uri(\"http:\/\/localhost:5000\")\r\n\r\ndata = get_data(\"diabetes\")\r\nsetup(data, target=\"Class variable\", log_experiment=True, silent=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\r\nsetup(data, target=\"Class variable\", log_experiment=True, silent=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\n```\n\n\n### Expected Behavior\n\nExpected display:\r\n![p2](https:\/\/user-images.githubusercontent.com\/1991802\/190944134-3490628a-4eca-490a-af11-c4cdfe41953e.png)\r\n\r\nActual display:\r\n![p1](https:\/\/user-images.githubusercontent.com\/1991802\/190944304-08c41ae2-93fd-4b79-b3ff-ebb594ad2664.png)\r\n\n\n### Actual Results\n\n```python-traceback\nAttached the figure also in 'Expected Behavior'.\n```\n\n\n### Installed Versions\n\n<details>\r\n'2.3.10'\r\n<\/details>\r\n",
        "Challenge_closed_time":1663775400000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1663557360000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to see any models in the `mlflow ui` during training, despite several models having already converged and logged to the file system. The user has tried both the nightly and release branch and has confirmed the issue on the latest version and develop branch of pycaret. The user has provided a reproducible example and expects to see the models that have already converged in the `mlflow ui` dashboard.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2975",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":14.4,
        "Challenge_reading_time":20.35,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":60.5666666667,
        "Challenge_title":"[BUG]: Runs recorded in MLflow nests all recursively",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":145,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Cannot reproduce on master. Please try again with `pip install -U --pre pycaret` @nagamatz and reopen the issue if it persists.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.2,
        "Solution_reading_time":1.57,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":21.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1399363600132,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Heidelberg, Germany",
        "Answerer_reputation_count":8423.0,
        "Answerer_view_count":1313.0,
        "Challenge_adjusted_solved_time":2.4000266667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using the <code>HuggingFacePredictor<\/code> from <code>sagemaker.huggingface<\/code> to inference some text and I would like to get all label scores.<\/p>\n<p>Is there any way of getting, as response from the endpoint:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;labels&quot;: [&quot;help&quot;, &quot;Greeting&quot;, &quot;Farewell&quot;] ,\n    &quot;score&quot;: [0.81, 0.1, 0.09],\n}\n<\/code><\/pre>\n<p>(or similar)<\/p>\n<p>Instead of:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;label&quot;: &quot;help&quot;,\n    &quot;score&quot;: 0.81,\n}\n<\/code><\/pre>\n<p>Here is some example code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nfrom sagemaker.huggingface import HuggingFacePredictor\nfrom sagemaker.session import Session\n\nsagemaker_session = Session(boto_session=boto3.session.Session())\n\npredictor = HuggingFacePredictor(\n    endpoint_name=project, sagemaker_session=sagemaker_session\n)\nprediciton = predictor.predict({&quot;inputs&quot;: text})[0]\n<\/code><\/pre>",
        "Challenge_closed_time":1643809235227,
        "Challenge_comment_count":1,
        "Challenge_created_time":1643804083130,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using HuggingFacePredictor from sagemaker.huggingface to inference some text and wants to get all label scores as a response from the endpoint instead of just one label and score. The user has provided example code for reference.",
        "Challenge_last_edit_time":1643805132912,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70955450",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":15.1,
        "Challenge_reading_time":14.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":1.4311380556,
        "Challenge_title":"How to return all labels and scores in SageMaker Inference?",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":193.0,
        "Challenge_word_count":91,
        "Platform":"Stack Overflow",
        "Poster_created_time":1632991357332,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Barcelona",
        "Poster_reputation_count":373.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>With your current code sample, it is not quite clear what specific task you are performing, but for the sake of this answer, I'll assume you're doing text classification.<\/p>\n<p>Most importantly, though, we can read the following in <a href=\"https:\/\/huggingface.co\/docs\/sagemaker\/reference#inference-toolkit-api\" rel=\"nofollow noreferrer\">Huggingface's Sagemaker reference document<\/a> (bold highlight by me):<\/p>\n<blockquote>\n<p>The Inference Toolkit accepts inputs in the inputs key, and <strong>supports additional <code>pipelines<\/code> parameters in the <code>parameters<\/code> key<\/strong>. You can provide any of the supported <code>kwargs<\/code> from <code>pipelines<\/code> as parameters.<\/p>\n<\/blockquote>\n<p>If we check out the <a href=\"https:\/\/huggingface.co\/docs\/transformers\/v4.16.2\/en\/main_classes\/pipelines#transformers.TextClassificationPipeline.__call__\" rel=\"nofollow noreferrer\">accepted arguments by the <code>TextClassificationPipeline<\/code><\/a>, we can see that there is indeed one that returns all samples:<\/p>\n<blockquote>\n<p><code>return_all_scores<\/code> (bool, optional, defaults to False) \u2014 Whether to return scores for all labels.<\/p>\n<\/blockquote>\n<p>While I unfortunately don't have access to Sagemaker inference, I can run a sample to illustrate the output with a local pipeline:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import pipeline\n# uses 2-way sentiment classification model per default\npipe = pipeline(&quot;text-classification&quot;) \n\npipe(&quot;I am really angry right now &gt;:(&quot;, return_all_scores=True)\n# Output: [[{'label': 'NEGATIVE', 'score': 0.9989138841629028},\n#           {'label': 'POSITIVE', 'score': 0.0010860705515369773}]]\n<\/code><\/pre>\n<p>Based on the slightly different input format expected by Sagemaker, coupled with the example given in <a href=\"https:\/\/github.com\/huggingface\/notebooks\/blob\/master\/sagemaker\/10_deploy_model_from_s3\/deploy_transformer_model_from_s3.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a>, I would assume that a corrected input in your own example code should look like this:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;inputs&quot;: text,\n    &quot;parameters&quot;: {&quot;return_all_scores&quot;: True}\n}\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1643813773008,
        "Solution_link_count":3.0,
        "Solution_readability":17.8,
        "Solution_reading_time":29.73,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":222.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":151.5135516667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,    <\/p>\n<p>I'm trying to create an Azure Machine Learning model to classify text files. I have hundreds of text files that have been organized into a subfolder named its correct label. Similar to how you train Image classification.     <\/p>\n<p>How would I get this data into a data set. I have been trying to use the python sdk since I was able to successfully get the Image classification to work.     <\/p>\n<p>Thanks,    <br \/>\nKyle<\/p>",
        "Challenge_closed_time":1659586063576,
        "Challenge_comment_count":1,
        "Challenge_created_time":1659040614790,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to create an Azure Machine Learning model to classify text files, but is facing challenges in importing the data into a dataset using the Python SDK. The user has successfully used the SDK for image classification.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/946690\/azure-ml-text-classification-import-text-files",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.6,
        "Challenge_reading_time":5.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":151.5135516667,
        "Challenge_title":"Azure ML Text Classification Import Text Files",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":81,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=8a26106b-c601-41ff-b38c-3aa0f07e4ea0\">@Domsohn, Kyle  <\/a> Thanks for the question. Here is the sample to import text files and explore azure ml text classification.    <\/p>\n<p><a href=\"https:\/\/github.com\/microsoft\/nlp-recipes\/blob\/master\/examples\/text_classification\/tc_bert_azureml.ipynb\">https:\/\/github.com\/microsoft\/nlp-recipes\/blob\/master\/examples\/text_classification\/tc_bert_azureml.ipynb<\/a>    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":23.2,
        "Solution_reading_time":5.93,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":26.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1533546244860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":29059.0,
        "Answerer_view_count":2445.0,
        "Challenge_adjusted_solved_time":7447.77258,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>So I've noticed that whenever I do a machine learning train\/retrain (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/retrain-machine-learning-model\" rel=\"nofollow noreferrer\">from here<\/a>), it generates a lot of files in my Azure blob storage as shown here<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/QN08i.png\" alt=\"Screenshot\"><\/p>\n\n<p>I wanted to ask if it was possible to automatically delete all these files or prevent them from ever being generated?<\/p>",
        "Challenge_closed_time":1565832229083,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565811221207,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue where machine learning train\/retrain generates a lot of files in their Azure blob storage, and they are looking for a way to automatically delete or prevent these files from being generated.",
        "Challenge_last_edit_time":1565832393772,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57500954",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.7,
        "Challenge_reading_time":6.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":5.8355211111,
        "Challenge_title":"Automatically delete files in storage",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2077.0,
        "Challenge_word_count":58,
        "Platform":"Stack Overflow",
        "Poster_created_time":1526863814910,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":45.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>For automatically delete all these files in blob storage, you can use the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-lifecycle-management-concepts#azure-portal-list-view\" rel=\"nofollow noreferrer\">Lifecycle Management<\/a> of blob storage.<\/p>\n<p>It's easy to set up a rule and filter, after the rule is set up, all the files will be deleted as per the rule you defined.<\/p>\n<p>Simple steps:<\/p>\n<p>1.Nav to azure portal -&gt; your storage account -&gt; Blob services -&gt; Lifecycle Management, then click &quot;Add rule&quot;.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/n2Wne.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/n2Wne.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>2.In the &quot;Action set&quot; tab, select Delete blob and fill in the textbox; Then in &quot;Filter set&quot; tab, select a path.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/a2cdQ.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/a2cdQ.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>For more details\/instructions, please follow this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-lifecycle-management-concepts#azure-portal-list-view\" rel=\"nofollow noreferrer\">article<\/a>.<\/p>\n<p>Also note that the rule runs once per day, and for the first time, it may take 24 hours to take effect.<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1592644375060,
        "Solution_link_count":6.0,
        "Solution_readability":10.8,
        "Solution_reading_time":18.02,
        "Solution_score_count":4.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":141.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1577919980176,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hamburg, Germany",
        "Answerer_reputation_count":5588.0,
        "Answerer_view_count":398.0,
        "Challenge_adjusted_solved_time":0.5756725,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to use more than 50 labels with AWS Ground Truth?<\/p>\n<p>For example <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-bounding-box.html\" rel=\"nofollow noreferrer\">here<\/a> are 3 labels:<\/p>\n<ul>\n<li>bird<\/li>\n<li>plane<\/li>\n<li>kite<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/GII4X.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/GII4X.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It shows that only 50 labels can be created. Is it possible to create more than 50 labels via AWS-CLI or any other API?<\/p>",
        "Challenge_closed_time":1606832097008,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606830024587,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is asking if it is possible to use more than 50 labels with AWS Ground Truth, as the platform currently only allows for up to 50 labels to be created. They are seeking information on whether it is possible to create more than 50 labels through AWS-CLI or any other API.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65091602",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":8.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.5756725,
        "Challenge_title":"Is it possible to use more than 50 Labels in AWS Ground Truth",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":149.0,
        "Challenge_word_count":73,
        "Platform":"Stack Overflow",
        "Poster_created_time":1510064331503,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"M\u00fcnchen, Deutschland",
        "Poster_reputation_count":5537.0,
        "Poster_view_count":215.0,
        "Solution_body":"<p>No, according to the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-text-classification-multilabel.html\" rel=\"nofollow noreferrer\">documentation<\/a> the maximum is 50.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":31.6,
        "Solution_reading_time":2.6,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":12.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1526368885180,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Munich, Germany",
        "Answerer_reputation_count":3944.0,
        "Answerer_view_count":217.0,
        "Challenge_adjusted_solved_time":0.0123147222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I feel like this should be possible, but I looked through the wandb SDK code and I can't find an easy\/logical way to do it. It <em>might<\/em> be possible to hack it by modifying the manifest entries at some point later (but maybe before the artifact is logged to wandb as then the manifest and the entries might be locked)? I saw things like this in the SDK code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>version = manifest_entry.extra.get(&quot;versionID&quot;)\netag = manifest_entry.extra.get(&quot;etag&quot;)\n<\/code><\/pre>\n<p>So, I figure we can probably edit those?<\/p>\n<h2>UPDATE<\/h2>\n<p>So, I tried to hack it together with something like this and it works but it feels wrong:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport wandb\nimport boto3\nfrom wandb.util import md5_file\n\nENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nPROJECT = os.environ.get(&quot;WANDB_PROJECT&quot;)\nAPI_KEY = os.environ.get(&quot;WANDB_API_KEY&quot;)\n\napi = api = wandb.Api(overrides={&quot;entity&quot;: ENTITY, &quot;project&quot;: ENTITY})\nrun = wandb.init(entity=ENTITY, project=PROJECT, job_type=&quot;test upload&quot;)\nfile = &quot;admin2Codes.txt&quot;  # &quot;admin1CodesASCII.txt&quot; # (both already on s3 with a couple versions)\nartifact = wandb.Artifact(&quot;test_data&quot;, type=&quot;dataset&quot;)\n\n# modify one of the local files so it has a new md5hash etc.\nwith open(file, &quot;a&quot;) as f:\n    f.write(&quot;new_line_1\\n&quot;)\n\n# upload local file to s3\nlocal_file_path = file\ns3_url = f&quot;s3:\/\/bucket\/prefix\/{file}&quot;\ns3_url_arr = s3_url.replace(&quot;s3:\/\/&quot;, &quot;&quot;).split(&quot;\/&quot;)\ns3_bucket = s3_url_arr[0]\nkey = &quot;\/&quot;.join(s3_url_arr[1:])\n\ns3_client = boto3.client(&quot;s3&quot;)\nfile_digest = md5_file(local_file_path)\ns3_client.upload_file(\n    local_file_path,\n    s3_bucket,\n    key,\n    # save the md5_digest in metadata,\n    # can be used later to only upload new files to s3,\n    # as AWS doesn't digest the file consistently in the E-tag\n    ExtraArgs={&quot;Metadata&quot;: {&quot;md5_digest&quot;: file_digest}},\n)\nhead_response = s3_client.head_object(Bucket=s3_bucket, Key=key)\nversion_id: str = head_response[&quot;VersionId&quot;]\nprint(version_id)\n\n# upload a link\/ref to this s3 object in wandb:\nartifact.add_reference(s3_dir)\n# at this point we might be able to modify the artifact._manifest.entries and each entry.extra.get(&quot;etag&quot;) etc.?\nprint([(name, entry.extra) for name, entry in artifact._manifest.entries.items()])\n# set these to an older version on s3 that we know we want (rather than latest) - do this via wandb public API:\ndataset_v2 = api.artifact(f&quot;{ENTITY}\/{PROJECT}\/test_data:v2&quot;, type=&quot;dataset&quot;)\n# artifact._manifest.add_entry(dataset_v2.manifest.entries[&quot;admin1CodesASCII.txt&quot;])\nartifact._manifest.entries[&quot;admin1CodesASCII.txt&quot;] = dataset_v2.manifest.entries[\n    &quot;admin1CodesASCII.txt&quot;\n]\n# verify that it did change:\nprint([(name, entry.extra) for name, entry in artifact._manifest.entries.items()])\n\nrun.log_artifact(artifact)  # at this point the manifest is locked I believe?\nartifact.wait()  # wait for upload to finish (blocking - but should be very quick given it is just an s3 link)\nprint(artifact.name)\nrun_id = run.id\nrun.finish()\ncurr_run = api.run(f&quot;{ENTITY}\/{PROJECT}\/{run_id}&quot;)\nused_artifacts = curr_run.used_artifacts()\nlogged_artifacts = curr_run.logged_artifacts()\n<\/code><\/pre>\n<p>Am I on the right track here? I guess the other workaround is to make a copy on s3 (so that older version is the latest again) but I wanted to avoid this as the 1 file that I want to use an old version of is a large NLP model and the only files I want to change are small config.json files etc. (so seems very wasteful to upload all files again).<\/p>\n<p>I was also wondering if when I copy an old version of an object back into the same key in the bucket if that creates a real copy or just like a pointer to the same underlying object. Neither boto3 nor AWS documentation makes that clear - although it seems like it is a proper copy.<\/p>",
        "Challenge_closed_time":1643119052816,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641691804600,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to find a way to add a specific version ID or ETag to an artifact in wandb, to avoid the need for re-uploading to s3. They have attempted to modify the manifest entries, but are unsure if this is the correct approach. They have also considered making a copy on s3, but this would be wasteful for their use case. They are seeking guidance on the best approach to achieve their goal.",
        "Challenge_last_edit_time":1643119769043,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70637798",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":54.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":396.4578377778,
        "Challenge_title":"wandb: artifact.add_reference() option to add specific (not current) versionId or ETag to stop the need for re-upload to s3?",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":121.0,
        "Challenge_word_count":484,
        "Platform":"Stack Overflow",
        "Poster_created_time":1526368885180,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Munich, Germany",
        "Poster_reputation_count":3944.0,
        "Poster_view_count":217.0,
        "Solution_body":"<p>I think I found the correct way to do it now:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport wandb\nimport boto3\nfrom wandb.util import md5_file\n\nENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nPROJECT = os.environ.get(&quot;WANDB_PROJECT&quot;)\n\n\ndef wandb_update_only_some_files_in_artifact(\n    existing_artifact_name: str,\n    new_s3_file_urls: list[str],\n    entity: str = ENTITY,\n    project: str = PROJECT,\n) -&gt; Artifact:\n    &quot;&quot;&quot;If you want to just update a config.json file for example,\n    but the rest of the artifact can remain the same, then you can\n    use this functions like so:\n    wandb_update_only_some_files_in_artifact(\n        &quot;old_artifact:v3&quot;,\n        [&quot;s3:\/\/bucket\/prefix\/config.json&quot;],\n    )\n    and then all the other files like model.bin will be the same as in v3,\n    even if there was a v4 or v5 in between (as the v3 VersionIds are used)\n\n    Args:\n        existing_artifact_name (str): name with version like &quot;old_artifact:v3&quot;\n        new_s3_file_urls (list[str]): files that should be updated\n        entity (str, optional): wandb entity. Defaults to ENTITY.\n        project (str, optional): wandb project. Defaults to PROJECT.\n\n    Returns:\n        Artifact: the new artifact object\n    &quot;&quot;&quot;\n    api = wandb.Api(overrides={&quot;entity&quot;: entity, &quot;project&quot;: project})\n    old_artifact = api.artifact(existing_artifact_name)\n    old_artifact_name = re.sub(r&quot;:v\\d+$&quot;, &quot;&quot;, old_artifact.name)\n    with wandb.init(entity=entity, project=project) as run:\n        new_artifact = wandb.Artifact(old_artifact_name, type=old_artifact.type)\n\n        s3_file_names = [s3_url.split(&quot;\/&quot;)[-1] for s3_url in new_s3_file_urls]\n        # add the new ones:\n        for s3_url, filename in zip(new_s3_file_urls, s3_file_names):\n            new_artifact.add_reference(s3_url, filename)\n        # add the old ones:\n        for filename, entry in old_artifact.manifest.entries.items():\n            if filename in s3_file_names:\n                continue\n            new_artifact.add_reference(entry, filename)\n            # this also works but feels hackier:\n            # new_artifact._manifest.entries[filename] = entry\n\n        run.log_artifact(new_artifact)\n        new_artifact.wait()  # wait for upload to finish (blocking - but should be very quick given it is just an s3 link)\n        print(new_artifact.name)\n        print(run.id)\n    return new_artifact\n\n\n# usage:\nlocal_file_path = &quot;config.json&quot; # modified file\ns3_url = &quot;s3:\/\/bucket\/prefix\/config.json&quot;\ns3_url_arr = s3_url.replace(&quot;s3:\/\/&quot;, &quot;&quot;).split(&quot;\/&quot;)\ns3_bucket = s3_url_arr[0]\nkey = &quot;\/&quot;.join(s3_url_arr[1:])\n\ns3_client = boto3.client(&quot;s3&quot;)\nfile_digest = md5_file(local_file_path)\ns3_client.upload_file(\n    local_file_path,\n    s3_bucket,\n    key,\n    # save the md5_digest in metadata,\n    # can be used later to only upload new files to s3,\n    # as AWS doesn't digest the file consistently in the E-tag\n    ExtraArgs={&quot;Metadata&quot;: {&quot;md5_digest&quot;: file_digest}},\n)\n\nwandb_update_only_some_files_in_artifact(\n    &quot;old_artifact:v3&quot;,\n    [&quot;s3:\/\/bucket\/prefix\/config.json&quot;],\n)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1643119813376,
        "Solution_link_count":0.0,
        "Solution_readability":14.0,
        "Solution_reading_time":38.99,
        "Solution_score_count":0.0,
        "Solution_sentence_count":30.0,
        "Solution_word_count":282.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":525.8622483334,
        "Challenge_answer_count":7,
        "Challenge_body":"<p>I have a guild operation <code>main<\/code> that runs 3 steps which are other operations: <code>impute<\/code>, <code>evaluate<\/code>, and <code>predict<\/code>. The latter two require on the <code>impute<\/code> operation (specifically a model checkpoint and some data output).<br>\n(<a href=\"https:\/\/github.com\/davzaman\/autopopulus\/blob\/dev\/guild.yml\" rel=\"noopener nofollow ugc\">guild.yml<\/a> file for reference)<\/p>\n<ol>\n<li>When one of the steps fails (e.g. <code>evaluate<\/code>), the <code>main<\/code> op shows error and so does <code>evaluate<\/code>. If I fix the error in the code and restart the run with something like <code>for hash in $(guild select --operation evaluate --error --all); do guild run -y --background --restart $hash --force-sourcecode; done<\/code>,  then the <code>evaluate<\/code> op fixes to completed, but the <code>main<\/code> operation does not. It doesn\u2019t seem very possible to update it, but it is slightly unclean and annoying to keep track of what broke and what is fixed. I end up with something like:<\/li>\n<\/ol>\n<pre><code class=\"lang-plaintext\">[71:ec03c916]   evaluate  2023-02-20 14:43:57  completed  dvae myexperiment\n[72:957ecb30]   evaluate  2023-02-20 14:43:56  completed  dvae myexperiment\n[73:19493e6b]   evaluate  2023-02-20 14:43:56  completed  dvae myexperiment\n...\n[127:fe72a7ff]  predict   2023-02-18 20:58:56  completed  dvae \n[128:617bc8fd]  impute    2023-02-18 20:26:16  completed  dvae myexperiment\n[129:2b155ff0]  main      2023-02-18 20:26:14  error      dvae \n[130:39125144]  predict   2023-02-18 20:21:08  completed  dvae \n[131:5c4ed46a]  impute    2023-02-18 19:45:25  completed  dvae myexperiment\n[132:c542fcbe]  main      2023-02-18 19:45:24  error      dvae \n<\/code><\/pre>\n<p>It said <code>error<\/code> for <code>main<\/code> but it\u2019s really been fixed sine the <code>evaluate<\/code> op was fixed.<br>\nAnother issue is also what files are stored under each op which leads me to the next point, where ill use <code>run 132<\/code> as an example:<\/p>\n<ol start=\"2\">\n<li>If I look at what is stored under the <code>main<\/code> op I see:<\/li>\n<\/ol>\n<pre><code class=\"lang-shell\">me@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/c542fcbe24ab4a86b1ea0e33fabd839a$ ls\nevaluate  impute  options.yml  predict\n<\/code><\/pre>\n<p>If I drill into the directories I see:<\/p>\n<pre><code class=\"lang-plaintext\">me@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/c542fcbe24ab4a86b1ea0e33fabd839a$ cd evaluate\nme@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/c542fcbe24ab4a86b1ea0e33fabd839a\/evaluate$ ls\nF.O.  options.yml  serialized_models\n<\/code><\/pre>\n<p>If <code>evaluate<\/code> fails and I rerun it, does that mean that the <code>evaluate<\/code>folder will be updated too (is it a symlink)? There seems to be some redundancy too which leads me to:<\/p>\n<ol start=\"3\">\n<li>If I look at the output of the substeps <code>impute<\/code> and <code>predict<\/code> I see:<\/li>\n<\/ol>\n<pre><code class=\"lang-plaintext\"># impute op\nme@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/5c4ed46a12e145158b2351621ee81345\/serialized_models$ ls\nAEDitto_STATIC.pt  imputed_data.pkl  STATIC_test_dataloader.pt\n# predict op\nme@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/391251446fe041048d93d80deda6ac8a\/serialized_models$ ls\nAEDitto_STATIC.pt  imputed_data.pkl  STATIC_test_dataloader.pt\n<\/code><\/pre>\n<p>I also see<\/p>\n<pre><code class=\"lang-plaintext\"># impute op\nme@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/5c4ed46a12e145158b2351621ee81345$ ls F.O.\/0.33\/MNAR\\(G\\)\/dvae\/lightning_logs\/version_0\/\nevents.out.tfevents.1676780435.lambda2.6521.0\nevents.out.tfevents.1676780445.lambda2.6521.1\nevents.out.tfevents.1676780453.lambda2.6521.2\nhparams.yaml\n# predict op\nme@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/391251446fe041048d93d80deda6ac8a$ ls F.O.\/0.33\/MNAR\\(G\\)\/dvae\/lightning_logs\/version_0\/\nevents.out.tfevents.1676780435.lambda2.6521.0\nevents.out.tfevents.1676780445.lambda2.6521.1\nevents.out.tfevents.1676780453.lambda2.6521.2\nhparams.yaml\n<\/code><\/pre>\n<p>It looks like it copies over everything from the <code>impute<\/code> op top the parents: <code>main<\/code>, and dependent steps: <code>predict<\/code>, and <code>evaluate<\/code>. This is a lot of redundancy especially for expensive\/large models and artifacts. This is making me run out of space on my machine.<\/p>\n<p>My questions are<br>\na) How do I avoid redundancy in stored artifacts between parent and child steps like <code>main<\/code> having substeps.<br>\nb) How do I avoid redundancy amongst sibling runs where one may be dependent on another? While <code>evaluate<\/code> relies on the artifacts from <code>impute<\/code> I don\u2019t want it to store all the artifacts all over again (including the model checkpoints, data, and the logging files), I just want <code>evaluate<\/code> to use the checkpointed data and model. <a href=\"https:\/\/my.guild.ai\/t\/guild-file-cheatsheet\/192#required-operation-files-14\">I know there\u2019s a <code>select:<\/code> option<\/a> but it seems to be regex, making it complicated to select the checkpointed model AND data. Also even if that solves excluding the logged files, I don\u2019t want to copy over the files it relies on to the final logged artifacts.<\/p>",
        "Challenge_closed_time":1678831800198,
        "Challenge_comment_count":0,
        "Challenge_created_time":1676938696104,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing confusion regarding multistep operations, restarting substeps, and copied files. When one of the steps fails, the main operation shows an error and fixing the error in the code and restarting the run does not update the main operation. The user is also facing redundancy in stored artifacts between parent and child steps and amongst sibling runs, which is making them run out of space on their machine. They are seeking solutions to avoid redundancy in stored artifacts and to avoid copying over files that the final logged artifacts rely on.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/my.guild.ai\/t\/confusion-on-multistep-operations-restarting-substeps-and-copied-files\/998",
        "Challenge_link_count":2,
        "Challenge_participation_count":7,
        "Challenge_readability":13.3,
        "Challenge_reading_time":67.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":525.8622483334,
        "Challenge_title":"Confusion on multistep operations, restarting substeps, and copied files?",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":128.0,
        "Challenge_word_count":550,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a class=\"mention\" href=\"\/u\/davzaman\">@davzaman<\/a> It looks like there was a regression and Guild is indeed <em>copying<\/em> resolved operation dependency files. This is not the intended behavior and we\u2019ll fix that ASAP.<\/p>\n<p>As a workaround, avoid copying files by adding <code>target-type<\/code> to your dependency def like this:<\/p>\n<pre><code class=\"lang-yaml\">upstream: {}\n\ndownstream:\n  requires:\n    - operation: upstream\n      target-type: link  # tells Guild to link to the resolved files, not copy\n<\/code><\/pre>\n<p>The <code>downstream<\/code> operation is any operation that requires an upstream run.<\/p>\n<p>Sorry about that! This will make a big difference in disk space for you. We\u2019ll post here when the fix is applied, after which you can remove the explicit <code>target-type<\/code> in your dependencies.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.5,
        "Solution_reading_time":10.34,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":108.0,
        "Tool":"Guild AI"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":26.7248641667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have keras training script on my machine. I am experimenting to run my script on AWS sagemaker container. For that I have used below code.<\/p>\n<pre><code>from sagemaker.tensorflow import TensorFlow\nest = TensorFlow(\n    entry_point=&quot;caller.py&quot;,\n    source_dir=&quot;.\/&quot;,\n    role='role_arn',\n    framework_version=&quot;2.3.1&quot;,\n    py_version=&quot;py37&quot;,\n    instance_type='ml.m5.large',\n    instance_count=1,\n    hyperparameters={'batch': 8, 'epochs': 10},\n)\n\nest.fit()\n<\/code><\/pre>\n<p>here <code>caller.py<\/code> is my entry point. After executing the above code I am getting <code>keras is not installed<\/code>. Here is the stacktrace.<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;executor.py&quot;, line 14, in &lt;module&gt;\n    est.fit()\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/estimator.py&quot;, line 682, in fit\n    self.latest_training_job.wait(logs=logs)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/estimator.py&quot;, line 1625, in wait\n    self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/session.py&quot;, line 3681, in logs_for_job\n    self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/session.py&quot;, line 3240, in _check_job_status\n    raise exceptions.UnexpectedStatusException(\nsagemaker.exceptions.UnexpectedStatusException: Error for Training job tensorflow-training-2021-06-09-07-14-01-778: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/usr\/local\/bin\/python3.7 caller.py --batch 4 --epochs 10\n\nModuleNotFoundError: No module named 'keras'\n\n<\/code><\/pre>\n<ol>\n<li>Which instance has pre-installed keras?<\/li>\n<li>Is there any way I can install the python package to the AWS container? or any workaround for the issue?<\/li>\n<\/ol>\n<p>Note: I have tried with my own container uploading to ECR and successfully run my code. I am looking for AWS's existing container capability.<\/p>",
        "Challenge_closed_time":1623320398007,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623223568407,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to train a Keras model on AWS Sagemaker using a TensorFlow estimator. However, upon executing the code, the user encounters an error stating that Keras is not installed. The user is seeking a solution to either install Keras on the AWS container or find an alternative workaround.",
        "Challenge_last_edit_time":1623224188496,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67899421",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.0,
        "Challenge_reading_time":28.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":26.8971111111,
        "Challenge_title":"Training keras model in AWS Sagemaker",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":316.0,
        "Challenge_word_count":191,
        "Platform":"Stack Overflow",
        "Poster_created_time":1426675778223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Coimbatore, Tamil Nadu, India",
        "Poster_reputation_count":10189.0,
        "Poster_view_count":1471.0,
        "Solution_body":"<p>Keras is now part of tensorflow, so you can just reformat your code to use <code>tf.keras<\/code> instead of <code>keras<\/code>. Since version 2.3.0 of tensorflow they are in sync, so it should not be that difficult.\nYou container is <a href=\"https:\/\/aws.amazon.com\/releasenotes\/aws-deep-learning-containers-for-tensorflow-2-3-1-with-cuda-11-0\/\" rel=\"nofollow noreferrer\">this<\/a>, as you can see from the list of the packages, there is no <code>Keras<\/code>.\nIf you instead want to extend a pre-built container you can take a look <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/prebuilt-containers-extend.html\" rel=\"nofollow noreferrer\">here<\/a> but I don't recommend in this specific use-case, because also for future code maintainability you should go for <code>tf.keras<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.4,
        "Solution_reading_time":10.34,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":93.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":243.3152777778,
        "Challenge_answer_count":0,
        "Challenge_body":"Only allow access to project members for the given MLflow.",
        "Challenge_closed_time":1620648494000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619772559000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a \"Connection aborted\" error while trying to perform multi-label classification with \"doc_classification_multilabel.py\". The error occurred during the training process and the user confirmed that their internet connection was stable. The error message suggests that the remote end closed the connection without response. The user is seeking clarification on why this error occurred.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/404",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":5.2,
        "Challenge_reading_time":1.2,
        "Challenge_repo_contributor_count":16.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":909.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":243.3152777778,
        "Challenge_title":"Users can access to any MLflow project",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":16,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1034.8597222222,
        "Challenge_answer_count":0,
        "Challenge_body":"> These are reported by @tapadipti (thanks). I'm moving here to discuss and follow: \r\n\r\nI was running experiments by following the docs (https:\/\/dvc.org\/doc\/start\/experiments) and encountered the following issues. Sharing here for any required action.\r\n1. dvc is not installed by `pip install -r requirements.txt`. So, if someone is trying to use a new virtual env, they need to install dvc separately. Would be good to include `dvc` in `requirements.txt`.\r\n2. `dvc pull` gave this error:\r\n   ```\r\n   ERROR: failed to pull data from the cloud - Checkout failed for following targets:\r\n   models\/model.h5\r\n   metrics\r\n   Is your cache up to date?\r\n   <https:\/\/error.dvc.org\/missing-files>\r\n   ```\r\n\r\n3. `dvc exp run` lists all the image when running the `extract` stage. Would be good to remove `-v` from `tar -xvzf data\/images.tar.gz --directory data`\r\n4. `If you used dvc repro before` section in the doc is a little unclear. Does `dvc exp run` replace `dvc repro`? If yes, can we state this clearly? Also would be great to change this statement `We use dvc repro to run the pipeline...` to `dvc repro runs the pipeline...`",
        "Challenge_closed_time":1642605521000,
        "Challenge_comment_count":11,
        "Challenge_created_time":1638880026000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the \"pull\" command on DagsHub remote, as it fails to pull dvc on Windows. The user has to manually pull dvc again, which is a permanent issue on Windows. Although not urgent, it is an annoyance for the user.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/iterative\/example-repos-dev\/issues\/98",
        "Challenge_link_count":2,
        "Challenge_participation_count":11,
        "Challenge_readability":5.9,
        "Challenge_reading_time":13.98,
        "Challenge_repo_contributor_count":17.0,
        "Challenge_repo_fork_count":11.0,
        "Challenge_repo_issue_count":154.0,
        "Challenge_repo_star_count":15.0,
        "Challenge_repo_watch_count":14.0,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":1034.8597222222,
        "Challenge_title":"Various issues in `example-dvc-experiments`",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":176,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This seems high priority. We can remove `bug` and change to `p1` after 2. is addressed at least, I think. > 1. dvc is not installed by `pip install -r requirements.txt`. So, if someone is trying to use a new virtual env, they need to install dvc separately. Would be good to include `dvc` in `requirements.txt`.\r\n\r\nThis was a bit intentional to let the users install DVC themselves, and a bit to prevent version conflicts. There are some conditions (like installing DVC to system and venv both with different dependencies) that cause weird behavior. \r\n\r\nWe can go on to this route though, it's a single line of change. Is it better to add `dvc` to the `requirements.txt` @shcheklein?  If this was intentional and we don't want to include `dvc` in `requirements.txt`, then we should add an instruction that the user should install `dvc`. Currently, such an instruction is missing. It is unlikely that many people will reach the experiments page of the tutorial without first having installed `dvc`. But in case they try to work a new venv, it can be a `lil confusing. I remembered why I left `-v` in `tar`, it was taking some time after `extract` to start running and the experiment looks like it's frozen. I've now updated the project not to use `-v` in `tar`, and also updated `model.h5` in the remote. (We had a bug in DVC that was preventing to upload experiments.) Could you now check whether the project works as intended? @tapadipti \r\n\r\nI'll create separate PRs in the docs for content updates. Thank you.  Thanks @iesahin \r\n\r\n`dvc pull` gave this error:\r\n```                                                                                                                    \r\nERROR: failed to pull data from the cloud - Checkout failed for following targets:\r\n\/Users\/tapadiptisitaula\/Documents\/test\/example-dvc-experiments\/models\/model.h5\r\nIs your cache up to date?\r\n<https:\/\/error.dvc.org\/missing-files>\r\n```\r\nSo looks like `metrics` worked but not `model.h5`. And this time, the full file path is displayed.\r\n\r\nRemoving `-v` worked. The files are not listed anymore.\r\n\r\n ```\r\n> ERROR: failed to pull data from the cloud - Checkout failed for following targets:\r\n\/Users\/tapadiptisitaula\/Documents\/test\/example-dvc-experiments\/models\/model.h5\r\n```\r\n\r\nInteresting. I double checked yesterday that the script pushing the artifacts has completed successfully. Now, I've checked again and it says:\r\n\r\n```\r\ndvc push\r\nEverything is up to date.\r\n```\r\n\r\nCould you check the MD5 line in `dvc.lock`, corresponding to this line: https:\/\/github.com\/iterative\/example-dvc-experiments\/blob\/main\/dvc.lock#L36\r\n\r\nWhat's the MD5 hash value there, in your installation?\r\n Also, I've checked after cloning the repository: \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/476310\/145449841-16ae8f43-7ce3-4459-a0d3-225d67214ab0.png)\r\n\r\n@tapadipti  The current staging version in https:\/\/github.com\/iterative\/example-dvc-staging resolves all of these issues. I think we can push it to `example-dvc-experiments`.  @iesahin sounds good. The most recent https:\/\/github.com\/iterative\/example-dvc-experiments resolves all these issues. The codification changes are in #97. Closing this. ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":7.3,
        "Solution_reading_time":37.61,
        "Solution_score_count":null,
        "Solution_sentence_count":41.0,
        "Solution_word_count":426.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":2.15214,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to execute the following code in Azure ML Studio notebook:<\/p>\n\n<pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.cross_validation import KFold, cross_val_score\n\nfor C in np.linspace(0.01, 0.2, 30):\n    cv = KFold(n=X_train.shape[0], n_folds=7, shuffle=True, random_state=12345)\n    clf = LogisticRegression(C=C, random_state=12345)\n    print C, sum(cross_val_score(clf, X_train_scaled, y_train, scoring='roc_auc', cv=cv, n_jobs=2)) \/ 7.0\n<\/code><\/pre>\n\n<p>and I'm getting this error:<\/p>\n\n<pre><code>Failed to save &lt;type 'numpy.ndarray'&gt; to .npy file:\nTraceback (most recent call last):\n  File \"\/home\/nbcommon\/env\/lib\/python2.7\/site-packages\/sklearn\/externals\/joblib\/numpy_pickle.py\", line 271, in save\n    obj, filename = self._write_array(obj, filename)\n  File \"\/home\/nbcommon\/env\/lib\/python2.7\/site-packages\/sklearn\/externals\/joblib\/numpy_pickle.py\", line 231, in _write_array\n    self.np.save(filename, array)\n  File \"\/home\/nbcommon\/env\/lib\/python2.7\/site-packages\/numpy\/lib\/npyio.py\", line 491, in save\n    pickle_kwargs=pickle_kwargs)\n  File \"\/home\/nbcommon\/env\/lib\/python2.7\/site-packages\/numpy\/lib\/format.py\", line 585, in write_array\n    array.tofile(fp)\nIOError: 19834920 requested and 8384502 written\n\n---------------------------------------------------------------------------\nIOError                                   Traceback (most recent call last)\n&lt;ipython-input-29-9740e9942629&gt; in &lt;module&gt;()\n      6     cv = KFold(n=X_train.shape[0], n_folds=7, shuffle=True, random_state=12345)\n      7     clf = LogisticRegression(C=C, random_state=12345)\n----&gt; 8     print C, sum(cross_val_score(clf, X_train_scaled, y_train, scoring='roc_auc', cv=cv, n_jobs=2)) \/ 7.0\n\n\/home\/nbcommon\/env\/lib\/python2.7\/site-packages\/sklearn\/cross_validation.pyc in cross_val_score(estimator, X, y, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\n   1431                                               train, test, verbose, None,\n   1432                                               fit_params)\n-&gt; 1433                       for train, test in cv)\n   1434     return np.array(scores)[:, 0]\n   1435 \n\n\/home\/nbcommon\/env\/lib\/python2.7\/site-packages\/sklearn\/externals\/joblib\/parallel.pyc in __call__(self, iterable)\n    808                 # consumption.\n    809                 self._iterating = False\n--&gt; 810             self.retrieve()\n    811             # Make sure that we get a last message telling us we are done\n    812             elapsed_time = time.time() - self._start_time\n\n\/home\/nbcommon\/env\/lib\/python2.7\/site-packages\/sklearn\/externals\/joblib\/parallel.pyc in retrieve(self)\n    725                 job = self._jobs.pop(0)\n    726             try:\n--&gt; 727                 self._output.extend(job.get())\n    728             except tuple(self.exceptions) as exception:\n    729                 # Stop dispatching any new job in the async callback thread\n\n\/home\/nbcommon\/env\/lib\/python2.7\/multiprocessing\/pool.pyc in get(self, timeout)\n    565             return self._value\n    566         else:\n--&gt; 567             raise self._value\n    568 \n    569     def _set(self, i, obj):\n\nIOError: [Errno 28] No space left on device\n<\/code><\/pre>\n\n<p>With <code>n_jobs=1<\/code> it works fine.<\/p>\n\n<p>I think this is because <code>joblib<\/code> library tries to save my data to <code>\/dev\/shm<\/code>. The problem is that it has only 64M capacity:<\/p>\n\n<pre><code>Filesystem         Size  Used Avail Use% Mounted on\nnone               786G  111G  636G  15% \/\ntmpfs               56G     0   56G   0% \/dev\nshm                 64M     0   64M   0% \/dev\/shm\ntmpfs               56G     0   56G   0% \/sys\/fs\/cgroup\n\/dev\/mapper\/crypt  786G  111G  636G  15% \/etc\/hosts\n<\/code><\/pre>\n\n<p>I can't change this folder by setting <code>JOBLIB_TEMP_FOLDER<\/code> environment variable (<code>export<\/code> doesn't work).<\/p>\n\n<pre><code>In [35]: X_train_scaled.nbytes\n\nOut[35]: 158679360\n<\/code><\/pre>\n\n<p>Thanks for any advice!<\/p>",
        "Challenge_closed_time":1457947589352,
        "Challenge_comment_count":0,
        "Challenge_created_time":1457871506333,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while executing a code in Azure ML Studio notebook due to the limited capacity of \/dev\/shm folder which is causing the joblib library to fail while saving data. The user is unable to change the folder by setting JOBLIB_TEMP_FOLDER environment variable.",
        "Challenge_last_edit_time":1457939841648,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35970126",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.9,
        "Challenge_reading_time":46.35,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":21.1341719444,
        "Challenge_title":"Increase the size of \/dev\/shm in Azure ML Studio",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":630.0,
        "Challenge_word_count":346,
        "Platform":"Stack Overflow",
        "Poster_created_time":1452022246888,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Moscow, Russia",
        "Poster_reputation_count":708.0,
        "Poster_view_count":167.0,
        "Solution_body":"<p>The <code>\/dev\/shm<\/code> is a virtual filesystem for passing data between programs that implementation of traditional shared memory on Linux.<\/p>\n\n<p>So you could not increase it via set up some options on Application Layout.<\/p>\n\n<p>But for example, you can remount <code>\/dev\/shm<\/code> with 8G size in Linux Shell with administrator permission like <code>root<\/code> as follows.<\/p>\n\n<p><code>mount -o remount,size=8G \/dev\/shm<\/code><\/p>\n\n<p>However, it seems that Azure ML studio not support remote access via SSH protocol, so the feasible plan is upgrade the standard tier if using free tier at present.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.9,
        "Solution_reading_time":7.74,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":86.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1431288135943,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"England",
        "Answerer_reputation_count":121.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":269.4953261111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run a mlflow tracking server that is installed inside of a virtualenv as a systemd service on Ubuntu 20.04 but I am getting an error indicating that it is unable to find gunicorn. Here is my journal<\/p>\n<pre><code>nov 27 10:37:17 Atrium-Power mlflow[81375]: Traceback (most recent call last):\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/bin\/mlflow&quot;, line 8, in &lt;module&gt;\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     sys.exit(cli())\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 829, in __call__\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     return self.main(*args, **kwargs)\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 782, in main\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     rv = self.invoke(ctx)\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 1259, in invoke\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     return _process_result(sub_ctx.command.invoke(sub_ctx))\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 1066, in invoke\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     return ctx.invoke(self.callback, **ctx.params)\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 610, in invoke\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     return callback(*args, **kwargs)\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/mlflow\/cli.py&quot;, line 392, in server\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     _run_server(\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/mlflow\/server\/__init__.py&quot;, line 138, in _run_server\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     exec_cmd(full_command, env=env_map, stream_output=True)\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/mlflow\/utils\/process.py&quot;, line 34, in exec_cmd\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     child = subprocess.Popen(\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/subprocess.py&quot;, line 947, in __init__\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     self._execute_child(args, executable, preexec_fn, close_fds,\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/subprocess.py&quot;, line 1819, in _execute_child\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     raise child_exception_type(errno_num, err_msg, err_filename)\nnov 27 10:37:17 Atrium-Power mlflow[81375]: FileNotFoundError: [Errno 2] No such file or directory: 'gunicorn'\n<\/code><\/pre>\n<p>and my systemd is this:<\/p>\n<pre><code>[Unit]\nStartLimitBurst=5\nStartLimitIntervalSec=33\n\n[Service]\nUser=praxasense\nWorkingDirectory=\/home\/praxasense\nRestart=always\nRestartSec=5\nExecStart=\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/bin\/mlflow server --port 3569 --backend-store-uri .mlruns\n\n[Install]\nWantedBy=multi-user.target\n<\/code><\/pre>\n<p>The strange thing is that if I run the command from <code>ExecStart<\/code> in my terminal it works fine in fish shell, but not in bash, <em>but<\/em> if I do <code>conda activate mlflow-server<\/code> and then do <code>mlflow ...<\/code> it <em>does<\/em> work. As far as I understood the Python interpreter should be aware of it's virtual environment and so it should work as I tried it, but apparently I am missing something that makes it not able to find the gunicon package, which is obviously there.<\/p>\n<p>Any ideas?<\/p>",
        "Challenge_closed_time":1607442042567,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606471859393,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to run a mlflow tracking server as a systemd service on Ubuntu 20.04. The error indicates that gunicorn is not found. The user has provided their journal and systemd files, and has noted that running the command from ExecStart in the terminal works fine in fish shell but not in bash. The user is unsure why the Python interpreter is not able to find the gunicorn package, which is present in the virtual environment.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65035488",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":55.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":269.4953261111,
        "Challenge_title":"Running mlflow as a systemd service - gunicorn not found",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1018.0,
        "Challenge_word_count":386,
        "Platform":"Stack Overflow",
        "Poster_created_time":1314532759332,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Rotterdam, Netherlands",
        "Poster_reputation_count":2732.0,
        "Poster_view_count":609.0,
        "Solution_body":"<p>Try adding the venv's bin path to the environment that systemd runs in:<\/p>\n<pre><code>[Service]\n...\nEnvironment=&quot;PATH=\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/bin&quot;\n...\n<\/code><\/pre>\n<p>I also recommend setting <code>KillMode=mixed<\/code>, since MLFlow will spawn gunicorn instances that won't be terminated if you terminate the service otherwise. <code>mixed<\/code> means that child processes will also be terminated.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.0,
        "Solution_reading_time":5.8,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":46.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1455667285907,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":283.0,
        "Answerer_view_count":85.0,
        "Challenge_adjusted_solved_time":97.8512702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a set of pre-processing stages in sklearn <code>Pipeline<\/code> and an estimator which is a <code>KerasClassifier<\/code> (<code>from tensorflow.keras.wrappers.scikit_learn import KerasClassifier<\/code>).<\/p>\n<p>My overall goal is to tune and log the whole sklearn pipeline in <code>mlflow<\/code> (in databricks evn). I get a confusing type error which I can't figure out how to reslove:<\/p>\n<blockquote>\n<p>TypeError: can't pickle _thread.RLock objects<\/p>\n<\/blockquote>\n<p>I have the following code (without tuning stage) which returns the above error:<\/p>\n<pre><code>conda_env = _mlflow_conda_env(\n    additional_conda_deps=None,\n    additional_pip_deps=[\n        &quot;cloudpickle=={}&quot;.format(cloudpickle.__version__),\n        &quot;scikit-learn=={}&quot;.format(sklearn.__version__),\n        &quot;numpy=={}&quot;.format(np.__version__),\n        &quot;tensorflow=={}&quot;.format(tf.__version__),\n    ],\n    additional_conda_channels=None,\n)\n\nsearch_space = {\n    &quot;estimator__dense_l1&quot;: 20,\n    &quot;estimator__dense_l2&quot;: 20,\n    &quot;estimator__learning_rate&quot;: 0.1,\n    &quot;estimator__optimizer&quot;: &quot;Adam&quot;,\n}\n\n\ndef create_model(n):\n\n    model = Sequential()\n    model.add(Dense(int(n[&quot;estimator__dense_l1&quot;]), activation=&quot;relu&quot;))\n    model.add(Dense(int(n[&quot;estimator__dense_l2&quot;]), activation=&quot;relu&quot;))\n    model.add(Dense(1, activation=&quot;sigmoid&quot;))\n    model.compile(\n        loss=&quot;binary_crossentropy&quot;,\n        optimizer=n[&quot;estimator__optimizer&quot;],\n        metrics=[&quot;accuracy&quot;],\n    )\n\n    return model\n\n\nmlflow.sklearn.autolog()\nwith mlflow.start_run(nested=True) as run:\n\n    classfier = KerasClassifier(build_fn=create_model, n=search_space)\n    # fit the pipeline\n    clf = Pipeline(steps=[(&quot;preprocessor&quot;, preprocessor), \n                          (&quot;estimator&quot;, classfier)])\n    h = clf.fit(\n        X_train,\n        y_train.values,\n        estimator__validation_split=0.2,\n        estimator__epochs=10,\n        estimator__verbose=2,\n    )\n\n    # log scores\n    acc_score = clf.score(X=X_test, y=y_test)\n    mlflow.log_metric(&quot;accuracy&quot;, acc_score)\n\n    signature = infer_signature(X_test, clf.predict(X_test))\n    # Log the model with a signature that defines the schema of the model's inputs and outputs.\n    mlflow.sklearn.log_model(\n        sk_model=clf, artifact_path=&quot;model&quot;, \n        signature=signature, \n        conda_env=conda_env\n    )\n<\/code><\/pre>\n<p>I also get this warning before the error:<\/p>\n<pre><code>\n    WARNING mlflow.sklearn.utils: Truncated the value of the key `steps`. Truncated value: `[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n                      transformer_weights=None,\n                      transformers=[('num',\n                                   Pipeline(memory=None,\n<\/code><\/pre>\n<p>note the the whole pipeline runs outside mlflow.\ncan someone help?<\/p>",
        "Challenge_closed_time":1631593268676,
        "Challenge_comment_count":0,
        "Challenge_created_time":1631241004103,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to tune and log a whole sklearn pipeline in mlflow, which includes a KerasClassifier estimator. However, they are encountering a type error \"can't pickle _thread.RLock objects\" and a warning related to truncation of the value of the key 'steps'. The user has shared their code and is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69126555",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":18.5,
        "Challenge_reading_time":36.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":97.8512702778,
        "Challenge_title":"how to log KerasClassifier model in a sklearn pipeline mlflow?",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":435.0,
        "Challenge_word_count":209,
        "Platform":"Stack Overflow",
        "Poster_created_time":1455667285907,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":283.0,
        "Poster_view_count":85.0,
        "Solution_body":"<p>I think I find sort of a workaround\/solution for this for now, but I think this issue needs to be addressed in MLFloow anyways.<\/p>\n<p>What I did is not the best way probably.\nI used a python package called <a href=\"https:\/\/scikeras.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">scikeras<\/a> that does this wrapping and then could log the model<\/p>\n<p>The code:<\/p>\n<pre><code>import scikeras \nimport tensorflow as tf \nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Flatten, Activation \n \nfrom scikeras.wrappers import KerasClassifier \n  \n \nclass ModelWrapper(mlflow.pyfunc.PythonModel): \n    def __init__(self, model): \n        self.model = model \n \n    def predict(self, context, model_input): \n        return self.model.predict(model_input) \n \nconda_env =  _mlflow_conda_env( \n      additional_conda_deps=None, \n      additional_pip_deps=[ \n        &quot;cloudpickle=={}&quot;.format(cloudpickle.__version__),  \n        &quot;scikit-learn=={}&quot;.format(sklearn.__version__), \n        &quot;numpy=={}&quot;.format(np.__version__), \n        &quot;tensorflow=={}&quot;.format(tf.__version__), \n        &quot;scikeras=={}&quot;.format(scikeras.__version__), \n      ], \n      additional_conda_channels=None, \n  ) \n \nparam = { \n   &quot;dense_l1&quot;: 20, \n   &quot;dense_l2&quot;: 20, \n   &quot;optimizer__learning_rate&quot;: 0.1, \n   &quot;optimizer&quot;: &quot;Adam&quot;, \n   &quot;loss&quot;:&quot;binary_crossentropy&quot;, \n} \n \n  \ndef create_model(dense_l1, dense_l2, meta): \n  \n  n_features_in_ = meta[&quot;n_features_in_&quot;] \n  X_shape_ = meta[&quot;X_shape_&quot;] \n  n_classes_ = meta[&quot;n_classes_&quot;] \n \n  model = Sequential() \n  model.add(Dense(n_features_in_, input_shape=X_shape_[1:], activation=&quot;relu&quot;)) \n  model.add(Dense(dense_l1, activation=&quot;relu&quot;)) \n  model.add(Dense(dense_l2, activation=&quot;relu&quot;)) \n  model.add(Dense(1, activation=&quot;sigmoid&quot;)) \n \n  return model   \n \nmlflow.sklearn.autolog() \nwith mlflow.start_run(run_name=&quot;sample_run&quot;): \n \n  classfier = KerasClassifier( \n    create_model, \n    loss=param[&quot;loss&quot;], \n    dense_l1=param[&quot;dense_l1&quot;], \n    dense_l2=param[&quot;dense_l2&quot;], \n    optimizer__learning_rate = param[&quot;optimizer__learning_rate&quot;], \n    optimizer= param[&quot;optimizer&quot;], \n) \n \n  # fit the pipeline \n  clf = Pipeline(steps=[('preprocessor', preprocessor), \n                      ('estimator', classfier)])   \n \n  h = clf.fit(X_train, y_train.values) \n  # log scores \n  acc_score = clf.score(X=X_test, y=y_test) \n  mlflow.log_metric(&quot;accuracy&quot;, acc_score) \n  signature = infer_signature(X_test, clf.predict(X_test)) \n  model_nn = ModelWrapper(clf,)  \n \n  mlflow.pyfunc.log_model( \n      python_model= model_nn, \n      artifact_path = &quot;model&quot;,  \n      signature = signature,  \n      conda_env = conda_env \n  ) \n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":19.3,
        "Solution_reading_time":35.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":180.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":68.5333333333,
        "Challenge_answer_count":0,
        "Challenge_body":"Firstly I'd like to apologize if this is a dummy question.\r\nI'm following the tutorial to get introduced to kedro mlflow,; after running the command \"kedro mlflow init\" I tried to run the command \"kedro mlflofw ui\" but I get an error:\r\n\r\nINFO     The 'mlflow_tracking_uri' key in mlflow.yml is relative ('server.mlflow_tracking_uri = mlruns'). It is converted to a valid uri: 'file:\/\/\/C:\/Users\/e107338\/PycharmProjects\/mlflow\/kedro-mlflow-example\/mlruns'                                                   kedro_mlflow_config.py:202\r\n\r\nAfter the Traceback I get an error: FileNotFoundErrror\r\n",
        "Challenge_closed_time":1664786016000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1664539296000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to store a PartitionedDataSet as an mlflow artifact with the MlflowArtifactDataSet. The user tried to save a dict with many small result tables to mlflow using PartitionedDataSet, but an error \"dataset has not attribute '_filepath'\" was raised. The bug also happens with the last version on master. A potential solution is to add a better condition to default to \"path\" if there is no \"filepath\" attribute.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/361",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":7.26,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":68.5333333333,
        "Challenge_title":"kedro mlflow ui gets a FileNotFoundError",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":74,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi, \r\n\r\nI am sorry to see you are experiencing issues. this is not a dummy question, it sounds like a bug. \r\n\r\nI've just ran this: \r\n\r\n```bash\r\nconda create -n km-361 python=3.9 -y\r\nconda activate km-361\r\npip install kedro==0.18.3\r\npip install mlflow==1.29.0\r\npip install kedro-mlflow==0.11.3\r\nkedro new --starter=pandas-iris\r\ncd iris\r\nkedro mlflow init\r\nkedro mlflow ui\r\n```\r\n\r\nthen I opened ``http:\/\/127.0.0.1:5000`` and th UI opened as expected. \r\n\r\nCan you tell me: \r\n- your python version\r\n- your OS\r\n- your ``kedro`` \/ ``mlflow`` \/ ``kedro-mlflow`` version\r\n- the project using\r\n- the exact error message\r\n- check if you have a ``MLFLOW_TRACKING_URI`` environment set It turned out fine  after trying again! Sorry and thanks for your consideration!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":5.1,
        "Solution_reading_time":8.84,
        "Solution_score_count":null,
        "Solution_sentence_count":10.0,
        "Solution_word_count":107.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1379265931347,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Rotterdam, Netherlands",
        "Answerer_reputation_count":6502.0,
        "Answerer_view_count":561.0,
        "Challenge_adjusted_solved_time":0.4821711111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a python script that is written in different files (one for importing, one for calculations, et cetera). These are all in the same folder, and when I need a function from another function I do something like<\/p>\n\n<pre><code>import file_import\nfile_import.do_something_usefull()\n<\/code><\/pre>\n\n<p>where, of course, in the <code>file_import<\/code> there is a function <code>do_something_usefull()<\/code> that, uhm, does something usefull. How can I accomplish the same in Azure?<\/p>",
        "Challenge_closed_time":1457451133736,
        "Challenge_comment_count":4,
        "Challenge_created_time":1457449397920,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is asking if it is possible to import python scripts in Azure and how to accomplish the same process as in their local environment where they import different files from the same folder using the import function.",
        "Challenge_last_edit_time":1457540530820,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35870839",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":8.0,
        "Challenge_reading_time":6.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.4821711111,
        "Challenge_title":"Is it possible to import python scripts in Azure?",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1473.0,
        "Challenge_word_count":75,
        "Platform":"Stack Overflow",
        "Poster_created_time":1379265931347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Rotterdam, Netherlands",
        "Poster_reputation_count":6502.0,
        "Poster_view_count":561.0,
        "Solution_body":"<p>I found it out myself. It is documenten on Microsoft's site <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-execute-python-scripts\/\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>The steps, very short, are:<\/p>\n\n<ol>\n<li>Include all the python you want in a .zip<\/li>\n<li>Upload that zip as a dataset<\/li>\n<li>Drag the dataset as the third option parameter in the 'execute python'-block (example below)<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9FuMr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9FuMr.png\" alt=\"Example dragging zip to Python script\"><\/a><\/p>\n\n<ol start=\"4\">\n<li>execute said function by importing <code>import Hello<\/code> (the name of the file, not the zip) and running <code>Hello.do_something_usefull()<\/code><\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1457451850803,
        "Solution_link_count":3.0,
        "Solution_readability":11.5,
        "Solution_reading_time":10.59,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":83.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1386311998172,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Remote, OR, USA",
        "Answerer_reputation_count":2153.0,
        "Answerer_view_count":205.0,
        "Challenge_adjusted_solved_time":122.6327544444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to read a <code>.csv<\/code> file stored in an s3 bucket, and I'm getting errors. I'm following the instructions <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/r_kernel\/using_r_with_amazon_sagemaker.ipynb\" rel=\"nofollow noreferrer\">here<\/a>, but either it does not work or I am making a mistake and I'm not getting what I'm doing wrong.<\/p>\n\n<p>Here's what I'm trying to do:<\/p>\n\n<pre><code># I'm working on a SageMaker notebook instance\nlibrary(reticulate)\nlibrary(tidyverse)\n\nsagemaker &lt;- import('sagemaker')\nsagemaker.session &lt;- sagemaker$Session()\n\nregion &lt;- sagemaker.session$boto_region_name\nbucket &lt;- \"my-bucket\"\nprefix &lt;- \"data\/staging\"\nbucket.path &lt;- sprintf(\"https:\/\/s3-%s.amazonaws.com\/%s\", region, bucket)\nrole &lt;- sagemaker$get_execution_role()\n\nclient &lt;- sagemaker.session$boto_session$client('s3')\nkey &lt;- sprintf(\"%s\/%s\", prefix, 'my_file.csv')\n\nmy.obj &lt;- client$get_object(Bucket=bucket, Key=key)\n\nmy.df &lt;- read_csv(my.obj$Body) # This is where it all breaks down:\n## \n## Error: `file` must be a string, raw vector or a connection.\n## Traceback:\n## \n## 1. read_csv(my.obj$Body)\n## 2. read_delimited(file, tokenizer, col_names = col_names, col_types = col_types, \n##  .     locale = locale, skip = skip, skip_empty_rows = skip_empty_rows, \n##  .     comment = comment, n_max = n_max, guess_max = guess_max, \n##  .     progress = progress)\n## 3. col_spec_standardise(data, skip = skip, skip_empty_rows = skip_empty_rows, \n##  .     comment = comment, guess_max = guess_max, col_names = col_names, \n##  .     col_types = col_types, tokenizer = tokenizer, locale = locale)\n## 4. datasource(file, skip = skip, skip_empty_rows = skip_empty_rows, \n##  .     comment = comment)\n## 5. stop(\"`file` must be a string, raw vector or a connection.\", \n##  .     call. = FALSE)\n<\/code><\/pre>\n\n<p>When working with Python, I can read a CSV file using someting like this:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>import pandas as pd\n# ... Lots of boilerplate code\nmy_data = pd.read_csv(client.get_object(Bucket=bucket, Key=key)['Body'])\n<\/code><\/pre>\n\n<p>This is very similar to what I'm trying to do in R, and it works with Python... so why does it not work on R?<\/p>\n\n<p>Can you point me in the right path?<\/p>\n\n<p><strong>Note:<\/strong> Although I could use a Python kernel for this, I'd like to stick to R, because I'm more fluent with it than with Python, at least when it comes to dataframe crunching.<\/p>",
        "Challenge_closed_time":1586562277483,
        "Challenge_comment_count":1,
        "Challenge_created_time":1586299117960,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to read a .csv file stored in an s3 bucket using tidyverse in R, following the instructions provided in a GitHub repository. However, the user is encountering an error while reading the file, and the error message suggests that the file must be a string, raw vector, or a connection. The user is seeking guidance to resolve the issue and wants to stick to R for dataframe crunching.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61090530",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":32.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":73.0998675,
        "Challenge_title":"Using tidyverse to read data from s3 bucket",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1735.0,
        "Challenge_word_count":281,
        "Platform":"Stack Overflow",
        "Poster_created_time":1275433277096,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Mexico",
        "Poster_reputation_count":20017.0,
        "Poster_view_count":2754.0,
        "Solution_body":"<p>I'd recommend trying the <code>aws.s3<\/code> package instead:<\/p>\n\n<p><a href=\"https:\/\/github.com\/cloudyr\/aws.s3\" rel=\"nofollow noreferrer\">https:\/\/github.com\/cloudyr\/aws.s3<\/a><\/p>\n\n<p>Pretty simple - set your env variables:<\/p>\n\n<pre><code>Sys.setenv(\"AWS_ACCESS_KEY_ID\" = \"mykey\",\n           \"AWS_SECRET_ACCESS_KEY\" = \"mysecretkey\",\n           \"AWS_DEFAULT_REGION\" = \"us-east-1\",\n           \"AWS_SESSION_TOKEN\" = \"mytoken\")\n<\/code><\/pre>\n\n<p>and then once that is out of the way:<\/p>\n\n<p><code>aws.s3::s3read_using(read.csv, object = \"s3:\/\/bucket\/folder\/data.csv\")<\/code><\/p>\n\n<p>Update: I see you're also already familiar with boto and trying to use reticulate so leaving this easy wrapper for that here:\n<a href=\"https:\/\/github.com\/cloudyr\/roto.s3\" rel=\"nofollow noreferrer\">https:\/\/github.com\/cloudyr\/roto.s3<\/a><\/p>\n\n<p>Looks like it has a great api for example the variable layout you're aiming to use:<\/p>\n\n<pre><code>download_file(\n  bucket = \"is.rud.test\", \n  key = \"mtcars.csv\", \n  filename = \"\/tmp\/mtcars-again.csv\", \n  profile_name = \"personal\"\n)\n\nread_csv(\"\/tmp\/mtcars-again.csv\")\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1586740595876,
        "Solution_link_count":4.0,
        "Solution_readability":12.2,
        "Solution_reading_time":14.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":91.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1346946252340,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":136.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":105.3494969444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a CSV file I'm trying to RCF on.  If I put a date or string in the CSV then I get an error like the one below.  If I limit it to just the integer and float fields the script runs fine.  Is there some way to process dates and string?  I see the taxi example from AWS and it has dates which appear the same as mine<\/p>\n<pre><code>eventData = pd.read_csv(data_location, delimiter=&quot;,&quot;, header=None, parse_dates=True)\n\nprint('Starting RCF Training')\n# specify general training job information\nrcf = RandomCutForest(role=sagemaker.get_execution_role(),\n                      instance_count=1,\n                      instance_type='ml.m4.xlarge',\n                      data_location=data_location,\n                      output_path='s3:\/\/{}\/{}\/output'.format(bucket, prefix),\n                      base_job_name=&quot;ad-rcf&quot;,\n                      num_samples_per_tree=512,\n                      num_trees=50)\n\nrcf.fit(rcf.record_set(eventData.values))\n<\/code><\/pre>\n<p>CSV Data that fails<\/p>\n<pre><code>392507,1613744,1\/2\/2020 19:11,1577238693,2469,3.30E+01,-9.67E+01\n691381,1888551,12\/10\/2019 9:22,1575641745,3460,2.37E+01,9.04E+01\n392507,1613744,1\/2\/2020 19:20,1577236815,1797,3.30E+01,-9.67E+01\n392507,1613744,1\/29\/2020 19:04,1577264188,1797,3.30E+01,-9.67E+01\n<\/code><\/pre>\n<p>Error output<\/p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-35-ba19bf5d66a2&gt; in &lt;module&gt;\n---&gt; 21 rcf.fit(rcf.record_set(eventData.values))\n     22 \n     23 print('Done RCF Training')\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/amazon\/amazon_estimator.py in record_set(self, train, labels, channel, encrypt)\n    281         logger.debug(&quot;Uploading to bucket %s and key_prefix %s&quot;, bucket, key_prefix)\n    282         manifest_s3_file = upload_numpy_to_s3_shards(\n--&gt; 283             self.instance_count, s3, bucket, key_prefix, train, labels, encrypt\n    284         )\n    285         logger.debug(&quot;Created manifest file %s&quot;, manifest_s3_file)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/amazon\/amazon_estimator.py in upload_numpy_to_s3_shards(num_shards, s3, bucket, key_prefix, array, labels, encrypt)\n    443                 s3.Object(bucket, key_prefix + file).delete()\n    444         finally:\n--&gt; 445             raise ex\n    446 \n    447 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/amazon\/amazon_estimator.py in upload_numpy_to_s3_shards(num_shards, s3, bucket, key_prefix, array, labels, encrypt)\n    424                     write_numpy_to_dense_tensor(file, shard, label_shards[shard_index])\n    425                 else:\n--&gt; 426                     write_numpy_to_dense_tensor(file, shard)\n    427                 file.seek(0)\n    428                 shard_index_string = str(shard_index).zfill(len(str(len(shards))))\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/amazon\/common.py in write_numpy_to_dense_tensor(file, array, labels)\n    154             )\n    155         resolved_label_type = _resolve_type(labels.dtype)\n--&gt; 156     resolved_type = _resolve_type(array.dtype)\n    157 \n    158     # Write each vector in array into a Record in the file object\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/amazon\/common.py in _resolve_type(dtype)\n    288     if dtype == np.dtype(&quot;float32&quot;):\n    289         return &quot;Float32&quot;\n--&gt; 290     raise ValueError(&quot;Unsupported dtype {} on array&quot;.format(dtype))\n    291 \n    292 \n\nValueError: Unsupported dtype object on array\n<\/code><\/pre>",
        "Challenge_closed_time":1615349115252,
        "Challenge_comment_count":0,
        "Challenge_created_time":1614969857063,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a ValueError when trying to run a script on a CSV file using AWS Sagemaker. The error occurs when the CSV file contains dates or strings, but the script runs fine when limited to integer and float fields. The user is seeking a solution to process dates and strings.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66497968",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.6,
        "Challenge_reading_time":43.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":105.3494969444,
        "Challenge_title":"AWS Sagemaker ValueError: Unsupported dtype object on array when using strings and dates",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1395.0,
        "Challenge_word_count":274,
        "Platform":"Stack Overflow",
        "Poster_created_time":1346946252340,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":136.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>Figured out my issue, the RCF can't handle dates and strings.  There's this page for the Kenesis offering from AWS that covers the same Random Cut Forest algorithm <a href=\"https:\/\/docs.aws.amazon.com\/kinesisanalytics\/latest\/sqlref\/sqlrf-random-cut-forest.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/kinesisanalytics\/latest\/sqlref\/sqlrf-random-cut-forest.html<\/a>  It says the function only supports &quot;The algorithm accepts the DOUBLE, INTEGER, FLOAT, TINYINT, SMALLINT, REAL, and BIGINT data types.&quot;<\/p>\n<p>The gotcha part that AWS does with the NYC Taxi example is they use .value which is referring to only the value column of the data.  They are basically dropping the dates from the RCF as a feature.  It doesn't help that .values on the array does work and looks very similar to .value<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.4,
        "Solution_reading_time":10.49,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":106.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1489593596310,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":162.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":0.2549580556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am retrieving a list of image filenames from DynamoDB and using those image filenames to replace the default <code>src=<\/code> image in a portion of a website.<\/p>\n\n<p>I'm a JS novice, so I'm certainly missing something, but my function is returning the list of filenames too late.<\/p>\n\n<p>My inline script is:<\/p>\n\n<pre><code>&lt;script&gt;\n        customElements.whenDefined( 'crowd-bounding-box' ).then( () =&gt; {  \n        var imgBox = document.getElementById('annotator');\n        newImg = imageslist();\n        console.log(\"Result of newImg is: \" + newImg);\n        imgBox.src = \"https:\/\/my-images-bucket.s3.amazonaws.com\/\" + newImg;\n    } )\n&lt;\/script&gt;\n<\/code><\/pre>\n\n<p>My JS function is:<\/p>\n\n<pre><code>async function imageslist() {\n    const username = \"sampleuser\";\n    const params = {\n        TableName: \"mytable\",\n        FilterExpression: \"attribute_not_exists(\" + username + \")\",\n        ReturnConsumedCapacity: \"NONE\"\n    };\n    try {\n        var data = await ddb.scan(params).promise()\n        var imglist = [];\n        for(let i = 0; i &lt; data.Items.length; i++) {\n            imglist.push(data.Items[i].img.S);\n        };\n        imglist.sort();\n        var firstimg = imglist[0];\n        console.log(firstimg);\n        return imglist\n    } catch(error) {\n        console.error(error);\n    }\n}\n<\/code><\/pre>\n\n<p>The console reports <code>Result of newImg is: [object Promise]<\/code> and shortly after that, it reports the expected filename.  After the page has been rendered, I can input <code>newImg<\/code> in the console and I receive the expected result.<\/p>\n\n<p>Am I using the <strong>await<\/strong> syntax improperly?<\/p>\n\n<p>Side note: This site uses the Crowd HTML Elements (for Ground Truth and Mechanical Turk), so I'm forced to have the <code>src=<\/code> attribute present in my <code>crowd-bounding-box<\/code> tag and it must be a non-zero value.  I'm loading a default image and replacing it with another image.<\/p>",
        "Challenge_closed_time":1588365818456,
        "Challenge_comment_count":1,
        "Challenge_created_time":1588364900607,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is retrieving a list of image filenames from DynamoDB and using them to replace the default image in a portion of a website. However, the function is returning the list of filenames too late, and the console reports \"Result of newImg is: [object Promise].\" The user is unsure if they are using the await syntax improperly.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61550297",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":10.4,
        "Challenge_reading_time":22.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":0.2549580556,
        "Challenge_title":"Await returns too soon",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":63.0,
        "Challenge_word_count":208,
        "Platform":"Stack Overflow",
        "Poster_created_time":1483444144907,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hoth",
        "Poster_reputation_count":312.0,
        "Poster_view_count":65.0,
        "Solution_body":"<p>Anytime you use the <code>await<\/code> keyword, you must use the <code>async<\/code> keyword before the function definition (which you have done). The thing is, any time an async function is called, it will always return a <code>Promise<\/code> object since it expects that some asynchronous task will take place within the function.<\/p>\n\n<p>Therefore,you'll need to <code>await<\/code> the result of the imageslist function and make the surrounding function <code>async<\/code>.<\/p>\n\n<pre class=\"lang-js prettyprint-override\"><code>&lt;script&gt;\n    customElements.whenDefined( 'crowd-bounding-box' ).then( async () =&gt; {  \n        var imgBox = document.getElementById('annotator');\n        newImg = await imageslist();\n        console.log(\"Result of newImg is: \" + newImg);\n        imgBox.src = \"https:\/\/my-images-bucket.s3.amazonaws.com\/\" + newImg;\n    } )\n&lt;\/script&gt;\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.1,
        "Solution_reading_time":10.99,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":90.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1345114008840,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Lyon, France",
        "Answerer_reputation_count":4233.0,
        "Answerer_view_count":151.0,
        "Challenge_adjusted_solved_time":58.2732816667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Lately i've been testing Azure Machine Learning, and i like it. However, when i try to transform my dataset, there's a step that i can't perform easily : replacing a specific value in a column by another one.<\/p>\n\n<p>The <code>Missing Values Scrubber<\/code> module allows me to deal with undefined values, but in my case i need to change a specific value, or remove rows where that value appears. I don't see which module meets my requirement.<\/p>\n\n<p>Do you have any suggestion about this issue ? <\/p>",
        "Challenge_closed_time":1412637208267,
        "Challenge_comment_count":0,
        "Challenge_created_time":1412427424453,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having difficulty replacing a specific value in a column of their dataset using Azure Machine Learning. They have tried using the \"Missing Values Scrubber\" module but it only deals with undefined values. They are seeking suggestions for a module that can help them replace or remove rows with a specific value.",
        "Challenge_last_edit_time":1446191259743,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/26193051",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":6.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":58.2732816667,
        "Challenge_title":"Replacing specific values in dataset with Azure ML",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":3772.0,
        "Challenge_word_count":91,
        "Platform":"Stack Overflow",
        "Poster_created_time":1345114008840,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lyon, France",
        "Poster_reputation_count":4233.0,
        "Poster_view_count":151.0,
        "Solution_body":"<p>I found a solution <a href=\"http:\/\/social.msdn.microsoft.com\/Forums\/en-US\/bf8f76c7-f976-4552-8553-8e54133ff2c6\/replacing-specific-values-in-dataset-with-azure-ml?forum=MachineLearning\" rel=\"nofollow\">there<\/a>, by using a <code>Convert to Dataset<\/code> module.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":29.6,
        "Solution_reading_time":3.75,
        "Solution_score_count":4.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":14.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426675778223,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Coimbatore, Tamil Nadu, India",
        "Answerer_reputation_count":10189.0,
        "Answerer_view_count":1471.0,
        "Challenge_adjusted_solved_time":19.2195019445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following this <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_neo_compilation_jobs\/gluoncv_ssd_mobilenet\/gluoncv_ssd_mobilenet_neo.ipynb\" rel=\"nofollow noreferrer\">link<\/a> to train an object detection model. I am able to successfully deploy the model on EC2 instance. The accuracy was good. I complied the same model file for m edge Device Jetson Nano. My inference code looks like below,<\/p>\n<pre><code>from dlr import DLRModel\nimport json\nimport cv2\n\nmodel = DLRModel('model', 'gpu')\n\nimg = cv2.imread('test.jpg')\nimg = cv2.resize(img, (512, 512), interpolation=cv2.INTER_AREA)\nimg = np.expand_dims(img, 0)\n\noutputs = model.run(img)\nobjects=outputs[0][0]\nscores=outputs[1][0]\nbounding_boxes=outputs[2][0]\n<\/code><\/pre>\n<p>When I look the result, it's not at all matched with SageMaker Notebook instance result. Boudning boxes' values are sometime in ~70000. I couldn't understand the format of result produced by DLR.<\/p>\n<p>Sample result for an image.<\/p>\n<p>Classes:\n[[10.0], [14.0], [4.0], [10.0], [14.0], [-1.0], [-1.0], [-1.0], [7.0], [-1.0], [6.0], [11.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [2.0], [-1.0], [-1.0], [0.0], [-1.0], [17.0], [-1.0], [-1.0], [6.0], [18.0], [-1.0], [-1.0], [-1.0], [18.0], [-1.0], [12.0], [-1.0], [-1.0], [13.0], [-1.0], [-1.0], [-1.0], [1.0], [-1.0], [-1.0], [5.0], [-1.0], [0.0], [-1.0], [-1.0], [-1.0], [-1.0], [3.0], [8.0], [5.0], [-1.0], [-1.0], [15.0], [-1.0], [9.0], [3.0], [-1.0], [10.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [16.0], [8.0], [-1.0], [16.0], [19.0], [-1.0], [9.0], [-1.0], [4.0], [-1.0], [-1.0], [15.0], [10.0], [-1.0], [-1.0], [4.0], [-1.0], [8.0], [2.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0]]<\/p>\n<p>scores:\n[[0.9527158737182617], [0.910746157169342], [0.28013306856155396], [0.059000786393880844], [0.04898739233613014], [-1.0], [-1.0], [-1.0], [0.04864144325256348], [-1.0], [0.04847110062837601], [0.04843416064977646], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [0.04821664094924927], [-1.0], [-1.0], [0.04808368161320686], [-1.0], [0.0479729063808918], [-1.0], [-1.0], [0.04782549664378166], [0.04778601974248886], [-1.0], [-1.0], [-1.0], [0.0475776381790638], [-1.0], [0.047538403421640396], [-1.0], [-1.0], [0.047468967735767365], [-1.0], [-1.0], [-1.0], [0.04737424850463867], [-1.0], [-1.0], [0.047330085188150406], [-1.0], [0.04730956256389618], [-1.0], [-1.0], [-1.0], [-1.0], [0.04710235074162483], [0.04710135608911514], [0.047083333134651184], [-1.0], [-1.0], [0.047033149749040604], [-1.0], [0.0469636432826519], [0.046939317137002945], [-1.0], [0.04687687009572983], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [0.046708278357982635], [0.046680934727191925], [-1.0], [0.04660974070429802], [0.046597886830568314], [-1.0], [0.04656397923827171], [-1.0], [0.046513814479112625], [-1.0], [-1.0], [0.04647510126233101], [0.04644943028688431], [-1.0], [-1.0], [0.046245038509368896], [-1.0], [0.01647786796092987], [0.010514501482248306], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0]]<\/p>\n<p>Bouding boxes:\n[[523.2319946289062, -777751.875, 523.5722045898438, 778992.625], [425.0546875, -1141093.0, 429.6099853515625, 1142524.5], [680.2073364257812, 542.9566650390625, 680.2202758789062, 543.36376953125], [425.0546875, -1141093.0, 429.6099853515625, 1142524.5], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [591.7652587890625, -1121890.0, 592.9493408203125, 1123299.75], [523.2319946289062, -777751.875, 523.5722045898438, 778992.625], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0]]<\/p>\n<p>What causes this notorious result?\nIs there any issue while compiling model in Neo?\nAny issue in inference Code?<\/p>\n<p>Any hint would be appreciable.<\/p>",
        "Challenge_closed_time":1609912383447,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609843193240,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"the user is encountering challenges with their object detection model, as the results produced by the dlr model are not matching the results from the notebook instance, with bounding boxes values sometimes reaching ~70000.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65577286",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":143.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":19.2195019445,
        "Challenge_title":"Dlr model gives notorious result for object detection model",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":98.0,
        "Challenge_word_count":747,
        "Platform":"Stack Overflow",
        "Poster_created_time":1426675778223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Coimbatore, Tamil Nadu, India",
        "Poster_reputation_count":10189.0,
        "Poster_view_count":1471.0,
        "Solution_body":"<p>The reason behind the abnormal result is due to improper pre-processing method was applied. Here is the complete inference code for Mobilenet-ssd model.<\/p>\n<pre><code>def transform(img):\n    # Normalize\n    mean_vec = np.array([0.485, 0.456, 0.406])\n    stddev_vec = np.array([0.229, 0.224, 0.225])\n    image = (img \/ 255 - mean_vec) \/ stddev_vec\n\n    # Transpose\n    if len(image.shape) == 2:  # for greyscale image\n        image = np.expand_dims(image, axis=2)\n\n    image = np.rollaxis(image, axis=2, start=0)[np.newaxis, :]\n    return image\n\nmodel = DLRModel(model_dir, 'gpu')\n\n\nfor file_name in image_folder:\n    image = PIL.Image.open(file_name)\n    image = np.asarray(image.resize((512, 512)))\n    image = transform(image)\n    # flatten within a input array\n    input_data = {'data': image}\n    outputs = model.run(input_data)\n    objects = outputs[0]\n    scores = outputs[1]\n    bounding_boxes = outputs[2]\n    result = [objects.tolist(), scores.tolist(), bounding_boxes.tolist()]\n    print(json.dumps(result))\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.2,
        "Solution_reading_time":12.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":91.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":103.2534313889,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I was training a yolov5 model, using the pre-configured wandb settings. But the weights weren\u2019t uploaded because the session was killed. I tried <code>wandb sync path\/to\/run<\/code> but the model file didn\u2019t get synced.<\/p>\n<p>I want to upload the resulting <code>best.pt<\/code> file to the artifacts regardless without messing up with the current summary and results of the finished run. I looked up in the documentation and tried multiple guides but couldn\u2019t manage to do that.<\/p>\n<p>TL;DR: I have a finished run and a weights file. I need to upload the weights file as a model artifact to that finished run using the run path.<\/p>",
        "Challenge_closed_time":1654587679840,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654215967487,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue while trying to upload model weights to the Artifacts of a finished run using pre-configured wandb settings. The session was killed and the weights were not uploaded. The user tried to sync the file but it didn't work. The user wants to upload the resulting best.pt file to the artifacts without affecting the current summary and results of the finished run. The user is seeking guidance on how to upload the weights file as a model artifact to the finished run using the run path.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/upload-model-weights-to-the-artifacts-of-a-finished-run\/2540",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":6.9,
        "Challenge_reading_time":8.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":103.2534313889,
        "Challenge_title":"Upload model weights to the Artifacts of a finished run",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":235.0,
        "Challenge_word_count":112,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey <a class=\"mention\" href=\"\/u\/alyetama\">@alyetama<\/a>, here is a code snippet you can use: <a href=\"https:\/\/docs.wandb.ai\/guides\/artifacts\/artifacts-faqs#how-do-i-log-an-artifact-to-an-existing-run\">https:\/\/docs.wandb.ai\/guides\/artifacts\/artifacts-faqs#how-do-i-log-an-artifact-to-an-existing-run<\/a><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":41.8,
        "Solution_reading_time":4.35,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":14.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":59.0207758333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>My dataset now has enough samples to start pre-labeling a set of labels (bounding boxes for image identification).  <\/p>\n<p>However, rather worryingly this seems fundamentally broken?   <br \/>\nWe appear to have lost the ability to zoom the image (zoom just appears to zoom the bounding boxes, and not the underlying image) which basically makes this entire functionality useless.  <\/p>\n<p>Am I missing something or is this feature completely broken?  <br \/>\nI hope the former, as the pre-labeling was a significant factor in choosing this platform.  <\/p>\n<p>We have tried multiple browsers in case this was a browser issue but to no success, they all present the same issue.  <\/p>\n<p>Is anyone able to advise??  <\/p>",
        "Challenge_closed_time":1616398055640,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616185580847,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with Azure Machine Learning Data Labelling where the zoom feature is not working properly on pre-labelled tasks. This makes the pre-labelling functionality useless and the user is seeking advice on how to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/323305\/azure-machine-learning-data-labelling-zoom-broken",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":9.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":59.0207758333,
        "Challenge_title":"Azure Machine Learning Data Labelling - Zoom broken on prelabelled tasks??",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":124,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=06d543d4-352b-4f24-bd0a-512b74834f7c\">@ChrisH  <\/a> Thanks for the question. Can you please share image and snapshot for the same. We are able to zoom the underlying image using the <a href=\"http:\/\/ml.azure.com\">data labeling<\/a>.     <br \/>\n<img src=\"\/answers\/storage\/temp\/80116-image.png\" alt=\"80116-image.png\" \/>    <br \/>\nPlease follow the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-label-images#tag-images-and-specify-bounding-boxes-for-object-detection\">doc<\/a> to Tag images and specify bounding boxes for object detection.    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.9,
        "Solution_reading_time":7.84,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":51.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1558713599190,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":58.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":45.7170236111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>we are running a Spark job against our Kubernetes cluster and try to log the model to MLflow. We are running Spark 3.2.1 and MLflow 1.26.1 and we are using the following jars to communicate with s3 <code>hadoop-aws-3.2.2.jar<\/code> and <code>aws-java-sdk-bundle-1.11.375.jar<\/code> and configure our spark-submit job with the following parameters:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>  --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider \\\n  --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\\n  --conf spark.hadoop.fs.s3a.fast.upload=true \\\n<\/code><\/pre>\n<p>When we try to save our Spark model with <code>mlflow.spark.log_model()<\/code> we are getting the following exception:<\/p>\n<pre class=\"lang-java prettyprint-override\"><code>22\/06\/24 13:27:21 ERROR Instrumentation: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme &quot;s3&quot;\n    at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n    at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n    at org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:673)\n    at org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n    at org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n    at org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n    at org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n    at org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n    at org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n    at org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n    at org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n    at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n    at scala.util.Try$.apply(Try.scala:213)\n    at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n    at org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n    at java.base\/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at java.base\/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n    at java.base\/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n    at java.base\/java.lang.reflect.Method.invoke(Unknown Source)\n    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n    at py4j.Gateway.invoke(Gateway.java:282)\n    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)\n    at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n    at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n    at java.base\/java.lang.Thread.run(Unknown Source)\n<\/code><\/pre>\n<p>We tried to start our MLflow server with <code>-default-artifact-root<\/code> set to <code>s3a:\/\/...<\/code> but when we run our spark job and we call <code>mlflow.get_artifact_uri()<\/code> (which is also used to construct the upload uri in <code>mlflow.spark.log_model()<\/code>) the result starts with <code>s3<\/code> which probably cause the former mentioned exception.\nSince Hadoop dropped support for the <code>s3:\/\/<\/code> filesystem does anyone know how to log spark models to s3 using MLflow?<\/p>\n<p>Cheers<\/p>",
        "Challenge_closed_time":1656330551532,
        "Challenge_comment_count":1,
        "Challenge_created_time":1656078725497,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an exception \"No FileSystem for scheme 's3'\" when attempting to save a Spark model to MLflow using jars to communicate with s3 and configuring spark-submit job with specific parameters. The MLflow server was started with \"-default-artifact-root\" set to \"s3a:\/\/...\" but calling \"mlflow.get_artifact_uri()\" results in a uri starting with \"s3\" which may be causing the exception. The user is seeking a solution to log Spark models to s3 using MLflow.",
        "Challenge_last_edit_time":1656165970247,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72745109",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":24.1,
        "Challenge_reading_time":54.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":48,
        "Challenge_solved_time":69.9516763889,
        "Challenge_title":"No FileSystem for scheme \"s3\" exception when using spark with mlflow",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":148.0,
        "Challenge_word_count":234,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442649179160,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":773.0,
        "Poster_view_count":58.0,
        "Solution_body":"<p>Additional to the <code>spark.hadoop.fs.s3a.impl<\/code> config parameter, you can try to also set <code>spark.hadoop.fs.s3.impl<\/code> to <code>org.apache.hadoop.fs.s3a.S3AFileSystem<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.3,
        "Solution_reading_time":2.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":15.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11.5141666667,
        "Challenge_answer_count":2,
        "Challenge_body":"Is there any standard for ML S3 dataset tracking or versioning?\nBasically, what setup allows to track a given model training execution to a given dataset?\nInterested to hear about proven or state-of-the-art ideas",
        "Challenge_closed_time":1549437509000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1549396058000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a standard or setup that allows for tracking or versioning of S3 datasets used in machine learning model training. They are seeking proven or state-of-the-art ideas for this purpose.",
        "Challenge_last_edit_time":1668615929200,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUhYC1EJQuSWqpwTByAtB_fg\/s3-dataset-versioning-with-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":3.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":11.5141666667,
        "Challenge_title":"S3 Dataset versioning with SageMaker?",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":775.0,
        "Challenge_word_count":38,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Unfortunately, managing versions of datasets and which models used them is not embedded in SageMaker. But, you can use SageMaker search to manage the differences in data location between experiments. In that case, if your dataset isn't too big, my recommendation will be to create a standard for data structure in S3. i.e. for each new dataset, create a new prefix in S3 with your logic.\nUsing SageMaker search you'll be able to find all your jobs and compare between datasets.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925558875,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":5.83,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":81.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1452004052636,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Buenos Aires, Argentina",
        "Answerer_reputation_count":606.0,
        "Answerer_view_count":168.0,
        "Challenge_adjusted_solved_time":72.5592313889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi I have a web service which is the result of my machine learning azure training. I would like to set a new datasource in power bi, which calls the web service with the current datetime as a parameter in order to create a report with the result predictions. I cannot find a way to call the api. Is this any? I am thinking another solution of creating a service and execute the api, and insert the result in a table in order to connect to this table. But, I would like to avoid doing this.<\/p>",
        "Challenge_closed_time":1463150592243,
        "Challenge_comment_count":0,
        "Challenge_created_time":1462889379010,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in setting a new datasource in Power BI that calls a web service with the current datetime as a parameter to create a report with the result predictions from their machine learning Azure training. They are unable to find a way to call the API and are considering creating a service to execute the API and insert the result in a table to connect to it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37140987",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.7,
        "Challenge_reading_time":6.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":72.5592313889,
        "Challenge_title":"power by dashboard with machine learning azure data",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":444.0,
        "Challenge_word_count":103,
        "Platform":"Stack Overflow",
        "Poster_created_time":1452004052636,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Buenos Aires, Argentina",
        "Poster_reputation_count":606.0,
        "Poster_view_count":168.0,
        "Solution_body":"<p>I used something called Azure Data Factory (ADF). It allows you to schedule a job by defining a pipeline with activities. There are activities for training your model or scoring your predictive ML. The scoring result, I am storing it in Azure DB (it could be another storage) and connected it to Power BI.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.4,
        "Solution_reading_time":3.8,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":67.9020225,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>We can use <code>wandb.watch(model, criterion, ...)<\/code> in order to log a model + a loss function.<br>\nBut my loss function is not something simple like: <code>criterion = nn.CrossEntropyLoss()<\/code>.<\/p>\n<p>Rather, here\u2019s how I calculate my loss:<\/p>\n<pre><code class=\"lang-auto\">            # `set_to_none=True` boosts performance\n            optimizer.zero_grad(set_to_none=True)\n            masks_pred = model(imgs)\n\n            probs = F.softmax(masks_pred, dim=1).float()\n            ground_truth = F.one_hot(masks, model.n_classes).permute(0, 3, 1, 2).float()\n\n            loss = criterion(masks_pred, masks) + dice_loss(probs, ground_truth)\n            loss.backward()\n            optimizer.step()\n<\/code><\/pre>\n<p>As you can see, the loss is a composition of 2 functions: the criterion and the <code>dice_loss<\/code> function.<br>\nWhat should I pass to <code>wandb.watch<\/code> for the <code>criterion<\/code> argument?<\/p>",
        "Challenge_closed_time":1657306434860,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657061987579,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to log a custom loss function using wandb.watch() but is unsure of what to pass as the criterion argument since their loss function is a composition of two functions.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-log-custom-criterion-function\/2703",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":9.8,
        "Challenge_reading_time":11.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":67.9020225,
        "Challenge_title":"How to log custom criterion function?",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":116.0,
        "Challenge_word_count":91,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/vroomerify\">@vroomerify<\/a>,<\/p>\n<p>Thanks for reaching out. <code>wandb.watch<\/code> expects a torch function as a criterion parameter. You can set up a custom criterion function by subclassing <code>torch.nn.Module<\/code>.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.1,
        "Solution_reading_time":3.79,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":30.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1577919980176,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hamburg, Germany",
        "Answerer_reputation_count":5588.0,
        "Answerer_view_count":398.0,
        "Challenge_adjusted_solved_time":9.1191208333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm a newbie in Sagemaker and i'm trying to load a pickle dataset into sagemaker notebook.\nI'm using the Python 3 (Data Science) kernel and ml.t3.medium instance.\nEither i load the pickle from S3 or I upload it directly from the studio like this:<\/p>\n<pre><code>import pickle5\nwith open('filename', 'rb') as f:\n    x = pickle.load(f)\n<\/code><\/pre>\n<p><strong>I get this Error:<\/strong><\/p>\n<pre><code>---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n\/opt\/conda\/lib\/python3.7\/site-packages\/IPython\/core\/formatters.py in __call__(self, obj)\n    700                 type_pprinters=self.type_printers,\n    701                 deferred_pprinters=self.deferred_printers)\n--&gt; 702             printer.pretty(obj)\n    703             printer.flush()\n    704             return stream.getvalue()\n\n..................... more errors here\n\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pandas\/core\/generic.py in __getattr__(self, name)\n   5268             or name in self._accessors\n   5269         ):\n-&gt; 5270             return object.__getattribute__(self, name)\n   5271         else:\n   5272             if self._info_axis._can_hold_identifiers_and_holds_name(name):\n\npandas\/_libs\/properties.pyx in pandas._libs.properties.AxisProperty.__get__()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pandas\/core\/generic.py in __getattr__(self, name)\n   5268             or name in self._accessors\n   5269         ):\n-&gt; 5270             return object.__getattribute__(self, name)\n   5271         else:\n   5272             if self._info_axis._can_hold_identifiers_and_holds_name(name):\n\nAttributeError: 'DataFrame' object has no attribute '_data'\n<\/code><\/pre>",
        "Challenge_closed_time":1620025682632,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619992853797,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to load a pickle dataset into Sagemaker notebook using Python 3 (Data Science) kernel and ml.t3.medium instance. The user is getting an AttributeError stating that the 'DataFrame' object has no attribute '_data'.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67361483",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":20.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":9.1191208333,
        "Challenge_title":"AWS Sagemaker Studio, cannot load pickle files",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":237.0,
        "Challenge_word_count":141,
        "Platform":"Stack Overflow",
        "Poster_created_time":1553704286212,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":27.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Can you check your Pandas versions? This error typically occurs when the pickled file was written in an old Pandas version. Your Sagemaker notebook probably runs Pandas &gt; 1.1 where as the Pandas in which the dataframe was pickled is probably &lt; 1.1<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.3,
        "Solution_reading_time":3.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":43.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10.8830108333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,  <\/p>\n<p>I've started a new Data labeling project in Azure Machine Learning and I configured the incremental refresh.   <\/p>\n<p>How often is the data refreshed? Is it possible to force a refresh manually? Is it possible to execute this command via SDK (Python or PowerShell)?  <\/p>\n<p>Thanks.  <\/p>\n<p>G<\/p>",
        "Challenge_closed_time":1636424689196,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636385510357,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information about the data refreshing process in Azure Machine Learning's data labeling project, including the frequency of automatic refreshes, the possibility of manual refreshes, and the ability to execute refresh commands through Python or PowerShell SDK.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/619198\/azure-machine-learning-data-labeling-refresh",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.8,
        "Challenge_reading_time":4.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":10.8830108333,
        "Challenge_title":"Azure Machine Learning - Data Labeling - Refresh",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":55,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, data is <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-image-labeling-projects#--configure-incremental-refresh\">refreshed<\/a> within 24hrs. Currently, incremental refresh is only enabled using the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-image-labeling-projects#details-tab\">portal<\/a> and there's no option to trigger refresh manually.<\/p>\n<hr \/>\n<p>--- *Kindly <em><strong>Accept Answer<\/strong><\/em> if the information helps. Thanks.*<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":22.5,
        "Solution_reading_time":7.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":35.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":91.9480555556,
        "Challenge_answer_count":0,
        "Challenge_body":"'m going through this notebook: https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training\/train-on-remote-vm\/train-on-remote-vm.ipynb\r\n\r\nI need to start the training using the docker image from my local registry. I provided all required data in the environment I created:\r\n\r\nconda_env.docker.enabled = True\r\nconda_env.docker.base_image = \"tf_od_api:latest\"\r\nconda_env.docker.base_image_registry.address = \"mylocalacr.azurecr.io\"\r\nconda_env.docker.base_image_registry.username = \"MyToken\"\r\nconda_env.docker.base_image_registry.password = \"MyPassword\"\r\n\r\nconda_env.python.user_managed_dependencies = True\r\n\r\nsrc = ScriptRunConfig(source_directory='azureml-examples\/workflows\/train\/fastai\/pets\/src',\r\n                      script='aml_wrapper.py',\r\n                      compute_target=attached_dsvm_compute,\r\n                      environment=conda_env)\r\nrun = exp.submit(config=src)\r\nrun.wait_for_completion(show_output=True)\r\n\r\nAnd when I start the pipeline I got: \"FailedPullingImage: Unable to pull docker image\\n\\timageName: Run docker command to pull public image failed with error: error response from daemon: unauthorized: authentication required\"\r\n\r\nIf I set conda_env.python.user_managed_dependencies = False\r\n\r\nthen the pipeline can pull my image from my local registry, build a new image with all required python dependencies on top of my base image and push the new image to my local registry. But on the second step of the pipeline, when it tries to pull the image for running it, that was just created and pushed, it again crashes with the same error: \"Run docker command to pull public image failed with error: error response from daemon: unauthorized: authentication required\"",
        "Challenge_closed_time":1614604742000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1614273729000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"the user encountered a challenge when attempting to upload data to a datastore, resulting in an \"authorization permission mismatch\" error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1371",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":14.6,
        "Challenge_reading_time":21.99,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":91.9480555556,
        "Challenge_title":"Azure ML Run docker command to pull public image failed ",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":179,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"please try this \r\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1247#issuecomment-738887772 Seems like it solved the issue. Thanks!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.7,
        "Solution_reading_time":1.91,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1551701850312,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Poland, Cracow",
        "Answerer_reputation_count":11273.0,
        "Answerer_view_count":1888.0,
        "Challenge_adjusted_solved_time":340.1792388889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to create a python utility that will take dataset from vertex ai datasets and will generate statistics for that dataset. But I am unable to check the dataset using jupyter notebook. Is there any way out for this?<\/p>",
        "Challenge_closed_time":1631634873007,
        "Challenge_comment_count":3,
        "Challenge_created_time":1630410227747,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing difficulty in accessing a dataset from Vertex AI in Jupyter Notebook and is seeking a solution to this problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68998065",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":4.4,
        "Challenge_reading_time":3.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":340.1792388889,
        "Challenge_title":"Read vertex ai datasets in jupyter notebook",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":737.0,
        "Challenge_word_count":47,
        "Platform":"Stack Overflow",
        "Poster_created_time":1616070829583,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":45.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>If I understand correctly, you want to use <a href=\"https:\/\/cloud.google.com\/vertex-ai\" rel=\"nofollow noreferrer\">Vertex AI<\/a> dataset inside <code>Jupyter Notebook<\/code>. I don't think that this is currently possible. You are able to export <code>Vertex AI<\/code> datasets to <code>Google Cloud Storage<\/code> in JSONL format:<\/p>\n<blockquote>\n<p>Your dataset will be exported as a list of text items in JSONL format. Each row contains a Cloud Storage path, any label(s) assigned to that item, and a flag that indicates whether that item is in the training, validation, or test set.<\/p>\n<\/blockquote>\n<p>At this moment, you can use <code>BigQuery<\/code> data inside <code>Notebook<\/code> using <code>%%bigquery<\/code> like it's mentioned in <a href=\"https:\/\/cloud.google.com\/bigquery\/docs\/visualize-jupyter\" rel=\"nofollow noreferrer\">Visualizing BigQuery data in a Jupyter notebook.<\/a> or use <code>csv_read()<\/code> from machine directory or <code>GCS<\/code> like it's showed in the <a href=\"https:\/\/stackoverflow.com\/questions\/61956470\/\">How to read csv file in Google Cloud Platform jupyter notebook<\/a> thread.<\/p>\n<p>However, you can fill a <code>Feature Request<\/code> in <a href=\"https:\/\/developers.google.com\/issue-tracker\" rel=\"nofollow noreferrer\">Google Issue Tracker<\/a> to add the possibility to use <code>VertexAI<\/code> dataset directly in the <code>Jupyter Notebook<\/code> which will be considered by the <code>Google Vertex AI Team<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.8,
        "Solution_reading_time":19.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":174.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":21.5194444444,
        "Challenge_answer_count":0,
        "Challenge_body":"```\r\nAttributeError: 'DatabaseServiceMetadataPipeline' object has no attribute 'mlModelFilterPattern'\r\n```\r\n\r\nWe need to review which configuration param is being sent here",
        "Challenge_closed_time":1662467440000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1662389970000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The MLFlowLogging is always disabled for training FARMReader models, which prevents the logging of training statistics and metrics to MLFlow. The issue arises due to the initialization of an Inferencer that disables all logging to MLFlow. A workaround is to manually set MLFlowLogger.disable_logging to False before calling the train method.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/open-metadata\/OpenMetadata\/issues\/7232",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":2.53,
        "Challenge_repo_contributor_count":126.0,
        "Challenge_repo_fork_count":360.0,
        "Challenge_repo_issue_count":9147.0,
        "Challenge_repo_star_count":1692.0,
        "Challenge_repo_watch_count":21.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":21.5194444444,
        "Challenge_title":"Mlflow UI deployment error",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":22,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"sourceConfig type missing to be sent from the UI",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.3,
        "Solution_reading_time":0.59,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":9.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":71.3592933333,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I can only draw image with [wandb.Image(Numpy.array()),] like this<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/discourse.aicrowd.com\/t\/maskrcnn-integrated-with-wandb-and-direct-submit-from-colab\/3961\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b48aa9bfc94603e638f56ff8452ed88b900f00db.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/discourse.aicrowd.com\/t\/maskrcnn-integrated-with-wandb-and-direct-submit-from-colab\/3961\" target=\"_blank\" rel=\"noopener nofollow ugc\" title=\"11:54AM - 20 November 2020\">AIcrowd Forum \u2013 20 Nov 20<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:600\/325;\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/e05a631c976b165047261523c356b3fa7e5eab41.gif\" class=\"thumbnail animated\" width=\"600\" height=\"325\"><\/div>\n\n<h3><a href=\"https:\/\/discourse.aicrowd.com\/t\/maskrcnn-integrated-with-wandb-and-direct-submit-from-colab\/3961\" target=\"_blank\" rel=\"noopener nofollow ugc\">MaskRCNN integrated with WandB and DIRECT SUBMIT FROM COLAB!<\/a><\/h3>\n\n  <p>Hi everyone!    @rohitmidha23 and me have been following this challenge for quite a while. We have written a starter notebook using MaskRCNN. We further integrate MaskRCNN with WandB which really helps to keep track of the various experiments that...<\/p>\n\n  <p>\n    <span class=\"label1\">Reading time: 1 mins \ud83d\udd51<\/span>\n      <span class=\"label2\">Likes: 17 \u2764<\/span>\n  <\/p>\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n<p>\nBut how can I draw many images with a bar like this<br>\n<a href=\"https:\/\/sooftware.io\/static\/fd6ffa741fe53de299a57e6a8852f68d\/f312c\/wandb_image.webp\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https:\/\/sooftware.io\/static\/fd6ffa741fe53de299a57e6a8852f68d\/f312c\/wandb_image.webp<\/a><\/p>",
        "Challenge_closed_time":1652432634120,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652175740664,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a challenge in drawing multiple images with a bar using wandb.Image(Numpy.array()). They are seeking guidance on how to achieve this.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-draw-many-images-with-a-bar\/2391",
        "Challenge_link_count":7,
        "Challenge_participation_count":3,
        "Challenge_readability":17.9,
        "Challenge_reading_time":26.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":71.3592933333,
        "Challenge_title":"How to draw many images with a bar",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":186.0,
        "Challenge_word_count":144,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Oh\uff0cyes! I got it ,the step slider.<br>\nIt\u2019s on the left top of my panel. Thanks!<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/7477d027660f227b355c1b7090095a0ca0e72264.png\" alt=\"FireShot Capture 043 - warm-sea-50 - deepfillv2_512x512_dv5_0pv8_1 \u2013 Weights &amp; Biases_ - 192.168.23.40\" data-base62-sha1=\"gCk5gzKgcCCqUAxrDVgTMn9CtF2\" width=\"274\" height=\"249\"><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.7,
        "Solution_reading_time":5.42,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":30.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1386098048127,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":619.0,
        "Answerer_view_count":25.0,
        "Challenge_adjusted_solved_time":1733.7563858333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am afraid that my Neural Network in MXNet, written in Python, has a memory leak. I have tried the MXNet profiler and the tracemalloc module to get an understanding of memory profiling, but I want to get information on any potential memory leaks, just like I'd do with valgrind in C.<\/p>\n\n<p>I found <a href=\"https:\/\/cwiki.apache.org\/confluence\/display\/MXNET\/Detecting+Memory+Leaks+and+Buffer+Overflows+in+MXNet\" rel=\"nofollow noreferrer\">Detecting Memory Leaks and Buffer Overflows in MXNet<\/a>, and after managing to build like described in section \"Using ASAN builds with MXNet\", by replacing the \"ubuntu_cpu\" part in <code>docker\/Dockerfile.build.ubuntu_cpu -t mxnetci\/build.ubuntu_cpu<\/code> with \"ubuntu_cpu_python\", I tried executing in an AWS Sagemaker Notebook like this:<\/p>\n\n<pre><code>root@33e38e00f825:\/work\/mxnet# nosetests3 --verbose \/home\/ec2-user\/SageMaker\/run_predict.py\n<\/code><\/pre>\n\n<p>and I get this import error:<\/p>\n\n<blockquote>\n  <p>Failure: ImportError (No module named 'run_predict') ... ERROR<\/p>\n<\/blockquote>\n\n<p>My run_predict.py looks like this:<\/p>\n\n<pre><code>#!\/usr\/bin\/env python\ndef run_predict(n):\n  # calling MXNet inference method\n\nrun_predict(-1)  # tried it putting it under 'if __name__ == \"__main__\":'\n<\/code><\/pre>\n\n<p>What I am missing in my script, what should I change?<\/p>\n\n<p>The example script they use in the link is <a href=\"https:\/\/github.com\/apache\/incubator-mxnet\/blob\/faccd91071cc34ed0b3a192d3c7932441fe7e35e\/tests\/python\/unittest\/test_rnn.py\" rel=\"nofollow noreferrer\">rnn_test.py<\/a>, but even when I run this example, I still get an analogous Import Error.<\/p>",
        "Challenge_closed_time":1598820674152,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592493163510,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to find a memory leak in their Neural Network in MXNet, written in Python. They have tried using the MXNet profiler and the tracemalloc module but want to get information on potential memory leaks. They found a guide on detecting memory leaks and buffer overflows in MXNet and tried to execute a script in an AWS Sagemaker Notebook but encountered an import error. They are seeking help to identify what they are missing in their script and what needs to be changed.",
        "Challenge_last_edit_time":1592579151163,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62453292",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.9,
        "Challenge_reading_time":21.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":1757.641845,
        "Challenge_title":"How to find memory leak in Python MXNet?",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":263.0,
        "Challenge_word_count":188,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369257942212,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":70285.0,
        "Poster_view_count":13121.0,
        "Solution_body":"<p>In MXNet, we automatically test for this through examining the garbage collection records. You can find how it's implemented here: <a href=\"https:\/\/github.com\/apache\/incubator-mxnet\/blob\/c3aff732371d6177e5d522c052fb7258978d8ce4\/tests\/python\/conftest.py#L26-L79\" rel=\"nofollow noreferrer\">https:\/\/github.com\/apache\/incubator-mxnet\/blob\/c3aff732371d6177e5d522c052fb7258978d8ce4\/tests\/python\/conftest.py#L26-L79<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.7,
        "Solution_reading_time":5.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":24.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.7333333333,
        "Challenge_answer_count":1,
        "Challenge_body":"Customer wants to configure the SageMaker Ground Truth interface seen by the workers such that the labeler can navigate to previous or next tasks. For example, if one is labelling images, they could skip the current image, label the next one, and then return to the skipped image. The Ground Truth interface does not seem to have this capability. Is there an option for it that I missed? I could not find anything about it here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-labeling.html.",
        "Challenge_closed_time":1596058119000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596055479000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge with the SageMaker Ground Truth interface as it does not provide an option to skip a task and then return to it later. The user wants to configure the interface to allow labelers to navigate to previous or next tasks. However, the user could not find any information about this option in the documentation.",
        "Challenge_last_edit_time":1668591869336,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU_S8ylg4UQdKh76o3zp3dWQ\/sagemaker-groundtruth-interface-option-to-skip-a-task-and-then-return",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":7.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.7333333333,
        "Challenge_title":"SageMaker GroundTruth Interface - option to skip a task and then return",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":139.0,
        "Challenge_word_count":87,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Currently, there is no functionality to skip a task and go back to it later. However, you could add a field like\n\n `[ ] this task was skipped` \n\nwhere the annotator could check the box for those items to be reviewed and processed at another time. ",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1612484096992,
        "Solution_link_count":0.0,
        "Solution_readability":9.5,
        "Solution_reading_time":2.89,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":44.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0527777778,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nI am using the python run client and am trying to use 'pending' correctly. Here is an example of my code:\n\nclient = RunClient(owner=\"owner\", project=\"project\")\nclient.create(content=operation, pending='upload')\nclient.upload_artifacts_dir('.\/')\n\nI can see in the UI a new job is created, the artifacts have been uploaded correctly but in the info panel pending is still in the 'upload' state and the status remains as 'created'.How do I progress the job \/ remove the pending status after the upload has occurred?",
        "Challenge_closed_time":1649330351000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649330161000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in progressing a job and removing the pending status after uploading artifacts using the python run client. The job remains in the 'upload' state and the status remains as 'created' even after the artifacts have been uploaded correctly.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1476",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":10.0,
        "Challenge_reading_time":7.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.0527777778,
        "Challenge_title":"Programmatic upload and start an operation without CLI",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":85,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"To start the operation and remove the pending state, you need to call client.approve().\n\nTo correct creation process with an upload should be:\n\nfrom polyaxon.schemas import V1RunPending\nfrom polyaxon.constants.globals import DEFAULT_UPLOADS_PATH\nfrom polyaxon.constants.metadata import META_UPLOAD_ARTIFACTS\n\nmeta_info = {}\nmeta_info[META_UPLOAD_ARTIFACTS] =  DEFAULT_UPLOADS_PATH  # or a custom path if you pass a custom upload to path \n# 1. Create\nclient.create(content=operation, meta_info=meta_info, pending=V1RunPending.UPLOAD)\n# 2. Upload\nclient.upload_artifacts_dir('.\/')\n# 3. Approve\nclient.approve()",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.3,
        "Solution_reading_time":7.86,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":61.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3357.2136111111,
        "Challenge_answer_count":0,
        "Challenge_body":"After https:\/\/github.com\/iterative\/dvc\/pull\/5265\r\nWe do not allow ignoring lockfile. `dvc-bench` is running currently on some older version of `dvc`, though it would be good to adjust it so that it works with `>2.0.0`.",
        "Challenge_closed_time":1628758546000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1616672577000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where the example-get-started is broken with the latest DVC, as they are unable to fetch data from the cloud due to a corrupted lockfile.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/iterative\/dvc-bench\/issues\/244",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":7.7,
        "Challenge_reading_time":3.07,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":9.0,
        "Challenge_repo_issue_count":400.0,
        "Challenge_repo_star_count":19.0,
        "Challenge_repo_watch_count":17.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":3357.2136111111,
        "Challenge_title":"requirements: update dvc",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":34,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@pared Sorry, not sure I understand what do we need to update here. Could you elaborate, please? Fixed by #267, forgot to close.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.8,
        "Solution_reading_time":1.56,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":23.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1476195722390,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":399.0,
        "Answerer_view_count":75.0,
        "Challenge_adjusted_solved_time":12.5487494444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using the generated code from huggingface, Task: <code>Zero-Shot Classification<\/code>, Configuration: <code>AWS<\/code> and running it in Sagemaker's jupyterlab<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.huggingface import HuggingFaceModel\nimport sagemaker\n\nrole = sagemaker.get_execution_role()\n# Hub Model configuration. https:\/\/huggingface.co\/models\nhub = {\n    'HF_MODEL_ID':'facebook\/bart-large-mnli',\n    'HF_TASK':'zero-shot-classification'\n}\n\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n    transformers_version='4.6.1',\n    pytorch_version='1.7.1',\n    py_version='py36',\n    env=hub,\n    role=role, \n)\n\n# deploy model to SageMaker Inference\npredictor = huggingface_model.deploy(\n    initial_instance_count=1, # number of instances\n    instance_type='ml.m5.xlarge' # ec2 instance type\n)\n\npredictor.predict({\n    'inputs': &quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;\n})\n<\/code><\/pre>\n<p>The following error returned:<\/p>\n<blockquote>\n<p>ModelError: An error occurred (ModelError) when calling the\nInvokeEndpoint operation: Received client error (400) from primary\nwith message &quot;{   &quot;code&quot;: 400,   &quot;type&quot;: &quot;InternalServerException&quot;,<br \/>\n&quot;message&quot;: &quot;<strong>call<\/strong>() missing 1 required positional argument:\n\\u0027candidate_labels\\u0027&quot; } &quot;. See ...\nin account **** for more information.<\/p>\n<\/blockquote>\n<p>I tried running them differently such as this,<\/p>\n<pre><code>predictor.predict({\n    'inputs': &quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;,\n    'candidate_labels': ['science', 'life']\n})\n<\/code><\/pre>\n<p>but still don't work. How should I run it?<\/p>",
        "Challenge_closed_time":1635482654648,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635437479150,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while deploying huggingface zero-shot classification in Sagemaker using a template. The error message states that a positional argument 'candidate_labels' is missing. The user has tried running the code with the 'candidate_labels' argument, but it still does not work. The user is seeking guidance on how to resolve the issue.",
        "Challenge_last_edit_time":1635487565192,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69757539",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":16.5,
        "Challenge_reading_time":25.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":12.5487494444,
        "Challenge_title":"Deploying huggingface zero-shot classification in Sagemaker using template returns error, missing positional argument 'candidate_labels'",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":287.0,
        "Challenge_word_count":191,
        "Platform":"Stack Overflow",
        "Poster_created_time":1476195722390,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":399.0,
        "Poster_view_count":75.0,
        "Solution_body":"<p>The schema of request body for a zero-shot classification model is defined in this <a href=\"https:\/\/github.com\/huggingface\/notebooks\/blob\/772ddf7140bccc443da265c90c95eda99e69c564\/sagemaker\/10_deploy_model_from_s3\/deploy_transformer_model_from_s3.ipynb\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>\n<pre><code>{\n    &quot;inputs&quot;: &quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;,\n    &quot;parameters&quot;: {\n        &quot;candidate_labels&quot;: [\n            &quot;refund&quot;,\n            &quot;legal&quot;,\n            &quot;faq&quot;\n        ]\n    }\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.2,
        "Solution_reading_time":7.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":49.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":26.8941822222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi All    <\/p>\n<p>I have been working with Azure Machine Learning Studio (Classic) and have always found its integration with Excel super mega useful.    <\/p>\n<p>All I had to do was to get the URI and the API_Key of my web service and paste them on the Azure Machine Learning Add-In, that I had downloaded. Easy and useful.    <\/p>\n<p>However, with the new Azure Machine Learning studio that does not seem possible any more.     <\/p>\n<p>Under the new Azure Machine Learning studio when I deploy a model I get a REST endpoint and that's it? !? I cannot find anywhere the API_key for my web service. I cannot even find a web service section  as such.     <\/p>\n<ol>\n<li> How do I get the API_Key for the web service I need?    <\/li>\n<li> If I get the API_Key could I use it on the Excel Azure Machine Learning add-in. It looks as if this is no longer an option and we need to start using Power BI instead.    <\/li>\n<li>  I have read this interesting post where someone mentions a work around that consist of creating an Excel macro. Is this the best option? <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/236781\/consume-scoreing-api-in-excel.html\">https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/236781\/consume-scoreing-api-in-excel.html<\/a>     <\/li>\n<\/ol>\n<p>Thank you    <\/p>",
        "Challenge_closed_time":1651319301696,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651222482640,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges with the new Azure Machine Learning studio as they are unable to find the API_Key for their web service and cannot locate a web service section. They are seeking guidance on how to obtain the API_Key and whether it can still be used on the Excel Azure Machine Learning add-in or if they need to switch to Power BI. They are also considering a workaround of creating an Excel macro.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/831512\/new-azure-machine-learning-excel",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":16.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":26.8941822222,
        "Challenge_title":"New Azure Machine Learning & Excel",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":204,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. The new AzureML integration with Excel isn't supported at this time. More details are provided on this <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/778717\/replacement-for-azure-ml-classic-excel-add-in.html\">thread<\/a>. The alternative approach would be to use a Client or PowerBI to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-consume-web-service?tabs=python\">consume<\/a> the model. For future reference, you can find your webservice endpoint and keys under Studio &gt; Endpoints &gt; Endpoint &gt; Consume.    <\/p>\n<p>--please don't forget to <code>Accept Answer<\/code> if the reply is helpful. Thanks.--<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.8,
        "Solution_reading_time":8.99,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":72.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":8.3539944444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am getting memory error while creating simple dataframe read from CSV file on Azure Machine Learning using notebook VM as compute instance. The VM has config of DS 13 56gb RAM, 8vcpu, 112gb storage on Ubuntu (Linux (ubuntu 16.04). CSV file is 5gb file. <\/p>\n\n<pre><code>blob_service = BlockBlobService(account_name,account_key)\nblobstring = blob_service.get_blob_to_text(container,filepath).content\ndffinaldata = pd.read_csv(StringIO(blobstring), sep=',')\n<\/code><\/pre>\n\n<p>What I am doing wrong here ?<\/p>",
        "Challenge_closed_time":1579583849896,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579544749467,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a memory error while creating a dataframe from a 5GB CSV file on Azure Machine Learning using a notebook VM with DS 13 configuration, 56GB RAM, 8vcpu, and 112GB storage on Ubuntu 16.04. The code used to create the dataframe involves reading the CSV file using pandas. The user is seeking assistance in identifying the issue.",
        "Challenge_last_edit_time":1579556126092,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59829017",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":7.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":10.8612302778,
        "Challenge_title":"Azure Machine Learning - Memory Error while creating dataframe",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":507.0,
        "Challenge_word_count":68,
        "Platform":"Stack Overflow",
        "Poster_created_time":1500744375327,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":255.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>you need to provide the right encoding when calling get_blob_to_text, please refer to the <a href=\"https:\/\/github.com\/Azure\/azure-storage-python\/blob\/master\/samples\/blob\/block_blob_usage.py#L390\" rel=\"nofollow noreferrer\">sample<\/a>.<\/p>\n\n<p>The code below is what  normally use for reading data file in blob storages. Basically, you can use blob\u2019s url along with sas token and use a request method. However, You might want to edit the \u2018for loop\u2019 depending what types of data you have (e.g. csv, jpg, and etc).<\/p>\n\n<p>-- Python code below --<\/p>\n\n<pre><code>import requests\nfrom azure.storage.blob import BlockBlobService, BlobPermissions\nfrom azure.storage.blob.baseblobservice import BaseBlobService\nfrom datetime import datetime, timedelta\n\naccount_name = '&lt;account_name&gt;'\naccount_key = '&lt;account_key&gt;'\ncontainer_name = '&lt;container_name&gt;'\n\nblob_service=BlockBlobService(account_name,account_key)\ngenerator = blob_service.list_blobs(container_name)\n\nfor blob in generator:\n    url = f\"https:\/\/{account_name}.blob.core.windows.net\/{container_name}\"\n    service = BaseBlobService(account_name=account_name, account_key=account_key)\n    token = service.generate_blob_shared_access_signature(container_name, img_name, permission=BlobPermissions.READ, expiry=datetime.utcnow() + timedelta(hours=1),)\n    url_with_sas = f\"{url}?{token}\"\n    response = requests.get(url_with_sas)\n<\/code><\/pre>\n\n<p>Please follow the below link to read data on Azure Blob Storage.\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1579586200472,
        "Solution_link_count":4.0,
        "Solution_readability":17.2,
        "Solution_reading_time":22.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":134.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":384.0669444444,
        "Challenge_answer_count":0,
        "Challenge_body":"### System Specs\r\n**Operating System:** Windows 10\r\n**Python Version:** 3.9.5 64-bit\r\n\r\nWhen I run the command:\r\n\r\n```terminal\r\npip install azureml-core\r\n```\r\n\r\nI get an error during the installation, specifically on the `ruamel.yaml` package. I guess the first question I have is there any reason we are restricted to that specific version of `ruamel.yaml`? I was able to install the latest version **(0.17.10)** no problem, so if we could use a later version that would be the easiest fix.\r\n\r\n### Partial Log\r\n```terminal\r\nAttempting uninstall: ruamel.yaml\r\nFound existing installation: ruamel.yaml 0.17.10\r\nUninstalling ruamel.yaml-0.17.10:\r\nSuccessfully uninstalled ruamel.yaml-0.17.10\r\nRunning setup.py install for ruamel.yaml ... error\r\nERROR: Command errored out with exit status 1:\r\n```\r\n\r\n### Full Log\r\n[Error Log From Installation Run](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/files\/6913613\/error.log)",
        "Challenge_closed_time":1629244160000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1627861519000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an AzureMLException while trying to download a registered model from the AMLS workspace. The file is being created in the target directory but with 0 bytes, indicating that no data is being transferred into it. The traceback shows a FileNotFoundError and an AzureMLException.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1564",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.2,
        "Challenge_reading_time":12.03,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":384.0669444444,
        "Challenge_title":"pip install `azureml-core` fails on `ruamel.yaml`",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":119,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"0.17.5 introduced a breaking change hence there is an upperbound Thank you @vizhur! @areed1192, I'm closing this issue. Please reopen if you still have questions.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.2,
        "Solution_reading_time":2.03,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1431018627572,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":86.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":216.8863813889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a jupyter notebook in SageMaker in which I want to run the XGBoost algorithm. The data has to match 3 criteria: \n-No header row\n-Outcome variable in the first column, features in the rest of the columns \n-All columns need to be numeric<\/p>\n\n<p>The error I get is the following:<\/p>\n\n<pre><code>    Error for Training job xgboost-2019-03-13-16-21-25-000: \n    Failed Reason: ClientError: Blankspace and colon not found in firstline \n'0.0,0.0,99.0,314.07,1.0,0.0,0.0,0.0,0.48027846,0.0...' of file 'train.csv'\n<\/code><\/pre>\n\n<p>In the error itself it can be seen that there are no headers, the output is the first column (it just takes 1.0 and 0.0 values) and all features are numerical. The data is stored in its own bucket. <\/p>\n\n<p>I have seen a related question in GitHub but there are no solution there. Also, the example notebook that Amazon has does not take care of change the default sep or anything when saving a dataframe to csv for using it later on. <\/p>",
        "Challenge_closed_time":1553278582280,
        "Challenge_comment_count":0,
        "Challenge_created_time":1552497791307,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while running the XGBoost algorithm in a Jupyter notebook in SageMaker. The error message indicates that there are no headers, the output is in the first column, and all features are numerical. The error specifically mentions that blankspace and colon are not found in the first line of the file 'train.csv'. The user has checked a related question on GitHub but has not found a solution.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55147861",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":12.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":216.8863813889,
        "Challenge_title":"Blankspace and colon not found in firstline",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":915.0,
        "Challenge_word_count":163,
        "Platform":"Stack Overflow",
        "Poster_created_time":1523298968403,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1754.0,
        "Poster_view_count":197.0,
        "Solution_body":"<p>The error message indicated XGBoost was expecting the input data set as libsvm format instead of csv. SageMaker XGBoost by default assumed the input data set was in libsvm format. For using input data set in csv, please explicitly specify <code>content-type<\/code> as <code>text\/csv<\/code>.<\/p>\n\n<p>For more information: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#InputOutput-XGBoost\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#InputOutput-XGBoost<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.8,
        "Solution_reading_time":7.04,
        "Solution_score_count":4.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":50.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1494171603136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":73187.0,
        "Answerer_view_count":8473.0,
        "Challenge_adjusted_solved_time":0.2887638889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an Azure ML Workspace which comes by default with some pre-installed packages.<\/p>\n<p>I tried to install<\/p>\n<pre><code>!pip install -U imbalanced-learn\n<\/code><\/pre>\n<p>But I got this error<\/p>\n<pre><code>Requirement already up-to-date: scikit-learn in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (0.24.2)\nRequirement already satisfied, skipping upgrade: scipy&gt;=0.19.1 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (1.4.1)\nRequirement already satisfied, skipping upgrade: joblib&gt;=0.11 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (0.14.1)\nRequirement already satisfied, skipping upgrade: numpy&gt;=1.13.3 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (1.18.5)\nRequirement already satisfied, skipping upgrade: threadpoolctl&gt;=2.0.0 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (2.1.0)\nCollecting imbalanced-learn\n  Using cached imbalanced_learn-0.9.0-py3-none-any.whl (199 kB)\nRequirement already satisfied, skipping upgrade: threadpoolctl&gt;=2.0.0 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from imbalanced-learn) (2.1.0)\nRequirement already satisfied, skipping upgrade: joblib&gt;=0.11 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from imbalanced-learn) (0.14.1)\nRequirement already satisfied, skipping upgrade: scipy&gt;=1.1.0 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from imbalanced-learn) (1.4.1)\nERROR: Could not find a version that satisfies the requirement scikit-learn&gt;=1.0.1 (from imbalanced-learn) (from versions: 0.9, 0.10, 0.11, 0.12, 0.12.1, 0.13, 0.13.1, 0.14, 0.14.1, 0.15.0b1, 0.15.0b2, 0.15.0, 0.15.1, 0.15.2, 0.16b1, 0.16.0, 0.16.1, 0.17b1, 0.17, 0.17.1, 0.18, 0.18.1, 0.18.2, 0.19b2, 0.19.0, 0.19.1, 0.19.2, 0.20rc1, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.20.4, 0.21rc2, 0.21.0, 0.21.1, 0.21.2, 0.21.3, 0.22rc2.post1, 0.22rc3, 0.22, 0.22.1, 0.22.2, 0.22.2.post1, 0.23.0rc1, 0.23.0, 0.23.1, 0.23.2, 0.24.dev0, 0.24.0rc1, 0.24.0, 0.24.1, 0.24.2)\nERROR: No matching distribution found for scikit-learn&gt;=1.0.1 (from imbalanced-\n<\/code><\/pre>\n<p>learn)<\/p>\n<p>Not sure how to solve this, I have read in other posts to use conda, but that didnt work either.<\/p>",
        "Challenge_closed_time":1644935036627,
        "Challenge_comment_count":1,
        "Challenge_created_time":1644933997077,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to install imbalanced-learn on an Azure ML Workspace, but is encountering an error stating that it cannot find a matching distribution for scikit-learn>=1.0.1. The user has tried using conda but it did not work.",
        "Challenge_last_edit_time":1644960360047,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71127858",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.6,
        "Challenge_reading_time":31.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":0.2887638889,
        "Challenge_title":"Cant install imbalanced-learn on an Azure ML Environment",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":219.0,
        "Challenge_word_count":225,
        "Platform":"Stack Overflow",
        "Poster_created_time":1302030303092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brussels, B\u00e9lgica",
        "Poster_reputation_count":30340.0,
        "Poster_view_count":2937.0,
        "Solution_body":"<p><a href=\"https:\/\/pypi.org\/project\/scikit-learn\/1.0.1\/\" rel=\"nofollow noreferrer\"><code>scikit-learn<\/code> 1.0.1<\/a> and up require Python &gt;= 3.7; you use Python 3.6. You need to upgrade Python or downgrade <code>imbalanced-learn<\/code>. <a href=\"https:\/\/pypi.org\/project\/imbalanced-learn\/0.8.1\/\" rel=\"nofollow noreferrer\"><code>imbalanced-learn<\/code> 0.8.1<\/a> allows Python 3.6 so<\/p>\n<pre><code>!pip install -U &quot;imbalanced-learn &lt; 0.9&quot;\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.3,
        "Solution_reading_time":6.38,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1639972620503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1653.0,
        "Answerer_view_count":1212.0,
        "Challenge_adjusted_solved_time":24.1372647222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am passing the values of lables as below to create a featurestore with labels. But after creation of the featurestore, I do not see the featurestore created with labels. Is it still not supported in VertexAI<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>    fs = aiplatform.Featurestore.create(\n        featurestore_id=featurestore_id,\n        labels=dict(project='retail', env='prod'),\n        online_store_fixed_node_count=online_store_fixed_node_count,\n        sync=sync\n    )\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/viOSu.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/viOSu.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1651709813300,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651616413553,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to create a feature store in VertexAI using labels. They have passed the values of labels to create a feature store, but after creation, they are unable to see the feature store created with labels. The user is unsure if this feature is supported in VertexAI.",
        "Challenge_last_edit_time":1651623411107,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72106030",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":9.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":25.9443741667,
        "Challenge_title":"I am not able to create a feature store in vertexAI using labels",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":83.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1530457174832,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1043.0,
        "Poster_view_count":212.0,
        "Solution_body":"<p>As mentioned in this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/managing-featurestores\" rel=\"nofollow noreferrer\">featurestore documentation<\/a>:<\/p>\n<blockquote>\n<p>A <strong>featurestore<\/strong> is a top-level container for entity types, features,\nand feature values.<\/p>\n<\/blockquote>\n<p>With this, the GCP console UI &quot;labels&quot; are the &quot;labels&quot; at the <strong>Feature<\/strong> level.<\/p>\n<p>Once a <strong>featurestore<\/strong> is created, you will need to create an <strong>entity<\/strong> and then create a <strong>Feature<\/strong> that has the <em>labels<\/em> parameter as shown on the below sample python code.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from google.cloud import aiplatform\n\ntest_label = {'key1' : 'value1'}\n\ndef create_feature_sample(\n    project: str,\n    location: str,\n    feature_id: str,\n    value_type: str,\n    entity_type_id: str,\n    featurestore_id: str,\n):\n\n    aiplatform.init(project=project, location=location)\n\n    my_feature = aiplatform.Feature.create(\n        feature_id=feature_id,\n        value_type=value_type,\n        entity_type_name=entity_type_id,\n        featurestore_id=featurestore_id,\n        labels=test_label,\n    )\n\n    my_feature.wait()\n\n    return my_feature\n\ncreate_feature_sample('your-project','us-central1','test_feature3','STRING','test_entity3','test_fs3')\n<\/code><\/pre>\n<p>Below is the screenshot of the GCP console which shows that <em>labels<\/em> for <strong>test_feature3<\/strong> feature has the values defined in the above sample python code.\n<a href=\"https:\/\/i.stack.imgur.com\/7S2oa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7S2oa.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>You may refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/managing-features#create-feature\" rel=\"nofollow noreferrer\">creation of feature documentation<\/a> using python for more details.<\/p>\n<p>On the other hand, you may still view the <em>labels<\/em> you defined for your featurestore using the REST API as shown on the below sample.<\/p>\n<pre><code>curl -X GET \\\n-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print-access-token) \\\n&quot;https:\/\/&lt;your-location&gt;-aiplatform.googleapis.com\/v1\/projects\/&lt;your-project&gt;\/locations\/&lt;your-location&gt;\/featurestores&quot;\n<\/code><\/pre>\n<p>Below is the result of the REST API which also shows the value of the <em>labels<\/em> I defined for my &quot;test_fs3&quot; featurestore.\n<a href=\"https:\/\/i.stack.imgur.com\/gW45X.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gW45X.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1651710305260,
        "Solution_link_count":7.0,
        "Solution_readability":16.7,
        "Solution_reading_time":34.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":226.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1443426419048,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation_count":842.0,
        "Answerer_view_count":219.0,
        "Challenge_adjusted_solved_time":3592.1438852778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Suppose I have a dataframe (myDataframe) with two column of values (third and fourth). I want to plot them in a bi-dimensional graph. If I do it in R it works, but it returns me a graph without labels when I run the script from Azure Machine Learning. Someone with ideas?<\/p>\n\n<pre><code>...\nplot(myDataframe[,3],myDataframe[,4], \n       main=\"my title\",\n       xlab= \"x\"\n       ylab= \"y\",\n       col= \"blue\", pch = 19, cex = 0.1, lty = \"solid\", lwd = 2)\n\n# lines(x,y=x, col=\"yellow\")\n\n# add LABELS\ntext(DF_relativo[,A], DF_relativo[,B], \n       labels=DF_relativo$names, cex= 0.7, pos=2)\n...\n<\/code><\/pre>",
        "Challenge_closed_time":1466402236047,
        "Challenge_comment_count":0,
        "Challenge_created_time":1453470518060,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with plotting a bi-dimensional graph in Azure Machine Learning. Although the same code works in R, the graph generated in Azure Machine Learning does not have any labels. The user is seeking suggestions to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34948242",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":7.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3592.1438852778,
        "Challenge_title":"Azure: plot without labels",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":91.0,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436432728608,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Colleferro, Italy",
        "Poster_reputation_count":809.0,
        "Poster_view_count":361.0,
        "Solution_body":"<p>using ggplot2() in AzureML is bit different. It can use to have plots with labels. Here's the <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/b1c26728eb6c4e4d80dddceae992d653\" rel=\"nofollow\">Cortana Intelligence gallery example<\/a> for the particular task.  <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.9,
        "Solution_reading_time":3.66,
        "Solution_score_count":-1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":28.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":28.7265325,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to get instance used time by instance type and day, in EC2 and Sagemaker service,<\/p>\n<p>But in AWSCUR it seems no value of instance running time(hour\/minute\/second),<\/p>\n<p>How can I get the instance actual used time?<\/p>",
        "Challenge_closed_time":1635436765040,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635333349523,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to obtain the actual used time of AWS EC2\/Sagemaker instances by instance type and day, but is unable to find this information in AWSCUR. They are seeking guidance on how to obtain this data.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69737649",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.5,
        "Challenge_reading_time":3.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":28.7265325,
        "Challenge_title":"How to get AWS EC2\/Sagemaker instance used time by instance type and day?",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":61.0,
        "Challenge_word_count":49,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458116093247,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1803.0,
        "Poster_view_count":75.0,
        "Solution_body":"<p>you can see daily (or even hourly if you opt-in) cost and usage by instance type with:<br \/>\n<code>aws ce get-cost-and-usage --time-period Start=2021-10-26,End=2021-10-27 --granularity DAILY --metrics &quot;UsageQuantity&quot; &quot;BlendedCost&quot; --group-by Type=DIMENSION,Key=INSTANCE_TYPE<\/code><br \/>\nNote that SageMaker instance types names starts with: <code>ml.*<\/code><\/p>\n<p>To view things in the finest resolution you'll need to produce detailed billing reports (DBR): <a href=\"https:\/\/docs.aws.amazon.com\/cur\/latest\/userguide\/detailed-billing.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cur\/latest\/userguide\/detailed-billing.html<\/a><br \/>\nIt will generates CSV reports in S3, which you could query with Athena using SQL: <a href=\"https:\/\/docs.aws.amazon.com\/cur\/latest\/userguide\/cur-query-athena.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cur\/latest\/userguide\/cur-query-athena.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":18.7,
        "Solution_reading_time":12.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":76.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":165.0966666667,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nFor some reason, `mlflow deployment create ...` can fail unexpectedly. \r\n\r\n```\r\nmlflow deployments create -t triton --flavor triton --name sid-minibert-onnx -m models:\/sid-minibert-onnx\/1 -C \"version=1\"\r\nCopied \/mlflow\/artifacts\/0\/41f4069628e5429eb5c75728486a247a\/artifacts\/triton\/sid-minibert-onnx to \/common\/triton-model-repo\/sid-minibert-onnx\r\nSaved mlflow-meta.json to \/common\/triton-model-repo\/sid-minibert-onnx\r\nTraceback (most recent call last):\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/mlflow_triton\/deployments.py\", line 109, in create_deployment\r\n    self.triton_client.load_model(name)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/tritonclient\/http\/__init__.py\", line 622, in load_model\r\n    _raise_if_error(response)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/tritonclient\/http\/__init__.py\", line 64, in _raise_if_error\r\n    raise error\r\ntritonclient.utils.InferenceServerException: failed to load 'sid-minibert-onnx', no version is available\r\n```\r\n\r\nFix is to delete the mlflow pod and start over.\r\n\r\n**Steps\/Code to reproduce bug**\r\nFollow steps in docs\/source\/morpheus_quickstart_guide.md#model-deployment\r\n\r\n**Expected behavior**\r\nSuccessful deployment as described at docs\/source\/morpheus_quickstart_guide.md#model-deployment\r\n\r\n**Environment overview (please complete the following information)**\r\n - Environment location: LaunchPad\r\n - Method of Morpheus install: Kubernetes\r\n\r\n**Environment details**\r\nLaunchPad Helm deployment on A30. Unfortunately, unable to capture the print_env.sh output from ipykernel there.\r\n\r\n**Additional context**\r\nMLflow sqlite db likely gets corrupted or otherwise \"confused\". Possibly an issue in tritonclient?\r\nTriton logging complains about unable to read config.pbtxt\r\n",
        "Challenge_closed_time":1654018977000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1653424629000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The issue is related to the failure of MlflowArtifactDataset.load() when both artifact_path and run_id are specified. An error is encountered when the artifact_path is not None and run_id is specified.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/nv-morpheus\/Morpheus\/issues\/125",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.9,
        "Challenge_reading_time":23.83,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":43.0,
        "Challenge_repo_issue_count":536.0,
        "Challenge_repo_star_count":131.0,
        "Challenge_repo_watch_count":10.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":165.0966666667,
        "Challenge_title":"[BUG] mlflow deployments create can fail (k8s\/Helm)",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":157,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Error in LaunchPad notebooks. There was a change in the triton upstream where you previously didn't need to specify the model suffix as the path. Now you do. ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":1.91,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":28.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1467943515392,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Danang, H\u1ea3i Ch\u00e2u District, Da Nang, Vietnam",
        "Answerer_reputation_count":173.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":67.8829583333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to integrate MLFlow to my project. Because I'm using <code>tf.keras.fit_generator()<\/code> for my training so I take advantage of <code>mlflow.tensorflow.autolog()<\/code>(<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.tensorflow.html#mlflow.tensorflow.autolog\" rel=\"nofollow noreferrer\">docs<\/a> here) to enable automatic logging of metrics and parameters:<\/p>\n<pre><code>    model = Unet()\n    optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n\n    metrics = [IOUScore(threshold=0.5), FScore(threshold=0.5)]\n    model.compile(optimizer, customized_loss, metrics)\n\n    callbacks = [\n        tf.keras.callbacks.ModelCheckpoint(&quot;model.h5&quot;, save_weights_only=True, save_best_only=True, mode='min'),\n        tf.keras.callbacks.TensorBoard(log_dir='.\/logs', profile_batch=0, update_freq='batch'),\n    ]\n\n\n    train_dataset = Dataset(src_dir=SOURCE_DIR)\n\n    train_data_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True)\n\n   \n    with mlflow.start_run():\n        mlflow.tensorflow.autolog()\n        mlflow.log_param(&quot;batch_size&quot;, BATCH_SIZE)\n\n        model.fit_generator(\n            train_data_loader,\n            steps_per_epoch=len(train_data_loader),\n            epochs=EPOCHS,\n            callbacks=callbacks   \n            )\n<\/code><\/pre>\n<p>I expected something like this (just a demonstration taken from the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#visualizing-metrics\" rel=\"nofollow noreferrer\">docs<\/a>):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/eG56Z.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eG56Z.png\" alt=\"Visualization on the docs\" \/><\/a><\/p>\n<p>However, after the training finished, this is what I got:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fS1JD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fS1JD.png\" alt=\"f1_score visualization\" \/><\/a><\/p>\n<p>How can I configure so that the metric plot will update and display its value at each epoch instead of just showing the latest value?<\/p>",
        "Challenge_closed_time":1594008525280,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593764146630,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to integrate MLFlow into their project and is using mlflow.tensorflow.autolog() to enable automatic logging of metrics and parameters. They expected a visualization of metrics to be displayed at each epoch during training, but instead, only the latest value is shown. The user is seeking guidance on how to configure the metric plot to update and display its value at each epoch.",
        "Challenge_last_edit_time":1594008626392,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62711259",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":17.5,
        "Challenge_reading_time":26.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":67.8829583333,
        "Challenge_title":"Customize metric visualization in MLFlow UI when using mlflow.tensorflow.autolog()",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1035.0,
        "Challenge_word_count":145,
        "Platform":"Stack Overflow",
        "Poster_created_time":1467943515392,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Danang, H\u1ea3i Ch\u00e2u District, Da Nang, Vietnam",
        "Poster_reputation_count":173.0,
        "Poster_view_count":28.0,
        "Solution_body":"<p>After searching around, I found <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/2390\" rel=\"nofollow noreferrer\">this issue<\/a> related to my problem above. Actually, all my metrics just logged once each training (instead of each epoch as my intuitive thought). The reason is I didn't specify the <code>every_n_iter<\/code> parameter in <code>mlflow.tensorflow.autolog()<\/code>, which indicates how many 'iterations' must pass before MLflow logs metric executed (see the <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tensorflow.html#mlflow.tensorflow.autolog\" rel=\"nofollow noreferrer\">docs<\/a>). So, changing my code to:<\/p>\n<p><code>mlflow.tensorflow.autolog(every_n_iter=1)<\/code><\/p>\n<p>fixed the problem.<\/p>\n<p>P\/s: Remember that in TF 2.x, an 'iteration' is an epoch (in TF 1.x it's a batch).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.7,
        "Solution_reading_time":10.74,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":87.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.51785,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hola a todos, perdon quiza sea muy basica mi pregunta, no se como importar un excel como Dataset. Solo puedo importar CSV, etc. Muchas gracias<\/p>",
        "Challenge_closed_time":1614974319960,
        "Challenge_comment_count":0,
        "Challenge_created_time":1614968855700,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is having difficulty importing an Excel file as a dataset in Microsoft Azure and is only able to import CSV files. They are seeking assistance on how to import an Excel file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/301247\/excel-en-microsoft-azure",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":2.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1.51785,
        "Challenge_title":"Excel en Microsoft Azure",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":28,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. Excel is not a <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.tabulardataset?view=azure-ml-py\">supported format<\/a> for Azure ML Tabular datasets. I recommend that you convert your excel file to .csv file (save as .csv) before importing to Azure ML. Hope this helps!    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":6.7,
        "Solution_reading_time":4.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":40.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.2394444444,
        "Challenge_answer_count":0,
        "Challenge_body":"```\r\n0: jdbc:hive2:\/\/localhost:10001\/default> CREATE MODEL ssd1 using \"mlflow:\/model\/ssd\"\r\n. . . . . . . . . . . . . . . . . . . .> ;\r\nError: org.apache.hive.service.cli.HiveSQLException: Error running query: org.mlflow.tracking.MlflowHttpException: statusCode=404 reasonPhrase=[NOT FOUND] bodyMessage=[{\"error_code\": \"RESOURCE_DOES_NOT_EXIST\", \"message\": \"Registered Model with name=ssd1 not found\"}]\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperti\r\n```",
        "Challenge_closed_time":1642206718000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1642187856000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a warning message while training mlflow-pytorch 2.0.0, which states that an unexpected error has occurred during autologging and that the argument must be a string or a number, not 'Accuracy'. This warning message is printed after every epoch.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/eto-ai\/rikai\/issues\/493",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":45.4,
        "Challenge_reading_time":14.23,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":715.0,
        "Challenge_repo_star_count":127.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":5.2394444444,
        "Challenge_title":"Can not create model in MLflowCatalog",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":41,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Duplicated to #496 ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.2,
        "Solution_reading_time":0.24,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":94.4830555556,
        "Challenge_answer_count":0,
        "Challenge_body":"```Azure ML SDK Version:  1.11.0```\r\n\r\nIn a ```PythonScriptStep``` I'm getting a crash error that: \"\r\n```\r\nazureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\r\n```\r\n\r\nHere is my RunConfiguration:\r\n```\r\ncompute_target = ComputeTarget(workspace=f.ws, name=compute_name)\r\n\r\ncd = CondaDependencies.create(\r\n    pip_packages=[\"pandas\", \"numpy\",\r\n                  \"azureml-defaults\", \"azureml-sdk[explain,automl]\", \"azureml-train-automl-runtime\"],\r\n    conda_packages=[\"xlrd\", \"scikit-learn\", \"numpy\", \"pyyaml\", \"pip\"])\r\namlcompute_run_config = RunConfiguration(conda_dependencies=cd)\r\namlcompute_run_config.environment.docker.enabled = True\r\n```\r\n\r\nhere is the step:\r\n```\r\nadd_vendor_sets = PythonScriptStep(\r\n    name='Add Vendor set',\r\n    script_name='add_vendor_set.py',\r\n    arguments=['--respondent_dir', level_respondent,\r\n                '--my_dir', my_raw,\r\n                '--output_dir', factset_processed],\r\n    compute_target=compute_target,\r\n    inputs=[level_respondent, my_raw],\r\n    outputs=[my_processed],\r\n    runconfig=amlcompute_run_config,\r\n    source_directory=os.path.join(os.getcwd(), 'pipes\/add_vendor_set'),\r\n    allow_reuse=True\r\n)\r\n```\r\n\r\nThe environment is obviously included, but also definitely missing.  I'm stuck and now none of my pipelines, that were running in previous version, will work. \r\n\r\n",
        "Challenge_closed_time":1598387113000,
        "Challenge_comment_count":5,
        "Challenge_created_time":1598046974000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where version history is not maintained when pulling data from an Azure SQL DB or DW into Azure ML datasets. Only the first version is refreshed every time new data is pulled. The user has provided a reproducible example to explain the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1111",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":18.0,
        "Challenge_reading_time":18.26,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":94.4830555556,
        "Challenge_title":"error: azureml-train-automl-runtime is required however it is included",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":115,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"can you share the full stacktrace? and is the error happening when you submit the pipeline script? or is it happening in the logs of the `PythonScriptStep`? ```\r\n\"error\": {\r\n        \"code\": \"UserError\",\r\n        \"message\": \"azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\",\r\n        \"detailsUri\": \"https:\/\/aka.ms\/azureml-known-errors\",\r\n        \"details\": [],\r\n        \"debugInfo\": {\r\n            \"type\": \"UserScriptException\",\r\n            \"message\": \"UserScriptException:\\n\\tMessage: azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\\n\\tInnerException OptionalDependencyMissingException:\\n\\tMessage: azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\\n\\tInnerException: None\\n\\tErrorResponse \\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"inner_error\\\": {\\n            \\\"code\\\": \\\"ValidationError\\\",\\n            \\\"inner_error\\\": {\\n                \\\"code\\\": \\\"ScenarioNotSuported\\\",\\n                \\\"inner_error\\\": {\\n                    \\\"code\\\": \\\"OptionalDependencyMissing\\\"\\n                }\\n            }\\n        },\\n        \\\"message\\\": \\\"azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\\\"\\n    }\\n}\\n\\tErrorResponse \\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\\\"\\n    }\\n}\",\r\n            \"stackTrace\": \"  File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/pjx-d-cu1-mlw-models\/azureml\/d881de2a-9dba-4e74-805e-d3c6eaab2076\/mounts\/workspaceblobstore\/azureml\/d881de2a-9dba-4e74-805e-d3c6eaab2076\/azureml-setup\/context_manager_injector.py\\\", line 197, in execute_with_context\\n    raise UserScriptException(baseEx).with_traceback(exceptionInfo[2])\\n  File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/pjx-d-cu1-mlw-models\/azureml\/d881de2a-9dba-4e74-805e-d3c6eaab2076\/mounts\/workspaceblobstore\/azureml\/d881de2a-9dba-4e74-805e-d3c6eaab2076\/azureml-setup\/context_manager_injector.py\\\", line 166, in execute_with_context\\n    runpy.run_path(sys.argv[0], globals(), run_name=\\\"__main__\\\")\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/runpy.py\\\", line 263, in run_path\\n    pkg_name=pkg_name, script_name=fname)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/runpy.py\\\", line 96, in _run_module_code\\n    mod_name, mod_spec, pkg_name, script_name)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/runpy.py\\\", line 85, in _run_code\\n    exec(code, run_globals)\\n  File \\\"run_models.py\\\", line 286, in \\n    main()\\n  File \\\"run_models.py\\\", line 197, in main\\n    run = experiment.submit(config=automl_config, tags=tags)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/core\/experiment.py\\\", line 211, in submit\\n    run = submit_func(config, self.workspace, self.name, **kwargs)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\\\", line 97, in _automl_static_submit\\n    show_output)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\\\", line 255, in _start_execution\\n    automl_run = _default_execution(experiment, settings_obj, fit_params, True, show_output, parent_run_id)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\\\", line 121, in _default_execution\\n    return automl_estimator.fit(**fit_params)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/_azureautomlclient.py\\\", line 349, in fit\\n    \\\"azureml-train-automl-runtime must be installed in the current environment to run local in \\\"\\n\"\r\n        },\r\n        \"messageFormat\": \"azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\",\r\n        \"messageParameters\": {}\r\n    },\r\n    \"time\": \"0001-01-01T00:00:00.000Z\"\r\n}\r\n``` Here is my stack trace from the 70_driver_log.txt:\r\n```\r\nTraceback (most recent call last):\r\n  File \"run_models.py\", line 286, in <module>\r\n    main()\r\n  File \"run_models.py\", line 197, in main\r\n    run = experiment.submit(config=automl_config, tags=tags)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/core\/experiment.py\", line 211, in submit\r\n    run = submit_func(config, self.workspace, self.name, **kwargs)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\", line 97, in _automl_static_submit\r\n    show_output)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\", line 255, in _start_execution\r\n    automl_run = _default_execution(experiment, settings_obj, fit_params, True, show_output, parent_run_id)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\", line 121, in _default_execution\r\n    return automl_estimator.fit(**fit_params)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/_azureautomlclient.py\", line 349, in fit\r\n    \"azureml-train-automl-runtime must be installed in the current environment to run local in \"\r\nUserScriptException: UserScriptException:\r\n\tMessage: azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\r\n``` @swatig007 this is an error, @BillmanH is experiencing when submitting an AutoML run from within a `PythonScriptStep` rather than using an `AutoMLStep`. This approach worked for over a year, but is now throwing an error about `azureml-train-automl-runtime` not being installed. upgraded to 1.12.0, which solved this problem and opened other issues. ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.5,
        "Solution_reading_time":83.64,
        "Solution_score_count":null,
        "Solution_sentence_count":50.0,
        "Solution_word_count":486.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":32.7057008333,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>I am new to the Azure ML Studio and just deployed the bike-rental regression model. When I tried to test it using the built in test tool in the studio, I am getting the attached error. Similar results running the Python code as well. Can someone please help me?    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/176918-mlerror.png?platform=QnA\" alt=\"176918-mlerror.png\" \/>    <\/p>",
        "Challenge_closed_time":1645695329740,
        "Challenge_comment_count":4,
        "Challenge_created_time":1645577589217,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while testing a bike-rental regression model in Azure ML Studio's built-in test tool and Python code. The error message indicates a \"list index out of range\" issue. The user is seeking assistance to resolve the problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/746784\/azure-ml-studio-error-while-testing-real-time-endp",
        "Challenge_link_count":1,
        "Challenge_participation_count":9,
        "Challenge_readability":8.8,
        "Challenge_reading_time":6.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":32.7057008333,
        "Challenge_title":"Azure ML Studio error while testing real-time endpoint -  list index out of range",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":66,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=b7844017-59f9-4d2e-a021-76c2270e06ca\">@Kumar, Priya  <\/a> Thanks for the question. It's known issue and the product team working on the fix to change in the UI.    <\/p>\n<p>Workaround: As shown below please set the GlobalParameters flag to 1.0 or a float number or remove it.     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/177485-image.png?platform=QnA\" alt=\"177485-image.png\" \/>    <\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.4,
        "Solution_reading_time":5.69,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":48.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1262067470272,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Los Angeles, CA, United States",
        "Answerer_reputation_count":11653.0,
        "Answerer_view_count":727.0,
        "Challenge_adjusted_solved_time":162.6529575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>So I'm trying to use <code>tf.contrib.learn.preprocessing.VocabularyProcessor.restore()<\/code> to restore a vocabulary file from an S3 bucket. First, I tried to get the path name to the bucket to use in <code>.restore()<\/code> and I kept getting 'object doesn't exist' error. Afterwards, upon further research, I found a method people use to load text files and JSON files and applied the same method here:<\/p>\n\n<pre><code>obj = s3.Object(BUCKET_NAME, KEY).get()['Body'].read()\nvocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor.restore(obj)\n<\/code><\/pre>\n\n<p>This worked for a while until the contents of file increased and eventually got a 'File name too long' error. Is there a better way to load and restore a file from an S3 bucket? <\/p>\n\n<p>By the way, I tested this out locally on my machine and it works perfectly fine since there it just needs to take the path to the file, not the entire contents of the file. <\/p>",
        "Challenge_closed_time":1521485629848,
        "Challenge_comment_count":0,
        "Challenge_created_time":1521471311783,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to restore a vocabulary file from an S3 bucket using tf.contrib.learn.preprocessing.VocabularyProcessor.restore(). Initially, they faced an 'object doesn't exist' error while trying to get the path name to the bucket. Later, they used a method to load text and JSON files, which worked until they encountered a 'File name too long' error. The user is seeking a better way to load and restore a file from an S3 bucket.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49365900",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":12.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":3.9772402778,
        "Challenge_title":"What's a better way to load a file using boto? (getting filename too long error)",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":532.0,
        "Challenge_word_count":153,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521470035783,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":210.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>It looks like you\u2019re passing in the actual contents of the file as the file name?<\/p>\n\n<p>I think you\u2019ll need to download the object from S3 to a tmp file and pass the path to that file into restore.<\/p>\n\n<p>Try using the method here: <a href=\"http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/s3.html#S3.Object.download_file\" rel=\"nofollow noreferrer\">http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/s3.html#S3.Object.download_file<\/a><\/p>\n\n<p>Update:\nI went through the code here: <a href=\"https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/contrib\/learn\/python\/learn\/preprocessing\/text.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/contrib\/learn\/python\/learn\/preprocessing\/text.py<\/a> and it looks like this just saves a pickle so you can really easily just import pickle and call the following:<\/p>\n\n<pre><code>import pickle\nobj = s3.Object(BUCKET_NAME, KEY).get()['Body']\nvocab_processor = pickle.loads(obj.read())\n<\/code><\/pre>\n\n<p>Hopefully that works?<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1522056862430,
        "Solution_link_count":4.0,
        "Solution_readability":16.7,
        "Solution_reading_time":13.9,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":91.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1342628508448,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"United States",
        "Answerer_reputation_count":5147.0,
        "Answerer_view_count":1739.0,
        "Challenge_adjusted_solved_time":13.2526622222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to do some basic multi-label classification in Azure ML. I have some basic data in the following format:<\/p>\n\n<pre><code>value_x value_y label\nx1      y1      label1\nx2      y2      label1\nx3      y3      label2\n.....\n<\/code><\/pre>\n\n<p>My problem is that in my data certain labels (out of a total of five) are overrepresented, as about 40% of the data is label1, about 20% is label 2 and the rest around 10%. <\/p>\n\n<p>I would like to get a sampling out of these to train my model, so that each label is represented in equal amounts. <\/p>\n\n<p>Tried the stratification option in the Sampling module on the labels column, but that just gives me a sampling with the same distribution of labels as in the initial dataset.<\/p>\n\n<p>Any idea how I could do this with a module?<\/p>",
        "Challenge_closed_time":1456458689160,
        "Challenge_comment_count":2,
        "Challenge_created_time":1456404822827,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in performing multi-label classification in Azure ML due to over-representation of certain labels in the data. They are looking for a way to get a sampling of the data where each label is represented in equal amounts, but the stratification option in the Sampling module did not work. The user is seeking suggestions for a module that can help them achieve their goal.",
        "Challenge_last_edit_time":1456410979576,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35627916",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.8,
        "Challenge_reading_time":9.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":14.9628702778,
        "Challenge_title":"Azure machine learning even sampling",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":539.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1381405661540,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":185.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>I was able to do this using a combination of <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/70530644-c97a-4ab6-85f7-88bf30a8be5f\" rel=\"nofollow\">Split Data<\/a>, <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/a8726e34-1b3e-4515-b59a-3e4a475654b8\" rel=\"nofollow\">Partition and Sample<\/a>, and <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/b2ebdabd-217d-4915-86cc-5b05972f7270\" rel=\"nofollow\">Add Rows<\/a> modules.  There may be an easier way to do it, but I did confirm it works.  :)  I published my work at <a href=\"http:\/\/gallery.azureml.net\/Details\/1245147fd7004e91bc7a3683cda19cc7\" rel=\"nofollow\">http:\/\/gallery.azureml.net\/Details\/1245147fd7004e91bc7a3683cda19cc7<\/a> so you can grab it directly from there, and run to confirm it does what you expect.  <\/p>\n\n<p>Since you said you wanted a sampling of the data, I just reduced each of the labels to 10% to have all labels represented equally.  Since you have a good understanding of the distribution in your dataset, leave label 3, 4, and 5 all at about 10%, and reduce label 1 by 1\/4 and label 2 by 1\/2 to get about 10% of them as well.  <\/p>\n\n<p>To explain what I did in the workspace linked above:<\/p>\n\n<ul>\n<li>I used some \"Split Data\" modules to filter out the label1 and label2 data.  In the Split Data module, change the Splitting mode to \"Regular Expression\" and set the regular expression to <strong>\\\"Label\" ^label1<\/strong> (to get the label1 data, for example).  <\/li>\n<li>Then I used some \"Partition and Sample\" modules to reduce the size of the label1 and label2 data appropriately.  <\/li>\n<li>Finally, I used some \"Add Rows\" modules to join all of the data back together again.  <\/li>\n<\/ul>\n\n<p>Finally, I didn't include this in my work, but you can also look at the <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/9f3fe1c4-520e-49ac-a152-2e104169912a\" rel=\"nofollow\">SMOTE<\/a> module.  It will increase the number of low-occurring samples using synthetic minority oversampling.  <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":9.5,
        "Solution_reading_time":24.9,
        "Solution_score_count":3.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":256.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":46.9975786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello , I'm new to azureML and I have a question about Azure ML designer , after creating  a workflow (drag and drop components , let's say problem of linear  regression ) can I export, download a file of the  metadata of the workflow that  contains the names of components used and its parameters.  <br \/>\nif it's possible can automate the process of creating a workflow in azureML designer by using already exsiting metadata file (json, xml , yaml ... ) similar to the one mentioned previously.   <\/p>\n<p>if there any other services in azure that's capable of solving this issue please feel free to mention it   <\/p>\n<p>thank youu.<\/p>",
        "Challenge_closed_time":1645441227963,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645272036680,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is new to AzureML and wants to know if it is possible to export a metadata file of the components and parameters used in a workflow created in Azure ML designer. They also want to know if it is possible to automate the process of creating a workflow using an existing metadata file. The user is open to suggestions for other Azure services that can solve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/742573\/export-metadata-file-of-componenets-and-its-parame",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":8.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":46.9975786111,
        "Challenge_title":"Export metadata file of componenets and its parameters  of trained and submitted model  in AzureML  designer",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":118,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=ca3ed354-350d-4b5d-be80-486f8db9ec5b\">@Achraf DRIDI  <\/a> Yes, you can export your designer experiment as a pipeline. The option to export is available from the designer from the top right hand corner. This is basically a cli command that helps you export the experiment in two ways.    <\/p>\n<ol>\n<li> Shallow     <\/li>\n<li> Deep    <\/li>\n<\/ol>\n<p><strong>UPDATE<\/strong>    <br \/>\nThe feature mentioned above is in private preview and not available to all users. Exact ETA not available at this point of time.    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.8,
        "Solution_reading_time":11.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":105.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1310893185208,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Thiruvananthapuram, Kerala, India",
        "Answerer_reputation_count":2763.0,
        "Answerer_view_count":851.0,
        "Challenge_adjusted_solved_time":789.2015219444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to connect mlflow with Minio server, both are running on my local machine, I am able to connect my client code to minio by adding the below lines to the code,<\/p>\n<pre><code>os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] =&quot;xxxx&quot;\nos.environ['AWS_SECRET_ACCESS_KEY'] =&quot;xxxxxx&quot; \nos.environ['MLFLOW_TRACKING_URI'] = 'http:\/\/localhost:5000'\n<\/code><\/pre>\n<p>But the mlflow server is not getting connected to Minio. To run Mlflow server, command I use:<\/p>\n<pre><code>mlflow server -h 0.0.0.0 -p 5000 --default-artifact-root s3:\/\/mlbucket --backend-store-uri sqlite:\/\/\/mlflow.db\n<\/code><\/pre>\n<p>The mlflow server runs, but while accessing the artifacts page the server, it throws the error:<\/p>\n<pre><code>raise NoCredentialsError()\nbotocore.exceptions.NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>\n<p>So how can I pass the credentials of the Minio to the mlflow server command?<\/p>",
        "Challenge_closed_time":1634743751772,
        "Challenge_comment_count":5,
        "Challenge_created_time":1631902626293,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to connect MLflow server to Minio server on their local machine. They have successfully connected their client code to Minio but are facing issues connecting the MLflow server to Minio. The MLflow server runs but throws an error while accessing the artifacts page, stating that it is unable to locate credentials. The user is seeking help on how to pass the credentials of Minio to the MLflow server command.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69227917",
        "Challenge_link_count":2,
        "Challenge_participation_count":6,
        "Challenge_readability":10.2,
        "Challenge_reading_time":13.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":789.2015219444,
        "Challenge_title":"Connect MLflow server to minio in local",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1136.0,
        "Challenge_word_count":116,
        "Platform":"Stack Overflow",
        "Poster_created_time":1310893185208,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Thiruvananthapuram, Kerala, India",
        "Poster_reputation_count":2763.0,
        "Poster_view_count":851.0,
        "Solution_body":"<p>Just add the below environment variables:<\/p>\n<pre><code>export AWS_ACCESS_KEY_ID=&lt;your-aws-access-key-id&gt;\nexport AWS_SECRET_ACCESS_KEY = &lt;your-aws-secret-access-key&gt;\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":22.1,
        "Solution_reading_time":2.69,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":12.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1589293508567,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":833.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":140.5201130556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>On AzureML Batchendpoint, I'm recently hitting the following error:<\/p>\n<pre><code>Unable to get image details : Environment version Autosave_(date)T(time)Z_******** provided in request doesn't match environ.\n<\/code><\/pre>\n<p>when I setup the batch-endpoint with a <code>yml<\/code> config:<\/p>\n<p><code>environment: azureml:env-name:env-version<\/code><\/p>\n<p>So, AzureML creates and builds the environment with the version I specify <code>env-version<\/code>, which is just a number (in my case = 3).<\/p>\n<p>and then for some weird reason, AzureML creates an extra environment version called <code>Autosave_(date)T(time)Z_********<\/code>, which is not built, but based on the previous one just created, and then it becomes the <code>latest<\/code> version of that environment.<\/p>\n<p>In summary, AzureML instead of looking for the version that I specified as <code>env-name:3<\/code> it seems to be looking for <code>env-name:Autosave_(date)T(time)Z_********<\/code> and then throws the error message mentioned above.<\/p>",
        "Challenge_closed_time":1648808005063,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648738692993,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while setting up a batch-endpoint on AzureML with a yml config. The error message states that the environment version provided in the request does not match the environment. AzureML creates an extra environment version called Autosave_(date)T(time)Z_********, which becomes the latest version of the environment. Instead of looking for the specified version, AzureML seems to be looking for the Autosave version, causing the error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71694816",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.2,
        "Challenge_reading_time":14.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":19.2533527778,
        "Challenge_title":"Unable to get image details : Environment version Autosave_(date)T(time)Z_******** provided in request doesn't match environ",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":92.0,
        "Challenge_word_count":135,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589293508567,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":833.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>I found the problem was that when creating an environment from a YAML specification file, one of my <strong>conda dependencies<\/strong> was <code>cmake<\/code>, which I needed to allow installation of another python module. The docker image is exactly the same as a previously created environment.<\/p>\n<p>Removing the <code>cmake<\/code> dependency from the YAML file, eliminated the issue. So the workaround is to install it using a Dockerfile.<\/p>\n<p>The error message was very misleading to start with, but got there in the end after understanding that AzureML reuses a cached image, based on the hash value, from the environment definition accordingly to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-environments#image-caching-and-reuse\" rel=\"nofollow noreferrer\">this<\/a><\/p>\n<p>So for that reason, the automatically created <code>Autosave<\/code> docker image  references to that same build, which only happens once when the first job is sent.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1649244565400,
        "Solution_link_count":1.0,
        "Solution_readability":15.1,
        "Solution_reading_time":12.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":128.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1490025251112,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":349.0,
        "Answerer_view_count":24.0,
        "Challenge_adjusted_solved_time":4421.6864666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run linear learner on a simple dataset.  My csv of data is uploaded to a bucket.  The problem is that when I run it I get the following error:<\/p>\n\n<pre><code>UnexpectedStatusException: Error for Training job linear-learner-2020-05-23-22-31-40-894: Failed. Reason: ClientError: Unable to read data channel 'train'. Requested content-type is 'application\/x-recordio-protobuf'. Please verify the data matches the requested content-type. (caused by MXNetError)\n\nCaused by: [22:34:37] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsCppLibs\/AIAlgorithmsCppLibs-2.0.2746.0\/AL2012\/generic-flavor\/src\/src\/aialgs\/io\/iterator_base.cpp:100: (Input Error) The header of the MXNet RecordIO record at position 0 in the dataset does not start with a valid magic number.\n<\/code><\/pre>\n\n<p>I did some googling and it says to change the content_type to 'text\/csv'.  My question is, how do I do this?  Or does anyone know how to get this working?  Thanks!  Here is my linear learner code:<\/p>\n\n<pre><code>container = get_image_uri(boto3.Session().region_name, 'linear-learner')\n\nlinear = sagemaker.estimator.Estimator(container,\n                                      role,\n                                      train_instance_count = 1,\n                                      train_instance_type = 'ml.c4.xlarge',\n                                      output_path = output_location,\n                                      sagemaker_session = sess)\n\nlinear.set_hyperparameters(predictor_type = 'regressor',\n                          mini_batch_size = 200)\n<\/code><\/pre>",
        "Challenge_closed_time":1606193228063,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590275156783,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while running linear learner on a dataset uploaded to a bucket. The error message suggests that the content-type requested is not matching the data. The user has found a solution online to change the content_type to 'text\/csv', but is unsure how to implement it. The user has also provided their linear learner code.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61979691",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":17.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":4421.6864666667,
        "Challenge_title":"Changing input type for linear learner to csv",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":406.0,
        "Challenge_word_count":154,
        "Platform":"Stack Overflow",
        "Poster_created_time":1585954841920,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Chicago, IL, USA",
        "Poster_reputation_count":163.0,
        "Poster_view_count":31.0,
        "Solution_body":"<p>You can use SageMaker input channels:<\/p>\n<pre><code>\ntrain_data = sagemaker.inputs.TrainingInput(\n    \"s3:\/\/my-bucket\/path\/to\/train\",\n    distribution=\"FullyReplicated\",\n    content_type=\"text\/csv\",\n    s3_data_type=\"S3Prefix\",\n    record_wrapping=None,\n    compression=None\n)\n\nvalidation_data = sagemaker.inputs.TrainingInput(\n    \"s3:\/\/my-bucket\/path\/to\/validation\",\n    distribution=\"FullyReplicated\",\n    content_type=\"text\/csv\",\n    s3_data_type=\"S3Prefix\",\n    record_wrapping=None,\n    compression=None\n)\n\nlinear.fit({\"train\": train_data, \"validation\": validation_data})\n<\/pre><\/code>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/linear_learner_abalone\/Linear_Learner_Regression_csv_format.ipynb\" rel=\"nofollow noreferrer\">See this example<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":44.9,
        "Solution_reading_time":10.77,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":34.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":524.7811111111,
        "Challenge_answer_count":3,
        "Challenge_body":"Hello,  \nI have followed the DeepAR Chicago Traffic violations notebook example. The Model and Endpoint has been created and the forecasting is working.  \n  \nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_applying_machine_learning\/deepar_chicago_traffic_violations\/deepar_chicago_traffic_violations.ipynb  \n  \nHowevr, I haven't deleted the model nor the endpoint in order to use it externally. I have created a Python script on an EC2 that tries to load the endpoint and passes the data to it to get a prediction, and here is what I am doing:  \n  \n1. Loading the CSV exactly the way I did it on the notebook  \n2. Parsing the CSV the same way I did on the notebook for the \"predictor.predict\" command  \n3. Instead of using the \"predictor.predict\", I am using \"invoke_endpoint\" to load the endpoint and passing the data from the previous point  \n4. Instead of getting the same response I got on the notebook, I am getting the following message:  \n\"type: <class 'list'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object\"  \n  \nNot sure what the issue is, seems that it requires a byte data... I guess I cannot send the data as a list to the endpoint and I need to serialize it or to encode it? convert to to JSON? to Bytes?  \n  \nAny help will be appreciated.  \nRegards",
        "Challenge_closed_time":1626970917000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625081705000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has followed the DeepAR Chicago Traffic violations notebook example and created a model and endpoint for forecasting. However, when trying to load the endpoint and pass data to it using a Python script on an EC2, they are getting an error message that requires byte data. They are unsure how to serialize or encode the data and need help to resolve the issue.",
        "Challenge_last_edit_time":1668423337511,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUpamBayk2RT6c6KuNop0DQQ\/how-to-pass-data-to-an-endpoint",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":9.0,
        "Challenge_reading_time":16.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":524.7811111111,
        "Challenge_title":"How to pass data to an endpoint",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":613.0,
        "Challenge_word_count":205,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello,  \n  \nSo the issue here is the predictor.predict command converts the data to the format necessary for the endpoint to understand, thus you need to serialize or encode the payload by yourself. To do this you can work with something like json.dumps(payload) or for a byte array json.dumps(payload).encode().  \n  \n If you want to use the predictor class this is taken care of by the serializer option. The serializer encodes\/decodes the data for us and lets you simply call the endpoint through the predictor class. An example of this is the following code snippet:   \n  \nfrom sagemaker.serializers import IdentitySerializer  \nfrom sagemaker.deserializers import JSONDeserializer  \nserializer=IdentitySerializer(content_type=\"application\/json\")  \n  \nHope this helps!  \n  \nTo check out the various serializer options that can work for your different use cases check the following link.   \nSerializers: https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html  \n  \nEdited by: rvegira-aws on Jul 22, 2021 9:22 AM  \n  \nEdited by: rvegira-aws on Jul 22, 2021 9:24 AM",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1626971072000,
        "Solution_link_count":1.0,
        "Solution_readability":13.1,
        "Solution_reading_time":13.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":143.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1378136257732,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Budapest, Hungary",
        "Answerer_reputation_count":8162.0,
        "Answerer_view_count":283.0,
        "Challenge_adjusted_solved_time":0.0478905556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working on Azure ML implementation on text analytics with NLTK, the following execution is throwing <\/p>\n\n<pre><code>AssertionError: 1 columns passed, passed data had 2 columns\\r\\nProcess returned with non-zero exit code 1\n<\/code><\/pre>\n\n<p>Below is the code <\/p>\n\n<pre><code># The script MUST include the following function,\n# which is the entry point for this module:\n# Param&lt;dataframe1&gt;: a pandas.DataFrame\n# Param&lt;dataframe2&gt;: a pandas.DataFrame\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    # import required packages\n    import pandas as pd\n    import nltk\n    import numpy as np\n    # tokenize the review text and store the word corpus\n    word_dict = {}\n    token_list = []\n    nltk.download(info_or_id='punkt', download_dir='C:\/users\/client\/nltk_data')\n    nltk.download(info_or_id='maxent_treebank_pos_tagger', download_dir='C:\/users\/client\/nltk_data')\n    for text in dataframe1[\"tweet_text\"]:\n        tokens = nltk.word_tokenize(text.decode('utf8'))\n        tagged = nltk.pos_tag(tokens)\n\n\n      # convert feature vector to dataframe object\n    dataframe_output = pd.DataFrame(tagged, columns=['Output'])\n    return [dataframe_output]\n<\/code><\/pre>\n\n<p>Error is throwing here <\/p>\n\n<pre><code> dataframe_output = pd.DataFrame(tagged, columns=['Output'])\n<\/code><\/pre>\n\n<p>I suspect this to be the tagged data type passed to dataframe, can some one let me know the right approach to add this to dataframe.<\/p>",
        "Challenge_closed_time":1471040769603,
        "Challenge_comment_count":0,
        "Challenge_created_time":1471040597197,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary_original":"The user is encountering an AssertionError while working on Azure ML implementation on text analytics with NLTK. The error message states that 1 column was passed, but the passed data had 2 columns. The error is being thrown at the line where the user is trying to convert a feature vector to a dataframe object. The user suspects that the issue is with the tagged data type passed to the dataframe.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38927230",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":18.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.0478905556,
        "Challenge_title":"Panda AssertionError columns passed, passed data had 2 columns",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":48200.0,
        "Challenge_word_count":158,
        "Platform":"Stack Overflow",
        "Poster_created_time":1370924418390,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":1748.0,
        "Poster_view_count":339.0,
        "Solution_body":"<p>Try this:<\/p>\n\n<pre><code>dataframe_output = pd.DataFrame(tagged, columns=['Output', 'temp'])\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.4,
        "Solution_reading_time":1.5,
        "Solution_score_count":13.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":7.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":185.8020663889,
        "Challenge_answer_count":7,
        "Challenge_body":"<p>Dear W&amp;B Community,<\/p>\n<p>I have system metrics logged like the \u201c<em>time per step<\/em>\u201d or \u201c<em>time per backward pass<\/em>\u201d for a model.<br>\nWhen doing this on different hardware, I would like to compare the effect this has on these metrics.<br>\nIn the following examples, I profile the basic Torch CIFAR10 model on a 1,2,4,8,16 and 32 CPU VM.<\/p>\n<p>When looking at a <code>Linechart<\/code>, the full history of these metrics is visible, however, it is very hard to compare them due to the overlapping and oscillation:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351.png\" data-download-href=\"\/uploads\/short-url\/zZROm2jlGDN4WUrxx2lQXAA8jYZ.png?dl=1\" title=\"W&amp;amp;B Chart 9_19_2022, 6_22_16 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351_2_690x362.png\" alt=\"W&amp;B Chart 9_19_2022, 6_22_16 PM\" data-base62-sha1=\"zZROm2jlGDN4WUrxx2lQXAA8jYZ\" width=\"690\" height=\"362\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351_2_690x362.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351_2_1035x543.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351_2_1380x724.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">W&amp;amp;B Chart 9_19_2022, 6_22_16 PM<\/span><span class=\"informations\">3539\u00d71859 509 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>When using a <code>Barchart<\/code>, only the last value is visualized:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/48a4597177e867b3eb511112ad23b561f18f1137.png\" data-download-href=\"\/uploads\/short-url\/amCuG3pzRgnimYoyoJeru5muDMH.png?dl=1\" title=\"W&amp;amp;B Chart 9_19_2022, 6_20_31 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/48a4597177e867b3eb511112ad23b561f18f1137_2_690x362.png\" alt=\"W&amp;B Chart 9_19_2022, 6_20_31 PM\" data-base62-sha1=\"amCuG3pzRgnimYoyoJeru5muDMH\" width=\"690\" height=\"362\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/48a4597177e867b3eb511112ad23b561f18f1137_2_690x362.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/48a4597177e867b3eb511112ad23b561f18f1137_2_1035x543.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/48a4597177e867b3eb511112ad23b561f18f1137_2_1380x724.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/48a4597177e867b3eb511112ad23b561f18f1137_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">W&amp;amp;B Chart 9_19_2022, 6_20_31 PM<\/span><span class=\"informations\">3539\u00d71859 251 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>The functionality that would be nice is to group values based on their count or occurrence, as grouping by runs already works perfectly. Here\u2019s the same data but run through <code>seaborn.barplot<\/code>:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479.png\" data-download-href=\"\/uploads\/short-url\/7VTQur5SLq8cPHTQtTqGrDuwPRn.png?dl=1\" title=\"download\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479_2_690x427.png\" alt=\"download\" data-base62-sha1=\"7VTQur5SLq8cPHTQtTqGrDuwPRn\" width=\"690\" height=\"427\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479_2_690x427.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479_2_1035x640.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479_2_1380x854.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">download<\/span><span class=\"informations\">3777\u00d72341 159 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Would this be possible to implement? Or does anybody know a way to get that functionality?<\/p>\n<p>My current workaround is to download the data manually and run it through seaborn. Unfortunately, I did not understand the errors I\u2019ve gotten with the <code>Custom Chart<\/code> functionality when trying to port Vega examples to use wandb as a data basis.<\/p>\n<p>I\u2019d be very glad if anybody can point me to a tutorial on how to migrate existing Vega examples to be used with wandb (and the common problems, like differences between v3\/v4\/v5, as these seemed to be an issue for me).<\/p>",
        "Challenge_closed_time":1664274024356,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663605136917,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has system metrics logged for a model and wants to compare the effect of different hardware on these metrics. They have tried using Linechart and Barchart but are facing difficulties in comparing the metrics due to overlapping and oscillation. The user is looking for a way to group values based on their count or occurrence and is seeking help in implementing this functionality or migrating existing Vega examples to be used with wandb.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/barchart-grouping-by-time-step-count\/3157",
        "Challenge_link_count":18,
        "Challenge_participation_count":7,
        "Challenge_readability":21.3,
        "Challenge_reading_time":80.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":185.8020663889,
        "Challenge_title":"Barchart Grouping by Time\/Step\/Count",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":803.0,
        "Challenge_word_count":367,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Alexander,<\/p>\n<p>Thanks for sending this detailed explanation! I have been exploring it and I think that the issue here is that, in lines 22, 29 and 43 you have \u201cdata\u201d: \u201ctable\u201d but as the name has been changed to \u201cwandb\u201d, then you should have \u201cdata\u201d: \u201cwandb\u201d. To solve the error between lines 4 and 6, you can use <span class=\"chcklst-box fa fa-square-o fa-fw\"><\/span> and it is solved, but it seems that it is not affecting to the chart.<\/p>\n<pre><code>\"data\": [{ \"name\": \"wandb\" }]\n<\/code><\/pre>\n<p>Please let me know if this would be useful for you!<\/p>\n<p>Best,<br>\nLuis<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":7.17,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":96.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.7235097222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>We are experimenting with programmatic report generation with WandB.<br>\nI would like to be able to add a Confusion Matrix to a report, but this is not one of the base types (as far as I can tell). Is there a good way to do this?<br>\nI could generate a PNG\/Image and insert it, but I can\u2019t figure out how to add an Image to a report yet (see recent question).  Are there other ways?<br>\nThanks.<\/p>",
        "Challenge_closed_time":1666015569168,
        "Challenge_comment_count":0,
        "Challenge_created_time":1665987764533,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to generate a report programmatically using WandB and wants to add a Confusion Matrix to the report. However, they are unable to find a way to add it as it is not one of the base types. They are considering generating a PNG\/Image and inserting it, but are unsure how to add an image to the report.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/adding-confusion-matrix-to-report-programatically\/3267",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.4,
        "Challenge_reading_time":5.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":7.7235097222,
        "Challenge_title":"Adding Confusion Matrix to report programatically",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":140.0,
        "Challenge_word_count":81,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/kevinashaw\">@kevinashaw<\/a> I am also posting here <a href=\"https:\/\/colab.research.google.com\/drive\/1Fepp-JLFvK-wLL2BZ_BnkCG_fAC6HFbo#scrollTo=An_example_with_all_of_the_blocks_and_panels\" rel=\"noopener nofollow ugc\">this Colab<\/a> and the Python SDK commands of our <a href=\"https:\/\/docs.wandb.ai\/guides\/reports\/edit-a-report#add-plots\">Reports reference docs<\/a> which may be helpful.<\/p>\n<p>The confusion matrix isn\u2019t <a href=\"https:\/\/github.com\/wandb\/wandb\/blob\/main\/wandb\/apis\/reports\/panels.py\" rel=\"noopener nofollow ugc\">currently exposed<\/a> but I have increased this feature requests for our engineering team. We will reach out to you once this is implemented. I hope this helps, please let me know if you have any further questions or issues with the Reports API.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.7,
        "Solution_reading_time":10.75,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":81.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":233.3348527778,
        "Challenge_answer_count":7,
        "Challenge_body":"<p>I\u2019m using AWS Sagemaker to train a Keras model with the Wandb callback. In my Sagemaker script, I save checkpoints to <code>'\/opt\/ml\/checkpoints\/'<\/code> which it redirects to an s3 bucket continuously. After the model has finished training, I create my artifact and add a reference to that bucket.<\/p>\n<p>Later, if I try to download the model with:<\/p>\n<pre><code class=\"lang-auto\">model_path = run.use_artifact(...)\nmodel_path.download()\n<\/code><\/pre>\n<p>I get the following error:<\/p>\n<blockquote>\n<p>ValueError: Digest mismatch for object s3:\/\/\u2026\/variables\/variables.data-00000-of-00001: expected 4f8d37a52a3e87f1f0ee2d3101688848-3 but found 8ad5ef5242d547d7edaa76f620597b60-3<\/p>\n<\/blockquote>\n<p>My guess is that I\u2019ve added the reference to the artifact before Sagemaker has pushed the final model from the local directory to S3. I\u2019m not sure how to get around this, is there a better way to have my Artifacts be linked to an S3 bucket?<\/p>",
        "Challenge_closed_time":1666891898080,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666051892610,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a digest mismatch error while trying to download a model artifact from S3 in AWS Sagemaker. The error is caused by adding a reference to the artifact before the final model is pushed from the local directory to S3. The user is unsure how to resolve the issue and is seeking a better way to link Artifacts to an S3 bucket.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/digest-mismatch-error-when-trying-to-download-model-artifact-from-s3\/3269",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":9.9,
        "Challenge_reading_time":12.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":233.3348527778,
        "Challenge_title":"Digest mismatch error when trying to download model artifact from S3",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":308.0,
        "Challenge_word_count":136,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/dspectrum\">@dspectrum<\/a>,<\/p>\n<p>Looking at your error and tracing back through our code - looks like versioning is not enabled on your S3 bucket, which means the artifact is changing the file itself, leading to different hashes. I would suggest turning on versioning on your S3 bucket and letting me know if you still run into the same error.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.6,
        "Solution_reading_time":4.74,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":59.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1618467374027,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":539.4853230556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created a Tabular Dataset using Azure ML python API. Data under question is a bunch of parquet files (~10K parquet files each of size of 330 KB) residing in Azure Data Lake Gen 2 spread across multiple partitions. When I trigger &quot;Generate Profile&quot; operation for the dataset, it throws following error while handling empty parquet file and then the profile generation stops.<\/p>\n<pre><code>User program failed with ExecutionError: \nError Code: ScriptExecution.StreamAccess.Validation\nValidation Error Code: NotSupported\nValidation Target: ParquetFile\nFailed Step: 77866d0a-8243-4d3d-8bc6-599d466488dd\nError Message: ScriptExecutionException was caused by StreamAccessException.\n  Failed to read Parquet file at: &lt;my_blob_path&gt;\/20211217.parquet\n    Current parquet file is not supported.\n      Exception of type 'Thrift.Protocol.TProtocolException' was thrown.\n| session_id=6be4db0b-bdc1-4dd6-b8a6-6e9466f7bc54\n\n<\/code><\/pre>\n<p>By empty parquet file, I mean that the if I read the individual parquet file using pandas (<code>pd.read_parquet<\/code>), it results in an empty DF (df.empty == True).<\/p>\n<p>Any suggestion to avoid this error will be appreciated.<\/p>\n<p><strong>Update<\/strong>\nThe issue has been fixed in the following version:<\/p>\n<ul>\n<li>azureml-dataprep : 3.0.1<\/li>\n<li>azureml-core :  1.40.0<\/li>\n<\/ul>",
        "Challenge_closed_time":1646432534340,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644490387177,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while using Azure ML python API to generate a dataset profile for a large number of parquet files stored in Azure Data Lake Gen 2. The error is caused by an empty parquet file, which is not supported by the script. The user is seeking suggestions to avoid this error. An update has been provided, stating that the issue has been fixed in the latest versions of azureml-dataprep and azureml-core.",
        "Challenge_last_edit_time":1648643496672,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71063820",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.2,
        "Challenge_reading_time":17.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":539.4853230556,
        "Challenge_title":"AzureML: Dataset Profile fails when parquet file is empty",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":255.0,
        "Challenge_word_count":169,
        "Platform":"Stack Overflow",
        "Poster_created_time":1280505139752,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, India",
        "Poster_reputation_count":4265.0,
        "Poster_view_count":403.0,
        "Solution_body":"<p>Thanks for reporting it.\nThis is a bug in handling of the parquet files with columns but empty row set. This has been fixed already and will be included in next release.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.9,
        "Solution_reading_time":2.13,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":32.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1361290436103,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":690.0,
        "Answerer_view_count":38.0,
        "Challenge_adjusted_solved_time":3.1432802778,
        "Challenge_answer_count":8,
        "Challenge_body":"<p>I've just started to experiment with AWS SageMaker and would like to load data from an S3 bucket into a pandas dataframe in my SageMaker python jupyter notebook for analysis.<\/p>\n\n<p>I could use boto to grab the data from S3, but I'm wondering whether there is a more elegant method as part of the SageMaker framework to do this in my python code?<\/p>\n\n<p>Thanks in advance for any advice.<\/p>",
        "Challenge_closed_time":1516036562536,
        "Challenge_comment_count":0,
        "Challenge_created_time":1516025246727,
        "Challenge_favorite_count":15.0,
        "Challenge_gpt_summary_original":"The user is seeking advice on how to load data from an S3 bucket into a pandas dataframe in their AWS SageMaker python jupyter notebook for analysis. They are wondering if there is a more elegant method within the SageMaker framework to do this.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48264656",
        "Challenge_link_count":0,
        "Challenge_participation_count":8,
        "Challenge_readability":8.3,
        "Challenge_reading_time":5.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":57.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":3.1432802778,
        "Challenge_title":"Load S3 Data into AWS SageMaker Notebook",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":78494.0,
        "Challenge_word_count":75,
        "Platform":"Stack Overflow",
        "Poster_created_time":1487350945116,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":673.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>If you have a look <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-dg.pdf\" rel=\"noreferrer\">here<\/a> it seems you can specify this in the <em>InputDataConfig<\/em>. Search for \"S3DataSource\" (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_S3DataSource.html\" rel=\"noreferrer\">ref<\/a>) in the document. The first hit is even in Python, on page 25\/26.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.2,
        "Solution_reading_time":5.17,
        "Solution_score_count":11.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":36.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":77.7987138889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I got this error when running a job in Azure Machine Learning Studio. Any ideas about how to fix it?<\/p>\n<pre><code>Error Code: ScriptExecution.StreamAccess.Unexpected\nNative Error: error in streaming from input data sources\n\tStreamError(Unknown(&quot;unsuccessful status code 409 Conflict, body &quot;, None))\n=&gt; unsuccessful status code 409 Conflict, body \n\tUnknown(&quot;unsuccessful status code 409 Conflict, body &quot;, None)\nError Message: Got unexpected error: unsuccessful status code 409 Conflict, body . | session_id=96032e2f-c1e6-423c-8225-c1c460b3192f\n<\/code><\/pre>",
        "Challenge_closed_time":1676870036790,
        "Challenge_comment_count":0,
        "Challenge_created_time":1676589961420,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error code ScriptExecution.StreamAccess.Unexpected while running a job in Azure Machine Learning Studio, which resulted in an unsuccessful status code 409 Conflict. The user is seeking help to fix this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1181563\/error-code-scriptexecution-streamaccess-unexpected",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":8.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":77.7987138889,
        "Challenge_title":"Error Code: ScriptExecution.StreamAccess.Unexpected when running a job in AzureML Studio",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":77,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=2506b2a8-0b14-4610-bdd6-41ac619af16a\">@Maria Rivera Araya  <\/a>The error message &quot;unsuccessful status code 409 Conflict, body&quot; suggests that there is a conflict with the input data sources. This error can occur when the input data sources are being modified while the job is running.&lt;sup&gt;<a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/main\/articles\/machine-learning\/component-reference\/designer-error-codes.md\">[2]<\/a>&lt;\/sup&gt;<\/p>\n<p>You can try the following steps to resolve the issue: Wait for the input data sources to finish being modified.<\/p>\n<ol>\n<li> If the input data sources are not being modified, try restarting the job.<\/li>\n<li> If the issue persists, try using a different input data source.<\/li>\n<\/ol>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.9,
        "Solution_reading_time":10.12,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":79.1199802778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We are defining in Databricks a PythonScriptStep(). When using PythonScriptStep() within our pipeline script we can't find the scoring.py file.<\/p>\n<pre><code>scoring_step = PythonScriptStep(\n    name=&quot;Scoring_Step&quot;,\n    source_directory=os.getenv(&quot;DATABRICKS_NOTEBOOK_PATH&quot;, &quot;\/Users\/USER_NAME\/source_directory&quot;),\n    script_name=&quot;.\/scoring.py&quot;,\n    arguments=[&quot;--input_dataset&quot;, ds_consumption],\n    compute_target=pipeline_cluster,\n    runconfig=pipeline_run_config,\n    allow_reuse=False)\n<\/code><\/pre>\n<p>We getting the following error message:<\/p>\n<pre><code>Step [Scoring_Step]: script not found at: \/databricks\/driver\/scoring.py. Make sure to specify an appropriate source_directory on the Step or default_source_directory on the Pipeline.\n<\/code><\/pre>\n<p>For some reason Databricks is searching for the file in '\/databricks\/driver\/' instead of the folder we entered.<\/p>\n<p>There is also the way to use DatabricksStep() instead of PythonScriptStep(), but because of specific reasons we need to use the PythonSriptStep() class.<\/p>\n<p>Could anybody help us with this specific problem?<\/p>\n<p>Thank you very much for any help!<\/p>",
        "Challenge_closed_time":1656324774632,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655997421433,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while defining a PythonScriptStep() in Databricks. They are unable to find the scoring.py file and are receiving an error message indicating that Databricks is searching for the file in '\/databricks\/driver\/' instead of the specified folder. The user needs to use PythonScriptStep() instead of DatabricksStep() due to specific reasons. They are seeking assistance to resolve this issue.",
        "Challenge_last_edit_time":1656039942703,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72732616",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":16.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":90.9314441667,
        "Challenge_title":"Can't find scoring.py when using PythonScriptStep() in Databricks",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":68.0,
        "Challenge_word_count":123,
        "Platform":"Stack Overflow",
        "Poster_created_time":1544598969960,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":37.0,
        "Poster_view_count":21.0,
        "Solution_body":"<pre><code>scoring_step = PythonScriptStep(\n    name=&quot;Scoring_Step&quot;,\n    source_directory=os.getenv(&quot;DATABRICKS_NOTEBOOK_PATH&quot;, &quot;\/Users\/USER_NAME\/source_directory&quot;),\n    script_name=&quot;.\/scoring.py&quot;,\n    arguments=[&quot;--input_dataset&quot;, ds_consumption],\n    compute_target=pipeline_cluster,\n    runconfig=pipeline_run_config,\n    allow_reuse=False)\n<\/code><\/pre>\n<p>Change the above code block with below code block. It will resolve the error<\/p>\n<pre><code>data_ref = OutputFileDatasetConfig(\n    name='data_ref',\n    destination=(ds, '\/data')\n).as_upload()\n\n\ndata_prep_step = PythonScriptStep(\n    name='data_prep',\n    script_name='pipeline_steps\/data_prep.py',\n    source_directory='\/.',\n    arguments=[\n        '--main_path', main_ref,\n        '--data_ref_folder', data_ref\n                ],\n    inputs=[main_ref, data_ref],\n    outputs=[data_ref],\n    runconfig=arbitrary_run_config,\n    allow_reuse=False\n)\n<\/code><\/pre>\n<p>Reference link for the <a href=\"https:\/\/scoring_step%20=%20PythonScriptStep(%20%20%20%20%20name=%22Scoring_Step%22,%20%20%20%20%20source_directory=os.getenv(%22DATABRICKS_NOTEBOOK_PATH%22,%20%22\/Users\/USER_NAME\/source_directory%22),%20%20%20%20%20script_name=%22.\/scoring.py%22,%20%20%20%20%20arguments=%5B%22--input_dataset%22,%20ds_consumption%5D,%20%20%20%20%20compute_target=pipeline_cluster,%20%20%20%20%20runconfig=pipeline_run_config,%20%20%20%20%20allow_reuse=False)\" rel=\"nofollow noreferrer\">documentation<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":32.4,
        "Solution_reading_time":19.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":56.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.6372222222,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nWhat parquet data loading logic is known to work well to train with SageMaker on parquet? ml-io? pyarrow? any examples? That would be to train a classifier, either logistic regression, XGBoost or custom TF.",
        "Challenge_closed_time":1588843302000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1588841008000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking advice on the best parquet data loading logic to use with SageMaker for training a classifier, specifically logistic regression, XGBoost, or custom TF. They are asking for examples and recommendations for ml-io and pyarrow.",
        "Challenge_last_edit_time":1668588105088,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUCqvDUq4hSQqRT97tBUvE8Q\/training-a-classifier-on-parquet-with-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.8,
        "Challenge_reading_time":3.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.6372222222,
        "Challenge_title":"Training a classifier on parquet with SageMaker ?",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":424.0,
        "Challenge_word_count":42,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"XGBoost as a framework container (v0.90+) can read parquet for training (see example [notebook][1]).  \nThe full list of valid content types are CSV, LIBSVM, PARQUET, RECORDIO_PROTOBUF (see [source][2]) \n\nAdditionally:  \n[Uber Petastorm][3] for reading parquet into Tensorflow, Pytorch, and PySpark inputs.   \nAs XGBoost accepts numpy, you can convert from PySpark to numpy\/pandas using the mentioned PyArrow.\n\n\n  [1]: https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/caf9363c0242d0da2de7f5765e7318fd843ce4c3\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_parquet_input_training.ipynb\n  [2]: https:\/\/github.com\/aws\/sagemaker-xgboost-container\/blob\/5e778770e009ce989e288e7bbc1255556129e75b\/src\/sagemaker_xgboost_container\/data_utils.py#L40\n  [3]: https:\/\/github.com\/uber\/petastorm",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925592848,
        "Solution_link_count":3.0,
        "Solution_readability":19.1,
        "Solution_reading_time":10.59,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":61.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1395413944963,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":422.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":426.1035044444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to get MLFlow on another machine in a local network to run and I would like to ask for some help because I don't know what to do now.<\/p>\n\n<p>I have a mlflow server running on a <em>server<\/em>. The mlflow server is running under my user on the <em>server<\/em> and has been started like this: <\/p>\n\n<pre><code>mlflow server --host 0.0.0.0 --port 9999 --default-artifact-root sftp:\/\/&lt;MYUSERNAME&gt;@&lt;SERVER&gt;:&lt;PATH\/TO\/DIRECTORY\/WHICH\/EXISTS&gt;\n<\/code><\/pre>\n\n<p>My program which should log all the data to the mlflow server looks like this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from mlflow import log_metric, log_param, log_artifact, set_tracking_uri\n\nif __name__ == \"__main__\":\n    remote_server_uri = '&lt;SERVER&gt;' # this value has been replaced\n    set_tracking_uri(remote_server_uri)\n    # Log a parameter (key-value pair)\n    log_param(\"param1\", 5)\n\n    # Log a metric; metrics can be updated throughout the run\n    log_metric(\"foo\", 1)\n    log_metric(\"foo\", 2)\n    log_metric(\"foo\", 3)\n\n    # Log an artifact (output file)\n    with open(\"output.txt\", \"w\") as f:\n        f.write(\"Hello world!\")\n    log_artifact(\"output.txt\")\n\n<\/code><\/pre>\n\n<p>The parameters get and metrics get transfered to the server but not the artifacts. Why is that so?<\/p>\n\n<p>Note on the SFTP part:\nI can log in via SFTP and the pysftp package is installed<\/p>",
        "Challenge_closed_time":1575967801236,
        "Challenge_comment_count":1,
        "Challenge_created_time":1574429522373,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to run MLFlow on a remote server in a local network and has set up the MLFlow server on the remote server. The user's program logs parameters, metrics, and artifacts to the MLFlow server, but only the parameters and metrics are being transferred, not the artifacts. The user is seeking help to understand why the artifacts are not being transferred.",
        "Challenge_last_edit_time":1574433828620,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58995329",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.1,
        "Challenge_reading_time":17.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":9.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":427.2996841667,
        "Challenge_title":"Artifact storage and MLFLow on remote server",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":3483.0,
        "Challenge_word_count":181,
        "Platform":"Stack Overflow",
        "Poster_created_time":1395413944963,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":422.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>I don't know if I will get an answer to my problem but I did <em>solved<\/em> it this way.<\/p>\n\n<p>On the server I created the directory <code>\/var\/mlruns<\/code>. I pass this directory to mlflow via <code>--backend-store-uri file:\/\/\/var\/mlruns<\/code><\/p>\n\n<p>Then I mount this directory via e.g. <code>sshfs<\/code> on my local machine under the same path.<\/p>\n\n<p>I don't like this solution but it solved the problem good enough for now.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.3,
        "Solution_reading_time":5.51,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":66.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2386394444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am looking for sample of notebook\/SDK, but this link is not working at all. Any new repo for reference? <a href=\"https:\/\/learn.microsoft.com\/en-us\/samples\/azure\/azureml-examples\/azure-machine-learning-examples\/\">https:\/\/learn.microsoft.com\/en-us\/samples\/azure\/azureml-examples\/azure-machine-learning-examples\/<\/a><\/p>",
        "Challenge_closed_time":1669743760392,
        "Challenge_comment_count":0,
        "Challenge_created_time":1669742901290,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to access the Azure machine learning samples from the provided link and is looking for an alternative repository for reference.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1109020\/azure-machine-learning-samples-404",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.5,
        "Challenge_reading_time":4.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.2386394444,
        "Challenge_title":"Azure machine learning samples 404",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":26,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=79877fab-184b-4267-bfdd-49ea0b34bfe1\">@Yadama Kenzan  <\/a>     <\/p>\n<p>Thanks for reporting this issue, is there any place you got the link or it's from the web search? This link has been deprecated.    <\/p>\n<p>Please see this repo for SDK V2 samples\/ CLI V2 samples - <a href=\"https:\/\/github.com\/Azure\/azureml-examples\">https:\/\/github.com\/Azure\/azureml-examples<\/a>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/265362-image.png?platform=QnA\" alt=\"265362-image.png\" \/>    <\/p>\n<p>Please let me know where this link is from so that I can fix the resource as well.     <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.4,
        "Solution_reading_time":9.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":89.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1479363468550,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pune, Maharashtra, India",
        "Answerer_reputation_count":108.0,
        "Answerer_view_count":35.0,
        "Challenge_adjusted_solved_time":131.9191969444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to do some kind of web job application that can run for period time and make prediction on azure machine learning studio. After that i want get the result of this experiment and do something with that in my console application. What is the best way to do this in azure with machine learning or maybe some similiar stuff to prediction data from data series ? <\/p>",
        "Challenge_closed_time":1486537080116,
        "Challenge_comment_count":0,
        "Challenge_created_time":1486062508683,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a web job application that can run for a period of time and make predictions on Azure Machine Learning Studio. They are seeking advice on the best way to pass input for the experiment from their console application and retrieve the results.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42010405",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":5.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":131.8253980556,
        "Challenge_title":"The way to pass input for azure machine experiment from app ( for example console app )",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":62.0,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432141466928,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":327.0,
        "Poster_view_count":78.0,
        "Solution_body":"<p>You can try using Azure Data Factory to create a Machine Learning pipeline or use Azure ML Studio's Predictive Web Services.<\/p>\n\n<ol>\n<li><p>With Azure Data Factory\nFollow <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/data-factory\/data-factory-azure-ml-batch-execution-activity\" rel=\"nofollow noreferrer\">this link<\/a> for details. Azure Data Factory implementations would seem difficult at first but they do work great with Azure ML experiments. <\/p>\n\n<p>Azure Data Factory can run your ML Experiment on a schedule or one-off at a specified time (I guess you can set only for UTC Timezone right now) and monitor it through a dashboard (which is pretty cool).<\/p>\n\n<p>As an example you can look @ <a href=\"https:\/\/github.com\/Microsoft\/azure-docs\/blob\/master\/articles\/data-factory\/data-factory-azure-ml-batch-execution-activity.md\" rel=\"nofollow noreferrer\">ML Batch Execution<\/a>. I used this in one of our implementations (we do have latency issues, but trying to solve that).<\/p><\/li>\n<li><p>If you directly want to use the experiment in your console (assuming it is a web application), use create a Predictive Web service out of your ML Experiment, details <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-walkthrough-5-publish-web-service\" rel=\"nofollow noreferrer\">here<\/a><\/p><\/li>\n<\/ol>\n\n<p>I couldn't exactly understand your use case so I posted two alternatives that should help you. Hope this might lead you to a better solution\/approach.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1486537417792,
        "Solution_link_count":3.0,
        "Solution_readability":14.4,
        "Solution_reading_time":19.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":180.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1639972620503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1653.0,
        "Answerer_view_count":1212.0,
        "Challenge_adjusted_solved_time":500.2513305555,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>While the artifact repository was successfully creating, running a docker push to push the image to the google artifact registry fails with a permissions error even after granting all artifact permissions to the accounting I am using on gcloud cli.<\/p>\n<p><strong>Command used to push image:<\/strong><\/p>\n<pre><code>docker push us-central1-docker.pkg.dev\/project-id\/repo-name:v2\n<\/code><\/pre>\n<p><strong>Error message:<\/strong><\/p>\n<pre><code>The push refers to repository [us-central1-docker.pkg.dev\/project-id\/repo-name]\n6f6f4a472f31: Preparing\nbc096d7549c4: Preparing\n5f70bf18a086: Preparing\n20bed28d4def: Preparing\n2a3255c6d9fb: Preparing\n3f5d38b4936d: Waiting\n7be8268e2fb0: Waiting\nb889a93a79dd: Waiting\n9d4550089a93: Waiting\na7934564e6b9: Waiting\n1b7cceb6a07c: Waiting\nb274e8788e0c: Waiting\n78658088978a: Waiting\ndenied: Permission &quot;artifactregistry.repositories.downloadArtifacts&quot; denied on resource &quot;projects\/project-id\/locations\/us-central1\/repositories\/repo-name&quot; (or it may not exist)\n\n\n<\/code><\/pre>",
        "Challenge_closed_time":1652683203067,
        "Challenge_comment_count":3,
        "Challenge_created_time":1652644857243,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered a permissions error while trying to push an image to the Google Artifact Registry using the \"docker push\" command. The error message indicates that the permission \"artifactregistry.repositories.downloadArtifacts\" was denied on the resource \"projects\/project-id\/locations\/us-central1\/repositories\/repo-name\". The user had already granted all artifact permissions to the accounting being used on gcloud cli.",
        "Challenge_last_edit_time":1652667678670,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72251787",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":19.2,
        "Challenge_reading_time":14.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":12.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":10.6516177778,
        "Challenge_title":"Permission \"artifactregistry.repositories.downloadArtifacts\" denied on resource",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":5722.0,
        "Challenge_word_count":100,
        "Platform":"Stack Overflow",
        "Poster_created_time":1426920929352,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangkok",
        "Poster_reputation_count":194.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>I was able to recreate your use case. This happens when you are trying to push an image on a <code>repository<\/code> in which its specific hostname (associated with it's repository location) is not yet  added to the credential helper configuration for authentication. You may refer to this <a href=\"https:\/\/cloud.google.com\/artifact-registry\/docs\/docker\/authentication\" rel=\"noreferrer\">Setting up authentication for Docker <\/a> as also provided by @DazWilkin in the comments for more details.<\/p>\n<p>In my example, I was trying to push an image on a repository that has a location of <code>us-east1<\/code> and got the same error since it is not yet added to the credential helper configuration.\n<a href=\"https:\/\/i.stack.imgur.com\/NQeIf.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NQeIf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>And after I ran the authentication using below command (specifically for us-east1 since it is the <code>location<\/code> of my repository), the image was successfully pushed:<\/p>\n<pre><code>gcloud auth configure-docker us-east1-docker.pkg.dev\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q2Q9x.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q2Q9x.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><em><strong>QUICK TIP<\/strong><\/em>: You may  get your authentication command specific for your repository when you open your desired repository in the <a href=\"https:\/\/console.cloud.google.com\/artifacts\" rel=\"noreferrer\">console<\/a>, and then click on the <code>SETUP INSTRUCTIONS<\/code>.\n<a href=\"https:\/\/i.stack.imgur.com\/KBjqa.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KBjqa.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1654468583460,
        "Solution_link_count":8.0,
        "Solution_readability":14.8,
        "Solution_reading_time":22.51,
        "Solution_score_count":23.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":188.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1403392071732,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":91.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":21.2274555556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>This command:<\/p>\n\n<pre><code>BUCKET_TO_READ='my-bucket'\nFILE_TO_READ='myFile'\ndata_location = 's3:\/\/{}\/{}'.format(BUCKET_TO_READ, FILE_TO_READ)\ndf=pd.read_csv(data_location)\n<\/code><\/pre>\n\n<p>is failing with a <\/p>\n\n<pre><code>ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden\n<\/code><\/pre>\n\n<p>Error and I'm unable to figure out why.  That should work according to <a href=\"https:\/\/stackoverflow.com\/a\/50244897\/3763782\">https:\/\/stackoverflow.com\/a\/50244897\/3763782<\/a> <\/p>\n\n<p>Here are my permissions on the bucket:<\/p>\n\n<pre><code>            \"Action\": [\n                \"s3:ListMultipartUploadParts\",\n                \"s3:ListBucket\",\n                \"s3:GetObjectVersionTorrent\",\n                \"s3:GetObjectVersionTagging\",\n                \"s3:GetObjectVersionAcl\",\n                \"s3:GetObjectVersion\",\n                \"s3:GetObjectTorrent\",\n                \"s3:GetObjectTagging\",\n                \"s3:GetObjectAcl\",\n                \"s3:GetObject\"\n<\/code><\/pre>\n\n<p>And these commands work as expected: <\/p>\n\n<pre><code>role = get_execution_role()\nregion = boto3.Session().region_name\nprint(role)\nprint(region)\n\ns3 = boto3.resource('s3')\nbucket = s3.Bucket(BUCKET_TO_READ)\nprint(bucket.creation_date)\n\nfor my_bucket_object in bucket.objects.all():\n    print(my_bucket_object)\n    FILE_TO_READ = my_bucket_object.key\n    break\n\nobj = s3.Object(BUCKET_TO_READ, FILE_TO_READ)\nprint(obj)\n\n<\/code><\/pre>\n\n<p>All of those print statements worked just fine.  <\/p>\n\n<p>I'm not sure if it matters, but each file is within a folder, so my FILE_TO_READ looks like <code>folder\/file<\/code>.<\/p>\n\n<p>This command which should download the file to sagemaker also falied with a 403:<\/p>\n\n<pre><code>import boto3\ns3 = boto3.resource('s3')\ns3.Object(BUCKET_TO_READ, FILE_TO_READ).download_file(FILE_TO_READ)\n<\/code><\/pre>\n\n<p>This is also happening when I open a terminal and use <\/p>\n\n<pre><code>aws s3 cp AWSURI local_file_name\n<\/code><\/pre>",
        "Challenge_closed_time":1582382844412,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582298932137,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a 403 forbidden error when trying to read a file from s3 to sagemaker on AWS using the pd.read_csv() command and is unable to figure out why. The user has provided the permissions on the bucket and other commands that work as expected. The error is also occurring when trying to download the file to sagemaker or using the aws s3 cp command in the terminal.",
        "Challenge_last_edit_time":1582306425572,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60341782",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":15.0,
        "Challenge_reading_time":24.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":23.3089652778,
        "Challenge_title":"Reading a file from s3 to sagemaker on AWS gives 403 forbidden error, but other operations work on the file",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2322.0,
        "Challenge_word_count":179,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403392071732,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":91.0,
        "Poster_view_count":28.0,
        "Solution_body":"<p>The reason was that we granted permission to the bucket not the objects.  That would be granting <code>\"Resource\": \"arn:aws:s3:::bucket-name\/\"<\/code> but not <code>\"Resource\": \"arn:aws:s3:::bucket-name\/*\"<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.0,
        "Solution_reading_time":2.86,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":23.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1604747085276,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":24.4361597222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working on a CNN project and I would like to log the model.summary to neptune.ai. The intention of that is to have an idea about the model parameters while comparing different models. Any help\/tips would be much appreciated!<\/p>",
        "Challenge_closed_time":1604748950752,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604660980577,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is working on a CNN project and wants to log the model.summary to neptune.ai to compare different models and have an idea about the model parameters. They are seeking help or tips to achieve this.",
        "Challenge_last_edit_time":1660057709880,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64713492",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.1,
        "Challenge_reading_time":3.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":24.4361597222,
        "Challenge_title":"Is there a way to log the keras model summary to neptune?",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":285.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604146329127,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>You can log <code>model.summary<\/code> (assuming it's keras), like this:<\/p>\n<pre><code>neptune.init('workspace\/project')\nneptune.create_experiment()\n\nmodel = keras.Sequential(...)\nmodel.summary(print_fn=lambda x: neptune.log_text('model_summary', x))\n<\/code><\/pre>\n<p>This will log entire summary as lines of text. You can later browse it in the <em>Logs<\/em> section of the experiment. Look for tile: &quot;model_summary&quot; in this <a href=\"https:\/\/ui.neptune.ai\/o\/USERNAME\/org\/example-project\/e\/HELLO-325\/logs\" rel=\"nofollow noreferrer\">example<\/a>.<\/p>\n<p>Another option - for easier compare - is to log hyper-parameters at experiment creation, like this:<\/p>\n<pre><code># Define parameters as Python dict\nPARAMS = {'batch_size': 64,\n          'n_epochs': 100,\n          'shuffle': True,\n          'activation': 'elu'}\n\n# Pass PARAMS dict to params at experiment creation\nneptune.create_experiment(params=PARAMS)\n<\/code><\/pre>\n<p>You will have them in <em>Parameters<\/em> tab of the experiment, like in this <a href=\"https:\/\/ui.neptune.ai\/o\/USERNAME\/org\/example-project\/e\/HELLO-44\/parameters\" rel=\"nofollow noreferrer\">example<\/a>. You will be able to add each parameter as a column to the dashboard for quick compare. Look for greenish columns in this <a href=\"https:\/\/ui.neptune.ai\/o\/USERNAME\/org\/example-project\/experiments?viewId=d7f80ebe-5bfe-4d12-97c1-2b1e6184a2ed\" rel=\"nofollow noreferrer\">dashboard<\/a>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":14.0,
        "Solution_reading_time":18.48,
        "Solution_score_count":2.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":132.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":63.8910877778,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Related: <a href=\"https:\/\/community.wandb.ai\/t\/unable-to-manage-columns-in-project-run-table\/3551\/4\" class=\"inline-onebox\">Unable to manage columns in project run table - #4 by artsiom<\/a><\/p>\n<p>I was unable to make step metric columns visible in the Table view. I tried logging metrics both via <code>run.log<\/code> and <code>wandb.log<\/code>, as well as refreshing the page in my browser. When attempting to drag and drop a column name from \u201cHidden Columns\u201d to \u201cVisible Columns\u201d (see the screenshot), a gap is created, but on mouse release the column name returns to \u201cHidden Columns\u201d. Clicking on column names to move them to \u201cVisible\u201d does not work either. The logged values appear in the web interface elsewhere. Manipulation with non-metric columns (e.g. config values, name, state etc) worked flawlessly as expected.<\/p>\n<p>The problem remained <em>for a fraction of a minute<\/em> after I logged a summary metric using <code>wandb.summary[...] = ...<\/code>. In particular, I tried moving all columns by pressing \u201cShow all\u201d, but without any visible result, and I closed the pop-up (on the screenshot). Suddenly, after 10 or so seconds, all columns became visible.<\/p>\n<p>The problem is similar to the one in the linked post. Unlike there, in my case, refreshing the web-page did not seem to help. I\u2019ll take a wild guess and suggest possible reasons for the bug:<\/p>\n<ol>\n<li>Something was going on in your back-end, and I had to wait till all necessary data validation or calculations are completed that would enable adding metric columns. This is unacceptably long time (several minutes), within which I was able to read relevant reference, search issues, and do a couple of empty test runs to see what\u2019s going on.<\/li>\n<li>There is a bug which prevents conversion step metrics to summary metrics unless at least one summary metric is explicitly added via <code>wandb.summary<\/code>.<\/li>\n<\/ol>\n<p>I hope you will be able to get to the bottom of it and fix it.<\/p>\n<p>I hope this helps.<\/p>\n<p>Regards,<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/2\/2ee6cd9cf398c018ac88a42196119a096fc12763.png\" data-download-href=\"\/uploads\/short-url\/6GUslld1E38x1uAv9m6acBIMSwH.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/2\/2ee6cd9cf398c018ac88a42196119a096fc12763.png\" alt=\"image\" data-base62-sha1=\"6GUslld1E38x1uAv9m6acBIMSwH\" width=\"518\" height=\"500\" data-dominant-color=\"F7F8F8\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">667\u00d7643 8.92 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Challenge_closed_time":1676062281055,
        "Challenge_comment_count":0,
        "Challenge_created_time":1675832273139,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue where they were unable to add step metric columns to the Table view in the web interface. They tried logging metrics using both `run.log` and `wandb.log`, but were unable to move the column names from \"Hidden Columns\" to \"Visible Columns\". The problem persisted for a short time even after logging a summary metric using `wandb.summary[...] = ...`. The user suggests two possible reasons for the bug: either something was going on in the back-end that required several minutes to complete, or there is a bug preventing the conversion of step metrics to summary metrics unless at least one summary metric is explicitly added via `wandb.summary`.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/could-not-add-summary-columns-for-display-in-table\/3841",
        "Challenge_link_count":3,
        "Challenge_participation_count":4,
        "Challenge_readability":11.1,
        "Challenge_reading_time":38.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":63.8910877778,
        "Challenge_title":"Could not add summary columns for display in Table",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":226.0,
        "Challenge_word_count":348,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/avm21\">@avm21<\/a> , I\u2019ve been able to to consistently  reproduce this behavior on my end and flagged it as a bug. I will update you on a timeline for a fix once I have additional info. Thanks again for the insight!<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":3.07,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":42.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1491327759476,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":111.3944786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there any way to use a tsv instead of a csv as the input into sagemaker's autopilot ?<\/p>\n\n<p>Currently I'm inputting the data as such:<\/p>\n\n<pre><code>input_data_config = [{\n      'DataSource': {\n        'S3DataSource': {\n          'S3DataType': 'S3Prefix',\n          'S3Uri': 's3:\/\/{}\/{}\/train'.format(bucket,prefix)\n        }\n      },\n      'TargetAttributeName': 'sentiment'\n    }\n  ]\n<\/code><\/pre>\n\n<p>this seems to work file for .csv files but fails for my .tsv files.<\/p>",
        "Challenge_closed_time":1580778531436,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580377511313,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while using a TSV file as input into sagemaker's autopilot. The current method of inputting data works fine for CSV files but fails for TSV files.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59983062",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":5.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":111.3944786111,
        "Challenge_title":"TSV as input to sagemaker",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":204.0,
        "Challenge_word_count":54,
        "Platform":"Stack Overflow",
        "Poster_created_time":1479115407580,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Earth",
        "Poster_reputation_count":2944.0,
        "Poster_view_count":381.0,
        "Solution_body":"<p>I am a developer at AWS SageMaker. Autopilot currently only supports CSV data. While we are working on extending the support to more file formats: JSON, TSV, etc, this might be something that you can try to convert your .tsv file to .csv:<\/p>\n\n<pre><code>import csv\n\n# read tab-delimited file\nwith open('yourfile.tsv','rb') as fin:\n    cr = csv.reader(fin, delimiter='\\t')\n    filecontents = [line for line in cr]\n\n# write comma-delimited file (comma is the default delimiter)\nwith open('yourfile.csv','wb') as fou:\n    cw = csv.writer(fou, quotechar='', quoting=csv.QUOTE_NONE)\n    cw.writerows(filecontents)\n<\/code><\/pre>\n\n<p>Hope this helps.<\/p>\n\n<p>Ref: <a href=\"https:\/\/stackoverflow.com\/questions\/5590631\/how-to-convert-a-tab-separated-file-to-csv-format\">How to convert a tab separated file to CSV format?<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.7,
        "Solution_reading_time":10.34,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":94.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1499171495843,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bhubaneswar, Odisha, India",
        "Answerer_reputation_count":521.0,
        "Answerer_view_count":77.0,
        "Challenge_adjusted_solved_time":6.5169719444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am creating a Question Answering model using <a href=\"https:\/\/simpletransformers.ai\/docs\/qa-specifics\/\" rel=\"nofollow noreferrer\">simpletransformers<\/a>. I would also like to use wandb to track model artifacts. As I understand from <a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/other\/simpletransformers\" rel=\"nofollow noreferrer\">wandb docs<\/a>, there is an integration touchpoint for simpletransformers but there is no mention of logging artifacts.<\/p>\n<p>I would like to log artifacts generated at the train, validation, and test phase such as train.json, eval.json, test.json, output\/nbest_predictions_test.json and best performing model.<\/p>",
        "Challenge_closed_time":1634729204572,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634705743473,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use wandb to track model artifacts while creating a Question Answering model using simpletransformers, but is unable to find any mention of logging artifacts in the wandb documentation for simpletransformers integration. The user wants to log artifacts generated during the train, validation, and test phases.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69640534",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.1,
        "Challenge_reading_time":9.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":6.5169719444,
        "Challenge_title":"How to log artifacts in wandb while using saimpletransformers?",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":53.0,
        "Challenge_word_count":79,
        "Platform":"Stack Overflow",
        "Poster_created_time":1528765704783,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Ahmedabad, Gujarat, India",
        "Poster_reputation_count":13.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>Currently simpleTransformers doesn't support logging artifacts within the training\/testing scripts. But you can do it manually:<\/p>\n<pre><code>import os \n\nwith wandb.init(id=model.wandb_run_id, resume=&quot;allow&quot;, project=wandb_project) as training_run:\n    for dir in sorted(os.listdir(&quot;outputs&quot;)):\n        if &quot;checkpoint&quot; in dir:\n            artifact = wandb.Artifact(&quot;model-checkpoints&quot;, type=&quot;checkpoints&quot;)\n            artifact.add_dir(&quot;outputs&quot; + &quot;\/&quot; + dir)\n            training_run.log_artifact(artifact)\n<\/code><\/pre>\n<p>For more info, you can follow along with the W&amp;B notebook in the SimpleTransofrmer's README.md<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.6,
        "Solution_reading_time":8.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":55.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1448964835883,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Z\u00fcrich, Schweiz",
        "Answerer_reputation_count":269.0,
        "Answerer_view_count":42.0,
        "Challenge_adjusted_solved_time":0.3934,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying a machine learning model and logging metrics using mlflow. But I am getting <code>TypeError: 'numpy.float32' object is not iterable<\/code>. I have tried using <code>.tolist()<\/code> and <code>dict()<\/code> but nothing seems to work.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def train(max_epochs, model, optimizer, scheduler, train_loader, valid_loader, project_name):\n    best_val_loss = 100\n    for epoch in range(max_epochs):\n        model.train()\n        running_loss = []\n        tq_loader = tqdm(train_loader)\n        o = {}\n        for samples in tq_loader:\n            optimizer.zero_grad()\n            outputs, interaction_map = model(\n                [samples[0].to(device), samples[1].to(device), torch.tensor(samples[2]).to(device),\n                 torch.tensor(samples[3]).to(device)])\n            l1_norm = torch.norm(interaction_map, p=2) * 1e-4\n            loss = loss_fn(outputs, torch.tensor(samples[4]).to(device).float()) + l1_norm\n            loss.backward()\n            optimizer.step()\n            loss = loss - l1_norm\n            running_loss.append(loss.cpu().detach())\n            tq_loader.set_description(\n                &quot;Epoch: &quot; + str(epoch + 1) + &quot;  Training loss: &quot; + str(np.mean(np.array(running_loss))))\n        model.eval()\n        val_loss, mae_loss = get_metrics(model, valid_loader)\n        scheduler.step(val_loss)\n        \n        #metrics mlflow\n        mlflow.log_metrics('train_loss',(np.mean(np.array(running_loss))).tolist())\n        mlflow.log_metrics('validation_loss',(val_loss).tolist())\n        mlflow.log_metrics('MAE Val_loss', (mae_loss).tolist())\n\n        print(&quot;Epoch: &quot; + str(epoch + 1) + &quot;  train_loss &quot; + str(np.mean(np.array(running_loss))) + &quot; Val_loss &quot; + str(\n            val_loss) + &quot; MAE Val_loss &quot; + str(mae_loss))\n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), &quot;.\/runs\/run-&quot; + str(project_name) + &quot;\/models\/best_model.tar&quot;)\n\nmlflow.set_experiment('CIGIN_V2')\nmlflow.start_run(nested=True)\ntrain(max_epochs, model, optimizer, scheduler, train_loader, valid_loader, project_name)\nmlflow.end_run()\n<\/code><\/pre>\n<p>Error<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>Epoch: 1  Training loss: 6770.575: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:04&lt;00:00,  4.35s\/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:03&lt;00:00,  3.86s\/it]\n\n---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\n&lt;ipython-input-96-8c3a6eb822c3&gt; in &lt;module&gt;()\n      1 mlflow.set_experiment('CIGIN_V2')\n      2 mlflow.start_run(nested=True)\n----&gt; 3 train(max_epochs, model, optimizer, scheduler, train_loader, valid_loader, project_name)\n      4 mlflow.end_run()\n\n&lt;ipython-input-95-ab0a6c80b65b&gt; in train(max_epochs, model, optimizer, scheduler, train_loader, valid_loader, project_name)\n     55 \n     56         #metrics mlflow\n---&gt; 57         mlflow.log_metrics('train_loss',dict(np.mean(np.array(running_loss))).tolist())\n     58         mlflow.log_metrics('validation_loss',dict(val_loss).tolist())\n     59         mlflow.log_metrics('MAE Val_loss', dict(mae_loss).tolist())\n\nTypeError: 'numpy.float32' object is not iterable\n<\/code><\/pre>",
        "Challenge_closed_time":1653903536587,
        "Challenge_comment_count":1,
        "Challenge_created_time":1653902310097,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a TypeError while logging metrics using mlflow in a machine learning model. The error message states that 'numpy.float32' object is not iterable. The user has tried using .tolist() and dict() but the issue persists.",
        "Challenge_last_edit_time":1653902478312,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72431938",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.0,
        "Challenge_reading_time":40.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":0.3406916667,
        "Challenge_title":"TypeError: 'numpy.float32' object is not iterable when logging in mlflow",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":51.0,
        "Challenge_word_count":221,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651898762636,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":41.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>Youre logging a single value into log_metrics and i dont think thats correct based on the implementation of log_metric and log_metrics in the documentation:<\/p>\n<p><a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_metric\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_metric<\/a> and\n<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_metrics\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_metrics<\/a><\/p>\n<p>So i would suggest to maybe change the &quot;log_metrics&quot; to &quot;log_metric&quot; and leave the tolist out<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1653903894552,
        "Solution_link_count":4.0,
        "Solution_readability":16.6,
        "Solution_reading_time":9.3,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":49.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1298484007147,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY, United States",
        "Answerer_reputation_count":9271.0,
        "Answerer_view_count":1819.0,
        "Challenge_adjusted_solved_time":6.4954608333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Cannot read AWS open data datasets into Sagemaker. Error is<\/p>\n\n<pre><code>download failed: s3:\/\/fast-ai-imageclas\/cifar100.tgz to ..\/..\/..\/tmp\/fastai-images\/cifar100.tgz An error occurred (AccessDenied) when calling the GetObject operation: Access Denied\n<\/code><\/pre>\n\n<p>code\n<a href=\"https:\/\/i.stack.imgur.com\/2b73H.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2b73H.png\" alt=\"sagemaker notebook s3 download access denied\"><\/a><\/p>\n\n<p>The user has the s3:getObjects * permission<\/p>\n\n<p>The user's permissions are the full s3 read policy and the full Sagemaker policies. The policies are<\/p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:Get*\",\n                \"s3:List*\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"application-autoscaling:DeleteScalingPolicy\",\n                \"application-autoscaling:DeleteScheduledAction\",\n                \"application-autoscaling:DeregisterScalableTarget\",\n                \"application-autoscaling:DescribeScalableTargets\",\n                \"application-autoscaling:DescribeScalingActivities\",\n                \"application-autoscaling:DescribeScalingPolicies\",\n                \"application-autoscaling:DescribeScheduledActions\",\n                \"application-autoscaling:PutScalingPolicy\",\n                \"application-autoscaling:PutScheduledAction\",\n                \"application-autoscaling:RegisterScalableTarget\",\n                \"aws-marketplace:ViewSubscriptions\",\n                \"cloudwatch:DeleteAlarms\",\n                \"cloudwatch:DescribeAlarms\",\n                \"cloudwatch:GetMetricData\",\n                \"cloudwatch:GetMetricStatistics\",\n                \"cloudwatch:ListMetrics\",\n                \"cloudwatch:PutMetricAlarm\",\n                \"cloudwatch:PutMetricData\",\n                \"codecommit:BatchGetRepositories\",\n                \"codecommit:CreateRepository\",\n                \"codecommit:GetRepository\",\n                \"codecommit:ListBranches\",\n                \"codecommit:ListRepositories\",\n                \"cognito-idp:AdminAddUserToGroup\",\n                \"cognito-idp:AdminCreateUser\",\n                \"cognito-idp:AdminDeleteUser\",\n                \"cognito-idp:AdminDisableUser\",\n                \"cognito-idp:AdminEnableUser\",\n                \"cognito-idp:AdminRemoveUserFromGroup\",\n                \"cognito-idp:CreateGroup\",\n                \"cognito-idp:CreateUserPool\",\n                \"cognito-idp:CreateUserPoolClient\",\n                \"cognito-idp:CreateUserPoolDomain\",\n                \"cognito-idp:DescribeUserPool\",\n                \"cognito-idp:DescribeUserPoolClient\",\n                \"cognito-idp:ListGroups\",\n                \"cognito-idp:ListIdentityProviders\",\n                \"cognito-idp:ListUserPoolClients\",\n                \"cognito-idp:ListUserPools\",\n                \"cognito-idp:ListUsers\",\n                \"cognito-idp:ListUsersInGroup\",\n                \"cognito-idp:UpdateUserPool\",\n                \"cognito-idp:UpdateUserPoolClient\",\n                \"ec2:CreateNetworkInterface\",\n                \"ec2:CreateNetworkInterfacePermission\",\n                \"ec2:CreateVpcEndpoint\",\n                \"ec2:DeleteNetworkInterface\",\n                \"ec2:DeleteNetworkInterfacePermission\",\n                \"ec2:DescribeDhcpOptions\",\n                \"ec2:DescribeNetworkInterfaces\",\n                \"ec2:DescribeRouteTables\",\n                \"ec2:DescribeSecurityGroups\",\n                \"ec2:DescribeSubnets\",\n                \"ec2:DescribeVpcEndpoints\",\n                \"ec2:DescribeVpcs\",\n                \"ecr:BatchCheckLayerAvailability\",\n                \"ecr:BatchGetImage\",\n                \"ecr:CreateRepository\",\n                \"ecr:GetAuthorizationToken\",\n                \"ecr:GetDownloadUrlForLayer\",\n                \"ecr:Describe*\",\n                \"elastic-inference:Connect\",\n                \"glue:CreateJob\",\n                \"glue:DeleteJob\",\n                \"glue:GetJob\",\n                \"glue:GetJobRun\",\n                \"glue:GetJobRuns\",\n                \"glue:GetJobs\",\n                \"glue:ResetJobBookmark\",\n                \"glue:StartJobRun\",\n                \"glue:UpdateJob\",\n                \"groundtruthlabeling:*\",\n                \"iam:ListRoles\",\n                \"kms:DescribeKey\",\n                \"kms:ListAliases\",\n                \"lambda:ListFunctions\",\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:DescribeLogStreams\",\n                \"logs:GetLogEvents\",\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ecr:SetRepositoryPolicy\",\n                \"ecr:CompleteLayerUpload\",\n                \"ecr:BatchDeleteImage\",\n                \"ecr:UploadLayerPart\",\n                \"ecr:DeleteRepositoryPolicy\",\n                \"ecr:InitiateLayerUpload\",\n                \"ecr:DeleteRepository\",\n                \"ecr:PutImage\"\n            ],\n            \"Resource\": \"arn:aws:ecr:*:*:repository\/*sagemaker*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"codecommit:GitPull\",\n                \"codecommit:GitPush\"\n            ],\n            \"Resource\": [\n                \"arn:aws:codecommit:*:*:*sagemaker*\",\n                \"arn:aws:codecommit:*:*:*SageMaker*\",\n                \"arn:aws:codecommit:*:*:*Sagemaker*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:CreateSecret\",\n                \"secretsmanager:DescribeSecret\",\n                \"secretsmanager:ListSecrets\",\n                \"secretsmanager:TagResource\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:GetSecretValue\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"secretsmanager:ResourceTag\/SageMaker\": \"true\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"robomaker:CreateSimulationApplication\",\n                \"robomaker:DescribeSimulationApplication\",\n                \"robomaker:DeleteSimulationApplication\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"robomaker:CreateSimulationJob\",\n                \"robomaker:DescribeSimulationJob\",\n                \"robomaker:CancelSimulationJob\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::*SageMaker*\",\n                \"arn:aws:s3:::*Sagemaker*\",\n                \"arn:aws:s3:::*sagemaker*\",\n                \"arn:aws:s3:::*aws-glue*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:CreateBucket\",\n                \"s3:GetBucketLocation\",\n                \"s3:ListBucket\",\n                \"s3:ListAllMyBuckets\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEqualsIgnoreCase\": {\n                    \"s3:ExistingObjectTag\/SageMaker\": \"true\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"lambda:InvokeFunction\"\n            ],\n            \"Resource\": [\n                \"arn:aws:lambda:*:*:function:*SageMaker*\",\n                \"arn:aws:lambda:*:*:function:*sagemaker*\",\n                \"arn:aws:lambda:*:*:function:*Sagemaker*\",\n                \"arn:aws:lambda:*:*:function:*LabelingFunction*\"\n            ]\n        },\n        {\n            \"Action\": \"iam:CreateServiceLinkedRole\",\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:iam::*:role\/aws-service-role\/sagemaker.application-autoscaling.amazonaws.com\/AWSServiceRoleForApplicationAutoScaling_SageMakerEndpoint\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"iam:AWSServiceName\": \"sagemaker.application-autoscaling.amazonaws.com\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"iam:CreateServiceLinkedRole\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"iam:AWSServiceName\": \"robomaker.amazonaws.com\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:PassRole\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"iam:PassedToService\": [\n                        \"sagemaker.amazonaws.com\",\n                        \"glue.amazonaws.com\",\n                        \"robomaker.amazonaws.com\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>The Sagemaker instance is in us-east-1 same as the dataset.<\/p>\n\n<p>The dataset is <a href=\"https:\/\/registry.opendata.aws\/fast-ai-imageclas\/\" rel=\"nofollow noreferrer\">https:\/\/registry.opendata.aws\/fast-ai-imageclas\/<\/a><\/p>",
        "Challenge_closed_time":1549864663812,
        "Challenge_comment_count":2,
        "Challenge_created_time":1549841280153,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to read AWS open data datasets into Sagemaker and is receiving an Access Denied error. The user has the s3:getObjects * permission and the full Sagemaker policies, but is still unable to access the dataset. The Sagemaker instance and the dataset are both in us-east-1.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54622191",
        "Challenge_link_count":4,
        "Challenge_participation_count":3,
        "Challenge_readability":40.9,
        "Challenge_reading_time":86.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":6.4954608333,
        "Challenge_title":"Access Denied read open data into Sagemaker",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1192.0,
        "Challenge_word_count":309,
        "Platform":"Stack Overflow",
        "Poster_created_time":1298484007147,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"New York, NY, United States",
        "Poster_reputation_count":9271.0,
        "Poster_view_count":1819.0,
        "Solution_body":"<p>thanks to Matthew I looked into the permissions of the notebook itself, not just the user using Sagemaker.<\/p>\n\n<p>The policies on the notebook look like this and I can download from the aws open data datasets!<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/s0jwV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s0jwV.png\" alt=\"notebook settings\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/OqEHi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/OqEHi.png\" alt=\"notebook permissions\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":13.1,
        "Solution_reading_time":7.05,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":50.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1431525955023,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cherry Hill, NJ, United States",
        "Answerer_reputation_count":2069.0,
        "Answerer_view_count":145.0,
        "Challenge_adjusted_solved_time":167.8033111111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been testing some small examples with MLflow tracking but for my usecase I would like to have the weights saved after each epoch. \nSometimes I kill the runs before they are completely finished (I cannot use earlystopping), but what I experience now is that the weights do not get saved to the tracking ui server.\nIs there a way to do this after each epoch?<\/p>",
        "Challenge_closed_time":1572525493020,
        "Challenge_comment_count":2,
        "Challenge_created_time":1571921401100,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is using MLflow tracking for their project and wants to save weights after each epoch. However, they are facing issues as the weights are not getting saved to the tracking UI server when they kill the runs before completion. They are seeking a solution to save weights after each epoch.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58541794",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":6.8,
        "Challenge_reading_time":4.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":167.8033111111,
        "Challenge_title":"MLflow saving weights after each epoch",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1121.0,
        "Challenge_word_count":72,
        "Platform":"Stack Overflow",
        "Poster_created_time":1534367348472,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Leuven, Belgium",
        "Poster_reputation_count":344.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>Save the weights to disk and then log them as an artifact.  As long as the checkpoints\/weights are saved to disk, you can log them with <code>mlflow_log_artifact()<\/code> or <code>mlflow_log_artifacts()<\/code>.  From the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#logging-functions\" rel=\"nofollow noreferrer\">docs<\/a>,<\/p>\n\n<blockquote>\n  <p><strong>mlflow.log_artifact()<\/strong> logs a local file or directory as an artifact,\n  optionally taking an artifact_path to place it in within the run\u2019s\n  artifact URI. Run artifacts can be organized into directories, so you\n  can place the artifact in a directory this way.<\/p>\n  \n  <p><strong>mlflow.log_artifacts()<\/strong> logs all the files in a given directory as\n  artifacts, again taking an optional artifact_path.<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.9,
        "Solution_reading_time":10.15,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":94.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1407761610168,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Geneva, Switzerland",
        "Answerer_reputation_count":118.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":7430.662325,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm new to mlflow and I can't figure out why the <code>artifact store<\/code> can't be the same as the <code>backend store<\/code>? <\/p>\n\n<p>The only reason I can think of is to be able to query the experiments with SQL syntax... but since we can interact with the runs using <code>mlflow ui<\/code> I just don't understand why all artifacts and parameters can't go to a same location (which is what happens when using local storage).<\/p>\n\n<p>Can anyone shed some light on this?<\/p>",
        "Challenge_closed_time":1611045077983,
        "Challenge_comment_count":1,
        "Challenge_created_time":1584294693613,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is new to MLflow and is confused about why the artifact store and backend store cannot be the same, as they can interact with runs using mlflow ui. They are seeking clarification on why all artifacts and parameters cannot go to the same location.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60695933",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.8,
        "Challenge_reading_time":6.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":7430.662325,
        "Challenge_title":"MLflow: Why can't backend-store-uri be an s3 location?",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":768.0,
        "Challenge_word_count":88,
        "Platform":"Stack Overflow",
        "Poster_created_time":1528574640848,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":133.0,
        "Poster_view_count":76.0,
        "Solution_body":"<p>MLflow's Artifacts are typically ML models, i.e. relatively large binary files. On the other hand, run data are typically a couple of floats.<\/p>\n<p>In the end it is not a question of what is possible or not (many things are possible if you put enough effort into it), but rather to follow good practices:<\/p>\n<ul>\n<li>storing large binary artifacts in an SQL database is possible but is bound the degrade the performance of the database sooner or later, and this in turn will degrade your user experience.<\/li>\n<li>storing a couple of floats from a SQL database for quick retrieval for display in a front-end or via command line is a robust industry-proven classic<\/li>\n<\/ul>\n<p>It remains true that the documentation of MLflow on the architecture design rationale could be improved (as of 2020)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.5,
        "Solution_reading_time":9.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":133.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1491898605956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sydney NSW, Australia",
        "Answerer_reputation_count":161.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":373.8896469444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The following code snippet is inspired by <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"nofollow noreferrer\">this<\/a>.<\/p>\n<pre><code>hyperparameters = {\n        &quot;max_depth&quot;:&quot;5&quot;,\n        &quot;eta&quot;:&quot;0.2&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;objective&quot;:&quot;reg:squarederror&quot;,\n        &quot;num_round&quot;:&quot;10&quot;}\n\noutput_path = 's3:\/\/{}\/{}\/output'.format(s3_bucket_name, s3_prefix)\n\nestimator = sagemaker.estimator.Estimator(image_uri=sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region_name, &quot;1.2-2&quot;), \n                                          hyperparameters=hyperparameters,\n                                          role=role,\n                                          instance_count=1, \n                                          instance_type='ml.m5.2xlarge', \n                                          volume_size=1, # 1 GB \n                                          output_path=output_path)\n\nestimator.fit({'train': s3_input_train, 'validation': s3_input_val})\n<\/code><\/pre>\n<p>It works fine. I was trying to use:<\/p>\n<pre><code>training_image_name = image_uris.retrieve(framework='xgboost', region=region_name, version='latest')\n<\/code><\/pre>\n<p>instead of:<\/p>\n<pre><code>sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region_name, &quot;1.2-2&quot;)\n<\/code><\/pre>\n<p>to (I believe) get hold of the latest training image but reg:squarederror is not supported? Is my code to get hold of the latest image name incorrect?<\/p>",
        "Challenge_closed_time":1651279786032,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649933783303,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use the latest AWS XGBoost image for training a model, but the \"reg:squarederror\" objective is not supported in the latest image. The user is unsure if their code to retrieve the latest image is correct.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71870508",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":19.7,
        "Challenge_reading_time":18.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":373.8896469444,
        "Challenge_title":"latest aws xgb image does not support reg:",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":26.0,
        "Challenge_word_count":92,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>Using &quot;latest&quot; it not suggested as per documentation(see note): <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html<\/a><\/p>\n<p>Use specific versions as they are more stable.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":32.0,
        "Solution_reading_time":4.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":21.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1425426748316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":91.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":316.7987177778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I played a bit around with Azure ML studio. So as I understand the process goes like this:<\/p>\n\n<p>a) Create training experiment. Train it with data. <\/p>\n\n<p>b) Create Scoring experiment. This will include the trained model from the training experiment. Expose this as a service to be consumed over REST.<\/p>\n\n<p>Maybe a stupid question but what is the recommended way to get the complete experience like the one i get when I use an app like <a href=\"https:\/\/datamarket.azure.com\/dataset\/amla\/mba\" rel=\"nofollow\">https:\/\/datamarket.azure.com\/dataset\/amla\/mba<\/a> (Frequently Bought Together API built with Azure Machine Learning). <\/p>\n\n<p>I mean the following:<\/p>\n\n<p>a) Expose 2 or more services - one to train the model and the other to consume (test) the trained model. <\/p>\n\n<p>b) User periodically sends training data to train the model <\/p>\n\n<p>c) The trained model\/models now gets saved available for consumption<\/p>\n\n<p>d) User is now able to send a dataframe to get the predicted results.<\/p>\n\n<p>Is there an additional wrapper that needs to be built?<\/p>\n\n<p>If there is a link documenting this please point me to the same. <\/p>",
        "Challenge_closed_time":1425426748316,
        "Challenge_comment_count":0,
        "Challenge_created_time":1424282724780,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to create a complete experience in Azure ML Studio, similar to the one provided by the Frequently Bought Together API. They want to know how to expose two or more services, train the model periodically with user data, save the trained model for consumption, and allow users to send a dataframe to get predicted results. They are also asking if an additional wrapper needs to be built and if there is any documentation available on this topic.",
        "Challenge_last_edit_time":1424286272932,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/28590690",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":8.8,
        "Challenge_reading_time":14.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":317.7843155556,
        "Challenge_title":"Azure ML App - Complete Experince - Train automatically and Consume",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":769.0,
        "Challenge_word_count":182,
        "Platform":"Stack Overflow",
        "Poster_created_time":1424282355667,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>The Azure ML retraining API is designed to handle the workflow you describe:<\/p>\n\n<p><a href=\"http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-retrain-models-programmatically\/\" rel=\"nofollow\">http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-retrain-models-programmatically\/<\/a><\/p>\n\n<p>Hope this helps,<\/p>\n\n<p>Roope - Microsoft Azure ML Team<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":29.2,
        "Solution_reading_time":5.44,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":24.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1305851487736,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5993.0,
        "Answerer_view_count":457.0,
        "Challenge_adjusted_solved_time":18966.6006311111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want my data and models stored in separate Google Cloud buckets. The idea is that I want to be able to share the data with others without sharing the models.<\/p>\n\n<p>One idea I can think of is using separate git submodules for data and models. But that feels cumbersome and imposes some additional requirements from the end user (e.g. having to do <code>git submodule update<\/code>).<\/p>\n\n<p>So can I do this without using git submodules?<\/p>",
        "Challenge_closed_time":1574267475363,
        "Challenge_comment_count":0,
        "Challenge_created_time":1574248229420,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user wants to store data and models in separate Google Cloud buckets to share data without sharing models. They are looking for a way to use different remotes for different folders without using git submodules, which they find cumbersome.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58952962",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.9,
        "Challenge_reading_time":6.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":12.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":5.3460952778,
        "Challenge_title":"How to use different remotes for different folders?",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1984.0,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Poster_created_time":1311330349880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tel Aviv",
        "Poster_reputation_count":3784.0,
        "Poster_view_count":342.0,
        "Solution_body":"<p>You can first add the different <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\" rel=\"nofollow noreferrer\">DVC remotes<\/a> you want to establish (let's say you call them <code>data<\/code> and <code>models<\/code>, each one pointing to a different <a href=\"https:\/\/cloud.google.com\/storage\/docs\/json_api\/v1\/buckets\" rel=\"nofollow noreferrer\">GC bucket<\/a>). <strong>But don't set any remote as the project's default<\/strong>; This way, <a href=\"https:\/\/dvc.org\/doc\/command-reference\/push\" rel=\"nofollow noreferrer\"><code>dvc push<\/code><\/a> won't work without the <code>-r<\/code> (or <code>--remote<\/code>) option.<\/p>\n<p>You would then need to push each directory or file individually to the appropriate remote, like <code>dvc push data\/ -r data<\/code> and <code>dvc push model.dat -r models<\/code>.<\/p>\n<p>Note that a feature request to configure this exists on the DVC repo too. See <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2095\" rel=\"nofollow noreferrer\">Specify file types that can be pushed to remote<\/a>.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1642527991692,
        "Solution_link_count":4.0,
        "Solution_readability":13.6,
        "Solution_reading_time":13.5,
        "Solution_score_count":13.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":112.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1294339469960,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Santiago, Chile",
        "Answerer_reputation_count":3926.0,
        "Answerer_view_count":255.0,
        "Challenge_adjusted_solved_time":528.8209525,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a dataset in which the Rating column is an integer column with values ranging from 1 to 10.<\/p>\n\n<p>I would like to convert that column into a simple boolean positive\/negative categorical column, so that if the value is less than 6 it is a negative rating, and if it is greater or equal 6 it would become a positive rating.<\/p>\n\n<p>I'm not sure how to do that.<\/p>",
        "Challenge_closed_time":1498563773572,
        "Challenge_comment_count":0,
        "Challenge_created_time":1496660018143,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to convert an integer rating column in a dataset into a boolean positive\/negative categorical column based on a custom filter in Azure ML. They want values less than 6 to be considered negative and values greater or equal to 6 to be considered positive. The user is seeking guidance on how to accomplish this task.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44367367",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":5.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":528.8209525,
        "Challenge_title":"Converting rating column into boolean column with custom filter in Azure ML",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":240.0,
        "Challenge_word_count":80,
        "Platform":"Stack Overflow",
        "Poster_created_time":1318705387390,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Poland",
        "Poster_reputation_count":15172.0,
        "Poster_view_count":2544.0,
        "Solution_body":"<p>Azure Machine Learning allows at least 3 options to do that:<\/p>\n\n<ul>\n<li>Apply SQL Transformation <code>select *,case when rating&lt;6 then 0 else 1 end RatingB from t1<\/code><\/li>\n<li>Execute Python Script <code>return dataframe1.rating[dataframe1.rating &lt; 6] = 0<\/code><\/li>\n<li>Execute R Script <code>dataset1$rating[dataset1$rating &lt; 6] &lt;- 0<\/code><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.9,
        "Solution_reading_time":4.88,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":44.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1430113774790,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1181.0,
        "Answerer_view_count":225.0,
        "Challenge_adjusted_solved_time":8429.4126688889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Background: I am working on a project that aims to classify product reviews into positive and negative using Sentiment Analysis in Azure ML. I got stuck when I was classifying reviews into different departments.<\/p>\n\n<p>I am basically reading words from csv files and checking whether the review(v: list of sentences) contains these words. If some of these words are found in the review then I am noting the sentence number and pushing it into respective lists( FinanceList, QualityList, LogisticsList ). In the end I am converting the lists to strings and pushing them into a dataframe.<\/p>\n\n<p>The output is not getting logged for the print statements that I have written in the script in Azure ML.<\/p>\n\n<p>The values in the dataframe are always turning out to be 0 but when I run the code locally I get the expected output.<\/p>\n\n<p>Description of First Image: The columns of the dataframe showing 0 values.<\/p>\n\n<p>Description of Second Image: I have highlighted the expected output that I got locally for the same review which was used in AzureML.<\/p>\n\n<p><a href=\"http:\/\/imgur.com\/0C3wcYj.png\" rel=\"nofollow\">Image 1<\/a><\/p>\n\n<p><a href=\"http:\/\/i.imgur.com\/lyHsM8z.png\" rel=\"nofollow\">Image 2<\/a><\/p>\n\n<p>The things that I have already checked:<\/p>\n\n<ol>\n<li>The csv files are read properly.<\/li>\n<li>The review contains the words that I am searching.<\/li>\n<\/ol>\n\n<p>I am unable to understand where I am going wrong.<\/p>\n\n<p>'<\/p>\n\n<pre><code>import csv\nimport math\nimport pandas as pd\nimport numpy as np\n\ndef azureml_main( data, ud):\n\n   FinanceDept = []\n   LogisticsDept = []\n   QualityDept = []\n  #Reading from the csv files\n   with open('.\\Script Bundle\\\\quality1.csv', 'rb') as fin:\n      reader = csv.reader(fin)\n      QualityDept = list(reader)\n\n   with open('.\\Script Bundle\\\\finance1.csv', 'rb') as f:\n      reader = csv.reader(f)\n      FinanceDept = list(reader)\n\n   with open('.\\Script Bundle\\\\logistics1.csv', 'rb') as f:\n      reader = csv.reader(f)\n      LogisticDept = list(reader)\n\n   FinanceList = []\n   LogisticsList = []\n   QualityList = []\n\n#Initializing the Lists   \n   FinanceList.append(0)\n   LogisticsList.append(0)\n   QualityList.append(0)\n\n   rev = data['Data']\n   v = rev[0].split('.')\n\n   print FinanceDept\n\n   S = 0   \n   for sentence in v:\n      S = S + 1\n      z = sentence.split(' ')\n      for c in z:\n         c = c.lower()\n         if c in FinanceDept and S not in FinanceList:\n            FinanceList.append(S)\n         if c in LogisticsDept and S not in LogisticsList:\n            LogisticsList.append(S)\n         if c in QualityDept and S not in QualityList:\n            QualityList.append(S)\n   #Compute User Reputation Score\n   Upvotes = int(ud['upvotes'].tolist()[0])\n   Downvotes = int(ud['downvotes'].tolist()[0])\n   TotalVotes = max(1,Upvotes+Downvotes)\n\n   q = data['Score']\n\n   print FinanceList\n\n   repScore = float(Upvotes)\/TotalVotes \n   repScore = repScore*float( q[0] )\n   str1 = ','.join(str(e) for e in FinanceList) \n   str2 = ','.join(str(e) for e in QualityList)\n   str3 = ','.join(str(e) for e in LogisticsList)\n\n   x = ud['id']\n\n   #df = pd.DataFrame(  [str(repScore), str1  , str2  , str3 ], columns=[Write the columns])\n   d = {'id': x[0], 'Score': float(repScore),'Logistics':str3,'Finance':str1,'Quality':str2}\n   df = pd.DataFrame(data=d, index=np.arange(1))\n   return df,`\n<\/code><\/pre>",
        "Challenge_closed_time":1443582461283,
        "Challenge_comment_count":0,
        "Challenge_created_time":1441602696040,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while classifying product reviews into positive and negative using Sentiment Analysis in Azure ML. The user is reading words from csv files and checking whether the review contains these words. If some of these words are found in the review then the user is noting the sentence number and pushing it into respective lists. In the end, the user is converting the lists to strings and pushing them into a dataframe. The output is not getting logged for the print statements that the user has written in the script in Azure ML. The values in the dataframe are always turning out to be 0 but when the user runs the code locally, the expected output is obtained.",
        "Challenge_last_edit_time":1441604124888,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/32431471",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":7.6,
        "Challenge_reading_time":39.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":549.9347897222,
        "Challenge_title":"Not Getting the Expected Output in AzureML",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1013.0,
        "Challenge_word_count":398,
        "Platform":"Stack Overflow",
        "Poster_created_time":1441555651888,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation_count":143.0,
        "Poster_view_count":45.0,
        "Solution_body":"<p>@Anuj Shankar,\nAfter my colleague tested, we can read data from <code>CSV<\/code> files and get the expected results. Please refer to this experience:<\/p>\n\n<p>1)  Input data  - It has <code>apple.zip<\/code> file which has two <code>csv<\/code> files similar to you and each csv file includes bag of words related to company.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/yrXst.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/yrXst.jpg\" alt=\"enter image description here\"><\/a>\n2)  Python script: <\/p>\n\n<pre><code># The script MUST contain a function named azureml_main\n# which is the entry point for this module.\n#\n# The entry point function can contain up to two input arguments:\n#   Param&lt;dataframe1&gt;: a pandas.DataFrame\n#   Param&lt;dataframe2&gt;: a pandas.DataFrame\nimport csv\nimport numpy as np\nimport pandas as pd\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    # Execution logic goes here\n    #print('Input pandas.DataFrame #1:\\r\\n\\r\\n{0}'.format(dataframe1))\n\n    # If a zip file is connected to the third input port is connected,\n    # it is unzipped under \".\\Script Bundle\". This directory is added\n    # to sys.path. Therefore, if your zip file contains a Python file\n    # mymodule.py you can import it using:\n    # import mymodule\n\n    apple = {}\n    microsoft = {}\n  #Reading from the csv files\n    with open('.\\Script Bundle\\\\apple.csv', 'rb') as f:\n      reader = csv.reader(f)\n      apple = list_to_dict(list(reader)[0])\n\n    with open('.\\Script Bundle\\\\microsoft.csv', 'rb') as f:\n      reader = csv.reader(f)\n      microsoft = list_to_dict(list(reader)[0])\n\n#    print('hello world' + ' '.join(apple[0]))\n    applecount = 0\n    microsoftcount = 0\n\n    input = \"i want to buy surface which runs on windows\"\n    splitted_input = input.split(' ')\n\n    for word in splitted_input:\n        if word in apple:\n            applecount = applecount + 1\n        if word in microsoft:\n            microsoftcount = microsoftcount + 1\n\n    print(\"apple bag of words count - \" + str(applecount))\n    print(\"microsoft bag of words count - \" + str(microsoftcount))\n    mydata = [{'input words': len(splitted_input)}, {'applecount':applecount},\n        {'microsoftcount':microsoftcount}]       \n    # Return value must be of a sequence of pandas.DataFrame\n    return pd.DataFrame(mydata),\n\n\ndef list_to_dict(li):      \n    dct = {}  \n    for item in li:\n        if dct.has_key(item):              \n            dct[item] = dct[item] + 1  \n        else:  \n            dct[item] = 1  \n    return dct  \n<\/code><\/pre>\n\n<p>3)  Output  - if I consider a string \"i want to buy surface which runs on windows\". It has 2 words related to microsoft and 0 related to apple which are visualized in below snapshot.\n<a href=\"https:\/\/i.stack.imgur.com\/ifE1t.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ifE1t.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1471950010496,
        "Solution_link_count":4.0,
        "Solution_readability":9.4,
        "Solution_reading_time":33.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":25.0,
        "Solution_word_count":314.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1518706063680,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":95.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":866.1332491667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>To overcome <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-save-write-experiment-files#storage-limits-of-experiment-snapshots\" rel=\"nofollow noreferrer\">300MB snapshot size limit<\/a> I created an .amlignore file in the root of my repository:<\/p>\n\n<pre><code>\/*\n!\/root\n<\/code><\/pre>\n\n<p>The intention is to exclude everything except <code>\/root<\/code> directory where all python code is. The size of the <code>root<\/code> directory is less than 1MB, still I get an error of exceeding snapshot limit size of 300MB. What am I doing wrong?<\/p>",
        "Challenge_closed_time":1573932699567,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570814619870,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user created an .amlignore file to exclude everything except the \/root directory where all the Python code is located in order to overcome the 300MB snapshot size limit. However, even though the size of the \/root directory is less than 1MB, the user is still getting an error of exceeding the snapshot limit size of 300MB.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58345935",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":8.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":866.1332491667,
        "Challenge_title":"The amlignore file doesn't reduce the size of snapshot",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":522.0,
        "Challenge_word_count":72,
        "Platform":"Stack Overflow",
        "Poster_created_time":1518706063680,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>This is fixed in version <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/azure-machine-learning-release-notes#azure-machine-learning-sdk-for-python-v1074\" rel=\"nofollow noreferrer\">1.0.74 of azureml-sdk<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":28.3,
        "Solution_reading_time":3.35,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":11.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1265234764768,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Denver, CO",
        "Answerer_reputation_count":30577.0,
        "Answerer_view_count":6460.0,
        "Challenge_adjusted_solved_time":19.0194533333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>If I have a column of data of type string in an incoming Azure ML dataset that contains HTML tags screwing up my results, how can I remove those tags?<\/p>",
        "Challenge_closed_time":1484610622880,
        "Challenge_comment_count":0,
        "Challenge_created_time":1484610622880,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in removing HTML tags from a string column in an incoming Azure ML dataset to avoid interference with the results.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41686871",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":2.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"How to strip HTML from a text column in Azure ML Execute Python Script step",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":325.0,
        "Challenge_word_count":44,
        "Platform":"Stack Overflow",
        "Poster_created_time":1265234764768,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Denver, CO",
        "Poster_reputation_count":30577.0,
        "Poster_view_count":6460.0,
        "Solution_body":"<p>Like this:<\/p>\n\n<pre><code>def azureml_main(dataframe1 = None, dataframe2 = None):\n  dataframe1[1] = dataframe1['text'].str.replace('&lt;[^&lt;]+?&gt;', ' ', case=False)\n  return dataframe1,\n<\/code><\/pre>\n\n<p>Remember to precede the <code>Execute Python Script<\/code> step with <code>Clean Missing Data<\/code> step and change the action to remove the entire row (if appropriate). This is important because the <code>Execute Python Script<\/code> step cannot return an empty <code>dataframe<\/code>. Only you know your data, in this case.<\/p>\n\n<p>Let me also point out that the <code>Preprocessing Text<\/code> step allows you to apply a Regular Expression. That is another alternative that might be right for your situation.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1484679092912,
        "Solution_link_count":0.0,
        "Solution_readability":10.1,
        "Solution_reading_time":9.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":87.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1629385138956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":395.0,
        "Answerer_view_count":38.0,
        "Challenge_adjusted_solved_time":26.6152038889,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I would like to know what is the OPTIMAL way to store the result of a Google BigQuery table query, to Google Cloud storage. My code, which is currently being run in some Jupyter Notebook (in Vertex AI Workbench, same project than both the BigQuery data source, as well as the Cloud Storage destination), looks as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># CELL 1 OF 2\n\nfrom google.cloud import bigquery\nbqclient = bigquery.Client()\n\n# The query string can vary:\nquery_string = &quot;&quot;&quot;\n        SELECT *  \n        FROM `my_project-name.my_db.my_table` \n        LIMIT 2000000\n        &quot;&quot;&quot;\n\ndataframe = (\n    bqclient.query(query_string)\n    .result()\n    .to_dataframe(\n        create_bqstorage_client=True,\n    )\n)\nprint(&quot;Dataframe shape: &quot;, dataframe.shape)\n\n# CELL 2 OF 2:\n\nimport pandas as pd\ndataframe.to_csv('gs:\/\/my_bucket\/test_file.csv', index=False)\n<\/code><\/pre>\n<p>This code takes around 7.5 minutes to successfully complete.<\/p>\n<p><strong>Is there a more OPTIMAL way to achive what was done above?<\/strong> (It would mean <em>faster<\/em>, but maybe something else could be improved).<\/p>\n<p>Some additional notes:<\/p>\n<ol>\n<li>I want to run it &quot;via a Jupyter Notebook&quot; (in Vertex AI Workbench), because sometimes some data preprocessing, or special filtering must be done, which cannot be easily accomplished via SQL queries.<\/li>\n<li>For the first part of the code, I have discarded <a href=\"https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_gbq.html\" rel=\"nofollow noreferrer\">pandas.read_gbq<\/a>, as it was giving me some weird EOF errors, when (experimentally) &quot;storing as .CSV and reading back&quot;.<\/li>\n<li>Intuitively, I would focus the optimization efforts in the second half of the code (<code>CELL 2 OF 2<\/code>), as the first one was borrowed from <a href=\"https:\/\/cloud.google.com\/bigquery\/docs\/bigquery-storage-python-pandas\" rel=\"nofollow noreferrer\">the official Google documentation<\/a>. I have tried <a href=\"https:\/\/stackoverflow.com\/a\/57404119\/16706763\">this<\/a> but it does not work, however in the same thread <a href=\"https:\/\/stackoverflow.com\/a\/60644694\/16706763\">this<\/a> option worked OK.<\/li>\n<li>It is likley that this code will be included in some Docker image afterwards, so &quot;as little libraries as possible&quot; must be used.<\/li>\n<\/ol>\n<p>Thank you.<\/p>",
        "Challenge_closed_time":1651612810316,
        "Challenge_comment_count":2,
        "Challenge_created_time":1651600671333,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to know the optimal way to store the result of a Google BigQuery table query to Google Cloud storage. The user's code is currently running in a Jupyter Notebook in Vertex AI Workbench, and takes around 7.5 minutes to complete. The user wants to optimize the code to make it faster and is looking for suggestions. The user has discarded pandas.read_gbq and is focusing on optimizing the second half of the code. The user also wants to use as few libraries as possible and plans to include the code in a Docker image.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72103557",
        "Challenge_link_count":4,
        "Challenge_participation_count":5,
        "Challenge_readability":9.2,
        "Challenge_reading_time":30.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":3.3719397222,
        "Challenge_title":"Save the result of a query in a BigQuery Table, in Cloud Storage",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1409.0,
        "Challenge_word_count":292,
        "Platform":"Stack Overflow",
        "Poster_created_time":1629385138956,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":395.0,
        "Poster_view_count":38.0,
        "Solution_body":"<p>After some experiments, I think I have got to a solution for my original post. First, the updated code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd  # Just one library is imported this time\n\n# This SQL query can vary, modify it to match your needs\nquery_string = &quot;&quot;&quot;\nSELECT *\nFROM `my_project.my_db.my_table`\nLIMIT 2000000\n&quot;&quot;&quot;\n\n# One liner to query BigQuery data.\ndownloaded_dataframe = pd.read_gbq(query_string, dialect='standard', use_bqstorage_api=True)\n\n# Data processing (OPTIONAL, modify it to match your needs)\n# I won't do anything this time, just upload the previously queried data\n\n# Data store in GCS\ndownloaded_dataframe.to_csv('gs:\/\/my_bucket\/uploaded_data.csv', index=False)\n<\/code><\/pre>\n<p>Some final notes:<\/p>\n<ol>\n<li>I have not done an &quot;in-depth research&quot; about the processing speed VS the number of rows existing in a BigQuery table, however I saw that the processing time with the updated code and the original query, now takes ~6 minutes; that's enough for the time being. <em>This answer might have some room for further improvements<\/em> therefore, but it's better than the original situation.<\/li>\n<li>The EOF error I mentioned in  my original post was: <code>ParserError: Error tokenizing data. C error: EOF inside string starting at row 70198<\/code>. In the end I got to realize that it did not have anything to do with <a href=\"https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_gbq.html\" rel=\"nofollow noreferrer\">pandas_gbq<\/a> function, but with &quot;how I was saving the data&quot;. See, <em>I was 'experimentally' storing the .csv file in the Vertex AI Workbench local storage, then downloading it to my local device, and when trying to open that data from my local device, I kept stumbling upon that error, however not getting the same when downloading the .csv data from Cloud Storage<\/em> ... Why? Well, it happens that if you download the .csv data &quot;very quickly&quot; after &quot;it gets generated&quot; (i.e., after few seconds), from Vertex AI Workbench local storage, the data is simply still incomplete, but it does not give any error or warning message: it will simply &quot;let you start with the download&quot;. For this reason, I think it is safer to export your data to Cloud Storage, and then download safely from there. This behaviour is more noticeable on large files (i.e. my own generated file, which had ~3.1GB in size).<\/li>\n<\/ol>\n<p>Hope this helps.<\/p>\n<p>Thank you.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1651696486067,
        "Solution_link_count":1.0,
        "Solution_readability":9.6,
        "Solution_reading_time":31.41,
        "Solution_score_count":2.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":357.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1588674524307,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":208.0,
        "Answerer_view_count":29.0,
        "Challenge_adjusted_solved_time":0.8600380556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I use Amazon Sagemaker for model training and prediction. I have a problem with the returned data with predictions.  I am trying to convert prediction data to pandas dataframe format.<\/p>\n<p>After the model is deployed:<\/p>\n<pre><code>from sagemaker.serializers import CSVSerializer\n\nxgb_predictor=estimator.deploy(\n    initial_instance_count=1,\n    instance_type='ml.g4dn.xlarge',\n    serializer=CSVSerializer()\n)\n\n<\/code><\/pre>\n<p>I made a prediction on the test data:<\/p>\n<pre><code>predictions=xgb_predictor.predict(first_day.to_numpy())\n\n<\/code><\/pre>\n<p>The returned prediction results are in a binary file<\/p>\n<pre><code>predictions\n<\/code><\/pre>\n<pre><code>b'2.092024326324463\\n10.584211349487305\\n18.23127555847168\\n2.092024326324463\\n8.308058738708496\\n32.35516357421875\\n4.129155158996582\\n7.429899215698242\\n55.65376281738281\\n116.5504379272461\\n1.0734045505523682\\n5.29403018951416\\n1.0924320220947266\\n1.9484598636627197\\n5.29403018951416\\n2.190509080886841\\n2.085641860961914\\n2.092024326324463\\n7.674410343170166\\n2.1198673248291016\\n5.293967247009277\\n7.088096618652344\\n2.092024326324463\\n10.410735130310059\\n10.36008358001709\\n2.092024326324463\\n10.565692901611328\\n15.495997428894043\\n15.61841106414795\\n1.0533703565597534\\n6.262670993804932\\n31.02411460876465\\n10.43086051940918\\n3.116995096206665\\n3.2846100330352783\\n108.82835388183594\\n26.210166931152344\\n1.0658172369003296\\n10.55643367767334\\n6.245237350463867\\n15.951444625854492\\n10.195240020751953\\n1.0734045505523682\\n48.720497131347656\\n2.119992256164551\\n9.41071605682373\\n2.241959810256958\\n3.1907501220703125\\n10.415051460266113\\n1.2154537439346313\\n2.13691782951355\\n31.1861515045166\\n3.0827555656433105\\n6.261478424072266\\n5.279026985168457\\n15.897627830505371\\n20.483125686645508\\n20.874958038330078\\n53.2086296081543\\n10.731611251831055\\n2.115110397338867\\n13.79739761352539\\n2.1198673248291016\\n26.628803253173828\\n10.030998229980469\\n15.897627830505371\\n5.278475284576416\\n45.371158599853516\\n2.2791690826416016\\n15.58777141571045\\n15.947166442871094\\n30.88138771057129\\n10.388553619384766\\n48.22294235229492\\n10.565692901611328\\n20.808977127075195\\n10.388553619384766\\n15.910200119018555\\n8.252408981323242\\n1.109586238861084\\n15.58777141571045\\n13.718815803527832\\n3.1227424144744873\\n32.171592712402344\\n10.524396896362305\\n15.897627830505371\\n2.092024326324463\\n14.52088737487793\\n5.293967247009277\\n57.61208724975586\\n21.161712646484375\\n14.173937797546387\\n5.230247974395752\\n16.257652282714844\n\n<\/code><\/pre>\n<p>How can I convert prediction data to pandas dataframe?<\/p>",
        "Challenge_closed_time":1659449084300,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659445988163,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in converting binary file data to pandas dataframe format after making a prediction on test data using Amazon Sagemaker for model training and prediction. The returned prediction results are in binary file format and the user is seeking guidance on how to convert this data to pandas dataframe.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73208208",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.6,
        "Challenge_reading_time":36.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.8600380556,
        "Challenge_title":"how to convert binary file to pandas dataframe",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":31.0,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Poster_created_time":1414361702887,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>you mean this:<\/p>\n<pre><code>import pandas as pd\n\na = a.decode(encoding=&quot;utf-8&quot;).split(&quot;\\n&quot;)\n\ndf = pd.DataFrame(data=a)\ndf.head()\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.7,
        "Solution_reading_time":2.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0147222222,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nIs there some way we could save the output logs to have them be accessible?",
        "Challenge_closed_time":1649327870000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649327817000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of saving and accessing output logs from a Polyaxon run through the cloud UI.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1470",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":5.0,
        "Challenge_reading_time":1.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":0.0147222222,
        "Challenge_title":"Is there a way to have the logs from a polyaxon run viewable via the cloud UI?",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":33,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"That's not possible I am afraid. Logs, as well as other artifacts, are only viewable via the gateway deployed with the agent.\n\nIn order to provide such option, Polyaxon will have to have access to the artifacts store, but I do not think that we want to provide such functionality at the moment since logs also can include sensitive information.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":4.17,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":60.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":43.8208166667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>for a project we are doing azure ml assisted labeling (object detection). we also want to check the model for false positives\/negatives, by receiving the results of the camera and performing manual labeling on it. then we want to take the results from the check and put those in the original dataset that we created using azure labeling. is there a built in function for this or do we need to create this ourselfs?<\/p>",
        "Challenge_closed_time":1676534037760,
        "Challenge_comment_count":1,
        "Challenge_created_time":1676376282820,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is working on an Azure ML assisted labeling project for object detection and wants to check the model for false positives\/negatives by manually labeling the camera results. They want to add these results to the original dataset created using Azure labeling and are unsure if there is a built-in function for this or if they need to create it themselves.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1180554\/feedback-loop-azure-ml-possibilities",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.4,
        "Challenge_reading_time":5.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":43.8208166667,
        "Challenge_title":"feedback loop azure ml possibilities?",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":78,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=6ab37d97-2a03-49e5-8bb2-90417150ab68\">Hamza Outa<\/a> Are you looking to add new labels after some feedback for your systems? <\/p>\n<p>I think there is an option to add new labels to a project by pausing it and then you have the following options:<\/p>\n<ul>\n<li> Start over, removing all existing labels. Choose this option if you want to start labeling from the beginning with the new full set of labels.<\/li>\n<li> Start over, keeping all existing labels. Choose this option to mark all data as unlabeled, but keep the existing labels as a default tag for images that were previously labeled.<\/li>\n<li> Continue, keeping all existing labels. Choose this option to keep all data already labeled as is, and start using the new label for data not yet labeled.<\/li>\n<\/ul>\n<p>Is this what you are looking for? I think the second option might work for you to add new labels if the default tags need to be changed. As per <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-image-labeling-projects#add-new-labels-to-a-project\">documentation <\/a>you can start using the new labels for labeling and the ML assisted labeling will start after a certain threshold is reached or you can manually start an ML assisted training run. I hope this helps!!<\/p>\n",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.6,
        "Solution_reading_time":16.69,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":195.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.2959119444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to convert a web service output as a dataset or a csv file ? I want to consume this in another experiment.<\/p>",
        "Challenge_closed_time":1592416585020,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592408319737,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to know if it is possible to convert a web service output into a dataset or CSV file so that it can be used in another experiment.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/37214\/convert-web-service-output-to-a-dataset-azure-mls",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":2.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":2.2959119444,
        "Challenge_title":"Convert web service output to a dataset Azure MLS classic",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":33,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>You can delete or export in-product data stored by Azure Machine Learning Studio (classic) by using the Azure portal, the Studio (classic) interface, PowerShell, and authenticated REST APIs. This article tells you how.    <\/p>\n<p>Telemetry data can be accessed through the Azure Privacy portal.    <\/p>\n<p>More details please refer to: <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/studio\/export-delete-personal-data-dsr\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/studio\/export-delete-personal-data-dsr<\/a>    <\/p>\n<p>And also you can use one of the Azure Machine Learning Studio Module - &quot;Export Data&quot; to do it : <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/export-data?redirectedfrom=MSDN\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/export-data?redirectedfrom=MSDN<\/a>    <\/p>\n<p>Let me know if you have more questions.    <\/p>\n<p>Regards,    <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.3,
        "Solution_reading_time":13.03,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":86.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":30.4174394444,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hi all,<\/p>\n<p>I\u2019m trying to figure out how does the caching  of artifacts work. Let\u2019s say I want to download a model artifact to run some evaluation on. I don\u2019t need the file on disk to persist rather I just want to load it into memory. What I do right now in my evaluation script is:<\/p>\n<pre><code class=\"lang-auto\">import tempfile\nimport wandb\n\nartifact = wandb.use_artifact(model_weights_uri)\nwith tempfile.TemporaryDirectory() as tmpdirname:\n    artifact.download(tmpdirname)\n    model_weights = load_pickle(os.path.join(tmpdirname, \"model_weights.pickle\"))\n<\/code><\/pre>\n<p>And from that point on I use the <code>model_weights<\/code> as it was loaded into memory.<\/p>\n<p>My first question is: if I run the code twice (on the same machine), <strong>will the model-weights be downloaded again<\/strong> or are they cached somewhere? assuming the logged artifact wasn\u2019t changed of course. And if they are cached, where are they cached?<br>\nI\u2019m also not clear about the <code>artifact<\/code> directory (which is used if I run <code>artifact.download()<\/code> without any argument). Does that directory serve as cache? if so, what does the <code>.cache<\/code> directory used for?<\/p>\n<p>I would appreciate answers to my questions and perhaps a  general explanation of the artifact caching mechanism &amp; best practices.<\/p>\n<p>Thanks!<br>\nRan<\/p>",
        "Challenge_closed_time":1650312955392,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650203452610,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to understand how the caching of artifacts works in their evaluation script. They want to know if the model-weights will be downloaded again if they run the code twice on the same machine and where the cached files are stored. They also have questions about the artifact directory and the .cache directory. The user is seeking answers and best practices for artifact caching.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/artifacts-local-caching-how-does-it-really-work\/2255",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.4,
        "Challenge_reading_time":17.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":30.4174394444,
        "Challenge_title":"Artifacts (local) caching - how does it really work?",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":861.0,
        "Challenge_word_count":191,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ranshadmi-nexite\">@ranshadmi-nexite<\/a>,<\/p>\n<p>Thank you for your question. You are right, all Artifacts are cached on your system under <code>~\/.cache\/wandb\/artifacts<\/code> and organized by their checksum. So if you try to download a file with checksum <code>x<\/code> and that file has been logged in an Artifact from your machine or downloaded to your machine as part of an artifact before, we just pull it from the cache by checking if there is a cached Artifact file with checksum <code>x<\/code>.<\/p>\n<p>So, if you run the same code twice, assuming the version of the artifact you are trying to download has not changed, the artifact can simply be pickked up from your cache directory.<\/p>\n<p>Also, when calling <code>artifact.download()<\/code> without any arguments, the artifact is saved in the directory in which the code is running. This, however,  is not the directory that serves as a cache, that still remains <code>.cache<\/code> which acts as a central location to look for artifacts before fetching it.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":13.49,
        "Solution_score_count":null,
        "Solution_sentence_count":10.0,
        "Solution_word_count":162.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1592301866083,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3565.0,
        "Answerer_view_count":366.0,
        "Challenge_adjusted_solved_time":3.2437644444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm having troubles with a Letter Recognition model I'm creating in Azure ML Studio.<\/p>\n<p>I'm running a few algorithms - Decision Jungle, Neural Network, Decision Forest, Logistic Regression, One vs. All Multiclass, and then I append them using the Add rows method (Neural Network and Desicion Jungle\/ Decision Forest and Logistic Regression), until I append them all.<\/p>\n<p>However, appending Decision Forest and Logistic Regression I get the following error:<\/p>\n<pre><code>requestId = 9292bc066f51404eb5e0d0d219d3a072 errorComponent=Module. taskStatusCode=400. {&quot;Exception&quot;:{&quot;ErrorId&quot;:&quot;NotInRangeValue&quot;,&quot;ErrorCode&quot;:&quot;0008&quot;,&quot;ExceptionType&quot;:&quot;ModuleException&quot;,&quot;Message&quot;:&quot;Error 0008: Parameter \\&quot;Dataset2(number of columns)\\&quot; value should be in the range of [3, 3].&quot;}}Error: Error 0008: Parameter &quot;Dataset2(number of columns)&quot; value should be in the range of [3, 3]. Process exited with error code -2\n<\/code><\/pre>\n<p>Any advice what should I do? Huge thanks in advance<\/p>",
        "Challenge_closed_time":1599653828448,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599640980777,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while creating a Letter Recognition model in Azure ML Studio. They are running multiple algorithms and appending them using the Add rows method, but when they try to append Decision Forest and Logistic Regression, they receive an error message stating that the \"Dataset2(number of columns)\" parameter value should be in the range of [3, 3]. The user is seeking advice on how to resolve this issue.",
        "Challenge_last_edit_time":1599642150896,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63807950",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.4,
        "Challenge_reading_time":14.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":3.5687975,
        "Challenge_title":"Letter Recognition Error in Azure ML Studio",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":37.0,
        "Challenge_word_count":125,
        "Platform":"Stack Overflow",
        "Poster_created_time":1599640856987,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>This error occurs when there is a mismatch of number of columns of the two dataset you are appending.<\/p>\n<p>Looking at the error :<\/p>\n<p>The output of one model is returning rows with 3 columns and other one is having either more or less than 3 columns.<\/p>\n<p>Before this step &quot;Add Rows&quot; step -&gt; Do quick Visualize<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/PsYQT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PsYQT.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>This will give a view of the dataset that you are planning to append.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/x442d.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/x442d.png\" alt=\"![enter image description here\" \/><\/a><\/p>\n<p>Ensure for both, the columns numbers are same.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":8.1,
        "Solution_reading_time":10.4,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":99.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":258.9256763889,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>We need to set a dataset folder in S3 as an artifact.  The folder has many sub-directories (only one layer though).<br>\nWhen I use the a <code>add_reference()<\/code> command it only stores the directory names of the top-level.<br>\nOf course, I could loop across it, but I\u2019m wondering if there is a command option to make the operation recursive?<\/p>\n<pre><code class=\"lang-auto\">run  = wandb.init(project=WB_PROJECT)\nart = wandb.Artifact(WB_ENTITY, type=WB_DATASET)\nart.add_reference(s3_full, max_objects=WB_MAX_OBJECTS_TO_UPLOAD)\nrun.log_artifact(art)\nwandb.finish()\n<\/code><\/pre>\n<p>EDIT 1: I conclude that the all files are not being added because the <code>Num Files<\/code> in the Artifact Overview shows only <code>5<\/code>.  If I click on the directories, it seems I can see the files, but I assume they are not actually there because of the <code>5<\/code> being reported for the number of files.<\/p>",
        "Challenge_closed_time":1663759922548,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662827790113,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to set a dataset folder in S3 as an artifact using the `add_reference()` command in WandB. However, the command only stores the directory names of the top-level and not the files within the sub-directories. The user is looking for a command option to make the operation recursive. The `Num Files` in the Artifact Overview shows only 5, indicating that not all files are being added.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/add-reference-with-nested-folders\/3092",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.1,
        "Challenge_reading_time":11.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":258.9256763889,
        "Challenge_title":"Add_reference() with nested folders",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":450.0,
        "Challenge_word_count":127,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Kevin,<\/p>\n<p>Thanks for the detailed explanation! I see your issue, I will create a request for this feature, thanks for reporting it! May I help you with any other issue?<\/p>\n<p>Best,<br>\nLuis<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.2,
        "Solution_reading_time":2.53,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":33.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":92.1736986111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have some data with very particular format (e.g., tdms files generated by NI systems) and I stored them in a S3 bucket. Typically, for reading this data in python if the data was stored in my local computer, I would use npTDMS package. But, how should is read this tdms files when they are stored in a S3 bucket? One solution is to download the data for instance to the EC2 instance and then use npTDMS package for reading the data into python. But it does not seem to be a perfect solution. Is there any way that I can read the data similar to reading CSV files from S3? <\/p>",
        "Challenge_closed_time":1577181091056,
        "Challenge_comment_count":3,
        "Challenge_created_time":1576870865157,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has data stored in a specific format (tdms files) in an AWS S3 bucket and is looking for a way to read the data in Python without having to download it to an EC2 instance. They are wondering if there is a way to read the data similar to reading CSV files from S3.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59430560",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":6.6,
        "Challenge_reading_time":7.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":86.1738608333,
        "Challenge_title":"Reading Data from AWS S3",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":4393.0,
        "Challenge_word_count":116,
        "Platform":"Stack Overflow",
        "Poster_created_time":1534965197292,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seattle, WA",
        "Poster_reputation_count":320.0,
        "Poster_view_count":48.0,
        "Solution_body":"<p>Some Python packages (such as Pandas) support reading data directly from S3, as it is the most popular location for data. See <a href=\"https:\/\/stackoverflow.com\/questions\/37703634\/how-to-import-a-text-file-on-aws-s3-into-pandas-without-writing-to-disk\">this question<\/a> for example on the way to do that with Pandas.<\/p>\n\n<p>If the package (npTDMS) doesn't support reading directly from S3, you should copy the data to the local disk of the notebook instance.<\/p>\n\n<p>The simplest way to copy is to run the AWS CLI in a cell in your notebook<\/p>\n\n<pre><code>!aws s3 cp s3:\/\/bucket_name\/path_to_your_data\/ data\/\n<\/code><\/pre>\n\n<p>This command will copy all the files under the \"folder\" in S3 to the local folder <code>data<\/code><\/p>\n\n<p>You can use more fine-grained copy using the filtering of the files and other specific requirements using the boto3 rich capabilities. For example:<\/p>\n\n<pre><code>s3 = boto3.resource('s3')\nbucket = s3.Bucket('my-bucket')\nobjs = bucket.objects.filter(Prefix='myprefix')\nfor obj in objs:\n   obj.download_file(obj.key)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1577202690472,
        "Solution_link_count":1.0,
        "Solution_readability":10.8,
        "Solution_reading_time":13.62,
        "Solution_score_count":3.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":133.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":338.3591666667,
        "Challenge_answer_count":0,
        "Challenge_body":"https:\/\/github.com\/canonical\/mlflow-operator\/blob\/c856446074868d4735627c95878960d91555f4da\/charms\/mlflow-server\/src\/charm.py#L20\r\n\r\nThe name of the bucket for MLFlow is hardcoded. This is a big issue because this makes using Minio in Gateway mode + MLFlow impossible on AWS (S3 buckets are globally unique).\r\n\r\nIt's a good first issue :)",
        "Challenge_closed_time":1647350309000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646132216000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The mlflow chart has a bug where the newly added staticPrefix parameter under extraArgs breaks the chart when used because it tries to add an extra argument to the mlflow server command that doesn't exist. The user suggests a solution to handle the staticPrefix as a separate argument in the extraEnv when starting up the mlflow server to make it work smoother for the final user. The user is creating a pull request to address this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/canonical\/mlflow-operator\/issues\/24",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":9.1,
        "Challenge_reading_time":5.14,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":6.0,
        "Challenge_repo_issue_count":79.0,
        "Challenge_repo_star_count":6.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":338.3591666667,
        "Challenge_title":"MLFlow hardcoded bucket name - impossible to use MLFlow with AWS S3",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":47,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1589984605967,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":20.0,
        "Challenge_adjusted_solved_time":50.0349444445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to test Sagemaker Groundtruth's active learning capability, but cannot figure out how to get the auto-labeling part to work. I started a previous labeling job with an initial model that I had to create manually. This allowed me to retrieve the model's ARN as a starting point for the next job. I uploaded 1,758 dataset objects and labeled 40 of them. I assumed the auto-labeling would take it from here, but the job in Sagemaker just says \"complete\" and is only displaying the labels that I created. How do I make the auto-labeler work?<\/p>\n\n<p>Do I have to manually label 1,000 dataset objects before it can start working? I saw this post: <a href=\"https:\/\/stackoverflow.com\/questions\/57852690\/information-regarding-amazon-sagemaker-groundtruth\">Information regarding Amazon Sagemaker groundtruth<\/a>, where the representative said that some of the 1,000 objects can be auto-labeled, but how is that possible if it needs 1,000 objects to start auto-labeling? <\/p>\n\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1589986381867,
        "Challenge_comment_count":0,
        "Challenge_created_time":1589806256067,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is having trouble getting the auto-labeling feature to work in Amazon Sagemaker Groundtruth's active learning capability. They have labeled 40 out of 1,758 dataset objects and assumed the auto-labeling would take over, but it did not. The user is unsure if they need to manually label 1,000 dataset objects before the auto-labeler can work. They are seeking advice on how to make the auto-labeler work.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61870000",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":13.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":50.0349444445,
        "Challenge_title":"Amazon Sagemaker Groundtruth: Cannot get active learning to work",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":738.0,
        "Challenge_word_count":159,
        "Platform":"Stack Overflow",
        "Poster_created_time":1489377488790,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":437.0,
        "Poster_view_count":68.0,
        "Solution_body":"<p>I'm an engineer at AWS. In order to understand the \"active learning\"\/\"automated data labeling\" feature, it will be helpful to start with a broader recap of how SageMaker Ground Truth works.<\/p>\n\n<p>First, let's consider the workflow without the active learning feature. Recall that Ground Truth annotates data in batches [<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-batching.html]\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-batching.html]<\/a>. This means that your dataset is submitted for annotation in \"chunks.\" The size of these batches is controlled by the API parameter MaxConcurrentTaskCount [<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_HumanTaskConfig.html#sagemaker-Type-HumanTaskConfig-MaxConcurrentTaskCount]\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_HumanTaskConfig.html#sagemaker-Type-HumanTaskConfig-MaxConcurrentTaskCount]<\/a>. This parameter has a default value of 1,000. You cannot control this value when you use the AWS console, so the default value will be used unless you alter it by submitting your job via the API instead of the console.<\/p>\n\n<p>Now, let's consider how active learning fits into this workflow. Active learning runs <em>in between<\/em> your batches of manual annotation. Another important detail is that Ground Truth will partition your dataset into a validation set and an unlabeled set. For datasets smaller than 5,000 objects, the validation set will be 20% of your total dataset; for datasets largert than 5,000 objects, the validation set will be 10% of your total dataset. Once the validation set is collected, any data that is subsequently annotated manually consistutes the training set. The collection of the validation set and training set proceeds according to the batch-wise process described in the previous paragraph. A longer discussion of active learning is available in [<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html]\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html]<\/a>.<\/p>\n\n<p>That last paragraph was a bit of a mouthful, so I'll provide an example using the numbers you gave.<\/p>\n\n<h1>Example #1<\/h1>\n\n<ul>\n<li>Default MaxConcurrentTaskCount (\"batch size\") of 1,000<\/li>\n<li>Total dataset size: 1,758 objects<\/li>\n<li>Computed validation set size: 0.2 * 1758 = 351 objects<\/li>\n<\/ul>\n\n<p>Batch #<\/p>\n\n<ol>\n<li>Annotate 351 objects to populate the validation set (1407 remaining).<\/li>\n<li>Annotate 1,000 objects to populate the first iteration of the training set (407 remaining).<\/li>\n<li>Run active learning. This step may, depending on the accuracy of the model at this stage, result in the annotation of zero, some, or all of the remaining 407 objects.<\/li>\n<li>(Assume no objects were automatically labeled in step #3) Annotate 407 objects. End labeling job.<\/li>\n<\/ol>\n\n<h1>Example #2<\/h1>\n\n<ul>\n<li>Non-default MaxConcurrentTaskCount (\"batch size\") of 250<\/li>\n<li>Total dataset size: 1,758 objects<\/li>\n<li>Computed validation set size: 0.2 * 1758 = 351 objects<\/li>\n<\/ul>\n\n<p>Batch #<\/p>\n\n<ol>\n<li>Annotate 250 objects to begin populating the validation set (1508 remaining).<\/li>\n<li>Annotate 101 objects to finish populating the validation set (1407 remaining).<\/li>\n<li>Annotate 250 objects to populate the first iteration of the training set (1157 remaining).<\/li>\n<li>Run active learning. This step may, depending on the accuracy of the model at this stage, result in the annotation of zero, some, or all of the remaining 1157 objects. All else being equal, we would expect the model to be less accurate than the model in example #1 at this stage, because our training set is only 250 objects here.<\/li>\n<li>Repeat alternating steps of annotating batches of 250 objects and running active learning.<\/li>\n<\/ol>\n\n<p>Hopefully these examples illustrate the workflow and help you understand the process a little better. Since your dataset consists of 1,758 objects, the upper bound on the number of automated labels that can be supplied is 407 objects (assuming you use the default MaxConcurrentTaskCount).<\/p>\n\n<p>Ultimately, 1,758 objects is still a relatively small dataset. We typically recommend at least 5,000 objects to see meaningful results [<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html]\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html]<\/a>. Without knowing any other details of your labeling job, it's difficult to gauge why your job didn't result in more automated annotations. A useful starting point might be to inspect the annotations you received, and to determine the quality of the model that was trained during the Ground Truth labeling job.<\/p>\n\n<p>Best regards from AWS! <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":8.0,
        "Solution_readability":12.6,
        "Solution_reading_time":62.4,
        "Solution_score_count":4.0,
        "Solution_sentence_count":42.0,
        "Solution_word_count":618.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":96.0666666667,
        "Challenge_answer_count":3,
        "Challenge_body":"Hi,\n\nI created my models with Auto ML (image classification or object detection).\n\nNow, I would like to use these in my application, on local (disconnected).\n\nIs it possible to extract a model file from Auto ML that I can use (.pb for instance) ?\n\nAfter some researches, it seems to me that it is not possible but I would like to be sure.\n\nElse, how?\n\nRegards.",
        "Challenge_closed_time":1671501360000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1671155520000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user created models with Auto ML for image classification or object detection and wants to use them in their local application. They are unsure if it is possible to extract a model file from Auto ML that they can use, such as .pb. They have researched but are still unsure and are seeking clarification.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Can-I-export-a-my-model-for-internal-usage\/m-p\/500236#M951",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.0,
        "Challenge_reading_time":4.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":96.0666666667,
        "Challenge_title":"Can I export a my model for internal usage ?",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":421.0,
        "Challenge_word_count":75,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I just didn't understand that I need to select the \"edge\" option to have the \"export model\" available.\n\nThank you for the useful documentation .\n\nAs a additionnal question : can I know the version of Tensorflow used for these export model? (I have some incompatibiliy to use these in my software).\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":3.97,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":55.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.6311111111,
        "Challenge_answer_count":1,
        "Challenge_body":"Some Amazon SageMaker algorithms can train with a manifest JSON file that stores the mapping between images and their Amazon S3 ARNs and metadata, such as labels. This is a great option, because the manifest file is much smaller than the dataset itself. Because the manifest files are small, they can be used easily in versioning tools or saved as part of the model artifact. This appears to be the best construct enabling exact dataset versioning within SageMaker. i.e., if we exclude the creation of a unique training set hard copy per training job that can't be scaled to large datasets. Is my understanding accurate?",
        "Challenge_closed_time":1607684202000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607681930000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is asking if Amazon SageMaker manifest files allow for dataset versioning, as they are smaller than the dataset and can be easily used in versioning tools or saved as part of the model artifact. They are wondering if this is the best way to enable exact dataset versioning within SageMaker, aside from creating a unique training set hard copy per training job that cannot be scaled to large datasets.",
        "Challenge_last_edit_time":1667981435996,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUq44kZCYWTiOnwXblHSQSTA\/do-amazon-sagemaker-manifest-files-enable-dataset-versioning",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":8.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.6311111111,
        "Challenge_title":"Do Amazon SageMaker manifest files enable dataset versioning?",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":124.0,
        "Challenge_word_count":112,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"If you create the conditions for immutability of the assets the manifest points to, then manifest enables exact dataset versioning with SageMaker. You can have a data store in Amazon S3 with all versions of the data assets and use the manifest files for creating and versioning datasets for specific usage.\n\nIf you don't guarantee immutability for the assets that the manifest points to, then your manifest becomes invalid.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925565630,
        "Solution_link_count":0.0,
        "Solution_readability":12.3,
        "Solution_reading_time":5.2,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":69.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":457.0916797222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I need to train a custom OCR in vertex AI. My data with have folder of cropped image, each image is a line, and a csv file with 2 columns: image name and text in image.\nBut when I tried to import it into a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-managed-datasets\" rel=\"nofollow noreferrer\">dataset<\/a> in vertex AI, I see that image dataset only support for classification, segmentation, object detection. All of dataset have fixed number of label, but my data have a infinite number of labels(if we view text in image as label), so all types doesn't match with my requirement. Can I use vertex AI for training, and how to do that ?<\/p>",
        "Challenge_closed_time":1652091441187,
        "Challenge_comment_count":2,
        "Challenge_created_time":1650445911140,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while trying to train a custom OCR in Vertex AI using a custom data format that includes a folder of cropped images and a CSV file with two columns. The user is unable to import the data into a dataset in Vertex AI as the image dataset only supports classification, segmentation, and object detection, which do not match the user's requirements. The user is seeking guidance on whether Vertex AI can be used for training and how to proceed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71937033",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":8.7,
        "Challenge_reading_time":8.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":457.0916797222,
        "Challenge_title":"Google Cloud Platform - Vertex AI training with custom data format",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":303.0,
        "Challenge_word_count":118,
        "Platform":"Stack Overflow",
        "Poster_created_time":1412860343896,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hanoi, Vietnam",
        "Poster_reputation_count":803.0,
        "Poster_view_count":114.0,
        "Solution_body":"<p>Since Vertex AI managed datasets do not support OCR applications, you can train and deploy a custom model using Vertex AI\u2019s training and prediction services.<\/p>\n<p>I found a good <a href=\"https:\/\/medium.com\/geekculture\/building-a-complete-ocr-engine-from-scratch-in-python-be1fd184753b\" rel=\"nofollow noreferrer\">article<\/a> on building an OCR system from scratch. This OCR system is implemented in 2 steps<\/p>\n<ol>\n<li>Text detection<\/li>\n<li>Text recognition<\/li>\n<\/ol>\n<p>Please note that this article is not officially supported by Google Cloud.<\/p>\n<p>Once you have tested the model locally, you can train the same on Vertex AI using the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/custom-training\" rel=\"nofollow noreferrer\">custom model training service<\/a>. Please follow this <a href=\"https:\/\/codelabs.developers.google.com\/vertex_custom_training_prediction\" rel=\"nofollow noreferrer\">codelab<\/a> for step-by-step instructions on training and deploying a custom model.<\/p>\n<p>Once the training is complete, the model can be deployed for inference using a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/pre-built-containers\" rel=\"nofollow noreferrer\">pre-built container<\/a> offered by Vertex AI or a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements\" rel=\"nofollow noreferrer\">custom container<\/a> based on your requirements. You can also choose between batch predictions for synchronous requests and online predictions for asynchronous requests.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":15.5,
        "Solution_reading_time":20.2,
        "Solution_score_count":2.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":157.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":105.3469,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to build a binary classifier based on a tabular dataset that is rather sparse, but training is failing with the following message:<\/p>\n<blockquote>\n<p>Training pipeline failed with error message: Too few input rows passed validation. Of 1169548 inputs, 194 were valid. At least 50% of rows must pass validation.<\/p>\n<\/blockquote>\n<p>My understanding was that tabular AutoML should be able to handle Null values, so I'm not sure what's happening here, and I would appreciate any suggestions. The <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/tabular-data\/tabular101\" rel=\"nofollow noreferrer\">documentation<\/a> explicitly mentions reviewing each column's nullability, but I don't see any way to set or check a column's nullability on the dataset tab (perhaps the documentation is out of date?). Additionally, the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/data-types-tabular#what_values_are_treated_as_null_values\" rel=\"nofollow noreferrer\">documentation<\/a> explicitly mentions that missing values are treated as null, which is how I've set up my CSV. The <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/data-types-tabular#numeric\" rel=\"nofollow noreferrer\">documentation for numeric<\/a> however does not explicitly list support for missing values, just NaN and inf.<\/p>\n<p>The dataset is 1 million rows, 34 columns, and only 189 rows are null-free. My most sparse column has data in 5,000 unique rows, with the next rarest having data in 72k and 274k rows respectively. Columns are a mix of categorical and numeric, with only a handful of columns without nulls.<\/p>\n<p>The data is stored as a CSV, and the Dataset import seems to run without issue. Generate statistics ran on the dataset, but for some reason the missing % column failed to populate. What might be the best way to address this? I'm not sure if this is a case where I need to change my null representation in the CSV, change some dataset\/training setting, or if its an AutoML bug (less likely). Thanks!<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bF1Nn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bF1Nn.png\" alt=\"Image of missing % column being blank\" \/><\/a><\/p>",
        "Challenge_closed_time":1657366819400,
        "Challenge_comment_count":3,
        "Challenge_created_time":1656554846450,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing issues while building a binary classifier using VertexAI Tabular AutoML due to the rejection of rows containing null values. The dataset is sparse, with only 189 null-free rows out of 1 million rows and 34 columns. The user is unsure about how to set or check a column's nullability on the dataset tab and is seeking suggestions to address the issue. The missing % column failed to populate during the Generate statistics run on the dataset.",
        "Challenge_last_edit_time":1656987570560,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72809603",
        "Challenge_link_count":5,
        "Challenge_participation_count":5,
        "Challenge_readability":11.2,
        "Challenge_reading_time":28.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":225.5480416667,
        "Challenge_title":"VertexAI Tabular AutoML rejecting rows containing nulls",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":122.0,
        "Challenge_word_count":300,
        "Platform":"Stack Overflow",
        "Poster_created_time":1614739500312,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":51.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>To allow invalid &amp; null values during training &amp; prediction, we have to explicitly set the <code>allow invalid values<\/code> flag to <code>Yes<\/code> during training as shown in the image below. You can find this setting under model training settings on the dataset page. The flag has to be set on a column by column basis.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jWDGm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jWDGm.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.5,
        "Solution_reading_time":6.49,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":65.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1334762714136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation_count":6557.0,
        "Answerer_view_count":2005.0,
        "Challenge_adjusted_solved_time":15.7899352778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When I upload dataset with more then 100 columns I can see only part of them in the visualisation block. Can I see stats for all columns from dataset? Thanks<\/p>",
        "Challenge_closed_time":1459517016507,
        "Challenge_comment_count":0,
        "Challenge_created_time":1459460172740,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge with Azure machine learning as they are unable to view all columns of a dataset with more than 100 columns in the visualization block. They are seeking a solution to view statistics for all columns in the dataset.",
        "Challenge_last_edit_time":1459517035743,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36344278",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":3.5,
        "Challenge_reading_time":2.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":15.7899352778,
        "Challenge_title":"Azure machine learning. How can I see all columns",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":736.0,
        "Challenge_word_count":38,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459459387887,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>If you are an owner in the workspace, you can open your dataset in Python inside of a Jupyter Notebook. By the visualize should be an open in notebook button. Then just execute the code that is provided for you, and it should print your dataset. You can then also select specific columns to visualize as well.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":3.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":57.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1363352099923,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Washington, DC, USA",
        "Answerer_reputation_count":828.0,
        "Answerer_view_count":530.0,
        "Challenge_adjusted_solved_time":2983.5262194445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create a custom Image Classifier in Amazon SageMaker. It is giving me the following error:<\/p>\n\n<p><code>\"ClientError: Data download failed:NoSuchKey (404): The specified key does not exist.\"<\/code><\/p>\n\n<p>I'm assuming this means one of the pictures in my <code>.lst<\/code> file is missing from the directory. Is there some way to find out which <code>.lst<\/code> listing it is specifically having trouble with?<\/p>",
        "Challenge_closed_time":1545063624710,
        "Challenge_comment_count":1,
        "Challenge_created_time":1534309681237,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while creating a custom Image Classifier in Amazon SageMaker. The error message indicates that there is a missing picture in the .lst file, but the user is unsure which listing is causing the issue and is seeking a way to track it down.",
        "Challenge_last_edit_time":1534322930320,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51853249",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":5.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":2987.2065202778,
        "Challenge_title":"Error Tracking in Amazon SageMaker",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":305.0,
        "Challenge_word_count":67,
        "Platform":"Stack Overflow",
        "Poster_created_time":1363352099923,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Washington, DC, USA",
        "Poster_reputation_count":828.0,
        "Poster_view_count":530.0,
        "Solution_body":"<p>Upon further examination (of the log files), it appears the issue does not lie with the .lst file itself, but with the image files it was referencing (which now leaves me wondering why AWS doesn't just say that instead of saying the .lst file is corrupt). I'm going through the image files one-by-one to verify they are correct, hopefully that will solve the problem.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":4.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":0.6663211111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to write delta tables in Kedro. Changing file format to delta makes the write as delta tables with mode as overwrite.<\/p>\n<p>Previously, a node in the raw layer (meta_reload) creates a dataset that determines what's the start date for incremental load for each dataset. each node uses that raw dataset to filter the working dataset to apply the transformation logic and write partitioned parquet tables incrementally.<\/p>\n<p>But now writing delta with mode as overwrite with just file type change to delta makes current incremental data overwrite all the past data instead of just those partitions. So I need to use replaceWhere option in save_args in the catalog.\nHow would I determine the start date for replaceWhere in the catalog when I need to read the meta_reload raw dataset to determine the date.\nIs there a way to dynamically pass the save_args from inside the node?<\/p>\n<pre><code>my_dataset:\n  type: my_project.io.pyspark.SparkDataSet\n  filepath: &quot;s3:\/\/${bucket_de_pipeline}\/${data_environment_project}\/${data_environment_intermediate}\/my_dataset\/&quot;\n  file_format: delta\n  layer: intermediate\n  save_args:\n    mode: &quot;overwrite&quot;\n    replaceWhere: &quot;DATE_ID &gt; xyz&quot;  ## what I want to implement dynamically\n    partitionBy: [ &quot;DATE_ID&quot; ]\n<\/code><\/pre>",
        "Challenge_closed_time":1632930071223,
        "Challenge_comment_count":0,
        "Challenge_created_time":1632927672467,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to write delta tables in Kedro, but changing the file format to delta makes the write as delta tables with mode as overwrite. This causes the current incremental data to overwrite all the past data instead of just those partitions. The user needs to use the replaceWhere option in save_args in the catalog to determine the start date for replaceWhere in the catalog when they need to read the meta_reload raw dataset to determine the date. They are looking for a way to dynamically pass the save_args from inside the node.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69378898",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":17.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.6663211111,
        "Challenge_title":"How to dynamically pass save_args to kedro catalog?",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":341.0,
        "Challenge_word_count":179,
        "Platform":"Stack Overflow",
        "Poster_created_time":1477986647030,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Atlanta, GA, USA",
        "Poster_reputation_count":171.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>I've answered this on the GH <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/discussions\/910\" rel=\"nofollow noreferrer\">discussion<\/a>. In short you would need to subclass and define your own <code>SparkDataSet<\/code> we avoid changing the underlying API of the datasets at a Kedro level, but you're encouraged to alter and remix this for your own purposes.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.6,
        "Solution_reading_time":4.72,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":47.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1424548126510,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"India",
        "Answerer_reputation_count":665.0,
        "Answerer_view_count":151.0,
        "Challenge_adjusted_solved_time":0.365185,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a Sagemaker instance that's linked to a github repo <code>my-repo<\/code>, and every time I open a new terminal, I see this immediately at startup: <\/p>\n\n<pre><code>sh-4.2$ cd \"my-repo\"\nsh: cd: my-repo: No such file or directory\n<\/code><\/pre>\n\n<p>I assumed something was in the .bashrc or .bash_profile that prompted this (failed) <code>cd<\/code> but it's not in there. Any ideas where I should look for what's causing this behavior? <\/p>",
        "Challenge_closed_time":1567699632856,
        "Challenge_comment_count":0,
        "Challenge_created_time":1567698318190,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has a Sagemaker instance linked to a Github repo, and every time they open a new terminal, they see a failed \"cd\" command to the repo directory. The user is unsure of what is causing this behavior and is seeking advice on where to look for the cause.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57808963",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.0,
        "Challenge_reading_time":6.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.365185,
        "Challenge_title":"Sagemaker instance does automatic cd at startup",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":304.0,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Poster_created_time":1386023479736,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":725.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>The issue is not specific to SageMaker Notebook instances. Rather, it is a bug in the Git extension of JupyterLab. You can find details around this here: <a href=\"https:\/\/github.com\/jupyterlab\/jupyterlab-git\/issues\/346\" rel=\"nofollow noreferrer\">https:\/\/github.com\/jupyterlab\/jupyterlab-git\/issues\/346<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.1,
        "Solution_reading_time":4.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2210.2472222222,
        "Challenge_answer_count":0,
        "Challenge_body":"\r\nWhen I try to run a pipeline with target as \"local\" it gives me an error. \r\nValueError: Please specify a remote compute_target. \r\nThis should be mentioned somewhere in the end of the page under target section. \r\nAlso please specify why pipelines cannot be run on local target? People like me waste a lot of time trying this & then realize its a shortcoming in the Azure ML Python SDK. \r\nPlease update this documentation page as soon as possible.\r\n![image](https:\/\/user-images.githubusercontent.com\/17008122\/106663751-73fe0000-65a4-11eb-87f7-fcc7613dd42f.png)\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: f2c8e18c-8443-67fe-b1f9-531de3599c8f\r\n* Version Independent ID: a8c897b7-c44b-1a72-52f2-f81bbdbce753\r\n* Content: [azureml.core.runconfig.RunConfiguration class - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.runconfig.runconfiguration?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.runconfig.RunConfiguration.yml](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.runconfig.RunConfiguration.yml)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @DebFro\r\n* Microsoft Alias: **debfro**",
        "Challenge_closed_time":1620257629000,
        "Challenge_comment_count":8,
        "Challenge_created_time":1612300739000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue with Azure's TabularDataset implementation when creating or reading parquet files that were originally written by Pandas\/Python. An index, \\_\\_index\\_level_0\\_\\_, is introduced which causes errors if not handled when making changes to datasets. The issue occurs when an index is unnamed but has been modified at some point. The user has provided an example notebook to reproduce the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1316",
        "Challenge_link_count":3,
        "Challenge_participation_count":8,
        "Challenge_readability":15.0,
        "Challenge_reading_time":19.6,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":2210.2472222222,
        "Challenge_title":"Local execution is not supported for Azure ML pipelines. ValueError: Please specify a remote compute_target. ",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":134,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"apologies, we understand the frustration and are working to fully support local execution through Azure Machine Learning with our v2 developer experience, which is approaching public preview While it is allowed to Run AzureML experiments in Local Target using the Python SDK, I am expecting the pipelines as well to be allowed to run on local target. If this is an exception then it should be clearly flagged out & documented by Microsoft at all relevant places. Below 2 pages should definitely contain this note\r\n1. \r\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.workspace(class)?view=azure-ml-py#azureml_core_Workspace_compute_targets\r\n(under compute_targets section)\r\n\r\n2.\r\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.runconfig.runconfiguration?view=azure-ml-py\r\n(under target section)\r\n\r\nAlso please mention the target release date of v2 developer experience unfortunately the initial preview of v2 will not address this issue, I will allow the Pipelines team to give a more clear ETA for that. but initial preview is tentatively March 2021 Thank you for quick reply. I would be happy if this feature is included in the 2.0 release. Let me know if there is any way to rate this feature on higher priority.\r\n\r\nPS: Please change your screen name,  \"lostmygithubaccount\" is very confusing & unprofessional.  Hi @lostmygithubaccount and @meghalv .  I'm currently blocked by this issue.  I'm unable to allocate a remote Compute Target and I don't find an example on how to use my local computer.\r\n\r\nIs this feature already delivered?.  Do you have an example? Hi @lostmygithubaccount, \r\n\r\nwhat is the status of local execution of Pipelines in Azure Machine Learning? Why was this issue closed without any conclusive information or workaround? \r\n\r\nThis missing feature is blocking customers that want to use local IDE and debugging. The local pipeline is still in development. We don't have an ETA for the release date. Hi, I just wanted to contribute to the conversation and say that this feature would be much appreciated. Currently, it is difficult to bounce between local debugging and cloud deployment. This is because the lack of local pipeline support requires change in data-flow as well as various azureml-core variables that are accessible during pipeline runs. ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.4,
        "Solution_reading_time":28.7,
        "Solution_score_count":null,
        "Solution_sentence_count":22.0,
        "Solution_word_count":333.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0894444444,
        "Challenge_answer_count":0,
        "Challenge_body":"We\u2019re implementing a few things and I\u2019ve got a quick question.\nI see the example here https:\/\/github.com\/polyaxon\/polyaxon\/blob\/faec6649ed6a09ad29365f17795a404cc714c22e\/site\/integrations\/data-on-s3.md but I don\u2019t quite understand how to setup that S3Service(...) object. what would I pass in? a connection? how do I create the s3 connection object to pass in? I\u2019ve got the connection defined in polyaxon\u2019s config\/polyaxonfile but I\u2019m not sure what to create in python there.",
        "Challenge_closed_time":1649333512000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649333190000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to configure an S3 connection to upload\/download artifacts programmatically. They are having trouble understanding how to set up the S3Service object and what to pass in, specifically how to create the S3 connection object in Python. They have the connection defined in Polyaxon's config\/polyaxonfile.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1480",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":9.7,
        "Challenge_reading_time":7.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.0894444444,
        "Challenge_title":"How to configured S3 connection to upload\/download artifacts programmatically",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":72,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"We have to update that section, those services are deprecated, I would use the new implementation:\n\nfrom polyaxon.fs.fs import get_fs_from_name\n\nfs = get_fs_from_name(\"model-registry-s3\")\n\nThis will return a fully resolved s3fs object. More information about how to use the fs object: https:\/\/s3fs.readthedocs.io\/en\/latest\/#examples.\n\nNote 1: The s3 rquirement is not installed by default, you wiil need pip install \"polyaxon[s3]\"\n\nNote2: You will have to request the connection:\n\nrun:\n  connections: [\"model-registry-s3\"]\n\nAlso by requesting the connection, the secret\/config will be available in the container, so you can also use boto3 automatically if you do not like the to the s3fs implementation.\n\nThe docs for:\n\nGCS\nS3\nAzure",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.5,
        "Solution_reading_time":9.08,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":103.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1280505139752,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bangalore, India",
        "Answerer_reputation_count":4265.0,
        "Answerer_view_count":403.0,
        "Challenge_adjusted_solved_time":0.0257813889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Looks like AzureML Python SDK has two Dataset packages exposed over API:<\/p>\n<ol>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.tabulardataset?view=azure-ml-py\" rel=\"nofollow noreferrer\">azureml.contrib.dataset<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-contrib-dataset\/azureml.contrib.dataset.tabulardataset?view=azure-ml-py\" rel=\"nofollow noreferrer\">azureml.data<\/a><\/li>\n<\/ol>\n<p>The documentation doesn't clearly mention the difference or when should we use which one? But, it creates confusion for sure. For example, There are two Tabular Dataset classes exposed over API. And they have different APIs for different functions:<\/p>\n<ol>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.tabulardataset?view=azure-ml-py\" rel=\"nofollow noreferrer\">azureml.data.TabularDataset<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-contrib-dataset\/azureml.contrib.dataset.tabulardataset?view=azure-ml-py\" rel=\"nofollow noreferrer\">azureml.contrib.dataset.TabularDataset<\/a><\/li>\n<\/ol>\n<p>Any suggestion about when should I use which package will be helpful.<\/p>",
        "Challenge_closed_time":1645168074896,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645165311677,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is confused about the difference between two Dataset packages, azureml.contrib.dataset and azureml.data, in the AzureML Python SDK. The documentation does not clearly explain when to use which package, and there are two Tabular Dataset classes with different APIs for different functions. The user is seeking suggestions on when to use which package.",
        "Challenge_last_edit_time":1645167982083,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71169178",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":17.6,
        "Challenge_reading_time":16.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":0.7675608333,
        "Challenge_title":"azureml.contrib.dataset vs azureml.data",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":24.0,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Poster_created_time":1280505139752,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, India",
        "Poster_reputation_count":4265.0,
        "Poster_view_count":403.0,
        "Solution_body":"<p>As per the <a href=\"https:\/\/pypi.org\/project\/azureml-contrib-dataset\/\" rel=\"nofollow noreferrer\">PyPi<\/a>, <code>azureml.contrib.dataset<\/code> has been deprecated and <code>azureml.data<\/code> should be used instead:<\/p>\n<blockquote>\n<p>The azureml-contrib-dataset package has been deprecated and might not\nreceive future updates and removed from the distribution altogether.\nPlease use azureml-core instead.<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.6,
        "Solution_reading_time":5.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":41.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1544390307847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Palo Alto, CA, USA",
        "Answerer_reputation_count":151.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":167.9029461111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm working on a supervised machine learning problem, and I am setting up a custom labeling task to send out to Amazon Mechanical Turk for human annotation.<\/p>\n\n<p>I have uploaded the data to AWS S3 in the json-lines (<code>.jsonl<\/code>) format as follows, pursuant to the instructions as specified in the AWS documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-input.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-input.html<\/a>: <\/p>\n\n<pre><code>{\"source\": \"value0\"}\n{\"source\": \"value1\"}    \n{\"source\": \"value2\"}\n...\n{\"source\": \"value2\"}\n<\/code><\/pre>\n\n<p>When I click on the default text classification template, I can see my data come through and everything appears to work.<\/p>\n\n<p>However, I am getting the following error when I attempt to use the custom annotation task template interface: <code>MissingRequiredParameter: Missing required key 'FunctionName' in params<\/code> <\/p>\n\n<p>The error resembles an AWS Lambda error, except the strange thing is that I am not using AWS Lambda. Suggestions for how to proceed? <\/p>",
        "Challenge_closed_time":1552342307056,
        "Challenge_comment_count":0,
        "Challenge_created_time":1551737856450,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a \"MissingRequiredParameter\" error while setting up a custom labeling task for a supervised machine learning problem on Amazon Mechanical Turk. The error message is similar to an AWS Lambda error, but the user is not using AWS Lambda. The user has uploaded the data to AWS S3 in the json-lines format as per the AWS documentation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54992434",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.1,
        "Challenge_reading_time":15.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":167.9029461111,
        "Challenge_title":"MissingRequiredParameter: Missing required key 'FunctionName' in params",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":7776.0,
        "Challenge_word_count":144,
        "Platform":"Stack Overflow",
        "Poster_created_time":1336429658227,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Francisco, CA, USA",
        "Poster_reputation_count":947.0,
        "Poster_view_count":61.0,
        "Solution_body":"<p>I am from the engineering team and happy to help you here. I think the issue is not related to the manifest as it looks correct to me. The error suggests that you may haven't provided a correct lambda ARN for pre or post labeling task. Please see this doc for more details: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step3.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step3.html<\/a><\/p>\n\n<p>I can also help further if you can send me details on how you starting the job and what parameters you are sending.  <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.7,
        "Solution_reading_time":7.58,
        "Solution_score_count":5.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1541802293200,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":163.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":109.1823416667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The short story is, when I try to submit an azure ML pipeline run (an <em>azure ML pipeline<\/em>, not an <em>Azure pipeline<\/em>) from a jupyter notebook, I get PermissionError: [Errno 13] Permission denied: '.\\NTUSER.DAT'.  More details:<\/p>\n\n<p>Relevant code:<\/p>\n\n<pre><code>from azureml.train.automl import AutoMLConfig\nfrom azureml.train.automl.runtime import AutoMLStep\nautoml_settings = {\n    \"iteration_timeout_minutes\": 20,\n    \"experiment_timeout_minutes\": 30,\n    \"n_cross_validations\": 3,\n    \"primary_metric\": 'r2_score',\n    \"preprocess\": True,\n    \"max_concurrent_iterations\": 3,\n    \"max_cores_per_iteration\": -1,\n    \"verbosity\": logging.INFO,\n    \"enable_early_stopping\": True,\n    'time_column_name': \"DateTime\"\n}\n\nautoml_config = AutoMLConfig(task = 'forecasting',\n                             debug_log = 'automl_errors.log',\n                             path = \".\",\n                             compute_target=compute_target,\n                             run_configuration=conda_run_config,                               \n                             training_data = financeforecast_dataset,\n                             label_column_name = 'TotalUSD',\n                             **automl_settings\n                            )\n\nautoml_step = AutoMLStep(\n    name='automl_module',\n    automl_config=automl_config,\n    allow_reuse=False)\n\ntraining_pipeline = Pipeline(\n    description=\"training_pipeline\",\n    workspace=ws,    \n    steps=[automl_step])\n\ntraining_pipeline_run = Experiment(ws, 'test').submit(training_pipeline)\n<\/code><\/pre>\n\n<p>The training_pipeline step runs for apx 20 seconds, and then I get a long trace, ending in:<\/p>\n\n<pre><code>~\\AppData\\Local\\Continuum\\anaconda2\\envs\\forecasting\\lib\\site- \npackages\\azureml\\pipeline\\core\\_module_builder.py in _hash_from_file_paths(hash_src)\n    100             hasher = hashlib.md5()\n    101             for f in hash_src:\n--&gt; 102                 with open(str(f), 'rb') as afile:\n    103                     buf = afile.read()\n    104                     hasher.update(buf)\n\nPermissionError: [Errno 13] Permission denied: '.\\\\NTUSER.DAT'\n<\/code><\/pre>\n\n<p>According to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-your-first-pipeline\" rel=\"nofollow noreferrer\">Azure's docs on this topic<\/a>, submitting a pipeline uploads a \"snapshot\" of the \"source directory\" you specified.  Initially, I hadn't specified a source directory, so, to test that out, I added: <\/p>\n\n<pre><code>default_source_directory=\"testing\",\n<\/code><\/pre>\n\n<p>as a parameter for the training_pipeline object, but saw the same behavior when I then tried to run it.  Not sure if that is the same source directory the documentation is referring to.  The docs also say that if no source directory is specified, the \"current local directory\" is uploaded.  I used print (os.getcwd()) to get the working directory and gave \"Everyone\" full control permissions on the directory (working in a windows env).<\/p>\n\n<p>All the preceding code works fine, and I can submit an experiment if I use a ScriptRunConfig and run it on attached compute rather than using a pipeline\/training cluster.  <\/p>\n\n<p>Any ideas?  Thanks in advance to anyone who tries to help.  P.S. There is no \"azure-machine-learning-pipelines\" tag, and I can't add one because I don't have enough reputation points.  Someone else could though!  <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-ml-pipelines\" rel=\"nofollow noreferrer\">General<\/a> info on what they are.<\/p>",
        "Challenge_closed_time":1577994637207,
        "Challenge_comment_count":0,
        "Challenge_created_time":1577601580777,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a PermissionError: [Errno 13] Permission denied: '.\\NTUSER.DAT' when trying to submit an Azure ML pipeline run from a Jupyter notebook. The error occurs when the pipeline tries to upload a \"snapshot\" of the \"source directory\" specified. The user has tried specifying a source directory and giving \"Everyone\" full control permissions on the working directory, but the issue persists. The code works fine when using a ScriptRunConfig and running it on attached compute rather than using a pipeline\/training cluster.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59517355",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.4,
        "Challenge_reading_time":41.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":109.1823416667,
        "Challenge_title":"Permission denied: '.\\NTUSER.DAT' when trying to run an Azure ML Pipeline",
        "Challenge_topic":"File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":409.0,
        "Challenge_word_count":336,
        "Platform":"Stack Overflow",
        "Poster_created_time":1541802293200,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":163.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>I resolved this answer by setting the path and the data_script variables in the AutoMLConfig task object, like this (relevant code indicated by -->):<\/p>\n\n<pre><code>automl_config = AutoMLConfig(task = 'forecasting',\n                             debug_log = 'automl_errors.log',\n                             compute_target=compute_target,\n                             run_configuration=conda_run_config,\n                             --&gt;path = \"c:\\\\users\\\\me\",\n                             data_script =\"script.py\",&lt;--\n                             **automl_settings\n                            )\n<\/code><\/pre>\n\n<p>Setting the data_script variable to include the full path, as shown below, did not work.<\/p>\n\n<pre><code>automl_config = AutoMLConfig(task = 'forecasting',\n                             debug_log = 'automl_errors.log',\n                             path = \".\",\n                             --&gt;data_script = \"c:\\\\users\\\\me\\\\script.py\"&lt;--\n                             compute_target=compute_target,\n                             run_configuration=conda_run_config, \n                             **automl_settings\n                            )\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.7,
        "Solution_reading_time":10.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1436184843608,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":305.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":0.8062980556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is there any way I can make my dataset features in Azure ML into something else than what it already is? <\/p>\n\n<p>I found a dataset of the Titanic ship in the sample datasets which I would like to work with but all of my columns are either a numeric feature or string feature, but I would like to categorize these. Also is there any possibility to rename the columns within my model so it\u2019s more descriptive than what I initially got? I have no clue what SibSp means for instance.<\/p>",
        "Challenge_closed_time":1465484184003,
        "Challenge_comment_count":0,
        "Challenge_created_time":1465481281330,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to change the dataset features in Azure ML from numeric and string to categorized features. They also want to rename the columns within the model to make them more descriptive.",
        "Challenge_last_edit_time":1465977179120,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37728314",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.6,
        "Challenge_reading_time":6.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.8062980556,
        "Challenge_title":"Refactor columns and features in Azure Machine Learning",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":297.0,
        "Challenge_word_count":98,
        "Platform":"Stack Overflow",
        "Poster_created_time":1465326515432,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>What you are doing is essentially recreating this experiment made by Raja Iqbal for the Titanic dataset. I recommend you check that out here: <a href=\"http:\/\/gallery.cortanaintelligence.com\/Experiment\/Tutorial-Building-a-classification-model-in-Azure-ML-8?share=1\" rel=\"nofollow noreferrer\">http:\/\/gallery.cortanaintelligence.com\/Experiment\/Tutorial-Building-a-classification-model-in-Azure-ML-8?share=1<\/a><\/p>\n\n<p>To answer your question, the module you can drag to your canvas in order to make the features into categories; is the Edit Metadata module where you select the columns you want and change the \u201cunchanged\u201d into \u201cMake categorical\u201d within the Categorical-properties pane like in the image below:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/2NDht.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2NDht.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>You can also use the same module to make better sense from your columns by giving them a different column name. SibSp means SiblingSpouse like I have renamed it to in the image below:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Gm9Rr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Gm9Rr.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>And at last you can assign the targeted value (survived) and make the field into a label for ease of use.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/LyN0j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LyN0j.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":8.0,
        "Solution_readability":14.3,
        "Solution_reading_time":19.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":158.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1611841996688,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":21.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":0.3887183334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've found an almost identical question <a href=\"https:\/\/stackoverflow.com\/questions\/63727235\/mlflow-artifacts-storing-artifactsgoogle-cloud-storage-but-not-displaying-them?newreg=923da08a362547daab64c7d7e2275423\">here<\/a> but don't have enough reputation to add comments so will ask again hoping that someone has found a solution in the mean time.<\/p>\n<p>I am using MLflow (1.13.1) to track model performance and GCP Storage to store model artifacts.\nMLflow is running on a GCP VM instance and my python application uses a service account with Storage Object Creator and Storage Object Viewer roles (and then I've also added storage.buckets.get permissions) to store artifacts in GCP buckets and read from them.\nEverything is working as expected with parameters and metrics correctly displaying in MLflow UI and model artifacts correctly stored in buckets. The problem is that the model artifacts do not show up in MLflow UI because of this error:<\/p>\n<pre><code>Unable to list artifacts stored under gs:\/******\/artifacts for the current run. \nPlease contact your tracking server administrator to notify them of this error, \nwhich can happen when the tracking server lacks permission to list artifacts under the current run's root artifact directory.\n<\/code><\/pre>\n<p>The quoted artifacts location exists and contains the correct model artifacts, and MLflow should be able to read the artifacts because of the Storage Object Viewer role and the storage.buckets.get permissions.<\/p>\n<p>Any suggestion on what could be wrong? Thank you.<\/p>",
        "Challenge_closed_time":1611845294603,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611843895217,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with MLflow where it is not able to read the model artifacts stored on GCP buckets, even though the artifacts location exists and the user has the necessary permissions. The error message suggests that the tracking server lacks permission to list artifacts under the current run's root artifact directory. The user is seeking suggestions to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65939058",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.7,
        "Challenge_reading_time":20.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.3887183334,
        "Challenge_title":"MLflow stores artifacts on GCP buckets but is not able to read them",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":428.0,
        "Challenge_word_count":223,
        "Platform":"Stack Overflow",
        "Poster_created_time":1611841996688,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":21.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>I've found the problem just after posting the question.\nI had forgotten to install the <code>google-cloud-storage<\/code> library on the GCP VM. Everything works as expected now.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.9,
        "Solution_reading_time":2.34,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":26.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.1744444444,
        "Challenge_answer_count":0,
        "Challenge_body":"I tried to translate with the following command line and trace.\r\nThe command is meant to run locally, but there is an error about ClearML credentials. The ClearML argument was not set in the command line.\r\n\r\n```\r\npython -m silnlp.nmt.translate --checkpoint 6000 --src-project GELA3_2021_11_22 --book OT --trg-iso en  nlg-en-4\r\n2021-11-22 12:53:27.859063: I tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-11-22 12:53:30,996 - silnlp.common.environment - INFO - Using workspace: \/home\/david\/Gutenberg_new as per environment variable SIL_NLP_DATA_PATH.\r\n2021-11-22 12:53:31,372 - silnlp.common.utils - INFO - Git commit: 12aca87cab\r\nTraceback (most recent call last):\r\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 181, in <module>\r\n    main()\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 169, in main\r\n    translator.translate_book(\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 82, in translate_book\r\n    self.init_translation_task(experiment_suffix=f\"_{self.checkpoint}_{book}\")\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 57, in init_translation_task\r\n    self.clearml = SILClearML(\r\n  File \"<string>\", line 8, in __init__\r\n  File \"\/home\/david\/silnlp\/silnlp\/common\/clearml.py\", line 27, in __post_init__\r\n    self.task = Task.init(\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/task.py\", line 491, in init\r\n    task = cls._create_dev_task(\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/task.py\", line 2554, in _create_dev_task\r\n    task = cls(\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/task.py\", line 164, in __init__\r\n    super(Task, self).__init__(**kwargs)\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_interface\/task\/task.py\", line 151, in __init__\r\n    super(Task, self).__init__(id=task_id, session=session, log=log)\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_interface\/base.py\", line 131, in __init__\r\n    super(IdObjectBase, self).__init__(session, log, **kwargs)\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_interface\/base.py\", line 34, in __init__\r\n    self._session = session or self._get_default_session()\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_interface\/base.py\", line 101, in _get_default_session\r\n    InterfaceBase._default_session = Session(\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_api\/session\/session.py\", line 198, in __init__\r\n    self.refresh_token()\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_api\/session\/token_manager.py\", line 104, in refresh_token\r\n    self._set_token(self._do_refresh_token(self.__token, exp=self.req_token_expiration_sec))\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_api\/session\/session.py\", line 713, in _do_refresh_token\r\n    six.reraise(*sys.exc_info())\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/six.py\", line 703, in reraise\r\n    raise value\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_api\/session\/session.py\", line 699, in _do_refresh_token\r\n    raise LoginError(\r\nclearml.backend_api.session.session.LoginError: Failed getting token (error 401 from https:\/\/api.pro.clear.ml): Unauthorized (invalid credentials) (failed to locate provided credentials)\r\ndavid@pop-os:~\/silnlp$ \r\n```\r\n\r\n\r\n",
        "Challenge_closed_time":1637601038000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1637586010000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to create a comet logger when using pytorch lightning cli. The error message shows that the `self._kwargs` has an unexpected keyword argument 'agg_key_funcs'.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/sillsdev\/silnlp\/issues\/109",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":18.5,
        "Challenge_reading_time":56.44,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":147.0,
        "Challenge_repo_star_count":19.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":48,
        "Challenge_solved_time":4.1744444444,
        "Challenge_title":"Translate is trying to use ClearML even though it was not requested. Preventing translation on local machine.",
        "Challenge_topic":"Epoch Logging",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":275,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@davidbaines, Did that fix it? Yes! Thanks so much.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-1.0,
        "Solution_reading_time":0.63,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":9.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1254957460063,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"North Carolina, USA",
        "Answerer_reputation_count":2484.0,
        "Answerer_view_count":362.0,
        "Challenge_adjusted_solved_time":21.4263258333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>My \"experiment\" is like this,<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/AgnGE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AgnGE.png\" alt=\"Experiment\"><\/a><\/p>\n\n<p>I have 10 rows (excluding header) in \"Dataset.csv\" and 3 rows (excluding header) in the CSV being imported by <em>Import Data<\/em>. The schema of both CSVs is same. I want <em>Add Rows<\/em> to <strong>append<\/strong> the 3 rows to Dataset.csv.<\/p>\n\n<p>The real \"Dataset.csv\" has more than 25,000 rows and is expected to grow. Hence, using <em>Export Data<\/em> to generate a merged dataset (as a new CSV) is not a feasible solution. Any way to implement <strong>append<\/strong> for this scenario?<\/p>\n\n<p>Thanks<\/p>\n\n<p>Update 1:\nDataset.csv is present in ML Studios <em>Dataset<\/em>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/LBimY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LBimY.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":1535538465923,
        "Challenge_comment_count":8,
        "Challenge_created_time":1535452910717,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to append 3 rows to a dataset in Azure Machine Learning Studio, but the dataset already has more than 25,000 rows and exporting the data to generate a merged dataset is not feasible. The user is looking for a way to implement append for this scenario.",
        "Challenge_last_edit_time":1535461331150,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52055933",
        "Challenge_link_count":4,
        "Challenge_participation_count":9,
        "Challenge_readability":8.5,
        "Challenge_reading_time":12.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":23.765335,
        "Challenge_title":"Azure Machine Learning Studio append rows to dataset",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":590.0,
        "Challenge_word_count":115,
        "Platform":"Stack Overflow",
        "Poster_created_time":1409841727700,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":805.0,
        "Poster_view_count":137.0,
        "Solution_body":"<p>So it turns out the <a href=\"https:\/\/github.com\/Azure\/Azure-MachineLearning-ClientLibrary-Python\" rel=\"nofollow noreferrer\">Python SDK<\/a> has an <code>update_from_dataframe<\/code> method on it that can be used to update a dataset that has been uploaded to Azure ML Studio. If you're unable to use a new CSV and need to update an existing data set, then this should do the trick.<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.1,
        "Solution_reading_time":4.89,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1428951492492,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":1261.0,
        "Answerer_view_count":111.0,
        "Challenge_adjusted_solved_time":20.6618036111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Despite prominent how-to posts on how to add datasets to Azure Machine Learning that say Excel is supported, when I actually go to add a dataset and select a local Excel file, there's no option for \"Excel\" in the required datatype property dropdown. I'm surprised that Azure wouldn't support Excel (right?) - am I missing something?<\/p>",
        "Challenge_closed_time":1476307260447,
        "Challenge_comment_count":0,
        "Challenge_created_time":1476303145067,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble adding a dataset from a local Excel file to Azure Machine Learning Studio. Despite reading that Excel is supported, there is no option for \"Excel\" in the required datatype property dropdown. The user is unsure if they are missing something or if Azure does not support Excel.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40007515",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":5.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.1431611111,
        "Challenge_title":"Azure Machine Learning Studio: how to add a dataset from a local Excel file?",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1250.0,
        "Challenge_word_count":68,
        "Platform":"Stack Overflow",
        "Poster_created_time":1357592818807,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Ann Arbor, MI",
        "Poster_reputation_count":99.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>The dropdown list indicates the \"Destination\" datatype for the new DATASET file you are creating, not the source type.<\/p>\n\n<p>I just uploaded a <code>.xlsx<\/code> file successfully into a <code>.CSV<\/code> file in AML.<\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1476377527560,
        "Solution_link_count":0.0,
        "Solution_readability":5.2,
        "Solution_reading_time":2.85,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":32.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1365701399963,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"California, USA",
        "Answerer_reputation_count":1279.0,
        "Answerer_view_count":146.0,
        "Challenge_adjusted_solved_time":0.4401530556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've uploaded my <code>csv<\/code> file on Azure, but for some reason it became like this<\/p>\n\n<pre><code> nominal;data;curs;cdx         Column 1\n0          1;21.06.2000;28  2300;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n1          1;22.06.2000;28  2200;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n2          1;23.06.2000;28  1900;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n3          1;24.06.2000;28  1700;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n4          1;27.06.2000;28  1300;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n5          1;28.06.2000;28  1100;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n<\/code><\/pre>\n\n<p>Basically instead of four columns <code>nominal<\/code>, <code>data<\/code>, <code>curs<\/code>, <code>cdx<\/code> I got two columns with one having all the values and the last one (it is empty or something because the last column has encoding issue) - no idea what.<\/p>\n\n<p>I have deleted the column <code>Column 1<\/code> like this<\/p>\n\n<pre><code>import pandas as pd\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    dataframe1.drop(['Column 1'], axis = 1, inplace = True)\n    print('Input pandas.DataFrame #1:\\r\\n\\r\\n{0}'.format(dataframe1))\n    return dataframe1,\n<\/code><\/pre>\n\n<p>How to split the first column into multiple now? To get 4 separate columns<\/p>\n\n<p>I am using pandas 0.18<\/p>",
        "Challenge_closed_time":1532031297043,
        "Challenge_comment_count":9,
        "Challenge_created_time":1532029428013,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user uploaded a CSV file on Azure, but the column headers and values got merged into two columns instead of four. The user deleted the extra column but now needs to split the first column into four separate columns using pandas 0.18.",
        "Challenge_last_edit_time":1532029712492,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51430645",
        "Challenge_link_count":0,
        "Challenge_participation_count":10,
        "Challenge_readability":7.8,
        "Challenge_reading_time":15.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.519175,
        "Challenge_title":"split dataframe column header and values into multiple columns",
        "Challenge_topic":"Tabular Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1086.0,
        "Challenge_word_count":139,
        "Platform":"Stack Overflow",
        "Poster_created_time":1338678668792,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Eindhoven, Netherlands",
        "Poster_reputation_count":11479.0,
        "Poster_view_count":887.0,
        "Solution_body":"<p>You need to split the column with:<\/p>\n\n<pre><code>dataframe1['nominal;data;curs;cdx'].str.split(';',expand=True)\n<\/code><\/pre>\n\n<p>Then change the headers with:<\/p>\n\n<pre><code>dataframe1.columns = 'nominal;data;curs;cdx'.split(';')\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.0,
        "Solution_reading_time":3.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.7277777778,
        "Challenge_answer_count":0,
        "Challenge_body":"### Describe the bug a clear and concise description of what the bug is.\n\nFirst of all, thanks to everyone creating this Helm Chart as it is really good and easy to use.\r\n\r\nHowever, I encountered a problem when choosing to include ServiceMonitor and Prometheus metrics along the Deployment. Generally, the created ServiceMonitor for MLFlow is correct, yet in the current form it does not work for me.\r\nI use the latest Prometheus deployed using the official Helm Chart and the MLFlow metrics did not show up in the Targets, yet it was visible in Service Discovery panel in Prometheus Dashboard, but appeared as `0\/1 active targets`.\r\n\r\nAfter a couple of hours of educated debugging I changed manually the `targetPort: 80` to `port: http` in the deployed ServiceMonitor manifest. It worked straightaway! \r\n\r\n\r\nWhat I propose is a simple fix:\r\nAccording to official Prometheus Troubleshooting docs the port specified in ServiceMonitor should use `name` instead of port number ([Link to docs](https:\/\/github.com\/prometheus-operator\/prometheus-operator\/blob\/main\/Documentation\/troubleshooting.md#using-textual-port-number-instead-of-port-name)) \r\nSimple fix would be to change `targetPort: 80` to `port: http` in `templates\/servicemonitor.yaml`. Port name `http` is already hardcoded, so can be used directly or new parameter could be introduced to give the freedom to choose port name.\r\nI am aware that port number of type Integer should also work...\r\n\n\n### What's your helm version?\n\n3.6.0\n\n### What's your kubectl version?\n\n1.19\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.2.21\n\n### What happened?\n\n_No response_\n\n### What you expected to happen?\n\n_No response_\n\n### How to reproduce it?\n\n_No response_\n\n### Enter the changed values of values.yaml?\n\n_No response_\n\n### Enter the command that you execute and failing\/misfunctioning.\n\n helm install --namespace mlflow mlflow-tracking-server community-charts\/mlflow --set serviceMonitor.enabled=true\n\n### Anything else we need to know?\n\n_No response_",
        "Challenge_closed_time":1658854769000,
        "Challenge_comment_count":4,
        "Challenge_created_time":1658844949000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is requesting an improvement in mlflow logging for population by having separate graphs for each individual's performance and sub runs on mlflow.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/22",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":9.3,
        "Challenge_reading_time":25.56,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":8.0,
        "Challenge_repo_issue_count":39.0,
        "Challenge_repo_star_count":9.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":2.7277777778,
        "Challenge_title":"[mlflow] Use port name instead of port number in ServiceMonitor ",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":286,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @mikwieczorek \r\n\r\nThanks to inform us. Yes, probably it's my mistake. I changed it to the name some time ago. Let's write some tests and fix the problem. Well, it looks like your link refers to the port (service port) rather than targetPort (pod's port. This is currently what we use.). But we can even make it optional (port or targetPort selection) and use a name rather than a port number. It looks like it works with the latest version of Prometheus but I think we need to support all versions together.\r\n\r\nAnd [this is the full schema of endpoints field](https:\/\/github.com\/prometheus-operator\/prometheus-operator\/blob\/main\/Documentation\/api.md#monitoring.coreos.com\/v1.Endpoint).\r\n\r\nI will do some additional manual tests and send the PR. Hi @mikwieczorek \r\n\r\nChart version 0.3.0 should solve your problem. If it still accrues, feel free to reopen this issue. You can use the following command to update your deployment without the need for additional changes.\r\n\r\n```\r\nhelm repo update\r\nhelm upgrade --install --namespace mlflow mlflow-tracking-server community-charts\/mlflow --set serviceMonitor.enabled=true\r\n```\r\n\r\nBest,\r\nBurak Thank you @burakince for your prompt fix. It works correctly after the update. \r\nNext time, I will make an MR instead of just reporting the issue",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.3,
        "Solution_reading_time":15.73,
        "Solution_score_count":null,
        "Solution_sentence_count":18.0,
        "Solution_word_count":187.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1443426419048,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation_count":842.0,
        "Answerer_view_count":219.0,
        "Challenge_adjusted_solved_time":133.5554527778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Azure ML support says to me that delimiter must be comma, this would cause too much hassle with data having semicolon as separator and with a lot of commas in the cell values. <\/p>\n\n<p>So how to process semicolon separated CSV files in Azure ML? <\/p>",
        "Challenge_closed_time":1485838771543,
        "Challenge_comment_count":1,
        "Challenge_created_time":1485357971913,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in processing semicolon separated CSV files in Azure ML as the platform only supports comma as a delimiter. The user is seeking a solution to this problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41855344",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":3.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":133.5554527778,
        "Challenge_title":"Azure ML: How to save and process CSV files with semicolon as delimiter?",
        "Challenge_topic":"Bucket File Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2908.0,
        "Challenge_word_count":58,
        "Platform":"Stack Overflow",
        "Poster_created_time":1251372839052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":48616.0,
        "Poster_view_count":3348.0,
        "Solution_body":"<p>Azure ML only accepts the comma <code>,<\/code> separated CSV. Do a little work around.\nOpen your data file using a text editor. (Notepad will do the trick). Find and replace all semicolons with 'tab' (Make it a TSV) and the commas in data values may not occur a problem then. Make sure to define that the input is a TSV; not a CSV. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.9,
        "Solution_reading_time":4.05,
        "Solution_score_count":7.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1602690898272,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Mumbai, India",
        "Answerer_reputation_count":153.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":318.0961611111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have directory in AzureML notebook in which I have 300k files and need to list their names.\nApproach below works but takes 1.5h to execute:<\/p>\n<pre><code>from os import listdir\nfrom os.path import isfile, join\nmypath = &quot;.\/temp\/&quot;\ndocsOnDisk = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n<\/code><\/pre>\n<p>What is the azure way to quickly list those files? (both notebook and this directory is in FileShare).<\/p>\n<p>I am also aware that the approach below will give some gain, but still it is not the azure way to do this.<\/p>\n<pre><code>docsOnDisk = [f.name for f in scandir(mypath) ] # shall be 2-20x faster\n<\/code><\/pre>",
        "Challenge_closed_time":1648651666183,
        "Challenge_comment_count":3,
        "Challenge_created_time":1647505772703,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has a directory in AzureML notebook with 300k files and needs to list their names. The current approach takes 1.5 hours to execute. The user is looking for a faster way to list the files and is asking for the Azure way to do it.",
        "Challenge_last_edit_time":1647506520003,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71509160",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":5.0,
        "Challenge_reading_time":8.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":318.3037444445,
        "Challenge_title":"AzureML list huge amount of files",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":102.0,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1567151674136,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Krak\u00f3w, Poland",
        "Poster_reputation_count":304.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>Try using glob module and filter method instead of list comprehension.<\/p>\n<pre><code>import glob\nfrom os.path import isfile\nmypath = &quot;.\/temp\/*&quot;\ndocsOnDisk = glob.glob(mypath)\nverified_docsOnDisk = list(filter(lambda x:isfile(x), docsOnDisk))\n<\/code><\/pre>\n<p>glob should give only existing files. Its not needed to verify them by using isfile(). But still if you need to try it out then you can use filter method instead of list comprehension. To skip verification, you can comment last line.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.8,
        "Solution_reading_time":6.46,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":68.0,
        "Tool":"Azure Machine Learning"
    }
]