[
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":141.7067575,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>With an <code>sagemaker.estimator.Estimator<\/code>, I want to re-<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html#sagemaker.estimator.EstimatorBase.deploy\" rel=\"nofollow noreferrer\">deploy<\/a> a model after retraining (calling <code>fit<\/code> with new data).<\/p>\n<p>When I call this<\/p>\n<pre><code>estimator.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')\n<\/code><\/pre>\n<p>I get an error<\/p>\n<pre><code>botocore.exceptions.ClientError: An error occurred (ValidationException) \nwhen calling the CreateEndpoint operation: \nCannot create already existing endpoint &quot;arn:aws:sagemaker:eu-east- \n1:1776401913911:endpoint\/zyx&quot;.\n<\/code><\/pre>\n<p>Apparently I want to use functionality like <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_UpdateEndpoint.html\" rel=\"nofollow noreferrer\">UpdateEndpoint<\/a>. How do I access that functionality from this API?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601630651783,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64169189",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":20.5,
        "Challenge_reading_time":13.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I deploy a re-trained Sagemaker model to an endpoint?",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1958.0,
        "Challenge_word_count":78,
        "Platform":"Stack Overflow",
        "Poster_created_time":1227171471292,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Israel",
        "Poster_reputation_count":17500.0,
        "Poster_view_count":1561.0,
        "Solution_body":"<p>Yes, under the hood the <code>model.deploy<\/code> creates a model, an endpoint configuration and an endpoint. When you call again the method from an already-deployed, trained estimator it will create an error because a similarly-configured endpoint is already deployed. What I encourage you to try:<\/p>\n<ul>\n<li><p>use the <code>update_endpoint=True<\/code> parameter. From the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\" rel=\"noreferrer\">SageMaker SDK doc<\/a>:\n<em>&quot;Additionally, it is possible to deploy a different endpoint configuration, which links to your model, to an already existing\nSageMaker endpoint. This can be done by specifying the existing\nendpoint name for the <code>endpoint_name<\/code> parameter along with the\n<code>update_endpoint<\/code> parameter as True within your <code>deploy()<\/code> call.&quot;<\/em><\/p>\n<\/li>\n<li><p>Alternatively, if you want to create a separate model you can specify a new <code>model_name<\/code> in your <code>deploy<\/code><\/p>\n<\/li>\n<\/ul>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1602140796110,
        "Solution_link_count":1,
        "Solution_readability":14.0,
        "Solution_reading_time":13.16,
        "Solution_score_count":6.0,
        "Solution_sentence_count":8,
        "Solution_word_count":123,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi\nCouldn't find answers in the documentation for the following questions when selling a model package on the AWS Marketplace:\n\n1. Pricing: Can we offer only private offers? (completely disable the hourly and per inference pricing)\n\n2. Autoscaling: Is it possible to define an autoscaling policy for a hosted endpoint that runs a model package?\n\n3. Parameters: What's the interface for making an inference call? Can we pass any parameters to the inference endpoint?\n\n4. S3: Can we use S3 to load additional dependencies?\n\nThank you very much!",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1673190912248,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1673538025192,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUghZxipLKTSSn45EOV-S_Yg\/sagemaker-on-aws-marketplace-autoscaling-parameters-pricing-and-s3",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.6,
        "Challenge_reading_time":7.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker on AWS Marketplace - autoscaling, parameters, pricing and S3",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":60.0,
        "Challenge_word_count":96,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi,\n\n1. In general, AWS Marketplace uses a pay-as-you-go pricing model, which means that customers are charged for the resources they consume on an hourly or per-inference basis. I'm not aware of any way to disable this pricing model when selling a model package on AWS Marketplace. However, it's worth noting that AWS Marketplace also offers private listings, which allow you to sell your model package directly to a specific customer or group of customers. Private offers are not discoverable by other customers and are not subject to the same pricing and billing terms as public listings. You may want to consider using a private listing if you want to offer a different pricing model for your model package. Reference: https:\/\/docs.aws.amazon.com\/marketplace\/latest\/buyerguide\/buyer-private-offers.html\n\n2. Yes, it is possible to define an auto scaling policy for a hosted Amazon SageMaker endpoint that runs a model package. To define an auto scaling policy for a SageMaker endpoint, you can use the *UpdateEndpoint* API or the SageMaker console. When updating an endpoint, you can specify the desired number of instances and the minimum and maximum number of instances for the auto scaling policy. SageMaker will automatically scale the number of instances up or down based on the incoming traffic and the defined policy.\n\nHere's an example of how you can use the *UpdateEndpoint* API to update an endpoint with an auto scaling policy:\n\n```\nimport boto3\n\nsm = boto3.client('sagemaker')\n\nresponse = sm.update_endpoint(\n    EndpointName='your-endpoint-name',\n    DesiredInferenceUnits=1,\n    MinInferenceUnits=1,\n    MaxInferenceUnits=8\n)\n\n```\nMore details: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_UpdateEndpoint.html\n\n3. To make an inference call to a hosted Amazon SageMaker endpoint, you can use the *invoke_endpoint* method of the SageMaker runtime client. This method allows you to send an HTTP POST request to the endpoint and receive the prediction results in the response.\n\nHere's an example of how you can use the *invoke_endpoint* method to make an inference request:\n\n```\nimport boto3\n\nsm = boto3.client('sagemaker-runtime')\n\nresponse = sm.invoke_endpoint(\n    EndpointName='your-endpoint-name',\n    Body=b'your-request-data',\n    ContentType='application\/json'\n)\nprediction = response['Body'].read()\n\n```\nYou can pass any parameters that your model expects in the request body. The format of the request data and the expected parameters depend on the specific model that you are using. For example, if your model expects a JSON object with a single field called \"input\", you can pass the input data as a JSON string in the request body.\n\nMore details: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_runtime_InvokeEndpoint.html\n\n4. Yes, you can use S3 to store additional dependencies for your ML model and load them into Amazon SageMaker. SageMaker allows you to specify additional code and libraries to be included in your training or inference environment by using the CodeRepository parameter of the CreateTrainingJob or CreateEndpoint API. The CodeRepository parameter should be set to the Amazon S3 URI of a Git repository that contains the code and dependencies you want to include. SageMaker will clone the repository and build the code as part of the training or inference environment.\n\nHere's an example of how you can use the CodeRepository parameter to specify an S3-based Git repository in a CreateTrainingJob request:\n\n```\nimport boto3\n\nsm = boto3.client('sagemaker')\n\nresponse = sm.create_training_job(\n    TrainingJobName='your-training-job-name',\n    HyperParameters={...},\n    InputDataConfig=[{...}],\n    OutputDataConfig={...},\n    ResourceConfig={...},\n    RoleArn='your-role-arn',\n    CodeRepository='s3:\/\/your-bucket\/your-repository.git'\n)\n\n```\nMore details: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1673208081703,
        "Solution_link_count":4,
        "Solution_readability":13.6,
        "Solution_reading_time":48.81,
        "Solution_score_count":0.0,
        "Solution_sentence_count":29,
        "Solution_word_count":504,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1369787017728,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Atlanta, Georgia",
        "Answerer_reputation_count":55.0,
        "Answerer_view_count":49.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>This is what I use to deploy an Auto-ML model:<\/p>\n<pre><code>            MachineSpec machineSpec = MachineSpec.newBuilder().setMachineType(&quot;n1-standard-2&quot;).build();\n            DedicatedResources dedicatedResources =\n                    DedicatedResources.newBuilder().setMinReplicaCount(1).setMachineSpec(machineSpec).build();            \n            String model = ModelName.of(project, location, modelId).toString();\n            DeployedModel deployedModel =\n                    DeployedModel.newBuilder()\n                            .setModel(model)\n                            .setDisplayName(deployedModelDisplayName)\n                            .setDedicatedResources(dedicatedResources)\n                            .build();\n            Map&lt;String, Integer&gt; trafficSplit = new HashMap&lt;&gt;();\n            trafficSplit.put(&quot;0&quot;, 100);\n            EndpointName endpoint = EndpointName.of(project, location, endpointId);\n            OperationFuture&lt;DeployModelResponse, DeployModelOperationMetadata&gt; response =\n                    client.deployModelAsync(endpoint, deployedModel, trafficSplit);\n            response.getInitialFuture().get().getName());\n<\/code><\/pre>\n<p>The error appears when I hit this line <code>response.getInitialFuture().get().getName());<\/code><\/p>\n<p>Here is the error:\n<code>INVALID_ARGUMENT: 'dedicated_resources' is not supported for Model projects\/***\/locations\/us-central1\/models\/***<\/code><\/p>\n<p>I can deploy the model using cloud console but not programmatically using java 8. It is a new model and the endpoint is also new without any assigned model to it.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":8,
        "Challenge_created_time":1640406502527,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70477987",
        "Challenge_link_count":0,
        "Challenge_participation_count":9,
        "Challenge_readability":21.9,
        "Challenge_reading_time":18.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Vertex Ai issue when deploying a model using Java",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":244.0,
        "Challenge_word_count":109,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369787017728,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Atlanta, Georgia",
        "Poster_reputation_count":55.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>I am sorry everyone, I was implementing the wrong section of the documentation. I had to follow AutoML image, not Custom-trained one.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/a4xPR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/a4xPR.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":12.9,
        "Solution_reading_time":4.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5,
        "Solution_word_count":32,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1372408547912,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Farnborough, United Kingdom",
        "Answerer_reputation_count":7360.0,
        "Answerer_view_count":372.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running some code in AWS Lambda that dynamically creates SageMaker models.\nI am locking Sagemaker's API version like so:<\/p>\n\n<p><code>const sagemaker = new AWS.SageMaker({apiVersion: '2017-07-24'});<\/code><\/p>\n\n<p>And here's the code to create the model:<\/p>\n\n<pre><code>await sagemaker.createModel({\n        ExecutionRoleArn: 'xxxxxx',\n        ModelName: sageMakerConfigId,\n        Containers: [{\n            Image: ecrUrl\n        }]\n    }).promise()\n<\/code><\/pre>\n\n<p>This code runs just fine locally with <code>aws-sdk<\/code> on <code>2.418.0<\/code>. <\/p>\n\n<p>However, when this code is deployed to Lambda, it doesn't work due to some validation errors upon creating the model:<\/p>\n\n<blockquote>\n  <ul>\n  <li>MissingRequiredParameter: Missing required key 'PrimaryContainer' in params<\/li>\n  <li>UnexpectedParameter: Unexpected key 'Containers' found in params<\/li>\n  <\/ul>\n<\/blockquote>\n\n<p>Is anyone aware of existing bugs in the <code>aws-sdk<\/code> for NodeJS using the SDK provided by AWS in the Lambda context? I believe the SDK available inside AWS Lambda is more up-to-date than <code>2.418.0<\/code> but apparently there are compatibility issues.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1553074522987,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1553082788147,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55257580",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":14.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker NodeJS's SDK is not locking the API Version",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":277.0,
        "Challenge_word_count":142,
        "Platform":"Stack Overflow",
        "Poster_created_time":1548169897263,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":6496.0,
        "Poster_view_count":759.0,
        "Solution_body":"<p>As you've noticed the 'embedded' lambda version of the aws-sdk lags behind. It's actually on <code>2.290.0<\/code> (you can see the full details on the environment here: <a href=\"https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/current-supported-versions.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/current-supported-versions.html<\/a>)<\/p>\n\n<p>You can see here: <a href=\"https:\/\/github.com\/aws\/aws-sdk-js\/blame\/master\/clients\/sagemaker.d.ts\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/aws-sdk-js\/blame\/master\/clients\/sagemaker.d.ts<\/a> that it is not until <code>2.366.0<\/code> that the params for this method included <code>Containers<\/code> and did not require <code>PrimaryContainer<\/code>.<\/p>\n\n<p>As you've noted, the <em>workaround<\/em> is to deploy your lambda with the <code>aws-sdk<\/code> version that you're using. This is sometimes noted as a best practice, as it pins the <code>aws-sdk<\/code> on the functionality you've built and tested against.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_readability":14.1,
        "Solution_reading_time":13.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10,
        "Solution_word_count":95,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"I'm not exactly sure on how to interpret this and what to refine. The error I'm getting is in the azureml.PipelineStep automl step. Here is the [link](https:\/\/mlworkspace.azure.ai\/portal\/subscriptions\/ff2e23ae-7d7c-4cbd-99b8-116bb94dca6e\/resourceGroups\/RG-ITSMLTeam-Dev\/providers\/Microsoft.MachineLearningServices\/workspaces\/avadevitsmlsvc\/experiments\/deal-deal-nema\/runs\/7abe9617-ac79-413b-8843-7fd3878313f0).\r\n\r\nWhen my dataset has more than ~1200 features, I consistently get this error, but when there are fewer features it works fine. Is there some limitation here?",
        "Challenge_closed_time":1581034,
        "Challenge_comment_count":0,
        "Challenge_created_time":1566233711000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/534",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":10.1,
        "Challenge_reading_time":8.08,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Azureml Automl \"Error: Null\" Vague Error",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":59,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi Nema. Unfortunately I don't have access to your workspace. Would you provide more details on the failed experiment such as experiment\/pipeline id so that I can take a look at the logs of it? Hi Sonny, here is the run id: `eb6f111d-1251-40d2-b745-e3c4fbb31fcf` Thank you for the runid. I found automl setup has been timed out after some time. I will work with automl team for more details.  It seems to have been a one-off random occurrence. Considering solved. Somehow I lost track on this. You can let me know if you have any further issues.  ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":4.4,
        "Solution_reading_time":6.6,
        "Solution_score_count":null,
        "Solution_sentence_count":8,
        "Solution_word_count":96,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1577817693600,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am aware that Sagemaker <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-model-endpoints.html#multi-model-support\" rel=\"nofollow noreferrer\">does not support multi-model endpoints for their built-in image classification algorithm<\/a>. However, in the documentation they hint at building a custom container to use &quot;any other framework or algorithm&quot; with the multi-model endpoint functionality:<\/p>\n<blockquote>\n<p>To use any other framework or algorithm, use the SageMaker inference toolkit to build a container that supports multi-model endpoints. For information, see <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html\" rel=\"nofollow noreferrer\">Build Your Own Container with Multi Model Server<\/a>.<\/p>\n<\/blockquote>\n<p>Ideally, I would like to deploy many (20+) image classification models I have already trained to a single endpoint to save on costs. However, after reading the &quot;Build Your Own Container&quot; guide it is still not exactly clear to me how to build a custom inference container for the models produced by a non-custom algorithm. Most of the tutorials and example notebooks refer to using Pytorch or Sklearn. It is not clear to me that I could make inferences using these libraries on the models I've created with the built-in image classification algorithm.<\/p>\n<p><em>Is<\/em> it possible to create a container to support multi-model endpoints for unsupported built-in Sagemaker algorithms? If so, would somebody be able to hint at how this might be done?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611192699733,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65819978",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.8,
        "Challenge_reading_time":20.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker multi-model endpoints with unsupported built-in algorithms",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":524.0,
        "Challenge_word_count":207,
        "Platform":"Stack Overflow",
        "Poster_created_time":1611191329712,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>yes, it is possible to deploy the built in image classification models as a SageMaker multi model endpoint. The key is that the image classification uses <a href=\"https:\/\/mxnet.apache.org\/versions\/1.7.0\/\" rel=\"nofollow noreferrer\">Apache MXNet<\/a>. You can extract the model artifacts (SageMaker stores them in a zip file named model.tar.gz in S3), then load them in to MXNet. The SageMaker MXNet container supports multi model endpoints, so you can use that to deploy the model.<\/p>\n<p>If you unzip the model.tar.gz from this algorithm, you'll find three files:<\/p>\n<p>image-classification-****.params<\/p>\n<p>image-classification-symbol.json<\/p>\n<p>model-shapes.json<\/p>\n<p>The MxNet container expects these files to be named <strong>image-classification-0000.params, model-symbol.json, and model-shapes.json<\/strong>. So I unzipped the zip file, renamed the files and rezipped them. For more information on the MXNet container check out the <a href=\"https:\/\/github.com\/aws\/sagemaker-mxnet-inference-toolkit\" rel=\"nofollow noreferrer\">GitHub repository<\/a>.<\/p>\n<p>After that you can deploy the model as a single MXNet endpoint using the SageMaker SDK with the following code:<\/p>\n<pre><code>from sagemaker import get_execution_role\nfrom sagemaker.mxnet.model import MXNetModel\n\nrole = get_execution_role()\n\nmxnet_model = MXNetModel(model_data=s3_model, role=role, \n                         entry_point='built_in_image_classifier.py', \n                         framework_version='1.4.1',\n                         py_version='py3')\n\npredictor = mxnet_model.deploy(instance_type='ml.c4.xlarge', initial_instance_count=1)\n<\/code><\/pre>\n<p>The entry point Python script can be an empty Python file for now. We will be using the default inference handling provided by the MXNet container.<\/p>\n<p>The default MXNet container only accepts JSON, CSV, and Numpy arrays as valid input. So you will have to format your input in to one of these three formats. The code below demonstrates how I did it with Numpy arrays:<\/p>\n<pre><code>import cv2\nimport io\n\nnp_array = cv2.imread(filename=img_filename)\nnp_array = np_array.transpose((2,0,1))\nnp_array = np.expand_dims(np_array, axis=0)\n\nbuffer = io.BytesIO()\nnp.save(buffer, np_array)\n\nresponse = sm.invoke_endpoint(EndpointName='Your_Endpoint_name', Body=buffer.getvalue(), ContentType='application\/x-npy')\n<\/code><\/pre>\n<p>Once you have a single endpoint working with MXNet container, you should be able to get it running in multi model endpoint using the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/multi_data_model.html\" rel=\"nofollow noreferrer\">SageMaker MultiDataModel constructor<\/a>.<\/p>\n<p>If you want to use a different input data type so you don't have to do the preprocessing in your application code, you can overwrite the input_fn method in the MxNet container by providing it in the entry_point script. <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/mxnet\/using_mxnet.html\" rel=\"nofollow noreferrer\">See here<\/a> for more information. If you do this, you could pass the image bytes directly to SageMaker, without formatting the numpy arrays.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_readability":12.5,
        "Solution_reading_time":39.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":30,
        "Solution_word_count":347,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>similar question to\n<a href=\"https:\/\/stackoverflow.com\/a\/66683538\/6896705\">AWS Lambda send image file to Amazon Sagemaker<\/a><\/p>\n<p>I try to make simple-mnist work (the model was built by referring to <a href=\"https:\/\/sagemaker-immersionday.workshop.aws\/en\/lab3\/option1.html\" rel=\"nofollow noreferrer\">aws tutorial<\/a>)<\/p>\n<p>Then I am using API gateway (REST API w\/ proxy integration) to post image data to lambda, and would like to send it to sagemaker endpoint and make an inference.<\/p>\n<p>In lambda function, I wrote the code(.py) like this.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>runtime = boto3.Session().client('sagemaker-runtime')\n\nendpoint_name = 'tensorflow-training-YYYY-mm-dd-...'\nres = runtime.invoke_endpoint(EndpointName=endpoint_name,\n                              Body=Image,\n                              ContentType='image\/jpeg',\n                              Accept='image\/jpeg')\n<\/code><\/pre>\n<p>However, when I send image to lambda via API gateway, this error occurs.<\/p>\n<blockquote>\n<p>[ERROR] ModelError: An error occurred (ModelError) when calling the\nInvokeEndpoint operation: Received client error (415) from model with\nmessage &quot; {\n&quot;error&quot;: &quot;Unsupported Media Type: image\/jpeg&quot; }<\/p>\n<\/blockquote>\n<p>I think I need to do something referring to <a href=\"https:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/api-gateway-payload-encodings.html\" rel=\"nofollow noreferrer\">Working with binary media types for REST APIs\n<\/a><\/p>\n<p>But since I am very new, I have no idea about the appropriate thing to do, on which page (maybe API Gateway page?) or how...<\/p>\n<p>I need some clues to solve this problem. Thank you in advance.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1626929018517,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1626941375923,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68479297",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":12.6,
        "Challenge_reading_time":21.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS send image to Sagemaker from Lambda: how to set content handling?",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":390.0,
        "Challenge_word_count":191,
        "Platform":"Stack Overflow",
        "Poster_created_time":1475109151950,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":43.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>Looking <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/adapt-inference-container.html\" rel=\"nofollow noreferrer\">here<\/a> you can see that only some specific content types are supported by default, and images are not in this list. I think you have to either implement your <code>input_fn<\/code> function or adapt your data to one of the supported content types.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":12.8,
        "Solution_reading_time":4.89,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3,
        "Solution_word_count":46,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have some data in S3 and I want to create a lambda function to predict the output with my deployed aws sagemaker endpoint then I put the outputs in S3 again. Is it necessary in this case to create an api gateway like decribed in this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">link<\/a> ? and in the lambda function what I have to put. I expect to put (where to find the data, how to invoke the endpoint, where to put the data) <\/p>\n\n<p>Thanks<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1549472411217,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54558832",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":7.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"call sagemaker endpoint using lambda function",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":5259.0,
        "Challenge_word_count":87,
        "Platform":"Stack Overflow",
        "Poster_created_time":1502010899808,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tunis, Tunisia",
        "Poster_reputation_count":109.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>you definitely don't have to create an API in API Gateway. You can invoke the endpoint directly using the invoke_endpoint() API, passing the endpoint name, the content type, and the payload.<\/p>\n\n<p>For example:<\/p>\n\n<pre><code>import boto3\n\nendpoint_name = &lt;INSERT_ENDPOINT_NAME&gt;\nruntime = boto3.Session().client(service_name='sagemaker-runtime',region_name='us-east-1')\n\nresponse = runtime.invoke_endpoint(EndpointName=endpoint_name, ContentType='application\/x-image', Body=payload)\nprint(response['Body'].read())\n<\/code><\/pre>\n\n<p>More examples here using a Lambda function: <a href=\"https:\/\/medium.com\/@julsimon\/using-chalice-to-serve-sagemaker-predictions-a2015c02b033\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/@julsimon\/using-chalice-to-serve-sagemaker-predictions-a2015c02b033<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":20.6,
        "Solution_reading_time":10.89,
        "Solution_score_count":4.0,
        "Solution_sentence_count":7,
        "Solution_word_count":56,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1360655430743,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Belgium",
        "Answerer_reputation_count":2947.0,
        "Answerer_view_count":355.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to call an AzureML UDF from Stream Analytics query and that UDF expects an array of 5 rows and 2 columns.  The input data is streamed from an IoT hub and we have two fields in the incoming messages: temperature &amp; humidity.<\/p>\n<p>This would be the 'passthrough query' :<\/p>\n<pre><code>SELECT GetMetadataPropertyValue([room-telemetry], 'IoTHub.ConnectionDeviceId') AS RoomId, \n       Temperature, Humidity\nINTO\n    [maintenance-alerts]\nFROM\n    [room-telemetry]\n<\/code><\/pre>\n<p>I have an AzureML UDF (successfully created) that should be called with the last 5 records per RoomId and that will return one value from the ML Model.  Obviously, there are multiple rooms in my stream, so I need to find a way to get some kind of windowing of 5 records Grouped per RoomId.  I don't seem to find a way to call the UDF with the right arrays selected from the input stream.  I know I can create a Javascript UDF that would return an array from the specific fields, but that would be record\/by record, where here I would need this with multiple records that are grouped by the RoomId.<\/p>\n<p>Someone has any insights?<\/p>\n<p>Best regards<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":4,
        "Challenge_created_time":1596178511590,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63187116",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":9.8,
        "Challenge_reading_time":15.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Call Azure Stream Analytics UDF with multi-dimensional array of last 5 records, grouped by record",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":374.0,
        "Challenge_word_count":197,
        "Platform":"Stack Overflow",
        "Poster_created_time":1360655430743,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belgium",
        "Poster_reputation_count":2947.0,
        "Poster_view_count":355.0,
        "Solution_body":"<p>After the good suggestion of @jean-s\u00e9bastien and an answer to an isolated question for the <a href=\"https:\/\/stackoverflow.com\/questions\/63357901\/how-to-convert-a-dictionary-like-structure-in-azure-stream-analytics-to-a-mult\/63373103#63373103\">array-parsing<\/a>, I finally was able to stitch everything together in a solution that builds.  (still have to get it to run at runtime, though).<\/p>\n<p>So, the solution exists in using <code>CollectTop<\/code> to aggregate the latest rows of the entity you want to group by, including the specification of a Time Window.<\/p>\n<p>And the next step was to create the javascript UDF to take that data structure and parse it into a multi-dimensional array.<\/p>\n<p>This is the query I have right now:<\/p>\n<pre class=\"lang-sql prettyprint-override\"><code>-- Taking relevant fields from the input stream\nWITH RelevantTelemetry AS\n(\n    SELECT  engineid, tmp, hum, eventtime\n    FROM    [engine-telemetry] \n    WHERE   engineid IS NOT NULL\n),\n-- Grouping by engineid in TimeWindows\nTimeWindows AS\n(\n    SELECT engineid, \n        CollectTop(2) OVER (ORDER BY eventtime DESC) as TimeWindow\n    FROM\n        [RelevantTelemetry]\n    WHERE engineid IS NOT NULL\n    GROUP BY TumblingWindow(hour, 24), engineid\n)\n--Output timewindows for verification purposes\nSELECT engineid, Udf.Predict(Udf.getTimeWindows(TimeWindow)) as Prediction\nINTO debug\nFROM TimeWindows\n<\/code><\/pre>\n<p>And this is the Javascript UDF:<\/p>\n<pre class=\"lang-js prettyprint-override\"><code>    function getTimeWindows(input){\n        var output = [];\n        for(var x in input){\n            var array = [];\n            array.push(input[x].value.tmp);\n            array.push(input[x].value.hum);\n            output.push(array);\n        }\n        return output;\n    }\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":16.2,
        "Solution_reading_time":21.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8,
        "Solution_word_count":191,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am new to AWS infra and currently doing some POC\/Feasibility for new work.<\/p>\n\n<p>So I have created a S3 bucket in Ireland server, train and publish Sagemaker endpoint in Ireland server and its giving result in Jupyter notebook there. Now I want to use that endpoint in my browser javascript library to show some graphics. When I try to test my endpoint in Postman then its giving region specific error <\/p>\n\n<pre><code> {\n        \"message\": \"Credential should be scoped to a valid region, not 'us-east-1'. \nCredential should be scoped to correct service: 'sagemaker'. \"\n }\n<\/code><\/pre>\n\n<p>My AWS account is not yet enterprise managed so I am using as 'root user', Whenever I go to my profile>Security_Credential page and generate any security credential then it always create for 'us-east-1' region, As Sagemaker is region specific service, I am not able to find the way to create region specific security key for root user, can someone please help<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526107432860,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50303607",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":12.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS Sagemaker | region specific security credentials for endpoint",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":750.0,
        "Challenge_word_count":161,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501403168107,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":1370.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>You should create an IAM role first that defines what should be permitted (mainly calling the invoke-endpoint API call for SageMaker runtime). Then you should create an IAM user, add the above role to that user, and then generate credentials that you can use in your Postman to call the service. Here you can find some details about the IAM role for SageMaker that you can use in this process: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/using-identity-based-policies.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/using-identity-based-policies.html<\/a><\/p>\n\n<p>A popular option to achieve external access to a SageMaker endpoint, is to create an API Gateway that calls a Lambda Function that is then calling the invoke-endpoint API. This chain is giving you various options such as different authentication options for the users and API keys as part of API-GW, processing the user input and inference output using API-GW and Lambda code, and giving the permission to call the SageMaker endpoint to the Lambda function. This chain removes the need for the credentials creation, update and distribution.  <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":13.6,
        "Solution_reading_time":14.59,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7,
        "Solution_word_count":163,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am facing error when I deploy in ACI. Is there a way to deploy the models when AMLS and  vnet are in different resource groups?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1601173833227,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/108659\/from-amls-deploying-models-in-aci-in-a-vnet",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.3,
        "Challenge_reading_time":2.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"From AMLS Deploying  models in ACI in a vnet",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":34,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=abfc7f57-eb48-411e-acbd-c71bd241842b\">@AI866  <\/a> Thanks, If you are using AMLS SDK, Unfortunately this is a limitation today that we plan to address this in the near future.    <br \/>\nYou can create a pipeline, DevOps or manual process to deploy to any ACI in any VNET\/different subscription    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/container-instances\/container-instances-vnet\">https:\/\/learn.microsoft.com\/en-us\/azure\/container-instances\/container-instances-vnet<\/a>    <\/p>\n<p>Please follow the below for common troubleshooting issues.    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/container-instances\/container-instances-troubleshooting\">https:\/\/learn.microsoft.com\/en-us\/azure\/container-instances\/container-instances-troubleshooting<\/a>    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":19.9,
        "Solution_reading_time":10.75,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4,
        "Solution_word_count":61,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1432829415467,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":501.0,
        "Answerer_view_count":76.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm posting this more as a 'probe' question and plan to expand the discussion in case some interest shows up. The reason behind this is that in my experience, the SO community on <code>azure-ml<\/code> (and related) is still developing and there is not much feedback - but I would be happy to help it grow stronger. <\/p>\n\n<p>My situation is as follows: I have an experiment in Azure ML which does all its work inside an <code>R<\/code> module. I published this as a web service and set the 'max concurrent calls' slider to 10 - which I believe guarantees me that there will be at most 10 instances of my web service up and running at any time, to serve requests (please correct me if i am wrong). <\/p>\n\n<p>Now, I am trying to do some performance testing by firing 10 parallel calls to my webservice, but get unexpected results...<\/p>\n\n<p>I am trying to run the load tests and log where each of them actually goes to (which instance). My idea is to get a glimpse into how these calls are actually distributed to the instances by the load balancer, under certain max number of concurrent calls = X. I am doing this by firing a call to \"bot.whatismyipaddress.com\" from inside the <code>R<\/code> script. Here is the important snip of the code:<\/p>\n\n<pre><code>library(rjson)\nmachine.ip &lt;- readLines(\"http:\/\/bot.whatismyipaddress.com\/\", warn=F)\nresult$MachineIP &lt;- machine.ip\n<\/code><\/pre>\n\n<p>Additionally, I am using the sample <code>R<\/code> code from the web service RRS help page to fire up to 70 (sequential) calls to my web service. This sample code returns some info back to the console : the results of my web service as well as some info on to which hostname the call goes through. Here is a sample :<\/p>\n\n<pre><code>* Hostname was NOT found in DNS cache\n*   Trying 40.114.242.9...\n* Connected to europewest.services.azureml.net (40.114.242.9) port 443 (#0)\n<\/code><\/pre>\n\n<p>The difficulty that I am facing is that I cannot <strong>uniquely identify<\/strong> the different instances of my web service. The info out to console from the call (the second snippet) often shows a different IP address than the one from inside-<code>R<\/code>-code logs (<code>result$MachineIP<\/code>)...<\/p>\n\n<p>Can someone point out what am i doing wrong, and how could i uniquely identify the different instances that are serving the calls? Any help would be really appreciated. Thanks!<\/p>\n\n<p>P.S. I've tried <a href=\"https:\/\/stackoverflow.com\/questions\/14357219\/function-for-retrieving-own-ip-address-from-within-r\">this<\/a> as well, but the first apporach does not work when calling it from inside the <code>R<\/code> script and I'm using a modified version of the second apporach (the one suggested there does not work). <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/93f07abf-f0ec-4baa-8225-1ca1a072ca2d\/system-call-from-inside-r-script-does-not-work?forum=MachineLearning\" rel=\"nofollow noreferrer\">Here<\/a> are also my <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/ee6ff5a6-2995-4f3f-b4db-0229b1d9d1d3\/lifetime-of-azure-ml-web-service-container?forum=MachineLearning\" rel=\"nofollow noreferrer\">questions<\/a> on the Azure forum, in case someone is interested.<\/p>\n\n<p>If anyone could help or point me to some source of info I would be really grateful! <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1450358009790,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1495540319592,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34335483",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":42.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":null,
        "Challenge_title":"Uniquely identify instances of VMs (Azure ML - web services)",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":61.0,
        "Challenge_word_count":464,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432829415467,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":501.0,
        "Poster_view_count":76.0,
        "Solution_body":"<p>This question was resolved thanks to some people on the Azure ML forum so \nI'm going to post an answer for anyone landing here in search for some answers...<\/p>\n\n<p>The short answer is no, this is not possible. The more detailed version is:<br>\n\"From within the R script you cannot identify the internal AzureML IP addresses or the unique web service instances. When you make an external network call from the R script to an outside URL, that URL will see one of the AzureML public virtual IP's as the source IP. These are IP's of the load balancers, and not of the machines that are physically running the web service. AzureML dynamically allocates the instances of R engine in the backend, handles failures, and uses multiple nodes for running the web service for high availability. The exact layout of these for a given web service is not programmatically discoverable.\"<br>\nHere is also the <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/dd1f0658-7b0b-46d8-8e32-3fe4e96ec4be\/uniquely-identify-instances-of-vms-web-services?forum=MachineLearning#cde28631-828d-4d83-9c93-1a1cf0dfb6fb\" rel=\"nofollow\">link<\/a> to the original discussion. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":11.5,
        "Solution_reading_time":14.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9,
        "Solution_word_count":162,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a code R (manipulateXY.R) taking parameters X (from picklist), Y (a \"not constrained\" value) from a text file (parameter.txt) and producing n images.\nI want to put this code as a \"R script\" in Azure ML, and to produce a web service pointing to that logic (manipulateXY). The question is: how can I pass parameters to the Azure code? I need it because I want a web app with the following outfit<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/dq0Kr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/dq0Kr.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>such that I choose the X and Y and press \"Run\", it calls the logic in Azure ML, it takes the generated images and put them on the web-app. <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1466520080903,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37947524",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":9.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"AzureML: taking parameters as input",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":91.0,
        "Challenge_word_count":117,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436432728608,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Colleferro, Italy",
        "Poster_reputation_count":809.0,
        "Poster_view_count":361.0,
        "Solution_body":"<p>You can use web service parameters as shown here - <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-web-service-parameters\/\" rel=\"nofollow\">https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-web-service-parameters<\/a>\/<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":52.8,
        "Solution_reading_time":3.97,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1,
        "Solution_word_count":12,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1397242650447,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been pulling my hair trying to figure out what's wrong with mlflow. Iam deploying mlflow v1.26 in google cloudRun . back end artitfactory is google storage and backend database is google cloudsql postgres v13 instance.<\/p>\n<p>here is my entrypoint using pg8000 v1.21.3 (I tried latest version as well) and psycopg2-binary v2.9.3<\/p>\n<pre><code>\nset -e\nexport ARTIFACT_URL=&quot;gs:\/\/ei-cs-dev01-ein-sb-teambucket-chaai-01\/mlflow\/&quot;\nexport DATABASE_URL=&quot;postgresql+pg8000:\/\/mlflow:change2022@10.238.139.37:5432\/mlflowps&quot; #&quot;$(python3 \/app\/get_secret.py --project=&quot;${GCP_PROJECT}&quot; --secret=mlflow_database_url)&quot;\n\nif [[ -z &quot;${PORT}&quot; ]]; then\n    export PORT=8080\nfi\n\nexec mlflow server -h 0.0.0.0 -w 4 -p ${PORT} --default-artifact-root ${ARTIFACT_URL} --backend-store-uri ${DATABASE_URL}\n<\/code><\/pre>\n<p>now when I open mlflow ui page I see this error happening:\n(<\/p>\n<blockquote>\n<p>BAD_REQUEST: (pg8000.dbapi.ProgrammingError) {'S': 'ERROR', 'V':\n'ERROR', 'C': '42883', 'M': 'operator does not exist: integer =\ncharacter varying', 'H': 'No operator matches the given name and\nargument types. You might need to add explicit type casts.', 'P':\n'382', 'F': 'parse_oper.c', 'L': '731', 'R': 'op_error'} [SQL: SELECT\nDISTINCT runs.run_uuid..<\/p>\n<\/blockquote>\n<p>)\n<a href=\"https:\/\/i.stack.imgur.com\/gjbLj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gjbLj.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653690611447,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72411618",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":19.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"MLFLOW and Postgres getting Bad Request error",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":185.0,
        "Challenge_word_count":162,
        "Platform":"Stack Overflow",
        "Poster_created_time":1644540925376,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>You should use psycopg2 instead, e.g.:<\/p>\n<p><code>postgresql+psycopg2:\/\/&lt;username&gt;:&lt;password&gt;@\/&lt;dbname&gt;?host=\/cloudsql\/&lt;my-project&gt;:&lt;us-central1&gt;:&lt;dbinstance&gt;<\/code><\/p>\n<p>It works for me, with versions:<\/p>\n<p>mlflow==1.26.1<\/p>\n<p>psycopg2-binary==2.9.3<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":22.7,
        "Solution_reading_time":4.22,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2,
        "Solution_word_count":15,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1227171471292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":17500.0,
        "Answerer_view_count":1561.0,
        "Challenge_adjusted_solved_time":233.4898177778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Invoking a multimodel Sagemaker Endpoint, I get an error that it is not multimodel. I create it like this.<\/p>\n<pre><code>create_endpoint_config_response = client.create_endpoint_config(\n    EndpointConfigName=endpoint_config_name,\n    ProductionVariants=[\n        {\n            &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n            &quot;InitialVariantWeight&quot;: 0.5,\n            &quot;InitialInstanceCount&quot;: 1,\n            &quot;ModelName&quot;: model_name1,\n            &quot;VariantName&quot;: model_name1,\n        },\n         {\n            &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n            &quot;InitialVariantWeight&quot;: 0.5,\n            &quot;InitialInstanceCount&quot;: 1,\n            &quot;ModelName&quot;: model_name2,\n            &quot;VariantName&quot;: model_name2,\n        }\n    ]\n)\n<\/code><\/pre>\n<p>I confirm in the GUI that it in fact has multiple models. I invoke it like this:<\/p>\n<pre><code>response = client.invoke_endpoint(\n    EndpointName=endpoint_name, \n    TargetModel=model_name1,\n    ContentType=&quot;text\/x-libsvm&quot;, \n    Body=payload\n)\n<\/code><\/pre>\n<p>and get this error:<\/p>\n<blockquote>\n<p>ValidationError: An error occurred (ValidationError) when calling the\nInvokeEndpoint operation: Endpoint\nmy-endpoint1 is not a multi-model endpoint\nand does not support target model header.<\/p>\n<\/blockquote>\n<p>The same problem was discussed <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1026\" rel=\"nofollow noreferrer\">here<\/a> with no resolution.<\/p>\n<p>How can I invoke a multimodel endpoint?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629114933853,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1629119102992,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68802388",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":16.4,
        "Challenge_reading_time":19.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Why do I get an error that Sagemaker Endpoint does not have multiple models when it does?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":247.0,
        "Challenge_word_count":133,
        "Platform":"Stack Overflow",
        "Poster_created_time":1227171471292,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Israel",
        "Poster_reputation_count":17500.0,
        "Poster_view_count":1561.0,
        "Solution_body":"<p>The answer (see <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1026\" rel=\"nofollow noreferrer\">GitHub<\/a> discussion) is that this error message is simply false.<\/p>\n<p>To avoid this error, the model's local filename (usually for the form <code>model_filename.tar.gz<\/code>) must be used, not the model name.<\/p>\n<p>The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/invoke-multi-model-endpoint.html\" rel=\"nofollow noreferrer\">documentation<\/a> does say this, though it lacks essential detail.<\/p>\n<p>I found <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb\" rel=\"nofollow noreferrer\">this to be the best example<\/a>.  See the last part  of that Notebook, in which <code>invoke_endpoint<\/code> is used (rather than a predictor as used earlier in the Notebook).<\/p>\n<p>As to the location of that model file: This <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">Notebook<\/a> says:<\/p>\n<blockquote>\n<p>When creating the Model entity for multi-model endpoints, the container's ModelDataUrl is the S3 prefix where the model\nartifacts that are invokable by the endpoint are located. The rest of the S3 path will be specified when invoking the model.<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1629959666336,
        "Solution_link_count":4,
        "Solution_readability":17.2,
        "Solution_reading_time":19.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12,
        "Solution_word_count":138,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"Hello,  \n  \nI have trained my model on sagemaker. I have deleted the endpoint, but I am keeping the model and the endpoint configuration which points to the model.  \n  \nFrom the sagemaker dashboard I am able to recreate the endpoint using the existing endpoint configuration. However I don't want to keep the endpoint on all the time, as I will use it only once a day for a few minutes.  \n  \nIs it possible to create in on demand from a Python script? I would assume that it is possible, but can't find how. Can someone point me in the right direction?  \n  \nRegards.",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625083671000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668599351248,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUTyUMHH4QRDaMa6L24rhOMg\/create-endpoint-from-python",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.0,
        "Challenge_reading_time":6.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Create endpoint from Python",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":125.0,
        "Challenge_word_count":104,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello hugoflores,   \n  \nYou can use SageMaker APIs - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DeleteEndpoint.html to delete the endpoint and https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateEndpoint.html. to create an endpoint. This an be automated either using SageMaker Pipelines or a Lambda function.  \n  \nHere are a few resources towards that:  \n  \nhttps:\/\/awsfeed.com\/whats-new\/machine-learning\/build-a-ci-cd-pipeline-for-deploying-custom-machine-learning-models-using-aws-services  \nhttps:\/\/github.com\/aws-samples\/aws-lambda-layer-create-script  \nhttps:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1200  \nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/  \nhttps:\/\/www.sagemakerworkshop.com\/step\/deploymodel\/  \n  \nHTH,   \n  \nChaitanya",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1626971777000,
        "Solution_link_count":7,
        "Solution_readability":32.1,
        "Solution_reading_time":11.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6,
        "Solution_word_count":44,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running this tutorial: <a href=\"https:\/\/learn.microsoft.com\/de-de\/azure\/machine-learning\/tutorial-first-experiment-automated-ml\">https:\/\/learn.microsoft.com\/de-de\/azure\/machine-learning\/tutorial-first-experiment-automated-ml<\/a>    <\/p>\n<p>and struggle under &quot;next steps&quot; to deploy this model to a browser user interface of some kind (where I can manually type in the input values and press &quot;predict&quot; to get the output value).     <\/p>\n<p>Background: I would like to present this for a seminar &quot;AI without any code&quot; and hence I will not call this REST-API in any other place but try to stay in the (Azure) web ecosystem. Any chance to get such a webinterface (functionality)?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603290567957,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/134024\/unable-to-deploy-a-automl-as-a-webservice-without",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.9,
        "Challenge_reading_time":10.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Unable to deploy a autoML as a webservice without using C#, Go, Java, or Python (just a browser)",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":103,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Thanks for reaching out. Currently, Azure AutoML does not support consuming deployed web services via UI. You can create a client for the service, or use python to consume the web service via Azure ML Notebooks. Sorry for the inconvenience.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":6.0,
        "Solution_reading_time":3.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":40,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1342685175156,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":12103.0,
        "Answerer_view_count":1451.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am able to deploy a Azure Machine learning prediction service in my workspace <code>ws<\/code> using the syntax<\/p>\n\n<pre><code>aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n                                               memory_gb=8, \n                                               tags={\"method\" : \"some method\"}, \n                                               description='Predict something')\n<\/code><\/pre>\n\n<p>and then<\/p>\n\n<pre><code>service = Webservice.deploy_from_image(deployment_config = aciconfig,\n                                       image = image,\n                                       name = service_name,\n                                       workspace = ws)\n<\/code><\/pre>\n\n<p>as described in the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where#aci\" rel=\"nofollow noreferrer\">documentation<\/a>.<br>\nHowever, this exposes a service publicly and this is not really optimal.<\/p>\n\n<p>What's the easiest way to shield the ACI service? I understand that passing an <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.webservice.aciwebservice?view=azure-ml-py#deploy-configuration-cpu-cores-none--memory-gb-none--tags-none--properties-none--description-none--location-none--auth-enabled-none--ssl-enabled-none--enable-app-insights-none--ssl-cert-pem-file-none--ssl-key-pem-file-none--ssl-cname-none-\" rel=\"nofollow noreferrer\"><code>auth_enabled=True<\/code><\/a> parameter may do the job, but then how can I instruct a client (say, using <code>curl<\/code> or Postman) to use the service afterwards? <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1556135731340,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1556186547292,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55837639",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":18.8,
        "Challenge_reading_time":19.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"How to enable authentication for an ACI webservice in Azure Machine Learning service?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":676.0,
        "Challenge_word_count":113,
        "Platform":"Stack Overflow",
        "Poster_created_time":1415722650716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Verona, VR, Italy",
        "Poster_reputation_count":4811.0,
        "Poster_view_count":713.0,
        "Solution_body":"<p>See <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-consume-web-service#call-the-service-c\" rel=\"nofollow noreferrer\">here<\/a> for an example (in C#). When you enable auth, you will need to send the API key in the \"Authorization\" header in the HTTP request:<\/p>\n\n<pre><code>client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Bearer\", authKey);\n<\/code><\/pre>\n\n<p>See <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-consume-web-service#authentication-key\" rel=\"nofollow noreferrer\">here<\/a> how to retrieve the key.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1556183204092,
        "Solution_link_count":2,
        "Solution_readability":19.8,
        "Solution_reading_time":8.26,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5,
        "Solution_word_count":45,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>hello.  <\/p>\n<p>I am currently using Machine Learning Studio (classic).  <\/p>\n<p>'From now through 31 August 2024, you can continue to use the existing Machine Learning Studio (classic) experiments and web services. Beginning 1 December 2021, new creation of Machine Learning Studio (classic) resources will not be available.'  <\/p>\n<p>As mentioned above, what do resources mean?  <br \/>\nIs it possible to continue creating experiments and web sales, and using APIs from outside until 2024?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1635690920907,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/610432\/about-the-end-of-machine-learning-studio-(classic)",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.6,
        "Challenge_reading_time":6.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"About the end of Machine Learning Studio (classic)",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":81,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=6fde402b-b49f-41d7-8640-5316dd9a5cd2\">@gatsby53  <\/a> ,    <\/p>\n<p>Thanks for reaching out to us here. There are several important dates.    <\/p>\n<p>Beginning 1 December 2021, you will <strong>not be able to create new<\/strong> Machine Learning Studio (classic) resources. You can still work on your <strong>existing resource<\/strong> from 1 December 2021 to 31 August 2024.    <\/p>\n<p>Support for Machine Learning Studio (classic) will end on 31 August 2024. We recommend you transition to Azure Machine Learning by that date.    <\/p>\n<p>Please refer to this guidance for how to migrate your project for better experience.    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/migrate-overview\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/migrate-overview<\/a>    <\/p>\n<p>Hope this will help. Please let us know if any further queries.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/543261\/index.html\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6,
        "Solution_readability":11.9,
        "Solution_reading_time":25.33,
        "Solution_score_count":0.0,
        "Solution_sentence_count":20,
        "Solution_word_count":201,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1324988509368,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Moscow, Russia",
        "Answerer_reputation_count":1593.0,
        "Answerer_view_count":93.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to Invoke Endpoint, previously deployed on Amazon SageMaker.\nHere is my code:<\/p>\n\n<pre><code>import numpy as np\nimport boto3\n\nclient = boto3.client('sagemaker-runtime')\n\ndef np2csv(arr):\n    csv = io.BytesIO()\n    np.savetxt(csv, arr, delimiter=',', fmt='%g')\n    return csv.getvalue().decode().rstrip()\n\nendpoint_name = 'DEMO-XGBoostEndpoint-2018-12-12-22-07-28'\ntest_vector = np.array([3.60606061e+00, \n                        3.91395664e+00, \n                        1.34200000e+03, \n                        4.56100000e+03,\n                        2.00000000e+02, \n                        2.00000000e+02]) \ncsv_test_vector = np2csv(test_vector)\n\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                               ContentType='text\/csv',\n                               Body=csv_test_vector)\n<\/code><\/pre>\n\n<p>And here is the error I get:<\/p>\n\n<blockquote>\n  <p>ModelErrorTraceback (most recent call last)\n   in ()\n        1 response = client.invoke_endpoint(EndpointName=endpoint_name,\n        2                                    ContentType='text\/csv',\n  ----> 3                                    Body=csv_test_vector)<\/p>\n  \n  <p>\/home\/ec2-user\/anaconda3\/envs\/python2\/lib\/python2.7\/site-packages\/botocore\/client.pyc\n  in _api_call(self, *args, **kwargs)\n      318                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n      319             # The \"self\" in this scope is referring to the BaseClient.\n  --> 320             return self._make_api_call(operation_name, kwargs)\n      321 \n      322         _api_call.<strong>name<\/strong> = str(py_operation_name)<\/p>\n  \n  <p>\/home\/ec2-user\/anaconda3\/envs\/python2\/lib\/python2.7\/site-packages\/botocore\/client.pyc\n  in _make_api_call(self, operation_name, api_params)\n      621             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n      622             error_class = self.exceptions.from_code(error_code)\n  --> 623             raise error_class(parsed_response, operation_name)\n      624         else:\n      625             return parsed_response<\/p>\n  \n  <p>ModelError: An error occurred (ModelError) when calling the\n  InvokeEndpoint operation: Received client error (415) from model with\n  message \"setting an array element with a sequence.\". See\n  <a href=\"https:\/\/us-east-1.console.aws.amazon.com\/cloudwatch\/home?region=us-east-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/DEMO-XGBoostEndpoint-2018-12-12-22-07-28\" rel=\"nofollow noreferrer\">https:\/\/us-east-1.console.aws.amazon.com\/cloudwatch\/home?region=us-east-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/DEMO-XGBoostEndpoint-2018-12-12-22-07-28<\/a>\n  in account 249707424405 for more information.<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544739509967,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53770876",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":17.3,
        "Challenge_reading_time":31.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS Sagemaker, InvokeEndpoint operation, Model error: \"setting an array element with a sequence.\"",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":2561.0,
        "Challenge_word_count":171,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324988509368,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Moscow, Russia",
        "Poster_reputation_count":1593.0,
        "Poster_view_count":93.0,
        "Solution_body":"<p>This works:<\/p>\n\n<pre><code>import numpy as np\nimport boto3\n\nclient = boto3.client('sagemaker-runtime')\nendpoint_name = 'DEMO-XGBoostEndpoint-2018-12-12-22-07-28'\ntest_vector = [3.60606061e+00, \n               3.91395664e+00, \n               1.34200000e+03, \n               4.56100000e+03,\n               2.00000000e+02, \n               2.00000000e+02]) \n\nbody = ',',join([str(item) for item in test_vector])\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                               ContentType='text\/csv',\n                               Body=body)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":12.1,
        "Solution_reading_time":5.95,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4,
        "Solution_word_count":30,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>There is a requirement -    <\/p>\n<ol>\n<li> ML models are created by third party vendors in their Azure environment. ML models will be readily available for consumption.    <\/li>\n<li> As an admin, we need to setup new environment for Azure machine learning for our organization.    <\/li>\n<li> Once point#2 is completed, need to import ML models from point#1 to the newly setup ML environment.    <\/li>\n<\/ol>\n<p>Can you please let us know all possible ways to do this?    <\/p>\n<p>Thank You!    <\/p>\n<p>Regards,    <br \/>\nPreetha    <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1672980780277,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1153982\/all-possible-options-to-import-exising-azure-ml-mo",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":7.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"All possible options to import exising Azure ML models to newly created Azure ML environment!",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":100,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=9dd3fdca-62ad-43a4-9e40-b142520a64a4\">@Preetha Rajesh  <\/a> I believe the models are trained in another workspace or subscription that are used by your company which you would need to use in your workspace or environment.    <\/p>\n<p>The common scenario is to register the models in your workspace by providing a valid path or through studio\/cli\/SDK. This <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-models?tabs=use-local%2Ccli\">document<\/a> should help you setup the same.    <br \/>\nThe newer or a <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-share-models-pipelines-across-workspaces-with-registries?tabs=cli\">preview version<\/a> of sharing the models across workspaces is available to use, you can try the same and check if this works for your organization.    <\/p>\n<p>Once the models are registered in your workspace, the usage of the same in an environment is pretty much the same as using the models you have trained in your workspace. Initially, you might want to use the UI but the SDK and CLI can also be used for automating if you have large number of models from your vendors.     <\/p>\n<p>The documents referenced contains all possible ways to achieve this, I hope this helps!!    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_readability":12.0,
        "Solution_reading_time":21.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12,
        "Solution_word_count":195,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>My code is:<\/p>\n<pre><code>   client = aiplatform_v1.EndpointServiceClient(client_options=options)\n    parent = client.common_location_path(project=project_id, location=location)\n    \n\n    # Initialize request argument(s)\n    request = aiplatform_v1.ListEndpointsRequest(\n        parent=parent\n    )\n\n    # Make the request\n    endpoints_pager = client.list_endpoints(request=request)\n    for endpoint in endpoints_pager.pages:\n        latest_endpoint=endpoint\n        print(endpoint.deployed_models.id)\n<\/code><\/pre>\n<p>If I print <code>endpoint<\/code> I see <code>{...deployed_models { id: ...}}<\/code>\n<code>endpoint.deployed_models.id)<\/code> doesn't work so how do I get the deployed model id?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1662992136247,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73690729",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":16.3,
        "Challenge_reading_time":9.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"How do I get deployed model from `ListEndpointsRequest`?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":44.0,
        "Challenge_word_count":55,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>As @DazWilkin mentioned in the comments, you need to iterate through <code>deployed_models<\/code> to get the <code>id<\/code> per model. Applying this, your code should look like this:<\/p>\n<p>IMPORTANT NOTE FOR FUTURE READERS: When creating <code>client<\/code> it is needed to define the <code>api_endpoint<\/code> in <code>client_options<\/code>. If not not defined you will encounter a <strong>google.api_core.exceptions.MethodNotImplemented: 501 Received http2 header with status: 404<\/strong> error.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from google.cloud import aiplatform_v1\n\ndef sample_list_endpoints():\n\n    project_id = &quot;your-project-id&quot;\n    location = &quot;us-central1&quot;\n\n    client = aiplatform_v1.EndpointServiceClient(client_options={&quot;api_endpoint&quot;:&quot;us-central1-aiplatform.googleapis.com&quot;})\n\n    parent = client.common_location_path(project=project_id, location=location)\n    # Initialize request argument(s)\n    request = aiplatform_v1.ListEndpointsRequest(\n        parent=parent,\n    )\n\n    # Make the request\n    page_result = client.list_endpoints(request=request)\n\n    # Handle the response\n    for response in page_result:\n        for model in response.deployed_models:\n            print(model.id)\n\nsample_list_endpoints()\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":16.4,
        "Solution_reading_time":16.36,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11,
        "Solution_word_count":102,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1621658973823,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":3213.0,
        "Answerer_view_count":1896.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been following the learning path for <a href=\"https:\/\/docs.microsoft.com\/en-us\/learn\/certifications\/exams\/ai-900\" rel=\"nofollow noreferrer\">Microsoft Azure AI 900<\/a>. In the second module, I have deployed my model as an endpoint. It says Container instances for compute type. How much will this cost me. Azure doesn't seem to show any pricing for this. Is this endpoint always active? If yes how much does it cost?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1635485800157,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69764100",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":6.1,
        "Challenge_reading_time":5.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Endpoints cost on Azure Machine Learning",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":633.0,
        "Challenge_word_count":66,
        "Platform":"Stack Overflow",
        "Poster_created_time":1566078293736,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Mumbai, Maharashtra, India",
        "Poster_reputation_count":449.0,
        "Poster_view_count":77.0,
        "Solution_body":"<p>The price depends on the number of <strong>vCPU<\/strong> and <strong>GBs<\/strong> of memory requested for the container group. You are charged based on the <strong>vCPU request<\/strong> for your container group rounded up to the nearest whole number for the duration (measured in seconds) <strong>your instance is running<\/strong>. You are also charged for the <strong>GB request<\/strong> for your container group rounded up to the nearest tenths place for the duration (measured in seconds) your <strong>container group is running<\/strong>. There is an additional charge of $0.000012 per vCPU second for Windows software duration on Windows container groups. Check here <a href=\"https:\/\/azure.microsoft.com\/en-us\/pricing\/details\/container-instances\/\" rel=\"nofollow noreferrer\">Pricing - Container Instances | Microsoft Azure<\/a> for details<\/p>\n<ul>\n<li>After Deployed the Azure Machine Learning managed online endpoint (preview).<\/li>\n<li>Have at least <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/role-based-access-control\/role-assignments-portal.md\" rel=\"nofollow noreferrer\">Billing Reader<\/a> access on the subscription where the endpoint is deployed<\/li>\n<\/ul>\n<p>To know the costs estimation<\/p>\n<ol>\n<li><p>In the <a href=\"https:\/\/portal.azure.com\/\" rel=\"nofollow noreferrer\">Azure portal<\/a>, Go to your subscription<\/p>\n<\/li>\n<li><p>Select <strong>Cost Analysis<\/strong> for your subscription.<\/p>\n<\/li>\n<\/ol>\n<p><img src=\"https:\/\/i.imgur.com\/W2eaRIO.png\" alt=\"enter image description here\" \/><\/p>\n<p>Create a filter to scope data to your Azure Machine learning workspace resource:<\/p>\n<ol>\n<li><p>At the top navigation bar, select <strong>Add filter<\/strong>.<\/p>\n<\/li>\n<li><p>In the first filter dropdown, select <strong>Resource<\/strong> for the filter type.<\/p>\n<\/li>\n<li><p>In the second filter dropdown, select your Azure Machine Learning workspace.<\/p>\n<\/li>\n<\/ol>\n<p><img src=\"https:\/\/i.imgur.com\/HEvprph.png\" alt=\"enter image description here\" \/><\/p>\n<p>Create a tag filter to show your managed online endpoint and\/or managed online deployment:<\/p>\n<ol>\n<li><p>Select <strong>Add filter<\/strong> &gt; <strong>Tag<\/strong> &gt; <strong>azuremlendpoint<\/strong>: &quot;&lt; your endpoint name&gt;&quot;<\/p>\n<\/li>\n<li><p>Select <strong>Add filter<\/strong> &gt; <strong>Tag<\/strong> &gt; <strong>azuremldeployment<\/strong>: &quot;&lt; your deployment name&gt;&quot;.<\/p>\n<\/li>\n<\/ol>\n<p><img src=\"https:\/\/i.imgur.com\/1aapYGB.png\" alt=\"enter image description here\" \/><\/p>\n<p>Refer  <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/how-to-view-online-endpoints-costs.md\" rel=\"nofollow noreferrer\">here <\/a> for more detailed steps<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7,
        "Solution_readability":12.6,
        "Solution_reading_time":35.84,
        "Solution_score_count":2.0,
        "Solution_sentence_count":19,
        "Solution_word_count":280,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":9.3958480556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I trained a model on GCP Vertex AI, and deployed it on an endpoint.<\/p>\n<p>I am able to execute predictions from a sample to my model with this python code <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-automl#aiplatform_predict_image_classification_sample-python\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-automl#aiplatform_predict_image_classification_sample-python<\/a><\/p>\n<p>It works within my GCP project.<\/p>\n<p>My question is, is it possible to request this endpoint from another GCP project ? If I set a service account and set IAM role in both projects ?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643047926007,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70838510",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.6,
        "Challenge_reading_time":9.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Is it possible to request a Vertex AI endpoint from another GCP project?",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":417.0,
        "Challenge_word_count":80,
        "Platform":"Stack Overflow",
        "Poster_created_time":1598873976143,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Versailles, France",
        "Poster_reputation_count":140.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>Yes it is possible. For example you have Project A and Project B, assuming that Project A hosts the model.<\/p>\n<ul>\n<li><p>Add service account of Project B in Project A and provide at least <code>roles\/aiplatform.user<\/code> predefined role. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/access-control#predefined-roles\" rel=\"nofollow noreferrer\">predefined roles<\/a> and look for <code>roles\/aiplatform.user<\/code> to see complete roles it contains.<\/p>\n<\/li>\n<li><p>This role contains <strong>aiplatform.endpoints.<\/strong>* and <strong>aiplatform.batchPredictionJobs.<\/strong>* as these are the roles needed to run predictions.<\/p>\n<blockquote>\n<p>See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/iam-permissions\" rel=\"nofollow noreferrer\">IAM permissions for Vertex AI<\/a><\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Resource<\/th>\n<th>Operation<\/th>\n<th>Permissions needed<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>batchPredictionJobs<\/td>\n<td>Create a batchPredictionJob<\/td>\n<td>aiplatform.batchPredictionJobs.create (permission needed on the parent resource)<\/td>\n<\/tr>\n<tr>\n<td>endpoints<\/td>\n<td>Predict an endpoint<\/td>\n<td>aiplatform.endpoints.predict (permission needed on the endpoint resource)<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div><\/blockquote>\n<\/li>\n<\/ul>\n<p>With this set up, Project B will be able to use the model in Project A to run predictions.<\/p>\n<p>NOTE: Just make sure that the script of Project B points to the resources in Project A like <code>project_id<\/code> and <code>endpoint_id<\/code>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1643081751060,
        "Solution_link_count":2,
        "Solution_readability":11.1,
        "Solution_reading_time":20.79,
        "Solution_score_count":1.0,
        "Solution_sentence_count":15,
        "Solution_word_count":163,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Is there any way to return a custom HTTP status code from R Web Service in Azure ML?  <\/p>\n<p>All the examples of entry scripts in documentation return the response body from the scoring function. In Python Web Service, it is possible to return a HTTP response object with a custom status code. However, R's httr library does not seem to have any function to create response objects directly (only via HTTP method objects such as POST, which call a given URL).  <\/p>\n<p>I would like to implement a custom exception handling scheme in R Web Service. Is there any way to return a custom HTTP code from the entry script?  <\/p>\n<p>EDIT: Found this idea on the feedback forum, which suggests that the option is not available in Python Web Service either:  <br \/>\n<a href=\"https:\/\/feedback.azure.com\/forums\/257792-machine-learning\/suggestions\/40122838-make-http-status-codes-controllable-from-your-scor\">https:\/\/feedback.azure.com\/forums\/257792-machine-learning\/suggestions\/40122838-make-http-status-codes-controllable-from-your-scor<\/a>  <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1612354860267,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/257156\/how-to-specify-http-response-status-code-in-aml-r",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":12.9,
        "Challenge_reading_time":13.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"How to specify HTTP response status code in AML R Web Service",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":148,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello Lauri,  <\/p>\n<p>Thanks for the feedback. Yes, we have this product idea in our backlog. I will help to bump up this idea to product group again. ^^  <\/p>\n<p>Regards,  <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":2.9,
        "Solution_reading_time":2.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":32,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Is it possible to train and deploy ML models in Greengrass? Or is Greengrass limited to inference while training is done using SageMaker in cloud?",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1556295399000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668529586004,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU2FEvRboNRL2Ipn1yE7IBvg\/greengrass-for-data-processing-and-ml-model-training",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":2.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Greengrass for data processing and ML model training",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":78.0,
        "Challenge_word_count":32,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"As a native service offering, Greengrass has support for deploying models to the edge and running *inference* code against those models. Nothing prevents you from deploying your own code to the edge that would train a model, but I suspect you wouldn't be able to store it as a Greengrass local resource for later inferences without doing a round trip to the cloud and redeploying to GG.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1606141115182,
        "Solution_link_count":0,
        "Solution_readability":15.2,
        "Solution_reading_time":4.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2,
        "Solution_word_count":67,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1417744681680,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":4211.0,
        "Answerer_view_count":716.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to invoke an AWS SageMaker endpoint through the following simple code using boto3<\/p>\n<pre><code>import boto3\n\nsession = boto3.Session(profile_name='mlacc',\n                        region_name='us-west-2')\n\nsagemaker_client = session.client('sagemaker-runtime')\n\nrequest_body = &quot;{\\n    \\&quot;requestSource\\&quot;: \\&quot;unittest\\&quot;,\\n    \\&quot;clusters\\&quot;: [{\\n        \\&quot;clusterMetadata\\&quot;: {\\n &quot;\n&quot;\\&quot;clusterId\\&quot;: \\&quot;id1\\&quot;,\\n            \\&quot;topic\\&quot;: [\\&quot;corona virus\\&quot;, \\&quot;Donald Trump\\&quot;],\\n            &quot;\n&quot;\\&quot;clusterSize\\&quot;: 2\\n        },\\n        \\&quot;documents\\&quot;: [{\\n            \\&quot;uid\\&quot;: \\&quot;1\\&quot;,\\n            &quot;\n&quot;\\&quot;content\\&quot;: \\&quot;content2\\&quot;,\\n            \\&quot;domain\\&quot;: \\&quot;CNN.com\\&quot;,\\n            \\&quot;title\\&quot;: \\&quot;This is a &quot;\n&quot;title\\&quot;,\\n            \\&quot;similarityScore\\&quot;: 1.3,\\n            \\&quot;published_at\\&quot;: 1566264017,\\n            &quot;\n&quot;\\&quot;domain_rank\\&quot;: 1,\\n            \\&quot;trust_domain_score\\&quot;: 1\\n        }, {\\n            \\&quot;uid\\&quot;: \\&quot;2\\&quot;,&quot;\n&quot;\\n            \\&quot;content\\&quot;: \\&quot;content2\\&quot;,\\n            \\&quot;domain\\&quot;: \\&quot;CNN.com\\&quot;,\\n            \\&quot;title\\&quot;: &quot;\n&quot;\\&quot;This is a title\\&quot;,\\n            \\&quot;similarityScore\\&quot;: 1.3,\\n            \\&quot;published_at\\&quot;: 1566264017,&quot;\n&quot;\\n            \\&quot;domain_rank\\&quot;: 1,\\n            \\&quot;trust_domain_score\\&quot;: 1\\n        }, {\\n            \\&quot;uid\\&quot;: &quot;\n&quot;\\&quot;2\\&quot;,\\n            \\&quot;content\\&quot;: \\&quot;content3\\&quot;,\\n            \\&quot;domain\\&quot;: \\&quot;CNN.com\\&quot;,\\n            \\&quot;title\\&quot;: &quot;\n&quot;\\&quot;This is a title\\&quot;,\\n            \\&quot;similarityScore\\&quot;: 1.3,\\n            \\&quot;published_at\\&quot;: 1566264017,&quot;\n&quot;\\n            \\&quot;domain_rank\\&quot;: 1,\\n            \\&quot;trust_domain_score\\&quot;: 1\\n        }]\\n    }]\\n}&quot;\n\n\nresponse = sagemaker_client.invoke_endpoint(\n    EndpointName='myEndpoint22',\n    Body=request_body,\n    ContentType='application\/json',\n)\n\nresponse_json = response['Body'].read().decode('utf-8')\n\nprint(response_json)\n<\/code><\/pre>\n<p>I get the following error when I run this code<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/Users\/rppatwa\/Desktop\/WorkDocs\/CodePlayground\/SimplePythonProject\/src\/PrototypeTesting\/SummarizationLocal.py&quot;, line 205, in &lt;module&gt;\n    main()\n  File &quot;\/Users\/rppatwa\/Desktop\/WorkDocs\/CodePlayground\/SimplePythonProject\/src\/PrototypeTesting\/SummarizationLocal.py&quot;, line 186, in main\n    ContentType='application\/json',\n  File &quot;\/Users\/rppatwa\/anaconda3\/lib\/python3.7\/site-packages\/botocore\/client.py&quot;, line 316, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File &quot;\/Users\/rppatwa\/anaconda3\/lib\/python3.7\/site-packages\/botocore\/client.py&quot;, line 635, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message &quot;Unable to parse data as JSON. Make sure the Content-Type header is set to &quot;application\/json&quot;&quot;. See https:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/KeyurshaASMLModel in account 753843489946 for more information.\n<\/code><\/pre>\n<p>If I inline the Body (not using the request_json) this call succeeds. Please let me know what I am missing.<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":3,
        "Challenge_created_time":1598659048840,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63642175",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":18.2,
        "Challenge_reading_time":47.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":null,
        "Challenge_title":"Error when invoking AWS SageMaker endpoint using boto3 : \"Unable to parse data as JSON. Make sure the Content-Type header is set to \"application\/json\"",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1170.0,
        "Challenge_word_count":256,
        "Platform":"Stack Overflow",
        "Poster_created_time":1470101805440,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Santa Monica, CA, USA",
        "Poster_reputation_count":107.0,
        "Poster_view_count":21.0,
        "Solution_body":"<p>You need to remove the trailing comma after  <code>ContentType='application\/json',<\/code> and try below snippet for passing JSON to body field.<\/p>\n<pre><code>import json \njson.dumps(request_body) \ntest=json.dumps(request_body).encode()\n<\/code><\/pre>\n<p>This will also validate the JSON that you are passing.Now pass test to body for invoking the endopoint.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":12.1,
        "Solution_reading_time":4.7,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4,
        "Solution_word_count":42,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"I have an Endpoint inference pipeline model deployed from an AutoPilot training job. Now that this is successful, I want to add model monitor. I have a script for online validation of the endpoint, and the F1 score is ~99%. This indicates that the endpoint interprets the call correctly. \n\nModel Monitor is recognizing the data in my jsonl files as the data not being CSV formatted. When my Model Monitor processing job runs, I receive the following constraint violation: \"There are missing columns in current dataset. Number of columns in current dataset: 1, Number of columns in baseline constraints: 225\".\n\nGiven the results from the Endpoint and this Model Monitor constraint violation, I perceive there is a conflict between how the Endpoint is storing the data and how the Model Monitor Processing Job wants to consume the data.\n\nHere is one sample prediction from the jsonl file. The data value is comma separated. \n\n    {\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text\/csv\",\"mode\":\"INPUT\",\"data\":\"JHB,44443000.0,-0.0334,,44264000.0,,,,-2014000.0,,-2014000.0,,,,,,,-0.04,-0.04,55872000.0,,,0.996,,,,,,,,-0.0453,,2845000.0,,2845000.0,11636000.0,,,,,,,,,,,,190000000.0,,,,,,,,-18718000.0,,,,,,,,29000000.0,,,,,,,,-33000000.0,,-4000000.0,,,,,,,,,,,,,,,0.0,,,0.995972369102,1.0,-0.045316472785366,0.0,,,,,,,0.0,,,,,,,,,95.5638,,,,,,1.0,1.0,,0.15263157894737,,,,,,0.65252120693923,0.0,0.15263157894737,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18606500.0,,,95.5638,,,2.3886,,,,,-0.0326,,-1.0449,,-1.05,-1.05,,0.0,,-0.1471,,,,,,,,,,,,,,,,,-0.5451,,,,,,,Financial Services,16.67890010036862\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text\/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"1\\n\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"c97df615-0a2e-414d-9be3-bf3a14eb6363\",\"inferenceTime\":\"2020-04-15T16:26:46Z\"},\"eventVersion\":\"0\"}\n\nHere is the point within the log that the processing job recognizes a column mismatch. I see that it pulls down the data to store locally, pulls down the statistics and constraints files, errors with this constraint, and then gracefully ends the Processing Job. If more logs are needed to analyze, I have the Processing Job logs in CloudWatch Logs.\n\n    2020-04-15 17:11:49 INFO  FileUtil:66 - Read file from path \/opt\/ml\/processing\/baseline\/constraints\/constraints.json.\n    2020-04-15 17:11:50 INFO  FileUtil:66 - Read file from path \/opt\/ml\/processing\/baseline\/stats\/statistics.json.\n    2020-04-15 17:11:50 ERROR DataAnalyzer:65 - There are missing columns in current dataset. Number of columns in current dataset: 1, Number of columns in baseline constraints: 225\n    Skipping further processing because of column count mismatch.\n\nI could not find Model Monitor documentation on how to deal with column mismatch constraint violations.",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586974280000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668600874335,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU8Xkelo1ARA2zcn4rHuk09w\/sagemaker-model-monitor-missing-columns-constraint-violation",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":37.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker Model Monitor Missing Columns Constraint Violation",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":365.0,
        "Challenge_word_count":289,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"That violation fires when, for example, input to your endpoint has fewer columns than baseline input does. This is helpful to flag data quality issues. https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-interpreting-violations.html\n\nIn this case, however, this is an artifact of how we perform the analysis. We concatenate output and input CSVs into a single CSV to analyze the whole thing in one go. E.g. it would look like:\n\n```\noutput_col,input_col_1,input_col_2,...,input_col_n\n```\n\nIn this case, however, your output has a trailing newline which means that after concatenating this looks like:\n\n```\noutput_col # embedded newline in your output\n,input_col_1,input_col_2,...,input_col_n\n```\n\nTriggering the code to think there is only one column in dataset and hence failing the job.\n\nWe have a fix flowing through the pipeline now, while that goes out you can add a preprocessing script to your schedule to strip out the trailing newline from the output. We will create a sample notebook for this, in the meantime docs are at\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-pre-and-post-processing.html#model-monitor-pre-processing-script",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925546704,
        "Solution_link_count":2,
        "Solution_readability":12.2,
        "Solution_reading_time":14.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9,
        "Solution_word_count":152,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1492048364132,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cambridge, MA, USA",
        "Answerer_reputation_count":4436.0,
        "Answerer_view_count":713.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created an endpoint on us-east-1. try to create a predictor:<\/p>\n\n<pre><code>In [106]: sagemaker.predictor.RealTimePredictor(&lt;endpoint name&gt;)\n<\/code><\/pre>\n\n<p>and get<\/p>\n\n<pre><code>ClientError: An error occurred (ValidationException) when calling the DescribeEndpoint operation: \nCould not find endpoint \"arn:aws:sagemaker:us-east-2:&lt;account number&gt;:endpoint\/&lt;endpoint name&gt;\".\n<\/code><\/pre>\n\n<p>which is perfectly correct, since the endpoint is on us-east-1.  Probably I could change some defaults, but I'd rather not - I work on us-east-2 99% of the time.<\/p>\n\n<p>So, how can I set a different region when initializing the predictor?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1573497062720,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58806807",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.6,
        "Challenge_reading_time":9.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Create a predictor from an endpoint in a different region",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":478.0,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1553808322940,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":67.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>The (python) <code>Predictors<\/code> <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html\" rel=\"nofollow noreferrer\">documentation<\/a> shows that you can pass a <code>Session<\/code> object. In turn, the <code>Session<\/code> can be <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/session.html#sagemaker.session.Session\" rel=\"nofollow noreferrer\">initialized<\/a> with a <em>client<\/em> and a <em>runtime client<\/em> - the former does everything except endpoint invocations, the latter does... endpoint invocations.<\/p>\n\n<p>Those clients are tied to specific regions. It seems like you should be able to set the runtime client region to match your endpoint, by manually instantiating it, while leaving the regular client alone (disclaimer here: I haven't tried this - if you do, let me\/us know how it goes :)).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":14.1,
        "Solution_reading_time":10.83,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6,
        "Solution_word_count":94,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,  <\/p>\n<p>I am getting the following error message:  <br \/>\n&quot;WebserviceException: WebserviceException: Message: Service diabetes-service with the same name already exists, please use a different service name or delete the existing service. InnerException None ErrorResponse { &quot;error&quot;: { &quot;message&quot;: &quot;Service diabetes-service with the same name.&quot;  <\/p>\n<p>Please can you help with deleting the service in question?  <\/p>\n<p>Thanks,  <\/p>\n<p>Naveen  <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610661593007,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/231106\/webserviceexception-how-to-delete-an-existing-serv",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":6.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"WebserviceException: How to delete an existing service",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":65,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello, Naveen. This error message means that in your current AML workspace, there already exists a real-time endpoint(or service) whose name is &quot;diabetes-service&quot;, so you can't deploy a new service with this same name because it will cause duplication.   <\/p>\n<p>You can check your workspace in our portal <a href=\"https:\/\/ml.azure.com\/selectWorkspace\">https:\/\/ml.azure.com\/selectWorkspace<\/a> , in the sidebar you can find a &quot;Endpoints&quot; button, you can find all your &quot;real-time endpoint&quot; there. Then please delete the dup service, after deletion you can deploy your new service with the name &quot;diabetes-service&quot;.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":11.6,
        "Solution_reading_time":8.4,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":85,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1565376125572,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":31.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":10701.8852247222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a endpoint in Amazon SageMaker (Image-classification algorithm) in Jupyter notebook that works fine. In Lambda function works fine too, when I call the Lambda function from API Gateway, from test of API Gateway, works fine too.<\/p>\n<p>The problem is when I call the API from Postman according this answer: <a href=\"https:\/\/stackoverflow.com\/questions\/39660074\/post-image-data-using-postman\">&quot;Post Image data using POSTMAN&quot;<\/a><\/p>\n<p>The code in Lambda is:<\/p>\n<pre><code>import boto3\nimport json\nimport base64\n\nENDPOINT_NAME = &quot;DEMO-XGBoostEndpoint-Multilabel&quot;\nruntime= boto3.client(&quot;runtime.sagemaker&quot;)\nimagen_ = &quot;\/tmp\/imageToProcess.jpg&quot;\n\ndef write_to_file(save_path, data):\n    with open(save_path, &quot;wb&quot;) as f:\n        f.write(base64.b64decode(data))\n\ndef lambda_handler(event, context):\n    img_json = json.loads(json.dumps(event))\n\n    write_to_file(imagen_, json.dumps(event, indent=2))\n\n    with open(imagen_, &quot;rb&quot;) as image:\n        f = image.read()\n        b = bytearray(f)\n\n    payload = b\n\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                       ContentType=&quot;application\/x-image&quot;,\n                                       Body=payload)\n\n    #print(response)\n    result = json.loads(response[&quot;Body&quot;].read().decode())\n    print(result)\n    predicted_label=[]\n    classes = [&quot;chair&quot;, &quot;handbag&quot;, &quot;person&quot;, &quot;traffic light&quot;, &quot;clock&quot;]\n    for idx, val in enumerate(classes):\n        print(&quot;%s:%f &quot;%(classes[idx], result[idx]), end=&quot;&quot;)\n        predicted_label += (classes[idx], result[idx])\n\n    return {\n      &quot;statusCode&quot;: 200,\n      &quot;headers&quot;: { &quot;content-type&quot;: &quot;application\/json&quot;},\n      &quot;body&quot;:  predicted_label\n}\n<\/code><\/pre>\n<p>The error is:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/var\/task\/lambda_function.py&quot;, line 26, in lambda_handler\n    Body=payload)\n  File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 316, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 626, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message &quot;unable to evaluate payload provided&quot;. See https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/DEMO-XGBoostEndpoint-Multilabel in account 866341179300 for more information. ```\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1595742856227,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1595997749087,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63096583",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":18.4,
        "Challenge_reading_time":35.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker: An error occurred (ModelError) when calling the InvokeEndpoint operation: unable to evaluate payload provided",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":3996.0,
        "Challenge_word_count":218,
        "Platform":"Stack Overflow",
        "Poster_created_time":1565376125572,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I resolved with <a href=\"https:\/\/medium.com\/swlh\/upload-binary-files-to-s3-using-aws-api-gateway-with-aws-lambda-2b4ba8c70b8e\" rel=\"nofollow noreferrer\">this<\/a> post:<\/p>\n<p>Thank all<\/p>\n<p>Finally the code in lambda function is:<\/p>\n<pre><code>import os\nimport boto3\nimport json\nimport base64\n\nENDPOINT_NAME = os.environ['endPointName']\nCLASSES = &quot;[&quot;chair&quot;, &quot;handbag&quot;, &quot;person&quot;, &quot;traffic light&quot;, &quot;clock&quot;]&quot;\nruntime= boto3.client(&quot;runtime.sagemaker&quot;)\n\ndef lambda_handler(event, context):\n    file_content = base64.b64decode(event['content'])\n\n    payload = file_content\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType=&quot;application\/x-image&quot;, Body=payload)\n\n    result = json.loads(response[&quot;Body&quot;].read().decode())\n    print(result)\n    predicted_label=[]\n    classes = CLASSES\n    for idx, val in enumerate(classes):\n       print(&quot;%s:%f &quot;%(classes[idx], result[idx]), end=&quot;&quot;)\n       predicted_label += (classes[idx], result[idx])\n\n    return {\n      &quot;statusCode&quot;: 200,\n      &quot;headers&quot;: { &quot;content-type&quot;: &quot;application\/json&quot;},\n      &quot;body&quot;:  predicted_label\n    }\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1634524535896,
        "Solution_link_count":1,
        "Solution_readability":22.8,
        "Solution_reading_time":16.28,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7,
        "Solution_word_count":74,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Does Data Capture feature used for model monitor and analytics work with the multi model endpoint (one container).. we ran into an error.  See error \" An error occurred (ValidationException) when calling the CreateEndPointConfig operation: Data Capture Feature is not supported with MultiModel mode\"\nTheoretically, it should work because it is calling the DataCaptureConfig:\n\nfrom sagemaker.model_monitor import DataCaptureConfig\n\nendpoint_name = 'your-pred-model-monitor-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nprint(\"EndpointName={}\".format(endpoint_name))\n\ndata_capture_config=DataCaptureConfig(\n                        enable_capture = True,\n                        sampling_percentage=100,\n                        destination_s3_uri=s3_capture_upload_path)",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649794559479,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668514048207,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUlAvpGSsISyqu0MyebgRJDA\/sagemaker-multi-model-endpoint-and-inference-data-capture-feature",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":20.0,
        "Challenge_reading_time":9.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker Multi Model EndPoint and Inference Data Capture feature",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":412.0,
        "Challenge_word_count":75,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"SageMaker multi-model endpoints do not have support for SageMaker Model monitor as of writing this answer. So the error is pointing to exactly that. \n\nHowever, if you are looking to implement data drift using sagemaker model monitor then you can do that my mimicking data capture config functionality by capturing inference input and prediction output and storing it in the format supported by Model Monitor. And then setup a customer monitoring container using the instructions listed [https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-byoc-containers.html]()",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1649857157628,
        "Solution_link_count":1,
        "Solution_readability":16.7,
        "Solution_reading_time":7.26,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":77,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1263294862568,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":183045.0,
        "Answerer_view_count":13691.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I followed the aws documentation ( <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config<\/a>) to create a model and to use that model, i coded for a serverless endpoint config (sample code below) ,I have all the required values  but this throws an error below and i'm not sure why<\/p>\n<p>parameter validation failed unknown parameter inProductVariants [ 0 ]: &quot;ServerlessConfig&quot;, must be one of : VairantName, ModelName, InitialInstanceCount , Instancetype...<\/p>\n<pre><code>response = client.create_endpoint_config(\n   EndpointConfigName=&quot;abc&quot;,\n   ProductionVariants=[\n        {\n            &quot;ModelName&quot;: &quot;foo&quot;,\n            &quot;VariantName&quot;: &quot;variant-1&quot;,\n            &quot;ServerlessConfig&quot;: {\n                &quot;MemorySizeInMB&quot;: 1024,\n                &quot;MaxConcurrency&quot;: 2\n            }\n        } \n    ]\n)\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645067620433,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1645073711768,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71152047",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":21.6,
        "Challenge_reading_time":14.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"how to create a serverless endpoint in sagemaker?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":161.0,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Poster_created_time":1590797441983,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":525.0,
        "Poster_view_count":98.0,
        "Solution_body":"<p>You are probably using <strong>old boto3<\/strong> version. <code>ServerlessConfig<\/code> is a very new configuration option. You need to upgrade to the latest version (1.21.1) if possible.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":8.1,
        "Solution_reading_time":2.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4,
        "Solution_word_count":25,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1505194585676,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":35.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to deploy a model to Kubernetes in Azure Machine Learning Studio, it was working for a while, but now, it fails during deployment, the error message is as follows:<\/p>\n<pre><code>Deploy: Failed on step WaitServiceCreating. Details: AzureML service API error. \nYour container application crashed. This may be caused by errors in your scoring file's init() function.\nPlease check the logs for your container instance: pipeline-created-on-07-28-2020-r.\nFrom the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\nYou can also try to run image viennaglobal.azurecr.io\/azureml\/azureml_6ae744633f749472feb283065055dc2c:latest locally.\nPlease refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\n<\/code><\/pre>\n<pre class=\"lang-js prettyprint-override\"><code>{\n    &quot;code&quot;: &quot;KubernetesDeploymentFailed&quot;,\n    &quot;statusCode&quot;: 400,\n    &quot;message&quot;: &quot;Kubernetes Deployment failed&quot;,\n    &quot;details&quot;: [\n        {\n            &quot;code&quot;: &quot;CrashLoopBackOff&quot;,\n            &quot;message&quot;: &quot;Your container application crashed. This may be caused by errors in your scoring file's init() function.\nPlease check the logs for your container instance: pipeline-created-on-07-28-2020-r. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image viennaglobal.azurecr.io\/azureml\/azureml_6ae744633f749472feb283065055dc2c:latest locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.&quot;\n        }\n    ]\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1595913837627,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1595945366092,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63127521",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":10.5,
        "Challenge_reading_time":21.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"Deploying Model to Kubernetes",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":186.0,
        "Challenge_word_count":178,
        "Platform":"Stack Overflow",
        "Poster_created_time":1505194585676,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>It seems it was a bug, got corrected by itself today. Closing this question now<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":3.9,
        "Solution_reading_time":1.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2,
        "Solution_word_count":15,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\nIf you run \r\n\r\npy_test(\r\n name = \"sigopt_prior_beliefs_example\",\r\n size = \"medium\",\r\n srcs = [\"examples\/sigopt_prior_beliefs_example.py\"],\r\n deps = [\":tune_lib\"],\r\n tags = [\"exclusive\", \"example\"],\r\n args = [\"--smoke-test\"]\r\n)\r\n\r\nin python\/ray\/tune\/build (this part of the testing is commented out since you need a sigopt API key...)\r\nYou get an output that looks like this:\r\n\r\n\"\"\"\r\n...\r\n  File \"\/usr\/local\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 737, in _process_trial\r\n    self._validate_result_metrics(result)\r\n  File \"\/usr\/local\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 818, in _validate_result_metrics\r\n    elif search_metric and search_metric not in result:\r\nTypeError: unhashable type: 'list'\r\n...\r\n\"\"\"\r\nray 1.1.0.dev\r\n\r\n### Reproduction (REQUIRED)\r\nin python\/ray\/tune\/build  run the sigopt sections that are commented out.\r\n",
        "Challenge_closed_time":1603498,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603479422000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ray-project\/ray\/issues\/11581",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.8,
        "Challenge_reading_time":12.71,
        "Challenge_repo_contributor_count":425.0,
        "Challenge_repo_fork_count":4048.0,
        "Challenge_repo_issue_count":30819.0,
        "Challenge_repo_star_count":23050.0,
        "Challenge_repo_watch_count":435.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"[Tune] Sigopt (multi-metric) api fails with 1.1.0 (tries to hash list)",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":102,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This is a consequence of search metric being able to be multi-metric. cc @krfricke \r\n\r\nAlso, let me ping the sigopt folks for a working API key... Should be fixed on #11583 . We'll pick this onto the release.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":3.4,
        "Solution_reading_time":2.45,
        "Solution_score_count":null,
        "Solution_sentence_count":4,
        "Solution_word_count":37,
        "Tool":"SigOpt"
    },
    {
        "Answerer_created_time":1467237684900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":613.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I've trained a model on sagemaker and have created the endpoint. I'm trying to invoke the endpoint using postman. But when training the model and even after that, I have not specified any header for the training data. I'm at a loss as to how to create payload while sending a post request to sagemaker<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1522940592383,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49675637",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.0,
        "Challenge_reading_time":4.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How to pass a request to sagemaker using postman",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":6746.0,
        "Challenge_word_count":63,
        "Platform":"Stack Overflow",
        "Poster_created_time":1410972175307,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1124.0,
        "Poster_view_count":153.0,
        "Solution_body":"<p>Once the endpoint is created, you can invoke it as any other restful service, with credentials and payload. <\/p>\n\n<p>I am guessing, there could be two places where might be stuck. \nOne could be, sending an actual PostMan Request with all the headers and everything. \nNewer version of Postman has AWS Signature as one of the Authorization types. You can use that to invoke the service. There are no other spacial headers required. Note that there is a bug in Postman still open (<a href=\"https:\/\/github.com\/postmanlabs\/postman-app-support\/issues\/1663\" rel=\"noreferrer\">issue-1663<\/a>) that only affects if you are a AWS federated account. Individual accounts should not be affected by this issue. <\/p>\n\n<p>Or, you could be stuck at the actual payload. When you invoke the SageMaker endpoint, the payload is passed as is to the model. If you want to preprocess the input before feeding it to the model, you'd have to implement an input_fn method and specify that when instantiating the model. <\/p>\n\n<p>You might also be able to invoke SageMaker endpoint using AWS SDK boto3 as follows <\/p>\n\n<pre><code>import boto3\nruntime= boto3.client('runtime.sagemaker')\n\npayload = getImageData()\n\n\nresult  = runtime.invoke_endpoint(\n    EndpointName='my_endpoint_name',\n    Body=payload,\n    ContentType='image\/jpeg'\n)\n<\/code><\/pre>\n\n<p>Hope this helps.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":9.4,
        "Solution_reading_time":16.63,
        "Solution_score_count":11.0,
        "Solution_sentence_count":15,
        "Solution_word_count":189,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In my AML pipeline, I've got a model built and deployed to the AciWebservice. I now have a need to include some additional data that would be used by score.py. This data is in json format (~1mb) and is specific to the model that's built. To accomplish this, I was thinking of sticking this file in blob store and updating some \"placholder\" vars in the score.py during deployment, but it seems hacky. <\/p>\n\n<p>Here are some options I was contemplating but wasn't sure on the practicality<\/p>\n\n<p><strong>Option 1:<\/strong>\nIs it possible to include this file, during the model deployment itself so that it's part of the docker image? <\/p>\n\n<p><strong>Option 2:<\/strong>\nAnother possibility I was contemplating, would it be possible to include this json data part of the Model artifacts?<\/p>\n\n<p><strong>Option 3:<\/strong>\nHow about registering it as a dataset and pull that in the score file?<\/p>\n\n<p>What is the recommended way to deploy dependent files in a model deployment scenario?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1589475408817,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1589479621848,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61803031",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.8,
        "Challenge_reading_time":12.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML: Include additional files during model deployment",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":866.0,
        "Challenge_word_count":167,
        "Platform":"Stack Overflow",
        "Poster_created_time":1330016065408,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1704.0,
        "Poster_view_count":232.0,
        "Solution_body":"<p>There are few ways to accomplish this:<\/p>\n\n<ol>\n<li><p>Put the additional file in the same folder as your model file, and <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none--sample-input-dataset-none--sample-output-dataset-none--resource-configuration-none-\" rel=\"nofollow noreferrer\">register<\/a> the whole folder as the model. In this approach the file is stored alongside the model.<\/p><\/li>\n<li><p>Put the file in a local folder, and specify that folder as source_directory in <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.inferenceconfig?view=azure-ml-py\" rel=\"nofollow noreferrer\">InferenceConfig<\/a>. In this approach the file is re-uploaded every time you deploy a new endpoint.<\/p><\/li>\n<li><p>Use custom base image in InferenceConfig to bake the file into Docker image itself.<\/p><\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":18.8,
        "Solution_reading_time":14.5,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7,
        "Solution_word_count":88,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1439246522636,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I try to test my Azure ML model, I get the following error: \u201cError code: InternalError, Http status code: 500\u201d, so it appears something is failing inside of the machine learning service. How do I get around this error?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1439847861947,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1439906222223,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/32060196",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":3.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML Internal Error",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1408.0,
        "Challenge_word_count":43,
        "Platform":"Stack Overflow",
        "Poster_created_time":1439847618300,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>I've run into this error before, and unfortunately, the only workaround I found was to create a new ML workspace backed by a storage account that you know is online. Then copy your experiment over to the new workspace, and things should work. It can be a bit cumbersome, but it should get rid of your error message. With the service being relatively new, things sometimes get corrupted as updates are being made, so I recommend checking the box labeled \"disable updates\" within your experiment.  Hope that helps!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":9.0,
        "Solution_reading_time":6.33,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5,
        "Solution_word_count":88,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I've searched for hours for this and can't find a single thing that answers the question. I've created and published a new Azure Machine Learning service, and have created an endpoint. I can call the service using the Postman REST CLient, but accessing it via a JavaScript webpage returns a console log saying that CORS is enabled for the service. Now, for the life of me, I can't figure out how to disable CORS for Azure Machine Learning services. Any help would be much appreciated, thanks!<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":4,
        "Challenge_created_time":1421424031317,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1454343355232,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/27987910",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":6.7,
        "Challenge_reading_time":6.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":12,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning - CORS",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":3242.0,
        "Challenge_word_count":89,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403948655636,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":135.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>Currently, we don't support disabling CORS on API side but you can either use the above option or you can use the API management service to disable CORS. The links below should help you with this<\/p>\n\n<p>Here are the links: <a href=\"http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/api-management-get-started\/\" rel=\"noreferrer\">step by step<\/a> guide, also this <a href=\"http:\/\/channel9.msdn.com\/Blogs\/AzureApiMgmt\/Last-mile-Security\" rel=\"noreferrer\">video<\/a> on setting headers, and <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn894084.aspx#JSONP\" rel=\"noreferrer\">this doc<\/a> on policies.<\/p>\n\n<p>API Management service allow CORS by enabling it in the API configuration page<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3,
        "Solution_readability":11.6,
        "Solution_reading_time":9.27,
        "Solution_score_count":4.0,
        "Solution_sentence_count":6,
        "Solution_word_count":74,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1347347916327,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":271.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When publishing an Azure ML Web Service and preloading data in our R model we see inconsistent performance. First calls are slow but following calls are fast, waiting a bit (couple of minutes) for the next call ends up showing longer response times.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1466509663807,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1466610116060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37943572",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":3.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML Web Service for R models shows unpredictable",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":69.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_created_time":1466503688920,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":64.0,
        "Solution_body":"<p>The way Azure ML Web Services work in the background means that instances hosting the models are provisioned and moved in a very dynamic multi-tenant environment. Caching data (warming up) can be helpful but this doesn't mean all subsequent calls will land on the same instance with the same data available in the cache. <\/p>\n\n<p>For models that need a lot of in-memory data there is a limit to what the Azure ML Web Services hosting layer can offer at this point. Microsoft R server could be an alternative to host these big ML workloads and looking at Service Fabric to scale <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":11.0,
        "Solution_reading_time":7.08,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":103,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1585590244876,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":55.0,
        "Answerer_view_count":12.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>We created a webservice endpoint and tested it with the following code, and also with POSTMAN.<\/p>\n\n<p>We deployed the service to an AKS in the same resource group and subscription as the AML resource.<\/p>\n\n<p><strong>UPDATE: the attached AKS had a custom networking configuration and rejected external connections.<\/strong><\/p>\n\n<pre><code>import numpy\nimport os, json, datetime, sys\nfrom operator import attrgetter\nfrom azureml.core import Workspace\nfrom azureml.core.model import Model\nfrom azureml.core.image import Image\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.authentication import AzureCliAuthentication\n\ncli_auth = AzureCliAuthentication()\n# Get workspace\nws = Workspace.from_config(auth=cli_auth)\n\n# Get the AKS Details\ntry:\n    with open(\"..\/aml_config\/aks_webservice.json\") as f:\n        config = json.load(f)\nexcept:\n    print(\"No new model, thus no deployment on AKS\")\n    # raise Exception('No new model to register as production model perform better')\n    sys.exit(0)\n\nservice_name = config[\"aks_service_name\"]\n# Get the hosted web service\nservice = Webservice(workspace=ws, name=service_name)\n\n# Input for Model with all features\ninput_j = [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]]\nprint(input_j)\ntest_sample = json.dumps({\"data\": input_j})\ntest_sample = bytes(test_sample, encoding=\"utf8\")\ntry:\n    prediction = service.run(input_data=test_sample)\n    print(prediction)\nexcept Exception as e:\n    result = str(e)\n    print(result)\n    raise Exception(\"AKS service is not working as expected\")\n<\/code><\/pre>\n\n<p>In AML Studio, the deployment state is \"Healthy\".<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/RTB10.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RTB10.png\" alt=\"Endpoint attributes\"><\/a><\/p>\n\n<p>We get the following error when testing:<\/p>\n\n<pre><code>Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'\n<\/code><\/pre>\n\n<p><strong>Log just after deploying the AKS Webservice <a href=\"http:\/\/t.ly\/t79b\" rel=\"nofollow noreferrer\">here<\/a>.<\/strong><\/p>\n\n<p><strong>Log after running the test script <a href=\"http:\/\/t.ly\/79k5\" rel=\"nofollow noreferrer\">here<\/a>.<\/strong><\/p>\n\n<p>How can we know what is causing this problem and fix it?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592508291480,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1592590061567,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62457880",
        "Challenge_link_count":4,
        "Challenge_participation_count":3,
        "Challenge_readability":12.7,
        "Challenge_reading_time":30.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":null,
        "Challenge_title":"AML - Web service TimeoutError",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":332.0,
        "Challenge_word_count":275,
        "Platform":"Stack Overflow",
        "Poster_created_time":1585590244876,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>We checked the AKS networking configuration and realized it has an Azure CNI profile.<\/p>\n\n<p>In order to test the webservice we need to do it from inside the created virtual network.\nIt worked well!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":6.5,
        "Solution_reading_time":2.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3,
        "Solution_word_count":34,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1589738451347,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":179.0,
        "Answerer_view_count":53.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am having trouble contacting an AMLS web service hosted on AKS in a vnet. I am able to successfully provision AKS and deploy the models, but I am not able to access the web service using the Python requests module:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>headers = {'Content-Type':'application\/json',\n           'Authorization': 'Bearer ' + &lt;AKS_KEY&gt;}\nresp = requests.post(&lt;AKS_URI&gt;, json={&quot;data&quot;:{&quot;x&quot;: &quot;1&quot;}}, headers=headers)\nprint(resp.text)\n<\/code><\/pre>\n<p>I get the following error:<\/p>\n<blockquote>\n<p>Error: HTTPConnectionPool(host='', port=80): Max retries exceeded with url: &lt;AKS_URL&gt; (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f33f6035a10&gt;: Failed to establish a new connection: [Errno 110] Connection timed out'))<\/p>\n<\/blockquote>\n<p>However, I am able to successfully connect to the web service using Postman:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>curl --location --request POST &lt;AKS_URI&gt; \\\n--header 'Authorization: Bearer &lt;AKS_KEY&gt;' \\\n--header 'Content-Type: application\/json' \\\n--data-raw '{&quot;data&quot;: {&quot;x&quot;: &quot;1&quot;}}'\n<\/code><\/pre>\n<p>If I load the AKS service in my AMLS workspace <code>aks_service.run()<\/code> also gives me the same error message. I don't have these problems when I deploy without vnet integration.<\/p>\n<p>What could be causing this?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":4,
        "Challenge_created_time":1600383486567,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1600397592800,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63947132",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":13.9,
        "Challenge_reading_time":19.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Trouble connecting to AMLS web service on AKS using Python requests",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":76.0,
        "Challenge_word_count":164,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589738451347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":179.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>I fixed this by adding an inbound security rule enabled for the scoring endpoint in the NSG group that controls the virtual network.<\/p>\n<p>This should be done so that the scoring endpoint can be called from outside the virtual network (see <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-secure-inferencing-vnet\" rel=\"nofollow noreferrer\">documentation<\/a>), but apparently Postman can figure out how to access the endpoint without this security rule!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":13.5,
        "Solution_reading_time":6.23,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3,
        "Solution_word_count":60,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":1.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Customer wants to host multiple DNN models on same SageMaker container due to latency concerns. Customer does not want to spin-up different containers for each model due to network adding additional latency. Thus, my customer asked me a question below -\n\n> Can one SageMaker host more than one model? Each model then share the\n> same input and produce different outputs concatenated together?\n\nI answered as below -\n\nYes. Amazon SageMaker supports you hosting multiple models in several different ways \u2013\n\n 1. Using Multi-model Inference endpoints: \nAmazon SageMaker supports serving multiple models from same Inference endpoint. Details can be found [here](1). The sample code can be found [here](2).  Currently, this feature do not support Elastic Inference or serial inference pipelines. Multi-model endpoints also enable time-sharing of memory resources across your models. This works best when the models are fairly similar in size and invocation latency. When this is the case, multi-model endpoints can effectively use instances across all models. If you have models that have significantly higher transactions per second (TPS) or latency requirements, we recommend hosting them on dedicated endpoints. Multi-model endpoints are also well suited to scenarios that can tolerate occasional cold-start-related latency penalties that occur when invoking infrequently used models\n\n 2. Using Bring your own algorithm on SageMaker\nYou can also bring your own container with your own libs and runtime\/programming language for serving and training. See the example notebook on how you can bring your own algorithm\/container image on sagemaker [here](3)\n\n 3. Using Multi-model serving container by using multi-model archive file\n      You can find a sample example here [4] for tensorflow serving\n 4. If models are called sequentially, the SageMaker inference pipeline allows you to chain up to 5 models called one after the other on the same endpoint\nSagemaker endpoints include optimizations that will save costs, such as (1) 1-click deploy to pre-configured environments for popular ML frameworks with a managed serving stack, (2) autoscaling, (3) model compilation, (4) cost-effective hardware acceleration via Elastic Inference, (5) multi-variant model deployment for testing and overlapped model replacement, (6) multi-AZ backend. It is not necessarily a good idea to have multiple models on same endpoint (unless you have the reasons and requirements I mentioned in Option A above). Having one model per endpoint creates an isolation which has positive benefits on fault tolerance, security and scalability. Please keep in mind that SageMaker works on containers that runs on top of EC2.\n\n[1]https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/\n\n[2]https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb\n\n[3]https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\n\n[4]https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst#deploying-more-than-one-model-to-your-endpoint\n\n[5]https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\n\nAm I missing anything? Any other suggestions in terms of other approaches?",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1587366119000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668583474750,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUfmnWJIIZQs6_2K1uIH9stQ\/sagemaker-with-multiple-models",
        "Challenge_link_count":5,
        "Challenge_participation_count":1,
        "Challenge_readability":15.7,
        "Challenge_reading_time":44.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker with multiple models",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":703.0,
        "Challenge_word_count":419,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"> Customer does not want to spin-up different containers for each model due to network adding additional latency. \n\nI am assuming this is a pipeline scenario where different models need to be chained.\nIf so, it's important to keep in mind that all containers in pipeline run on the __same EC2 instance__ so that \"inferences run with low latency because the containers are co-located on the same EC2 instances.\"[1]\n\nHope this is useful.   \n[1] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925593992,
        "Solution_link_count":1,
        "Solution_readability":11.7,
        "Solution_reading_time":6.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":74,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1460664823627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":81.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I wanted to know if there is a way to call the Azure Machine Learning webservice using JavaScript Ajax.<\/p>\n\n<p>The Azure ML gives sample code for C#, Python and R.<\/p>\n\n<p>I did try out to call the webservice using JQuery Ajax but it returns a failure.<\/p>\n\n<p>I am able to call the same service using a python script.<\/p>\n\n<p>Here is my Ajax code : <\/p>\n\n<pre><code>  $.ajax({\n        url: webserviceurl,\n        type: \"POST\",           \n        data: sampleData,            \n        dataType:'jsonp',                        \n        headers: {\n        \"Content-Type\":\"application\/json\",            \n        \"Authorization\":\"Bearer \" + apiKey                       \n        },\n        success: function (data) {\n          console.log('Success');\n        },\n        error: function (data) {\n           console.log('Failure ' +  data.statusText + \" \" + data.status);\n        },\n  });\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1464104729493,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1526047792276,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37418265",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":7.9,
        "Challenge_reading_time":9.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning using Javascript Ajax call",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1607.0,
        "Challenge_word_count":94,
        "Platform":"Stack Overflow",
        "Poster_created_time":1460664823627,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":81.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>Well after a lot of RnD, I was able to finally call Azure ML using some workarounds.<\/p>\n\n<p>Wrapping Azure ML webservice on Azure API is one option.<\/p>\n\n<p>But, what I did was that I created a python webservice which calls the Azure webservice.<\/p>\n\n<p>So now my HTML App calls the python webservice which calls Azure ML and returns data to the HTML App.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":5.9,
        "Solution_reading_time":4.38,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4,
        "Solution_word_count":63,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1521276815912,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":731.0,
        "Answerer_view_count":41.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to download the best performing model for a certain ClearlML project. I have the following content in my ClearML experiment platform:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/LAWT9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LAWT9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>According to: <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/clearml_sdk\/model_sdk#querying-models\" rel=\"nofollow noreferrer\">https:\/\/clear.ml\/docs\/latest\/docs\/clearml_sdk\/model_sdk#querying-models<\/a> I can get a list of models for a specific project:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model_list = Model.query_models(\n    # Only models from `examples` project\n    project_name='YOLOv5', \n    # Only models with input name\n    model_name=None,\n    # Only models with `demo` tag but without `TF` tag\n    tags=['demo', '-TF'],\n    # If `True`, only published models\n    only_published=False,\n    # If `True`, include archived models\n    include_archived=True,\n    # Maximum number of models returned\n    max_results=5\n)\n\nprint(model_list)\n<\/code><\/pre>\n<p>Which prints:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>[&lt;clearml.model.Model object at 0x7fefbaf22130&gt;, &lt;clearml.model.Model object at 0x7fefbaf22340&gt;]\n<\/code><\/pre>\n<p>So I can run:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model_list[0].get_local_copy()\n<\/code><\/pre>\n<p>and get this specific model. But how do I download the best performing one for this project on a certain metric (in this case mAP_0.5:0.95 MAX)?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662537777000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1662645914387,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73632015",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":20.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"ClearML, how to query the best performing model for a specific project and metric",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":48.0,
        "Challenge_word_count":157,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521276815912,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":731.0,
        "Poster_view_count":41.0,
        "Solution_body":"<p>I ended up doing the following:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>try:\n    import clearml\n    from clearml import Dataset, Task, Model, OutputModel\n    assert hasattr(clearml, '__version__')  # verify package import not local dir\nexcept (ImportError, AssertionError):\n    clearml = None\n\ntasks = Task.get_tasks(project_name='YOLOv5', task_name='exp', task_filter={'status': ['completed']})\n\nresults = {}\nbest_task = None\nfor task in tasks:\n    results[task.id] = task.get_last_scalar_metrics()['metrics']['mAP_0.5:0.95']['max']\n\nbest_model_task_id = max(results, key=results.get)\nmodel_list = Task.get_task(best_model_task_id).get_models()\ndest = model_list['output'][0].get_local_copy()\nprint('Saved model at:', dest)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1662553432007,
        "Solution_link_count":0,
        "Solution_readability":15.3,
        "Solution_reading_time":9.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6,
        "Solution_word_count":58,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":37.2757888889,
        "Challenge_answer_count":1,
        "Challenge_body":"I found a similar posting [here](https:\/\/repost.aws\/questions\/QUEVxelof3TmimoLt1Kd1SBA\/how-to-configure-our-own-inference-py-for-two-different-py-torch-models-in-multi-data-model-to-build-single-endpoint-and-call-both-models-from-there) but I'm hoping my situation is a little simpler.\n\n---\n\n**Q: Is there a way to provide two separate inference scripts for each model in the multi-endpoint or does some dynamic\/custom inference script need to be made to handle both?**\n\nI have two model pipeline built using SageMaker Python SDK Scikit-learn processing\/models:\n* One is a clustering model to return cluster prediction and centroid distances when requesting inferencing\n* Other is simply PCA, returning 3-components when requesting inferencing\n---\n\n\nBecause of the odd formating of the data and the way that the output needs to be provided, both models are using custom inference scripts (e.g. predicting vs. transformation). \n\n\nFrom what I can see in the examples for MultiDataModel, it only accepts a single entry_point for inference.py when passing the model information and just the model artifacts are \"added\" later:\n\n```\ncluster_model = SKLearnModel(\n    model_data=cluster_artifact,\n    role=role,\n    entry_point=\"scripts\/cluster_inference.py\",\n    sagemaker_session=sagemaker_session\n)\npca_model = SKLearnModel(\n    model_data=pca_artifact,\n    role=role,\n    entry_point=\"scripts\/pca_inference.py\",\n    sagemaker_session=sagemaker_session\n)\nmme = MultiDataModel(\n    name='model',\n    model_data_prefix=model_data_prefix,\n    model= cluster_model,\n    sagemaker_session=sagemaker_session,\n)\n```\n---\n\n\nDeployed as separate endpoints, both perform inference as expected, but I cannot get to function as one endpoint.\n\n\nBelow is the most recent error I receive, but it is hard to understand where is failing, seeing as the serialization should be handled properly in my inference script and when invoking the endpoint:\n\n\n```\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (503) from primary with message \"{\n  \"code\": 503,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Unsupported model output data type.\"}\".\n```\n\nAny pointers or alternatives are appreciated!\nThanks",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1678900781216,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1679248487591,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUVF03ZNbBTL28_SaJ4RotQg\/multidatamodel-with-different-inference-scripts",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":19.3,
        "Challenge_reading_time":28.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"MultiDataModel with different inference scripts",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":70.0,
        "Challenge_word_count":251,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi there!\n\nIn short, our SageMaker scikit-learn container do not currently support model specific inference script.\n\nThe entry_point script that you referenced in your MultiDataModel object will be the inference script used for all the models. If you've added logging in your script, you will be able to see them in CloudWatch logs.\n\nIf you have some pre\/post-processing script that needs to be performed on a specific model, you need to write them all in one universal inference.py . You can then add some extra attributes in your data when invoking the endpoint and have the same script read the extra attribute so it knows which pre\/post-processing script to perform.\n\nOne important thing to note is that although you reference a model object in MultiDataModel, i.e.\n\n```\n\nmme = MultiDataModel(\n    name='model',\n    model_data_prefix=model_data_prefix,\n    model= cluster_model,\n    sagemaker_session=sagemaker_session,\n)\n```\n\n\nThe only information fetched from the model object is the image_uri and entry_point, these information is needed for during endpoint deployment.\n\nAll the model.tar.gz sitting in 'model_data_prefix' should not have a inference.py as it confuses the container and forces it to go back to default handler, hence why you're probably receiving ModelError.\n\n\nTry something like below:\n\n\n```\ncluster_model = SKLearnModel(\n    model_data=cluster_artifact,\n    role=role,\n    entry_point=\"scripts\/cluster_inference.py\",\n    sagemaker_session=sagemaker_session\n)\npca_model = SKLearnModel(\n    model_data=pca_artifact,\n    role=role,\n    entry_point=\"scripts\/pca_inference.py\",\n    sagemaker_session=sagemaker_session\n)\nmme = MultiDataModel(\n    name='model',\n    model_data_prefix=model_data_prefix,\t#make sure the directory of this prefix is empty, i.e. no models in this location\n    model= cluster_model,\n    sagemaker_session=sagemaker_session,\n)\n\n\nlist(mme.list_models()) # this should be empty\n\nmme.add_model(model_data_source=cluster_artifact, model_data_path='cluster_artifact.tar.gz')\t#make sure model artifact doesn't contain inference.py\nmme.add_model(model_data_source=pca_artifact, model_data_path='pca_artifact.tar.gz') #make sure model artifact doesn't contain inference.py\n\nlist(mme.list_models()) # there should be two models listed now, if you look at the location of model_data_prefix, there should also be two model artifact\n\noutput_cluster = predictor.predict(data='<your-data>', target_model='cluster_artifact.tar.gz')\nprint(output_cluster) #this should work since it's using the inference.py from cluster_inference.py\n\noutput_pca = predictor.predict(data='<your-data>', target_model='pca_artifact.tar.gz')\nprint(output_pca) \t#this might fail since it's using cluster_inference.py, add this model's inference script into cluster_inference.py to make it work \n\n```\n\nI know this approach is not ideal since if you have a new model with new pre\/post-processing script, you'd have to redeploy your endpoint for the new script to come into effect.\n\nWe actually just added support in our tensorflow container to allow model specific inference script: https:\/\/github.com\/aws\/deep-learning-containers\/pull\/2680\n\nYou can request for the same feature for our scikit container here: https:\/\/github.com\/aws\/sagemaker-scikit-learn-container\/issues",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1679382680431,
        "Solution_link_count":2,
        "Solution_readability":14.7,
        "Solution_reading_time":41.25,
        "Solution_score_count":0.0,
        "Solution_sentence_count":26,
        "Solution_word_count":365,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1565391938427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":23.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to invoke my sagemaker model using aws chalice, a lambda function, and an API Gateaway.<\/p>\n\n<p>I'm attempting to send the image over <code>POST<\/code> request but I'm having problem receiving it on the lambda function.<\/p>\n\n<p>My code looks like:<\/p>\n\n<pre><code>from chalice import Chalice\nfrom chalice import BadRequestError\nimport base64\nimport os\nimport boto3\nimport ast\nimport json\n\napp = Chalice(app_name='foo')\napp.debug = True\n\n\n@app.route('\/', methods=['POST'], content_types=['application\/json'])\ndef index():\n    body = ''\n\n    try:\n        body = app.current_request.json_body # &lt;- I suspect this is the problem\n        return {'response': body}\n    except Exception as e:\n        return  {'error':  str(e)}\n<\/code><\/pre>\n\n<p>It's just returning<\/p>\n\n<p><code>&lt;Response [200]&gt; {'error': 'BadRequestError: Error Parsing JSON'}<\/code><\/p>\n\n<p>As I mentioned before, my end goal is to receive my image and make a sagemaker request with it. But I just can't seem to read the image. <\/p>\n\n<p>My python test client looks like this:<\/p>\n\n<pre><code>import base64, requests, json\n\ndef test():\n\n    url = 'api_url_from_chalice'\n    body = ''\n\n    with open('b1.jpg', 'rb') as image:\n        f = image.read()\n        body = base64.b64encode(f)\n\n    payload = {'data': body}\n    headers = {'Content-Type': 'application\/json'}\n\n    r = requests.post(url, data=payload, headers=headers)\n    print(r)\n    r = r.json()\n    # r = r['response']\n\n    print(r)\n\ntest()\n<\/code><\/pre>\n\n<p>Please help me, I spent way to much time trying to figure this out<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1585411767370,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1585412572920,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60903256",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.6,
        "Challenge_reading_time":19.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS Chalice, can't get image from POST request",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1229.0,
        "Challenge_word_count":192,
        "Platform":"Stack Overflow",
        "Poster_created_time":1565391938427,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>So I was able to figure it out with the help of an aws engineer (i got lucky I suppose). I'm including the complete lambda function. Nothing changed on the client.<\/p>\n\n<pre><code>from chalice import Chalice\nfrom chalice import BadRequestError\nimport base64\nimport os\nimport boto3\nimport ast\nimport json\nimport sys\n\n\nfrom chalice import Chalice\nif sys.version_info[0] == 3:\n    # Python 3 imports.\n    from urllib.parse import urlparse, parse_qs\nelse:\n    # Python 2 imports.\n    from urlparse import urlparse, parse_qs\n\napp = Chalice(app_name='app_name')\napp.debug = True\n\n\n@app.route('\/', methods=['POST'])\ndef index():\n    parsed = parse_qs(app.current_request.raw_body.decode())\n\n    body = parsed['data'][0]\n    print(type(body))\n\n    try:\n        body = base64.b64decode(body)\n        body = bytearray(body)\n    except e:\n        return {'error': str(e)}\n\n\n    endpoint = \"object-detection-endpoint_name\"\n    runtime = boto3.Session().client(service_name='sagemaker-runtime', region_name='us-east-2')\n\n    response = runtime.invoke_endpoint(EndpointName=endpoint, ContentType='image\/jpeg', Body=body)\n\n    print(response)\n    results = response['Body'].read().decode(\"utf-8\")\n    results = results['predictions']\n\n    results = json.loads(results)\n    results = results['predictions']\n\n    return {'result': results}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":12.3,
        "Solution_reading_time":16.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15,
        "Solution_word_count":120,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1302088276340,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Dubai, United Arab Emirates",
        "Answerer_reputation_count":5263.0,
        "Answerer_view_count":637.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Working on a IoT telemetry project that receives humidity and weather pollution data from different sites on the field. I will then apply Machine Learning on the collected data. I'm using Event Hubs and Stream Analytics. Is there a way of pulling the data to Azure Machine Learning without the hassle of writing an application to get it from Stream Analytics and push to AML web service?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1467278958850,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38119062",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.1,
        "Challenge_reading_time":5.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Pulling data from Stream Analytics to Azure Machine Learning",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":627.0,
        "Challenge_word_count":75,
        "Platform":"Stack Overflow",
        "Poster_created_time":1311017514580,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Beirut, Lebanon",
        "Poster_reputation_count":408.0,
        "Poster_view_count":32.0,
        "Solution_body":"<p>Stream Analytics has a functionality called the \u201c<a href=\"https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/12\/10\/azure-ml-now-available-as-a-function-in-azure-stream-analytics\/\" rel=\"nofollow\">Functions<\/a>\u201d. You can call any web service you\u2019ve published using AML from within Stream Analytics and apply it within your Stream Analytics query. Check this <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/stream-analytics-machine-learning-integration-tutorial\/\" rel=\"nofollow\">link for a tutorial<\/a>.\nExample workflow in your case would be like the following;<\/p>\n\n<ul>\n<li>Telemetry arrives and reaches Stream Analytics<\/li>\n<li>Streaming Analytics (SA) calls the Machine Learning function to apply it on the data<\/li>\n<li>SA redirects it to the output accordingly, here you can use the PowerBI to create a predictions dashboards.<\/li>\n<\/ul>\n\n<p>Another way would be using R, and here\u2019s a good tutorial showing that <a href=\"https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/12\/10\/azure-ml-now-available-as-a-function-in-azure-stream-analytics\/\" rel=\"nofollow\">https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/12\/10\/azure-ml-now-available-as-a-function-in-azure-stream-analytics\/<\/a> . \nIt is more work of course but can give you more control as you control the code.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_readability":19.6,
        "Solution_reading_time":17.45,
        "Solution_score_count":3.0,
        "Solution_sentence_count":7,
        "Solution_word_count":123,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1367358004727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA",
        "Answerer_reputation_count":6421.0,
        "Answerer_view_count":642.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have one live AWS Sagemaker endpoint where we have auto scaled enabled. \nNow I want to updated it from 'ml.t2.xlarge' to 'ml.t2.2xlarge' but it is showing this error <\/p>\n\n<pre><code>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the \nUpdateEndpoint operation: The variant(s) \"[config1]\" must be deregistered as scalable targets with \nApplication Auto Scaling before they can be removed or have their instance type updated.\n<\/code><\/pre>\n\n<p>I believe we need to first de-register auto-scaling using this link \n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-delete.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-delete.html<\/a><\/p>\n\n<p>but I doubt if will take our application down and the new model with training will take multiple hours. We can't afford this so please let me know if there are any better way to do it.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582793187027,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60429339",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":13.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Update live AWS Sagemaker auto scaled endpoint instance type without putting it down",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":869.0,
        "Challenge_word_count":129,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501403168107,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":1370.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>You should have no problem updating your Endpoint instance type without taking the availability hit. The basic method looks like this when you have an active autoscaling policy:<\/p>\n\n<ol>\n<li>Create a new EndpointConfig that uses the new instance type, <code>ml.t2.2xlarge<\/code>\n\n<ol>\n<li>Do this by calling <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateEndpointConfig.html\" rel=\"nofollow noreferrer\"><code>CreateEndpointConfig<\/code><\/a>.<\/li>\n<li>Pass in the same values you used for your previous Endpoint config. You can point to the same <code>ModelName<\/code> that you did as well. By reusing the same model, you don't have to retrain it or anything<\/li>\n<\/ol><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-delete.html\" rel=\"nofollow noreferrer\">Delete the existing autoscaling policy<\/a>\n\n<ol>\n<li>Depending on your autoscaling, you might want to increase the desired count of your Endpoint in the event it needs to scale while you are doing this.<\/li>\n<li>If you are experience a spike in traffic while you are making these API calls, you risk an outage of your model if it can't keep up with traffic. Just keep this in mind and possibly scale in advance for this possibility.<\/li>\n<\/ol><\/li>\n<li>Call <code>UpdateEndpoint<\/code> like you did previously and specify this new <code>EndpointConfigName<\/code><\/li>\n<li>Wait for your Endpoint status to be <code>InService<\/code>. This should take 10-20 mins.<\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-add-policy.html\" rel=\"nofollow noreferrer\">Create a new autoscaling policy<\/a> for this new Endpoint and production variant<\/li>\n<\/ol>\n\n<p>You should be good to go without sacrificing availability.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3,
        "Solution_readability":11.7,
        "Solution_reading_time":22.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14,
        "Solution_word_count":215,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1491898605956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sydney NSW, Australia",
        "Answerer_reputation_count":161.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am looking at <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/sklearn_abalone_featurizer.py\" rel=\"nofollow noreferrer\">this example<\/a> to implement the data processing of incoming raw data for a sagemaker endpoint prior to model inference\/scoring. This is all great but I have 2 questions:<\/p>\n<ul>\n<li>How can one debug this (e.g can I invoke endpoint without it being exposed as restful API and then use Sagemaker debugger)<\/li>\n<li>Sagemaker can be used &quot;remotely&quot; - e.g. via VSC. Can such a script be uploaded programatically?<\/li>\n<\/ul>\n<p>Thanks.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651131440200,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72039744",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.0,
        "Challenge_reading_time":9.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"debug and deploy featurizer (data processor for imodel inference) of sagemaker endpoint",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":33.0,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>Sagemaker Debugger is only to monitor the training jobs.<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/train-debugger.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/train-debugger.html<\/a><\/p>\n<p>I dont think you can use it on Endpoints.<\/p>\n<p>The script that you have provided is used both for training and inference. The container used by the estimator will take care of what functions to run. So it is not possible to debug the script directly. But what are you debugging in the code ? Training part or the inference part ?<\/p>\n<p>While creating the estimator we need to give either the entry_point or the source directory. If you are using the &quot;entry_point&quot; then the value should be relative path to the file, if you are using &quot;source_dir&quot; then you should be able to give an S3 path. So before running the estimator, you can programmatically tar the files and upload it to S3 and then use the S3 path in the estimator.<\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":9.2,
        "Solution_reading_time":12.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12,
        "Solution_word_count":145,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><strong>The model<\/strong>\nI have designed, trained and published an Azure ML experiment (using a two class decision jungle) as a web service and can call it fine and it returns the expected result (based on a threshold of 0.5). <\/p>\n\n<p><strong>The problem<\/strong>\nHowever I want to manipulate the result returned to provide a result closer to my desired accuracy, precision and recall which don't happen to coincide with the default threshold of 0.5. I can easily do this via the ML studio by visualizing the evaluation results and moving the threshold slider from the center (0.5) to the left or right.<\/p>\n\n<p>I have googled and read many Azure ML documents and tutorials but so far cannot work out how to alter the threshold and return a different scored probability in my trained and published experiment. <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1455345569067,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1456850033427,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35376910",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":11.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"How to manipulate Azure ML recommendations in published web service by changing the models threshold",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":251.0,
        "Challenge_word_count":150,
        "Platform":"Stack Overflow",
        "Poster_created_time":1377669066150,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Auckland, New Zealand",
        "Poster_reputation_count":397.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p>The score module also returns the result with scored probabilities. I think you can add a simple math operation to compare the scored probability and add a new column or write a simple R script - see the image below with \"apply math operation\" to generate output based on probability exceeding 0.6 instead of 0.5<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/xNAtf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xNAtf.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":10.9,
        "Solution_reading_time":6.2,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6,
        "Solution_word_count":63,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"I just want to clarify my understanding. I can use my own servers for calling webhooks correct (as long as they return the json structure required). The webhooks will essentially reach out another API service and return data for fulfillment. Thanks in advance for your time.",
        "Challenge_closed_time":1673513,
        "Challenge_comment_count":0,
        "Challenge_created_time":1673512080000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Webhooks\/m-p\/509590#M1056",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.6,
        "Challenge_reading_time":3.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Webhooks",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":97.0,
        "Challenge_word_count":46,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Exactly correct.\u00a0 During the processing of a conversation, if you have a Web Hook enabled, the Dialogflow engine will call-out to the target URL passing in a JSON payload and expecting a correctly formatted JSON response.\n\nSee the following for details:\n\nhttps:\/\/cloud.google.com\/dialogflow\/cx\/docs\/concept\/webhook\n\nTake care to notice that the target service MUST be callable through HTTPS which means that it has a valid SSL certificate.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":10.0,
        "Solution_reading_time":5.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":69,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1443426419048,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation_count":842.0,
        "Answerer_view_count":219.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am new to Azure ML Studio. I tried creating an experiment that takes a numeric value as input and a gives a data table type output. I works fine when I run it in the portal , but not when I run it as a web service. It shows a single value numeric output , when it has to be a data table type.<\/p>\n\n<p>Is there a way to change the output type of web service output? <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/oq5Xb.png\" rel=\"nofollow noreferrer\">Visualizing output in portal<\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/wUmN7.png\" rel=\"nofollow noreferrer\">Test RRS output(web service)<\/a><\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":6,
        "Challenge_created_time":1485555724117,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1485558200660,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41903982",
        "Challenge_link_count":2,
        "Challenge_participation_count":7,
        "Challenge_readability":7.5,
        "Challenge_reading_time":7.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Web service output - Azure ML Studio",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":587.0,
        "Challenge_word_count":96,
        "Platform":"Stack Overflow",
        "Poster_created_time":1449123268407,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"East Newark, NJ, United States",
        "Poster_reputation_count":33.0,
        "Poster_view_count":43.0,
        "Solution_body":"<p>Make is a classic web service and see the JSON output getting from it. If it's providing all data you need.. go for it.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":2.9,
        "Solution_reading_time":1.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3,
        "Solution_word_count":24,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We have developed and deployed machine learning models in AML Studio. The models were deployed using ACI and we have REST endpoints that we can make calls to successfully. Next thing that I need to do is to secure the endpoints using TLS. I am going through the following article:    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-secure-web-service#enable\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-secure-web-service#enable<\/a>    <\/p>\n<p>The article suggests that I need to get a domain and then update our DNS point to the IP address of scoring endpoint. I have a subdomain  ready to use but as for the IP address, I can't work out where I would get the IP address of the scoring endpoint and how I would even be able to map this to the endpoint as the current endpoint do not contain and IP address and look nothing like the example in the article.    <\/p>\n<p>URIs currently look like the following:    <br \/>\n<a href=\"http:\/\/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx.northeurope.azurecontainer.io\/score\">http:\/\/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx.northeurope.azurecontainer.io\/score<\/a>    <\/p>\n<p>Anyone able to help with this one please as it's a little confusing and I can't find any guidance online anywhere?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615991231360,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/318807\/secure-azure-machine-learning-rest-endpoints-(depl",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":16.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Secure Azure Machine Learning REST Endpoints (deployed in ACI) with TLS",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":179,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello,<\/p>\n<p>You can do it according to DNS.<\/p>\n<p>A \u201cURL\u201d is a full specification to a page. For example:<\/p>\n<p><a href=\"http:\/\/example.com\/this_is_example.html\">http:\/\/example.com\/this_is_example.html<\/a> is a URL. It has three parts:<\/p>\n<p>The protocol specifier: http:<\/p>\n<p>The domain name: example.com<\/p>\n<p>The page location: \/this_is_example.html<\/p>\n<p>The protocol specifies the port that will be used. http, for example, is  <br \/>\nport 80. ftp uses ports 20 and 21. SMTP, the mail sending protocol, is usually  <br \/>\non port 25. You can actually find the full list of \u201cofficial\u201d ports here.<\/p>\n<p>It\u2019s only the domain name that has an IP address associated with it. So that\u2019s what you would be looking up.<\/p>\n<p>My approach is to use the \u201cping\u201d command in a Windows command prompt. For  <br \/>\nexample:<\/p>\n<p>C:\\&gt;ping example.com<\/p>\n<p>Then you can get it.<\/p>\n<p>Regards,  <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":6.3,
        "Solution_reading_time":11.49,
        "Solution_score_count":0.0,
        "Solution_sentence_count":17,
        "Solution_word_count":131,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1320061998252,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":778.0,
        "Answerer_view_count":89.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>same input is used in two cases, but different result is returned from python module<\/p>\n\n<p>here is the python script that return the result to the webservice:<\/p>\n\n<pre><code>import pandas as pd\nimport sys\n\n\n  def get_segments(dataframe):\n     dataframe['segment']=dataframe['segment'].astype('str')\n     segments = dataframe.loc[~dataframe['segment'].duplicated()]['segment']\n     return segments\n\n\n  def azureml_main(dataframe1 = None, dataframe2 = None):\n\n   df = dataframe1\n   segments = get_segments(df)\n   segmentCount =segments.size\n\n   if (segmentCount &gt; 0) :\n      res = pd.DataFrame(columns=['segmentId','recommendation'],index=[range(segmentCount)])\n    i=0    \n    for seg in segments:\n        d= df.query('segment ==[\"{}\"]'.format(seg)).sort(['count'],ascending=[0])\n\n        res['segmentId'][i]=seg\n        recommendation='['\n        for index, x in d.iterrows():\n            item=str(x['ItemId'])\n            recommendation = recommendation + item + ','\n        recommendation = recommendation[:-1] + ']'\n        res['recommendation'][i]= recommendation\n        i=i+1\n   else:\n\n      res = pd.DataFrame(columns=[seg,pdver],index=[range(segmentCount)])\n\nreturn res,\n<\/code><\/pre>\n\n<p>when in experiment it returnd the actual itemIds, when in webservice it returns some numbers<\/p>\n\n<p>the purpose of this code is to pivot some table by segment column for recommendation<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1467651436410,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1468140771380,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38189399",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":14.6,
        "Challenge_reading_time":17.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"azure ml experiment return different results than webservice",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":264.0,
        "Challenge_word_count":124,
        "Platform":"Stack Overflow",
        "Poster_created_time":1320061998252,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":778.0,
        "Poster_view_count":89.0,
        "Solution_body":"<p>After discussion with the product team from Microsoft. the issue was resolved.\nthe product team rolled out an update to the web service first, and only later to the ML-Studio, which fixed an issue with categorical attributes in \"Execute python script\".\nthe issue was in a earlier stage of the flow and has nothing to do with the python code above.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":6.9,
        "Solution_reading_time":4.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":61,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"I am trying to call an API to inference from a model I have uploaded to vertex AI.\n\nI have tried three methods, and none worked so far.\n\nAt first, I was following a youtube from standford university,\u00a0https:\/\/www.youtube.com\/watch?v=fw6NMQrYc6w&t=3876s\u00a0which uses ai platform.\n\n1. I also tried that, but I think google is trying to get rid of AI platform, and although I succesfully uploaded the model, it doesn't allow me to make a new version, basically allows me nothing.\n\n2. I tried to work this tutorial,\u00a0https:\/\/codelabs.developers.google.com\/vertex-p2p-predictions#5\u00a0\n\nand it keeps complaining that my payload is above 1.5MB limit, but my image is only 49KB, so it's ridiculous. maybe something happened in this code, but it's from the tutorial, so the tutorial must be wrong then.\n\n\u00a0\n\nIMAGE_PATH = \"test-image.jpg\"\nim = Image.open(IMAGE_PATH)\nx_test = np.asarray(im).astype(np.float32).tolist()\nendpoint.predict(instances=x_test).predictions\n\n\u00a0\n\n3. Last, I've been trying to call the API from the sample code,\n\nhttps:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/samples\/snippets\/prediction_service\/predic...\n\nbut it gives me a json format error.\n\nI have referenced from this website to get the json format.\n\nhttps:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/samples\/snippets\/prediction_service\/predic...\u00a0\n\nThe error I am getting is as is :\u00a0\n\n400 { \"error\": \"Failed to process element: 0 key: instances of 'instances' list. Error: Invalid argument: JSON object: does not have named input: instances\" }\n\u00a0\nThe code that I have used is :\n\u00a0\n\n\u00a0\n\n    encoded_content = base64.b64encode(image).decode(\"utf-8\")\n    instances = {\"instances\": {\"image\": {\"b64\": encoded_content}}, \"key\": \"0\"}\n\n\u00a0\n\nand the 400 error comes from API and the log from vertex AI is not very useful when debugging.\n\nHonestly I have been struggling with this issue for days and in my opinion, this should not be this difficult. My experience with GCP and vertex AI is very disappointing and I'm considering to explore other options. Please let me know if any of you have any advices. Thanks",
        "Challenge_closed_time":1682031,
        "Challenge_comment_count":0,
        "Challenge_created_time":1681326780000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-from-Vertex-AI-Getting-predictions-from-custom-trained\/m-p\/543292#M1647",
        "Challenge_link_count":4,
        "Challenge_participation_count":3,
        "Challenge_readability":9.9,
        "Challenge_reading_time":26.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":null,
        "Challenge_title":"Error from Vertex AI Getting predictions from custom trained models",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":179.0,
        "Challenge_word_count":291,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I actually solved this error by applying this document.\n\nhttps:\/\/cloud.google.com\/vertex-ai\/docs\/samples\/aiplatform-predict-image-classification-sample.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":23.9,
        "Solution_reading_time":2.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2,
        "Solution_word_count":15,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1263294862568,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":183045.0,
        "Answerer_view_count":13691.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>It took 45 minutes to create my endpoint from the stored endpoint configuration. (I tested it and it works too). This is the first time that I've used boto3 to do this, whereas previously I just used the Sagemaker web GUI to create an endpoint from endpoint configuration.  Suggestions to my code are appreciated:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nsagemaker_client = boto3.client('sagemaker')\n\nresponse = sagemaker_client.create_endpoint(\n    EndpointName='sagemaker-tensorflow-x',\n    EndpointConfigName='sagemaker-tensorflow-x'\n)\n<\/code><\/pre>\n<p>Note: I've replaced the last part of my endpoint name with <code>x<\/code>.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1598917151350,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1598923606240,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63679503",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":9.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Why did it take so long to create endpoint with AWS Sagemaker using Boto3?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":373.0,
        "Challenge_word_count":90,
        "Platform":"Stack Overflow",
        "Poster_created_time":1347733578128,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Chicago, IL, United States",
        "Poster_reputation_count":16557.0,
        "Poster_view_count":461.0,
        "Solution_body":"<p>AWS has currently <a href=\"https:\/\/status.aws.amazon.com\/\" rel=\"nofollow noreferrer\">issues<\/a> with Sagemaker:<\/p>\n<blockquote>\n<p>Increased Error Rates and Latencies for Multiple API operations<\/p>\n<\/blockquote>\n<blockquote>\n<p>5:33 PM PDT We are investigating increased error rates and latencies for CreateTrainingJob, CreateHyperParameterTuningJob, and CreateEndpoint API operations in the US-EAST-1 Region. Previously created jobs and endpoints are unaffected.<\/p>\n<\/blockquote>\n<blockquote>\n<p>6:04 PM PDT We are continuing to investigate increased error rates and latencies for CreateTrainingJob, CreateHyperParameterTuningJob, and CreateEndpoint API operations in the US-EAST-1 Region. Previously created jobs and endpoints are unaffected.<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rQHQC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rQHQC.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3,
        "Solution_readability":16.2,
        "Solution_reading_time":12.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8,
        "Solution_word_count":94,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Do we have guidelines on requirements gathering\/designing the provisioning of SageMaker Studio domains across large global enterprises with many business units? \n\nI've seen discussions where topics like number of users\/domain, org\/team structure, collaboration patterns, resource needs, classes of ML problems, framework\/library usage, security and others were raised when defining requirements and boundaries. Customer is starting their first Studio deployment and they are asking for guidance on how to scope and design that so that they can have a scalable process.",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1612447142000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1667925623704,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU0QiBS-GdSIKZdXxSf1NUYA\/sagemaker-studio-enterprise-deployment-guidelines",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.8,
        "Challenge_reading_time":7.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker Studio Enterprise Deployment guidelines",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":86.0,
        "Challenge_word_count":84,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You should guide your customer based on the general principles of multi account best practices that we provide for other services.\n\nHere are some high level boundaries.\n\nOne studio domain per account and region. No cross region AWS SSO configuration provided.\n\nMaximum numbers of users allowed in studio vary between 60 - 200 users. Although AWS SSO can support many more users, there are some considerations around other dependencies such as EFS among others.\n\nIf you need to isolate any model artifacts produced by SageMaker, you may want to have them use a separate account. Even if you use tag based access control, you can still technically list those artifacts.\n\nSageMaker feature store should follow the data lake pattern closely. As a general rule, you want to write in one account and can read from many other accounts perhaps using Lake formation to expose datasets into other accounts. Teams can create their own offline \/ online feature store for non production use cases.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925545476,
        "Solution_link_count":0,
        "Solution_readability":7.8,
        "Solution_reading_time":12.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11,
        "Solution_word_count":161,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":9,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nTrainer.fit fails with a pickle error when the logger is MLFlowLogger, and distributed_backend='ddp' on GPUs but without SLURM.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Instantiate MLFlowLogger in Pytorch 0.5.3.2 with Pytorch 1.3.1 and MLFlow 1.4.0. The execution environment has environment variables MLFLOW_TRACKING_URI, and also MLFLOW_TRACKING_USERNAME and MLFLOW_TRACKING_PASSWORD to connect to the MLflow tracking server with HTTP Basic Authentication. The MLflow tracking server is also v1.4.0.\r\n2. Instantiate Trainer with MLFlowLogger instance as logger, distributed_backend='ddp' and with the gpus parameter on a machine with NVIDIA GPUs but without SLURM.\r\n3. Run Trainer.fit\r\n\r\nFrom the error output, it looks like multiprocessing is attempting to pickle the nested function in MLflow function [_get_rest_store](https:\/\/github.com\/mlflow\/mlflow\/blob\/v1.4.0\/mlflow\/tracking\/_tracking_service\/utils.py#L81):\r\n```\r\nayla.khan@gpu12:~\/photosynthetic$ python test_mlflow.py\r\nTraceback (most recent call last):\r\n  File \"test_mlflow.py\", line 71, in <module>\r\n    trainer.fit(model)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 343, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_gpus, args=(model,))\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 162, in spawn\r\n    process.start()\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object '_get_rest_store.<locals>.get_default_host_creds'\r\n```\r\n\r\n#### Code sample\r\nSample code tested with a very simple test model ([gist](https:\/\/gist.github.com\/a-y-khan\/8693d2b186227561a4baf4d03ce75c34)):\r\n\r\n```\r\ntest_hparams = Namespace()\r\nmodel = XORGateModel(test_hparams)\r\n\r\nlogger = MLFlowLogger(experiment_name='test_lightning_logger',\r\n                                          tracking_uri=os.environ['MLFLOW_TRACKING_URI'])\r\ntrainer = pl.Trainer(logger=logger, distributed_backend='ddp', gpus='-1')\r\ntrainer.fit(model)\r\n```\r\n\r\n### Expected behavior\r\n\r\nTrainer.fit runs without error.\r\n\r\n### Environment\r\n\r\n```\r\n(photosynthetic) ayla.khan@gpu12:~\/photosynthetic$ python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 1.3.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 440.33.01\r\ncuDNN version: \/usr\/local\/cuda-10.0\/lib64\/libcudnn.so.7\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] pytorch-lightning==0.5.3.2\r\n[pip] pytorch-toolbelt==0.2.1\r\n[pip] torch==1.3.1\r\n[pip] torchsummary==1.5.1\r\n[pip] torchvision==0.4.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.3.0            py36he904b0f_0\r\n[conda] mkl_fft                   1.0.15           py36ha843d7b_0\r\n[conda] mkl_random                1.1.0            py36hd6b4f25_0\r\n[conda] pytorch                   1.3.1           py3.6_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] pytorch-lightning         0.5.3.2                  pypi_0    pypi\r\n[conda] pytorch-toolbelt          0.2.1                    pypi_0    pypi\r\n[conda] torchsummary              1.5.1                    pypi_0    pypi\r\n[conda] torchvision               0.4.2                py36_cu101    pytorch\r\n```",
        "Challenge_closed_time":1583540,
        "Challenge_comment_count":0,
        "Challenge_created_time":1576464430000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/630",
        "Challenge_link_count":2,
        "Challenge_participation_count":9,
        "Challenge_readability":14.0,
        "Challenge_reading_time":59.35,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":64,
        "Challenge_solved_time":null,
        "Challenge_title":"Pickle error from Trainer.fit when using MLFlowLogger and distributed data parallel without SLURM",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":409,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Will investigate. We have a test that is supposed to prevent these problems from sneaking back in, but apparently it's not doing it's job. I imagine everyone is busy with the build failures - but for the record, I am  having a similar problem. Essentially, I cannot get a logger to work using ddp. It's gving me one of those days when I wonder why I ever wanted to write software ;)\r\n\r\nThis is Ubuntu 18.04.2LTS, on a 14 core, 7 gpu machine. Python 3.6.8, pytorch 1.3.1, pytorch-lightning 0.5.3.2, Tensorboard 2.1.0. Everything else standard except pillow isis 6.2.2 due to known bug in 7.0.\r\n\r\nI am working with a tried and true model and hyperparameters. The model and logging work fine as cpu, gpu, or dp - and ddp if I don't log. But not ddp with logging. I am not using SLURM.\r\n\r\nI have tried to get around this several ways: passing a custom logger, not using the logger created by Trainer(), etc. They either fail when called from one of the new processes, with an attribute error in Tensorboard TTDummyFileWriter.get_logdir(), or they fail with a pickle error about thread.locks when being copied to a new process\r\n\r\nI will detail these in a bug report if you think they are NOT due to the recent build issues.\r\n\r\nBut thought you'd want to know ...\r\n\r\ns\r\n @dbczumar,  @smurching? @neggert is this fixed now? Can this issue be re-opened? I'm currently working with Pytorch-Lightning==0.7.6 and am getting an identical pickle issue when using DDP with the MLFLowLogger.\r\n\r\n**Reproducing**\r\n\r\nUsing the script the OP gave led to some other errors (mostly to do with lightning version differences), so a new gist to reproduce in Pytorch-Lightning 0.7.6 can be found [here](https:\/\/gist.github.com\/Polyphenolx\/39424e5673fc029567f7f3ae3551fffb).\r\n\r\nThis is easily reproducible in other projects as well.\r\n\r\n**Error Output**\r\n\r\n```\r\n\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/utilities\/distributed.py:23: DeprecationWarning: `logging` package has been renamed to `loggers` since v0.7.0 The deprecated package name will be removed in v0.9.0.\r\n  warnings.warn(*args, **kwargs)\r\n\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/utilities\/distributed.py:23: DeprecationWarning: `mlflow_logger` module has been renamed to `mlflow` since v0.6.0. The deprecated module name will be removed in v0.8.0.\r\n  warnings.warn(*args, **kwargs)\r\n\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/utilities\/distributed.py:23: DeprecationWarning: `data_loader` decorator deprecated in v0.7.0. Will be removed v0.9.0\r\n  warnings.warn(*args, **kwargs)\r\nGPU available: True, used: True\r\nNo environment variable for node rank defined. Set as 0.\r\nCUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nTraceback (most recent call last):\r\n  File \"mlflow_test.py\", line 65, in <module>\r\n    trainer.fit(model)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 844, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 200, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 149, in start_processes\r\n    process.start()\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object '_get_rest_store.<locals>.get_default_host_creds'\r\n```\r\n\r\n**Environment**\r\n```\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- GeForce GTX 1080 Ti\r\n\t\t- GeForce GTX 1080 Ti\r\n\t\t- GeForce GTX 1080 Ti\r\n\t\t- GeForce GTX 1080 Ti\r\n\t- available:         True\r\n\t- version:           10.2\r\n* Packages:\r\n\t- numpy:             1.18.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.5.0\r\n\t- pytorch-lightning: 0.7.6\r\n\t- tensorboard:       2.2.2\r\n\t- tqdm:              4.46.1\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.6.8\r\n\t- version:           #102-Ubuntu SMP Mon May 11 10:07:26 UTC 2020\r\n``` To add to this, it appears to be a greater issue with MLFLow and how their tracking utilities are coded. They use a higher order function that causes issues with pickling in torches DDP backend. I've created an issue on MLFLow git, and submitted a PR to remedy the problem. \r\n\r\nIn the interim, feel free to implement the fix described in the issue in the MLFlow git as a temporary fix until\/if they review\/merge mine Following up on this: The pickling fix was merged into the master branch of MLFlow a couple days ago (see the bug mention above). Training using DDP is now functional on MLFLow versions installed from master, but it may take them some time to release the fix to PyPi Running into this same issue as are a few others here:\r\nhttps:\/\/github.com\/minimaxir\/aitextgen\/issues\/135\r\n![image](https:\/\/user-images.githubusercontent.com\/4674698\/121708545-8923f780-ca8c-11eb-9483-56740fd6d401.png)\r\n Hi,\r\n I am still getting the below error:\r\n![image](https:\/\/user-images.githubusercontent.com\/57705684\/131129141-fa483cb4-cb95-43a1-b1d3-62bf78711de2.png)\r\n\r\nI am using DP strategy and PT version '1.8.1+cu111' and PL version '1.3.8'.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_readability":9.3,
        "Solution_reading_time":75.14,
        "Solution_score_count":null,
        "Solution_sentence_count":72,
        "Solution_word_count":675,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1280527017200,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3035.0,
        "Answerer_view_count":129.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>After setting up an endpoint for my model on Amazon SageMaker, I am trying to invoke it with a POST request which contains a file with a key <code>image<\/code> &amp; content type as <code>multipart\/form-data<\/code>.<\/p>\n\n<p>My AWS CLI command is like this:<\/p>\n\n<pre><code>aws sagemaker-runtime invoke-endpoint --endpoint-name &lt;endpoint-name&gt; --body image=@\/local\/file\/path\/dummy.jpg --content-type multipart\/form-data output.json --region us-east-1\n<\/code><\/pre>\n\n<p>which should be an equivalent of:<\/p>\n\n<pre><code>curl -X POST -F \"image=@\/local\/file\/path\/dummy.jpg\" http:\/\/&lt;endpoint&gt;\n<\/code><\/pre>\n\n<p>After running the <code>aws<\/code> command, the file is not transferred via the request, and my model is receiving the request without any file in it.<\/p>\n\n<p>Can someone please tell me what should be the correct format of the <code>aws<\/code> command in order to achieve this?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1533504341903,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51698373",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":12.3,
        "Challenge_reading_time":12.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Amazon SageMaker: Invoke endpoint with file as multipart\/form-data",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":2346.0,
        "Challenge_word_count":119,
        "Platform":"Stack Overflow",
        "Poster_created_time":1472843515756,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>The first problem is that you're using 'http' for your CURL request. Virtually all AWS services strictly use 'https' as their protocol, SageMaker included. <a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/rande.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/rande.html<\/a>. I'm going to assume this was a typo though.<\/p>\n\n<p>You can check the verbose output of the AWS CLI by passing the '--debug' argument to your call. I re-ran a similar experiment with my favorite duck.jpg image:<\/p>\n\n<pre><code>aws --debug sagemaker-runtime invoke-endpoint --endpoint-name MyEndpoint --body image=@\/duck.jpg --content-type multipart\/form-data  &gt;(cat)\n<\/code><\/pre>\n\n<p>Looking at the output, I see:<\/p>\n\n<pre><code>2018-08-10 08:42:20,870 - MainThread - botocore.endpoint - DEBUG - Making request for OperationModel(name=InvokeEndpoint) (verify_ssl=True) with params: {'body': 'image=@\/duck.jpg', 'url': u'https:\/\/sagemaker.us-west-2.amazonaws.com\/endpoints\/MyEndpoint\/invocations', 'headers': {u'Content-Type': 'multipart\/form-data', 'User-Agent': 'aws-cli\/1.15.14 Python\/2.7.10 Darwin\/16.7.0 botocore\/1.10.14'}, 'context': {'auth_type': None, 'client_region': 'us-west-2', 'has_streaming_input': True, 'client_config': &lt;botocore.config.Config object at 0x109a58ed0&gt;}, 'query_string': {}, 'url_path': u'\/endpoints\/MyEndpoint\/invocations', 'method': u'POST'}\n<\/code><\/pre>\n\n<p>It looks like the AWS CLI is using the string literal '@\/duck.jpg', not the file contents.<\/p>\n\n<p>Trying again with curl and the \"--verbose\" flag:<\/p>\n\n<pre><code>curl --verbose -X POST -F \"image=@\/duck.jpg\" https:\/\/sagemaker.us-west-2.amazonaws.com\/endpoints\/MyEndpoint\/invocations\n<\/code><\/pre>\n\n<p>I see the following:<\/p>\n\n<pre><code>Content-Length: 63097\n<\/code><\/pre>\n\n<p>Much better. The '@' operator is a CURL specific feature. The AWS CLI does have a way to pass files though: <\/p>\n\n<pre><code>--body fileb:\/\/\/duck.jpg\n<\/code><\/pre>\n\n<p>There is also a 'file' for non-binary files such as JSON. Unfortunately you cannot have the prefix. That is, you cannot say:<\/p>\n\n<pre><code> --body image=fileb:\/\/\/duck.jpg\n<\/code><\/pre>\n\n<p>You can prepend the string 'image=' to your file with a command such as the following. (You'll probably need to be more clever if your images are really big; this is really inefficient.)<\/p>\n\n<pre><code> echo -e \"image=$(cat \/duck.jpg)\" &gt; duck_with_prefix\n<\/code><\/pre>\n\n<p>Your final command would then be:<\/p>\n\n<pre><code> aws sagemaker-runtime invoke-endpoint --endpoint-name MyEndpoint --body fileb:\/\/\/duck_with_prefix --content-type multipart\/form-data  &gt;(cat)\n<\/code><\/pre>\n\n<p>Another note: Using raw curl with AWS services is extremely difficult due to the AWS Auth signing requirements - <a href=\"https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/API\/sig-v4-authenticating-requests.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/API\/sig-v4-authenticating-requests.html<\/a> <\/p>\n\n<p>It can be done, but you'll likely be more productive by using the AWS CLI or a pre-existing tool such as Postman - <a href=\"https:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/how-to-use-postman-to-call-api.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/how-to-use-postman-to-call-api.html<\/a> <\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":8,
        "Solution_readability":14.9,
        "Solution_reading_time":43.72,
        "Solution_score_count":3.0,
        "Solution_sentence_count":28,
        "Solution_word_count":323,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi,    <br \/>\ni deployed a real-time inference pipeline using ML Designer. Training and deploying works fine. But when I'm consuming\/testing my API it doesn't work. Postman gives me Errorcode 500 and &quot;Internal Server Error. Run: Server internal error is from Module Extract N-Gram Features from Text&quot;.    <\/p>\n<p>This is my training pipeline:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/42733-image.png?platform=QnA\" alt=\"42733-image.png\" \/>    <\/p>\n<p>I read this: <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/algorithm-module-reference\/extract-n-gram-features-from-text.md#score-or-publish-a-model-that-uses-n-grams\">https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/algorithm-module-reference\/extract-n-gram-features-from-text.md#score-or-publish-a-model-that-uses-n-grams<\/a>    <\/p>\n<p>But I don't know how to achieve this.    <\/p>\n<p>Thanks in advance.    <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606307286153,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/175242\/how-to-deploy-ml-designer-pipeline-as-real-time-in",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":18.5,
        "Challenge_reading_time":14.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"How to deploy ML Designer pipeline as real-time inference pipeline using N-Gram",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":87,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Once you create a real-time inference pipeline, please make the further modifications below:    <\/p>\n<ol>\n<li> Find the output <strong>Result_vocabulary<\/strong> dataset from <strong>Extract N-Gram Features from Text<\/strong> module.    <br \/>\n <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/42884-findmoduleoutputdataset.png?platform=QnA\" alt=\"42884-findmoduleoutputdataset.png\" \/>    <\/li>\n<li> Register the dataset as with a name    <br \/>\n <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/42905-registerdataset.png?platform=QnA\" alt=\"42905-registerdataset.png\" \/>    <\/li>\n<li> Update real-time inference pipeline like below:    <br \/>\n <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/42865-inferencepipeline.png?platform=QnA\" alt=\"42865-inferencepipeline.png\" \/>    <\/li>\n<\/ol>\n<p>We will improve the documentation accordingly. Thanks for reporting the issue!    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3,
        "Solution_readability":19.7,
        "Solution_reading_time":12.09,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6,
        "Solution_word_count":71,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hello, I am using Azure machine learning studio, which has been changed since last year.    <\/p>\n<p>Previously, the Azure Machine Learning designer function of the Classic version could be applied to Excel by importing the App function to Excel and downloading it. Like the picture below!    <\/p>\n<p>Has the function that can be linked to Excel be lost in this Azure Machine Learning Studio? it's very difficult....    <\/p>\n<p>If there is a function, can you tell me how to do it?    <\/p>\n<p>And I wonder if there are any lectures that explain the new azure machine learning designer features.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/178194-azure2.png?platform=QnA\" alt=\"178194-azure2.png\" \/>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/178202-azure1.png?platform=QnA\" alt=\"178202-azure1.png\" \/>    <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645970050413,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/752248\/azure-machine-learning-studio-for-designer-functio",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":9.0,
        "Challenge_reading_time":11.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure machine learning studio for designer function connected with excel?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":117,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=3b41fd96-f090-42a6-b421-e5af3d214f5f\">@Robin Jang  <\/a> The designer studio does not have an add-in for excel. This is only available with the classic version of Azure Machine Learning.     <br \/>\nIf you are new to Azure machine learning designer I would recommend to start with the tutorials from Microsoft Learn available <a href=\"https:\/\/learn.microsoft.com\/en-us\/learn\/browse\/?filter-products=machine&amp;products=azure-machine-learning\">here<\/a>.    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3,
        "Solution_readability":14.2,
        "Solution_reading_time":11.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6,
        "Solution_word_count":77,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1565528932887,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"United Kingdom",
        "Answerer_reputation_count":1579.0,
        "Answerer_view_count":91.0,
        "Challenge_adjusted_solved_time":17.8840719444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am deploying 50 NLP models on Azure Container Instances via the Azure Machine Learning service. All 50 models are quite similar and have the same input\/output format with just the model implementation changing slightly. <\/p>\n\n<p>I want to write a generic score.py entry file and pass in the model name as a parameter. The interface method signature does not allow a parameter in the init() method of score.py, so I moved the model loading into the run method. I am assuming the init() method gets run once whereas Run(data) will get executed on every invocation, so this is possibly not ideal (the models are 1 gig in size)<\/p>\n\n<p>So how can I pass in some value to the init() method of my container to tell it what model to load? <\/p>\n\n<p>Here is my current, working code:<\/p>\n\n<pre><code>def init():\n\ndef loadModel(model_name):\n    model_path = Model.get_model_path(model_name)  \n    return fasttext.load_model(model_path)\n\ndef run(raw_data):\n    # extract model_name from raw_data omitted...\n    model = loadModel(model_name)\n\n    ...\n<\/code><\/pre>\n\n<p>but this is what I would like to do (which breaks the interface)<\/p>\n\n<pre><code>def init(model_name):\n    model = loadModel(model_name)\n\ndef loadModel(model_name):\n    model_path = Model.get_model_path(model_name)  \n    return fasttext.load_model(model_path)\n\ndef run(raw_data):\n    ...\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1572325608637,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58601697",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":17.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"How to pass in the model name during init in Azure Machine Learning Service?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":851.0,
        "Challenge_word_count":195,
        "Platform":"Stack Overflow",
        "Poster_created_time":1256089885500,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sydney, Australia",
        "Poster_reputation_count":4947.0,
        "Poster_view_count":531.0,
        "Solution_body":"<p>If you're looking to use the same deployed container and switch models between requests; it's not the preferred design choice for Azure machine learning service, we need to specify the model name to load during build\/deploy.<\/p>\n\n<p>Ideally, each deployed web-service endpoint should allow inference of one model only; with the model name defined before the container the image starts building\/deploying. <\/p>\n\n<p>It is mandatory that the entry script has both <code>init()<\/code> and <code>run(raw_data)<\/code> with those <strong>exact<\/strong> signatures. <\/p>\n\n<p>At the moment, we can't change the signature of <code>init()<\/code> method to take a parameter like in <code>init(model_name)<\/code>.  <\/p>\n\n<p>The only dynamic user input you'd ever get to pass into this web-service is via <code>run(raw_data)<\/code> method. As you have tried, given the size of your model passing it via run is not feasible. <\/p>\n\n<p><code>init()<\/code> is run first and only <strong>once<\/strong> after your web-service deploy. Even if <code>init()<\/code> took the <code>model_name<\/code> parameter, there isn't a straight forward way to call this method directly and pass your desired model name.<\/p>\n\n<hr>\n\n<p>But, one possible solution is: <\/p>\n\n<p>You can create params file like below and store the file in azure blob storage.<\/p>\n\n<p>Example runtime parameters generation script:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pickle\n\nparams = {'model_name': 'YOUR_MODEL_NAME_TO_USE'}\n\nwith open('runtime_params.pkl', 'wb') as file:\n    pickle.dump(params, file)\n\n<\/code><\/pre>\n\n<p>You'll need to use <a href=\"https:\/\/github.com\/Azure\/azure-storage-python\" rel=\"nofollow noreferrer\">Azure Storage Python SDK<\/a> to write code that can read from your blob storage account. This also mentioned in the official docs <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where#prepare-to-deploy\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Then you can access this from <code>init()<\/code> function in your score script. <\/p>\n\n<p>Example <code>score.py<\/code> script:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azure.storage.blob import BlockBlobService\nimport pickle\n\ndef init():\n\n  global model\n\n  block_blob_service = BlockBlobService(connection_string='your_connection_string')\n\n  blob_item = block_blob_service.get_blob_to_bytes('your-container-name','runtime_params.pkl')\n\n  params = pickle.load(blob_item.content)\n\n  model = loadModel(params['model_name'])\n<\/code><\/pre>\n\n<p>You can store connection strings in Azure KeyVault for secure access. Azure ML Workspaces comes with built-in KeyVault integration. More info <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.keyvault.keyvault?view=azure-ml-py#get-secret-name-\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>With this approach, you're abstracting runtime params config to another cloud location rather than the container itself. So you wouldn't need to re-build the image or deploy the web-service again. Simply restarting the container will work.<\/p>\n\n<hr>\n\n<p>If you're looking to simply re-use <code>score.py<\/code> (not changing code) for <strong>multiple model deployments in multiple containers<\/strong> then here's another possible solution.<\/p>\n\n<p>You can define your model name to use in web-service in a text file and read it in score.py. You'll need to pass this text file as a dependency when setting up the image config.<\/p>\n\n<p>This would, however, need multiple params files for each container deployment.<\/p>\n\n<p>Passing 'runtime_params.pkl' in <code>dependencies<\/code> to your image config (More detail example <a href=\"https:\/\/github.com\/rithinch\/heartfulness-similar-content-service\/blob\/master\/experiments\/notebooks\/Deploy%20Model%20-%20Azure.ipynb\" rel=\"nofollow noreferrer\">here<\/a>):<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>image_config = ContainerImage.image_configuration(execution_script=\"score.py\", \n                                                  runtime=\"python\", \n                                                  conda_file=\"myenv.yml\",\n                                                  dependencies=[\"runtime_params.pkl\"],\n                                                  docker_file=\"Dockerfile\")\n<\/code><\/pre>\n\n<p>Reading this in your score.py <code>init()<\/code> function:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def init():\n\n  global model\n\n  with open('runtime_params.pkl', 'rb') as file:\n    params = pickle.load(file)\n\n  model = loadModel(params['model_name'])\n\n<\/code><\/pre>\n\n<p>Since your creating a new image config with this approach, you'll need to build the image and re-deploy the service.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1572389991296,
        "Solution_link_count":4,
        "Solution_readability":12.7,
        "Solution_reading_time":58.32,
        "Solution_score_count":3.0,
        "Solution_sentence_count":40,
        "Solution_word_count":476,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1588516515763,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"UK",
        "Answerer_reputation_count":29087.0,
        "Answerer_view_count":3080.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am new to Amazon SageMaker and I am closely following this tutorial <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/<\/a> to create a machine learning-powered REST API with Amazon API Gateway mapping templates and Amazon SageMaker<\/p>\n\n<p>when I run the following command on terminal (<strong>Step 2 of the Tutorial<\/strong> )<\/p>\n\n<pre><code>aws sagemaker-runtime invoke-endpoint \\\n  --endpoint-name &lt;endpoint-name&gt; \\\n  --body '{\"instances\": [{\"in0\":[863],\"in1\":[882]}]}' \\\n  --content-type application\/json \\\n  --accept application\/json \\\n  results\n<\/code><\/pre>\n\n<p>I get the following <strong>Error:<\/strong> <code>Invalid base64: \"{\"instances\": [{\"in0\":[863],\"in1\":[882]}]}\"<\/code>\nMy endpoint is <code>InService<\/code> on the SageMaker console and the example Jupyter notebook run successfully. (I also substituted <code>&lt;endpoint-name&gt;<\/code> with the actual name - same error received with\/without quotations around the name) <\/p>\n\n<p>Using <strong>zsh<\/strong> here is the aws cli version:<\/p>\n\n<pre><code>aws --version\naws-cli\/2.0.15 Python\/3.7.4 Darwin\/19.4.0 botocore\/2.0.0dev19\n<\/code><\/pre>\n\n<p>Wondering what the problem could be. Any help is appreciated<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590313799700,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1590314948843,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61984217",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":19.3,
        "Challenge_reading_time":21.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Invalid base64: \"{\"instances\": [{\"in0\":[863],\"in1\":[882]}]}\" when testing Amazon SageMaker model endpoint using the AWS CLI",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1117.0,
        "Challenge_word_count":140,
        "Platform":"Stack Overflow",
        "Poster_created_time":1578221300168,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":117.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>The problem is that the body contents is being expected to be base 64 encoded, try base64 encoding the body before passing it to the invoke statement.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":12.6,
        "Solution_reading_time":1.92,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1,
        "Solution_word_count":27,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi! I wanted to see if VBA and Azure Machine Learning Excel Add In can be connected to each other. Are there any way to code VBA (use VBA) for controlling or altering Azure Machine Learning Excel Add In? I have used Azure Machine Learning to rate candidate feedback as negative or positive, but it has like a 75 -80% success rate - there are still a good chunk of comments that are rated wrong. However, it is still an amazing tool that I want to use v- I was just wondering if I can increase the accuracy of it somehow by creating a VBA code that connects it to Azure Machine Learning where I can add words related to negative responses or vice versa for positive response to increase the accuracy. <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610149745593,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/224491\/vba-and-azure-machine-learning-excel-add-in",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":8.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"VBA  And Azure Machine Learning Excel Add In",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":139,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, we currently don't support VBA and Azure ML Excel add-in integration. You'll need to apply <a href=\"https:\/\/stackoverflow.com\/questions\/41447104\/how-to-improve-classification-accuracy-for-machine-learning\">ML techniques for improving your model<\/a> and re-deploy your model.  <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":15.2,
        "Solution_reading_time":3.8,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3,
        "Solution_word_count":28,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1451855177167,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Crici\u00fama - State of Santa Catarina, Brazil",
        "Answerer_reputation_count":68.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have some data in S3 and I want to create a lambda function to predict the output with my deployed aws sagemaker endpoint then I put the outputs in S3 again. Is it necessary in this case to create an api gateway like decribed in this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"noreferrer\">link<\/a> ? and in the lambda function what I have to put. I expect to put (where to find the data, how to invoke the endpoint, where to put the data) <\/p>\n\n<pre><code>import boto3\nimport io\nimport json\nimport csv\nimport os\n\n\nclient = boto3.client('s3') #low-level functional API\n\nresource = boto3.resource('s3') #high-level object-oriented API\nmy_bucket = resource.Bucket('demo-scikit-byo-iris') #subsitute this for your s3 bucket name. \n\nobj = client.get_object(Bucket='demo-scikit-byo-iris', Key='foo.csv')\nlines= obj['Body'].read().decode('utf-8').splitlines()\nreader = csv.reader(lines)\n\nimport io\nfile = io.StringIO(lines)\n\n# grab environment variables\nruntime= boto3.client('runtime.sagemaker')\n\nresponse = runtime.invoke_endpoint(\n    EndpointName= 'nilm2',\n    Body = file.getvalue(),\n    ContentType='*\/*',\n    Accept = 'Accept')\n\noutput = response['Body'].read().decode('utf-8')\n<\/code><\/pre>\n\n<p>my data is a csv file of 2 columns of floats with no headers, the problem is that lines return a list of strings(each row is an element of this list:['11.55,65.23', '55.68,69.56'...]) the invoke work well but the response is also a string: output = '65.23\\n,65.23\\n,22.56\\n,...' <\/p>\n\n<p>So how to save this output to S3 as a csv file <\/p>\n\n<p>Thanks<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1549885499183,
        "Challenge_favorite_count":4.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54629890",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.0,
        "Challenge_reading_time":21.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"Invoke aws sagemaker endpoint",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":3146.0,
        "Challenge_word_count":200,
        "Platform":"Stack Overflow",
        "Poster_created_time":1502010899808,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tunis, Tunisia",
        "Poster_reputation_count":109.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>If your Lambda function is scheduled, then you won't need an API Gateway. But if the predict action will be triggered by a user, by an application, for example, you will need.<\/p>\n\n<p>When you call the invoke endpoint, actually you are calling a SageMaker endpoint, which is not the same as an API Gateway endpoint. <\/p>\n\n<p>A common architecture with SageMaker is:<\/p>\n\n<ol>\n<li>API Gateway with receives a request then calls an authorizer, then\ninvoke your Lambda; <\/li>\n<li>A Lambda with does some parsing in your input data, then calls your SageMaker prediction endpoint, then, handles the result and returns to your application.<\/li>\n<\/ol>\n\n<p>By the situation you describe, I can't say if your task is some academic stuff or a production one.<\/p>\n\n<p>So, how you can save the data as a CSV file from your Lambda? <\/p>\n\n<p>I believe you can just parse the output, then just upload the file to S3. Here you will parse manually or with a lib, with boto3 you can upload the file. The output of your model depends on your implementation on SageMaker image. So, if you need the response data in another format, maybe you will need to use a <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">custom image<\/a>. I normally use a custom image, which I can define how I want to handle my data on requests\/responses.<\/p>\n\n<p>In terms of a production task, I certainly recommend you check Batch transform jobs from SageMaker. You can provide an input file (the S3 path) and also a destination file (another S3 path). The SageMaker will run the batch predictions and will persist a file with the results. Also, you won't need to deploy your model to an endpoint, when this job run, will create an instance of your endpoint, download your data to predict, do the predictions, upload the output, and shut down the instance. You only need a trained model.<\/p>\n\n<p>Here some info about Batch transform jobs:<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-batch.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-batch.html<\/a><\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-batch-transform.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-batch-transform.html<\/a><\/p>\n\n<p>I hope it helps, let me know if need more info.<\/p>\n\n<p>Regards.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5,
        "Solution_readability":10.5,
        "Solution_reading_time":30.91,
        "Solution_score_count":4.0,
        "Solution_sentence_count":22,
        "Solution_word_count":341,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1533631086080,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm writing a lambda (in node.js 6.10) to update an endpoint SageMaker. To do so I have to create a new HyperParamterTuningJob (and then describe it).\nI succeeded to call all functions of the service SageMaker from the sdk (like listModels, createTrainingJob, ...) (<a href=\"https:\/\/docs.aws.amazon.com\/AWSJavaScriptSDK\/latest\/AWS\/SageMaker.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AWSJavaScriptSDK\/latest\/AWS\/SageMaker.html<\/a>) except some of them.<\/p>\n\n<p>All the functions that are related to HyperParameterTuningJob \n(createHyperParameterTuningJob, describeHyperParameterTuningJob, listHyperParameterTuningJobs and stopHyperParameterTuningJob) \nare not known in the sdk by the lambda.<\/p>\n\n<p>I have attached the policy 'AmazonSageMakerFullAccess' to the role IAM used (where all these functions are allowed). So the error can't come from a problem of authorization.<\/p>\n\n<p>I have already created a HyperParameterTuningJob (by the interface of AWS) called 'myTuningJob'.\nI have an error everytime I use the function describeHyperParamterTuningJob.\nHere is my lambda code : <\/p>\n\n<pre><code>const AWS = require('aws-sdk');\nconst sagemaker = new AWS.SageMaker({region: 'eu-west-1', apiVersion: '2017-07-24'});\nvar role = 'arn:aws:iam::xxxxxxxxxxxx:role\/service-role\/AmazonSageMaker-ExecutionRole-xxxxxxxxxxxxxxx';\n\nexports.handler = (event, context, callback) =&gt; {\n    var params = {\n        HyperParameterTuningJobName: 'myTuningJob'\n    };\n\n    sagemaker.describeHyperParameterTuningJob(params, function(err, data) {\n        if (err) console.log(err, err.stack);\n        else console.log(data);\n    });\n};\n<\/code><\/pre>\n\n<p>When I try to test this code in AWS lambda, it returns this result in the console :<\/p>\n\n<pre><code>Function Logs:\nSTART RequestId: 6e79aaa4-9a18-11e8-8dcd-d58423b413c1 Version: $LATEST\n2018-08-07T08:03:56.336Z    6e79aaa4-9a18-11e8-8dcd-d58423b413c1    TypeError: sagemaker.describeHyperParameterTuningJob is not a function\nat exports.handler (\/var\/task\/index.js:10:15)\nEND RequestId: 6e79aaa4-9a18-11e8-8dcd-d58423b413c1\nREPORT RequestId: 6e79aaa4-9a18-11e8-8dcd-d58423b413c1  Duration: 50.00 ms   \nBilled Duration: 100 ms Memory Size: 128 MB Max Memory Used: 32 MB  \nRequestId: 6e79aaa4-9a18-11e8-8dcd-d58423b413c1 Process exited before completing request\n<\/code><\/pre>\n\n<p>When I call all other functions of the SageMaker service from the sdk, it runs correctly, whitout any error.<\/p>\n\n<p>I don't find any explanation in the documentation of why these functions related to HyperParameterTuningJob are not recognized as functions in the sdk.<\/p>\n\n<p>Does anyone have any idea of why it doesn't work ? Or any solutions to call theses functions ?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533631633087,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51722555",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":12.6,
        "Challenge_reading_time":35.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":null,
        "Challenge_title":"Functions of sagemaker service from js-sdk not recognize by AWS Lambda",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":212.0,
        "Challenge_word_count":296,
        "Platform":"Stack Overflow",
        "Poster_created_time":1533631086080,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>In AWS Lambda, only the sdk that have a sable version are available.\nThe sdk of the SageMaker service is not stable yet, so functions related to HyperParameterTuningJob are not in the version of the sdk included in AWS Lambda.<\/p>\n\n<p>To use theses functions, you need to install the latest version of the sdk on local on your machine (with npm install aws-sdk). \nThen zip the node_modules folder and your script (called index.js), then upload this zip folder into the AWS lambda.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":8.5,
        "Solution_reading_time":5.93,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5,
        "Solution_word_count":82,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>As the document     <br \/>\nA composed model is created by taking a collection of custom models and assigning them to a single model ID. You can assign up to 100 trained custom models to a single composed model ID. When a document is submitted to a composed model, the service performs a classification step to decide which custom model accurately represents the form presented for analysis.     <\/p>\n<p>What\u2019s the price for the classification step? <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1669041105107,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1098169\/compose-model",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":5.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Compose model",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":76,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=08dc8e09-5319-4bd7-9cf8-cce17f133981\">@KenSmith  <\/a>     <\/p>\n<p>Thanks for reaching out to us and sorry for the confusion of the document.     <\/p>\n<p>There is <strong>no extra fee<\/strong> for the classification you mentioned in the document. You only pay for the custom model you finally run for your document.    <\/p>\n<p>I will raise a ticket to fix the document, thanks a lot for pointing out it.    <\/p>\n<p>I hope this helps!    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks! <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":7.0,
        "Solution_reading_time":7.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7,
        "Solution_word_count":88,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am attempting to pass json data into my sagemaker model through a lambda function. Currently, I am using a testing model that makes relatively quick inferences and returns them to the lambda function through the invoke_endpoint call. However, eventually a more advanced model will be implemented which might take longer than a lambda function can fun for (15 minutes maximum) to produce inferences. In the case that I call invoke_endpoint in one lambda function, can I return the response to another lambda function which is invoked by the sagemaker endpoint response? Even better, can I shut down the current lambda function after sending the data to sagemaker, and re-invoke it upon a response? I need to store the inference in DynamoDB, which is why I need a response (Unless I can update the saved model to store inferences directly, in which case I need the lambda function to not expect a response from invoke_endpoint). Sorry for my ignorance, I am a bit new to sagemaker.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623875736457,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68009703",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.7,
        "Challenge_reading_time":12.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Can \"Invoke_endpoint\" calls timeout a lambda function?",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":181.0,
        "Challenge_word_count":172,
        "Platform":"Stack Overflow",
        "Poster_created_time":1622063222448,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":5.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>When calling <code>invoke_endpoint<\/code>, the underlying model invocation must take <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html#your-algorithms-inference-code-how-containe-serves-requests\" rel=\"nofollow noreferrer\">less than 1 minute<\/a>. If a single model execution needs more time to execute, consider running the model in Lambda itself, in SageMaker Training API (if its coldstart is acceptable) or in a custom service. If the invocation is made of several shorter calls you can also chain multiple services together with Step Functions.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":18.0,
        "Solution_reading_time":7.82,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":66,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"A wants to manage Sagemaker resources (such as models and endpoints) via CloudFormation. As part of their model deployment pipeline, they'd like to be able to create or update existing Sagemaker Endpoint with new model data. Customers wants to re-use the same endpoint name for a given workload. \n\n**Question:**\n\nHow to express in CF a following logic:\n1. If Sagemaker endpoint with name \"XYZ\" doesn't exist in customer account, then create a new endpoint;\n2. If Sagemaker endpoint with name \"XYZ\" already exist, then update existing endpoint with new model data.",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607356793000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668615829184,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUXiLSnlxkQHKzQVFj6GKT7w\/create-or-update-sagemaker-endpoint-via-cloudformation",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":7.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Create or update Sagemaker Endpoint via CloudFormation",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":437.0,
        "Challenge_word_count":97,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This functionality of \"UPSERT\" type does not exist in CFn natively. You would need to use a Custom Resource to handle this logic.\nOne alternative that is not exactly what you asked for but might be a decent compromise is to use a Parameter to supply the endpoint if it does exist. Then use a condition to check the value. If the paramter is blank then create an endpoint if not use the value supplied.\nI know this is not what you asked for but it allows you to avoid the custom resource solution.\n\nSample of similiar UPSERT example for a VPC:\n\n```\nParameters :\n\n  Vpc:\n    Type: AWS::EC2::VPC::Id\n\nConditions:\n\n  VpcNotSupplied: !Equals [!Ref Vpc, '']\n\nResources:\n\n  NewVpc:\n    Type: AWS::EC2::VPC\n    Condition: VpcNotSupplied\n    Properties:\n      CidrBlock: 10.0.0.0\/16\n\n  SecurityGroup:\n    Type: AWS::EC2::SecurityGroup\n    Properties:\n      GroupDescription: Sample\n      GroupName: Sample\n      VpcId: !If [VpcNotSupplied, !Ref NewVpc, !Ref Vpc ]\n```\n\nHere the `Vpc` input parameter can be supplied if the VPC you wish to use already exists, left blank if you want to create a new one. The NewVPC resource uses the `Condition` to only create if the supplied Vpc parameter value is blank. The Security group then uses the same condition to decide whetehr to use and existing Vpc or the newly created one.\n\nHope this makes sense.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925574694,
        "Solution_link_count":0,
        "Solution_readability":9.4,
        "Solution_reading_time":15.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13,
        "Solution_word_count":204,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi, I trained and deployed a ML model via Auto ML. The result looks like this:  <br \/>\n&quot;\\&quot;{\\\\&quot;result\\\\&quot;: [\\\\&quot;Test\\\\&quot;]}\\&quot;&quot;<\/p>\n<p>Once I did the same with an endpoint created with the Azure ML Designer my result looks like this:  <br \/>\n&quot;{\\&quot;Results\\&quot;: {\\&quot;WebServiceOutput0\\&quot;: [{\\&quot;Scored Labels\\&quot;: \\&quot;Test\\&quot;}]}}&quot;<\/p>\n<p>Is there a way to configure the response that it looks similar to the AutoML response?<\/p>\n<p>Thanks :)<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606819526077,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/181635\/how-to-configute-webserviceoutput",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":7.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"How to configute WebServiceOutput?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":65,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=fe5bc84e-425f-4db0-a234-78d2a8fbbae1\">@ID_27051995  <\/a> Unfortunately, AutoML and AML Designer currently generates 2 different swagger format automatically, and there is no way to configure the output format. We are working on to address this inconsistency, and the Designer swagger format will be the converged format. Cheers!<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":16.6,
        "Solution_reading_time":4.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2,
        "Solution_word_count":44,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1582182357612,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":155.0,
        "Answerer_view_count":27.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an ML model (trained locally) in python. Previously the model has been deployed to a Windows IIS server and it's working fine.<\/p>\n<p>Now, I am trying to deploy it as a service on Azure container instance (ACI) with 1 core, and 1 GB of memory. I took references from <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python\" rel=\"nofollow noreferrer\">one<\/a> and <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-existing-model\" rel=\"nofollow noreferrer\">two<\/a> Microsoft docs. The docs use SDK for all the steps, but <strong>I am using the GUI feature from the Azure portal<\/strong>.<\/p>\n<p>After registering the model, I created an entry script and a conda environment YAML file (see below), and uploaded both to &quot;Custom deployment asset&quot; (at Deploy model area).<\/p>\n<p>Unfortunately, after hitting deploy, the Deployment state is stuck at Transitioning state. Even after 4 hours, the state remains the same and there were no Deployment logs too, so I am unable to find what I am doing wrong here.<\/p>\n<blockquote>\n<p>NOTE: below is just an excerpt of the entry script<\/p>\n<\/blockquote>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport pickle\nimport re, json\nimport numpy as np\nimport sklearn\n\ndef init():\n    global model \n    global classes\n    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'randomForest50.pkl')\n    model = pickle.load(open(model_path, &quot;rb&quot;))\n\n    classes = lambda x : [&quot;F&quot;, &quot;M&quot;][x]\n\ndef run(data):\n    try:\n        namesList = json.loads(data)[&quot;data&quot;][&quot;names&quot;]\n        pred = list(map(classes, model.predict(preprocessing(namesList))))\n        return str(pred[0])\n    except Exception as e:\n        error = str(e)\n        return error\n<\/code><\/pre>\n<pre class=\"lang-yaml prettyprint-override\"><code>name: gender_prediction\ndependencies:\n- python\n- numpy\n- scikit-learn\n- pip:\n    - pandas\n    - pickle\n    - re\n    - json\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1611073723990,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1611079944120,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65795579",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":11.3,
        "Challenge_reading_time":25.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"Debugging AML Model Deployment",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":106.0,
        "Challenge_word_count":231,
        "Platform":"Stack Overflow",
        "Poster_created_time":1582182357612,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":155.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>The issue was in the YAML file. <strong>The dependencies\/libraries in the YAML should be according to conda environment<\/strong>. So, I changed everything accordingly, and it worked.<\/p>\n<p>Modified YAML file:<\/p>\n<pre><code>name: gender_prediction\ndependencies:\n- python=3.7\n- numpy\n- scikit-learn\n- pip:\n    - azureml-defaults\n    - pandas\n    - pickle4\n    - regex\n    - inference-schema[numpy-support]   \n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":10.1,
        "Solution_reading_time":5.17,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5,
        "Solution_word_count":42,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have followed the AWS tutorial(<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\/object_detection_image_json_format.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\/object_detection_image_json_format.ipynb<\/a>) and trained my first model using SageMaker.<\/p>\n\n<p>The end result is an archive containing the following files:\n- hyperparams.json\n- model_algo_1-0000.params\n- model_algo_1-symbol.json<\/p>\n\n<p>I am not familiar with this format, and was not able to load it into Keras via keras.models.model_from_json()<\/p>\n\n<p>I am assuming this is a different format or an AWS proprietary one.<\/p>\n\n<p>Can you please help me identify the format?\nIs it possible to load this into a Keras model and do inference without an EC2 instance(locally)?<\/p>\n\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580061744520,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59921196",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":18.0,
        "Challenge_reading_time":13.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Can you load a SageMaker trained model into Keras?",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":212.0,
        "Challenge_word_count":94,
        "Platform":"Stack Overflow",
        "Poster_created_time":1533465584552,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bucharest, Romania",
        "Poster_reputation_count":341.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>Built-in algorithms are implemented with Apache MXNet, so that's how you'd load the model locally. load_checkpoint() is the appropriate API: <a href=\"https:\/\/mxnet.apache.org\/api\/python\/docs\/api\/mxnet\/model\/index.html#mxnet.model.load_checkpoint\" rel=\"nofollow noreferrer\">https:\/\/mxnet.apache.org\/api\/python\/docs\/api\/mxnet\/model\/index.html#mxnet.model.load_checkpoint<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":22.9,
        "Solution_reading_time":5.23,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3,
        "Solution_word_count":24,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"Hello,  \nI have followed the DeepAR Chicago Traffic violations notebook example. The Model and Endpoint has been created and the forecasting is working.  \n  \nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_applying_machine_learning\/deepar_chicago_traffic_violations\/deepar_chicago_traffic_violations.ipynb  \n  \nHowevr, I haven't deleted the model nor the endpoint in order to use it externally. I have created a Python script on an EC2 that tries to load the endpoint and passes the data to it to get a prediction, and here is what I am doing:  \n  \n1. Loading the CSV exactly the way I did it on the notebook  \n2. Parsing the CSV the same way I did on the notebook for the \"predictor.predict\" command  \n3. Instead of using the \"predictor.predict\", I am using \"invoke_endpoint\" to load the endpoint and passing the data from the previous point  \n4. Instead of getting the same response I got on the notebook, I am getting the following message:  \n\"type: <class 'list'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object\"  \n  \nNot sure what the issue is, seems that it requires a byte data... I guess I cannot send the data as a list to the endpoint and I need to serialize it or to encode it? convert to to JSON? to Bytes?  \n  \nAny help will be appreciated.  \nRegards",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625081705000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668423337511,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUpamBayk2RT6c6KuNop0DQQ\/how-to-pass-data-to-an-endpoint",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":9.0,
        "Challenge_reading_time":16.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"How to pass data to an endpoint",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":613.0,
        "Challenge_word_count":205,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello,  \n  \nSo the issue here is the predictor.predict command converts the data to the format necessary for the endpoint to understand, thus you need to serialize or encode the payload by yourself. To do this you can work with something like json.dumps(payload) or for a byte array json.dumps(payload).encode().  \n  \n If you want to use the predictor class this is taken care of by the serializer option. The serializer encodes\/decodes the data for us and lets you simply call the endpoint through the predictor class. An example of this is the following code snippet:   \n  \nfrom sagemaker.serializers import IdentitySerializer  \nfrom sagemaker.deserializers import JSONDeserializer  \nserializer=IdentitySerializer(content_type=\"application\/json\")  \n  \nHope this helps!  \n  \nTo check out the various serializer options that can work for your different use cases check the following link.   \nSerializers: https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html  \n  \nEdited by: rvegira-aws on Jul 22, 2021 9:22 AM  \n  \nEdited by: rvegira-aws on Jul 22, 2021 9:24 AM",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1626971072000,
        "Solution_link_count":1,
        "Solution_readability":13.1,
        "Solution_reading_time":13.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11,
        "Solution_word_count":143,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1587281590603,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":473.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using sagemaker model monitor.<\/p>\n\n<p>When capturing data, it outputs the following json file.<\/p>\n\n<pre><code>{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text\/csv\",\"mode\":\"INPUT\",\"data\":\"MSwwLjUzLDAuNDIsMC4xMzUsMC42NzcsMC4yNTY1LDAuMTQxNSwwLjIx\",\"encoding\":\"BASE64\"},\"endpointOutput\":{\"observedContentType\":\"text\/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"MTEuNjQzNDU1NTA1MzcxMDk0\",\"encoding\":\"BASE64\"}},\"eventMetadata\":{\"eventId\":\"33404924-c0d4-4044-9dc2-1e1f5575cb0a\",\"inferenceTime\":\"2020-06-04T05:45:45Z\"},\"eventVersion\":\"0\"}\n<\/code><\/pre>\n\n<p>I want the encoding to be csv but somehow it outputs base64.<br>\nWhen or where do we change the setting of the encoding?<br>\nIs it during the invoking the endpoint? or set when making endpoint config.<br>\nI looked for some documents but I couldn't find it.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591250114543,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62187748",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":17.2,
        "Challenge_reading_time":11.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Change datacapture encoding data to csv",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":620.0,
        "Challenge_word_count":68,
        "Platform":"Stack Overflow",
        "Poster_created_time":1532422348876,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":27.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>I just came across this same problem! Seems like you need to specify <code>CaptureContentTypeHeader<\/code> params to tell SageMaker which content type headers to treat as CSV (or JSON), versus the default which is to base64 encode the payload!<\/p>\n<p>So e.g. adding the following to your <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateEndpointConfig.html\" rel=\"nofollow noreferrer\">CreateEndpointConfig<\/a> call or boto3\/sagemaker SDK equivalent should fix it:<\/p>\n<pre><code>{\n   &quot;DataCaptureConfig&quot;: { \n      &quot;CaptureContentTypeHeader&quot;: { \n         &quot;CsvContentTypes&quot;: [ &quot;text\/csv&quot; ]\n      },\n   }\n}\n<\/code><\/pre>\n<p>I guess this is to allow for non-standard Content-Type headers? Providing a layer of config to resolve e.g:<\/p>\n<ul>\n<li><code>application\/x-mycoolmodel<\/code> -&gt; <code>JSON<\/code>, versus<\/li>\n<li><code>application\/x-secretsauce<\/code> -&gt; <code>BASE64<\/code><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":14.2,
        "Solution_reading_time":12.37,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7,
        "Solution_word_count":90,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am designing a learning management system and inflow for the website is more in some cases and  less in another time. I would like to know about the getting the vCPU's which are scaled up to make it down after the stipulated time. I found a document regarding <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/best-practices\/auto-scaling\" rel=\"nofollow noreferrer\">scaling up<\/a> but didn't find a way to scale it down.<\/p>\n<p>Any help is appreciated.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655983168110,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72729267",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":6.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Degrading the services automatically by autoscaling in azure services - vCPU",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":49.0,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Poster_created_time":1652123310643,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":41.0,
        "Solution_body":"<p>There is a chance of auto scaling for the normal services in azure cloud services, that means for stipulated time you can increase or decrease as mentioned in the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/cloud-services\/cloud-services-how-to-scale-portal\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>\n<p>When it comes for vCPU which is cannot be performed automatically. vCPU can be scaled up based on the request criteria and in the same manner we need to request the support team to scale those down to the normal.<\/p>\n<p><strong>There is no specific procedure to make the auto scaling for vCPU operations. We can increase the capacity of core, but to reduce to the normal, we need to approach the support system for manual changing. You can change it from 10 cores to next level 16 cores, but cannot be performed automatic scaling down from 16 cores to 10 cores.<\/strong><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":9.6,
        "Solution_reading_time":11.0,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7,
        "Solution_word_count":134,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1566583092316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":479.0,
        "Answerer_view_count":51.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>My webservice deployed with Azure Machine Learning Studio exposes a classification model. Since the last re-training and re-deployment, in circa 1% of the processed cases in production, I get either of the two  following out-of-memory (possibly correlated) errors:<\/p>\n<ol>\n<li>&quot;The model consumed more memory than was appropriated for it. Maximum allowed memory for the model is 2560 MB. Please check your model for issues.&quot;<\/li>\n<li>&quot;The following error occurred during evaluation of R script: R_tryEval: return error: Error: cannot allocate vector of size 57.6 Mb&quot;<\/li>\n<\/ol>\n<p>I did not mange to find anything to solve this issue according to the official documentation.<\/p>\n<p>Note that these errors occur exclusively while trying to consume the webservice (and not while training, evaluation and deployment).<\/p>\n<p>Also, consuming the webservice in batch mode, as suggested <a href=\"https:\/\/social.microsoft.com\/Forums\/azure\/he-IL\/ccf4c683-f904-4117-8a4e-3258a56515f9\/azureml-execure-r-script-cannot-allocate-vector-of-size-818-mb?forum=MachineLearning\" rel=\"nofollow noreferrer\">here<\/a>, is not a viable option for my business use case.<\/p>\n<p>Is there a way to increase the memory usage limit for webservices deployed in Azure ML Studio?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592831881063,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62515398",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":17.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Out-of-memory error webservice deployed with Azure ML Studio",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":341.0,
        "Challenge_word_count":167,
        "Platform":"Stack Overflow",
        "Poster_created_time":1500322674452,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":91.0,
        "Poster_view_count":21.0,
        "Solution_body":"<p>Currently, there's no way to increase memory limit in Classic Studio. We encouraged customers to try <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-designer\" rel=\"nofollow noreferrer\">Azure Machine Learning designer (preview)<\/a>, which provides similar drag and drop ML modules plus scalability, version control, and enterprise security. Furthermore, with Designer, the endpoints are deployed to AKS where no limit other than cluster resource is imposed.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":13.6,
        "Solution_reading_time":6.35,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":57,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1553088438367,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tehran, Tehran Province, Iran",
        "Answerer_reputation_count":415.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am implementing an anomaly detection web service using <code>MLflow<\/code> and <code>sklearn.pipeline.Pipeline()<\/code>. The aim of the model is to detect web crawlers using server log and <code>response_length<\/code> column is one of my features. After serving model, for testing the web service I send below request that contains the 20 first columns of the train data.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>$ curl  --location --request POST '127.0.0.1:8000\/invocations'\n        --header 'Content-Type: text\/csv' \\\n        --data-binary 'datasets\/test.csv'\n<\/code><\/pre>\n<p>But response of the web server has status code 400 (BAD REQUEST) and this JSON body:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;error_code&quot;: &quot;BAD_REQUEST&quot;,\n    &quot;message&quot;: &quot;Incompatible input types for column response_length. Can not safely convert float64 to &lt;U0.&quot;\n}\n<\/code><\/pre>\n<p>Here is the model compilation MLflow Tracking component log:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>[Pipeline] ......... (step 1 of 3) Processing transform, total=11.8min\n[Pipeline] ............... (step 2 of 3) Processing pca, total=   4.8s\n[Pipeline] ........ (step 3 of 3) Processing rule_based, total=   0.0s\n2021\/07\/16 04:55:12 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n2021\/07\/16 04:55:12 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: &quot;\/home\/matin\/workspace\/Rahnema College\/venv\/lib\/python3.8\/site-packages\/mlflow\/models\/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values &lt;https:\/\/www.mlflow.org\/docs\/latest\/models.html#handling-integers-with-missing-values&gt;`_ for more details.&quot;\nLogged data and model in run: 8843336f5c31482c9e246669944b1370\n\n---------- logged params ----------\n{'memory': 'None',\n 'pca': 'PCAEstimator()',\n 'rule_based': 'RuleBasedEstimator()',\n 'steps': &quot;[('transform', &lt;log_transformer.LogTransformer object at &quot;\n          &quot;0x7f05a8b95760&gt;), ('pca', PCAEstimator()), ('rule_based', &quot;\n          'RuleBasedEstimator())]',\n 'transform': '&lt;log_transformer.LogTransformer object at 0x7f05a8b95760&gt;',\n 'verbose': 'True'}\n\n---------- logged metrics ----------\n{}\n\n---------- logged tags ----------\n{'estimator_class': 'sklearn.pipeline.Pipeline', 'estimator_name': 'Pipeline'}\n\n---------- logged artifacts ----------\n['model\/MLmodel',\n 'model\/conda.yaml',\n 'model\/model.pkl',\n 'model\/requirements.txt']\n<\/code><\/pre>\n<p>Could anyone tell me exactly how I can fix this model serve problem?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1626398499507,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1657456057947,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68402406",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":43.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":null,
        "Challenge_title":"MLflow webserver returns 400 status, \"Incompatible input types for column X. Can not safely convert float64 to <U0.\"",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":787.0,
        "Challenge_word_count":352,
        "Platform":"Stack Overflow",
        "Poster_created_time":1553088438367,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tehran, Tehran Province, Iran",
        "Poster_reputation_count":415.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p>The problem caused by <code>mlflow.utils.autologging_utils<\/code> WARNING.<\/p>\n<p>When the model is created, data input signature is saved on the <code>MLmodel<\/code> file with some.\nYou should change <code>response_length<\/code> signature input type from <code>string<\/code> to <code>double<\/code> by replacing<\/p>\n<pre><code>{&quot;name&quot;: &quot;response_length&quot;, &quot;type&quot;: &quot;double&quot;}\n<\/code><\/pre>\n<p>instead of<\/p>\n<pre><code>{&quot;name&quot;: &quot;response_length&quot;, &quot;type&quot;: &quot;string&quot;}\n<\/code><\/pre>\n<p>so it doesn't need to be converted. After serving the model with edited <code>MLmodel<\/code> file, the web server worked as expected.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":14.6,
        "Solution_reading_time":9.28,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4,
        "Solution_word_count":68,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1568318861627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":486.0,
        "Answerer_view_count":75.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I trained an xgboost model on AWS-Sagemaker and created an endpoint. Now I want to call the endpoint using AWS Lambda and AWS API. I created an lambda function and added the below mentioned code for my xgboost model. When I try to test it, the function is throwing a ParamValidation error. Here is my code<\/p>\n\n<pre><code>import json\nimport os\nimport csv\nimport io\nimport boto3\nendpointname =os.environ['endpointname'] #name of the endpoint I created in sagemaker\nruntime = boto3.client('runtime.sagemaker')\ndef lambda_handler(event, context):\n    print(\"Recieved Event: \"+json.dumps(event,indent=2))\n    data=json.loads(json.dumps(event))\n    print(data)\n    response = runtime.invoke_endpoint(EndpointName=endpointname,ContentType='text\/csv',Body=data)\n    print(response)\n    result = json.loads(response['Body'].read().decode())\n    print(int(float(result))) #sagemaker xgb returns bytes type for the test case\n<\/code><\/pre>\n\n<p>The test event I created is dict type. The function is throwing  <code>Invalid type for parameter Body, value: {'Time':'7'}, type: &lt;class 'dict'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object<\/code>\nIt means I should pass either byte or bytearray instead of dict type into my event. But when I read this <a href=\"https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/python-programming-model-handler-types.html\" rel=\"nofollow noreferrer\">AWS Lambda doc<\/a> It says that my event type can only be dict,int,list,float,str, or None type. I followed the steps mentioned in <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">this<\/a> aws doc to create my lambda function. Can someone please explain why my code is throwing above mentioned error?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1573829888267,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1573837537747,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58879596",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":24.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"Unable to call XG-Boost endpoint created in sagemaker using AWS-Lambda",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":519.0,
        "Challenge_word_count":210,
        "Platform":"Stack Overflow",
        "Poster_created_time":1568318861627,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hyderabad, Telangana, India",
        "Poster_reputation_count":486.0,
        "Poster_view_count":75.0,
        "Solution_body":"<p><code>data=json.loads(json.dumps(event))<\/code> is a redundant operation. <code>data=event<\/code> will return <code>True<\/code>. The event we provided for the test case is of type dict. It has a key value pair. key can be anything and the value should be a single string of all the predictor variables separated by comas. For predicting the output, we need value of the test case. So declare, for example, <code>payload=data['key']<\/code> then change <code>Body=payload<\/code> inside <code>response<\/code>. Then it will work.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":8.0,
        "Solution_reading_time":6.77,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8,
        "Solution_word_count":72,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1631803441500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"mexico",
        "Answerer_reputation_count":1258.0,
        "Answerer_view_count":685.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to un-deploy model from an endpoint following <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform.Endpoint#google_cloud_aiplatform_Endpoint_undeploy\" rel=\"nofollow noreferrer\">this documentation<\/a>.<\/p>\n<pre><code>Endpoint.undeploy(deployed_model_id=model_id)\n<\/code><\/pre>\n<p>I even tried <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.endpoints\/undeployModel\" rel=\"nofollow noreferrer\">google api<\/a>. Same Issue with this as well.<\/p>\n<p>Getting 404 error<\/p>\n<blockquote>\n<p>The Deployed Model with ID <code>2367889687867<\/code> is missing.<\/p>\n<\/blockquote>\n<p><strong>INFO:<\/strong><\/p>\n<ol>\n<li>Both model and Endpoint are in same region.<\/li>\n<li>There is a single model deployed in the endpoint with <code>traffic_percentage=100<\/code>.<\/li>\n<\/ol>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":4,
        "Challenge_created_time":1655220692347,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1655954014260,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72619696",
        "Challenge_link_count":2,
        "Challenge_participation_count":5,
        "Challenge_readability":16.2,
        "Challenge_reading_time":12.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"GCP AI Platform Vertex endpoint model undeploy : 404 The DeployedModel with ID `2367889687867` is missing",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":271.0,
        "Challenge_word_count":80,
        "Platform":"Stack Overflow",
        "Poster_created_time":1550779047856,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":363.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>The <code>deployed_model_id<\/code> is different from the <code>model_id<\/code>.That\u2019s why you are getting the Error 404, it is searching for something that is not the same.<\/p>\n<p>You can get the <code>deployed_model_id<\/code> by:<\/p>\n<ul>\n<li>list_models()<\/li>\n<li>list()<\/li>\n<\/ul>\n<p>Using <code>list_models()<\/code> brings you a list of all the deployed models ids, while using <code>list()<\/code> only brings one, you can add filters such as <code>display_name<\/code>, <code>model_id<\/code>, <code>region<\/code>, etc.<\/p>\n<pre><code>list(\n    filter= \u2018display_name= \u201cdisplay_name\u201d\u2019,\n)\n<\/code><\/pre>\n<p>You also can get the <code>deployed_model_id<\/code> using the Cloud SDK.<\/p>\n<pre><code>gcloud ai models list --region=$REGION --filter=&quot;DISPLAY_NAME: $NAME&quot; | grep &quot;MODEL_ID&quot; | cut -f2 -d: | sed 's\/\\s\/\/'\n<\/code><\/pre>\n<p>Additionally, you can specify the <code>deployed_model_id<\/code> when you are deploying your model using Cloud SDK the command should look like:<\/p>\n<pre><code>gcloud ai endpoints deploy-model $endpoint --project=$project --region=$region --model=$model_id --display-name=$model_name --deployed-model-id=$deployed_model_id\n<\/code><\/pre>\n<p>There are some flags that are required when you deploy a model such as endpoint, project, region, model and display name. And there are others that are <a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/ai\/endpoints\/deploy-model#:%7E:text=the%20uploaded%20model.-,OPTIONAL%20FLAGS,-%2D%2Daccelerator%3D%5Bcount\" rel=\"nofollow noreferrer\">optional flags<\/a> that you can use deployed_model_id is one of them.(I don\u2019t know if this is possible but you could set the deployed_model_id as the same as the model_id).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":15.7,
        "Solution_reading_time":22.36,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8,
        "Solution_word_count":182,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have 2 experiments A and B in Azure MLS classic. I need the web service output of experiment A as one of the web service inputs for experiment B.  Please let me know if it is possible and if yes, how I can do it.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1592407262483,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/37128\/connect-2-separate-experiments-via-webservice-azur",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.4,
        "Challenge_reading_time":3.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Connect 2 separate experiments via webservice - Azure MLS Classic",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":54,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I used export module in experiment A and import module in experiment B to transfer the output of A as input of B.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":12.3,
        "Solution_reading_time":1.44,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1,
        "Solution_word_count":23,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1571417311507,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Redmond, WA, USA",
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When trying to make predictions for forecasting models using Azure ML Service, the swagger.json includes the following schema for input:<\/p>\n\n<pre><code>\"example\": {\"data\": [{\"date\": \"2019-08-30T00:00:00.000Z\", \"y_query\": 1.0}]}\n<\/code><\/pre>\n\n<p>However, when I feed this as an input to generate predictions, I receive the following error:<\/p>\n\n<pre><code>data= {\"data\": [{\"date\": \"2019-08-30T00:00:00.000Z\", \"y_query\": 1 }]}\n# Convert to JSON string\ninput_data = json.dumps(data)\n\n# Set the content type\nheaders = {'Content-Type': 'application\/json'}\n# If authentication is enabled, set the authorization header\n#headers['Authorization'] = f'Bearer {key}'\n\n# Make the request and display the response\nresp = requests.post(scoring_uri, input_data, headers=headers)\nprint(resp.text)\n\n<\/code><\/pre>\n\n<pre><code>\"{\\\"error\\\": \\\"DataException:\\\\n\\\\tMessage: y values are present for each date. Nothing to forecast.\\\\n\\\\tInnerException None\\\\n\\\\tErrorResponse \\\\n{\\\\n    \\\\\\\"error\\\\\\\": {\\\\n        \\\\\\\"code\\\\\\\": \\\\\\\"UserError\\\\\\\",\\\\n        \\\\\\\"inner_error\\\\\\\": {\\\\n            \\\\\\\"code\\\\\\\": \\\\\\\"InvalidData\\\\\\\"\\\\n        },\\\\n        \\\\\\\"message\\\\\\\": \\\\\\\"y values are present for each date. Nothing to forecast.\\\\\\\"\\\\n    }\\\\n}\\\"}\"\n<\/code><\/pre>\n\n<p>I have tried not passing a y value, which causes an 'expected two axis got one' and passing 0 as the y_query. Any guidance on how to make predictions using this approach would be greatly appreciated. <\/p>\n\n<p>The documentation for web services is here: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-consume-web-service\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-consume-web-service<\/a><\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":6,
        "Challenge_created_time":1571058071523,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58377444",
        "Challenge_link_count":2,
        "Challenge_participation_count":8,
        "Challenge_readability":12.4,
        "Challenge_reading_time":22.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"swagger.json example json for forecast model doesn't seem to return predictions",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":530.0,
        "Challenge_word_count":180,
        "Platform":"Stack Overflow",
        "Poster_created_time":1464949169832,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":25.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>Try using nan as the value for y_query. and make sure the date is the next time unit after the one that was used in the training set.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":4.0,
        "Solution_reading_time":1.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2,
        "Solution_word_count":28,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":212.0517972223,
        "Challenge_answer_count":1,
        "Challenge_body":"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-data-capture-endpoint.html\n\nI have followed the steps mentioned in this and it appears I cannot change the encoding for EndpointOutput in datacapture file. It's coming BASE64 for xgboost model. I am using latest version 1.2.3.\n\nFor monitor scheduler it required both EndpointOutput and EndpointInput to have the same encoding. My EndpointInput  is CSV but EndpointOutput is coming to be BASE64 and nothing can change it.\n\nThis is causing issue while run of analyzer. After baseline is generated and data is captured, when monitoring schedule runs the analyzer it throws error of Encoding mismatch. For it to run EndpointOutput and EndpointInput should have same encoding.\n\nI saw we cannot do anything to change the encoding of output. I used LightGBM, CatBoost algorithms also and found for these EndpointOuput encoding is JSON, which is readable but still not solving the purpose.\n\nIs there a way we can change EndpointOutput Encoding for DataCapture.",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1673956972508,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1674302800224,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUGSFVfrFJS_KsdrOMeepDPg\/model-monitor-capture-data-endpointoutput-encoding-is-base64",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.5,
        "Challenge_reading_time":13.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Model Monitor Capture data - EndpointOutput Encoding is BASE64",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":77.0,
        "Challenge_word_count":156,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Output encoding can be configured by using the [CaptureContentTypeHeader \nin EndpointConfig.DataCaptureConfig](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DataCaptureConfig.html#sagemaker-Type-DataCaptureConfig-CaptureContentTypeHeader). I believe since this is not being set, default encoding i.e. base64 is being used. \n\nPlease try once with this attribute set as below:\n```\n\"CaptureContentTypeHeader\": { \n         \"CsvContentTypes\": [ \"text\/csv\" ]\n      }\n```\n> Assuming that content_type\/accept is \"text_csv\" for the concerned model.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1675066186694,
        "Solution_link_count":1,
        "Solution_readability":22.1,
        "Solution_reading_time":7.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":47,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":237.2008941667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The Amazon documentation lists several approaches to evaluate a model (e.g. cross validation, etc.) however these methods does not seem to be available in the Sagemaker Java SDK. \nCurrently if we want to do 5-fold cross validation it seems the only option is to create 5 models (and also deploy 5 endpoints) one model for each subset of data and manually compute the performance metric (recall, precision, etc.). <\/p>\n\n<p>This approach is not very efficient and can also be expensive need to deploy k-endpoints, based on the number of folds in the k-fold validation.<\/p>\n\n<p>Is there another way to test the performance of a model?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526673699553,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50418501",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":8.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker model evaluation",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1442.0,
        "Challenge_word_count":107,
        "Platform":"Stack Overflow",
        "Poster_created_time":1428454496052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Amazon SageMaker is a set of multiple components that you can choose which ones to use. <\/p>\n\n<p>The built-in algorithms are designed for (infinite) scale, which means that you can have huge datasets and be able to build a model with them quickly and with low cost. Once you have large datasets you usually don't need to use techniques such as cross-validation, and the recommendation is to have a clear split between training data and validation data. Each of these parts will be defined with an input channel when you are submitting a training job.  <\/p>\n\n<p>If you have a small amount of data and you want to train on all of it and use cross-validation to allow it, you can use a different part of the service (interactive notebook instance). You can bring your own algorithm or even container image to be used in the development, training or hosting. You can have any python code based on any machine learning library or framework, including scikit-learn, R, TensorFlow, MXNet etc. In your code, you can define cross-validation based on the training data that you copy from S3 to the worker instances. <\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1527527622772,
        "Solution_link_count":0,
        "Solution_readability":10.3,
        "Solution_reading_time":13.5,
        "Solution_score_count":3.0,
        "Solution_sentence_count":8,
        "Solution_word_count":192,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1550713581256,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Santiago, Chile",
        "Answerer_reputation_count":301.0,
        "Answerer_view_count":72.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I've tried deleting\/recreating endpoints with the same name, and wasted a lot of time before I realized that changes do not get applied unless you also delete the corresponding Model and Endpoint configuration so that new ones can be created with that name. <\/p>\n\n<p>Is there a way with the sagemaker python api to delete all three instead of just the endpoint?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1550711269467,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54797698",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.2,
        "Challenge_reading_time":5.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker delete Models and Endpoint configurations with python API",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":4760.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>It looks like AWS is currently in the process of supporting model deletion via API with <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/pull\/647\" rel=\"nofollow noreferrer\" title=\"sagemaker-python-sdk\/pull\/647\">this<\/a> pull request. <\/p>\n\n<p>For the time being Amazon's only <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-cleanup.html\" rel=\"nofollow noreferrer\" title=\"docs.aws.amazon.com\/sagemaker\">recommendation<\/a> is to delete everything via the console. <\/p>\n\n<p>If this is critical to your system you can probably manage everything via Cloud Formation and create\/delete services containing your Sagemaker models and endpoints.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":14.7,
        "Solution_reading_time":8.68,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6,
        "Solution_word_count":67,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":24.0835044444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed a model as a Webservice in Azure ML.Its a simple one and all it does is do a linear Regression .The underlying code is python . Now i need to pass which all columns have to selected as independent variables, dynamically, from the client side . How may i do this in Azure ML studio?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1458567955050,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1458612577147,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36132719",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.1,
        "Challenge_reading_time":4.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Select columns dynamically in Azure ML model",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":238.0,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1422821109972,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":1238.0,
        "Poster_view_count":172.0,
        "Solution_body":"<p>Based on my understanding, I think you want to dynamically get the selected columns data via request the Azure ML webservice with some parameters on the client.<\/p>\n\n<p>You can refer to the offical document <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-web-service-parameters\/\" rel=\"nofollow\">Use Azure Machine Learning Web Service Parameters<\/a> and the blog <a href=\"https:\/\/blogs.technet.microsoft.com\/machinelearning\/2014\/11\/25\/azureml-web-service-parameters\/\" rel=\"nofollow\">AzureML Web Service Parameters<\/a> to know how to set and use the web service parameters to implement your needs via add the selected column names as array into the json parameter <code>GlobalParameters<\/code>.<\/p>\n\n<p>Meanwhile, there is a client sample on GitHub <a href=\"https:\/\/github.com\/nk773\/AzureML_RRSApp\" rel=\"nofollow\">https:\/\/github.com\/nk773\/AzureML_RRSApp<\/a>. Althought it was writen in Java, I think it is easy to understand, then you can rewrite in Python with <code>requests<\/code> package.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1458699277763,
        "Solution_link_count":4,
        "Solution_readability":16.5,
        "Solution_reading_time":13.49,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6,
        "Solution_word_count":113,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1334762714136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation_count":6557.0,
        "Answerer_view_count":2005.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>So I am currently working with an Azure Machine Learning experiment. I was able to create a model and post it as a web service. I was also able to get the response using the sample request\/response code in C# provided in the API documentation that was generated when I created the web service.<\/p>\n\n<p>My problem is, the response provided by the web service contains many information (a long string of info) including the Prediction Score which is the only thing I need for my C# application. The only thing that comes in mind is to use string manipulation methods in order to extract the info I want. But I think there's a better way than that. I am new to HTTP Request\/Response, so please elaborate answers and explanations about it.<\/p>\n\n<p>Here's my code:<\/p>\n\n<pre><code>HttpResponseMessage response = await client.PostAsJsonAsync(\"\", scoreRequest);\n\nif (response.IsSuccessStatusCode)\n{\n    string result = await response.Content.ReadAsStringAsync();\n    Console.WriteLine(\"Result: {0}\", result);\n}\n\nelse\n{\n    Console.WriteLine(string.Format(\"The request failed with status code: {0}\", response.StatusCode));\n\n    \/\/ Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n    Console.WriteLine(response.Headers.ToString());\n\n    string responseContent = await response.Content.ReadAsStringAsync();\n    Console.WriteLine(responseContent);\n}\n<\/code><\/pre>\n\n<p>Here is the Response Message:<\/p>\n\n<pre><code>{\"Results\":{\"output1\":{\"type\":\"table\",\"value\":{\"ColumnNames\":[\"clump_thickness\",\"size_uniformity\",\"shape_uniformity\",\"marginal_adhesion\",\"epithelial_size\",\"bare_nucleoli\",\"bland_chromatin\",\"normal_nucleoli\",\"mitoses\",\"Scored Labels\",\"Scored Probabilities\"],\"ColumnTypes\":[\"Int32\",\"Int32\",\"Int32\",\"Int32\",\"Int32\",\"Nullable`1\",\"Int32\",\"Int32\",\"Int32\",\"Double\",\"Double\"],\"Values\":[[\"10\",\"10\",\"4\",\"8\",\"1\",\"8\",\"3\",\"10\",\"1\",\"1\",\"0.979712069034576\"],[\"10\",\"10\",\"4\",\"8\",\"1\",\"8\",\"3\",\"10\",\"1\",\"1\",\"0.979712069034576\"]]}}}}\n<\/code><\/pre>\n\n<p>I only want the value within \"Values\":[[...]], which in this case, the 9th index or \"1\".<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1456171917173,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1456850000163,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35562896",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.8,
        "Challenge_reading_time":28.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"How to get the Prediction Score in a HttpResponseMessage provided by a Azure ML Web Service?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":264.0,
        "Challenge_word_count":222,
        "Platform":"Stack Overflow",
        "Poster_created_time":1453137182072,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":79.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>You need to use project columns in your AML experiment. Currently, you have a module connected to Web Service Output. Use a <code>project columns<\/code> module before your <code>web service output<\/code> to select just the columns you would like to send to our output instead. <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":8.0,
        "Solution_reading_time":3.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3,
        "Solution_word_count":45,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"I have a customer asking me about the [Rendezvous architecture](https:\/\/towardsdatascience.com\/rendezvous-architecture-for-data-science-in-production-79c4d48f12b). What I'm thinking is, we could implement this in a number of ways, all using endpoint variants:\n\n- Lambda (and probably SQS) around the endpoint;\n- A custom monitoring job;\n- Step Functions\n\nWithout going into details of the above options or of how the evaluation and SLA check will be done, it looks like the several models would fit very well as variants of an endpoint. The thing is, the architecture expects to call them all. Is there a way to directly call all variants of a model, or will a wrapper to identify the variants, call them all and process the results be needed?",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604486652000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1667925743687,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU6bm-EMtOQV6robgbTXClLQ\/running-a-request-against-all-variants-in-an-endpoint",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":9.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Running a request against all variants in an endpoint",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":40.0,
        "Challenge_word_count":121,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"When I last looked into it, it was not possible to query all versions\/variants of the model automatically. You can specify what variant to use when using the `invoke_endpoint` method. I would therefore write a lambda function to invoke each of the endpoints one-by-one (see here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_runtime_InvokeEndpoint.html). To be especially rigorous about it, you can add a function in your lambda code that first retrieves all the endpoint variants (see here: https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.describe_endpoint) then queries them one-by-one, and returns all the results.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1607685345047,
        "Solution_link_count":2,
        "Solution_readability":14.3,
        "Solution_reading_time":9.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5,
        "Solution_word_count":81,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1517578984080,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2090.0,
        "Answerer_view_count":163.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been playing with Amazon Sagemaker. They have amazing sample notebooks in different areas. However, for testing purposes, I want to create an endpoint that returns the result from a function. From what I have seen so far, my understanding is that we can deploy only models but I would like to clarify it.<\/p>\n\n<p>Let's say I want to invoke the endpoint and it should give me the square of the input value. So, I will first create a function:<\/p>\n\n<pre><code>def my_square(x):\n    return x**2\n<\/code><\/pre>\n\n<p>Can we deploy this simple function in Amazon Sagemaker?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1534146357657,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1534368379667,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51817494",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":7.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"deploy a simple function to amazon sagemaker",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":216.0,
        "Challenge_word_count":101,
        "Platform":"Stack Overflow",
        "Poster_created_time":1429147641928,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2282.0,
        "Poster_view_count":264.0,
        "Solution_body":"<p>Yes this is possible but it will need some overhead:\nYou can pass your own docker images for training and inference to sagemaker.<\/p>\n\n<p>Inside this containers you can do anything you want including return your <code>my_square<\/code> function. Keep in mind that you have to write your own flask microservice including proxy and wsgi server(if needed).<\/p>\n\n<p>In my opinion <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">this example<\/a> is the most helpfull one.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":12.1,
        "Solution_reading_time":7.77,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5,
        "Solution_word_count":68,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to determine how the Az ML model deployment works with AKS. If you have a single AKS cluster but attach from two separate workspaces, will models from both workspaces get deployed into different azureml-fe's with different IP addresses OR a single azureml-fe with a single IP address? Reason I ask is because I want to purchase a certificate but am unsure if all the models (regardless of workspace) will get exposed under the same IP Address OR multiple IP Addresses? If its the former, I can do it with one certificate...otherwise I have to do it with multiple certificates or SAN based certificates. So if anyone has experience with this, please let me know!<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655326759547,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72637756",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":9.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning Model deployment as AKS web service from multiple workspaces",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":127.0,
        "Challenge_word_count":128,
        "Platform":"Stack Overflow",
        "Poster_created_time":1376577570772,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":301.0,
        "Poster_view_count":52.0,
        "Solution_body":"<p>By checking AKS webservice class, we can do the multiple services links to single AKS cluster. The endpoint management was described in <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.webservice.aks.akswebservice?view=azure-ml-py\" rel=\"nofollow noreferrer\">document<\/a>, but this is representing one-to-one cluster and service. For multiple workspaces refer <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.webservice.aksendpoint?view=azure-ml-py#azureml-core-webservice-aksendpoint-create-version\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>\n<p>Regarding <strong><code>azureml-fe<\/code><\/strong>. There will be one <strong>azureml-fe<\/strong> for one cluster. That means, when we are using different workspaces for deployment into one AKS, then only <strong>one<\/strong> <strong>azureml-fe<\/strong> and can be considered to take <strong>one certificate.<\/strong><\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/v1\/how-to-deploy-azure-kubernetes-service?tabs=python#azure-ml-router\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/v1\/how-to-deploy-azure-kubernetes-service?tabs=python#azure-ml-router<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_readability":19.4,
        "Solution_reading_time":16.89,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10,
        "Solution_word_count":81,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I trained a model with Designer, created a real-time inference pipeline which was succesfully submitted. When deploying to either ACI or AKS it fails and I get the error &quot;ModuleNotFoundError: No module named 'azureml.api'&quot;. I've had no problems deploying this model many times in the past and haven't changed anything. Even if I use one of the sample pipelines (automobiles basic), I get the same error when deploying to real-time. <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1632862175993,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/569925\/deployment-from-designer-fails-in-every-possible-w",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":9.4,
        "Challenge_reading_time":6.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Deployment from Designer fails in every possible way",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":78,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>It's an known issue caused by unexpected module version upgrade. It's been resolved by applying hotfix to all regions. For users, please rerun training pipeline by check on &quot;Regenerate Output&quot;, and run corresponding inference pipeline and try deployment again.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":9.5,
        "Solution_reading_time":3.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3,
        "Solution_word_count":39,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1225669466307,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cambridge, United Kingdom",
        "Answerer_reputation_count":236107.0,
        "Answerer_view_count":18730.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working with Azure ML and I have the code sample to invoke my web  service (alas it is only in C#).  Can someone help me translate this to F#?  I have everything but the async and await done.<\/p>\n\n<pre><code> static async Task InvokeRequestResponseService()\n        {\n            using (var client = new HttpClient())\n            {\n                ScoreData scoreData = new ScoreData()\n                {\n                    FeatureVector = new Dictionary&lt;string, string&gt;() \n                    {\n                        { \"Zip Code\", \"0\" },\n                        { \"Race\", \"0\" },\n                        { \"Party\", \"0\" },\n                        { \"Gender\", \"0\" },\n                        { \"Age\", \"0\" },\n                        { \"Voted Ind\", \"0\" },\n                    },\n                    GlobalParameters = new Dictionary&lt;string, string&gt;() \n                    {\n                    }\n                };\n\n                ScoreRequest scoreRequest = new ScoreRequest()\n                {\n                    Id = \"score00001\",\n                    Instance = scoreData\n                };\n\n                const string apiKey = \"abc123\"; \/\/ Replace this with the API key for the web service\n                client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue( \"Bearer\", apiKey);\n\n                client.BaseAddress = new Uri(\"https:\/\/ussouthcentral.services.azureml.net\/workspaces\/19a2e623b6a944a3a7f07c74b31c3b6d\/services\/f51945a42efa42a49f563a59561f5014\/score\");\n                HttpResponseMessage response = await client.PostAsJsonAsync(\"\", scoreRequest);\n                if (response.IsSuccessStatusCode)\n                {\n                    string result = await response.Content.ReadAsStringAsync();\n                    Console.WriteLine(\"Result: {0}\", result);\n                }\n                else\n                {\n                    Console.WriteLine(\"Failed with status code: {0}\", response.StatusCode);\n                }\n            }\n<\/code><\/pre>\n\n<p>Thanks<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":3,
        "Challenge_created_time":1410732744493,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1446025307110,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/25838512",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":10.9,
        "Challenge_reading_time":17.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"C# async\/await to F# using Azure ML example",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":607.0,
        "Challenge_word_count":136,
        "Platform":"Stack Overflow",
        "Poster_created_time":1349689794400,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Denver, CO, USA",
        "Poster_reputation_count":4174.0,
        "Poster_view_count":396.0,
        "Solution_body":"<p>I was not able to compile and run the code, but you probably need something like this:<\/p>\n\n<pre><code>let invokeRequestResponseService() = async {\n    use client = new HttpClient()\n    let scoreData = (...)\n    let apiKey = \"abc123\"\n    client.DefaultRequestHeaders.Authorization &lt;- \n        new AuthenticationHeaderValue(\"Bearer\", apiKey)\n    client.BaseAddress &lt;- Uri(\"https:\/\/ussouthcentral....\/score\");\n    let! response = client.PostAsJsonAsync(\"\", scoreRequest) |&gt; Async.AwaitTask\n    if response.IsSuccessStatusCode then\n        let! result = response.Content.ReadAsStringAsync() |&gt; Async.AwaitTask\n        Console.WriteLine(\"Result: {0}\", result);\n    else\n        Console.WriteLine(\"Failed with status code: {0}\", response.StatusCode) }\n<\/code><\/pre>\n\n<ul>\n<li><p>Wrapping the code in the <code>async { .. }<\/code> block makes it asynchronous and lets you use <code>let!<\/code> inside the block to perform asynchronous waiting (i.e. in places where you'd use <code>await<\/code> in C#)<\/p><\/li>\n<li><p>F# uses type <code>Async&lt;T&gt;<\/code> instead of .NET Task, so when you're awaiting a task, you need to insert <code>Async.AwaitTask<\/code> (or you can write wrappers for the most frequently used operations)<\/p><\/li>\n<li><p>The <code>invokeRequestResponseService()<\/code> function returns F# async, so if you need to pass it to some other library function (or if it needs to return a task), you can use <code>Async.StartAsTask<\/code><\/p><\/li>\n<\/ul>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":8.3,
        "Solution_reading_time":18.27,
        "Solution_score_count":4.0,
        "Solution_sentence_count":16,
        "Solution_word_count":156,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I followed Microsoft's tutorial on the German credit card risk model, step by step and without mistakes. The algorithm runs, it is deployed successfully, etc.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/tuyDt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tuyDt.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I am using the <code>Select Columns in Dataset<\/code> to select the columns to input, and I do the same to select the output columns. <\/p>\n\n<p>I noticed that when I look at the <code>Request\/Response<\/code> tab of the deployed model, the Sample Request includes <em>all<\/em> columns, ignoring the selection I provided. This includes the field to be predicted, which is column 21:<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"Col1\",\n        \"Col2\",\n        \"Col3\",\n        \"Col4\",\n        \"Col5\",\n        \"Col6\",\n        \"Col7\",\n        \"Col8\",\n        \"Col9\",\n        \"Col10\",\n        \"Col11\",\n        \"Col12\",\n        \"Col13\",\n        \"Col14\",\n        \"Col15\",\n        \"Col16\",\n        \"Col17\",\n        \"Col18\",\n        \"Col19\",\n        \"Col20\",\n        \"Col21\"\n<\/code><\/pre>\n\n<p><strong>The problem<\/strong>: column 21 is the credit risk itself, so the API is expecting to receive that value. Instead, <strong>that is the value that should be predicted!<\/strong><\/p>\n\n<p>There clearly is a problem with the input schema, but how can I change that? How can I make sure that field is not requested by the API?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569943900453,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58188104",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":17.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning REST API: why is the prediction included in the Sample Request?",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":119.0,
        "Challenge_word_count":189,
        "Platform":"Stack Overflow",
        "Poster_created_time":1445878520940,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Humboldt, Saskatchewan, Canada",
        "Poster_reputation_count":1271.0,
        "Poster_view_count":171.0,
        "Solution_body":"<p>Don't worry about the input schema for the <code>Col21<\/code> field. The <code>Col21<\/code> field in the input data just adapt for the <code>Edit Metadata<\/code> module which requires the <code>Col21<\/code> data in the training stage.<\/p>\n\n<p>You just fill an invalid value like <code>0<\/code> (<code>0<\/code> is an invalid classified value for risk) into <code>Col21<\/code> field, and then the web service will return a prediction classified value to replace the <code>Col21<\/code> value of your input data.<\/p>\n\n<p>At here, I use the first data record of the sample data with the <code>Col21<\/code> value <code>0<\/code> for testing via the link of <code>Test<\/code> feature on portal, it works fine and return <code>1<\/code> for <code>Credit risk<\/code><\/p>\n\n<p>Fig 1. To click <code>Test<\/code> link to test for <code>Col21<\/code> with <code>0<\/code><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9jqyo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9jqyo.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 2. Use the first record of sample to test<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/lIUiP.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/lIUiP.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 3. The <code>Col21<\/code> value of <code>input1<\/code> is <code>0<\/code>, and the <code>Credit risk<\/code> value of <code>output1<\/code> is <code>1<\/code><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/h5OEH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/h5OEH.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6,
        "Solution_readability":9.7,
        "Solution_reading_time":20.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13,
        "Solution_word_count":175,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi All    <\/p>\n<p>I have been working with Azure Machine Learning Studio (Classic) and have always found its integration with Excel super mega useful.    <\/p>\n<p>All I had to do was to get the URI and the API_Key of my web service and paste them on the Azure Machine Learning Add-In, that I had downloaded. Easy and useful.    <\/p>\n<p>However, with the new Azure Machine Learning studio that does not seem possible any more.     <\/p>\n<p>Under the new Azure Machine Learning studio when I deploy a model I get a REST endpoint and that's it? !? I cannot find anywhere the API_key for my web service. I cannot even find a web service section  as such.     <\/p>\n<ol>\n<li> How do I get the API_Key for the web service I need?    <\/li>\n<li> If I get the API_Key could I use it on the Excel Azure Machine Learning add-in. It looks as if this is no longer an option and we need to start using Power BI instead.    <\/li>\n<li>  I have read this interesting post where someone mentions a work around that consist of creating an Excel macro. Is this the best option? <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/236781\/consume-scoreing-api-in-excel.html\">https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/236781\/consume-scoreing-api-in-excel.html<\/a>     <\/li>\n<\/ol>\n<p>Thank you    <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651222482640,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/831512\/new-azure-machine-learning-excel",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":16.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"New Azure Machine Learning & Excel",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":204,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. The new AzureML integration with Excel isn't supported at this time. More details are provided on this <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/778717\/replacement-for-azure-ml-classic-excel-add-in.html\">thread<\/a>. The alternative approach would be to use a Client or PowerBI to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-consume-web-service?tabs=python\">consume<\/a> the model. For future reference, you can find your webservice endpoint and keys under Studio &gt; Endpoints &gt; Endpoint &gt; Consume.    <\/p>\n<p>--please don't forget to <code>Accept Answer<\/code> if the reply is helpful. Thanks.--<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":10.8,
        "Solution_reading_time":8.99,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7,
        "Solution_word_count":72,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1254957460063,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"North Carolina, USA",
        "Answerer_reputation_count":2484.0,
        "Answerer_view_count":362.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Suppose I have a trained model in <a href=\"https:\/\/studio.azureml.net\" rel=\"nofollow noreferrer\">Azure ML<\/a> and I deployed it as a Web Service. Is it possible to export the model, embed it in an Android app and use it <em>locally<\/em>, without making any requests to Azure Web service?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1527009105097,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50473170",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":7.9,
        "Challenge_reading_time":3.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Embedded Azure MLmodel",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":85.0,
        "Challenge_word_count":47,
        "Platform":"Stack Overflow",
        "Poster_created_time":1485085240960,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Ivanovo, Ivanovo Oblast, Russia",
        "Poster_reputation_count":490.0,
        "Poster_view_count":196.0,
        "Solution_body":"<p>From <a href=\"https:\/\/stackoverflow.com\/questions\/41236871\/how-to-download-the-trained-models-from-azure-machine-studio\">this answer<\/a> you won't be able to save the model locally if you do everything within Azure ML Studio.<\/p>\n\n<p>If you create the model using Python or R and execute it within Azure ML Studio, then you can save it from the library that you use.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":10.7,
        "Solution_reading_time":4.77,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3,
        "Solution_word_count":48,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,  <\/p>\n<p>I have made deployment of the model from the AutoML experiment, due to the issue in the resources associated. Deployment has failed.  <\/p>\n<p>But the real-time endpoint has been in the transition state for few hours,  I can't delete it and the model registered along with it due to this. How can I force delete in this case. Please provide a solution.   <\/p>\n<p>Thanks  <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1628858328673,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/513012\/how-to-delete-azure-ml-real-time-endpoints-which-i",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.3,
        "Challenge_reading_time":5.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"how to delete Azure ML real-time endpoints which is in transition state",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":78,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Thank you for the response <a href=\"\/users\/na\/?userid=bc467a93-95da-4dea-bc82-06951da4cfad\">@romungi-MSFT  <\/a>. I have left the feedback to the team.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":8.8,
        "Solution_reading_time":2.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2,
        "Solution_word_count":16,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I developed a machine learning model using Azure ML's clustering. Few of the requests made from the cluster are triggering 404 HTTP error. I followed the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script\" rel=\"nofollow noreferrer\">document<\/a> to do modifications in my swagger.json file. Finally ended up with &quot;list index out of range&quot; error. It seems to be having issue with the global parameter but I am no sure about it. I am using the API from postman with some default headers like mentioned in the body below<\/p>\n<pre><code>{\n    &quot;Inputs&quot;: {\n         &quot;input_1&quot; : &quot;content&quot;\n         &quot;input_2: : &quot;content&quot;\n         ......\n    },\n    &quot;GlobalParameters&quot;: 0\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651094225790,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1651098753480,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72035391",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":10.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"AzureML schema \"list index out of range\" error",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":78.0,
        "Challenge_word_count":97,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651093614703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Netherland",
        "Poster_reputation_count":19.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>Change the &quot;GlobalParameter&quot; value to any floating number other than 1.0 or even you can remove it and execute. Sometimes, Global parameter will cause the issue. Check the below documentation.<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/746784\/azure-ml-studio-error-while-testing-real-time-endp.html\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/746784\/azure-ml-studio-error-while-testing-real-time-endp.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":22.0,
        "Solution_reading_time":6.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6,
        "Solution_word_count":34,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":11,
        "Challenge_body":"<p>I can't use ml real-time inference endpoint becouse it's stuck on transitioning status (more than 20 hours). Could you help me with that?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600239147820,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/96645\/azure-ml-real-time-inference-endpoint-deloyment-st",
        "Challenge_link_count":0,
        "Challenge_participation_count":11,
        "Challenge_readability":8.6,
        "Challenge_reading_time":2.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML real-time inference endpoint deloyment stuck on transitioning status",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":32,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I've chacked in on some different algorithms and the issue appears when i'm using n-grams block for getting features. When i'm using feature hashing for example it looks like working well.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":8.2,
        "Solution_reading_time":2.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2,
        "Solution_word_count":31,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1505166133223,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":146.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am new to Sagemaker. I have deployed my well trained model in tensorflow  by using Json and Weight file. But it is strange that in my note book, I didn't see it says \"Endpoint successfully built\". Only the below is shown:<\/p>\n\n<pre><code>--------------------------------------------------------------------------------!\n<\/code><\/pre>\n\n<p>Instead, I found the endpoint number from my console. <\/p>\n\n<pre><code>import sagemaker\nfrom sagemaker.tensorflow.model import TensorFlowModel\n        predictor=sagemaker.tensorflow.model.TensorFlowPredictor(endpoint_name, sagemaker_session)\ndata= test_out2\npredictor.predict(data)\n<\/code><\/pre>\n\n<p>Then I try to invoke the endpoint by using 2D array:\n(1) If my 2D array is in size of (5000, 170), I am getting the error:<\/p>\n\n<pre><code>ConnectionResetError: [Errno 104] Connection reset by peer\n<\/code><\/pre>\n\n<p>(2) If reducing the array to size of (10,170), error is :<\/p>\n\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"\". See https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/sagemaker-tensorflow-2019-04-28-XXXXXXXXX in account 15XXXXXXXX for more information.\n<\/code><\/pre>\n\n<p>Any suggestion please? Found similar case in github, <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/issues\/589\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/issues\/589<\/a>.<\/p>\n\n<p>Is it the similar case please?<\/p>\n\n<p>Thank you very much in advance!<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1556470456677,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55892554",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":13.4,
        "Challenge_reading_time":21.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"Invoke endpoint after model deployment : [Err 104] Connection reset by peer",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":465.0,
        "Challenge_word_count":174,
        "Platform":"Stack Overflow",
        "Poster_created_time":1401287882528,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":61.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>The first error with data size (5000, 170) might be due to a capacity issue. SageMaker endpoint prediction has a size limit of 5mb. So if your data is larger than 5mb, you need to chop it into pieces and call predict multiple times. <\/p>\n\n<p>For the second error with data size (10, 170), the error message asks you to look into logs. Did you find anything interesting in the cloudwatch log? Anything can be shared in this question?<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":6.1,
        "Solution_reading_time":5.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6,
        "Solution_word_count":79,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1342685175156,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":12103.0,
        "Answerer_view_count":1451.0,
        "Challenge_adjusted_solved_time":257.7152338889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm completely new to Azure ML, but I wanted to try out their automated ML UX. So I've followed the instructions to finally deploy my app (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-portal-experiments#deploy-your-model\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-portal-experiments#deploy-your-model<\/a>). Now I've got my \"Scoring URI\", but I don't know how to use it? <strong>How can I test an input and get an output - can I do it with Postman?<\/strong><\/p>\n\n<ul>\n<li>the tutorial doesn't tell me what to do with this \"Scoring URI\", and so I am stuck<\/li>\n<\/ul>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565145529467,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1565163086470,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57386269",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":9.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How to use Azure ML Scoring URI?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":826.0,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Poster_created_time":1526863814910,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":45.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>On the bottom of the page that you have linked above, there is a link:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-consume-web-service\" rel=\"nofollow noreferrer\">Learn how to consume a web service.<\/a><\/p>\n\n<p>This is exactly on that topic on how to use the deployed web service for scoring (sending an input and getting an output).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1566090861312,
        "Solution_link_count":1,
        "Solution_readability":10.7,
        "Solution_reading_time":4.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3,
        "Solution_word_count":48,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1488536112480,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pune, Maharashtra, India",
        "Answerer_reputation_count":349.0,
        "Answerer_view_count":26.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy the existing breast cancer prediction model on Amazon Sagemanker using AWS Lambda and API gateway. I have followed the official documentation from the below url.<\/p>\n\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/<\/a><\/p>\n\n<p>I am getting a type error at \"predicted_label\".<\/p>\n\n<pre><code> result = json.loads(response['Body'].read().decode())\n print(result)\n pred = int(result['predictions'][0]['predicted_label'])\n predicted_label = 'M' if pred == 1 else 'B'\n\n return predicted_label\n<\/code><\/pre>\n\n<p>please let me know if someone could resolve this issue. Thank you. <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1538386997197,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1562245989407,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52588354",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":12.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"How to deploy breast cancer prediction endpoint created by AWS Sagemaker using Lambda and API gateway?",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":502.0,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1456483381212,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":999.0,
        "Poster_view_count":92.0,
        "Solution_body":"<p>By printing the result type by <code>print(type(result))<\/code> you can see its a dictionary. now you can see the key name is \"score\" instead of \"predicted_label\" that you are giving to pred. Hence replace it with<\/p>\n\n<pre><code>pred = int(result['predictions'][0]['score'])\n<\/code><\/pre>\n\n<p>I think this solves your problem.<\/p>\n\n<p>here is my lambda function:<\/p>\n\n<pre><code>import os\nimport io\nimport boto3\nimport json\nimport csv\n\n# grab environment variables\nENDPOINT_NAME = os.environ['ENDPOINT_NAME']\nruntime= boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n   print(\"Received event: \" + json.dumps(event, indent=2))\n\n   data = json.loads(json.dumps(event))\n   payload = data['data']\n   print(payload)\n\n   response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                      ContentType='text\/csv',\n                                      Body=payload)\n   #print(response)\n   print(type(response))\n   for key,value in response.items():\n       print(key,value)\n   result = json.loads(response['Body'].read().decode())\n   print(type(result))\n   print(result['predictions'])\n   pred = int(result['predictions'][0]['score'])\n   print(pred)\n   predicted_label = 'M' if pred == 1 else 'B'\n\n   return predicted_label\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":15.3,
        "Solution_reading_time":15.19,
        "Solution_score_count":4.0,
        "Solution_sentence_count":11,
        "Solution_word_count":106,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1452285224528,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA",
        "Answerer_reputation_count":870.0,
        "Answerer_view_count":21.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a custom container vertex endpoint that is passed a url as input so that the job can call it to get a particular frame of data needed for the job. (gcs:\/\/ buckets do work) but I want to specifically use an http request to a server in the same gcp project.<\/p>\n<p>I have tried setting the endpoint up as private using the --networks param on the endpoint but then get the message:<\/p>\n<pre><code>{\n  &quot;error&quot;: {\n    &quot;code&quot;: 400,\n    &quot;message&quot;: &quot;Making request from public OnePlatform API is not allowed on a private Endpoint peered with network (projects\/11111111111\/global\/networks\/some-dev-project-vpc).&quot;,\n    &quot;status&quot;: &quot;FAILED_PRECONDITION&quot;\n  }\n}\n<\/code><\/pre>\n<p>when I try to hit that private vertex endpoint.  I've tried curling it from within a running pod in the same project, but that didn't work either.<\/p>\n<p>Is there a way to do this?\nThanks<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1643145096180,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1643175262992,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70855730",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.7,
        "Challenge_reading_time":12.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Is it possible from a gcp vertex job to hit an http endpoint in another gcp pod in the same project?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":274.0,
        "Challenge_word_count":154,
        "Platform":"Stack Overflow",
        "Poster_created_time":1462581330168,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berkeley, CA, United States",
        "Poster_reputation_count":329.0,
        "Poster_view_count":65.0,
        "Solution_body":"<p>The error states that your request is to a public API, which may because you are using the public url schema to make your prediction. The structure of vertex endpoints differ between private and public, so double check that you are using the private endpoint url for your requests.<\/p>\n<p><strong>Public<\/strong><\/p>\n<pre><code>https:\/\/{REGION}-aiplatform.googleapis.com\/v1\/projects\/{PROJECT}\/locations\/{REGION}\/endpoints\/{ENDPOINT_ID}:predict\n<\/code><\/pre>\n<p><strong>Private<\/strong><\/p>\n<pre><code>http:\/\/{ENDPOINT_ID}.aiplatform.googleapis.com\/v1\/models\/{DEPLOYED_MODEL_ID}:predict\n<\/code><\/pre>\n<p>You can generate a private endpoint url using the following gcloud command:<\/p>\n<pre><code>gcloud beta ai endpoints describe {ENDPOINT_ID} \\\n  --region=us-central1 \\\n  --format=&quot;value(deployedModels.privateEndpoints.predictHttpUri)&quot;\n<\/code><\/pre>\n<p>More documentation on private endpoints can be found <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/using-private-endpoints#private-predict-uri-format\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3,
        "Solution_readability":18.8,
        "Solution_reading_time":14.37,
        "Solution_score_count":3.0,
        "Solution_sentence_count":7,
        "Solution_word_count":88,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"based on the sample code provided here , https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config\n\nI created a model via lambda, now when i try to create a serverless endpoint config (sample code below) , i keep getting -> parameter validation failed \nunknown parameter in` ProductVariants [ 0 ]:` \"ServerlessConfig\", must be one of : VairantName, ModelName, InitialInstanceCount , Instancetype...\n\n```\nresponse = client.create_endpoint_config(\n   EndpointConfigName=\"endpoint-new\",\n   ProductionVariants=[\n        {\n            \"ModelName\": \"MyModel\",\n            \"VariantName\": \"AllTraffic\",\n            \"ServerlessConfig\": {\n                \"MemorySizeInMB\": 2048,\n                \"MaxConcurrency\": 10\n            }\n        } \n    ]\n)\n```",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645067206226,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668438755896,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUfmAxh_aDQiS2nk0gbDicsg\/how-to-create-a-serverless-endpoint-configuration",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":20.3,
        "Challenge_reading_time":9.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"How to create a serverless endpoint configuration?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":260.0,
        "Challenge_word_count":66,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The cause might be that your SageMaker Python SDK is not updated to the latest version. Please make sure you update it to the latest version as well as the AWS SDK for Python (boto3). You can use pip:\n\n```\npip install --upgrade boto3\npip install --upgrade sagemaker\n```\n\n\nFor a sample notebook you can have a look [here](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/serverless-inference\/Serverless-Inference-Walkthrough.ipynb). More information on the documentation [page](https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#sagemaker-serverless-inference).",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1645171546007,
        "Solution_link_count":2,
        "Solution_readability":14.2,
        "Solution_reading_time":7.48,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4,
        "Solution_word_count":63,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1515120748952,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":76.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to invoke the iris endpointfrom the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators\/tensorflow_iris_dnn_classifier_using_estimators.ipynb\" rel=\"nofollow noreferrer\">SageMaker example notebooks<\/a> using the aws cli. I've tried using the following command:<\/p>\n\n<pre><code>!aws sagemaker-runtime invoke-endpoint \\\n--endpoint-name sagemaker-tensorflow-py2-cpu-2018-03-19-21-27-52-956 \\\n--body \"[6.4, 3.2, 4.5, 1.5]\" \\\n--content-type \"application\/json\" \\\noutput.json\n<\/code><\/pre>\n\n<p>I get the following response:<\/p>\n\n<pre><code>{\n    \"InvokedProductionVariant\": \"AllTraffic\", \n    \"ContentType\": \"*\/*\"\n}\n<\/code><\/pre>\n\n<p>What am I doing wrong?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1521505092823,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49374476",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":19.8,
        "Challenge_reading_time":10.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How do I call a SageMaker Endpoint using the AWS CLI (",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":4027.0,
        "Challenge_word_count":64,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443201378360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":749.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>If you've gotten that response, your request is successful. The output should be in the output file you specified - output.json :)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":7.4,
        "Solution_reading_time":1.7,
        "Solution_score_count":5.0,
        "Solution_sentence_count":2,
        "Solution_word_count":21,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1250158552416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying out <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning-service\/\" rel=\"nofollow noreferrer\">Azure Machine Learning Service<\/a> to deploy a ML model as web service.<\/p>\n\n<p>I have already <a href=\"https:\/\/stackoverflow.com\/a\/55281703\/4240413\">registered a model<\/a> and now would like to deploy it as web service following the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml#deploy-in-container-instances\" rel=\"nofollow noreferrer\">guide<\/a> using Azure (Python) Notebooks.<\/p>\n\n<p>The step<\/p>\n\n<pre><code> service = Webservice.deploy_from_model(my-model-svc',\n                                   deployment_config=aciconfig,\n                                   models=[model],\n                                   image_config=image_config)\n<\/code><\/pre>\n\n<p>fails for me with<\/p>\n\n<blockquote>\n  <p>Creating image<br>\n  Image creation operation finished for image my-model-svc:5, operation \"Succeeded\" Creating service<br>\n  Running.<br>\n  FailedACI service creation operation finished, operation<br>\n  \"Failed\" Service creation polling reached terminal state, current\n  service state: Transitioning Service creation polling reached terminal\n  state, unexpected response received.<\/p>\n<\/blockquote>\n\n<p>Not sure about what could be the root cause, as (AFAIK) I have no ways to access logs of the deployment in Azure portal.<\/p>\n\n<p>Can someone shed some light on this?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1553185573690,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1559204803203,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55285043",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":14.1,
        "Challenge_reading_time":18.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I troubleshoot my Azure ML service deployment?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1236.0,
        "Challenge_word_count":140,
        "Platform":"Stack Overflow",
        "Poster_created_time":1415722650716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Verona, VR, Italy",
        "Poster_reputation_count":4811.0,
        "Poster_view_count":713.0,
        "Solution_body":"<p>I think that your <code>init<\/code> function is failing. I would first try to isolate the image creation from the image deployment, and just test the image first:<\/p>\n\n<ul>\n<li>Create the image first, it's very much ok if do it through the interface<\/li>\n<li>Pull the image locally with Docker (for this you'll need <a href=\"https:\/\/www.docker.com\" rel=\"nofollow noreferrer\">Docker<\/a> and the <a href=\"https:\/\/docs.microsoft.com\/en-us\/cli\/azure\/install-azure-cli?view=azure-cli-latest\" rel=\"nofollow noreferrer\">Azure CLI<\/a> installed):<\/li>\n<\/ul>\n\n<pre class=\"lang-python prettyprint-override\"><code>az acr login -n &lt;container-registry&gt;\ndocker run -p 8000:5001  &lt;container-registry&gt;.azurecr.io\/&lt;image-name&gt;:&lt;image-version&gt;\n# basically, the entire image location, see pic below\n<\/code><\/pre>\n\n<ul>\n<li>test the image locally, it listens on the 8000 port:<\/li>\n<\/ul>\n\n<pre><code>POST http:\/\/localhost:8000\/score\nContent-Type: application\/json\n<\/code><\/pre>\n\n<ul>\n<li>if this works deploy it on ACI <\/li>\n<\/ul>\n\n<p><code>&lt;container-registry&gt;<\/code> is the name of the <code>Container Registry<\/code> associated with the ML Workspace, you can also extract it from the image location, taking care to remove everything after the first dot:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/W6YYj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/W6YYj.png\" alt=\"image location\"><\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1554828275376,
        "Solution_link_count":5,
        "Solution_readability":15.3,
        "Solution_reading_time":18.61,
        "Solution_score_count":3.0,
        "Solution_sentence_count":7,
        "Solution_word_count":151,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1251699930780,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Santa Clara, CA, United States",
        "Answerer_reputation_count":1538.0,
        "Answerer_view_count":198.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been exploring using Vertex AI for my machine learning workflows. Because deploying different models to the same endpoint utilizing only one node is not possible in Vertex AI, I am considering a <a href=\"https:\/\/stackoverflow.com\/questions\/69878915\/deploying-multiple-models-to-same-endpoint-in-vertex-ai\">workaround<\/a>. With this workaround, I will be unable to use many Vertex AI features, like model monitoring, feature attribution etc., and it simply becomes, I think, a managed alternative to running the prediction application on, say, a GKE cluster. So, besides the cost difference, I am exploring if running the custom prediction container on Vertex AI vs. GKE will involve any limitations, for example, only <strong>N1<\/strong> machine types are available for prediction in Vertex AI<\/p>\n<p>There is a similar <a href=\"https:\/\/stackoverflow.com\/questions\/67930882\/google-kubernetes-engine-vs-vertex-ai-ai-platform-unified-for-serving-model-pr\">question<\/a>, but I it does not raise the specific questions I hope to have answered.<\/p>\n<ul>\n<li>I am not sure of the available disk space. In Vertex AI, one can specify the machine type, such as n1-standard-2 etc., but I am not sure what disk space will be available and if\/how one can specify it? In the custom container code, I may copy multiple model artifacts, or data from outside sources to the local directory before processing them so understanding any disk space limitations is important.<\/li>\n<li>For custom training in Vertex AI, one can use an interactive shell to inspect the container where the training code is running, as described <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/monitor-debug-interactive-shell\" rel=\"nofollow noreferrer\">here<\/a>. Is something like this possible for a custom prediction container? I have not found anything in the docs.<\/li>\n<li>For custom training, one can use a private IP for custom training as described <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-private-ip\" rel=\"nofollow noreferrer\">here<\/a>. Again, I have not found anything similar for custom prediction in the docs, is it possible?<\/li>\n<\/ul>\n<p>If you know of any other possible limitations, please post.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636999914017,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69978953",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":29.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":null,
        "Challenge_title":"Vertex AI custom prediction vs Google Kubernetes Engine",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":304.0,
        "Challenge_word_count":299,
        "Platform":"Stack Overflow",
        "Poster_created_time":1471292986790,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":700.0,
        "Poster_view_count":90.0,
        "Solution_body":"<ol>\n<li>we don't specify a disk size, so default to 100GB<\/li>\n<li>I'm not aware of this right now. But if it's a custom container, you could just run it locally or on GKE for debugging purpose.<\/li>\n<li>are you looking for this? <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/using-private-endpoints\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/using-private-endpoints<\/a><\/li>\n<\/ol>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":12.5,
        "Solution_reading_time":5.7,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4,
        "Solution_word_count":46,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1296602388823,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA",
        "Answerer_reputation_count":1244.0,
        "Answerer_view_count":368.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a list of ML experiments which I have created in Azure Machine Learning Studio.  I have deployed them as web services (the new version, not classic).  <\/p>\n\n<p>How can I go into Azure Machine Learning Web Services, click on a web service (which was deployed from an experiment), then navigate back to the experiment \/ predictive model which feeds it?<\/p>\n\n<p>The only link I can find between the two is by updating the web service from the predictive experiment, which then confirms what the web service is. I can see that the \"ExperimentId\" is a GUID in the URL when in the experiment and the web service, so hopefully this is possible.<\/p>\n\n<p>My reasoning is that relying on matching naming conventions, etc., to select the appropriate model to update is subject to human error.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1489415750577,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42766263",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":10.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"How to Find Azure ML Experiment based on Deployed Web Service",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":224.0,
        "Challenge_word_count":145,
        "Platform":"Stack Overflow",
        "Poster_created_time":1377703476328,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bristol, United Kingdom",
        "Poster_reputation_count":592.0,
        "Poster_view_count":43.0,
        "Solution_body":"<p>The <em>new<\/em> web service does not store any information about the experiment or workspace that was deployed (not all <em>new<\/em> web services are deployed from an experiment).<\/p>\n\n<p>Here are the options available to track the relationship between the experiment and a <em>new<\/em> web service.<\/p>\n\n<h2>last deployment<\/h2>\n\n<p>However, the experiment keeps track of the <strong>last<\/strong> <em>new<\/em> web service that was deployed from the experiment. each deployment to a <em>new<\/em> web service overwrites this value.<\/p>\n\n<p>The value is stored in the experiment graph. One way to get the graph is to use the powershell module <a href=\"http:\/\/aka.ms\/amlps\" rel=\"nofollow noreferrer\">amlps<\/a><\/p>\n\n<p><code>Export-AmlExperimentGraph -ExperimentId &lt;Experiment Id&gt; -OutputFile e.json<\/code><\/p>\n\n<p><strong>e.json<\/strong><\/p>\n\n<pre><code>{\n\"ExperimentId\":\"&lt;Experiment Id&gt;\",\n\/\/ . . .\n\"WebService\":{\n\/\/ . . .\n\"ArmWebServiceId\":\"&lt;Arm Id&gt;\"\n},\n\/\/ . . . \n}\n<\/code><\/pre>\n\n<h2>azure resource tags<\/h2>\n\n<p>The tags feature for Azure resources is supported by the <em>new<\/em> web services. Setting a <code>tag<\/code> on the web service programmatically, with powershell or via the azure portal UX can be used to store a reference to the experiment on the <em>new<\/em> web service.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":11.8,
        "Solution_reading_time":16.63,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10,
        "Solution_word_count":158,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using one of the images listed here <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md<\/a>, to create an sagemaker endpoint, but I keep getting &quot;failed reason: Image size 15136109518 is greater that suppported size 1073741824&quot; .<\/p>\n<p>is there a way to find out the size of images provided <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md<\/a> or any aws managed images?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644892148237,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71120471",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":18.6,
        "Challenge_reading_time":9.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How to determine size of images available in aws?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":153.0,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1590797441983,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":525.0,
        "Poster_view_count":98.0,
        "Solution_body":"<p>I suspect you are trying to deploy a serverless endpoint provisioned with 1GB of memory. As discussed <a href=\"https:\/\/repost.aws\/questions\/QU35dVp2D9SKKUnnVYGw9Z7A\/how-to-check-determine-image-container-size-for-aws-managed-images\" rel=\"nofollow noreferrer\">here<\/a> &quot;You can increase the memory size of your endpoint with the MemorySizeInMB parameter, more info in this documentation: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config%22\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config&quot;<\/a><\/p>\n<p>In order to view the uncompressed size of an image you can use the following example command:<\/p>\n<pre><code>$ docker pull 763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-training:1.15.2-cpu-py27-ubuntu18.04\n\n$ docker inspect -f &quot;{{ .Size }}&quot; 763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-training:1.15.2-cpu-py27-ubuntu18.04\n<\/code><\/pre>\n<p>Kindly also note that you will need to provision enough memory to accommodate your model as well. Please see this <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints.html#serverless-endpoints-how-it-works-memory\" rel=\"nofollow noreferrer\">link<\/a> for more information.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_readability":20.1,
        "Solution_reading_time":18.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10,
        "Solution_word_count":99,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405317204728,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":69.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":196.8378408333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have an endpoint running a trained SageMaker model on AWS, which expects the data on a specific format.<\/p>\n<p>Initially, the data has been processed on the client side of the application, it means, the <code>API Gateway<\/code> (which receives the POST API calls on AWS) used to receive pre-processed data, but now there's a change, the <code>API Gateway<\/code> will receive <strong>raw data<\/strong> from the client, and the job of pre-processing this data before sending to our SageMaker model is up to our workflow.<\/p>\n<p>What is the best way to create a pre-processing job on this workflow, without needing to re-train the model? My pre-process is just a bunch of dataframe transformations, no standardization or calculation with the training set required (it would not need to save any model file).<\/p>\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1602162886373,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64263330",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.6,
        "Challenge_reading_time":10.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Data Preprocessing on AWS SageMaker",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":450.0,
        "Challenge_word_count":134,
        "Platform":"Stack Overflow",
        "Poster_created_time":1405317204728,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":69.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>After some research, this is the solution I've followed:<\/p>\n<ul>\n<li>First I have created a <code>SKLearn<\/code> sagemaker model to do all the preprocess setup (I've built a Scikit-Learn custom class to handle all the preprocess steps, following this <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb\" rel=\"nofollow noreferrer\">AWS code<\/a>)<\/li>\n<li>Trained this preprocess model on my training data. My model, in specific, didn't need to be trained (it does not have any standardization or anything that would need to store training data parameters), but sagemaker requires the model to be trained.<\/li>\n<li>Loaded the trained legacy model that we had using the <code>Model<\/code> parameter.<\/li>\n<li>Created a <code>PipelineModel<\/code> with the preprocessing model and legacy model in cascade:<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>pipeline_model = PipelineModel(name=model_name,\n                               role=role,\n                               models=[\n                                    preprocess_model,\n                                    trained_model\n                               ])\n<\/code><\/pre>\n<ul>\n<li>Create a new endpoint, calling the <code>PipelineModel<\/code> and then changed the <code>Lambda<\/code> function to call this new endpoint. With this I could send the <strong>raw data<\/strong> directly for the same <code>API Gateway<\/code> and it would call only <strong>one<\/strong> endpoint, without needing to pay two endpoints 24\/7 to perform the entire process.<\/li>\n<\/ul>\n<p>I've found this to be a good and &quot;<em>economic<\/em>&quot; way to perform the preprocess outside the trained model, without having to do hard processing jobs on a <code>Lambda<\/code> function.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1602871502600,
        "Solution_link_count":1,
        "Solution_readability":18.0,
        "Solution_reading_time":22.23,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7,
        "Solution_word_count":200,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1221667848150,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA",
        "Answerer_reputation_count":849.0,
        "Answerer_view_count":142.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>AWS pricing page describes how much it costs per hour to run AWS Sagemaker for online realtime inference.\n<a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/sagemaker\/pricing\/<\/a><\/p>\n<p>But AWS usually also charges for API requests.\nDo they charge extra per every API inference request to the Sagemaker model?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604509294833,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64684503",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":5.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Does AWS Sagemaker charges you per API request?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":187.0,
        "Challenge_word_count":50,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501710710163,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, \u0412\u0435\u043b\u0438\u043a\u043e\u0431\u0440\u0438\u0442\u0430\u043d\u0438\u044f",
        "Poster_reputation_count":404.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>I am on the AWS SageMaker team.  For &quot;Real-Time Inference&quot; you are only charged for:<\/p>\n<ol>\n<li>usage of the instance types you choose (instance hours)<\/li>\n<li>storage attached to those instance (GB storage hours)<\/li>\n<li>data in and out of your Endpoint (Bytes in\/out)<\/li>\n<\/ol>\n<p>See &quot;Pricing Example #6: Real-Time Inference&quot; as well.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":13.2,
        "Solution_reading_time":4.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2,
        "Solution_word_count":51,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Deploying my model to ACI takes forever and fails without any error message. In the ML workspace, the status of the deployed endpoint is unhealthy. I checked common errors while deployment but could not solve the problem. Pleas help. The deployment is never successful and it keeps running.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649140550093,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/800334\/issue-in-my-mlops-cd-pipeline",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.2,
        "Challenge_reading_time":4.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Issue in my MLOPs CD pipeline.",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":53,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><em>anonymous user<\/em> Thanks for the question. Could you clarify the architecture of your model deployment? In particular, are you using a custom docker container for it?  Also, usually ACI would be used for testing, but I'd recommend investigating AKS for production model deployment.     <\/p>\n<p> I would deploy the container into a local machine\/VM with Docker to see the exact detail error message which you don't see via ACI deployment.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":9.9,
        "Solution_reading_time":5.52,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5,
        "Solution_word_count":70,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When ticking any boxes inside Azure Machine Learning Studio - or in the YAML templates by specifying it's enabled by a true or false value- you can't specify which Application Insights instance to use - it will use the default one connected to that Azure Machine Learning Studio only.    <\/p>\n<p>For using something like Azure Machine Learning Studio as a solutions provider, and that might have multiple customers models within it, the ability to be able to specify a Applications Insights connection string to an instance OUTSIDE of the default in-built one would be a great addition to functionality.     <\/p>\n<p>With the current set up we are forced to have a different AML Studio \/ Storage Account \/ ACR \/ App Insights \/ Key vault for each customer to allow Applications Insights data collection to make any sense per customer.    <\/p>\n<p>Thanks    <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1671543228090,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1135890\/integration-with-application-insights-should-allow",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":11.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Integration with Application Insights should allow you to specify a choice of which instance to use",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":152,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=0ec06fb6-513e-4f5c-9aff-281bc5e44e22\">@Neil McAlister  <\/a> Thanks for the feedback. I have forwarded to the product team to support near future to specify a Applications Insights connection string to an instance OUTSIDE of the default in-built.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":11.1,
        "Solution_reading_time":3.6,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2,
        "Solution_word_count":34,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1558310526776,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":21.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>To start with, I understand that this question has been asked multiple times but I haven't found the solution to my problem.<\/p>\n<p>So, to start with I have used joblib.dump to save a locally trained sklearn RandomForest. I then uploaded this to s3, made a folder called code and put in an inference script there, called inference.py.<\/p>\n<pre><code>import joblib\nimport json\nimport numpy\nimport scipy\nimport sklearn\nimport os\n\n&quot;&quot;&quot;\nDeserialize fitted model\n&quot;&quot;&quot;\ndef model_fn(model_dir):\n    model_path = os.path.join(model_dir, 'test_custom_model')\n    model = joblib.load(model_path)\n    return model\n\n&quot;&quot;&quot;\ninput_fn\n    request_body: The body of the request sent to the model.\n    request_content_type: (string) specifies the format\/variable type of the request\n&quot;&quot;&quot;\ndef input_fn(request_body, request_content_type):\n    if request_content_type == 'application\/json':\n        request_body = json.loads(request_body)\n        inpVar = request_body['Input']\n        return inpVar\n    else:\n        raise ValueError(&quot;This model only supports application\/json input&quot;)\n\n&quot;&quot;&quot;\npredict_fn\n    input_data: returned array from input_fn above\n    model (sklearn model) returned model loaded from model_fn above\n&quot;&quot;&quot;\ndef predict_fn(input_data, model):\n    return model.predict(input_data)\n\n&quot;&quot;&quot;\noutput_fn\n    prediction: the returned value from predict_fn above\n    content_type: the content type the endpoint expects to be returned. Ex: JSON, string\n&quot;&quot;&quot;\n\ndef output_fn(prediction, content_type):\n    res = int(prediction[0])\n    respJSON = {'Output': res}\n    return respJSON\n<\/code><\/pre>\n<p>Very simple so far.<\/p>\n<p>I also put this into the local jupyter sagemaker session<\/p>\n<p>all_files (folder)\ncode (folder)\ninference.py (python file)\ntest_custom_model (joblib dump of model)<\/p>\n<p>The script turns this folder all_files into a tar.gz file<\/p>\n<p>Then comes the main script that I ran on sagemaker:<\/p>\n<pre><code>import boto3\nimport json\nimport os\nimport joblib\nimport pickle\nimport tarfile\nimport sagemaker\nimport time\nfrom time import gmtime, strftime\nimport subprocess\nfrom sagemaker import get_execution_role\n\n#Setup\nclient = boto3.client(service_name=&quot;sagemaker&quot;)\nruntime = boto3.client(service_name=&quot;sagemaker-runtime&quot;)\nboto_session = boto3.session.Session()\ns3 = boto_session.resource('s3')\nregion = boto_session.region_name\nprint(region)\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n\n#Bucket for model artifacts\ndefault_bucket = 'pretrained-model-deploy'\nmodel_artifacts = f&quot;s3:\/\/{default_bucket}\/test_custom_model.tar.gz&quot;\n\n#Build tar file with model data + inference code\nbashCommand = &quot;tar -cvpzf test_custom_model.tar.gz all_files&quot;\nprocess = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\noutput, error = process.communicate()\n\n#Upload tar.gz to bucket\nresponse = s3.meta.client.upload_file('test_custom_model.tar.gz', default_bucket, 'test_custom_model.tar.gz')\n\n# retrieve sklearn image\nimage_uri = sagemaker.image_uris.retrieve(\n    framework=&quot;sklearn&quot;,\n    region=region,\n    version=&quot;0.23-1&quot;,\n    py_version=&quot;py3&quot;,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n)\n\n#Step 1: Model Creation\nmodel_name = &quot;sklearn-test&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\nprint(&quot;Model name: &quot; + model_name)\ncreate_model_response = client.create_model(\n    ModelName=model_name,\n    Containers=[\n        {\n            &quot;Image&quot;: image_uri,\n            &quot;ModelDataUrl&quot;: model_artifacts,\n        }\n    ],\n    ExecutionRoleArn=role,\n)\nprint(&quot;Model Arn: &quot; + create_model_response[&quot;ModelArn&quot;])\n\n#Step 2: EPC Creation - Serverless\nsklearn_epc_name = &quot;sklearn-epc&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\nresponse = client.create_endpoint_config(\n   EndpointConfigName=sklearn_epc_name,\n   ProductionVariants=[\n        {\n            &quot;ModelName&quot;: model_name,\n            &quot;VariantName&quot;: &quot;sklearnvariant&quot;,\n            &quot;ServerlessConfig&quot;: {\n                &quot;MemorySizeInMB&quot;: 2048,\n                &quot;MaxConcurrency&quot;: 20\n            }\n        } \n    ]\n)\n\n# #Step 2: EPC Creation - Synchronous\n# sklearn_epc_name = &quot;sklearn-epc&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\n# endpoint_config_response = client.create_endpoint_config(\n#     EndpointConfigName=sklearn_epc_name,\n#     ProductionVariants=[\n#         {\n#             &quot;VariantName&quot;: &quot;sklearnvariant&quot;,\n#             &quot;ModelName&quot;: model_name,\n#             &quot;InstanceType&quot;: &quot;ml.m5.xlarge&quot;,\n#             &quot;InitialInstanceCount&quot;: 1\n#         },\n#     ],\n# )\n# print(&quot;Endpoint Configuration Arn: &quot; + endpoint_config_response[&quot;EndpointConfigArn&quot;])\n\n#Step 3: EP Creation\nendpoint_name = &quot;sklearn-local-ep&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\ncreate_endpoint_response = client.create_endpoint(\n    EndpointName=endpoint_name,\n    EndpointConfigName=sklearn_epc_name,\n)\nprint(&quot;Endpoint Arn: &quot; + create_endpoint_response[&quot;EndpointArn&quot;])\n\n\n#Monitor creation\ndescribe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\nwhile describe_endpoint_response[&quot;EndpointStatus&quot;] == &quot;Creating&quot;:\n    describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n    print(describe_endpoint_response)\n    time.sleep(15)\nprint(describe_endpoint_response)\n<\/code><\/pre>\n<p>Now, I mainly just want the serverless deployment but that fails after a while with this error message:<\/p>\n<pre><code>{'EndpointName': 'sklearn-local-ep2022-04-29-12-16-10', 'EndpointArn': 'arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-12-16-10', 'EndpointConfigName': 'sklearn-epc2022-04-29-12-16-03', 'EndpointStatus': 'Creating', 'CreationTime': datetime.datetime(2022, 4, 29, 12, 16, 10, 290000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 4, 29, 12, 16, 11, 52000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '1d25120e-ddb1-474d-9c5f-025c6be24383', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '1d25120e-ddb1-474d-9c5f-025c6be24383', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '305', 'date': 'Fri, 29 Apr 2022 12:21:59 GMT'}, 'RetryAttempts': 0}}\n{'EndpointName': 'sklearn-local-ep2022-04-29-12-16-10', 'EndpointArn': 'arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-12-16-10', 'EndpointConfigName': 'sklearn-epc2022-04-29-12-16-03', 'EndpointStatus': 'Failed', 'FailureReason': 'Unable to successfully stand up your model within the allotted 180 second timeout. Please ensure that downloading your model artifacts, starting your model container and passing the ping health checks can be completed within 180 seconds.', 'CreationTime': datetime.datetime(2022, 4, 29, 12, 16, 10, 290000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 4, 29, 12, 22, 2, 68000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '59fb8ddd-9d45-41f5-9383-236a2baffb73', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '59fb8ddd-9d45-41f5-9383-236a2baffb73', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '559', 'date': 'Fri, 29 Apr 2022 12:22:15 GMT'}, 'RetryAttempts': 0}}\n<\/code><\/pre>\n<p>The real time deployment is just permanently stuck at creating.<\/p>\n<p>Cloudwatch has the following errors:\nError handling request \/ping<\/p>\n<p>AttributeError: 'NoneType' object has no attribute 'startswith'<\/p>\n<p>with traceback:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base_async.py&quot;, line 55, in handle\n    self.handle_request(listener_name, req, client, addr)\n<\/code><\/pre>\n<p>Copy paste has stopped working so I have attached an image of it instead.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/hw80j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hw80j.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>This is the error message I get:\nEndpoint Arn: arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-13-18-09\n{'EndpointName': 'sklearn-local-ep2022-04-29-13-18-09', 'EndpointArn': 'arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-13-18-09', 'EndpointConfigName': 'sklearn-epc2022-04-29-13-18-07', 'EndpointStatus': 'Creating', 'CreationTime': datetime.datetime(2022, 4, 29, 13, 18, 9, 548000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 4, 29, 13, 18, 13, 119000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': 'ef0e49ee-618e-45de-9c49-d796206404a4', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'ef0e49ee-618e-45de-9c49-d796206404a4', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '306', 'date': 'Fri, 29 Apr 2022 13:18:24 GMT'}, 'RetryAttempts': 0}}<\/p>\n<p>These are the permissions I have associated with that role:<\/p>\n<pre><code>AmazonSageMaker-ExecutionPolicy\nSecretsManagerReadWrite\nAmazonS3FullAccess\nAmazonSageMakerFullAccess\nEC2InstanceProfileForImageBuilderECRContainerBuilds\nAWSAppRunnerServicePolicyForECRAccess\n<\/code><\/pre>\n<p>What am I doing wrong? I've tried different folder structures for the zip file, different accounts, all to no avail. I don't really want to use the model.deploy() method as I don't know how to use serverless with that, and it's also inconcistent between different model types (I'm trying to make a flexible deployment pipeline where different (xgb \/ sklearn) models can be deployed with minimal changes.<\/p>\n<p>Please send help, I'm very close to smashing my hair and tearing out my laptop, been struggling with this for a whole 4 days now.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651238636737,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1651255570927,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72058686",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":16.9,
        "Challenge_reading_time":128.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":60,
        "Challenge_solved_time":null,
        "Challenge_title":"How do I deploy a pre trained sklearn model on AWS sagemaker? (Endpoint stuck on creating)",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":397.0,
        "Challenge_word_count":816,
        "Platform":"Stack Overflow",
        "Poster_created_time":1558310526776,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":21.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>I've solved this problem - I used sagemaker.model.model to load in the model data I already had and I called the deploy method on the aforementioned model object to deploy it. Further, I had the inference script and the model file in the same place as the notebook and directly called them, as this gave me an error earlier as well.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":8.7,
        "Solution_reading_time":4.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3,
        "Solution_word_count":60,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to follow <a href=\"https:\/\/azure.github.io\/azureml-sdk-for-r\/articles\/train-and-deploy-first-model.html\" rel=\"nofollow noreferrer\">this post<\/a> to deploy a &quot;model&quot; in Azure.<\/p>\n<p>A code snipet is as follows and the model, which is simply a function adding 2 numbers, seems to register fine. I don't even use the model to isolate the problem after 1000s of attempts as this scoring code shows:<\/p>\n<pre><code>library(jsonlite)\n\ninit &lt;- function()\n{\n  message(&quot;hello world&quot;)\n  \n  function(data)\n  {\n    vars &lt;- as.data.frame(fromJSON(data))\n    prediction &lt;- 2\n    toJSON(prediction)\n  }\n}\n<\/code><\/pre>\n<p>Should be fine shouldn't it? Any way I run this code snippet:<\/p>\n<pre><code>r_env &lt;- r_environment(name = &quot;basic_env&quot;)\ninference_config &lt;- inference_config(\n  entry_script = &quot;score.R&quot;,\n  source_directory = &quot;.&quot;,\n  environment = r_env)\n\naci_config &lt;- aci_webservice_deployment_config(cpu_cores = 1, memory_gb = 0.5)\n\naci_service &lt;- deploy_model(ws, \n                            'xxxxx', \n                            list(model), \n                            inference_config, \n                            aci_config)\n\nwait_for_deployment(aci_service, show_output = TRUE)\n<\/code><\/pre>\n<p>Which produces this (after a looooong time):<\/p>\n<pre><code>Running.....................................................................\nFailed\nService deployment polling reached non-successful terminal state, current service state: Failed\nOperation ID: 14c35064-7ff4-46aa-9bfa-ab8a63218a2c\nMore information can be found using '.get_logs()'\nError:\n{\n  &quot;code&quot;: &quot;AciDeploymentFailed&quot;,\n  &quot;statusCode&quot;: 400,\n  &quot;message&quot;: &quot;Aci Deployment failed with exception: Error in entry script, RuntimeError: Error in file(filename, \\&quot;r\\&quot;, encoding = encoding) : , please run print(service.get_logs()) to get details.&quot;,\n  &quot;details&quot;: [\n    {\n      &quot;code&quot;: &quot;CrashLoopBackOff&quot;,\n      &quot;message&quot;: &quot;Error in entry script, RuntimeError: Error in file(filename, \\&quot;r\\&quot;, encoding = encoding) : , please run print(service.get_logs()) to get details.&quot;\n    }\n  ]\n}\n<\/code><\/pre>\n<p>It does not tell me much. Not sure how to debug this further? How can I run this:<\/p>\n<pre><code>print(service.get_logs())\n<\/code><\/pre>\n<p>and where please? Guess this is a Python artifact? Any other input very much welcome.<\/p>\n<p>PS:<\/p>\n<p>At this point in time, I have my suspicion that the above R entry file definition is not what is expected these days. Looking at the Python equivalent taken from <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=azcli\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<pre><code>import json\n\ndef init():\n    print(&quot;This is init&quot;)\n\ndef run(data):\n    test = json.loads(data)\n    print(f&quot;received data {test}&quot;)\n    return f&quot;test is {test}&quot;\n<\/code><\/pre>\n<p>Would something like this not be more suitable (tried it without success).<\/p>\n<pre><code>library(jsonlite)\n\ninit &lt;- function()\n{\n    message(&quot;hello world&quot;)\n}\n\ninit &lt;- function()\n{\n    return(42)\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620998276233,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1621008123540,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67535014",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":40.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":null,
        "Challenge_title":"deploy model and expose model as web service via azure machine learning + azuremlsdk in R",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":282.0,
        "Challenge_word_count":319,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>Great to see people putting the R SDK through it's paces!<\/p>\n<p>The vignette you're using is obviously a great way to get started. It seems you're almost all the way through without a hitch.<\/p>\n<p>Deployment is always tricky, and I'm not expert myself. I'd point you to this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment-local?WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">guide on troubleshooting deployment locally<\/a>. Similar functionality exists for the R SDK, namely: <a href=\"https:\/\/azure.github.io\/azureml-sdk-for-r\/reference\/local_webservice_deployment_config.html\" rel=\"nofollow noreferrer\"><code>local_webservice_deployment_config()<\/code><\/a>.<\/p>\n<p>So I think you change your example to this:<\/p>\n<pre class=\"lang-r prettyprint-override\"><code>deployment_config &lt;- local_webservice_deployment_config(port = 8890)\n<\/code><\/pre>\n<p>Once you know the service is working locally, the issue you're having with the ACI webservice becomes a lot easier to narrow down.<\/p>",
        "Solution_comment_count":8.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":13.8,
        "Solution_reading_time":13.74,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9,
        "Solution_word_count":107,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1525449880547,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Madrid, Spain",
        "Answerer_reputation_count":81.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am starting to use aws sagemaker on the development of my machine learning model and I'm trying to build a lambda function to process the responses of a sagemaker labeling job. I already created my own lambda function but when I try to read the event contents I can see that the event dict is completely empty, so I'm not getting any data to read.<\/p>\n\n<p>I have already given enough permissions to the role of the lambda function. Including:\n- AmazonS3FullAccess.\n- AmazonSagemakerFullAccess.\n- AWSLambdaBasicExecutionRole<\/p>\n\n<p>I've tried using this code for the Post-annotation Lambda (adapted for python 3.6):<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step2-demo1.html#sms-custom-templates-step2-demo1-post-annotation\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step2-demo1.html#sms-custom-templates-step2-demo1-post-annotation<\/a><\/p>\n\n<p>As well as this one in this git repository:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/aws-sagemaker-ground-truth-recipe\/blob\/master\/aws_sagemaker_ground_truth_sample_lambda\/annotation_consolidation_lambda.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/aws-sagemaker-ground-truth-recipe\/blob\/master\/aws_sagemaker_ground_truth_sample_lambda\/annotation_consolidation_lambda.py<\/a><\/p>\n\n<p>But none of them seemed to work.<\/p>\n\n<p>For creating the labeling job I'm using boto3's functions for sagemaker:\n<a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_labeling_job\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_labeling_job<\/a><\/p>\n\n<p>This is the code i have for creating the labeling job:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def create_labeling_job(client,bucket_name ,labeling_job_name, manifest_uri, output_path):\n\n    print(\"Creating labeling job with name: %s\"%(labeling_job_name))\n\n    response = client.create_labeling_job(\n        LabelingJobName=labeling_job_name,\n        LabelAttributeName='annotations',\n        InputConfig={\n            'DataSource': {\n                'S3DataSource': {\n                    'ManifestS3Uri': manifest_uri\n                }\n            },\n            'DataAttributes': {\n                'ContentClassifiers': [\n                    'FreeOfAdultContent',\n                ]\n            }\n        },\n        OutputConfig={\n            'S3OutputPath': output_path\n        },\n        RoleArn='arn:aws:myrolearn',\n        LabelCategoryConfigS3Uri='s3:\/\/'+bucket_name+'\/config.json',\n        StoppingConditions={\n            'MaxPercentageOfInputDatasetLabeled': 100,\n        },\n        LabelingJobAlgorithmsConfig={\n            'LabelingJobAlgorithmSpecificationArn': 'arn:image-classification'\n        },\n        HumanTaskConfig={\n            'WorkteamArn': 'arn:my-private-workforce-arn',\n            'UiConfig': {\n                'UiTemplateS3Uri':'s3:\/\/'+bucket_name+'\/templatefile'\n            },\n            'PreHumanTaskLambdaArn': 'arn:aws:lambda:us-east-1:432418664414:function:PRE-BoundingBox',\n            'TaskTitle': 'Title',\n            'TaskDescription': 'Description',\n            'NumberOfHumanWorkersPerDataObject': 1,\n            'TaskTimeLimitInSeconds': 600,\n            'AnnotationConsolidationConfig': {\n                'AnnotationConsolidationLambdaArn': 'arn:aws:my-custom-post-annotation-lambda'\n            }\n        }\n    )\n\n    return response\n<\/code><\/pre>\n\n<p>And this is the one i have for the lambda function:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>    print(\"Received event: \" + json.dumps(event, indent=2))\n    print(\"event: %s\"%(event))\n    print(\"context: %s\"%(context))\n    print(\"event headers: %s\"%(event[\"headers\"]))\n\n    parsed_url = urlparse(event['payload']['s3Uri']);\n    print(\"parsed_url: \",parsed_url)\n\n    labeling_job_arn = event[\"labelingJobArn\"]\n    label_attribute_name = event[\"labelAttributeName\"]\n\n    label_categories = None\n    if \"label_categories\" in event:\n        label_categories = event[\"labelCategories\"]\n        print(\" Label Categories are : \" + label_categories)\n\n    payload = event[\"payload\"]\n    role_arn = event[\"roleArn\"]\n\n    output_config = None # Output s3 location. You can choose to write your annotation to this location\n    if \"outputConfig\" in event:\n        output_config = event[\"outputConfig\"]\n\n    # If you specified a KMS key in your labeling job, you can use the key to write\n    # consolidated_output to s3 location specified in outputConfig.\n    kms_key_id = None\n    if \"kmsKeyId\" in event:\n        kms_key_id = event[\"kmsKeyId\"]\n\n    # Create s3 client object\n    s3_client = S3Client(role_arn, kms_key_id)\n\n    # Perform consolidation\n    return do_consolidation(labeling_job_arn, payload, label_attribute_name, s3_client)\n<\/code><\/pre>\n\n<p>I've tried debugging the event object with:<\/p>\n\n<pre><code>    print(\"Received event: \" + json.dumps(event, indent=2))\n<\/code><\/pre>\n\n<p>But it just prints an empty dictionary: <code>Received event: {}<\/code><\/p>\n\n<p>I expect the output to be something like:<\/p>\n\n<pre><code>    #Content of an example event:\n    {\n        \"version\": \"2018-10-16\",\n        \"labelingJobArn\": &lt;labelingJobArn&gt;,\n        \"labelCategories\": [&lt;string&gt;],  # If you created labeling job using aws console, labelCategories will be null\n        \"labelAttributeName\": &lt;string&gt;,\n        \"roleArn\" : \"string\",\n        \"payload\": {\n            \"s3Uri\": &lt;string&gt;\n        }\n        \"outputConfig\":\"s3:\/\/&lt;consolidated_output configured for labeling job&gt;\"\n    }\n<\/code><\/pre>\n\n<p>Lastly, when I try yo get the labeling job ARN with:<\/p>\n\n<pre><code>    labeling_job_arn = event[\"labelingJobArn\"]\n<\/code><\/pre>\n\n<p>I just get a KeyError (which makes sense because the dictionary is empty).<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1564494701657,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1575462189052,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57273357",
        "Challenge_link_count":6,
        "Challenge_participation_count":2,
        "Challenge_readability":23.8,
        "Challenge_reading_time":70.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"Empty dictionary on AnnotationConsolidation lambda event for aws Sagemaker",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":846.0,
        "Challenge_word_count":425,
        "Platform":"Stack Overflow",
        "Poster_created_time":1525449880547,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Madrid, Spain",
        "Poster_reputation_count":81.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I found the problem, I needed to add the ARN of the role used by my Lamda function as a Trusted Entity on the Role used for the Sagemaker Labeling Job.<\/p>\n\n<p>I just went to <code>Roles &gt; MySagemakerExecutionRole &gt; Trust Relationships<\/code> and added:<\/p>\n\n<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": [\n          \"arn:aws:iam::xxxxxxxxx:role\/My-Lambda-Role\",\n           ...\n        ],\n        \"Service\": [\n          \"lambda.amazonaws.com\",\n          \"sagemaker.amazonaws.com\",\n           ...\n        ]\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n<\/code><\/pre>\n\n<p>This made it work for me.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":14.0,
        "Solution_reading_time":7.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3,
        "Solution_word_count":64,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1477948823888,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":721.0,
        "Answerer_view_count":57.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an AWS Sagemaker inference endpoint with autoscaling enabled with SageMakerVariantInvocationsPerInstance target metric. When I send a lot of requests to the endpoint the number of instances correctly scales out to the maximum instance count. But after I stop sending the requests the number of instances doesn't scale in to 1, minimum instance count. I waited for many hours. Is there a reason for this behaviour?<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608117787513,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65322286",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":6.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS Sagemaker inference endpoint doesn't scale in with autoscaling",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1028.0,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Poster_created_time":1382978984190,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1311.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>AutoScaling requires a cloudwatch alarm to trigger to scale in.  Sagemaker doesn't push 0 value metrics when there's no activity (it just doesn't push anything).  This leads to the alarm being put into insufficient data and not triggering the autoscaling scale in action when your workload suddenly ends.<\/p>\n<p>Workarounds are either:<\/p>\n<ol>\n<li>Have a step scaling policy using the cloudwatch metric math FILL() function for your scale in.  This way you can tell CloudWatch &quot;if there's no data, pretend this was the metric value when evaluating the alarm.  This is only possible with step scaling since target tracking creates the alarms for you (and AutoScaling will periodically recreate them, so if you make manual changes they'll get deleted)<\/li>\n<li>Have scheduled scaling set the size back down to 1 every evening<\/li>\n<li>Make sure traffic continues at a low level for some times<\/li>\n<\/ol>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":11.4,
        "Solution_reading_time":11.24,
        "Solution_score_count":5.0,
        "Solution_sentence_count":6,
        "Solution_word_count":142,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created an experiment in Machine Learning Studio and deployed it as a web service. I've got a request-response API in my workspace that works. Can it be also used by other people?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1597493508787,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63425902",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.0,
        "Challenge_reading_time":3.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Are Machine Learning Studios's web services public?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":39.0,
        "Challenge_word_count":40,
        "Platform":"Stack Overflow",
        "Poster_created_time":1575044869560,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":21.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>When you deploy a model, a Webservice object is returned with information about the service.<\/p>\n<pre><code>from azureml.core.webservice import AciWebservice, Webservice\nfrom azureml.core.model import Model\n\ndeployment_config = AciWebservice.deploy_configuration(cpu_cores = 3, memory_gb = 15, location = &quot;centralus&quot;)\nservice = Model.deploy(ws, &quot;aciservice&quot;, [model], inference_config, deployment_config)\nservice.wait_for_deployment(show_output = True)\nprint(service.state)\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/AvVgY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AvVgY.png\" alt=\"enter image description here\" \/><\/a>\nPlease follow the below to Consume an Azure Machine Learning model deployed as a web service\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-consume-web-service\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-consume-web-service<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_readability":20.0,
        "Solution_reading_time":13.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11,
        "Solution_word_count":71,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1612454694036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Mexico City, CDMX, Mexico",
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have an ML model deployed on Azure ML Studio and I was updating it with an inference schema to allow compatibility with Power BI as described <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p>When sending data up to the model via REST api (before adding this inference schema), everything works fine and I get results returned. However, once adding the schema as described in the instructions linked above and personalising to my data, the same data sent via REST api only returns the error &quot;list index out of range&quot;. The deployment goes ahead fine and is designated as &quot;healthy&quot; with no error messages.<\/p>\n<p>Any help would be greatly appreciated. Thanks.<\/p>\n<p>EDIT:<\/p>\n<p>Entry script:<\/p>\n<pre><code> import numpy as np\n import pandas as pd\n import joblib\n from azureml.core.model import Model\n    \n from inference_schema.schema_decorators import input_schema, output_schema\n from inference_schema.parameter_types.standard_py_parameter_type import StandardPythonParameterType\n from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n from inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\n    \n def init():\n     global model\n     #Model name is the name of the model registered under the workspace\n     model_path = Model.get_model_path(model_name = 'databricksmodelpowerbi2')\n     model = joblib.load(model_path)\n    \n #Provide 3 sample inputs for schema generation for 2 rows of data\n numpy_sample_input = NumpyParameterType(np.array([[2400.0, 78.26086956521739, 11100.0, 3.612565445026178, 3.0, 0.0], [368.55, 96.88311688311687, 709681.1600000012, 73.88059701492537, 44.0, 0.0]], dtype = 'float64'))\n pandas_sample_input = PandasParameterType(pd.DataFrame({'1': [2400.0, 368.55], '2': [78.26086956521739, 96.88311688311687], '3': [11100.0, 709681.1600000012], '4': [3.612565445026178, 73.88059701492537], '5': [3.0, 44.0], '6': [0.0, 0.0]}))\n standard_sample_input = StandardPythonParameterType(0.0)\n    \n # This is a nested input sample, any item wrapped by `ParameterType` will be described by schema\n sample_input = StandardPythonParameterType({'input1': numpy_sample_input, \n                                             'input2': pandas_sample_input, \n                                             'input3': standard_sample_input})\n    \n sample_global_parameters = StandardPythonParameterType(1.0) #this is optional\n sample_output = StandardPythonParameterType([1.0, 1.0])\n    \n @input_schema('inputs', sample_input)\n @input_schema('global_parameters', sample_global_parameters) #this is optional\n @output_schema(sample_output)\n    \n def run(inputs, global_parameters):\n     try:\n         data = inputs['input1']\n         # data will be convert to target format\n         assert isinstance(data, np.ndarray)\n         result = model.predict(data)\n         return result.tolist()\n     except Exception as e:\n         error = str(e)\n         return error\n<\/code><\/pre>\n<p>Prediction script:<\/p>\n<pre><code> import requests\n import json\n from ast import literal_eval\n    \n # URL for the web service\n scoring_uri = ''\n ## If the service is authenticated, set the key or token\n #key = '&lt;your key or token&gt;'\n    \n # Two sets of data to score, so we get two results back\n data = {&quot;data&quot;: [[2400.0, 78.26086956521739, 11100.0, 3.612565445026178, 3.0, 0.0], [368.55, 96.88311688311687, 709681.1600000012, 73.88059701492537, 44.0, 0.0]]}\n # Convert to JSON string\n input_data = json.dumps(data)\n    \n # Set the content type\n headers = {'Content-Type': 'application\/json'}\n ## If authentication is enabled, set the authorization header\n #headers['Authorization'] = f'Bearer {key}'\n    \n # Make the request and display the response\n resp = requests.post(scoring_uri, input_data, headers=headers)\n print(resp.text)\n    \n result = literal_eval(resp.text)\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1602495267847,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1615561798207,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64315239",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":13.9,
        "Challenge_reading_time":48.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML Inference Schema - \"List index out of range\" error",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":785.0,
        "Challenge_word_count":386,
        "Platform":"Stack Overflow",
        "Poster_created_time":1600260166047,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>The Microsoft <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script#power-bi-compatible-endpoint\" rel=\"nofollow noreferrer\">documentation<\/a> say's: &quot;In order to generate conforming swagger for automated web service consumption, scoring script run() function must have API shape of:<\/p>\n<blockquote>\n<p>A first parameter of type &quot;StandardPythonParameterType&quot;, named\n<strong>Inputs<\/strong> and nested.<\/p>\n<p>An optional second parameter of type &quot;StandardPythonParameterType&quot;,\nnamed GlobalParameters.<\/p>\n<p>Return a dictionary of type &quot;StandardPythonParameterType&quot; named\n<strong>Results<\/strong> and nested.&quot;<\/p>\n<\/blockquote>\n<p>I've already test this and it is case sensitive\nSo it will be like this:<\/p>\n<pre><code>import numpy as np\nimport pandas as pd\nimport joblib\n\nfrom azureml.core.model import Model\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.standard_py_parameter_type import \n    StandardPythonParameterType\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\nfrom inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\n\ndef init():\n    global model\n    # Model name is the name of the model registered under the workspace\n    model_path = Model.get_model_path(model_name = 'databricksmodelpowerbi2')\n    model = joblib.load(model_path)\n\n# Provide 3 sample inputs for schema generation for 2 rows of data\nnumpy_sample_input = NumpyParameterType(np.array([[2400.0, 78.26086956521739, 11100.0, \n3.612565445026178, 3.0, 0.0], [368.55, 96.88311688311687, 709681.1600000012, \n73.88059701492537, 44.0, 0.0]], dtype = 'float64'))\n\npandas_sample_input = PandasParameterType(pd.DataFrame({'value': [2400.0, 368.55], \n'delayed_percent': [78.26086956521739, 96.88311688311687], 'total_value_delayed': \n[11100.0, 709681.1600000012], 'num_invoices_per30_dealing_days': [3.612565445026178, \n73.88059701492537], 'delayed_streak': [3.0, 44.0], 'prompt_streak': [0.0, 0.0]}))\n\nstandard_sample_input = StandardPythonParameterType(0.0)\n\n# This is a nested input sample, any item wrapped by `ParameterType` will be described \nby schema\nsample_input = StandardPythonParameterType({'input1': numpy_sample_input, \n                                         'input2': pandas_sample_input, \n                                         'input3': standard_sample_input})\n\nsample_global_parameters = StandardPythonParameterType(1.0) #this is optional\n\nnumpy_sample_output = NumpyParameterType(np.array([1.0, 2.0]))\n\n# 'Results' is case sensitive\nsample_output = StandardPythonParameterType({'Results': numpy_sample_output})\n\n# 'Inputs' is case sensitive\n@input_schema('Inputs', sample_input)\n@input_schema('global_parameters', sample_global_parameters) #this is optional\n@output_schema(sample_output)\ndef run(Inputs, global_parameters):\n    try:\n        data = inputs['input1']\n        # data will be convert to target format\n        assert isinstance(data, np.ndarray)\n        result = model.predict(data)\n        return result.tolist()\n    except Exception as e:\n        error = str(e)\n        return error\n<\/code><\/pre>\n<p>`<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1614801695372,
        "Solution_link_count":1,
        "Solution_readability":19.0,
        "Solution_reading_time":40.74,
        "Solution_score_count":1.0,
        "Solution_sentence_count":25,
        "Solution_word_count":253,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":235.0291777778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When using Inference Schema to autogenerate the swagger doc for my AzureML endpoint (as detailed <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script\" rel=\"nofollow noreferrer\">here<\/a> and <a href=\"https:\/\/github.com\/Azure\/InferenceSchema\" rel=\"nofollow noreferrer\">here<\/a>), I see that it creates a wrapper around my input_sample. Is there a way to\nnot wrap the input inside this &quot;data&quot; wrapper?<\/p>\n<p>Here is what my score.py looks like:<\/p>\n<pre><code>input_sample = {\n                &quot;id&quot;: 123,\n                &quot;language&quot;: &quot;en&quot;\n                &quot;items&quot;: [{\n                    &quot;item&quot;: 1,\n                    &quot;desc&quot;: &quot;desc&quot;\n                }]\n            }\noutput_sample = [{'prediction': 'true', 'predictionConfidence': 0.8279970776764844}]\n\n@input_schema('data', StandardPythonParameterType(input_sample))\n@output_schema(StandardPythonParameterType(output_sample))\ndef run(data):\n&quot;&quot;&quot;\n    {\n        data: { --&gt; DON'T WANT this &quot;data&quot; wrapper\n                &quot;id&quot;: 123,\n                &quot;language&quot;: &quot;en&quot;\n                &quot;items&quot;: [{\n                    &quot;item&quot;: 1,\n                    &quot;desc&quot;: &quot;desc&quot;\n                }]\n            }\n    }\n    &quot;&quot;&quot;\n    try:\n        id = data['id']\n        ...\n        \n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1600982458753,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1601039285472,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64054587",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":14.5,
        "Challenge_reading_time":16.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I remove the wrapper around the input when using Inference Schema",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":173.0,
        "Challenge_word_count":108,
        "Platform":"Stack Overflow",
        "Poster_created_time":1406298639460,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":435.0,
        "Poster_view_count":48.0,
        "Solution_body":"<p>InferenceSchema used with Azure Machine Learning deployments, then the code for this package was recently published at <a href=\"https:\/\/github.com\/Azure\/InferenceSchema\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/InferenceSchema<\/a> under an MIT license. So you could possibly use that to create a version specific to your needs.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1601885390512,
        "Solution_link_count":2,
        "Solution_readability":10.7,
        "Solution_reading_time":4.5,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":39,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to return output(or predictions) from a SageMaker endpoint with 'text\/csv' or 'text\/csv; charset=utf-8' as &quot;Content-Type&quot;. I have tried multiple ways, but the sagemaker always returns with 'text\/html; charset=utf-8' as the &quot;Content-Type&quot;, and I would like SageMaker to return 'text\/csv' or 'text\/csv; charset=utf-8'.<\/p>\n<p>Here's the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#process-output\" rel=\"nofollow noreferrer\"><code>output_fn<\/code><\/a> from my inference-code:<\/p>\n<pre><code>** my other code **\ndef output_fn(prediction, content_type='text\/csv'):\n    ** my other code **\n    return output_float\n<\/code><\/pre>\n<p>above function returns number with float data-type and I got error(in cloudwatch logs) that  this function should only be returning string, tuple, dict or Respoonse instance.<\/p>\n<p>So, here are all the different ways I have tried to have SageMaker return my number with 'text\/csv' but only receives 'text\/html; charset=utf-8'<\/p>\n<ol>\n<li><code>return json.dumps(output_float)<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float}&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float},&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float}\\n&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float},\\n&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li>by using <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html\" rel=\"nofollow noreferrer\"><code>sagemaker.serializers.CSVSerializer<\/code><\/a> like this:\n<pre><code>from sagemaker.serializers import CSVSerializer\ncsv_serialiser = CSVSerializer(content_type='text\/csv')\n\ndef output_fn(prediction, content_type='text\/csv'):\n    ** my other code **\n    return csv_serialiser.serialize(output_float)\n<\/code><\/pre>\nI got <code>'NoneType' object has no attribute 'startswith'<\/code> error with this.<\/li>\n<li>as a tuple: <code>return (output_float,)<\/code>. I haven't noted down what this did, but it sure didn't return the number with 'text\/csv' as &quot;Content-Type&quot;.<\/li>\n<li>made changes in my model object to return a float on calling <code>.predict_proba<\/code> on my model-object and deployed it from sagemaker studio without using any custom inference code and deployed this from SageMaker studio. but got this error after sending a request to the endpoint: <code>'NoneType' object has no attribute 'startswith'<\/code>, but at my side, when I pass proper inputs to the unpicked model and call .predict_proba, i get float as expected.<\/li>\n<li>by returning <a href=\"https:\/\/tedboy.github.io\/flask\/generated\/generated\/flask.Response.html#flask.Response\" rel=\"nofollow noreferrer\"><code>flask.Response<\/code><\/a> like this:\n<pre><code>from flask import Response\n\ndef output_fn(prediction, content_type='text\/csv'):\n    ** my other code **\n    return Response(response=output_float, status=200, headers={'Content-Type':'text\/csv; charset=utf-8'})\n<\/code><\/pre>\nbut, I got some IndexError with this(I didn't notedown the traceback.)<\/li>\n<\/ol>\n<p>Some other info:<\/p>\n<ol>\n<li>Model I am using is completely outside of SageMaker, not from a training job or anything like that.<\/li>\n<li>All of the aforementioned endpoints have been deployed entirely from aws-cli with relevant .json files, except the one in point-8 above.<\/li>\n<\/ol>\n<p>How do I return number with &quot;text\/csv&quot; as content-type from sagemaker? I need my output in &quot;text\/csv&quot; content-type specifically for Model-Quality-Monitor. How do I do this?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1646135687137,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1646652069630,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71308112",
        "Challenge_link_count":3,
        "Challenge_participation_count":3,
        "Challenge_readability":12.5,
        "Challenge_reading_time":49.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":null,
        "Challenge_title":"How to return a float with 'text\/csv' as \"Content-Type\" from SageMaker endpoint that uses custom inference code?",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":461.0,
        "Challenge_word_count":398,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559910246180,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":2046.0,
        "Poster_view_count":369.0,
        "Solution_body":"<p>From the <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/scikit_bring_your_own\/container\/decision_trees\/predictor.py#L88\" rel=\"nofollow noreferrer\">scikit_bring_your_own<\/a> example, I suggest testing by setting the Response as follows:<\/p>\n<pre><code>return flask.Response(response= output_float, status=200, mimetype=&quot;text\/csv&quot;)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":27.4,
        "Solution_reading_time":5.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3,
        "Solution_word_count":22,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1254957460063,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"North Carolina, USA",
        "Answerer_reputation_count":2484.0,
        "Answerer_view_count":362.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I used Machine learning tutorial: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/create-experiment\" rel=\"nofollow noreferrer\">Create your first data science experiment in Azure Machine Learning Studio<\/a> to create an <code>Experiment<\/code> and then converted it to a <code>predictive experiment<\/code>. Now I'm trying to deploy it as a Web Service by following this article that was referenced in the above article: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/publish-a-machine-learning-web-service#deploy-it-as-a-web-service\" rel=\"nofollow noreferrer\">Deploy it as a web service<\/a>. But when I click on <code>Run<\/code> and then on <code>Deploy Web Service<\/code>, I don't see the <code>Price Plan<\/code> dropdown and <code>Plan Name<\/code> input box etc as mentioned in the section <code>Machine Learning Web Service portal Deploy Experiment Page<\/code> of the second article above. After I clicked on Deploy Web Service link in ML studio, I got the page shown below.<strong>Question<\/strong>: What I may be doing wrong?<\/p>\n\n<p>Note: You can click on the picture to get a larger view.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/G3TKo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/G3TKo.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526313605237,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50334563",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":12.1,
        "Challenge_reading_time":18.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Deployment of an Azure ML Experiment as a Web Service through Azure Machine Learning Studio",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":330.0,
        "Challenge_word_count":162,
        "Platform":"Stack Overflow",
        "Poster_created_time":1330144099340,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":19815.0,
        "Poster_view_count":2272.0,
        "Solution_body":"<p>I think it depends on what workspace you're in. If you're in the free one then you get the screen that you already get, but if you create a workspace in the Azure portal and use that one, then you will get a screen like below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/drRpa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/drRpa.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>To create a new workspace, in the Azure Portal, create a new \"Machine Learning Studio Workspace\" and when you go to Azure ML Studio select the new workspace from the top right.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":7.7,
        "Solution_reading_time":7.3,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5,
        "Solution_word_count":87,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1434295027120,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Lusaka, Zambia",
        "Answerer_reputation_count":951.0,
        "Answerer_view_count":157.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I need to use the <a href=\"https:\/\/aws.amazon.com\/marketplace\/pp\/prodview-7y6xdiukxucr2\" rel=\"nofollow noreferrer\">WireframeToCode<\/a> model from the AWS Marketplace, I used Nodejs to read and send the file data to the model like this:<\/p>\n\n<pre><code>var sageMakerRuntime = new AWS.SageMakerRuntime();\n\nvar bitmap = fs.readFileSync(\"sample.jpeg\", \"utf8\");\nvar buffer = new Buffer.from(bitmap, \"base64\");\n\nvar params = {\n  Body: buffer.toJSON(),\n  EndpointName: \"wireframe-to-code\",\n  Accept: \"image\/jpeg\",\n  ContentType: \"application\/json\"\n};\n\nsageMakerRuntime.invokeEndpoint(params, function(err, data) {\n  if (err) console.log(err, err.stack);\n  else console.log(data);\n});\n<\/code><\/pre>\n\n<p>but i get this error:<\/p>\n\n<blockquote>\n  <p>message: 'Expected params.Body to be a string, Buffer, Stream, Blob,\n  or typed array object',   code: 'InvalidParameterType',   time:\n  2020-03-30T11:06:27.535Z<\/p>\n<\/blockquote>\n\n<p>From the documentation, the supported content type for input is  <code>image\/jpeg<\/code> output is <code>application\/json<\/code>.<\/p>\n\n<p>when I try to convert the Body to a string like this: <code>JSON.stringify(buffer.toJSON())<\/code> I get this error:<\/p>\n\n<blockquote>\n  <p>Received client error (415) from model with message \"This predictor\n  only supports JSON formatted data\"<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1585568125227,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60929678",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":11.9,
        "Challenge_reading_time":17.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"How to pass image to AWS SageMaker endpoint",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":2170.0,
        "Challenge_word_count":142,
        "Platform":"Stack Overflow",
        "Poster_created_time":1434295027120,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lusaka, Zambia",
        "Poster_reputation_count":951.0,
        "Poster_view_count":157.0,
        "Solution_body":"<p>I had to pass in bitmap and change <code>ContentType<\/code> to <code>\"image\/jpeg\"<\/code><\/p>\n\n<pre><code>const AWS = require(\"aws-sdk\");\nconst fs = require(\"fs\");\n\nconst sageMakerRuntime = new AWS.SageMakerRuntime({\n  region: \"us-east-1\",\n  accessKeyId: \"XXXXXXXXXXXX\",\n  secretAccessKey: \"XXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n});\n\nconst bitmap = fs.readFileSync(\"sample.jpeg\");\n\nvar params = {\n  Body: bitmap,\n  EndpointName: \"wireframe-to-code\",\n  ContentType: \"image\/jpeg\"\n};\n\nsageMakerRuntime.invokeEndpoint(params, function(err, data) {\n  if (err) {\n    console.log(err, err.stack);\n  } else {\n    responseData = JSON.parse(Buffer.from(data.Body).toString());\n    console.log(responseData);\n  }\n});\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":17.8,
        "Solution_reading_time":9.0,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5,
        "Solution_word_count":50,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi there, I'm trying to register a ML model to the mlflow model registry to be served from an Azure web app. I'm wondering if the databricks networking configuration will apply to the model api endpoint, as in, calls to the api from outside the VNET which the databricks is deployed to with private endpoints and disabled public access will be rejected and the call from within the vnet integrated web app will be successful?    <\/p>\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":4,
        "Challenge_created_time":1662480716897,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/996163\/databricks-mlflow-model-serving-networking",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":10.1,
        "Challenge_reading_time":6.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Databricks mlflow model serving networking",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":82,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=72c1697f-78db-46b7-813e-f61c7171cb88\">@Chammie Ho  <\/a>,    <\/p>\n<p>Thanks for the question and using MS Q&amp;A platform.    <\/p>\n<blockquote>\n<p>I'm wondering if the databricks networking configuration will apply to the model api endpoint    <\/p>\n<\/blockquote>\n<p>Yes, it will. The single node cluster on which the model is hosted (classic), is deployed in data plane and will have a private IP.     <\/p>\n<blockquote>\n<p>I should be able to call the model with the url &lt;databricks-instance&gt;\/model\/&lt;registered-model-name&gt;\/&lt;model-version&gt;\/invocations, my question is whether this url will have the same restrictions as the databricks where it resides    <\/p>\n<\/blockquote>\n<p>I believe, this should work as long as you have not defined any IP access list. The PAT will let you authenticate.    <\/p>\n<blockquote>\n<p>but I can't find information for IP restrictions    <\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/databricks\/security\/network\/ip-access-list\">IP access lists - Azure Databricks | Microsoft Learn<\/a>    <\/p>\n<p>Hope this will help. Please let us know if any further queries.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.pngsfe?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.pngjust?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.htmlstr\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is jhow you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/support\/community-champions-program\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6,
        "Solution_readability":12.1,
        "Solution_reading_time":28.75,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18,
        "Solution_word_count":241,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1619174589310,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bangalore, Karnataka, India",
        "Answerer_reputation_count":1200.0,
        "Answerer_view_count":804.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have a Vertex AI model deployed on an endpoint and want to do some prediction from my app in Golang.<\/p>\n<p>To do this I create code inspired by this example : <a href=\"https:\/\/cloud.google.com\/go\/docs\/reference\/cloud.google.com\/go\/aiplatform\/latest\/apiv1?hl=en\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/go\/docs\/reference\/cloud.google.com\/go\/aiplatform\/latest\/apiv1?hl=en<\/a><\/p>\n<pre><code>const file = &quot;MY_BASE64_IMAGE&quot;\n\nfunc main() {\n\n    ctx := context.Background()\n\n    c, err := aiplatform.NewPredictionClient(cox)\n    if err != nil {\n        log.Printf(&quot;QueryVertex NewPredictionClient - Err:%s&quot;, err)\n    }\n    defer c.Close()\n\n    parameters, err := structpb.NewValue(map[string]interface{}{\n        &quot;confidenceThreshold&quot;: 0.2,\n        &quot;maxPredictions&quot;:      5,\n    })\n    if err != nil {\n        log.Printf(&quot;QueryVertex structpb.NewValue parameters - Err:%s&quot;, err)\n    }\n\n    instance, err := structpb.NewValue(map[string]interface{}{\n        &quot;content&quot;: file,\n    })\n    if err != nil {\n        log.Printf(&quot;QueryVertex structpb.NewValue instance - Err:%s&quot;, err)\n    }\n\n    reqP := &amp;aiplatformpb.PredictRequest{\n        Endpoint:   &quot;projects\/PROJECT_ID\/locations\/LOCATION_ID\/endpoints\/ENDPOINT_ID&quot;,\n        Instances:  []*structpb.Value{instance},\n        Parameters: parameters,\n    }\n\n    resp, err := c.Predict(cox, reqP)\n    if err != nil {\n        log.Printf(&quot;QueryVertex Predict - Err:%s&quot;, err)\n    }\n\n    log.Printf(&quot;QueryVertex Res:%+v&quot;, resp)\n}\n<\/code><\/pre>\n<p>I put the path to my service account JSON file on GOOGLE_APPLICATION_CREDENTIALS environment variable.\nBut when I run my test app I obtain this error message:<\/p>\n<pre><code>QueryVertex Predict - Err:rpc error: code = Unimplemented desc = unexpected HTTP status code received from server: 404 (Not Found); transport: received unexpected content-type &quot;text\/html; charset=UTF-8&quot;\nQueryVertex Res:&lt;nil&gt;\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":3,
        "Challenge_created_time":1651731616723,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1654669807780,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72122744",
        "Challenge_link_count":2,
        "Challenge_participation_count":6,
        "Challenge_readability":15.0,
        "Challenge_reading_time":26.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":null,
        "Challenge_title":"Google Cloud Vertex AI with Golang: rpc error: code = Unimplemented desc = unexpected HTTP status code received from server: 404 (Not Found)",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":455.0,
        "Challenge_word_count":177,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651730962056,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>As @DazWilkin suggested, configure the client option to specify the specific <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest#service-endpoint\" rel=\"noreferrer\">regional endpoint<\/a> with a port 443:<\/p>\n<pre><code>option.WithEndpoint(&quot;&lt;region&gt;-aiplatform.googleapis.com:443&quot;)\n<\/code><\/pre>\n<p>Try like below:<\/p>\n<pre><code>func main() {\n \n   ctx := context.Background()\n   c, err := aiplatform.NewPredictionClient(\n       ctx,\n       option.WithEndpoint(&quot;&lt;region&gt;-aiplatform.googleapis.com:443&quot;),\n   )\n   if err != nil {\n       log.Printf(&quot;QueryVertex NewPredictionClient - Err:%s&quot;, err)\n   }\n   defer c.Close()\n       .\n       .\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1652712010567,
        "Solution_link_count":1,
        "Solution_readability":18.6,
        "Solution_reading_time":8.78,
        "Solution_score_count":5.0,
        "Solution_sentence_count":7,
        "Solution_word_count":43,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to use Azure ML to host an image classification model trained in lobe.ai (externally trained model).     <\/p>\n<p>I've used the 'no code' model deployment approach described <a href=\"https:\/\/learn.microsoft.com\/en-gb\/azure\/machine-learning\/how-to-deploy-no-code-deployment\">here<\/a>     <\/p>\n<p>I've been able to authenticate my workspace and register my TensorFlow model, but the endpoint is stuck on transitioning for over 2 hours.     <\/p>\n<p>Any ideas?    <\/p>\n<pre><code>from azureml.core import Model  \n  \nmodel = Model.register(workspace=ws,  \n                       model_name='cxr',                            # Name of the registered model in your workspace.  \n                       model_path='cxr_test',                       # Local Tensorflow SavedModel folder to upload and register as a model.  \n                       model_framework=Model.Framework.TENSORFLOW,  # Framework used to create the model.  \n                       model_framework_version='1.15.3',            # Version of Tensorflow used to create the model.  \n                       description='Pneumonia-prediction model')  \n  \nservice_name = 'tensorflow-cxr-service'  \nservice = Model.deploy(ws, service_name, [model])  \n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":5,
        "Challenge_created_time":1604939373570,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/156439\/endpoint-stuck-in-transitioning-state",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":11.2,
        "Challenge_reading_time":13.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Endpoint stuck in 'transitioning' state",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":114,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>We have created a support ticket for this issue and we will update the solution later. Thanks.  <\/p>\n<p>Regards,  <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":6.2,
        "Solution_reading_time":1.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2,
        "Solution_word_count":21,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to invoke a MultiModel Endpoint with a RandomCutForest Model. I receive error though, 'Error loading model'. I can invoke the endpoint with models given from the examples.\nAm I missing something e.g. limitations on what models I can use? <\/p>\n\n<p>For MultiModel inspiration I am using below:<\/p>\n\n<blockquote>\n  <p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb<\/a><\/p>\n  \n  <p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/<\/a><\/p>\n<\/blockquote>\n\n<p>I am trying to deploy the outputted 'model.tar.gz' from below RCF example in the MultiModel endpoint:<\/p>\n\n<blockquote>\n  <p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb<\/a><\/p>\n<\/blockquote>\n\n<pre><code>model_name = 'model'\nfull_model_name = '{}.tar.gz'.format(model_name)\nfeatures = data\n\nbody = ','.join(map(str, features)) + '\\n'\nresponse = runtime_sm_client.invoke_endpoint(\n                    EndpointName=endpoint_name,\n                    ContentType='text\/csv',\n                    TargetModel=full_model_name,\n                    Body=body)\nprint(response)\n<\/code><\/pre>\n\n<p><strong>Cloudwatch log Error:<\/strong><\/p>\n\n<pre><code>&gt; 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Error loading model: Unable\n&gt; to load model: invalid load key, '{'. [17:28:59]\n&gt; \/workspace\/src\/learner.cc:334: Check failed: fi-&gt;Read(&amp;mparam_,\n&gt; sizeof(mparam_)) == sizeof(mparam_) (25 vs. 136) : BoostLearner: wrong\n&gt; model format 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Stack trace: 2020-04-27\n&gt; 17:28:59,005 [INFO ] W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (0)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x24)\n&gt; [0x7f37ce1cacb4] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9 com.amazonaws.ml.mms.wlm.WorkerThread\n&gt; - Backend response time: 0 2020-04-27 17:28:59,005 [INFO ] W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (1)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(xgboost::LearnerImpl::Load(dmlc::Stream*)+0x4b5)\n&gt; [0x7f37ce266985] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (2)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(XGBoosterLoadModel+0x37)\n&gt; [0x7f37ce1bf417] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (3)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c)\n&gt; [0x7f37ee993ec0] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (4)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d)\n&gt; [0x7f37ee99387d] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (5)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce)\n&gt; [0x7f37eeba91de] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (6)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12c14)\n&gt; [0x7f37eeba9c14] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (7)\n&gt; \/miniconda3\/bin\/python(_PyObject_FastCallKeywords+0x48b)\n&gt; [0x563d71b4218b] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (8)\n&gt; \/miniconda3\/bin\/python(_PyEval_EvalFrameDefault+0x52cf)\n&gt; [0x563d71b91e8f] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  2020-04-27 17:28:59,005\n&gt; [WARN ] W-9003-b39b888fb4a3fa6cf83bb34a9\n&gt; com.amazonaws.ml.mms.wlm.WorkerThread - Backend worker thread\n&gt; exception. java.lang.IllegalArgumentException: reasonPhrase contains\n&gt; one of the following prohibited characters: \\r\\n: Unable to load\n&gt; model: Unable to load model: invalid load key, '{'. [17:28:59]\n&gt; \/workspace\/src\/learner.cc:334: Check failed: fi-&gt;Read(&amp;mparam_,\n&gt; sizeof(mparam_)) == sizeof(mparam_) (25 vs. 136) : BoostLearner: wrong\n&gt; model format Stack trace:   [bt] (0)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x24)\n&gt; [0x7f37ce1cacb4]   [bt] (1)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(xgboost::LearnerImpl::Load(dmlc::Stream*)+0x4b5)\n&gt; [0x7f37ce266985]   [bt] (2)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(XGBoosterLoadModel+0x37)\n&gt; [0x7f37ce1bf417]   [bt] (3)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c)\n&gt; [0x7f37ee993ec0]   [bt] (4)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d)\n&gt; [0x7f37ee99387d]   [bt] (5)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce)\n&gt; [0x7f37eeba91de]   [bt] (6)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12c14)\n&gt; [0x7f37eeba9c14]   [bt] (7)\n&gt; \/miniconda3\/bin\/python(_PyObject_FastCallKeywords+0x48b)\n&gt; [0x563d71b4218b]   [bt] (8)\n&gt; \/miniconda3\/bin\/python(_PyEval_EvalFrameDefault+0x52cf)\n&gt; [0x563d71b91e8f]\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1588009185817,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61464960",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":22.8,
        "Challenge_reading_time":86.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":52,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker multimodel and RandomCutForest",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":324.0,
        "Challenge_word_count":394,
        "Platform":"Stack Overflow",
        "Poster_created_time":1426492930156,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":398.0,
        "Poster_view_count":84.0,
        "Solution_body":"<p>SageMaker Random Cut Forest is part of the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">built-in algorithm library<\/a> and cannot be deployed in multi-model endpoint (MME). Built-in algorithms currently cannot be deployed to MME. XGboost is an exception, since it has an open-source container <a href=\"https:\/\/github.com\/aws\/sagemaker-xgboost-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-xgboost-container<\/a>.<\/p>\n\n<p>If you really need to deploy a RCF to a multi-model endpoint, one option is to find a reasonably similar open-source implementation (for example <a href=\"https:\/\/github.com\/kLabUM\/rrcf\" rel=\"nofollow noreferrer\"><code>rrcf<\/code><\/a> looks reasonably serious: based <a href=\"http:\/\/proceedings.mlr.press\/v48\/guha16.pdf\" rel=\"nofollow noreferrer\">on the same paper from Guha et al<\/a> and with 170+ github stars) and create a custom MME docker container. The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html\" rel=\"nofollow noreferrer\">documentation is here<\/a> and there is <a href=\"https:\/\/github.com\/giuseppeporcelli\/sagemaker-custom-serving-containers\/blob\/master\/multi-model-server-container\/notebook\/multi-model-server-container.ipynb\" rel=\"nofollow noreferrer\">an excellent tuto here<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7,
        "Solution_readability":19.2,
        "Solution_reading_time":17.98,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11,
        "Solution_word_count":116,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1623879163643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":224.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>i am trying to understand how input_fn, predict_fn and outout_fn work? I am able to understand what they are, but I am not able to understand  how they are called (invoked), can anyone help me understand the same<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1660714390293,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73383302",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":3.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"how does input_fn, predict_fn and output_fn work in aws sagemaker script mode?",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":39.0,
        "Challenge_word_count":49,
        "Platform":"Stack Overflow",
        "Poster_created_time":1573543021663,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":17.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>When your endpoint comes up, <code>model_fn<\/code> is invoked so that your model is loaded. When you invoke the endpoint, <code>input_fn<\/code> is called so that your input payload is parsed, immediately after that, <code>predict_fn<\/code> is called so that a prediction is generated, and then <code>output_fn<\/code> is called to parse the prediction before returning it to the caller.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":14.2,
        "Solution_reading_time":4.95,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2,
        "Solution_word_count":56,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1454844135036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"T\u00fcrkiye",
        "Answerer_reputation_count":462.0,
        "Answerer_view_count":83.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created an endpoint in <code>aws sagemaker<\/code> and it works well, I created a <code>lambda<\/code> function(<code>python3.6<\/code>) that takes files from <code>S3<\/code>, invoke the endpoint and then put the output in a file in <code>S3<\/code>. <\/p>\n\n<p>I wonder if I can create the endpoint at every event(a file uploaded in an <code>s3 bucket)<\/code> and then delete the endpoint <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1550832424633,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1550840459832,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54825390",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.5,
        "Challenge_reading_time":5.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"create aws sagemker endpoint with lambda function",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":180.0,
        "Challenge_word_count":64,
        "Platform":"Stack Overflow",
        "Poster_created_time":1502010899808,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tunis, Tunisia",
        "Poster_reputation_count":109.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>Yes you can Using <code>S3<\/code> event notification for object-created and call a <code>lambda<\/code> for creating endpoint for <code>sagemaker<\/code>.<\/p>\n\n<p>This example shows how to make <code>object-created event trigger lambda<\/code><\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-s3.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-s3.html<\/a><\/p>\n\n<p>You can use <code>python sdk<\/code> to create endpoint for <code>sagemaker<\/code><\/p>\n\n<p><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_endpoint\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_endpoint<\/a><\/p>\n\n<p>But it might be slow for creating endpoint so you may be need to wait.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1550837083230,
        "Solution_link_count":4,
        "Solution_readability":23.8,
        "Solution_reading_time":11.88,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6,
        "Solution_word_count":61,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1546969667040,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":1689.0,
        "Answerer_view_count":170.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm just testing out AWS Sagemaker notebook and created an endpoint using a partial script below:<\/p>\n\n<pre><code>endpoint_name = 'engine' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nendpoint_config_name = 'engine_config' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nmodel_name = 'engine_model' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n\nwhile status=='Creating':\n    time.sleep(60)\n    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n    status = resp['EndpointStatus']\n    print(\"Status: \" + status)\n\n<\/code><\/pre>\n\n<p>I'm trying to remove that endpoint by using:\nsm_client.delete_endpoint(EndpointName=endpoint_name)<\/p>\n\n<p>However, it didn't work because I naively used timestamps for the endpoint_name and I didn't remember them. The original variable values were overriden when I re-run the code. As a result, I can't delete the existing endpoint.\nI went to the Sagemaker management dashboard --> inference --> endpoints, but it's empty. I don't even know if I'm currently having any active endpoints or not. Please advise how to delete my endpoint in this case. Thank you in advance.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1574199115877,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1574275275427,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58943117",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.4,
        "Challenge_reading_time":14.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Confirming endpoints were deleted in SageMaker notebook",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":661.0,
        "Challenge_word_count":129,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521994074812,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":338.0,
        "Poster_view_count":114.0,
        "Solution_body":"<p><strong>If there are no endpoints active under the \"Endpoints\" tab in the SageMaker service console, then you will not be incurring any charges for inference or endpoint infrastructure.<\/strong><\/p>\n\n<p>If this is the case, your Endpoints tab should look like the following:\n<a href=\"https:\/\/i.stack.imgur.com\/5QvEn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/5QvEn.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Endpoint Configurations<\/strong>, on the other hand, involve the metadata necessary for an endpoint deployment. This is just the metadata, and are stored (without cost) in your account, visible in the console under the \"Endpoint Configurations\" tab. You do not need to remove these configurations when tearing down an endpoint.<\/p>\n\n<p><strong>Important note:<\/strong> Double check that you are checking in the console for the <em>region you would have deployed to<\/em>. For example, if you ran the notebook and deployed an endpoint in <code>us-east-1<\/code>, but check the SageMaker console for <code>us-west-2<\/code>, it would not be displaying endpoints from the other region.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":11.7,
        "Solution_reading_time":14.48,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8,
        "Solution_word_count":148,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1474169038352,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":66.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Recently, while using a known-good endpoint configuration, I keep on receiving a: \"The model data archive is too large. Please reduce the size of the model data archive or move to an instance type with more memory.\" error. The tar.gz file is 7.7G but is not loaded in memory (only a small part of it is). I am wondering if anything changes recently that may be causing this issue. <\/p>\n\n<p>Thanks for any insights<\/p>\n\n<p>Emmanuel<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1561395436830,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56740984",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":6.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker custom model : \"The model data archive is too large\" error when creation endpoint",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":444.0,
        "Challenge_word_count":88,
        "Platform":"Stack Overflow",
        "Poster_created_time":1315924476963,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lyon, France",
        "Poster_reputation_count":4510.0,
        "Poster_view_count":285.0,
        "Solution_body":"<p>When an Endpoint is created with SageMaker, the model data artifact is downloaded and uncompressed onto the associated disk\/EBS volume on the instance. This volume size is proportional[1] to the instance type that you chose. <\/p>\n\n<p>Please make sure that the instance type that you picked has enough disk space to accommodate the uncompressed .tar.gz file. (It does not matter if it is fully or partially loaded onto memory later on, it has to fit in the disk uncompressed).<\/p>\n\n<p>[1] Volume size for instance types - <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/host-instance-storage.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/host-instance-storage.html<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":12.6,
        "Solution_reading_time":9.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6,
        "Solution_word_count":89,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained and deployed a model in Pytorch with Sagemaker. I am able to call the endpoint and get a prediction. I am using the default input_fn() function (i.e. not defined in my serve.py).<\/p>\n\n<pre><code>model = PyTorchModel(model_data=trained_model_location,\n                     role=role,\n                     framework_version='1.0.0',\n                     entry_point='serve.py',\n                     source_dir='source')\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>A prediction can be made as follows:<\/p>\n\n<pre><code>input =\"0.12787057,  1.0612601,  -1.1081504\"\npredictor.predict(np.genfromtxt(StringIO(input), delimiter=\",\").reshape(1,3) )\n<\/code><\/pre>\n\n<p>I want to be able to serve the model with REST API and am HTTP POST using lambda and API gateway. I was able to use invoke_endpoint() for this with an XGBOOST model in Sagemaker this way. I am not sure what to send into the body for Pytorch.<\/p>\n\n<pre><code>client = boto3.client('sagemaker-runtime')\nresponse = client.invoke_endpoint(EndpointName=ENDPOINT  ,\nContentType='text\/csv',\nBody=???)\n<\/code><\/pre>\n\n<p>I believe I need to understand how to write the customer input_fn to accept and process the type of data I am able to send through invoke_client. Am I on the right track and if so, how could the input_fn be written to accept a csv from invoke_endpoint?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559765963530,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56467434",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.5,
        "Challenge_reading_time":17.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"Making a Prediction Sagemaker Pytorch",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":2346.0,
        "Challenge_word_count":167,
        "Platform":"Stack Overflow",
        "Poster_created_time":1294628108596,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1748.0,
        "Poster_view_count":393.0,
        "Solution_body":"<p>Yes you are on the right track. You can send csv-serialized input to the endpoint without using the <code>predictor<\/code> from the SageMaker SDK, and using other SDKs such as <code>boto3<\/code> which is installed in lambda:<\/p>\n\n<pre><code>import boto3\nruntime = boto3.client('sagemaker-runtime')\n\npayload = '0.12787057,  1.0612601,  -1.1081504'\n\nresponse = runtime.invoke_endpoint(\n    EndpointName=ENDPOINT_NAME,\n    ContentType='text\/csv',\n    Body=payload.encode('utf-8'))\n\nresult = json.loads(response['Body'].read().decode()) \n<\/code><\/pre>\n\n<p>This will pass to the endpoint a csv-formatted input, that you may need to reshape back in the <code>input_fn<\/code> to put in the appropriate dimension expected by the model.<\/p>\n\n<p>for example:<\/p>\n\n<pre><code>def input_fn(request_body, request_content_type):\n    if request_content_type == 'text\/csv':\n        return torch.from_numpy(\n            np.genfromtxt(StringIO(request_body), delimiter=',').reshape(1,3))\n<\/code><\/pre>\n\n<p><strong>Note<\/strong>: I wasn't able to test the specific <code>input_fn<\/code> above with your input content and shape but I used the approach on Sklearn RandomForest couple times, and looking at the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_pytorch.html#model-serving\" rel=\"nofollow noreferrer\">Pytorch SageMaker serving doc<\/a> the above rationale should work.<\/p>\n\n<p>Don't hesitate to use endpoint logs in Cloudwatch to diagnose any inference error (available from the endpoint UI in the console), those logs are usually <strong>much more verbose<\/strong> that the high-level logs returned by the inference SDKs<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":12.9,
        "Solution_reading_time":20.61,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11,
        "Solution_word_count":172,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1495715386880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":51.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to send a request on a model on sagemaker using .NET. The code I am using is: <\/p>\n\n<pre><code>var data = File.ReadAllBytes(@\"C:\\path\\file.csv\");\nvar credentials = new Amazon.Runtime.BasicAWSCredentials(\"\",\"\");\nvar awsClient = new AmazonSageMakerRuntimeClient(credentials, RegionEndpoint.EUCentral1);\nvar request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest\n{\n    EndpointName = \"EndpointName\",\n    ContentType = \"text\/csv\",\n    Body = new MemoryStream(data),\n};\n\nvar response = awsClient.InvokeEndpoint(request);\nvar predictions = Encoding.UTF8.GetString(response.Body.ToArray());\n<\/code><\/pre>\n\n<p>the error that I am getting on <code>awsClient.InvokeEndpoint(request)<\/code><\/p>\n\n<p>is:<\/p>\n\n<blockquote>\n  <p>Amazon.SageMakerRuntime.Model.ModelErrorException: 'The service\n  returned an error with Error Code ModelError and HTTP Body:\n  {\"ErrorCode\":\"INTERNAL_FAILURE_FROM_MODEL\",\"LogStreamArn\":\"arn:aws:logs:eu-central-1:xxxxxxxx:log-group:\/aws\/sagemaker\/Endpoints\/myEndpoint\",\"Message\":\"Received\n  server error (500) from model with message \\\"\\\". See\n  \"https:\/\/ url_to_logs_on_amazon\"\n  in account xxxxxxxxxxx for more\n  information.\",\"OriginalMessage\":\"\",\"OriginalStatusCode\":500}'<\/p>\n<\/blockquote>\n\n<p>the url that the error message suggests for more information does not help at all.<\/p>\n\n<p>I believe that it is a data format issue but I was not able to find a solution.<\/p>\n\n<p>Does anyone has encountered this behavior before?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1534946906973,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51968742",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":15.1,
        "Challenge_reading_time":19.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS sagemaker invokeEndpoint model internal error",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":4385.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1495715386880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":51.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>The problem relied on the data format as suspected. In my case all I had to do is send the data as a json serialized string array and use <code>ContentType = application\/json<\/code> because the python function running on the endpoint which is responsible for sending the data to the predictor was only accepting json strings. <\/p>\n\n<p>Another way to solve this issues is to modify the python function which is responsible for the input handling to accept all content types and modify the data in a way that the predictor will understand.<\/p>\n\n<p>example of working code for my case:<\/p>\n\n<pre><code>        var data = new string[] { \"this movie was extremely good .\", \"the plot was very boring .\" };\n        var serializedData = JsonConvert.SerializeObject(data);\n\n        var credentials = new Amazon.Runtime.BasicAWSCredentials(\"\",\"\");\n        var awsClient = new AmazonSageMakerRuntimeClient(credentials, RegionEndpoint.EUCentral1);\n        var request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest\n        {\n            EndpointName = \"endpoint\",\n            ContentType = \"application\/json\",\n            Body = new MemoryStream(Encoding.ASCII.GetBytes(serializedData)),\n        };\n\n        var response = awsClient.InvokeEndpoint(request);\n        var predictions = Encoding.UTF8.GetString(response.Body.ToArray());\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":13.8,
        "Solution_reading_time":15.84,
        "Solution_score_count":2.0,
        "Solution_sentence_count":12,
        "Solution_word_count":143,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,    <\/p>\n<p>We are sending data from IoT Central to Event Hubs and then to Data Explorer, with the hopes of then sending the data to Azure Machine Learning.    <\/p>\n<p>In order to send data from Event Hubs to Data Explorer it needs a data ingestion into a table on data explorer.    <\/p>\n<p>For this data ingestion, it needs a json mapping.    <\/p>\n<p>We could ingest the data, but the message from the iot central data goes to event hubs that goes to data explorer carries the telemetry data as a dynamic type (a json inside a json).     <\/p>\n<pre><code>(&quot;telemetry&quot;:{&quot;Temp:&quot;37&quot;,&quot;Vol&quot;:&quot;97&quot;})  \n<\/code><\/pre>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/90018-data.jpg?platform=QnA\" alt=\"90018-data.jpg\" \/>    <br \/>\nWe want to separate the telemetry data in different columns.    <\/p>\n<p>So Temp will have one column and Vol another.    <\/p>\n<p>I am wondering how that can be done?    <\/p>\n<p>And additionally, since we would like to send the data to ML, can data explorer be used as a datastore in ML?    <\/p>\n<p>Thanks!!    <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619036349177,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/366532\/separate-data-in-data-explorer-and-use-as-datastor",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":14.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Separate data in data explorer and use as datastore",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":168,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=a706cc9a-ba35-4065-a95e-7fd5a2c7ba9d\">@yjay  <\/a>,    <\/p>\n<p>You can use <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/data-explorer\/kusto\/query\/parseoperator\">parse operator<\/a> - Evaluates a string expression and parses its value into one or more calculated columns. The calculated columns will have nulls, for unsuccessfully parsed strings.     <\/p>\n<p>For more details, refer <a href=\"https:\/\/stackoverflow.com\/questions\/63779632\/split-column-string-with-delimiters-into-separate-columns-in-azure-kusto\">SO<\/a> thread addressing similar issue.     <\/p>\n<blockquote>\n<p>Unfortuantely, Azure Data Explorer is not a supported storage solution with Azure Machine Learning.     <\/p>\n<\/blockquote>\n<p>Datastores currently support storing connection information to the storage services listed in the following matrix.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/90159-image.png?platform=QnA\" alt=\"90159-image.png\" \/>    <\/p>\n<p>For unsupported storage solutions, and to save data egress cost during ML experiments, move your data to a supported Azure storage solution.    <\/p>\n<p><strong>Reference:<\/strong> <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data\">Connect to storage services on Azure - Azure Machine Learning<\/a>.     <\/p>\n<p>Hope this helps. Do let us know if you any further queries.    <\/p>\n<p>------------    <\/p>\n<p>Please don\u2019t forget to <code>Accept Answer<\/code> and <code>Up-Vote<\/code> wherever the information provided helps you, this can be beneficial to other community members.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_readability":14.5,
        "Solution_reading_time":20.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":15,
        "Solution_word_count":157,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I have loaded <a href=\"https:\/\/automlsamplenotebookdata.blob.core.windows.net\/automl-sample-notebook-data\/bankmarketing_train.csv\">bankmarketing_train.csv<\/a> to get a dataset and auto generated a model to predict &quot;y&quot; field value with AutoML.    <br \/>\nVoting Ensemble model was generated as the best model and tested its behavior after deployed to the endpoint.    <\/p>\n<p>Schema is generated like this for the endpoint.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/113929-schema-2021-07-13-124905.png?platform=QnA\" alt=\"113929-schema-2021-07-13-124905.png\" \/>    <\/p>\n<p>Tried with the endpoint test feature in ML Studio. It worked and responded an expected output (left side in the fig below).    <br \/>\nBut my python REST call fails with 502 Bad Gateway(right side)    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/114082-screenshot-2021-07-12-232443.png?platform=QnA\" alt=\"114082-screenshot-2021-07-12-232443.png\" \/>    <\/p>\n<p>Using the REST plug-in for VSCode, I have requested as below. This also failed with the same response status code.    <\/p>\n<pre><code>POST http:\/\/d8e9f6ad-4112-4417-97c0-01b4246b284a.japaneast.azurecontainer.io\/score  \nContent-Type: application\/json  \nAuthorization: Bearer === My correct key here ===  \n  \n{&quot;data&quot;: [{&quot;age&quot;: 87, &quot;campaign&quot;: 1, &quot;cons.conf.idx&quot;: -46.2, &quot;cons.price.idx&quot;: 92.893, &quot;contact&quot;: &quot;cellular&quot;, &quot;day_of_week&quot;: &quot;mon&quot;, &quot;default&quot;: &quot;no&quot;, &quot;duration&quot;: 471, &quot;education&quot;: &quot;university.degree&quot;, &quot;emp.var.rate&quot;: -1.8, &quot;euribor3m&quot;: 1.299, &quot;housing&quot;: &quot;yes&quot;, &quot;job&quot;: &quot;blue-collar&quot;, &quot;loan&quot;: &quot;yes&quot;, &quot;marital&quot;: &quot;married&quot;, &quot;month&quot;: &quot;may&quot;, &quot;nr.employed&quot;: 5099.1, &quot;pdays&quot;: 999, &quot;poutcome&quot;: &quot;failure&quot;, &quot;previous&quot;: 1}]}  \n<\/code><\/pre>\n<p>Investigated in the App Insight and queried the exceptions.    <br \/>\nI found this end point tries to convert 'yes' to int value. Of course it fails.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/114083-%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2021-07-13-003243.png?platform=QnA\" alt=\"114083-%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2021-07-13-003243.png\" \/>    <\/p>\n<p>The value 'yes' is set to 'loan' and 'housing&quot;. Both are defined string value in the swagger.json for this endpoint.    <\/p>\n<p>What do you think?    <br \/>\nAm I missing something?    <br \/>\nIs this a bug with the endpoint?    <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":3,
        "Challenge_created_time":1626149341823,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/473223\/endpoint-fails-with-the-model-generated-by-automat",
        "Challenge_link_count":5,
        "Challenge_participation_count":7,
        "Challenge_readability":11.7,
        "Challenge_reading_time":36.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":null,
        "Challenge_title":"Endpoint fails with the model generated by Automated ML",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":244,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Yes, I tried that. Following is the code coming from the consume, the values are set accordingly.    <\/p>\n<pre><code>import urllib.request  \nimport json  \nimport os  \nimport ssl  \n  \ndef allowSelfSignedHttps(allowed):  \n    # bypass the server certificate verification on client side  \n    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):  \n        ssl._create_default_https_context = ssl._create_unverified_context  \n  \nallowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.  \n  \n# Request data goes here  \n  \ndata = {&quot;data&quot;:  \n        [  \n          {  \n            &quot;age&quot;: &quot;17&quot;,  \n            &quot;campaign&quot;: &quot;1&quot;,  \n            &quot;cons.conf.idx&quot;: &quot;-46.2&quot;,  \n            &quot;cons.price.idx&quot;: &quot;92.893&quot;,  \n            &quot;contact&quot;: &quot;cellular&quot;,  \n            &quot;day_of_week&quot;: &quot;mon&quot;,  \n            &quot;default&quot;: &quot;no&quot;,  \n            &quot;duration&quot;: &quot;971&quot;,  \n            &quot;education&quot;: &quot;university.degree&quot;,  \n            &quot;emp.var.rate&quot;: &quot;-1.8&quot;,  \n            &quot;euribor3m&quot;: &quot;1.299&quot;,  \n            &quot;housing&quot;: &quot;yes&quot;,  \n            &quot;job&quot;: &quot;blue-collar&quot;,  \n            &quot;loan&quot;: &quot;yes&quot;,  \n            &quot;marital&quot;: &quot;married&quot;,  \n            &quot;month&quot;: &quot;may&quot;,  \n            &quot;nr.employed&quot;: &quot;5099.1&quot;,  \n            &quot;pdays&quot;: &quot;999&quot;,  \n            &quot;poutcome&quot;: &quot;failure&quot;,  \n            &quot;previous&quot;: &quot;1&quot;  \n          }  \n      ]  \n    }  \n  \n  \nbody = str.encode(json.dumps(data))  \n  \nurl = 'http:\/\/d8e9f6ad-4112-4417-97c0-01b4246b284a.japaneast.azurecontainer.io\/score'  \napi_key = '&lt;key&gt;' # Replace this with the API key for the web service  \nheaders = {'Content-Type':'application\/json', 'Authorization':('Bearer '+ api_key)}  \n  \nreq = urllib.request.Request(url, body, headers)  \n  \ntry:  \n    response = urllib.request.urlopen(req)  \n  \n    result = response.read()  \n    print(result)  \nexcept urllib.error.HTTPError as error:  \n    print(&quot;The request failed with status code: &quot; + str(error.code))  \n  \n    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure  \n    print(error.info())  \n    print(json.loads(error.read().decode(&quot;utf8&quot;, 'ignore')))  \n  \n<\/code><\/pre>\n<p>The result was the same. 'yes' was tried to cast to int and failed.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/114222-consume-2021-07-13-175924.png?platform=QnA\" alt=\"114222-consume-2021-07-13-175924.png\" \/>    <\/p>\n<p>In the deployment log, following exception observed. Something is happening inside the server call, which I cannot see.    <\/p>\n<pre><code>2021-07-13 08:56:49,684 | root | ERROR | Encountered Exception: Traceback (most recent call last):  \n  File &quot;\/var\/azureml-server\/synchronous\/routes.py&quot;, line 64, in run_scoring  \n    response = invoke_user_with_timer(service_input, request_headers)  \n  File &quot;\/var\/azureml-server\/synchronous\/routes.py&quot;, line 97, in invoke_user_with_timer  \n    result = user_main.run(**params)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/wrapt\/wrappers.py&quot;, line 567, in __call__  \n    args, kwargs)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/inference_schema\/schema_decorators.py&quot;, line 57, in decorator_input  \n    kwargs[param_name] = _deserialize_input_argument(kwargs[param_name], param_type, param_name)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/inference_schema\/schema_decorators.py&quot;, line 285, in _deserialize_input_argument  \n    input_data = param_type.deserialize_input(input_data)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/inference_schema\/parameter_types\/pandas_parameter_type.py&quot;, line 79, in deserialize_input  \n    data_frame = data_frame.astype(dtype=converted_types)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/pandas\/core\/generic.py&quot;, line 5865, in astype  \n    dtype=dtype[col_name], copy=copy, errors=errors, **kwargs  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/pandas\/core\/generic.py&quot;, line 5882, in astype  \n    dtype=dtype, copy=copy, errors=errors, **kwargs  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/pandas\/core\/internals\/managers.py&quot;, line 581, in astype  \n    return self.apply(&quot;astype&quot;, dtype=dtype, **kwargs)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/pandas\/core\/internals\/managers.py&quot;, line 438, in apply  \n    applied = getattr(b, f)(**kwargs)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/pandas\/core\/internals\/blocks.py&quot;, line 559, in astype  \n    return self._astype(dtype, copy=copy, errors=errors, values=values, **kwargs)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/pandas\/core\/internals\/blocks.py&quot;, line 643, in _astype  \n    values = astype_nansafe(vals1d, dtype, copy=True, **kwargs)  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/pandas\/core\/dtypes\/cast.py&quot;, line 707, in astype_nansafe  \n    return lib.astype_intsafe(arr.ravel(), dtype).reshape(arr.shape)  \n  File &quot;pandas\/_libs\/lib.pyx&quot;, line 547, in pandas._libs.lib.astype_intsafe  \nValueError: invalid literal for int() with base 10: 'yes'  \n  \nDuring handling of the above exception, another exception occurred:  \n  \nTraceback (most recent call last):  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/flask\/app.py&quot;, line 1832, in full_dispatch_request  \n    rv = self.dispatch_request()  \n  File &quot;\/azureml-envs\/azureml_429e58b1641c78c2352efc8ad21c49d9\/lib\/python3.6\/site-packages\/flask\/app.py&quot;, line 1818, in dispatch_request  \n    return self.view_functions[rule.endpoint](**req.view_args)  \n  File &quot;\/var\/azureml-server\/synchronous\/routes.py&quot;, line 43, in score_realtime  \n    return run_scoring(service_input, request.headers, request.environ.get('REQUEST_ID', '00000000-0000-0000-0000-000000000000'))  \n  File &quot;\/var\/azureml-server\/synchronous\/routes.py&quot;, line 77, in run_scoring  \n    raise RunFunctionException(str(exc))  \nrun_function_exception.RunFunctionException  \n<\/code><\/pre>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":18.1,
        "Solution_reading_time":86.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":52,
        "Solution_word_count":408,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":15.7482980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Our use case is as follows:\nWe have multiple custom trained models (in the hundreds, and the number increases as we allow the user of our application to create models through the UI, which we then train and deploy on the fly) and so deploying each model to a separate endpoint is expensive as Vertex AI <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/deploy-model-console#custom-trained\" rel=\"nofollow noreferrer\">charges per node used<\/a>. Based on the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/deployment#models-endpoint\" rel=\"nofollow noreferrer\">documentation<\/a> it seems that we can deploy models of different types to the same endpoint but I am not sure how that would work. Let's say I have 2 different custom trained models deployed using custom containers for prediction to the same endpoint. Also, say I specify the traffic split to be 50% for the two models. Now, how do I send a request to a specific model? Using the python SDK, we make calls to the endpoint, like so:<\/p>\n<pre><code>from google.cloud import aiplatform\nendpoint = aiplatform.Endpoint(endpoint_id)\nprediction = endpoint.predict(instances=instances)\n\n# where endpoint_id is the id of the endpoint and instances are the observations for which a prediction is required\n<\/code><\/pre>\n<p>My understanding is that in this scenario, vertex AI will route some calls to one model and some to the other based on the traffic split. I could use the parameters field, as specified in the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#prediction\" rel=\"nofollow noreferrer\">docs<\/a>, to specify the model and then process the request accordingly in the custom prediction container, but still some calls will end up going to a model which it will not be able to process (because Vertex AI is not going to be sending all requests to all models, otherwise the traffic split wouldn't make sense). How do I then deploy multiple models to the same endpoint and make sure that every prediction request is guaranteed to be served?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":9,
        "Challenge_created_time":1636348555970,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69878915",
        "Challenge_link_count":3,
        "Challenge_participation_count":10,
        "Challenge_readability":12.3,
        "Challenge_reading_time":26.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"Deploying multiple models to same endpoint in Vertex AI",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1271.0,
        "Challenge_word_count":303,
        "Platform":"Stack Overflow",
        "Poster_created_time":1471292986790,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":700.0,
        "Poster_view_count":90.0,
        "Solution_body":"<p>This <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/deployment#models-endpoint\" rel=\"nofollow noreferrer\">documentation<\/a> talks about a use case where 2 models are trained on the same feature set and are sharing the ingress prediction traffic. As you have understood correctly, this does not apply to models that have been trained on different feature sets, that is, different models.<\/p>\n<p>Unfortunately, deploying different models to the same endpoint utilizing only one node is not possible in Vertex AI at the moment. There is an ongoing feature request that is being worked on. However, we cannot provide an exact ETA on when that feature will be available.<\/p>\n<p>I reproduced the multi-model setup and noticed the below points.<\/p>\n<p><strong>Traffic Splitting<\/strong><\/p>\n<blockquote>\n<p>I deployed 2 different models to the same endpoint and sent predictions to it. I set a 50-50 traffic splitting rule and saw errors that implied requests being sent to the wrong model.<\/p>\n<\/blockquote>\n<p><strong>Cost Optimization<\/strong><\/p>\n<blockquote>\n<p>When multiple models are deployed to the same endpoint, they are deployed to separate, independent nodes. So, you will still be charged for each node used. Also, node autoscaling happens at the model level, not at the endpoint level.<\/p>\n<\/blockquote>\n<p>A plausible workaround would be to pack all your models into a single container and use a custom HTTP server logic to send prediction requests to the appropriate model. This could be achieved using the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#request_requirements\" rel=\"nofollow noreferrer\"><code>parameters<\/code><\/a> field of the prediction request body. The custom logic would look something like this.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@app.post(os.environ['AIP_PREDICT_ROUTE'])\nasync def predict(request: Request):\n    body = await request.json()\n    parameters = body[&quot;parameters&quot;]\n    instances = body[&quot;instances&quot;]\n    inputs = np.asarray(instances)\n    preprocessed_inputs = _preprocessor.preprocess(inputs)\n\n    if(parameters[&quot;model_name&quot;]==&quot;random_forest&quot;):\n        print(parameters[&quot;model_name&quot;])\n        outputs = _random_forest_model.predict(preprocessed_inputs)\n    else:\n        print(parameters[&quot;model_name&quot;])\n        outputs = _decision_tree_model.predict(inputs)\n\n    return {&quot;predictions&quot;: [_class_names[class_num] for class_num in outputs]}\n<\/code><\/pre>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1636405249843,
        "Solution_link_count":2,
        "Solution_readability":12.7,
        "Solution_reading_time":32.24,
        "Solution_score_count":1.0,
        "Solution_sentence_count":23,
        "Solution_word_count":273,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Status:<\/p>\n<ul>\n<li>Custom container is built using the doc - <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own<\/a><\/li>\n<li>predict.py is coded to accommodate the custom inference script and its working well<\/li>\n<li>Using the classsagemaker.model.Model() class to pass the trained model.tar.gz and custom container image inorder to deploy the model<\/li>\n<\/ul>\n<p>Challenge:<\/p>\n<ul>\n<li>In the same Model class there is a ENV  parameter through which we can apparently send the environment variables to the custom image<\/li>\n<li>Tried passing a python dict to this , but facing difficulty to read this json dict inide the predict.py script<\/li>\n<\/ul>\n<p>Somebody faced the same difficulty ?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":7,
        "Challenge_created_time":1638197351547,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70156631",
        "Challenge_link_count":2,
        "Challenge_participation_count":8,
        "Challenge_readability":15.2,
        "Challenge_reading_time":12.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How to pass additional parameters (as a dict) to sagemeker custom inference container?",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":183.0,
        "Challenge_word_count":108,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604146329127,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>You can pass your environment dict in your Model as:<\/p>\n<pre><code>Model(\n.\n.\nenv= {&quot;my_env&quot;: &quot;my_env_value&quot;}\n.\n.\n)\n<\/code><\/pre>\n<p>SageMaker will pass the enviroments dict to your container and you can access it in your predict.py script for example with:<\/p>\n<pre><code>my_env = os.environ.get('my_env',&quot;env key not set in Model&quot;)\nprint(my_env)\n<\/code><\/pre>\n<p>If your env dict was passed to your Model containing they <code>my_env<\/code> then you will receive the output : <code>my_env_value<\/code>. Else, then you will receive <code>env key not set in Model<\/code><\/p>\n<p>I work for AWS and my opinions are my own.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":8.8,
        "Solution_reading_time":8.33,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6,
        "Solution_word_count":85,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":10,
        "Challenge_body":"Hello,\r\n\r\nWhen running the experiment, the error message **Environment name can not start with the prefix AzureML** was displayed. How can I set the name of the environment? I'm following the GitHub tutorial and haven't found anything about it.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117882725-01767d80-b281-11eb-8df5-36d8683523e7.png)\r\n\r\nCode used:\r\n\r\n- Registering Dataset\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883192-81044c80-b281-11eb-9dec-d73431948061.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883230-8c577800-b281-11eb-8445-060839369fe5.png)\r\n\r\n- Training Pipeline\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883313-a5602900-b281-11eb-818d-3972111d7f9c.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883356-b315ae80-b281-11eb-99d9-1ac6c0989186.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883429-c7f24200-b281-11eb-88de-3979570adb55.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883465-d2144080-b281-11eb-8b9c-f74756bedd01.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883535-e48e7a00-b281-11eb-9d31-e035f44a0871.png)\r\n\r\nReferences:\r\n\r\n- https:\/\/github.com\/microsoft\/solution-accelerator-many-models\/tree\/master\/Automated_ML\/02_AutoML_Training_Pipeline\r\n- https:\/\/github.com\/MicrosoftDocs\/azure-docs\/issues\/65770\r\n\r\nBest regards,\r\nCristina\r\n\r\n\r\n---\r\n#### Detalhes do documento\r\n\r\n\u26a0 *N\u00e3o edite esta se\u00e7\u00e3o. \u00c9 necess\u00e1rio para a vincula\u00e7\u00e3o do problema do docs.microsoft.com \u279f GitHub.*\r\n\r\n* ID: 49399a7d-d4e8-370e-ce62-d60a6b64e412\r\n* Version Independent ID: 782d8ba4-75dd-27c3-5a46-a921c3ead4bf\r\n* Content: [azureml.contrib.automl.pipeline.steps.AutoMLPipelineBuilder class - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/pt-br\/python\/api\/azureml-contrib-automl-pipeline-steps\/azureml.contrib.automl.pipeline.steps.automlpipelinebuilder?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/stable\/docs-ref-autogen\/azureml-contrib-automl-pipeline-steps\/azureml.contrib.automl.pipeline.steps.AutoMLPipelineBuilder.yml](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr.pt-BR\/blob\/live\/AzureML-Docset\/stable\/docs-ref-autogen\/azureml-contrib-automl-pipeline-steps\/azureml.contrib.automl.pipeline.steps.AutoMLPipelineBuilder.yml)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @DebFro\r\n* Microsoft Alias: **debfro**",
        "Challenge_closed_time":1622654,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620766573000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1468",
        "Challenge_link_count":12,
        "Challenge_participation_count":10,
        "Challenge_readability":29.1,
        "Challenge_reading_time":35.07,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"AutoMLPipelineBuilder.get_many_models_train_steps - Error \"Environment name can not start with the prefix AzureML...\"",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":112,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi,\r\n\r\nI have some updates:\r\n\r\n- I put the code below to set the environment.\r\n\r\nfrom azureml.core.environment import Environment\r\n\r\nenv = Environment.get(workspace=ws, name=\"AzureML-Tutorial\")\r\nmyenv = env.clone(\"automl_env\")\r\n\r\ntrain_steps = AutoMLPipelineBuilder.get_many_models_train_steps(experiment=experiment,\r\n                                                                automl_settings=automl_settings,\r\n                                                                train_data=dataset_input,\r\n                                                                compute_target=compute_target,\r\n                                                                partition_column_names=partition_column_names,\r\n                                                                node_count=1,\r\n                                                                process_count_per_node=2,\r\n                                                                run_invocation_timeout=3700,\r\n                                                                train_env=myenv)\r\n\r\n- The environment problem has been resolved, but now the process displays the message **ValueError: None is not in list**. I don't know what this means.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/118014360-82894f80-b329-11eb-8e6a-558d6606d7b1.png)\r\n\r\nFailure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (pyarrow 3.0.0 (\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages), Requirement.parse('pyarrow<2.0.0,>=0.17.0'), {'azureml-dataset-runtime'}).\r\nTraceback (most recent call last):\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/runtime\/_many_models\/train_model.py\", line 212, in <module>\r\n    logs = run(data_file_path, args, automl_settings, current_step_run)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/runtime\/_many_models\/train_model.py\", line 100, in run\r\n    data = pd.read_csv(file_path, parse_dates=[timestamp_column])\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 676, in parser_f\r\n    return _read(filepath_or_buffer, kwds)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 448, in _read\r\n    parser = TextFileReader(fp_or_buf, **kwds)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 880, in __init__\r\n    self._make_engine(self.engine)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 1114, in _make_engine\r\n    self._engine = CParserWrapper(self.f, **self.options)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 1949, in __init__\r\n    self._set_noconvert_columns()\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 2015, in _set_noconvert_columns\r\n    _set(val)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 2005, in _set\r\n    x = names.index(x)\r\nValueError: None is not in list\r\n\r\nBest regards,\r\nCristina @crisansou is there any error surfaced in 70_driver_log? \r\nHi @shbijlan ,\r\n\r\nI deleted the workspace. I tried to reproduce the steps again but I couldn't even create the experiment, below is the error message. Can you tell which is the recommended version to use this solution?\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/119876912-c752e000-befe-11eb-9a18-c8f03afae48c.png)\r\n\r\nI don't know if it's related, but I realized that now compute instance is using version 1.29.\r\n\r\n!pip install --upgrade azureml-sdk[automl]\r\n!pip install azureml-contrib-automl-pipeline-steps\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/119877097-03864080-beff-11eb-8134-07d22a243284.png)\r\n\r\n\r\n\r\n\r\n> @crisansou is there any error surfaced in 70_driver_log?\r\n\r\n from azureml.core. import Environment\r\ntrain_env = Environment.get(workspace = ws, name = 'AzureML-AutoML')\r\n\r\nCan you please pass train_env like above for the workaround? There is a bug in our code that needs to be fixed. we will fix it in our next release.\r\n\r\nAlso this solution only supports 'forecasting' and needs a time_column_name passed in automl settings Hi @deeptim123 ,\r\n\r\nThanks for the instructions! After including the environment I was able to run the cell, but the pipeline ended with an error because I am using a regression model.\r\n\r\nIn the next release, in addition to fixing the bug, will it be possible to use regression?\r\n\r\n> from azureml.core. import Environment\r\n> train_env = Environment.get(workspace = ws, name = 'AzureML-AutoML')\r\n> \r\n> Can you please pass train_env like above for the workaround? There is a bug in our code that needs to be fixed. we will fix it in our next release.\r\n> \r\n> Also this solution only supports 'forecasting' and needs a time_column_name passed in automl settings\r\n\r\n There are currently no plans to support regression. @cartacioS  for visibility of this ask @deeptim123 ,\r\n\r\nThanks for the info. I think it's important to add this functionality for regression and classification as well.\r\n\r\n> There are currently no plans to support regression. @cartacioS for visibility of this ask\r\n\r\n @crisansou - this is currently not on our roadmap, and purposefully unprioritized as 90% of our customer base, especially customers investing in thousands+ models are leveraging only forecasting scenarios. Priorities change from one semester to the next, and it may be supported at a later date but is not in scope right now. If you are or have a direct customer who is blocked by the lack of many model support for regression and classification please contact me at sabina.cartacio@microsoft.com and we can further discuss.\r\n\r\nThanks! Hi @cartacioS ,\r\n\r\nGot it, thanks for the info! The project is starting now, but if it is really necessary to use the multiple models solution for regression I'll send you an email.\r\n\r\n> @crisansou - this is currently not on our roadmap, and purposefully unprioritized as 90% of our customer base, especially customers investing in thousands+ models are leveraging only forecasting scenarios. Priorities change from one semester to the next, and it may be supported at a later date but is not in scope right now. If you are or have a direct customer who is blocked by the lack of many model support for regression and classification please contact me at [sabina.cartacio@microsoft.com](mailto:sabina.cartacio@microsoft.com) and we can further discuss.\r\n> \r\n> Thanks!\r\n\r\n Closing as this is being tracked offline as a feature request for MANY MODELS, by Sabina.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":3,
        "Solution_readability":11.4,
        "Solution_reading_time":82.01,
        "Solution_score_count":null,
        "Solution_sentence_count":57,
        "Solution_word_count":664,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1569423384323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":96.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":8089.9413483334,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Im working on sentence classification using in-build  blazing text algorithm, while invoking endpoint inside lambda function it throughs the content type mismatching error. <\/p>\n\n<p>-- For blazing text it support only application\/jsonlines or application\/json but while invoking , it throughs the error like , it accepts only byte or bytearray<\/p>\n\n<pre><code>input format . application\/json\nevent={\n  \"features\": [\n    \"sensor_subtype Thermostats Thermal Switches product_features Hermetically sealed n Tight tolerances n Tight differentials n Logic level contacts n applications Computers n Medical electronics n Power supplies n Industrial controls n Test equipment n Infotech n description Technical Specifications technical_specs CloseTolerance 2 8 C 5 F DielectricStrength MIL STD 202 Method 301 1250 Vac 60 Hz Terminal to Case ContactResistance MIL STD\"\n  ]\n}\n<\/code><\/pre>\n\n<p>and also i tried application\/jsonlines<\/p>\n\n<p>My code looks like this>>>>>>>>>>>>>>>>>>>>>>>><\/p>\n\n<pre><code>def transform_data(data):\n    try:\n        features = data.copy()\n\n        return features\n\n    except Exception as err:\n        print('Error when transforming: {0},{1}'.format(data,err))\n        raise Exception('Error when transforming: {0},{1}'.format(data,err))\n\n\ndef lambda_handler(event, context):\n    try:    \n        print(\"Received event: \" + json.dumps(event, indent=2))\n\n        request = json.loads(json.dumps(event))\n\n        transformed_data = str(transform_data(request['features'])) #for instance in request['features'])\n        print(ENDPOINT_NAME, \"-------&gt;&gt;&gt;&gt;\")\n        payload=transformed_data\n        result = client.invoke_endpoint(EndpointName=ENDPOINT_NAME, \n                              Body=(payload.encode('utf-8')),\n                              ContentType='application\/json')\n        return result\n<\/code><\/pre>\n\n<pre><code>  \"statusCode\": 400,\n  \"isBase64Encoded\": false,\n  \"body\": \"Call Failed An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (406) from model with message \\\"Invalid payload format\\\".\n_______________LOGS__________________________________\n\uf141\n11:35:22\n[08\/18\/2019 11:35:22 ERROR 140074862942016] Customer Error: Unable to decode payload: Incorrect data format. (caused by ValueError)\n\uf141\n11:35:22\nCaused by: No JSON object could be decoded\n\uf141\n11:35:22\nTraceback (most recent call last): File \"\/opt\/amazon\/lib\/python2.7\/site-packages\/blazingtext\/serve.py\", line 317, in invocations data = json.loads(payload.decode(\"utf-8\")) File \"\/opt\/amazon\/python2.7\/lib\/python2.7\/json\/__init__.py\", line 339, in loads return _default_decoder.decode(s) File \"\/opt\/amazon\/python2.7\/lib\/python2.7\/json\/decoder.py\", line 364, in decode obj, end = self.\n\uf141\n11:35:22\nValueError: No JSON object could be decoded\n<\/code><\/pre>\n\n<p>I need to predict the sentence in realtime using invoke_endpoint option but it shows invalid payload format <\/p>\n\n<p>I tried with byte format and apllication\/jsonlines format.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1566128809927,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1566133069936,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57544237",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":15.8,
        "Challenge_reading_time":37.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"Inside lambda function - Blazing text algorithm invoke endpoint doesn't support the input content type",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":493.0,
        "Challenge_word_count":305,
        "Platform":"Stack Overflow",
        "Poster_created_time":1541220234400,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":13.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I encountered the same problem when trying to predict on text classification with a BlazingText container. What worked for me was simply changing the key in the payload while keeping the ContentType as application\/json:<\/p>\n<pre><code>sentence = &quot;I'm selling my PS4, practically brand new&quot;\n\npayload = {&quot;instances&quot;: [sentence]}\n\nresponse = client.invoke_endpoint(\n        EndpointName=&quot;text_classification&quot;,\n        Body=json.dumps(payload),\n        ContentType='application\/json'\n        \n    )\n<\/code><\/pre>\n<p>After playing around a little with the payload it seems that blazing text models only accept payloads as a dictionary with &quot;instances&quot; as its key and a list containing your data you want to predict on as its value.<\/p>\n<p>To get to your predictions simply :<\/p>\n<pre><code>print(&quot;ResponseMetadata:&quot;, response[&quot;ResponseMetadata&quot;])\nprint()\nprint(&quot;Body:&quot;, response['Body'].read())\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1595256858790,
        "Solution_link_count":0,
        "Solution_readability":16.0,
        "Solution_reading_time":12.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5,
        "Solution_word_count":103,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I've deployed an endpoint in sagemaker and was trying to invoke it through my python program. I had tested it using postman and it worked perfectly ok. Then I wrote the invocation code as follows<\/p>\n\n<pre><code>import boto3\nimport pandas as pd\nimport io\nimport numpy as np\n\ndef np2csv(arr):\n    csv = io.BytesIO()\n    np.savetxt(csv, arr, delimiter=',', fmt='%g')\n    return csv.getvalue().decode().rstrip()\n\n\nruntime= boto3.client('runtime.sagemaker')\npayload = np2csv(test_X)\n\nruntime.invoke_endpoint(\n    EndpointName='&lt;my-endpoint-name&gt;',\n    Body=payload,\n    ContentType='text\/csv',\n    Accept='Accept'\n)\n<\/code><\/pre>\n\n<p>Now whe I run this I get a validation error<\/p>\n\n<pre><code>ValidationError: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint &lt;my-endpoint-name&gt; of account &lt;some-unknown-account-number&gt; not found.\n<\/code><\/pre>\n\n<p>While using postman i had given my access key and secret key but I'm not sure how to pass it when using sagemaker apis. I'm not able to find it in the documentation also. <\/p>\n\n<p>So my question is, how can I use sagemaker api from my local machine to invoke my endpoint?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1516867519790,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1516875209620,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48438202",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.1,
        "Challenge_reading_time":15.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Errors while using sagemaker api to invoke endpoints",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":4467.0,
        "Challenge_word_count":155,
        "Platform":"Stack Overflow",
        "Poster_created_time":1410972175307,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1124.0,
        "Poster_view_count":153.0,
        "Solution_body":"<p>When you are using any of the AWS SDK (including the one for Amazon SageMaker), you need to configure the credentials of your AWS account on the machine that you are using to run your code. If you are using your local machine, you can use the AWS CLI flow. You can find detailed instructions on the Python SDK page: <a href=\"https:\/\/aws.amazon.com\/developers\/getting-started\/python\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/developers\/getting-started\/python\/<\/a> <\/p>\n\n<p>Please note that when you are deploying the code to a different machine, you will have to make sure that you are giving the EC2, ECS, Lambda or any other target a role that will allow the call to this specific endpoint. While in your local machine it can be OK to give you admin rights or other permissive permissions, when you are deploying to a remote instance, you should restrict the permissions as much as possible. <\/p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"sagemaker:InvokeEndpoint\",\n            \"Resource\": \"arn:aws:sagemaker:*:1234567890:endpoint\/&lt;my-endpoint-name&gt;\"\n        }\n    ]\n}\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":12.0,
        "Solution_reading_time":14.44,
        "Solution_score_count":4.0,
        "Solution_sentence_count":7,
        "Solution_word_count":156,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1485822323507,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New Delhi, Delhi, India",
        "Answerer_reputation_count":499.0,
        "Answerer_view_count":48.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy a model using my own custom inference container on sagemaker. I am following the documentation here <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/adapt-inference-container.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/adapt-inference-container.html<\/a><\/p>\n<p>I have an entrypoint file:<\/p>\n<pre><code>from sagemaker_inference import model_server\n#HANDLER_SERVICE = &quot;\/home\/model-server\/model_handler.py:handle&quot;\nHANDLER_SERVICE = &quot;model_handler.py&quot;\nmodel_server.start_model_server(handler_service=HANDLER_SERVICE)\n<\/code><\/pre>\n<p>I have a model_handler.py file:<\/p>\n<pre><code>from sagemaker_inference.default_handler_service import DefaultHandlerService\nfrom sagemaker_inference.transformer import Transformer\nfrom CustomHandler import CustomHandler\n\n\nclass ModelHandler(DefaultHandlerService):\n    def __init__(self):\n        transformer = Transformer(default_inference_handler=CustomHandler())\n        super(HandlerService, self).__init__(transformer=transformer)\n<\/code><\/pre>\n<p>And I have my CustomHandler.py file:<\/p>\n<pre><code>import os\nimport json\nimport pandas as pd\nfrom joblib import dump, load\nfrom sagemaker_inference import default_inference_handler, decoder, encoder, errors, utils, content_types\n\n\nclass CustomHandler(default_inference_handler.DefaultInferenceHandler):\n\n    def model_fn(self, model_dir: str) -&gt; str:\n        clf = load(os.path.join(model_dir, &quot;model.joblib&quot;))\n        return clf\n\n    def input_fn(self, request_body: str, content_type: str) -&gt; pd.DataFrame:\n        if content_type == &quot;application\/json&quot;:\n            items = json.loads(request_body)\n\n            for item in items:\n                processed_item1 = process_item1(items[&quot;item1&quot;])\n                processed_item2 = process_item2(items[&quot;item2])\n                all_item1 += [processed_item1]\n                all_item2 += [processed_item2]\n            return pd.DataFrame({&quot;item1&quot;: all_item1, &quot;comments&quot;: all_item2})\n\n    def predict_fn(self, input_data, model):\n        return model.predict(input_data)\n<\/code><\/pre>\n<p>Once I deploy the model to an endpoint with these files in the image, I get the following error: <code>ml.mms.wlm.WorkerLifeCycle - ModuleNotFoundError: No module named 'model_handler'<\/code>.<\/p>\n<p>I am really stuck what to do here. I wish there was an example of how to do this in the above way end to end but I don't think there is. Thanks!<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635955837330,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1635958560743,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69828187",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.6,
        "Challenge_reading_time":32.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":null,
        "Challenge_title":"sagemaker inference container ModuleNotFoundError: No module named 'model_handler'",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":728.0,
        "Challenge_word_count":209,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521737124656,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Milton Keynes",
        "Poster_reputation_count":738.0,
        "Poster_view_count":69.0,
        "Solution_body":"<p>This is because of the path mismatch. The entrypoint is trying to look for &quot;model_handler.py&quot; in <code>WORKDIR<\/code> directory of the container.\nTo avoid this, always specify absolute path when working with containers.<\/p>\n<p>Moreover your code looks confusing. Please use this sample code as the reference:<\/p>\n<pre><code>import subprocess\nfrom subprocess import CalledProcessError\nimport model_handler\nfrom retrying import retry\nfrom sagemaker_inference import model_server\nimport os\n\n\ndef _retry_if_error(exception):\n    return isinstance(exception, CalledProcessError or OSError)\n\n\n@retry(stop_max_delay=1000 * 50, retry_on_exception=_retry_if_error)\ndef _start_mms():\n    # by default the number of workers per model is 1, but we can configure it through the\n    # environment variable below if desired.\n    # os.environ['SAGEMAKER_MODEL_SERVER_WORKERS'] = '2'\n    print(&quot;Starting MMS -&gt; running &quot;, model_handler.__file__)\n    model_server.start_model_server(handler_service=model_handler.__file__ + &quot;:handle&quot;)\n\n\ndef main():\n    _start_mms()\n    # prevent docker exit\n    subprocess.call([&quot;tail&quot;, &quot;-f&quot;, &quot;\/dev\/null&quot;])\n\nmain()\n<\/code><\/pre>\n<p>Further, notice this line - <code>model_server.start_model_server(handler_service=model_handler.__file__ + &quot;:handle&quot;) <\/code>\nHere we are starting the server, and telling it to call <code>handle()<\/code> function in model_handler.py to invoke your custom logic for all incoming requests.<\/p>\n<p>Also remember that Sagemaker BYOC requires model_handler.py to implement another function <code>ping()<\/code><\/p>\n<p>So your &quot;model_handler.py&quot; should look like this -<\/p>\n<pre><code>custom_handler = CustomHandler()\n\n# define your own health check for the model over here\ndef ping():\n    return &quot;healthy&quot;\n\n\ndef handle(request, context): # context is necessary input otherwise Sagemaker will throw exception\n    if request is None:\n        return &quot;SOME DEFAULT OUTPUT&quot;\n    try:\n        response = custom_handler.predict_fn(request)\n        return [response] # Response must be a list otherwise Sagemaker will throw exception\n\n    except Exception as e:\n        logger.error('Prediction failed for request: {}. \\n'\n                     .format(request) + 'Error trace :: {} \\n'.format(str(e)))\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":13.4,
        "Solution_reading_time":29.19,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17,
        "Solution_word_count":234,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1467451434136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":111.0,
        "Answerer_view_count":36.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><strong>PROBLEM<\/strong> <\/p>\n\n<p>I deployed my experiments in Azure Machine Learning as a Web Service. The experiments ran without error. <\/p>\n\n<p>But when testing using <code>REQUEST\/RESPONSE<\/code>, I'm getting the error below:<\/p>\n\n<blockquote>\n  <p>Execute R Script Piped (RPackage) : The following error occurred during evaluation of R script: R_tryEval: return error: Error in split(df, list(df$PRO_NAME, df$Illness_Code))[Ind] : invalid subscript type 'list'<\/p>\n<\/blockquote>\n\n<p>This is the code:<\/p>\n\n<pre><code># Loop through the dataframe and apply model\nInd &lt;- sapply(split(df, list(df$PRO_NAME,df$Illness_Code)), \n              function(x)nrow(x)&gt;1)\n\nout &lt;- lapply(\n    split(df, list(df$PRO_NAME, df$Illness_Code))[Ind],\n    function(c){\n        m &lt;- lm(formula = COUNT ~ YEAR, data = c)\n        coef(m)\n    })\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544501617417,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53717284",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":11.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"invalid subscript type 'list' Azure Machine Learning",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":53.0,
        "Challenge_word_count":97,
        "Platform":"Stack Overflow",
        "Poster_created_time":1467451434136,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":111.0,
        "Poster_view_count":36.0,
        "Solution_body":"<p><strong>FIXED<\/strong><\/p>\n\n<p><strong>Problem:<\/strong><\/p>\n\n<p>Some R codes don't work if input data is limited (e.g 1-2 rows only)<\/p>\n\n<p><strong>Solution:<\/strong><\/p>\n\n<p>Load data by <code>Batch<\/code> instead of <code>REQUEST\/RESPONSE<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":11.5,
        "Solution_reading_time":3.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2,
        "Solution_word_count":24,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm trying to deploy my own GreengrassV2 components. It's a SageMaker ML model (optimized with SageMakerNeo and packaged as a Greengrass component) and the according inference app. I was trying to deploy it to my core device with SageMaker Edge Manager component. But it is always stuck in the status \"In progress\".\n\nMy logs show this error:\ncom.aws.greengrass.tes.CredentialRequestHandler: Error in retrieving AwsCredentials from TES. {iotCredentialsPath=\/role-aliases\/edgedevicerolealias\/credentials, credentialData=TES responded with status code: 403. Caching response. {\"message\":\"Access Denied\"}}\n\nBut how do I know which policies are missing?",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1682711108991,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1683057468614,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUlDu9VAj4Qx-O7cbAXDz28w\/greengrass-own-component-deployment-stuck-in-progress",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":9.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Greengrass own component deployment stuck \"in progress\"",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":65.0,
        "Challenge_word_count":90,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello, please refer to https:\/\/docs.aws.amazon.com\/greengrass\/v2\/developerguide\/troubleshooting.html#token-exchange-service-credentials-http-403 for troubleshooting,  you'll need `iot:AssumeRoleWithCertificate` permissions on your core device's AWS IoT role alias",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1682713832572,
        "Solution_link_count":1,
        "Solution_readability":21.2,
        "Solution_reading_time":3.58,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2,
        "Solution_word_count":19,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to pass parameters to an Azure ML Web Service with Postman? I created an R web service endpoint that runs in an Azure Container Instance. My run function has one argument (&quot;data&quot;). I can call the web service using Azure ML R SDK (using invoke_webservice()) and the input parameter is read successfully from the request content. The input is constructed like:  <\/p>\n<pre><code>toJSON(data.frame(data=&quot;This is my test string&quot;))\n<\/code><\/pre>\n<p>Result:  <\/p>\n<pre><code>[{&quot;data&quot;: &quot;This is my test string&quot;}]\n<\/code><\/pre>\n<p>If I create a Postman request and copy the input to the request body, the input parameter is not passed to the web service. The web service can return a static output to Postman but the variable data is always empty. Is this a property of the ML Web Service? If not, how can I set up the request body so that the argument is read successfully? I have tried many variations, but none have worked.  <\/p>\n<p>I have set content-type header to application\/json. I don't have authentication in the web service, since it is just a test instance.  <\/p>\n<p>Ultimately, we need to call the web service with C# from Azure Function. I know that we can use the C# template from documentation and it can probably pass the parameter to the web service, but it would be nice to be able to test the web service with Postman.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603965300457,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/144230\/consume-azure-ml-web-service-with-postman-how-to-p",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":17.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"Consume Azure ML Web Service with Postman: how to pass arguments?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":240,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Try this in postman.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/35960-postman.png?platform=QnA\" alt=\"35960-postman.png\" \/>    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":23.9,
        "Solution_reading_time":2.22,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2,
        "Solution_word_count":9,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1483472837568,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bangalore, Karnataka, India",
        "Answerer_reputation_count":72.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":67.5576083334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I've just deployed an ML model on Google vertex AI, it can make predictions using vertex AI web interface. But is it possible to send a request from a browser, for example, to this deployed model. Something like<\/p>\n<pre><code>http:\/\/myapp.cloud.google.com\/input=&quot;features of an example&quot; \n<\/code><\/pre>\n<p>and get the prediction as output.\nThanks<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1631189210517,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69117885",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.9,
        "Challenge_reading_time":5.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Sending http request Google Vertex AI end point",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":929.0,
        "Challenge_word_count":57,
        "Platform":"Stack Overflow",
        "Poster_created_time":1377724133300,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":349.0,
        "Poster_view_count":47.0,
        "Solution_body":"<p>Yes, you can send using endpoint URL as.<\/p>\n<pre><code>https:\/\/us-central1-aiplatform.googleapis.com\/v1beta1\/projects\/&lt;PROJECT_ID&gt;\/locations\/us-central1\/endpoints\/&lt;ENDPOINT_ID&gt;:predict\n<\/code><\/pre>\n<p>Data should be given as in POST parameter.<\/p>\n<pre><code>{\n  &quot;instances&quot;: \n    [1.4838871833555929,\n 1.8659883497083019,\n 2.234620276849616,\n 1.0187816540094903,\n -2.530890710602246,\n -1.6046416850441676,\n -0.4651483719733302,\n -0.4952254087173721,\n 0.774676376873553]\n}\n<\/code><\/pre>\n<p>URL should be Region Based.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1631432417907,
        "Solution_link_count":1,
        "Solution_readability":12.6,
        "Solution_reading_time":7.32,
        "Solution_score_count":4.0,
        "Solution_sentence_count":4,
        "Solution_word_count":35,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1567880532003,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Lahore, Pakistan",
        "Answerer_reputation_count":137.0,
        "Answerer_view_count":100.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using AWS sagemaker, and I have created an endpoint. I want to test endpoint on postman app. I give endpoint URL and JSON body to postman app. But I get this error that <code>&quot;message&quot;: &quot;Missing Authentication Token&quot;<\/code> I need to know from where I 'll get bearer token so that I can give it to postman app.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618912675733,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67176637",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":4.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How to get bearer token AWS for Postman",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1080.0,
        "Challenge_word_count":66,
        "Platform":"Stack Overflow",
        "Poster_created_time":1567880532003,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lahore, Pakistan",
        "Poster_reputation_count":137.0,
        "Poster_view_count":100.0,
        "Solution_body":"<p>I am answering my own question after searching and reading forums,<\/p>\n<p>The easiest way to get bearer token is to install AWS CLI and configure it, using <code>aws configure<\/code> command.\nFor configuring, we must need to know access key, secret key, region of user. These things can be get by AWS users section.\nAfter configuration by running this command, <code>aws ecr get-authorization-token<\/code>, we can get authorizationToken. <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/ecr\/get-authorization-token.html\" rel=\"nofollow noreferrer\">here<\/a> This token can be fed into bearer token, along with aws signature (access key and secret key) in authorization menu in Postman app.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":14.1,
        "Solution_reading_time":8.98,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5,
        "Solution_word_count":93,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1435766573232,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":10645.0,
        "Answerer_view_count":1173.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is that possible to use machine learning methods from Microsoft Azure Machine Learning  as an API from my own code (without ML Studio) with possibility to calculate everything on their side?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1450294016510,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34320449",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":2.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Use Azure ML methods like an API",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":180.0,
        "Challenge_word_count":37,
        "Platform":"Stack Overflow",
        "Poster_created_time":1336227824220,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":834.0,
        "Poster_view_count":122.0,
        "Solution_body":"<p>You can <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-publish-a-machine-learning-web-service\/\" rel=\"nofollow\">publish<\/a> an experiment (machine learning functions you hooked together in Azure ML Studio) as an API. When you call that API in your custom code you give it your data and all the computation runs in the cloud in Azure ML. <\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":12.8,
        "Solution_reading_time":4.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3,
        "Solution_word_count":46,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1546942930440,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":111.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a TensorFlow Serving container in a SageMaker endpoint. I'm able to take a batch of images as a Numpy array and get back predictions like this:<\/p>\n\n<pre><code>import numpy as np\nimport sagemaker\nfrom sagemaker.predictor import json_serializer, json_deserializer\n\nimage = np.random.uniform(low=-1.0, high=1.0, size=(1,128,128,3)).astype(np.float32)    \nimage = {'instances': image}\nimage = json_serializer(image)\n\nrequest_args = {}\nrequest_args['Body'] = image\nrequest_args['EndpointName'] = endpoint_name\nrequest_args['ContentType'] = 'application\/json'\nrequest_args['Accept'] = 'application\/json'\n\n# works successfully\nresponse = sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\nresponse_body = response['Body']\npredictions = json_deserializer(response_body, response['ContentType'])\n<\/code><\/pre>\n\n<p>The size of the <code>request_args<\/code> payload is large doing it this way. I'm wondering, is there a way to send this in a more compressed format? <\/p>\n\n<p>I've tried experimenting with <code>base64<\/code> and <code>json.dumps<\/code>, but can't get past <code>Invalid argument: JSON Value: ...<\/code> errors. Not sure if this isn't supported or if I'm just doing it incorrectly.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548093365813,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54295445",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.4,
        "Challenge_reading_time":16.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"TensorFlow Serving send data as b64 instead of Numpy Array",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":796.0,
        "Challenge_word_count":133,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>I've talked to AWS support about this (see <a href=\"https:\/\/stackoverflow.com\/questions\/54090270\/more-efficient-way-to-send-a-request-than-json-to-deployed-tensorflow-model-in-s\">More efficient way to send a request than JSON to deployed tensorflow model in Sagemaker?<\/a>).<\/p>\n\n<p>They suggest that it is possible to pass in a custom input_fn that will be used by the serving container where one can unpack a compressed format (such as protobuf).<\/p>\n\n<p>I'll be testing this soon and hopefully this stuff works since it would add a lot of flexibility to the input processing.<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":11.9,
        "Solution_reading_time":7.46,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4,
        "Solution_word_count":76,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>How do I get access to the Azure OpenAI service to evaluate it's capabilities?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1663989341807,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1021561\/azure-openai-service-capabilities",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.1,
        "Challenge_reading_time":1.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure OpenAI service capabilities",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":17,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=71c0cf97-895c-43f1-ac54-98e1e9833ae4\">@A-4824  <\/a> Thanks for the question. It is a Limited Access service so you have to apply for it <a href=\"https:\/\/aka.ms\/oai\/access\">https:\/\/aka.ms\/oai\/access<\/a><\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":11.1,
        "Solution_reading_time":3.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2,
        "Solution_word_count":22,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Because of a faulty score.py file in my InferenceConfig, a Model.Deploy failed to Azure Machine Learning, using ACI.  I wanted to create the endpoint in the cloud, but the only state I can see in the portal is Unhealthy.  My local script to deploy the model (using ) keeps running, until it times out. (using the <code>service.wait_for_deployment(show_output=True)<\/code>statement).<\/p>\n\n<p>Is there an option to get more insights in the actual reason\/error message of the deployment turning \"Unhealthy\"?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583937572180,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1584215688008,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60638587",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.4,
        "Challenge_reading_time":7.35,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"How to get insights in exceptions and logging of AzureML endpoint deployment",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":364.0,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Poster_created_time":1360655430743,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belgium",
        "Poster_reputation_count":2947.0,
        "Poster_view_count":355.0,
        "Solution_body":"<p>Usually the timeout is caused by an error in init() function in scoring script. You can get the detailed logs using <code>print(service.get_logs())<\/code> to find the Python error.<\/p>\n\n<p>For more comprehensive troubleshooting guide, see:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":13.3,
        "Solution_reading_time":6.32,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5,
        "Solution_word_count":37,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"I have an Amazon SageMaker endpoint with A1, a model with data capture activated, and I want to update the endpoint with A2, a new model.\n\nHow do I track the Model Monitor Data Capture that captured data in Amazon S3, and identify which data referred to model A1 and which data referred to model A2?",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1602088286000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668410094244,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUPWBH_xFoS4aq5i41Za5qaQ\/tracking-the-lineage-between-amazon-sagemaker-endpoint-model-and-model-monitor-captured-data",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.1,
        "Challenge_reading_time":4.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Tracking the lineage between Amazon SageMaker endpoint model and Model Monitor captured data",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":173.0,
        "Challenge_word_count":68,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"using boto3:    \nwhen you update the model endpoint, you need to create a new `EndpointConfig` where you specify a new `s3 uri` where data capture will be stored and thats how you can see different data captures from different versions of the model.\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_UpdateEndpoint.html\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateEndpointConfig.html",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925572435,
        "Solution_link_count":2,
        "Solution_readability":26.7,
        "Solution_reading_time":5.45,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1,
        "Solution_word_count":45,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1343167997556,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":191.0,
        "Answerer_view_count":27.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<h2><strong>ASKING THIS HERE AT THE EXPLICIT REQUEST OF THE MICROSOFT AZURE SUPPORT TEAM.<\/strong><\/h2>\n\n<p>I've been attempting to call the MS Luis.ai <em>programmatic<\/em> API (bit.ly\/2iev01n) and have been receiving a 401 unauthorized response to every request. Here's a simple GET example: <code>https:\/\/api.projectoxford.ai\/luis\/v1.0\/prog\/apps\/{appId}\/entities?subscription-key={subscription_key}<\/code>.  <\/p>\n\n<p>I am providing my appId from the Luis.ai GUI (as specified by the API docs), here:<br>\n<img src=\"https:\/\/i.stack.imgur.com\/Cg2Fw.png\" alt=\"Luis.ai App Settings App Id\"><\/p>\n\n<p>I am providing my subscription key from Azure (as specified by the API docs), here:<br>\n<img src=\"https:\/\/i.stack.imgur.com\/GS2Fe.png\" alt=\"Azure Console\"><\/p>\n\n<p>The app ID and subscription key, sourced from above, are the exact same as what I'm using to hit the query API successfully (see note at bottom). My account is pay-as-you-go (not free).<\/p>\n\n<p><strong><em>Am I doing something wrong here? Is this API deprecated, moved, down, or out-of-sync with the docs?<\/em><\/strong><\/p>\n\n<p><strong>NOTE:<\/strong> I can manipulate my model through the online GUI but that approach will be far too manual for our business needs where our model will need to be programmatically updated as new business entities come into existence.  <\/p>\n\n<p><strong>NOTE:<\/strong> The programmatic API is different from the query API which has this request URL, which is working fine for me:<br>\n<code>https:\/\/api.projectoxford.ai\/luis\/v2.0\/apps\/{appId}?subscription-key={subscription_key}&amp;verbose=true&amp;q={utterance}<\/code>  <\/p>\n\n<p><strong>NOTE:<\/strong> There doesn't seem to be a Luis.ai programmatic API for v2.0--which is why the URLs from the query and programmatic APIs have different versions.  <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1484180085280,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1484193011887,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41603082",
        "Challenge_link_count":4,
        "Challenge_participation_count":4,
        "Challenge_readability":10.4,
        "Challenge_reading_time":23.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"401 Errors Calling the Microsoft Luis.ai Programmatic API",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1280.0,
        "Challenge_word_count":229,
        "Platform":"Stack Overflow",
        "Poster_created_time":1343167997556,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":191.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>Answering my own question here:<\/p>\n\n<p>I have found my LUIS.ai programmatic API key. It is found by:\nLUIS.ai dashboard -> username (upper-right) -> settings in dropdown -> Subscription Keys tab -> Programmatic API Key<\/p>\n\n<p>It was not immediately obvious since it's found nowhere else: not alongside any of the other key listings in cognitive services or the LUIS.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":8.6,
        "Solution_reading_time":4.63,
        "Solution_score_count":7.0,
        "Solution_sentence_count":4,
        "Solution_word_count":54,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>I am new to the Azure ML Studio and just deployed the bike-rental regression model. When I tried to test it using the built in test tool in the studio, I am getting the attached error. Similar results running the Python code as well. Can someone please help me?    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/176918-mlerror.png?platform=QnA\" alt=\"176918-mlerror.png\" \/>    <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":4,
        "Challenge_created_time":1645577589217,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/746784\/azure-ml-studio-error-while-testing-real-time-endp",
        "Challenge_link_count":1,
        "Challenge_participation_count":9,
        "Challenge_readability":8.8,
        "Challenge_reading_time":6.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML Studio error while testing real-time endpoint -  list index out of range",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":66,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=b7844017-59f9-4d2e-a021-76c2270e06ca\">@Kumar, Priya  <\/a> Thanks for the question. It's known issue and the product team working on the fix to change in the UI.    <\/p>\n<p>Workaround: As shown below please set the GlobalParameters flag to 1.0 or a float number or remove it.     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/177485-image.png?platform=QnA\" alt=\"177485-image.png\" \/>    <\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":9.4,
        "Solution_reading_time":5.69,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5,
        "Solution_word_count":48,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1606378353316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Huskvarna, Sverige",
        "Answerer_reputation_count":275.0,
        "Answerer_view_count":36.0,
        "Challenge_adjusted_solved_time":3.7086241667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The yaml template <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-properties-sagemaker-model-containerdefinition.html\" rel=\"nofollow noreferrer\">documentation<\/a> for the AWS Cloudformation AWS::SageMaker::Model ContainerDefinition specifies that &quot;Environment&quot; is of type Json. I can't work out how to submit json in my yaml template that does not cause a &quot;CREATE_FAILED    Internal Failure&quot; after running a deploy with the below command.<\/p>\n<pre><code>aws cloudformation deploy --stack-name test1 --template-file test-template-export.yml\n<\/code><\/pre>\n<p>test-template-export.yml<\/p>\n<pre><code>Description: Example yaml\n\nResources:\n  Model:\n    Type: AWS::SageMaker::Model\n    Properties:\n      Containers:\n      - ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n      - Environment: '{&quot;SAGEMAKER_CONTAINER_LOG_LEVEL&quot;: &quot;20&quot;}'\n      ExecutionRoleArn: arn:aws:iam::123456789123:role\/service-role\/AmazonSageMakerServiceCatalogProductsUseRole\n<\/code><\/pre>\n<p>I have also tried the below formats as well and still no luck.<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n  Environment: '{&quot;SAGEMAKER_CONTAINER_LOG_LEVEL&quot;: &quot;20&quot;}'\n<\/code><\/pre>\n<p>--<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n- Environment: | \n         {\n            &quot;SAGEMAKER_CONTAINER_LOG_LEVEL&quot;: &quot;20&quot;\n          }\n<\/code><\/pre>\n<p>--<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n- Environment:\n  - SAGEMAKER_CONTAINER_LOG_LEVEL: &quot;20&quot;\n<\/code><\/pre>\n<p>Running without Environment deploys fine.<\/p>\n<p>I have tried everything in <a href=\"https:\/\/stackoverflow.com\/questions\/39041209\/how-to-specify-json-formatted-string-in-cloudformation\">this answer.<\/a>\nHow do I format this Environment argument?<\/p>\n<p>My version of aws cli is &quot;aws-cli\/2.4.10 Python\/3.8.8&quot;<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643283742143,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70877982",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":24.0,
        "Challenge_reading_time":28.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"How to format parameter of data type json in a aws cloudformation yaml template?",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":391.0,
        "Challenge_word_count":151,
        "Platform":"Stack Overflow",
        "Poster_created_time":1452981020536,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"UK",
        "Poster_reputation_count":311.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p>Hi when you see json format think more dict.\nSo write it like this:<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n  Environment:\n     SAGEMAKER_CONTAINER_LOG_LEVEL: 20\n<\/code><\/pre>\n<p>For IAM Policies the PolicyDocument is json type and this is how AWS do it in their exempel:<\/p>\n<pre><code>Type: 'AWS::IAM::Policy'\nProperties:\n  PolicyName: CFNUsers\n  PolicyDocument:\n    Version: &quot;2012-10-17&quot;\n    Statement:\n      - Effect: Allow\n        Action:\n          - 'cloudformation:Describe*'\n          - 'cloudformation:List*'\n          - 'cloudformation:Get*'\n        Resource: '*'\n  Groups:\n    - !Ref CFNUserGroup\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1643297093190,
        "Solution_link_count":0,
        "Solution_readability":18.1,
        "Solution_reading_time":8.26,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3,
        "Solution_word_count":59,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Where can I find the actual references to API definitions and descriptions for ModelBiasMonitor and ModelExplainabilityMonitor Classes?\n\nI can a find a few mentions in the Amazon SageMaker documentation in the following links.\nhttps:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model_monitor.html\nhttps:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_model_monitor\/fairness_and_explainability\/SageMaker-Model-Monitor-Fairness-and-Explainability.html\n\nWhere can I find the actual reference and the code implementation for these Classes?",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611507553000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1667925795096,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU9SPQKzelSUu3dr-D4zaXHQ\/api-definition-for-modelbiasmonitor-and-modelexplainabilitymonitor",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":27.4,
        "Challenge_reading_time":8.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"API definition for ModelBiasMonitor and ModelExplainabilityMonitor",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":46.0,
        "Challenge_word_count":54,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The actual reference to the classes can be found here: \n[https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/model_monitor\/clarify_model_monitoring.py ]()  \nIt encapsulates the definitions and descriptions for all of SageMaker Clarify related monitoring classes.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1613663015963,
        "Solution_link_count":1,
        "Solution_readability":24.7,
        "Solution_reading_time":3.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2,
        "Solution_word_count":25,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1394703217223,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cologne, Germany",
        "Answerer_reputation_count":486.0,
        "Answerer_view_count":26.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I would like to host a model on Sagemaker using the new <a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2021\/12\/amazon-sagemaker-serverless-inference\/?nc1=h_ls\" rel=\"nofollow noreferrer\">Serverless Inference<\/a>.<\/p>\n<p>I wrote my own container for inference and handler following several guides. These are the requirements:<\/p>\n<pre><code>mxnet\nmulti-model-server\nsagemaker-inference\nretrying\nnltk\ntransformers==4.12.4\ntorch==1.10.0\n<\/code><\/pre>\n<p>On non-serverless endpoints, this container works perfectly well. However, with the serverless version I get the following error message when loading the model:<\/p>\n<pre><code>ERROR - \/.sagemaker\/mms\/models\/model already exists.\n<\/code><\/pre>\n<p>The error is thrown by the following subprocess<\/p>\n<pre><code>['model-archiver', '--model-name', 'model', '--handler', '\/home\/model-server\/handler_service.py:handle', '--model-path', '\/opt\/ml\/model', '--export-path', '\/.sagemaker\/mms\/models', '--archive-format', 'no-archive']\n<\/code><\/pre>\n<p>So something that has to do with the <code>model-archiver<\/code> (which I guess is a process from the MMS package?).<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639400254370,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70335049",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":14.5,
        "Challenge_reading_time":15.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker Serverless Inference & custom container: Model archiver subprocess fails",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":373.0,
        "Challenge_word_count":116,
        "Platform":"Stack Overflow",
        "Poster_created_time":1394703217223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cologne, Germany",
        "Poster_reputation_count":486.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>So the issue really was related to hosting the model using the sagemaker inference toolkit and MMS which always uses the multi-model scenario which is not supported by serverless inference.<\/p>\n<p>I ended up writing my own Flask API which actually is nearly as easy and more customizable. Ping me for details if you're interested.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":11.5,
        "Solution_reading_time":4.17,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3,
        "Solution_word_count":54,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1351154914716,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2564.0,
        "Answerer_view_count":451.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>i am trying to deploy a model in Vertex AI pipeline component using <code>DeployModelRequest<\/code>. I try to get the model using <code>GetModelRequest<\/code><\/p>\n<pre><code>    model_name = f'projects\/{project}\/locations\/{location}\/models\/{model_id}'\n    model_request = aiplatform_v1.types.GetModelRequest(name=model_name)\n    model_info = client_model.get_model(request=model_request)       \n    \n    deploy_request = aiplatform_v1.types.DeployModelRequest(endpoint=end_point, \n                                                                deployed_model=model_info)\n    client.deploy_model(request=deploy_request)\n<\/code><\/pre>\n<p>but this gives:<\/p>\n<pre><code>TypeError: Parameter to MergeFrom() must be instance of same class: expected \ngoogle.cloud.aiplatform.v1.DeployedModel got Model\n<\/code><\/pre>\n<p>I have also tried <code>deployed_model=model_info.deployed_models[0]<\/code> but this gave:<\/p>\n<pre><code>TypeError: Parameter to MergeFrom() must be instance of same class: expected\n google.cloud.aiplatform.v1.DeployedModel got DeployedModelRef.\n<\/code><\/pre>\n<p>So what do I use for <code>deployed_model<\/code>?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":6,
        "Challenge_created_time":1663254259473,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73733464",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":20.8,
        "Challenge_reading_time":15.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"What should be used for deployed_model in DeployModelRequest in Vertex AI pipeline?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":39.0,
        "Challenge_word_count":93,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>As simple as:<\/p>\n<pre><code>machine_spec = MachineSpec(machine_type=&quot;n1-standard-2&quot;)\ndedicated_resources = DedicatedResources(machine_spec=machine_spec, \n                                         min_replica_count=1, \n                                         max_replica_count=1)\ndepmodel= DeployedModel(model=model_name, dedicated_resources=dedicated_resources) \n\ndeploy_request = aiplatform_v1.types.DeployModelRequest(\n                   endpoint=end_point, deployed_model=depmodel, \n                   traffic_split={'0':100})\nclient.deploy_model(request=deploy_request)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":49.5,
        "Solution_reading_time":6.68,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2,
        "Solution_word_count":19,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1403541426412,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bengaluru, India",
        "Answerer_reputation_count":191.0,
        "Answerer_view_count":22.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created the Azure ML experiment with R script module \nit works fine while we run the experiment but\n when we publish the web service it throws error http 500 \n ( I believe the error is causing in the R script module because other modules are running fine in web service but i can't debug the problem<\/p>\n\n<blockquote>\n  <p>Http status code: 500, Timestamp: Fri, 08 May 2015 04:23:14 GMT<\/p>\n<\/blockquote>\n\n<p>Also is there any limitation in r e.g. some function which wont work in web service<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1431059608047,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1446192965568,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/30115812",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.7,
        "Challenge_reading_time":7.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Error while running Azure Machine Learning web service but the experiment works fine",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":415.0,
        "Challenge_word_count":98,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403541426412,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, India",
        "Poster_reputation_count":191.0,
        "Poster_view_count":22.0,
        "Solution_body":"<p>I found the problem. I was facing this error because the R module in the Azure ML was was taking variable as the other data type and not producing any outputs results which i was checking through for loop which is why i was getting this error.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":8.9,
        "Solution_reading_time":3.0,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2,
        "Solution_word_count":47,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":23.7400955556,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm facing the below error when asynchronous endpoint is invoked in sagemaker for batch processing:\n\n```\n\"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\"\n```\nI found that the issue is due to the total processing time exceeds the default 15min window. \n\nSo, I would like to know how to extend the total processing time for Asynchronous endpoint upto 60 minutes as mentioned in aws docs: [https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/async-inference.html]()\n\nThank you.",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1678439351698,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1678787142156,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUzMAZWe2vRY2CsKp7Xyexqg\/how-to-extend-the-total-processing-time-for-asynchronous-endpoint-upto-60-minutes",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.9,
        "Challenge_reading_time":8.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"how to extend the total processing time for Asynchronous endpoint upto 60 minutes?",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":32.0,
        "Challenge_word_count":99,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi, I found the solution to extend the total processing time for Asynchronous endpoint upto 60 minutes. \nUse this parameter \"InvocationTimeoutSeconds=3600\" when invoking the async endpoint.\n\ncode looks like below:\n\n```\nresponse = sm_client.invoke_endpoint_async(\nEndpointName=endpoint_name,\nInputLocation=input_file_s3_path,\nContentType='text\/csv',\nAccept='application\/json',\nInvocationTimeoutSeconds=3600    \n)\n```\nThank you.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1678872606500,
        "Solution_link_count":0,
        "Solution_readability":18.9,
        "Solution_reading_time":5.54,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":39,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1395422283667,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1411.0,
        "Answerer_view_count":45.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed an AzureML published experiment with deployed web service. I tried to use the <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-consume-web-services\/\" rel=\"nofollow\">sample code provided in the configuration page<\/a>, but universal apps do not implement Http.Formatting yet, thus I couldn't use <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/hh944521(v=vs.118).aspx\" rel=\"nofollow\">postasjsonasync<\/a>.<\/p>\n\n<p>I tried to follow the sample code as much as possible, but I'm getting statuscode of 415 \"Unsupported Media Type\", What's the mistake I'm doing?<\/p>\n\n<pre><code>var client = new HttpClient();\nclient.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Bearer\", apiKey);\n\/\/ client.BaseAddress = uri;\n\nvar scoreRequest = new\n{\n            Inputs = new Dictionary&lt;string, StringTable&gt;() {\n                    {\n                        \"dataInput\",\n                        new StringTable()\n                        {\n                            ColumnNames = new [] {\"Direction\", \"meanX\", \"meanY\", \"meanZ\"},\n                            Values = new [,] {  { \"\", x.ToString(), y.ToString(), z.ToString() },  }\n                        }\n                    },\n                },\n            GlobalParameters = new Dictionary&lt;string, string&gt;() { }\n };\n var stringContent = new StringContent(scoreRequest.ToString());\n HttpResponseMessage response = await client.PostAsync(uri, stringContent);\n<\/code><\/pre>\n\n<p>Many Thanks<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1452005613750,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34614582",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":16.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Send request as Json on UWP",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":3194.0,
        "Challenge_word_count":117,
        "Platform":"Stack Overflow",
        "Poster_created_time":1352139399460,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cyprus",
        "Poster_reputation_count":820.0,
        "Poster_view_count":256.0,
        "Solution_body":"<p>You'll need to serialize the object to a JSON string (I recommend using NewtonSoft.Json to make it easier) and set the content type accordingly. Here's an implementation I'm using in my UWP apps (note that <code>_client<\/code> is an <code>HttpClient<\/code>):<\/p>\n\n<pre><code>    public async Task&lt;HttpResponseMessage&gt; PostAsJsonAsync&lt;T&gt;(Uri uri, T item)\n    {\n        var itemAsJson = JsonConvert.SerializeObject(item);\n        var content = new StringContent(itemAsJson);\n        content.Headers.ContentType = new MediaTypeHeaderValue(\"application\/json\");\n\n        return await _client.PostAsync(uri, content);\n    }\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":13.2,
        "Solution_reading_time":7.86,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6,
        "Solution_word_count":62,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1500629225150,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5939.0,
        "Answerer_view_count":886.0,
        "Challenge_adjusted_solved_time":133.8006258333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to create a module for Sagemaker endpoints. There's an optional object variable called <code>async_inference_config<\/code>. If you omit it, the endpoint being deployed is synchronous, but if you include it, the endpoint deployed is asynchronous. To satisfy both of these usecases, the <code>async_inference_config<\/code> needs to be an optional block.<\/p>\n<p>I am unsure of how to make this block optional though.<br \/>\nAny guidance would be greatly appreciated. See example below of structure of the optional parameter.<\/p>\n<p><strong>Example:<\/strong><\/p>\n<pre><code>resource &quot;aws_sagemaker_endpoint_configuration&quot; &quot;sagemaker_endpoint_configuration&quot; {\n  count = var.create ? 1 : 0\n\n  name = var.endpoint_configuration_name\n  production_variants {\n    instance_type          = var.instance_type\n    initial_instance_count = var.instance_count\n    model_name             = var.model_name\n    variant_name           = var.variant_name\n  }\n  async_inference_config {\n    output_config {\n      s3_output_path = var.s3_output_path\n    }\n    client_config {\n      max_concurrent_invocations_per_instance = var.max_concurrent_invocations_per_instance\n    }\n  }\n  lifecycle {\n    create_before_destroy = true\n    ignore_changes        = [&quot;name&quot;]\n  }\n\n  tags = var.tags\n\n  depends_on = [aws_sagemaker_model.sagemaker_model]\n}\n<\/code><\/pre>\n<p><strong>Update:<\/strong> What I tried based on the below suggestion, which seemed to work<\/p>\n<pre><code>dynamic &quot;async_inference_config&quot; {\n    for_each = var.async_inference_config == null ? [] : [true]\n    content {\n      output_config {\n        s3_output_path = lookup(var.async_inference_config, &quot;s3_output_path&quot;, null)\n      }\n      client_config {\n        max_concurrent_invocations_per_instance = lookup(var.async_inference_config, &quot;max_concurrent_invocations_per_instance&quot;, null)\n      }\n    }\n  }\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658386596203,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1658442017223,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73061907",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":17.7,
        "Challenge_reading_time":23.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":null,
        "Challenge_title":"Terraform - Optional Nested Variable",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":55.0,
        "Challenge_word_count":146,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443225809767,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Vancouver, BC, Canada",
        "Poster_reputation_count":2332.0,
        "Poster_view_count":560.0,
        "Solution_body":"<p>You could use a <code>dynamic<\/code> block [1] in combination with <code>for_each<\/code> meta-argument [2]. It would look something like:<\/p>\n<pre><code>dynamic &quot;async_inference_config&quot; {\n    for_each = var.s3_output_path != null &amp;&amp; var.max_concurrent_invocations_per_instance != null ? [1] : []\n    content {\n    output_config {\n      s3_output_path = var.s3_output_path\n    }\n    client_config {\n      max_concurrent_invocations_per_instance = var.max_concurrent_invocations_per_instance\n    }\n  }\n}\n<\/code><\/pre>\n<p>Of course, you could come up with a different variable, say <code>enable_async_inference_config<\/code> (probalby of type <code>bool<\/code>) and base the <code>for_each<\/code> on that, e.g.:<\/p>\n<pre><code>dynamic &quot;async_inference_config&quot; {\n    for_each = var.enable_async_inference_config ? [1] : []\n    content {\n    output_config {\n      s3_output_path = var.s3_output_path\n    }\n    client_config {\n      max_concurrent_invocations_per_instance = var.max_concurrent_invocations_per_instance\n    }\n  }\n}\n<\/code><\/pre>\n<hr \/>\n<p>[1] <a href=\"https:\/\/www.terraform.io\/language\/expressions\/dynamic-blocks\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/language\/expressions\/dynamic-blocks<\/a><\/p>\n<p>[2] <a href=\"https:\/\/www.terraform.io\/language\/meta-arguments\/for_each\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/language\/meta-arguments\/for_each<\/a><\/p>",
        "Solution_comment_count":13.0,
        "Solution_last_edit_time":1658923699476,
        "Solution_link_count":4,
        "Solution_readability":23.4,
        "Solution_reading_time":18.04,
        "Solution_score_count":2.0,
        "Solution_sentence_count":13,
        "Solution_word_count":82,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1435766573232,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":10645.0,
        "Answerer_view_count":1173.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I try to use a trained model from Microsoft Azure Machine Learning Studio in Azure Stream Analytics.\nBefore I start work with my IoT-Stream sensor data, I try this sample: \n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/stream-analytics\/stream-analytics-machine-learning-integration-tutorial\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/stream-analytics\/stream-analytics-machine-learning-integration-tutorial<\/a><\/p>\n\n<p>I can deploy the web service and it works fine with a console application.\nThe result from web service:<\/p>\n\n<pre><code>{\n    \"Results\": {\n        \"output1\": {\n            \"type\": \"table\",\n            \"value\": {\n                \"ColumnNames\": [\"Sentiment\", \"Score\"],\n                \"ColumnTypes\": [\"String\", \"Double\"],\n                \"Values\": [\n                    [\"neutral\", \"0.564501523971558\"]\n                ]\n            }\n        }\n    }\n}\n<\/code><\/pre>\n\n<p>The T-SQL in Stream Analytics from tutorial looks like:<\/p>\n\n<pre><code>WITH subquery AS (  \n    SELECT text, sentiment(text) as result from input  \n)  \n\nSelect text, result.[Scored Labels]  \nInto output  \nFrom subquery\n<\/code><\/pre>\n\n<p>Unfortunately it does not work. Can someone explain <code>result.[Scored Labels]<\/code><\/p>\n\n<p>Is it possible to debug my Stream Analytic job?\nI get no output. No result-file, no warning, no exception...<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1481402034540,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41080045",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.6,
        "Challenge_reading_time":16.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I use ML function in Azure Stream Analytics?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":752.0,
        "Challenge_word_count":135,
        "Platform":"Stack Overflow",
        "Poster_created_time":1400791038563,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":2437.0,
        "Poster_view_count":234.0,
        "Solution_body":"<p>It is not currently possible to test your query when you use a function to call out to Azure ML. The test query functionality runs in the web browser window so I guess they haven't implemented that feature yet. <\/p>\n\n<p>I expect if you start the job it will actually work. However you may need to change <code>result.[Scored Labels]<\/code> to match the columns in the Azure ML API output by saying <code>result.Sentiment<\/code> and <code>result.Score<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":5.7,
        "Solution_reading_time":5.73,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6,
        "Solution_word_count":74,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1254957460063,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"North Carolina, USA",
        "Answerer_reputation_count":2484.0,
        "Answerer_view_count":362.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I wanted to know how exactly the following works in backend<\/p>\n\n<p><strong>Scenario :<\/strong> <\/p>\n\n<blockquote>\n  <p>-> We get data from Edgex foundry in UTC format and we it store it in Azure Document DB in (CST\/CDT timezone) format<\/p>\n  \n  <p>-> We trained ML model on data(with Date in CST\/CDT timezone) and Deploy web service.<\/p>\n<\/blockquote>\n\n<p><strong>So I have few basic doubts below<\/strong><\/p>\n\n<blockquote>\n  <ol>\n  <li><p>When web job hits our predictive webservice , will the trained ML model be run again?<\/p><\/li>\n  <li><p>Do we need to convert the UTC timezone for new incoming test data( which we want to predict) into CST\/CDT timezone, as TimeStamp does\n  matter for our prediction?<\/p><\/li>\n  <li><p>What happens in backend when predictive webservice API is called?<\/p><\/li>\n  <\/ol>\n<\/blockquote>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1520339633757,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49130977",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":11.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Does a call to \"Deploy web service(via API key) \" re run trained Azure ML model again",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":92.0,
        "Challenge_word_count":133,
        "Platform":"Stack Overflow",
        "Poster_created_time":1504867604870,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":391.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>This is only based on my experience with Azure ML, but I think I can help with your questions.<\/p>\n\n<blockquote>\n  <p>When web job hits our predictive webservice, will the trained ML model be run again?<\/p>\n<\/blockquote>\n\n<p>Yes, in the sense that it will call the <code>predict<\/code> (or similar) method on the model on the new data. For instance, in <code>scikit-learn<\/code> you would train your model using the <code>fit<\/code> method. Once the model is in production, only the <code>predict<\/code> method would be called.<\/p>\n\n<p>It will also run the whole workflow you have set up to be deployed as the web service. As an example below is a workflow I've played around with before. Each time the web service is run with new data, this whole thing will be run. This is like creating a Pipeline in <code>scikit-learn<\/code>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YMFZb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YMFZb.png\" alt=\"Azure ML Workflow\"><\/a><\/p>\n\n<blockquote>\n  <p>Do we need to convert the UTC timezone for new incoming test data( which we want to predict) into CST\/CDT timezone, as TimeStamp does matter for our prediction?<\/p>\n<\/blockquote>\n\n<p>I would say yes, you would need to convert to the timezone that was used when training in the model. This can be done by adding a step in your workflow then when you call the web service it will do the necessary converting for you before making a prediction.<\/p>\n\n<blockquote>\n  <p>What happens in backend when predictive webservice API is called?<\/p>\n<\/blockquote>\n\n<p>I'm not sure if anyone knows for sure other than the folks at Microsoft, but for sure it will run the workflow you have set up.<\/p>\n\n<hr>\n\n<p>I know it's not much, but I hope this helps or at least gets you on the right track for what you need.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":7.5,
        "Solution_reading_time":22.21,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17,
        "Solution_word_count":286,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hallo, <\/p>\n<p>I have a student version of azure account. So, I want to classify images (e.g. cat vs dog) with Azure Machine Learning. I used data labeling and automatedML for the task. I registered my model and I tried to deploy it by means of Real-time endpoint. I use for that the standard_F2s__v2 VM, because for other VMs I don't have enough quota.<\/p>\n<p>During deployment I get this error (see below). Do you know what can I do? what's the problem? the VM or scripts (docker etc.) which are generated by azure? <\/p>\n<p>Thanks for answers!<\/p>\n<blockquote>\n<p>Instance status:\nSystemSetup: Succeeded\nUserContainerImagePull: Succeeded\nModelDownload: Succeeded\nUserContainerStart: InProgress\nContainer events:\nKind: Pod, Name: Downloading, Type: Normal, Time: 2023-05-05T18:52:51.640736Z, Message: Start downloading models\nKind: Pod, Name: Pulling, Type: Normal, Time: 2023-05-05T18:52:51.939809Z, Message: Start pulling container image\nKind: Pod, Name: Pulled, Type: Normal, Time: 2023-05-05T18:53:18.896965Z, Message: Container image is pulled successfully\nKind: Pod, Name: Downloaded, Type: Normal, Time: 2023-05-05T18:53:18.896965Z, Message: Models are downloaded successfully\nKind: Pod, Name: Created, Type: Normal, Time: 2023-05-05T18:53:18.983732Z, Message: Created container inference-server\nKind: Pod, Name: Started, Type: Normal, Time: 2023-05-05T18:53:19.121257Z, Message: Started container inference-server\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-05-05T18:53:33.609235Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-05-05T18:53:44.184435Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-05-05T18:53:53.609086Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-05-05T18:54:03.608893Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-05-05T18:54:13.608775Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-05-05T18:54:23.608705Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\nContainer logs:\n2023-05-05T18:53:19,317469780+00:00 - rsyslog\/run \n2023-05-05T18:53:19,320703107+00:00 - gunicorn\/run \n2023-05-05T18:53:19,322375521+00:00 | gunicorn\/run | \n2023-05-05T18:53:19,324133736+00:00 | gunicorn\/run | ###############################################\n2023-05-05T18:53:19,325868850+00:00 | gunicorn\/run | AzureML Container Runtime Information\n2023-05-05T18:53:19,328116769+00:00 | gunicorn\/run | ###############################################\n2023-05-05T18:53:19,331154294+00:00 | gunicorn\/run | \n2023-05-05T18:53:19,333284612+00:00 | gunicorn\/run | \n2023-05-05T18:53:19,341554081+00:00 | gunicorn\/run | AzureML image information: mlflow-ubuntu20.04-py38-cpu-inference:20230404.v14\n2023-05-05T18:53:19,343354796+00:00 | gunicorn\/run | \n2023-05-05T18:53:19,345147111+00:00 | gunicorn\/run | \n2023-05-05T18:53:19,346924426+00:00 | gunicorn\/run | PATH environment variable: \/opt\/miniconda\/envs\/amlenv\/bin:\/opt\/miniconda\/bin:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\n2023-05-05T18:53:19,348632240+00:00 | gunicorn\/run | PYTHONPATH environment variable: \n2023-05-05T18:53:19,350929659+00:00 | gunicorn\/run | \n2023-05-05T18:53:19,358940326+00:00 - nginx\/run \nnginx: [warn] the &quot;user&quot; directive makes sense only if the master process runs with super-user privileges, ignored in \/etc\/nginx\/nginx.conf:1\n2023-05-05T18:53:21,047186621+00:00 | gunicorn\/run | CONDAPATH environment variable: \/opt\/miniconda<\/p>\n<h1 id=\"conda-environments\">conda environments:<\/h1>\n<h1 id=\"section\"><\/h1>\n<p>base                     \/opt\/miniconda\namlenv                   \/opt\/miniconda\/envs\/amlenv\n2023-05-05T18:53:22,109803493+00:00 | gunicorn\/run | \n2023-05-05T18:53:22,111660209+00:00 | gunicorn\/run | Pip Dependencies (before dynamic installation)\nazure-core==1.26.3\nazure-identity==1.12.0\nazureml-inference-server-http==0.8.3\ncachetools==5.3.0\ncertifi==2022.12.7\ncffi==1.15.1\ncharset-normalizer==3.1.0\nclick==8.1.3\ncryptography==40.0.1\nFlask==2.2.3\nFlask-Cors==3.0.10\ngoogle-api-core==2.11.0\ngoogle-auth==2.17.1\ngoogleapis-common-protos==1.59.0\ngunicorn==20.1.0\nidna==3.4\nimportlib-metadata==6.1.0\ninference-schema==1.5.1\nitsdangerous==2.1.2\nJinja2==3.1.2\nMarkupSafe==2.1.2\nmsal==1.21.0\nmsal-extensions==1.0.0\nopencensus==0.11.2\nopencensus-context==0.1.3\nopencensus-ext-azure==1.1.9\nportalocker==2.7.0\nprotobuf==4.22.1\npsutil==5.9.4\npyasn1==0.4.8\npyasn1-modules==0.2.8\npycparser==2.21\npydantic==1.10.7\nPyJWT==2.6.0\npython-dateutil==2.8.2\npytz==2023.3\nrequests==2.28.2\nrsa==4.9\nsix==1.16.0\ntyping_extensions==4.5.0\nurllib3==1.26.15\nWerkzeug==2.2.3\nwrapt==1.12.1\nzipp==3.15.0\n2023-05-05T18:53:23,247714179+00:00 | gunicorn\/run | \n2023-05-05T18:53:23,249556992+00:00 | gunicorn\/run | Entry script directory: \/var\/mlflow_resources\/.\n2023-05-05T18:53:23,251367404+00:00 | gunicorn\/run | \n2023-05-05T18:53:23,253148416+00:00 | gunicorn\/run | ###############################################\n2023-05-05T18:53:23,254978929+00:00 | gunicorn\/run | Dynamic Python Package Installation\n2023-05-05T18:53:23,256700340+00:00 | gunicorn\/run | ###############################################\n2023-05-05T18:53:23,258536553+00:00 | gunicorn\/run | \n2023-05-05T18:53:23,260471566+00:00 | gunicorn\/run | Updating conda environment from \/var\/azureml-app\/azureml-models\/trained_05052023\/1\/mlflow-model\/conda.yaml !\nRetrieving notices: ...working... done\n.\/run: line 152:    62 Killed                  conda env create -n userenv -f &quot;${CONDA_FILENAME}&quot;\nCollecting package metadata (repodata.json): ...working... Error occurred. Sleeping to send error logs.\n2023-05-05T18:54:29,187641958+00:00 - gunicorn\/finish 95 0\n2023-05-05T18:54:29,189598769+00:00 - Exit code 95 is not normal. Killing image.<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1683350863183,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1276938\/ml-model-deployment-(endpoints)-throws-error",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.0,
        "Challenge_reading_time":81.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":null,
        "Challenge_title":"ML Model Deployment (Endpoints) throws error",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":504,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello @<strong>student2023 !<\/strong><\/p>\n<p>it seems that the Container has some errors<\/p>\n<p>Can you post the steps of the Process as you did it ?<\/p>\n<p>Also check on Azure , is the Container Healthy ?<\/p>\n<p>Is this a lab you found or your own ? <\/p>\n<p>Come back to see your feedback !<\/p>\n<hr \/>\n<p>Kindly mark the answer as accepted in case it helped or post your feedback to help !<\/p>\n<p>Regards<\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":5.1,
        "Solution_reading_time":5.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5,
        "Solution_word_count":69,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi,  <\/p>\n<p>I  was deploying a real-time inference pipeline into an AKS compute in East US region today. The endpoint deployment state was stuck at Transitioning for over 2 hours and never finished and I had to delete it. A separate deployment to region East US 2 got stuck as well.  I was able to deploy the same pipeline to East US  the day before yesterday.  <\/p>\n<p> I wonder if this is likely an error related to my account\/resources or a system wide issue? Did anyone else encounter the similar issue?  <\/p>\n<p>thanks in advance!  <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":6,
        "Challenge_created_time":1591823291753,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/34653\/azure-ml-real-time-inference-endpoint-deloyment-st",
        "Challenge_link_count":0,
        "Challenge_participation_count":8,
        "Challenge_readability":7.6,
        "Challenge_reading_time":7.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML real-time inference endpoint deloyment stuck - with deployment state as Transitioning for over 2 hours.",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":112,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello All,  <\/p>\n<p>We have deployed a fix now to all regions and this should be fixed. Could you please retry and let us know if there are any issues.  <\/p>\n<p>-Rohit<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":4.8,
        "Solution_reading_time":2.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2,
        "Solution_word_count":32,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm new to Azure ML so I have very little knowledge of this service..    <br \/>\nI've built a dummy regression model using automl package and now I'm trying to deploy it.    <br \/>\nI looked up some docs and followed a tutorial I found to deploy the model and I'm getting some errors..    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/157189-image.png?platform=QnA\" alt=\"157189-image.png\" \/> &lt;- this is the error I'm currently getting    <br \/>\nI think there is a problem with my score.py so I'm attaching the photo here as well.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/157272-image.png?platform=QnA\" alt=\"157272-image.png\" \/>    <\/p>\n<p>and this is the output i need to print out through the model..    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/157242-image.png?platform=QnA\" alt=\"157242-image.png\" \/>    <\/p>\n<p>I'd appreciate it much if somebody could give me some help     <\/p>\n<p>thank you    <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639419111370,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/662007\/getting-an-error-when-trying-to-deploy-azure-ml-mo",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":12.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"getting an error when trying to deploy azure ml model",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":129,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello,     <\/p>\n<p>Thanks for reaching out to us. From the above error it looks like the package did not install successfully. A more detailed procedure to install the SDK is available directly in the documentation: <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/install?view=azure-ml-py\">https:\/\/learn.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/install?view=azure-ml-py<\/a>    <\/p>\n<p>How to set up the environment: <a href=\"https:\/\/github.com\/Azure\/azureml-examples\/tree\/main\/python-sdk\/tutorials\/automl-with-azureml#3-setup-a-new-conda-environment\">https:\/\/github.com\/Azure\/azureml-examples\/tree\/main\/python-sdk\/tutorials\/automl-with-azureml#3-setup-a-new-conda-environment<\/a>    <\/p>\n<p>You can test if you have set the env correct by below code:    <\/p>\n<pre><code>import azureml.core  \n  \nprint(&quot;This notebook was created using version 1.35.0 of the Azure ML SDK.&quot;)  \nprint(&quot;You are currently using version&quot;, azureml.core.VERSION, &quot;of the Azure ML SDK.&quot;)  \nassert (  \n    azureml.core.VERSION &gt;= &quot;1.35&quot;  \n), &quot;Please upgrade the Azure ML SDK by running '!pip install --upgrade azureml-sdk' then restart the kernel.&quot;  \n<\/code><\/pre>\n<p>There are some prerequisites to deploy models:     <\/p>\n<ul>\n<li> An Azure Machine Learning workspace. For more information, see Create an Azure Machine Learning workspace. <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-workspace\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-workspace<\/a>     <\/li>\n<li> A model. The examples in this article use a pre-trained model.    <\/li>\n<li> The Azure Machine Learning software development kit (SDK) for Python. <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/intro\">https:\/\/learn.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/intro<\/a>    <\/li>\n<li> A machine that can run Docker, such as a compute instance. <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-manage-compute-instance\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-manage-compute-instance<\/a>    <\/li>\n<\/ul>\n<p>More information please refer to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python#prerequisites\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python#prerequisites<\/a>    <\/p>\n<p>Hope this will help. Please let us know if any further queries.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/543261\/index.html\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":11,
        "Solution_readability":15.3,
        "Solution_reading_time":47.21,
        "Solution_score_count":1.0,
        "Solution_sentence_count":31,
        "Solution_word_count":293,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1605283363407,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":31.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":289.2941775,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>This seems to be a tricky thing to do, as I haven't found too much documentation for it. I'm trying to deploy a Huggingface pre-trained model for NLU to a SageMaker endpoint. Naturally, I don't want to do this manually, I'd like to automate it through CloudFormation. I found a somewhat <a href=\"https:\/\/faun.pub\/mastering-the-mystical-art-of-model-deployment-part-2-deploying-amazon-sagemaker-endpoints-with-cf9539dc2579\" rel=\"nofollow noreferrer\">useful article<\/a> on how to deploy, but the name of the training model is confusing and I don't know where I would find the right name for the model I want to deploy or where I would put that name (I want to deploy an <a href=\"https:\/\/huggingface.co\/sentence-transformers\/all-MiniLM-L6-v2\" rel=\"nofollow noreferrer\">all-MiniLM-L6-v2<\/a> model).<\/p>\n<p>Is this possible to do? Do I need to deploy a container? If so, how do I set up the container to process requests and return the text embeddings from the model? I've looked into doing this with just a lambda (which would satisfy the automated deployment process), but the packages I need to use greatly exceed the 250MB limit for lambda+layers.<\/p>\n<p>How do I deploy an endpoint from CloudFormation? Does anyone have experience doing this? If so, please share your wisdom.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658934967237,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73140531",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":9.9,
        "Challenge_reading_time":17.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Deploy Pre-Trained model to SageMaker Endpoint from CloudFormation",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":103.0,
        "Challenge_word_count":193,
        "Platform":"Stack Overflow",
        "Poster_created_time":1605283363407,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>To anyone curious, this is how I ended up solving this issue:<\/p>\n<p>I ran a Jupyter notebook locally to create the model artifact. Once complete, I zipped the model artifact into a tar.gz file.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoModel, AutoTokenizer\nfrom os import makedirs\n\nsaved_model_dir = 'saved_model_dir'\nmakedirs(saved_model_dir, exist_ok=True)\n\n# models were obtained from https:\/\/huggingface.co\/models\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers\/multi-qa-MiniLM-L6-cos-v1')\nmodel = AutoModel.from_pretrained('sentence-transformers\/multi-qa-MiniLM-L6-cos-v1')\n\ntokenizer.save_pretrained(saved_model_dir)\nmodel.save_pretrained(saved_model_dir)\n<\/code><\/pre>\n<pre class=\"lang-bash prettyprint-override\"><code>cd saved_model_dir &amp;&amp; tar czvf ..\/model.tar.gz *\n<\/code><\/pre>\n<p>I included a script in my pipeline to then upload that artifact to S3.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>aws s3 cp path\/to\/model.tar.gz s3:\/\/bucket-name\/prefix\n<\/code><\/pre>\n<p>I also created a CloudFormation template that would stand up my SageMaker resources. The tricky part of this was finding a container image to use, and a colleague was able to point me to <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">this repo<\/a> that contained a massive list of AWS-maintained container images for deep learning and inference. From there, I just needed to select the one that fit my needs.<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>Resources:\n  SageMakerModel:\n    Type: AWS::SageMaker::Model\n    Properties:\n      PrimaryContainer:\n        Image: 763104351884.dkr.ecr.us-east-1.amazonaws.com\/pytorch-inference:1.12.0-cpu-py38-ubuntu20.04-sagemaker # image resource found at https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\n        Mode: SingleModel\n        ModelDataUrl: s3:\/\/path\/to\/model.tar.gz\n      ExecutionRole: \n      ModelName: inference-model\n\n  SageMakerEndpointConfig:\n    Type: AWS::SageMaker::EndpointConfig\n    Properties:\n      EndpointConfigName: endpoint-config-name\n      ProductionVariants:\n        - ModelName: inference-model\n          InitialInstanceCount: 1\n          InstanceType: ml.t2.medium\n          VariantName: dev\n  \n  SageMakerEndpoint:\n    Type: AWS::SageMaker::Endpoint\n    Properties:\n      EndpointName: endpoint-name\n      EndpointConfigName: !GetAtt SageMakerEndpointConfig.EndpointConfigName\n<\/code><\/pre>\n<p>Once the PyTorch model is created locally, this solution essentially automates the process of provisioning and deploying a SageMaker endpoint for inference. If I need to switch the model, I just need to run my notebook code and it will overwrite my existing model artifact. Then I can redeploy and my pipeline will re-upload the artifact to S3, modify the existing SageMaker resources, and the solution will begin operating using the new model.<\/p>\n<p>This may not be the most elegant solution out there, so any suggestions or pointers would be much appreciated!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1659976426276,
        "Solution_link_count":3,
        "Solution_readability":17.0,
        "Solution_reading_time":39.05,
        "Solution_score_count":2.0,
        "Solution_sentence_count":21,
        "Solution_word_count":297,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The results can be written to SQL Azure using the writer module in the experiment but after publishing the web service the output comes in the Json Structure and it doesn't go to the writer module <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1430890566837,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1431065815883,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/30068341",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":4.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"How to write the results of Azure ML web service to the azure sql database (The output of Azure ML web service is in Json structure)",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":990.0,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403541426412,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, India",
        "Poster_reputation_count":191.0,
        "Poster_view_count":22.0,
        "Solution_body":"<p>Don't set output port and use Batch execution service - details are provided here - <a href=\"http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-publish-a-machine-learning-web-service\/\" rel=\"nofollow\">Publish web service<\/a> and <a href=\"http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-consume-web-services\/\" rel=\"nofollow\">consume web service<\/a><\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":28.8,
        "Solution_reading_time":5.51,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3,
        "Solution_word_count":24,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1285219808283,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Perth WA, Australia",
        "Answerer_reputation_count":6770.0,
        "Answerer_view_count":1127.0,
        "Challenge_adjusted_solved_time":0.0213536111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm currently trying to integrate an ML model currently deployed as a webservice on AzureML with PowerBI.<\/p>\n<p>I see that it can be <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#invoking-the-azure-ml-model-in-power-bi\" rel=\"nofollow noreferrer\">integrated<\/a> but the model requires the addition of a schema file when it is <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#schema-discovery-for-machine-learning-models\" rel=\"nofollow noreferrer\">being deployed as a webservice<\/a>. Without this, the model can't be viewed in PowerBI.<\/p>\n<p>The problem that I have come up against is that I use MLflow to log ML model performances and subsequently to deploy a selected model onto AzureML as a webservice using MLflow's AzureML integration - mlflow.azureml.deploy(). This unfortunately doesn't have the option to define a schema file before the model is deployed, thus resulting in no model being available in PowerBI as it lacks the required schema file.<\/p>\n<p>My options seem to be:<\/p>\n<ol>\n<li>Find a workaround, possibly using the working <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/mlflow\/model-serving\" rel=\"nofollow noreferrer\">REST api of the model in a power query<\/a>.<\/li>\n<li>Rewrite the deployment code and handle the webservice deployment steps in Azure instead of MLflow.<\/li>\n<\/ol>\n<p>I thought I would ask to see if I am maybe missing something as I can't find a workaround using my current code to define a schema file in MLflow when deploying with mlflow.azureml.deploy().<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600261190477,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1600855880503,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63920599",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":21.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"PowerBI and MLflow integration (through AzureML)",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":405.0,
        "Challenge_word_count":204,
        "Platform":"Stack Overflow",
        "Poster_created_time":1600260166047,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Point number 2 is the way we solved this issue. Instead of using MLflow to deploy to a scoring service on Azure, we wrote a custom code which load MLflow model when container is initialised.<\/p>\n<p>Scoring code is something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nfrom mlflow.pyfunc import load_model\n\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n\ndef init():\n    global model\n    model = load_model(os.path.join(os.environ.get(&quot;AZUREML_MODEL_DIR&quot;), &quot;awesome_model&quot;))\n\n@input_schema('data', NumpyParameterType(input_sample))\n@output_schema(NumpyParameterType(output_sample))\n\ndef run(data):\n    return model.predict(data)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1600855957376,
        "Solution_link_count":0,
        "Solution_readability":16.9,
        "Solution_reading_time":10.68,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7,
        "Solution_word_count":75,
        "Tool":"MLflow"
    }
]