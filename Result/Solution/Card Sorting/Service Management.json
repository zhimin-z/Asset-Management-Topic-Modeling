[
    {
        "Answerer_created_time":1435766573232,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":10645.0,
        "Answerer_view_count":1173.0,
        "Challenge_adjusted_solved_time":13.1109433333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I try to use a trained model from Microsoft Azure Machine Learning Studio in Azure Stream Analytics.\nBefore I start work with my IoT-Stream sensor data, I try this sample: \n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/stream-analytics\/stream-analytics-machine-learning-integration-tutorial\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/stream-analytics\/stream-analytics-machine-learning-integration-tutorial<\/a><\/p>\n\n<p>I can deploy the web service and it works fine with a console application.\nThe result from web service:<\/p>\n\n<pre><code>{\n    \"Results\": {\n        \"output1\": {\n            \"type\": \"table\",\n            \"value\": {\n                \"ColumnNames\": [\"Sentiment\", \"Score\"],\n                \"ColumnTypes\": [\"String\", \"Double\"],\n                \"Values\": [\n                    [\"neutral\", \"0.564501523971558\"]\n                ]\n            }\n        }\n    }\n}\n<\/code><\/pre>\n\n<p>The T-SQL in Stream Analytics from tutorial looks like:<\/p>\n\n<pre><code>WITH subquery AS (  \n    SELECT text, sentiment(text) as result from input  \n)  \n\nSelect text, result.[Scored Labels]  \nInto output  \nFrom subquery\n<\/code><\/pre>\n\n<p>Unfortunately it does not work. Can someone explain <code>result.[Scored Labels]<\/code><\/p>\n\n<p>Is it possible to debug my Stream Analytic job?\nI get no output. No result-file, no warning, no exception...<\/p>",
        "Challenge_closed_time":1481449233936,
        "Challenge_comment_count":0,
        "Challenge_created_time":1481402034540,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use a trained model from Microsoft Azure Machine Learning Studio in Azure Stream Analytics to work with IoT-Stream sensor data. They have followed a tutorial and deployed the web service, which works fine with a console application. However, the T-SQL in Stream Analytics from the tutorial does not work, and the user is unable to get any output or debug their Stream Analytic job. They are seeking an explanation for the term \"result.[Scored Labels]\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41080045",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.6,
        "Challenge_reading_time":16.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":13.1109433333,
        "Challenge_title":"How can I use ML function in Azure Stream Analytics?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":752.0,
        "Challenge_word_count":135,
        "Platform":"Stack Overflow",
        "Poster_created_time":1400791038563,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":2437.0,
        "Poster_view_count":234.0,
        "Solution_body":"<p>It is not currently possible to test your query when you use a function to call out to Azure ML. The test query functionality runs in the web browser window so I guess they haven't implemented that feature yet. <\/p>\n\n<p>I expect if you start the job it will actually work. However you may need to change <code>result.[Scored Labels]<\/code> to match the columns in the Azure ML API output by saying <code>result.Sentiment<\/code> and <code>result.Score<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":5.73,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":74.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1526004205792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"China",
        "Answerer_reputation_count":28087.0,
        "Answerer_view_count":3298.0,
        "Challenge_adjusted_solved_time":16.0070080556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm having the following error while trying to submit an Azure ML Studio pipeline<\/p>\n<p><code>Get credentials or pull docker image failed with err: error response from daemon: get https:\/\/lgcrmldev.azurecr.io\/v2\/azureml\/azureml_977f5bda2f6f4f634482661c121c8959\/manifests\/latest: unauthorized: authentication required, visit https:\/\/aka.ms\/acr\/authorization for more information.<\/code><\/p>\n<p>The notebook python code I'm doing is something on these lines:<\/p>\n<pre><code># create a Python script to do the actual work and save it in the pipeline folder:\n\n%%writefile $experiment_folder\/batch_online_retail.py\nimport os\nimport numpy as np\nfrom azureml.core import Model\nimport joblib\n\n\n# Called when the service is loaded\ndef init():\n    global model\n    \n    # Load the model\n    model_path = Model.get_model_path('Random_Forest_model')\n    model = joblib.load(model_path)\n\ndef run(batch):\n    try:\n        result = []\n        \n    # Process each line\n    for in range (len(batch)):\n        # Read the comma-delimited data into an array\n        data = np.genfromtxt(f, delimiter=',')        \n        # Reshape into a 2-dimensional array for prediction (model expects multiple items)\n        prediction = model.predict(data.reshape(1, -1))        \n        # Append prediction to results\n        resultList.append(&quot;{}: {}&quot;.format(os.path.basename(f), prediction[0]))\n    return resultList      \n<\/code><\/pre>\n<pre><code># Creating the run context\nfrom azureml.core import Environment\nfrom azureml.core.runconfig import DEFAULT_CPU_IMAGE\nfrom azureml.core.runconfig import CondaDependencies\n\n# Add dependencies required by the model\n# For scikit-learn models, you need scikit-learn\n# For parallel pipeline steps, you need azureml-core and azureml-dataprep[fuse]\ncd = CondaDependencies.create(conda_packages=['scikit-learn','pip'],\n                              pip_packages=['azureml-defaults','azureml-core','azureml-dataprep[fuse,pandas]'])\n\nbatch_env = Environment(name='batch_environment')\nbatch_env.python.conda_dependencies = cd\nbatch_env.docker.enabled = True\nbatch_env.docker.base_image = DEFAULT_CPU_IMAGE\nprint('Configuration ready.')\n\n<\/code><\/pre>\n<pre><code># Creating the ParallelRunStep\nfrom azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\nfrom azureml.pipeline.core import PipelineData\n\ndefault_ds = ws.get_default_datastore()\n\noutput_dir = PipelineData(name='inferences', \n                          datastore=default_ds, \n                          output_path_on_compute='online-retail\/results')\n\nparallel_run_config = ParallelRunConfig(\n    source_directory=experiment_folder,\n    entry_script=&quot;batch_online_retail.py&quot;,\n    mini_batch_size=&quot;5&quot;,\n    error_threshold=10,\n    output_action=&quot;append_row&quot;,\n    environment=batch_env,\n    compute_target=inference_cluster,\n    node_count=2)\n\nparallelrun_step = ParallelRunStep(\n    name='batch-score-retail',\n    parallel_run_config=parallel_run_config,\n    inputs=[batch_data_set.as_named_input('online_retail_batch')],\n    output=output_dir,\n    arguments=[],\n    allow_reuse=True\n)\n\nprint('Steps defined')\n<\/code><\/pre>\n<p>and finally,<\/p>\n<pre><code># Create an Azure ML experiment in your workspace, put the step into a pipeline and run it\nfrom azureml.core import Experiment\nfrom azureml.pipeline.core import Pipeline\n\npipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\npipeline_run = Experiment(ws, 'online-retail-deployment-cf').submit(pipeline)\npipeline_run.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p>It's in this final step that I keep getting the error above.<\/p>\n<p>My Container Registry has my user and Azure ML resource as a Contributor in the access control panel, so I don't think it's lack of permissions.<\/p>\n<p>I've found this Microsoft page that seems to have a fix for the error I'm having:\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/container-registry\/container-registry-faq#docker-push-succeeds-but-docker-pull-fails-with-error-unauthorized-authentication-required\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/container-registry\/container-registry-faq#docker-push-succeeds-but-docker-pull-fails-with-error-unauthorized-authentication-required<\/a><\/p>\n<p>But I don't understand how can I implement the suggested fix. This is because the Docker image the notebook uses is inside the Compute Instance created in Azure ML which we have limited access.<\/p>\n<p>Any ideas on what is the problem and how to fix it?<\/p>\n<p>Thank you in advance,\nCarla<\/p>",
        "Challenge_closed_time":1613725945092,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613668319863,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to submit an Azure ML Studio pipeline, specifically a container registry error related to unauthorized authentication required. The user has tried to fix the issue by granting access to the Container Registry, but the error persists. The user is seeking help to understand the problem and how to fix it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66264795",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":17.1,
        "Challenge_reading_time":57.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":16.0070080556,
        "Challenge_title":"Azure ML studio - Container Registry Error while trying to submit a pipeline",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":869.0,
        "Challenge_word_count":389,
        "Platform":"Stack Overflow",
        "Poster_created_time":1613665696487,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>According to the example <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-with-custom-image#define-your-environment\" rel=\"nofollow noreferrer\">here<\/a>, I think you need to configure the environment variables for the docker images stored in the Azure Container Registry:<\/p>\n<pre><code>batch_env = Environment(name='batch_environment')\nbatch_env.python.conda_dependencies = cd\nbatch_env.docker.enabled = True\n# Set the container registry information.\nbatch_env.docker.base_image_registry.address = &quot;myregistry.azurecr.io&quot;\nbatch_env.docker.base_image_registry.username = &quot;username&quot;\nbatch_env.docker.base_image_registry.password = &quot;password&quot;\nbatch_env.docker.base_image = &quot;myregistry.azurecr.io\/DEFAULT_CPU_IMAGE&quot;\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":24.0,
        "Solution_reading_time":10.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":47.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1507058830472,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":1619.9066602778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I used AutoML in Databricks Notebooks for a binary classification problem and the winning model flavor was XGBoost (big surprise).<\/p>\n<p>The outputted model is of this variety:<\/p>\n<pre><code>mlflow.pyfunc.loaded_model:\n      artifact_path: model\n      flavor: mlflow.sklearn\n      run_id: 123456789\n<\/code><\/pre>\n<p>Any idea why when I use <code>model.predict_proba(X)<\/code>, I get this response?<\/p>\n<p><code>AttributeError: 'PyFuncModel' object has no attribute 'predict_proba'<\/code><\/p>\n<p>I know it is possible to get the probabilities because ROC\/AUC is a metric used for tuning the model. Any help would be amazing!<\/p>",
        "Challenge_closed_time":1646744187860,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640912523883,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user used AutoML in Databricks Notebooks for a binary classification problem and the winning model flavor was XGBoost. However, when the user tried to use the predict_proba() function, they received an AttributeError stating that the 'PyFuncModel' object has no attribute 'predict_proba'. The user is seeking help to understand why this is happening and how to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70538098",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":8.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1619.9066602778,
        "Challenge_title":"Databricks MLFlow AutoML XGBoost can't predict_proba()",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":451.0,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Poster_created_time":1562828557216,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Francisco, CA, USA",
        "Poster_reputation_count":77.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>I had the same issue with catboost model.\nThe way I solved it was by saving the artifacts in a local dir<\/p>\n<pre><code>import os\nfrom mlflow.tracking import MlflowClient\nclient = MlflowClient()\nlocal_dir = &quot;\/dbfs\/FileStore\/user\/models&quot;\nlocal_path = client.download_artifacts('run_id', &quot;model&quot;, local_dir)```\n\n```model_path = '\/dbfs\/FileStore\/user\/models\/model\/model.cb'\nmodel = CatBoostClassifier()\nmodel = model.load_model(model_path)\nmodel.predict_proba(test_set)```\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.2,
        "Solution_reading_time":6.7,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":44.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1502816769670,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Darmstadt, Germany",
        "Answerer_reputation_count":1998.0,
        "Answerer_view_count":76.0,
        "Challenge_adjusted_solved_time":54.6594472223,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've deployed a custom model with an async endpoint. I want to process video files with it because videos can have ~5-10 minutes I can't load all frames to memory. Of course, I want to make an inference on each frame.\nI've written<br \/>\n<code>input_fn<\/code> - download video file from s3 using boto and creates generator which loads video frames with a given batch size - return a generator - written with OpenCV<br \/>\n<code>predict_fn<\/code> - iterate over generator batched frames and generate prediction using model - save prediction in list<br \/>\n<code>output_fn<\/code> - transform prediction into json format, gzip all to reduce the size<\/p>\n<p>Endpoint works well, but the problem is concurrency. The sagemaker endpoint processes request after request (from cloudwatch and s3 save file time). I don't know why this happens.\nmax_concurrent_invocations_per_instance is set to 1000. Other settings from PyTorch serving are as follows:<\/p>\n<pre><code>SAGEMAKER_MODEL_SERVER_TIMEOUT: 100000\nSAGEMAKER_TS_MAX_BATCH_DELAY: 10000\nSAGEMAKER_TS_BATCH_SIZE: 1000\nSAGEMAKER_TS_MAX_WORKERS: 4\nSAGEMAKER_TS_RESPONSE_TIMEOUT: 100000\n<\/code><\/pre>\n<p>And still, it doesn't work. So how can I create an async inference endpoint with PyTorch to get concurrency?<\/p>",
        "Challenge_closed_time":1653766025390,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653569251380,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has deployed a custom model with an async endpoint on Sagemaker to process video files with a given batch size. However, the Sagemaker endpoint processes requests one after the other, causing concurrency issues. The user has tried adjusting the settings, but it still doesn't work. The user is seeking advice on how to create an async inference endpoint with PyTorch to achieve concurrency.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72392070",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.4,
        "Challenge_reading_time":16.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":54.6594472223,
        "Challenge_title":"Sagemaker doesn't inference in an async manner",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":109.0,
        "Challenge_word_count":173,
        "Platform":"Stack Overflow",
        "Poster_created_time":1495477835623,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Rzeszow, Poland",
        "Poster_reputation_count":148.0,
        "Poster_view_count":68.0,
        "Solution_body":"<p>The concurrency settings for TorchServe DLC are controlled by such mechanisms as # of workers, which can be set by defining the appropriate variables, such as <code>SAGEMAKER_TS_*<\/code>, and <code>SAGEMAKER_MODEL_*<\/code> (see, e.g., <a href=\"https:\/\/github.com\/pytorch\/serve\/blob\/master\/docs\/configuration.md\" rel=\"nofollow noreferrer\">this page<\/a> for details on their meaning and implications).<\/p>\n<p>While the latter are agnostic to any particular serving stack and are defined in the <a href=\"https:\/\/github.com\/aws\/sagemaker-inference-toolkit\" rel=\"nofollow noreferrer\">SageMaker Inference Toolkit<\/a>, the former are TorchServe-specific and are defined in <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\" rel=\"nofollow noreferrer\">TorchServe Inference Toolkit<\/a>. Moreover, since the TorchServe Inference Toolkit is built on top of the SageMaker Inference Toolkit, there is a non-trivial interplay between these two sets of params.<\/p>\n<p>Thus you may also want to experiment with such params as, e.g., <code>SAGEMAKER_MODEL_SERVER_WORKERS<\/code> to properly set up the concurrency setting of the SageMaker Async Endpoint.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":15.4,
        "Solution_reading_time":15.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":128.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1488711530187,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":29.0,
        "Answerer_view_count":12.0,
        "Challenge_adjusted_solved_time":21.4251527778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using a Classic Web Service with a non-default endpoint for a Update Resource activity on the Azure Data Factory. This is the error I get:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/shK0R.png\" rel=\"nofollow noreferrer\">Screenshot of Error<\/a><\/p>\n\n<p>I didn't find any info on the web and couldn't figure it out myself. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/data-factory\/data-factory-azure-ml-update-resource-activity\" rel=\"nofollow noreferrer\">This<\/a> website shows an example that I used by just filling in my values for mlEndpoint, apiKey and updateRessourceEndpoint:<\/p>\n\n<pre><code>{\n    \"name\": \"updatableScoringEndpoint2\",\n    \"properties\": {\n        \"type\": \"AzureML\",\n        \"typeProperties\": {\n            \"mlEndpoint\": \"https:\/\/ussouthcentral.services.azureml.net\/workspaces\/xxx\/services\/--scoring experiment--\/jobs\",\n            \"apiKey\": \"endpoint2Key\",\n            \"updateResourceEndpoint\": \"https:\/\/management.azureml.net\/workspaces\/xxx\/webservices\/--scoring experiment--\/endpoints\/endpoint2\"\n        }\n    }\n}\n<\/code><\/pre>\n\n<p>There is no mention of a token that needs to be passed...<\/p>",
        "Challenge_closed_time":1503393687923,
        "Challenge_comment_count":0,
        "Challenge_created_time":1503316557373,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while using a Classic Web Service with a non-default endpoint for an Update Resource activity on the Azure Data Factory. The error message is displayed in the provided screenshot, and the user has not been able to find any information on the web or figure it out themselves. The user has followed an example from a website by filling in their values for mlEndpoint, apiKey, and updateResourceEndpoint, but there is no mention of a token that needs to be passed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45796489",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":14.9,
        "Challenge_reading_time":14.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":21.4251527778,
        "Challenge_title":"Azure Machine Learning: What error is this?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":253.0,
        "Challenge_word_count":103,
        "Platform":"Stack Overflow",
        "Poster_created_time":1476806455803,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Holzkirchen, Deutschland",
        "Poster_reputation_count":3068.0,
        "Poster_view_count":386.0,
        "Solution_body":"<p>this error is basically saying the apiKey you provided is invalid to perform the update resource operation. Here is some posts for your reference: <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/3bb77e37-8860-43c6-bcaa-d6ebd70617b8\/retrain-predictive-web-service-programmatically-when-do-not-have-access-to-managementazuremlnet?forum=MachineLearning\" rel=\"nofollow noreferrer\">https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/3bb77e37-8860-43c6-bcaa-d6ebd70617b8\/retrain-predictive-web-service-programmatically-when-do-not-have-access-to-managementazuremlnet?forum=MachineLearning<\/a><\/p>\n\n<p>Please also be noted that if you modified your linked service in ADF, remember to re-deploy the pipeline as well to reflect your change in time.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":25.1,
        "Solution_reading_time":10.4,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.1845747222,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\nCan I use serverless inference as a pricing model for selling a SageMaker Model Package on the AWS Marketplace?",
        "Challenge_closed_time":1673186904128,
        "Challenge_comment_count":0,
        "Challenge_created_time":1673171839659,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of using serverless inference as a pricing model for selling a SageMaker Model Package on the AWS Marketplace.",
        "Challenge_last_edit_time":1673517930936,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUcZwxBuy-SROI7OF3-NFslA\/sagemaker-serverless-on-aws-marketplace",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.3,
        "Challenge_reading_time":1.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":4.1845747222,
        "Challenge_title":"SageMaker Serverless on AWS Marketplace?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":58.0,
        "Challenge_word_count":24,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"No, quoting from SageMaker documentation:\n\n\n```\n\u2026features currently available for SageMaker Real-time Inference are not supported for Serverless Inference, including GPUs, AWS marketplace model packages\u2026\n```\n\nReference: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints.html#serverless-endpoints-how-it-works-exclusions",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1673186904128,
        "Solution_link_count":1.0,
        "Solution_readability":28.8,
        "Solution_reading_time":4.52,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":93.4837152778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>So, I created an experiment, based on one of many examples and picked a dataset that required some transformations, columns choosing and categorical features. The model creation worked just fine, with only smaller hiccups. However as I deployed the webservice, the API is requesting the new column data set (created after the feature transformation) and not the original data set. This does not serve my purpose, as my aim for creating a service that would adhere to the original dataset features.  <\/p>\n<p>What am I doing wrong? Or shall I be required to implement the data transformation. Also, the feature I am trying to predict is also showing up as a input feature, and that does not make sense.  <\/p>\n<p>Any pointers?  <\/p>",
        "Challenge_closed_time":1602859603928,
        "Challenge_comment_count":4,
        "Challenge_created_time":1602523062553,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue while deploying an Azure ML classic webservice. The API is requesting the new column data set instead of the original data set, which does not serve the user's purpose. The user is unsure if they are doing something wrong or if they need to implement data transformation. Additionally, the feature they are trying to predict is showing up as an input feature, which does not make sense. The user is seeking pointers to resolve these issues.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/123854\/azure-ml-classic-webservices-deploy-error-when-doi",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":9.0,
        "Challenge_reading_time":9.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":93.4837152778,
        "Challenge_title":"Azure ML classic webservices deploy - Error When Doing Test Request-Response",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":132,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>After further investigation, it has been determined that by design, studio (classic) will echo this error when using convert to indicator values transformation, hence, we suggest not using convert to indicator values block in the inference pipeline. However, in <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/convert-to-indicator-values\">designer<\/a>, indicator values transformation is saved so that this module can be used in the inference pipeline. Sorry for the inconvenience, but hope this helps!    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.6,
        "Solution_reading_time":7.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1406731060412,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Washington, USA",
        "Answerer_reputation_count":139.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":406.3169388889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>SageMaker provides a full machine learning development environment on AWS. It works with the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/\" rel=\"nofollow noreferrer\">Amazon SageMaker Python SDK<\/a>, which allows Jupyter Notebooks to interact with the functionality. This also provides the path to using the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_featurestore.html\" rel=\"nofollow noreferrer\">Amazon SageMaker Feature Store<\/a>.<\/p>\n<p>Is there any REST API available for SageMaker? Say one wanted to create their own custom UI, but still use SageMaker features, is this possible?<\/p>\n<p>Can it be done using the <a href=\"https:\/\/aws.amazon.com\/api-gateway\/\" rel=\"nofollow noreferrer\">Amazon API Gateway<\/a>?<\/p>",
        "Challenge_closed_time":1626469400220,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625006659240,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the availability of a REST API for SageMaker and whether it is possible to interact with SageMaker through the Amazon API Gateway to create a custom UI while still utilizing SageMaker features.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68186468",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":12.0,
        "Challenge_reading_time":11.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":406.3169388889,
        "Challenge_title":"Is there a REST API available for SageMaker, or is it possible to interact with SageMaker over the Amazon API Gateway?",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1164.0,
        "Challenge_word_count":102,
        "Platform":"Stack Overflow",
        "Poster_created_time":1346443720088,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":11650.0,
        "Poster_view_count":977.0,
        "Solution_body":"<p>Amazon API Gateway currently does not provide first-class integration for SageMaker. But you can use these services via AWS SDK. If you wish, you can embed the AWS SDK calls into a service, host on AWS (e.g. running on EC2 or as lambda functions) and use API gateway to expose your REST API.<\/p>\n<p>Actually, SageMaker is not fundamentally different from any other AWS service from this aspect.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.3,
        "Solution_reading_time":4.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1395422283667,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1411.0,
        "Answerer_view_count":45.0,
        "Challenge_adjusted_solved_time":0.6555202778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed an AzureML published experiment with deployed web service. I tried to use the <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-consume-web-services\/\" rel=\"nofollow\">sample code provided in the configuration page<\/a>, but universal apps do not implement Http.Formatting yet, thus I couldn't use <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/hh944521(v=vs.118).aspx\" rel=\"nofollow\">postasjsonasync<\/a>.<\/p>\n\n<p>I tried to follow the sample code as much as possible, but I'm getting statuscode of 415 \"Unsupported Media Type\", What's the mistake I'm doing?<\/p>\n\n<pre><code>var client = new HttpClient();\nclient.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Bearer\", apiKey);\n\/\/ client.BaseAddress = uri;\n\nvar scoreRequest = new\n{\n            Inputs = new Dictionary&lt;string, StringTable&gt;() {\n                    {\n                        \"dataInput\",\n                        new StringTable()\n                        {\n                            ColumnNames = new [] {\"Direction\", \"meanX\", \"meanY\", \"meanZ\"},\n                            Values = new [,] {  { \"\", x.ToString(), y.ToString(), z.ToString() },  }\n                        }\n                    },\n                },\n            GlobalParameters = new Dictionary&lt;string, string&gt;() { }\n };\n var stringContent = new StringContent(scoreRequest.ToString());\n HttpResponseMessage response = await client.PostAsync(uri, stringContent);\n<\/code><\/pre>\n\n<p>Many Thanks<\/p>",
        "Challenge_closed_time":1452007973623,
        "Challenge_comment_count":0,
        "Challenge_created_time":1452005613750,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while trying to send a request as JSON on UWP for an AzureML published experiment with a deployed web service. The sample code provided in the configuration page is not working as universal apps do not implement Http.Formatting yet, resulting in a status code of 415 \"Unsupported Media Type\". The user is seeking help to identify the mistake in the code.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34614582",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":16.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.6555202778,
        "Challenge_title":"Send request as Json on UWP",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":3194.0,
        "Challenge_word_count":117,
        "Platform":"Stack Overflow",
        "Poster_created_time":1352139399460,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cyprus",
        "Poster_reputation_count":820.0,
        "Poster_view_count":256.0,
        "Solution_body":"<p>You'll need to serialize the object to a JSON string (I recommend using NewtonSoft.Json to make it easier) and set the content type accordingly. Here's an implementation I'm using in my UWP apps (note that <code>_client<\/code> is an <code>HttpClient<\/code>):<\/p>\n\n<pre><code>    public async Task&lt;HttpResponseMessage&gt; PostAsJsonAsync&lt;T&gt;(Uri uri, T item)\n    {\n        var itemAsJson = JsonConvert.SerializeObject(item);\n        var content = new StringContent(itemAsJson);\n        content.Headers.ContentType = new MediaTypeHeaderValue(\"application\/json\");\n\n        return await _client.PostAsync(uri, content);\n    }\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.2,
        "Solution_reading_time":7.86,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":62.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1251699930780,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Santa Clara, CA, United States",
        "Answerer_reputation_count":1538.0,
        "Answerer_view_count":198.0,
        "Challenge_adjusted_solved_time":33.9835508333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been exploring using Vertex AI for my machine learning workflows. Because deploying different models to the same endpoint utilizing only one node is not possible in Vertex AI, I am considering a <a href=\"https:\/\/stackoverflow.com\/questions\/69878915\/deploying-multiple-models-to-same-endpoint-in-vertex-ai\">workaround<\/a>. With this workaround, I will be unable to use many Vertex AI features, like model monitoring, feature attribution etc., and it simply becomes, I think, a managed alternative to running the prediction application on, say, a GKE cluster. So, besides the cost difference, I am exploring if running the custom prediction container on Vertex AI vs. GKE will involve any limitations, for example, only <strong>N1<\/strong> machine types are available for prediction in Vertex AI<\/p>\n<p>There is a similar <a href=\"https:\/\/stackoverflow.com\/questions\/67930882\/google-kubernetes-engine-vs-vertex-ai-ai-platform-unified-for-serving-model-pr\">question<\/a>, but I it does not raise the specific questions I hope to have answered.<\/p>\n<ul>\n<li>I am not sure of the available disk space. In Vertex AI, one can specify the machine type, such as n1-standard-2 etc., but I am not sure what disk space will be available and if\/how one can specify it? In the custom container code, I may copy multiple model artifacts, or data from outside sources to the local directory before processing them so understanding any disk space limitations is important.<\/li>\n<li>For custom training in Vertex AI, one can use an interactive shell to inspect the container where the training code is running, as described <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/monitor-debug-interactive-shell\" rel=\"nofollow noreferrer\">here<\/a>. Is something like this possible for a custom prediction container? I have not found anything in the docs.<\/li>\n<li>For custom training, one can use a private IP for custom training as described <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-private-ip\" rel=\"nofollow noreferrer\">here<\/a>. Again, I have not found anything similar for custom prediction in the docs, is it possible?<\/li>\n<\/ul>\n<p>If you know of any other possible limitations, please post.<\/p>",
        "Challenge_closed_time":1637122254800,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636999914017,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is exploring using Vertex AI for their machine learning workflows but is facing challenges with deploying different models to the same endpoint. They are considering a workaround but it will limit their use of Vertex AI features. The user is exploring if running the custom prediction container on Vertex AI vs. GKE will involve any limitations, such as available disk space, interactive shell access, and private IP usage. The user is seeking information on any other possible limitations.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69978953",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":29.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":33.9835508333,
        "Challenge_title":"Vertex AI custom prediction vs Google Kubernetes Engine",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":304.0,
        "Challenge_word_count":299,
        "Platform":"Stack Overflow",
        "Poster_created_time":1471292986790,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":700.0,
        "Poster_view_count":90.0,
        "Solution_body":"<ol>\n<li>we don't specify a disk size, so default to 100GB<\/li>\n<li>I'm not aware of this right now. But if it's a custom container, you could just run it locally or on GKE for debugging purpose.<\/li>\n<li>are you looking for this? <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/using-private-endpoints\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/using-private-endpoints<\/a><\/li>\n<\/ol>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.5,
        "Solution_reading_time":5.7,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":46.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1501163272143,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":118.0,
        "Challenge_adjusted_solved_time":1.6257427778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I wonder if it's possible to export my model to a json file, so I can do some kind of versioning.<\/p>\n\n<p>Building up a model with Azure Machine Learning Studio is easy, but I need to save the previous version anytime I do an update.<\/p>\n\n<p>It's possible to do this?<\/p>",
        "Challenge_closed_time":1544666744367,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544660891693,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to export their Azure Machine Learning model to a JSON file for versioning purposes. They are seeking advice on whether this is possible or not.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53753367",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.7,
        "Challenge_reading_time":3.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.6257427778,
        "Challenge_title":"How to export my azure machine learning model to json",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":445.0,
        "Challenge_word_count":59,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458558039430,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Trieste, Province of Trieste, Italy",
        "Poster_reputation_count":1657.0,
        "Poster_view_count":164.0,
        "Solution_body":"<p>In Azure ML Studio, the versioning is available as Run History: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/manage-experiment-iterations\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/manage-experiment-iterations<\/a>\nRegards,\nJaya<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":30.3,
        "Solution_reading_time":4.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.2104608333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,  <br \/>\nI am using the example provided in the Machine Learning Studio Docs for extracting Health Entities from a given string.  <br \/>\nThe code is shown below.  <\/p>\n<p>My question is: what is the easiest way to <strong>convert the output result into JSON format<\/strong>?  <\/p>\n<pre><code>from azure.core.credentials import AzureKeyCredential\nfrom azure.ai.textanalytics import TextAnalyticsClient\nimport json\n\ncredential = AzureKeyCredential(&quot;**********************************&quot;)\nendpoint=&quot;https:\/\/eastus.api.cognitive.microsoft.com\/&quot;\n\ntext_analytics_client = TextAnalyticsClient(endpoint, credential)\n\ndocuments = [&quot;Subject is taking 100mg of ibuprofen twice daily&quot;]\n\npoller = text_analytics_client.begin_analyze_healthcare_entities(documents)\nresult = poller.result()\n\ndocs = [doc for doc in result if not doc.is_error]\n\nprint(&quot;Results of Healthcare Entities Analysis:&quot;)\nfor idx, doc in enumerate(docs):\n    for entity in doc.entities:\n        print(&quot;Entity: {}&quot;.format(entity.text))\n        print(&quot;...Normalized Text: {}&quot;.format(entity.normalized_text))\n        print(&quot;...Category: {}&quot;.format(entity.category))\n        print(&quot;...Subcategory: {}&quot;.format(entity.subcategory))\n        print(&quot;...Offset: {}&quot;.format(entity.offset))\n        print(&quot;...Confidence score: {}&quot;.format(entity.confidence_score))\n        if entity.data_sources is not None:\n            print(&quot;...Data Sources:&quot;)\n            for data_source in entity.data_sources:\n                print(&quot;......Entity ID: {}&quot;.format(data_source.entity_id))\n                print(&quot;......Name: {}&quot;.format(data_source.name))\n        if entity.assertion is not None:\n            print(&quot;...Assertion:&quot;)\n            print(&quot;......Conditionality: {}&quot;.format(entity.assertion.conditionality))\n            print(&quot;......Certainty: {}&quot;.format(entity.assertion.certainty))\n            print(&quot;......Association: {}&quot;.format(entity.assertion.association))\n        for relation in doc.entity_relations:\n            print(&quot;Relation of type: {} has the following roles&quot;.format(relation.relation_type))\n        for role in relation.roles:\n            print(&quot;...Role '{}' with entity '{}'&quot;.format(role.name, role.entity.text))\n    print(&quot;------------------------------------------&quot;)\n<\/code><\/pre>",
        "Challenge_closed_time":1650982690652,
        "Challenge_comment_count":2,
        "Challenge_created_time":1650967532993,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to extract Health Entities from a given string using the example provided in the Machine Learning Studio Docs. They are looking for the easiest way to convert the output result into JSON format. The code provided uses Azure Text Analytics API to analyze healthcare entities and print the results.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/826603\/converting-textanalytics-result-to-json-format",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":21.2,
        "Challenge_reading_time":30.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":4.2104608333,
        "Challenge_title":"Converting textanalytics result to JSON Format",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":167,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=c7e28bb4-3bff-4f66-9bdf-9c63a80436b1\">@KA  <\/a> The result does not seem to be directly serializable to JSON. I found a library <a href=\"https:\/\/pypi.org\/project\/jsons\/\">JSONS<\/a> that can do the heavy lifting if you are using python 3.5 or higher.     <\/p>\n<p>Install jsons    <\/p>\n<pre><code>pip install jsons  \n<\/code><\/pre>\n<p>Import JSONS and using jsons.dump() on docs object.    <\/p>\n<pre><code>import jsons #import in the import section  \nprint(jsons.dump(docs)) #Printing the json after docs is created  \n<\/code><\/pre>\n<p>This should give a file of this format in this case. Uploaded the file in .txt format since JSON files cannot be uploaded on Q&amp;A, download the file and rename it to .json     <br \/>\nI hope this helps!!    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n<p><a href=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/196624-health.txt?platform=QnA\">196624-health.txt<\/a>    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":10.3,
        "Solution_reading_time":16.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":135.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1337759214688,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pune India",
        "Answerer_reputation_count":1036.0,
        "Answerer_view_count":124.0,
        "Challenge_adjusted_solved_time":810.2160711111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have implemented machine learning algorithms through sagemaker.<\/p>\n\n<p>I have installed SDK for .net, and tried by executing below code.<\/p>\n\n<pre><code>Uri sagemakerEndPointURI = new Uri(\"https:\/\/runtime.sagemaker.us-east-2.amazonaws.com\/endpoints\/MyEndpointName\/invocations\");\nAmazon.SageMakerRuntime.Model.InvokeEndpointRequest request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest();\nrequest.EndpointName = \"MyEndpointName\";\nAmazonSageMakerRuntimeClient aawsClient = new AmazonSageMakerRuntimeClient(myAwsAccessKey,myAwsSecreteKey);            \nAmazon.SageMakerRuntime.Model.InvokeEndpointResponse resposnse= aawsClient.InvokeEndpoint(request);\n<\/code><\/pre>\n\n<p>By executing this, I am getting validation error as \"<code>1 validation error detected: Value at 'body' failed to satisfy constraint: Member must not be null<\/code>\"<\/p>\n\n<p>Can anyone guide me on how and what more input data I need to pass to call the given API?<\/p>\n\n<p>EDIT<\/p>\n\n<p>Further I'd tried by provinding body parameter which contains a MemoryStream written by a '.gz' or '.pkl' file, and it giving me error as : \"Error unmarshalling response back from AWS,  HTTP content length exceeded 5246976 bytes.\"<\/p>\n\n<p>EDIT 1\/23\/2018<\/p>\n\n<p>Further I came up with the error message as <\/p>\n\n<blockquote>\n  <p>ERROR - model server - 'TypeError' object has no attribute 'message'<\/p>\n<\/blockquote>\n\n<p>Thanks<\/p>",
        "Challenge_closed_time":1519637555372,
        "Challenge_comment_count":4,
        "Challenge_created_time":1516531050743,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to call a Sagemaker training model endpoint API in C# using the SDK for .net. However, they are encountering a validation error stating that the member must not be null. They have tried passing a MemoryStream written by a '.gz' or '.pkl' file, but it is giving them an error stating that the HTTP content length exceeded 5246976 bytes. They have also encountered an error message stating that the 'TypeError' object has no attribute 'message'.",
        "Challenge_last_edit_time":1516720777516,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48365866",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":14.2,
        "Challenge_reading_time":18.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":862.9179525,
        "Challenge_title":"How to call Sagemaker training model endpoint API in C#",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":2093.0,
        "Challenge_word_count":153,
        "Platform":"Stack Overflow",
        "Poster_created_time":1337759214688,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune India",
        "Poster_reputation_count":1036.0,
        "Poster_view_count":124.0,
        "Solution_body":"<p>Later solved it by <code>Encoding.ASCII.GetBytes<\/code>as in below code.<\/p>\n\n<pre><code> byte[] bytes = System.IO.File.ReadAllBytes(@\"EXCEL_FILE_PATH\");\n    string listA = \"\";\n    while (!reader.EndOfStream)\n        {\n            var line = reader.ReadLine();\n            listA = listA + line + \"\\n\";\n        }\n    byte[] bytes = Encoding.ASCII.GetBytes(listA);\n    request.Body = new MemoryStream(bytes);\n    InvokeEndpointResponse response = sagemakerRunTimeClient.InvokeEndpoint(request);\n    string predictions = Encoding.UTF8.GetString(response.Body.ToArray());\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.8,
        "Solution_reading_time":6.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":36.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":57.5133555556,
        "Challenge_answer_count":2,
        "Challenge_body":"I've configured a model for async-inference, and its working correctly - I can submit a file via `invoke_endpoint_async` and download the output from s3.\n\nI'm now trying to configure auto-scaling. I'm trying experimentation with different options, but basically I want to configure 0-1 instances, have an instance created when`invoke_endpoint_async` is called, and have the instance shutdown shortly afterwards (along the lines of batch inference)\n\nI'm struggling to get it to work - I'm experiencing similar issues to https:\/\/github.com\/boto\/boto3\/issues\/2839\n\nFirst I think there's an issue with the `console` - if I  `aws register-scalable-target ...` it works but the console doesn't like the zero for `min-capacity`\n\n![Enter image description here](\/media\/postImages\/original\/IMWZdtU68ZSXSSvr46-_1nhw)\n\nI think this is just a UI nit though, I don't understand how the policy works - I have\n\n```json\n{\n    \"TargetValue\": 1.0,\n    \"CustomizedMetricSpecification\": {\n        \"MetricName\": \"ApproximateBacklogSizePerInstance\",\n        \"Namespace\": \"AWS\/SageMaker\",\n        \"Dimensions\": [{\"Name\": \"EndpointName\", \"Value\": \"***-test-endpoint-2023-03-24-04-28-06-341\"}],\n        \"Statistic\": \"Average\"\n    },\n    \"ScaleInCooldown\": 60,\n    \"ScaleOutCooldown\": 60\n}\n```\n\nThe first point of confusion was the console shows a built-in and custom policy. I was initially using the name of the built-in policy (SageMakerEndpointInvocationScalingPolicy) but `put-scaling-policy` doesn't appear to edit it - it creates a new policy with the same name.\n\nWhen I monitor the scaling activity ()\n\n```console\naws application-autoscaling describe-scaling-activities \\\n    --service-namespace sagemaker\n```\n\nI can initially see \"Successfully set desired instance count to 0. Change successfully fulfilled by sagemaker.\" \n\nBut when I involve the endpoint with \n\n```python\nresponse = sm_runtime.invoke_endpoint_async(\n    EndpointName=endpoint_name, \n    InputLocation=\"***\/input\/data.json\",\n    ContentType='application\/jsonlines',\n    Accept='application\/jsonlines')\n\noutput_location = response['OutputLocation']\n```\n\nI would expect to see the instance count increase to 1, then back to zero within a space of a few minutes. I have occasionally got it to do something but not reliably. I think the main issue is I don't understand the metric and how it interacts with the target.\n\nI've seen charts but I cannot figure out how to plot the \"ApproximateBacklogSizePerInstance\"? And how does it interact with \"TargetValue\"? What is the actual trigger for a scale in\/out?",
        "Challenge_closed_time":1679845259536,
        "Challenge_comment_count":0,
        "Challenge_created_time":1679638211456,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to configure auto-scaling for a model in SageMaker async-inference. They want to configure 0-1 instances, have an instance created when `invoke_endpoint_async` is called, and have the instance shutdown shortly afterwards. However, they are struggling to get it to work and are experiencing issues with the console and the policy. They are confused about how the metric interacts with the target and what triggers a scale in\/out.",
        "Challenge_last_edit_time":1679985798951,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUT4xru2SdTSqtoNyt1XV3VA\/configuring-auto-scaling-for-sagemaker-async-inference",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":14.4,
        "Challenge_reading_time":32.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":57.5133555556,
        "Challenge_title":"Configuring auto-scaling for sagemaker async-inference",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":190.0,
        "Challenge_word_count":310,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"A target tracking scaling policy will create 2 CloudWatch alarms (one for high and one for low usage), which you'll be able to see in the CloudWatch alarms console.  The high usage policy needs to have 3 consecutive 60 second breaching datapoints to trigger a scale-out; and the low alarm needs 15 consecutive 60 second breaching datapoints to scale-in\n\nYou may instead want to use step scaling policies, where you are able to create and control the alarms as well as the policy settings\nhttps:\/\/docs.aws.amazon.com\/autoscaling\/application\/userguide\/application-auto-scaling-step-scaling-policies.html",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1679845259536,
        "Solution_link_count":1.0,
        "Solution_readability":21.0,
        "Solution_reading_time":7.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":85.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1477948823888,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":721.0,
        "Answerer_view_count":57.0,
        "Challenge_adjusted_solved_time":50.7645952778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an AWS Sagemaker inference endpoint with autoscaling enabled with SageMakerVariantInvocationsPerInstance target metric. When I send a lot of requests to the endpoint the number of instances correctly scales out to the maximum instance count. But after I stop sending the requests the number of instances doesn't scale in to 1, minimum instance count. I waited for many hours. Is there a reason for this behaviour?<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1608300540056,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608117787513,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has encountered an issue with AWS Sagemaker inference endpoint where the autoscaling feature does not scale in to the minimum instance count of 1 after a period of inactivity, despite scaling out correctly when there are many requests. The user is seeking an explanation for this behavior.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65322286",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":6.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":50.7645952778,
        "Challenge_title":"AWS Sagemaker inference endpoint doesn't scale in with autoscaling",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1028.0,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Poster_created_time":1382978984190,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1311.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>AutoScaling requires a cloudwatch alarm to trigger to scale in.  Sagemaker doesn't push 0 value metrics when there's no activity (it just doesn't push anything).  This leads to the alarm being put into insufficient data and not triggering the autoscaling scale in action when your workload suddenly ends.<\/p>\n<p>Workarounds are either:<\/p>\n<ol>\n<li>Have a step scaling policy using the cloudwatch metric math FILL() function for your scale in.  This way you can tell CloudWatch &quot;if there's no data, pretend this was the metric value when evaluating the alarm.  This is only possible with step scaling since target tracking creates the alarms for you (and AutoScaling will periodically recreate them, so if you make manual changes they'll get deleted)<\/li>\n<li>Have scheduled scaling set the size back down to 1 every evening<\/li>\n<li>Make sure traffic continues at a low level for some times<\/li>\n<\/ol>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.4,
        "Solution_reading_time":11.24,
        "Solution_score_count":5.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":142.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":51.6131461111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to use Azure ML to host an image classification model trained in lobe.ai (externally trained model).     <\/p>\n<p>I've used the 'no code' model deployment approach described <a href=\"https:\/\/learn.microsoft.com\/en-gb\/azure\/machine-learning\/how-to-deploy-no-code-deployment\">here<\/a>     <\/p>\n<p>I've been able to authenticate my workspace and register my TensorFlow model, but the endpoint is stuck on transitioning for over 2 hours.     <\/p>\n<p>Any ideas?    <\/p>\n<pre><code>from azureml.core import Model  \n  \nmodel = Model.register(workspace=ws,  \n                       model_name='cxr',                            # Name of the registered model in your workspace.  \n                       model_path='cxr_test',                       # Local Tensorflow SavedModel folder to upload and register as a model.  \n                       model_framework=Model.Framework.TENSORFLOW,  # Framework used to create the model.  \n                       model_framework_version='1.15.3',            # Version of Tensorflow used to create the model.  \n                       description='Pneumonia-prediction model')  \n  \nservice_name = 'tensorflow-cxr-service'  \nservice = Model.deploy(ws, service_name, [model])  \n<\/code><\/pre>",
        "Challenge_closed_time":1605125180896,
        "Challenge_comment_count":5,
        "Challenge_created_time":1604939373570,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue with Azure ML where the endpoint for their image classification model is stuck in a 'transitioning' state for over 2 hours. They have used the 'no code' model deployment approach and have successfully authenticated their workspace and registered their TensorFlow model.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/156439\/endpoint-stuck-in-transitioning-state",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":11.2,
        "Challenge_reading_time":13.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":51.6131461111,
        "Challenge_title":"Endpoint stuck in 'transitioning' state",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":114,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>We have created a support ticket for this issue and we will update the solution later. Thanks.  <\/p>\n<p>Regards,  <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.2,
        "Solution_reading_time":1.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":21.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3402777778,
        "Challenge_answer_count":1,
        "Challenge_body":"If I deploy a SageMaker model, am I incurring hosting charges even while no one is accessing my model?",
        "Challenge_closed_time":1592313864000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592312639000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unsure if they will be charged for hosting their SageMaker model even when it is not being accessed by anyone.",
        "Challenge_last_edit_time":1668454706295,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUlNS8ujYmQqePwWS-mgso3Q\/sagemaker-model-spend",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.6,
        "Challenge_reading_time":1.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":0.3402777778,
        "Challenge_title":"SageMaker Model Spend",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":151.0,
        "Challenge_word_count":21,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"When you deploy a SageMaker model, it deploys it behind a SageMaker endpoint for real-time inference. You are charged by the second for on-demand ML hosting. Check the model deployment section of each region on the [SageMaker Pricing page][1]. In some use cases, you can save on inference cost by hosting several models behind the same endpoint (check [this blog post][2]).\n\n\n  [1]: https:\/\/aws.amazon.com\/sagemaker\/pricing\/?nc1=h_ls\n  [2]: https:\/\/aws.amazon.com\/fr\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1612926624243,
        "Solution_link_count":2.0,
        "Solution_readability":13.1,
        "Solution_reading_time":7.23,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":65.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1631803441500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"mexico",
        "Answerer_reputation_count":1258.0,
        "Answerer_view_count":685.0,
        "Challenge_adjusted_solved_time":26.1469897223,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to un-deploy model from an endpoint following <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform.Endpoint#google_cloud_aiplatform_Endpoint_undeploy\" rel=\"nofollow noreferrer\">this documentation<\/a>.<\/p>\n<pre><code>Endpoint.undeploy(deployed_model_id=model_id)\n<\/code><\/pre>\n<p>I even tried <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.endpoints\/undeployModel\" rel=\"nofollow noreferrer\">google api<\/a>. Same Issue with this as well.<\/p>\n<p>Getting 404 error<\/p>\n<blockquote>\n<p>The Deployed Model with ID <code>2367889687867<\/code> is missing.<\/p>\n<\/blockquote>\n<p><strong>INFO:<\/strong><\/p>\n<ol>\n<li>Both model and Endpoint are in same region.<\/li>\n<li>There is a single model deployed in the endpoint with <code>traffic_percentage=100<\/code>.<\/li>\n<\/ol>",
        "Challenge_closed_time":1655314821510,
        "Challenge_comment_count":4,
        "Challenge_created_time":1655220692347,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to un-deploy a model from an endpoint using GCP AI Platform Vertex and Google API, but is encountering a 404 error stating that the deployed model with the given ID is missing. The model and endpoint are in the same region and there is only one model deployed with a traffic percentage of 100.",
        "Challenge_last_edit_time":1655954014260,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72619696",
        "Challenge_link_count":2,
        "Challenge_participation_count":5,
        "Challenge_readability":16.2,
        "Challenge_reading_time":12.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":26.1469897223,
        "Challenge_title":"GCP AI Platform Vertex endpoint model undeploy : 404 The DeployedModel with ID `2367889687867` is missing",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":271.0,
        "Challenge_word_count":80,
        "Platform":"Stack Overflow",
        "Poster_created_time":1550779047856,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":363.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>The <code>deployed_model_id<\/code> is different from the <code>model_id<\/code>.That\u2019s why you are getting the Error 404, it is searching for something that is not the same.<\/p>\n<p>You can get the <code>deployed_model_id<\/code> by:<\/p>\n<ul>\n<li>list_models()<\/li>\n<li>list()<\/li>\n<\/ul>\n<p>Using <code>list_models()<\/code> brings you a list of all the deployed models ids, while using <code>list()<\/code> only brings one, you can add filters such as <code>display_name<\/code>, <code>model_id<\/code>, <code>region<\/code>, etc.<\/p>\n<pre><code>list(\n    filter= \u2018display_name= \u201cdisplay_name\u201d\u2019,\n)\n<\/code><\/pre>\n<p>You also can get the <code>deployed_model_id<\/code> using the Cloud SDK.<\/p>\n<pre><code>gcloud ai models list --region=$REGION --filter=&quot;DISPLAY_NAME: $NAME&quot; | grep &quot;MODEL_ID&quot; | cut -f2 -d: | sed 's\/\\s\/\/'\n<\/code><\/pre>\n<p>Additionally, you can specify the <code>deployed_model_id<\/code> when you are deploying your model using Cloud SDK the command should look like:<\/p>\n<pre><code>gcloud ai endpoints deploy-model $endpoint --project=$project --region=$region --model=$model_id --display-name=$model_name --deployed-model-id=$deployed_model_id\n<\/code><\/pre>\n<p>There are some flags that are required when you deploy a model such as endpoint, project, region, model and display name. And there are others that are <a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/ai\/endpoints\/deploy-model#:%7E:text=the%20uploaded%20model.-,OPTIONAL%20FLAGS,-%2D%2Daccelerator%3D%5Bcount\" rel=\"nofollow noreferrer\">optional flags<\/a> that you can use deployed_model_id is one of them.(I don\u2019t know if this is possible but you could set the deployed_model_id as the same as the model_id).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.7,
        "Solution_reading_time":22.36,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":182.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1340833876128,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":751.0,
        "Answerer_view_count":73.0,
        "Challenge_adjusted_solved_time":1.1074488889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to call an Azure Machine Learning Pipeline Endpoint I've set up using C# &amp; the Machine Learning REST api.<\/p>\n<p>I am certain that I have the Service Principal configured correctly, as I can successfully authenticate &amp; hit the endpoint using the <code>azureml-core<\/code> python sdk:<\/p>\n<pre><code>sp = ServicePrincipalAuthentication(\n    tenant_id=tenant_id,\n    service_principal_id=service_principal_id,\n    service_principal_password=service_principal_password)\nws =Workspace.get(\n    name=workspace_name, \n    resource_group=resource_group, \n    subscription_id=subscription_id, \n    auth=sp)\n\nendpoint = PipelineEndpoint.get(ws, name='MyEndpoint')\nendpoint.submit('Test_Experiment')\n<\/code><\/pre>\n<p>I'm using the following example in C# to attempt to run my endpoint: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-pipelines#run-a-published-pipeline-using-c\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-pipelines#run-a-published-pipeline-using-c<\/a><\/p>\n<p>I'm attempting to fill <code>auth_key<\/code> with the following code:<\/p>\n<pre><code>var clientId = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_ID&quot;);\nvar clientSecret = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_SECRET&quot;);\nvar tenantId = Environment.GetEnvironmentVariable(&quot;AZURE_TENANT_ID&quot;);\n\nvar cred = new ClientSecretCredential(tenantId, clientId, clientSecret);\nvar auth_key = cred.GetToken(new Azure.Core.TokenRequestContext(new string[] {&quot;.default&quot; }));\n<\/code><\/pre>\n<p>I receive a 401 (unauthorized).<\/p>\n<p>What am I am doing wrong?<\/p>\n<ul>\n<li>UPDATE *<\/li>\n<\/ul>\n<p>I changed the 'scopes' param in the <code>TokenRequestContext<\/code> to look like:<\/p>\n<pre><code>var auth_key = cred.GetToken(new Azure.Core.TokenRequestContext(new string[] { &quot;http:\/\/DataTriggerApp\/.default&quot; }));\n<\/code><\/pre>\n<p><code>http:\/\/DataTriggerApp<\/code> is one of the <code>servicePrincipalNames<\/code> that shows up when i query my Service Principal from the azure CLI.<\/p>\n<p>Now, when I attempt to use the returned token to call the Machine Learning Pipeline Endpoint, I receive a 403 instead of a 401.  Maybe some progress?<\/p>",
        "Challenge_closed_time":1634160031172,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634153827710,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to call an Azure Machine Learning Pipeline Endpoint using C# and the Machine Learning REST API. They have configured the Service Principal correctly and can successfully authenticate and hit the endpoint using the azureml-core python SDK. However, when attempting to run the endpoint in C#, they receive a 401 unauthorized error. They have tried to fill the auth_key with the correct code but still receive the same error. They have updated the scopes param in the TokenRequestContext and now receive a 403 error instead of a 401.",
        "Challenge_last_edit_time":1634156473112,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69561386",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":15.7,
        "Challenge_reading_time":30.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":1.7231838889,
        "Challenge_title":"How do I use Service Principal authentication with an Azure Machine Learning Pipeline Endpoint in C#?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":752.0,
        "Challenge_word_count":204,
        "Platform":"Stack Overflow",
        "Poster_created_time":1340833876128,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":751.0,
        "Poster_view_count":73.0,
        "Solution_body":"<p>Ok, through a lot of trial-and-error I was able to come up with two ways of acquiring a token that allows me to hit my Azure Machine Learning Pipeline Endpoint through the REST api.  One uses Microsoft.Identity.Client &amp; one uses Azure.Identity.<\/p>\n<pre><code>using Microsoft.Identity.Client;\n\n...\n\npublic static async Task&lt;string&gt; GetAccessToken()\n{\n      var clientId = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_ID&quot;);\n      var clientSecret = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_SECRET&quot;);\n      var tenantId = Environment.GetEnvironmentVariable(&quot;AZURE_TENANT_ID&quot;);\n\n   \n      var app = ConfidentialClientApplicationBuilder.Create(clientId)\n                                                .WithClientSecret(clientSecret)                                                \n                                                .WithAuthority(AzureCloudInstance.AzurePublic, tenantId)\n                                                .Build();\n      var result = await app.AcquireTokenForClient(new string[] { &quot;https:\/\/ml.azure.com\/.default&quot; }).ExecuteAsync();\n      return result.AccessToken;\n}\n<\/code><\/pre>\n<p>Or:<\/p>\n<pre><code>using Azure.Identity;\n...\n\npublic static async Task&lt;string&gt; GetAccessToken()\n{\n      var clientId = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_ID&quot;);\n      var clientSecret = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_SECRET&quot;);\n      var tenantId = Environment.GetEnvironmentVariable(&quot;AZURE_TENANT_ID&quot;);\n\n\n      var cred = new ClientSecretCredential(tenantId, clientId, clientSecret);\n      var token =  await cred.GetTokenAsync(new Azure.Core.TokenRequestContext(new string[] { &quot;https:\/\/ml.azure.com\/.default&quot; }));\n      return token.Token;\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1634160459928,
        "Solution_link_count":2.0,
        "Solution_readability":22.1,
        "Solution_reading_time":20.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":107.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1343167997556,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":191.0,
        "Answerer_view_count":27.0,
        "Challenge_adjusted_solved_time":132.4537347222,
        "Challenge_answer_count":2,
        "Challenge_body":"<h2><strong>ASKING THIS HERE AT THE EXPLICIT REQUEST OF THE MICROSOFT AZURE SUPPORT TEAM.<\/strong><\/h2>\n\n<p>I've been attempting to call the MS Luis.ai <em>programmatic<\/em> API (bit.ly\/2iev01n) and have been receiving a 401 unauthorized response to every request. Here's a simple GET example: <code>https:\/\/api.projectoxford.ai\/luis\/v1.0\/prog\/apps\/{appId}\/entities?subscription-key={subscription_key}<\/code>.  <\/p>\n\n<p>I am providing my appId from the Luis.ai GUI (as specified by the API docs), here:<br>\n<img src=\"https:\/\/i.stack.imgur.com\/Cg2Fw.png\" alt=\"Luis.ai App Settings App Id\"><\/p>\n\n<p>I am providing my subscription key from Azure (as specified by the API docs), here:<br>\n<img src=\"https:\/\/i.stack.imgur.com\/GS2Fe.png\" alt=\"Azure Console\"><\/p>\n\n<p>The app ID and subscription key, sourced from above, are the exact same as what I'm using to hit the query API successfully (see note at bottom). My account is pay-as-you-go (not free).<\/p>\n\n<p><strong><em>Am I doing something wrong here? Is this API deprecated, moved, down, or out-of-sync with the docs?<\/em><\/strong><\/p>\n\n<p><strong>NOTE:<\/strong> I can manipulate my model through the online GUI but that approach will be far too manual for our business needs where our model will need to be programmatically updated as new business entities come into existence.  <\/p>\n\n<p><strong>NOTE:<\/strong> The programmatic API is different from the query API which has this request URL, which is working fine for me:<br>\n<code>https:\/\/api.projectoxford.ai\/luis\/v2.0\/apps\/{appId}?subscription-key={subscription_key}&amp;verbose=true&amp;q={utterance}<\/code>  <\/p>\n\n<p><strong>NOTE:<\/strong> There doesn't seem to be a Luis.ai programmatic API for v2.0--which is why the URLs from the query and programmatic APIs have different versions.  <\/p>",
        "Challenge_closed_time":1484669845332,
        "Challenge_comment_count":2,
        "Challenge_created_time":1484180085280,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering a 401 unauthorized response when attempting to call the Microsoft Luis.ai programmatic API, despite providing the correct app ID and subscription key. The user is unsure if they are doing something wrong or if the API is deprecated, moved, down, or out-of-sync with the documentation. The user needs to programmatically update their model, but manipulating it through the online GUI is too manual for their business needs. The programmatic API is different from the query API, which is working fine for the user. There doesn't seem to be a Luis.ai programmatic API for v2.0.",
        "Challenge_last_edit_time":1484193011887,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41603082",
        "Challenge_link_count":4,
        "Challenge_participation_count":4,
        "Challenge_readability":10.4,
        "Challenge_reading_time":23.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":136.0444588889,
        "Challenge_title":"401 Errors Calling the Microsoft Luis.ai Programmatic API",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1280.0,
        "Challenge_word_count":229,
        "Platform":"Stack Overflow",
        "Poster_created_time":1343167997556,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":191.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>Answering my own question here:<\/p>\n\n<p>I have found my LUIS.ai programmatic API key. It is found by:\nLUIS.ai dashboard -> username (upper-right) -> settings in dropdown -> Subscription Keys tab -> Programmatic API Key<\/p>\n\n<p>It was not immediately obvious since it's found nowhere else: not alongside any of the other key listings in cognitive services or the LUIS.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":4.63,
        "Solution_score_count":7.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":111.6768886111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have some data in S3 and I want to create a lambda function to predict the output with my deployed aws sagemaker endpoint then I put the outputs in S3 again. Is it necessary in this case to create an api gateway like decribed in this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">link<\/a> ? and in the lambda function what I have to put. I expect to put (where to find the data, how to invoke the endpoint, where to put the data) <\/p>\n\n<p>Thanks<\/p>",
        "Challenge_closed_time":1549874448016,
        "Challenge_comment_count":0,
        "Challenge_created_time":1549472411217,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user wants to create a lambda function to predict the output using their deployed AWS Sagemaker endpoint and store the outputs in S3. They are unsure if they need to create an API gateway and what to include in the lambda function. They expect to include information on where to find the data, how to invoke the endpoint, and where to put the data.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54558832",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":7.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":111.6768886111,
        "Challenge_title":"call sagemaker endpoint using lambda function",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":5259.0,
        "Challenge_word_count":87,
        "Platform":"Stack Overflow",
        "Poster_created_time":1502010899808,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tunis, Tunisia",
        "Poster_reputation_count":109.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>you definitely don't have to create an API in API Gateway. You can invoke the endpoint directly using the invoke_endpoint() API, passing the endpoint name, the content type, and the payload.<\/p>\n\n<p>For example:<\/p>\n\n<pre><code>import boto3\n\nendpoint_name = &lt;INSERT_ENDPOINT_NAME&gt;\nruntime = boto3.Session().client(service_name='sagemaker-runtime',region_name='us-east-1')\n\nresponse = runtime.invoke_endpoint(EndpointName=endpoint_name, ContentType='application\/x-image', Body=payload)\nprint(response['Body'].read())\n<\/code><\/pre>\n\n<p>More examples here using a Lambda function: <a href=\"https:\/\/medium.com\/@julsimon\/using-chalice-to-serve-sagemaker-predictions-a2015c02b033\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/@julsimon\/using-chalice-to-serve-sagemaker-predictions-a2015c02b033<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":20.6,
        "Solution_reading_time":10.89,
        "Solution_score_count":4.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1520413126203,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":37123.0,
        "Answerer_view_count":4058.0,
        "Challenge_adjusted_solved_time":29.5944916667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I deployed a web-service from an experiment in ML studio. I tested the API, and everything was working fine. I tested it in Postman. After 2 hours, I got an authentication error when I sent a request using the same API. So to resolve this, I republished my Web Service and got new authentication code, so the API is working fine for now. I have two questions:<\/p>\n\n<p>1) Does the primary key automatically expire after a while or by signing out from ML studio? \n2) What is the application of the second key in ML Studio APIs? Where do we need the second key? <\/p>",
        "Challenge_closed_time":1535450343787,
        "Challenge_comment_count":0,
        "Challenge_created_time":1535343803617,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user deployed a web-service from an experiment in ML Studio and tested the API successfully in Postman. However, after 2 hours, the user encountered an authentication error when sending a request using the same API. To resolve this, the user republished the Web Service and obtained a new authentication code. The user has two questions: 1) Does the primary key expire automatically or by signing out from ML studio? 2) What is the application of the second key in ML Studio APIs?",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52032535",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":7.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":29.5944916667,
        "Challenge_title":"Does the primary key of Web Service API in ML Studio expire?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":228.0,
        "Challenge_word_count":116,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501114346136,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Australia",
        "Poster_reputation_count":37.0,
        "Poster_view_count":8.0,
        "Solution_body":"<blockquote>\n  <p>1) Does the primary key automatically expire after a while or by signing out from ML studio?<\/p>\n<\/blockquote>\n\n<p>I could not find any limit of the primary key in the office docs. Per my test, my primary key does not expire more than two hours or sign out from ML studio.<\/p>\n\n<blockquote>\n  <p>2) What is the application of the second key in ML Studio APIs? Where do we need the second key?<\/p>\n<\/blockquote>\n\n<p>The second key is the same usage of the primary key, like a backup of the primary key. Also, the primary key equals the API key in the ML studio.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.7,
        "Solution_reading_time":6.93,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":104.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.0832477778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an Azure ML Real-time inference endpoint deployed ran for a month till yesterday. Today it is in the state of &quot;Failed&quot;.  <\/p>\n<p>I did create a new compute and did a new deployment in the same region EAST US and it failed again.  <\/p>\n<p>What's going? Is this just a problem for me or a general issue?  <\/p>\n<p>Thanks  <br \/>\n-Dali<\/p>",
        "Challenge_closed_time":1594953029752,
        "Challenge_comment_count":0,
        "Challenge_created_time":1594945530060,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user's Azure ML Real-time inference endpoint deployment in the EAST US region has failed, and a new deployment in the same region has also failed. The user is unsure if this is a general issue or specific to their account.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/48609\/azure-ml-enpoint-deployment-failed-east-us-region",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.8,
        "Challenge_reading_time":4.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":2.0832477778,
        "Challenge_title":"Azure ML Enpoint deployment failed EAST US region",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":70,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. I successfully deployed in the east us region. Please review the following <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-debug-pipelines\">troubleshooting guidelines<\/a>. Also check for any <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/service-health\/service-health-overview\">service\/resource health issues<\/a> that could be impacting your service. Let me know if you're still experiencing issues afterwards and please share the logs so we can investigate further. Thanks.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.6,
        "Solution_reading_time":7.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":55.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1500629225150,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5939.0,
        "Answerer_view_count":886.0,
        "Challenge_adjusted_solved_time":133.8006258333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to create a module for Sagemaker endpoints. There's an optional object variable called <code>async_inference_config<\/code>. If you omit it, the endpoint being deployed is synchronous, but if you include it, the endpoint deployed is asynchronous. To satisfy both of these usecases, the <code>async_inference_config<\/code> needs to be an optional block.<\/p>\n<p>I am unsure of how to make this block optional though.<br \/>\nAny guidance would be greatly appreciated. See example below of structure of the optional parameter.<\/p>\n<p><strong>Example:<\/strong><\/p>\n<pre><code>resource &quot;aws_sagemaker_endpoint_configuration&quot; &quot;sagemaker_endpoint_configuration&quot; {\n  count = var.create ? 1 : 0\n\n  name = var.endpoint_configuration_name\n  production_variants {\n    instance_type          = var.instance_type\n    initial_instance_count = var.instance_count\n    model_name             = var.model_name\n    variant_name           = var.variant_name\n  }\n  async_inference_config {\n    output_config {\n      s3_output_path = var.s3_output_path\n    }\n    client_config {\n      max_concurrent_invocations_per_instance = var.max_concurrent_invocations_per_instance\n    }\n  }\n  lifecycle {\n    create_before_destroy = true\n    ignore_changes        = [&quot;name&quot;]\n  }\n\n  tags = var.tags\n\n  depends_on = [aws_sagemaker_model.sagemaker_model]\n}\n<\/code><\/pre>\n<p><strong>Update:<\/strong> What I tried based on the below suggestion, which seemed to work<\/p>\n<pre><code>dynamic &quot;async_inference_config&quot; {\n    for_each = var.async_inference_config == null ? [] : [true]\n    content {\n      output_config {\n        s3_output_path = lookup(var.async_inference_config, &quot;s3_output_path&quot;, null)\n      }\n      client_config {\n        max_concurrent_invocations_per_instance = lookup(var.async_inference_config, &quot;max_concurrent_invocations_per_instance&quot;, null)\n      }\n    }\n  }\n<\/code><\/pre>",
        "Challenge_closed_time":1658387166296,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658386596203,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a module for Sagemaker endpoints with an optional object variable called \"async_inference_config\". The endpoint being deployed is synchronous if the variable is omitted, and asynchronous if it is included. The user is unsure of how to make this block optional and is seeking guidance. The user has provided an example of the structure of the optional parameter and an update on what they have tried based on a suggestion.",
        "Challenge_last_edit_time":1658442017223,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73061907",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":17.7,
        "Challenge_reading_time":23.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":0.1583591667,
        "Challenge_title":"Terraform - Optional Nested Variable",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":55.0,
        "Challenge_word_count":146,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443225809767,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Vancouver, BC, Canada",
        "Poster_reputation_count":2332.0,
        "Poster_view_count":560.0,
        "Solution_body":"<p>You could use a <code>dynamic<\/code> block [1] in combination with <code>for_each<\/code> meta-argument [2]. It would look something like:<\/p>\n<pre><code>dynamic &quot;async_inference_config&quot; {\n    for_each = var.s3_output_path != null &amp;&amp; var.max_concurrent_invocations_per_instance != null ? [1] : []\n    content {\n    output_config {\n      s3_output_path = var.s3_output_path\n    }\n    client_config {\n      max_concurrent_invocations_per_instance = var.max_concurrent_invocations_per_instance\n    }\n  }\n}\n<\/code><\/pre>\n<p>Of course, you could come up with a different variable, say <code>enable_async_inference_config<\/code> (probalby of type <code>bool<\/code>) and base the <code>for_each<\/code> on that, e.g.:<\/p>\n<pre><code>dynamic &quot;async_inference_config&quot; {\n    for_each = var.enable_async_inference_config ? [1] : []\n    content {\n    output_config {\n      s3_output_path = var.s3_output_path\n    }\n    client_config {\n      max_concurrent_invocations_per_instance = var.max_concurrent_invocations_per_instance\n    }\n  }\n}\n<\/code><\/pre>\n<hr \/>\n<p>[1] <a href=\"https:\/\/www.terraform.io\/language\/expressions\/dynamic-blocks\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/language\/expressions\/dynamic-blocks<\/a><\/p>\n<p>[2] <a href=\"https:\/\/www.terraform.io\/language\/meta-arguments\/for_each\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/language\/meta-arguments\/for_each<\/a><\/p>",
        "Solution_comment_count":13.0,
        "Solution_last_edit_time":1658923699476,
        "Solution_link_count":4.0,
        "Solution_readability":23.4,
        "Solution_reading_time":18.04,
        "Solution_score_count":2.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":82.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.5504875,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi, I'm going to use the canvas by connecting to S3.\nWhen using sagemaker canvas, should the canvas region and S3 region be the same?\nThank you.",
        "Challenge_closed_time":1658234155411,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658221373656,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring whether the SageMaker Canvas region and S3 region should be the same when using the Canvas to connect to S3.",
        "Challenge_last_edit_time":1667926015744,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QULHZtj6HwQReouXor72UuSg\/should-sagemaker-canvas-region-and-s3-region-be-the-same",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":2.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":3.5504875,
        "Challenge_title":"Should SageMaker Canvas region and S3 region be the same?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":112.0,
        "Challenge_word_count":36,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi, S3 does not have to be in the same region as SageMaker Canvas, but make sure your user has the correct permissions to access the bucket!",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1658234155411,
        "Solution_link_count":0.0,
        "Solution_readability":11.5,
        "Solution_reading_time":1.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":27.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1517578984080,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2090.0,
        "Answerer_view_count":163.0,
        "Challenge_adjusted_solved_time":1.8957083334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been playing with Amazon Sagemaker. They have amazing sample notebooks in different areas. However, for testing purposes, I want to create an endpoint that returns the result from a function. From what I have seen so far, my understanding is that we can deploy only models but I would like to clarify it.<\/p>\n\n<p>Let's say I want to invoke the endpoint and it should give me the square of the input value. So, I will first create a function:<\/p>\n\n<pre><code>def my_square(x):\n    return x**2\n<\/code><\/pre>\n\n<p>Can we deploy this simple function in Amazon Sagemaker?<\/p>",
        "Challenge_closed_time":1534153182207,
        "Challenge_comment_count":0,
        "Challenge_created_time":1534146357657,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user wants to know if it is possible to deploy a simple function, such as a square function, to Amazon Sagemaker as an endpoint for testing purposes. They are unsure if Sagemaker only allows for the deployment of models.",
        "Challenge_last_edit_time":1534368379667,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51817494",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":7.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.8957083334,
        "Challenge_title":"deploy a simple function to amazon sagemaker",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":216.0,
        "Challenge_word_count":101,
        "Platform":"Stack Overflow",
        "Poster_created_time":1429147641928,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2282.0,
        "Poster_view_count":264.0,
        "Solution_body":"<p>Yes this is possible but it will need some overhead:\nYou can pass your own docker images for training and inference to sagemaker.<\/p>\n\n<p>Inside this containers you can do anything you want including return your <code>my_square<\/code> function. Keep in mind that you have to write your own flask microservice including proxy and wsgi server(if needed).<\/p>\n\n<p>In my opinion <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">this example<\/a> is the most helpfull one.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.1,
        "Solution_reading_time":7.77,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":68.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1360013608220,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bolton, United Kingdom",
        "Answerer_reputation_count":65842.0,
        "Answerer_view_count":4569.0,
        "Challenge_adjusted_solved_time":4.4584758333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>After deploying an Azure ML model to a container instance, call to the model fails when using the code provided in the &quot;Consume&quot; section of the endpoint (Python and C#).<\/p>\n<p>I have trained a model in Azure Auto-ML and deployed the model to a container instance.<\/p>\n<p><strong>Now when I am try to use the Python code provided in the Endpoint's &quot;Consume&quot; section I get the following error:<\/strong><\/p>\n<pre><code>The request failed with status code: 502\nContent-Length: 55\nContent-Type: text\/html; charset=utf-8\nDate: Mon, 07 Mar 2022 12:32:07 GMT\nServer: nginx\/1.14.0 (Ubuntu)\nX-Ms-Request-Id: 768c2eb5-10f3-4e8a-9412-3fcfc0f6d648\nX-Ms-Run-Function-Failed: True\nConnection: close\n\n---------------------------------------------------------------------------\nJSONDecodeError Traceback (most recent call last)\n&lt;ipython-input-1-6eeff158e915&gt; in &lt;module&gt;\n48 # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n49 print(error.info())\n---&gt; 50 print(json.loads(error.read().decode(&quot;utf8&quot;, 'ignore')))\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/json\/init.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n352 parse_int is None and parse_float is None and\n353 parse_constant is None and object_pairs_hook is None and not kw):\n--&gt; 354 return _default_decoder.decode(s)\n355 if cls is None:\n356 cls = JSONDecoder\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/json\/decoder.py in decode(self, s, _w)\n337\n338 &quot;&quot;&quot;\n--&gt; 339 obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n340 end = _w(s, end).end()\n341 if end != len(s):\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/json\/decoder.py in raw_decode(self, s, idx)\n355 obj, end = self.scan_once(s, idx)\n356 except StopIteration as err:\n--&gt; 357 raise JSONDecodeError(&quot;Expecting value&quot;, s, err.value) from None\n358 return obj, end\n\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\n<\/code><\/pre>\n<p><strong>If I use C# code provided in the Endpoint's &quot;Consume&quot; section I get the following error:<\/strong><\/p>\n<pre><code>The request failed with status code: BadGateway\nConnection: keep-alive\nX-Ms-Request-Id: 5c3543cf-29ac-46a3-a9fb-dcb6a0041b08\nX-Ms-Run-Function-Failed: True\nDate: Mon, 07 Mar 2022 12:38:32 GMT\nServer: nginx\/1.14.0 (Ubuntu)\n\n'&lt;=' not supported between instances of 'str' and 'int'\n<\/code><\/pre>\n<p><strong>The Python code I am using:<\/strong><\/p>\n<pre><code> import urllib.request\n import json\n import os\n import ssl\n    \n def allowSelfSignedHttps(allowed):\n     # bypass the server certificate verification on client side\n     if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n         ssl._create_default_https_context = ssl._create_unverified_context\n    \n allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n    \n data = {\n     &quot;Inputs&quot;: {\n         &quot;data&quot;:\n         [\n             {\n                 &quot;SaleDate&quot;: &quot;2022-02-08T00:00:00.000Z&quot;,\n                 &quot;OfferingGroupId&quot;: &quot;0&quot;,\n                 &quot;week_of_year&quot;: &quot;7&quot;,\n                 &quot;month_of_year&quot;: &quot;2&quot;,\n                 &quot;day_of_week&quot;: &quot;1&quot;\n             },\n         ]\n     },\n     &quot;GlobalParameters&quot;: {\n         &quot;quantiles&quot;: &quot;0.025,0.975&quot;\n     }\n }\n    \n body = str.encode(json.dumps(data))\n    \n url = 'http:\/\/4a0427c2-30d4-477e-85f5-dfdfdfdfdsfdff623f.uksouth.azurecontainer.io\/score'\n api_key = '' # Replace this with the API key for the web service\n headers = {'Content-Type':'application\/json', 'Authorization':('Bearer '+ api_key)}\n    \n req = urllib.request.Request(url, body, headers)\n    \n try:\n     response = urllib.request.urlopen(req)\n    \n     result = response.read()\n     print(result)\n except urllib.error.HTTPError as error:\n     print(&quot;The request failed with status code: &quot; + str(error.code))\n    \n     # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n     print(error.info())\n     print(json.loads(error.read().decode(&quot;utf8&quot;, 'ignore')))\n<\/code><\/pre>\n<p><strong>The C# code I have tried<\/strong>:<\/p>\n<pre><code> using System;\n using System.Collections.Generic;\n using System.IO;\n using System.Net.Http;\n using System.Net.Http.Headers;\n using System.Text;\n using System.Threading.Tasks;\n using Newtonsoft.Json;\n    \n namespace MLModelAPICall\n {\n     class Program\n     {\n         static void Main(string[] args)\n         {\n             InvokeRequestResponseService().Wait();\n         }\n    \n         static async Task InvokeRequestResponseService()\n         {\n             var handler = new HttpClientHandler()\n             {\n                 ClientCertificateOptions = ClientCertificateOption.Manual,\n                 ServerCertificateCustomValidationCallback =\n                         (httpRequestMessage, cert, cetChain, policyErrors) =&gt; { return true; }\n             };\n             using (var client = new HttpClient(handler))\n             {\n                 \/\/ Request data goes here\n                 var scoreRequest = new\n                 {\n                     Inputs = new Dictionary&lt;string, List&lt;Dictionary&lt;string, string&gt;&gt;&gt;()\n                     {\n                         {\n                             &quot;data&quot;,\n                             new List&lt;Dictionary&lt;string, string&gt;&gt;()\n                             {\n                                 new Dictionary&lt;string, string&gt;()\n                                 {\n                                     {\n                                         &quot;SaleDate&quot;, &quot;2022-02-08T00:00:00.000Z&quot;\n                                     },\n                                     {\n                                         &quot;OfferingGroupId&quot;, &quot;0&quot;\n                                     },\n                                     {\n                                         &quot;week_of_year&quot;, &quot;7&quot;\n                                     },\n                                     {\n                                         &quot;month_of_year&quot;, &quot;2&quot;\n                                     },\n                                     {\n                                         &quot;day_of_week&quot;, &quot;1&quot;\n                                     }\n                                 }\n                             }\n                         }\n                     },\n                     GlobalParameters = new Dictionary&lt;string, string&gt;()\n                     {\n                         {\n                             &quot;quantiles&quot;, &quot;0.025,0.975&quot;\n                         }\n                     }\n                 };\n    \n    \n                 const string apiKey = &quot;&quot;; \/\/ Replace this with the API key for the web service\n                 client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(&quot;Bearer&quot;, apiKey);\n                 client.BaseAddress = new Uri(&quot;http:\/\/4a0427c2-30d4-477e-85f5-xxxxxxxxxxxxx.uksouth.azurecontainer.io\/score&quot;);\n    \n                 \/\/ WARNING: The 'await' statement below can result in a deadlock\n                 \/\/ if you are calling this code from the UI thread of an ASP.Net application.\n                 \/\/ One way to address this would be to call ConfigureAwait(false)\n                 \/\/ so that the execution does not attempt to resume on the original context.\n                 \/\/ For instance, replace code such as:\n                 \/\/      result = await DoSomeTask()\n                 \/\/ with the following:\n                 \/\/      result = await DoSomeTask().ConfigureAwait(false)\n    \n                 var requestString = JsonConvert.SerializeObject(scoreRequest);\n                 var content = new StringContent(requestString);\n    \n                 content.Headers.ContentType = new MediaTypeHeaderValue(&quot;application\/json&quot;);\n    \n                 HttpResponseMessage response = await client.PostAsync(&quot;&quot;, content);\n    \n                 if (response.IsSuccessStatusCode)\n                 {\n                     string result = await response.Content.ReadAsStringAsync();\n                     Console.WriteLine(&quot;Result: {0}&quot;, result);\n                 }\n                 else\n                 {\n                     Console.WriteLine(string.Format(&quot;The request failed with status code: {0}&quot;, response.StatusCode));\n    \n                     \/\/ Print the headers - they include the requert ID and the timestamp,\n                     \/\/ which are useful for debugging the failure\n                     Console.WriteLine(response.Headers.ToString());\n    \n                     string responseContent = await response.Content.ReadAsStringAsync();\n                     Console.WriteLine(responseContent);\n                     Console.ReadLine();\n                 }\n             }\n         }\n     }\n }\n<\/code><\/pre>\n<p>Could you please help me with this issue? I am not sure what do to if Microsoft's provided code is erroring out, don't know what else to do.<\/p>",
        "Challenge_closed_time":1646835298180,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646819247667,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error when trying to call an Azure ML model deployed to a container instance using the Python and C# code provided in the \"Consume\" section of the endpoint. The Python code returns a 502 error with a JSONDecodeError, while the C# code returns a BadGateway error with a message indicating a problem with the code. The user is seeking assistance in resolving the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71407308",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":95.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":58,
        "Challenge_solved_time":4.4584758333,
        "Challenge_title":"Azure ML model to a container instance, call to the model fails when using the code provided in the \"Consume\" section of the endpoint (Python and C#)",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":89.0,
        "Challenge_word_count":697,
        "Platform":"Stack Overflow",
        "Poster_created_time":1360013608220,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bolton, United Kingdom",
        "Poster_reputation_count":65842.0,
        "Poster_view_count":4569.0,
        "Solution_body":"<p>After much more digging I found out that the &quot;Consume&quot; scripts provided with the endpoint are wrong (Python and C#) .<\/p>\n<p>When making a call to the endpoint the GlobalParameters expects an integer value, but the provided scripts have wrapped the values in double quotes hence making it a string:<\/p>\n<pre><code> },\n &quot;GlobalParameters&quot;: {\n     &quot;quantiles&quot;: &quot;0.025,0.975&quot;\n }\n<\/code><\/pre>\n<p>If you are using Python to consume the model, when making call to the endpoint your GlobalParameters should be define as this:<\/p>\n<pre><code> },\n &quot;GlobalParameters&quot;: {\n     &quot;quantiles&quot;: [0.025,0.975]\n }\n<\/code><\/pre>\n<p>wrapped in square brackets<\/p>\n<blockquote>\n<p>[0.025,0.975]<\/p>\n<\/blockquote>\n<p>and not in double quotes &quot;<\/p>\n<blockquote>\n<p><em>I have also opened a ticket with microsoft so hopefully they will fix the code provided in the &quot;consume&quot; section of every endpoint<\/em><\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.8,
        "Solution_reading_time":12.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":120.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.4038472222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hallo, i would like make an appointment for Exam AI-900: Microsoft Azure AI Fundamentals.     <br \/>\nHowever this exam is currently not available at Pearson vue or Certiport. When can i expect this again? Is there an alternative ?<\/p>",
        "Challenge_closed_time":1662210933980,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662205880130,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to schedule a certification test for AI-900: Microsoft Azure AI Fundamentals as it is currently unavailable at Pearson vue or Certiport. They are seeking information on when it will be available again or if there is an alternative option.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/992629\/certification-test-for-ai-900-microsoft-azure-ai-f",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.1,
        "Challenge_reading_time":3.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.4038472222,
        "Challenge_title":"certification test for AI-900: Microsoft Azure AI Fundamentals not available",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":47,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi Jurian,    <\/p>\n<p>This is available in PearsonVue check this.  <a href=\"https:\/\/learn.microsoft.com\/en-us\/certifications\/exams\/ai-900\">ai-900<\/a>    <\/p>\n<p>Any specific region you are trying from?    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/237503-image.png?platform=QnA\" alt=\"237503-image.png\" \/>    <\/p>\n<p>==    <br \/>\nPlease &quot;Accept the answer&quot; if the information helped you. This will help us and others in the community as well.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.8,
        "Solution_reading_time":6.23,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":48.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":17.1430497222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We have built numerous diagnostic models which can be reduced to equations and code that will allow us to repeat the work. We have the code physically available to us, so it can be installed in our own software.  <\/p>\n<p>Now I would like to use artificial neural networks to build a prediction model. After I build that model, will I be able to take that model and transfer it to our own software environment? My concern is that the prediction model will just be a black box. Thanks<\/p>",
        "Challenge_closed_time":1591950718436,
        "Challenge_comment_count":1,
        "Challenge_created_time":1591889003457,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has built diagnostic models using code and equations that can be installed in their own software. They now want to build a prediction model using artificial neural networks and transfer it to their own software environment, but are concerned that the model will be a black box.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/34890\/access-to-neural-network-model",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.0,
        "Challenge_reading_time":6.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":17.1430497222,
        "Challenge_title":"Access to neural network model",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":93,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a>@OliverBathe-8330<\/a> Please follow the below Deployment scenarios. If possible can you please add more details about the use case.<\/p>\n<p>Option A: Use the DevOps pipeline integration to rollout to production Using same approach as in the <a href=\"https:\/\/github.com\/Microsoft\/MLOpsPython\">MLOps repo<\/a>, set up a <a href=\"https:\/\/github.com\/microsoft\/MLOpsPython\/blob\/master\/docs\/getting_started.md#set-up-build-release-trigger-and-release-multi-stage-pipeline\">release trigger for your DevOps release pipeline<\/a> listening from your dev workspace model registry but then deploy to your production workspace (requires registering again in Prod model registry, call model.deploy() in the Prod workspace<\/p>\n<p>Option B: Use the AML pipeline to rollout to production Following same example as above, add additional PythonScriptStep in your AML pipeline to register and deploy model in the Production workspace<\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where<\/a><\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":16.3,
        "Solution_reading_time":14.62,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":110.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1273856732636,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London",
        "Answerer_reputation_count":20277.0,
        "Answerer_view_count":615.0,
        "Challenge_adjusted_solved_time":1183.6423358333,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Permanent deletion of an experiment isn't documented anywhere. I'm using Mlflow w\/ backend postgres db<\/p>\n\n<p>Here's what I've run: <\/p>\n\n<pre><code>client = MlflowClient(tracking_uri=server)\nclient.delete_experiment(1)\n<\/code><\/pre>\n\n<p>This deletes the the experiment, but when I run a new experiment with the same name as the one I just deleted, it will return this error:<\/p>\n\n<pre><code>mlflow.exceptions.MlflowException: Cannot set a deleted experiment 'cross-sell' as the active experiment. You can restore the experiment, or permanently delete the  experiment to create a new one.\n<\/code><\/pre>\n\n<p>I cannot find anywhere in the documentation that shows how to permanently delete everything.<\/p>",
        "Challenge_closed_time":1585231513452,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580970401043,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in permanently deleting an experiment in Mlflow with a backend postgres db. Although the user has tried deleting the experiment using the MlflowClient, they are still unable to create a new experiment with the same name due to an error. The user is unable to find any documentation on how to permanently delete everything.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60088889",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":11.2,
        "Challenge_reading_time":9.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":20.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1183.6423358333,
        "Challenge_title":"How Do You \"Permanently\" Delete An Experiment In Mlflow?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":13984.0,
        "Challenge_word_count":100,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443225809767,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Vancouver, BC, Canada",
        "Poster_reputation_count":2332.0,
        "Poster_view_count":560.0,
        "Solution_body":"<p>Unfortunately it seems there is no way to do this via the UI or CLI at the moment :-\/<\/p>\n\n<p>The way to do it depends on the type of backend file store that you are using.<\/p>\n\n<p><strong>Filestore<\/strong>:<\/p>\n\n<p>If you are using the filesystem as a storage mechanism (the default) then it is easy. The 'deleted' experiments are moved to a <code>.trash<\/code> folder. You just need to clear that out:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>rm -rf mlruns\/.trash\/*\n<\/code><\/pre>\n\n<p>As of the current version of the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/cli.html#mlflow-experiments-delete\" rel=\"noreferrer\">documentation<\/a> (1.7.2), they remark:<\/p>\n\n<blockquote>\n  <p>It is recommended to use a cron job or an alternate workflow mechanism to clear <code>.trash<\/code> folder.<\/p>\n<\/blockquote>\n\n<p><strong>SQL Database:<\/strong><\/p>\n\n<p>This is more tricky, as there are dependencies that need to be deleted. I am using MySQL, and these commands work for me:<\/p>\n\n<pre class=\"lang-sql prettyprint-override\"><code>USE mlflow_db;  # the name of your database\nDELETE FROM experiment_tags WHERE experiment_id=ANY(\n    SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n);\nDELETE FROM latest_metrics WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n    )\n);\nDELETE FROM metrics WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n    )\n);\nDELETE FROM tags WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n    )\n);\nDELETE FROM runs WHERE experiment_id=ANY(\n    SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n);\nDELETE FROM experiments where lifecycle_stage=\"deleted\";\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.3,
        "Solution_reading_time":24.34,
        "Solution_score_count":22.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":220.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1499171495843,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bhubaneswar, Odisha, India",
        "Answerer_reputation_count":521.0,
        "Answerer_view_count":77.0,
        "Challenge_adjusted_solved_time":0.7756461111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>For example, I have A, B acounts.<\/p>\n<p>First, I log in Google Colab with A account.\nand I want to log in wandb with B acounts. ( using !wandb login )\nis it possible??<\/p>",
        "Challenge_closed_time":1644559750243,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644556957917,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to know if it is possible to log in to wandb with a different account while using Google Colab. They have two accounts, A and B, and have already logged in to Colab with account A. They want to know if they can log in to wandb with account B using the command \"!wandb login\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71075704",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.1,
        "Challenge_reading_time":2.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.7756461111,
        "Challenge_title":"how to login wandb with another acount using colab?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":316.0,
        "Challenge_word_count":40,
        "Platform":"Stack Overflow",
        "Poster_created_time":1644556763936,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>You can you the following commands to force a relogin:<\/p>\n<ul>\n<li>from terminal<\/li>\n<\/ul>\n<pre><code>wandb login --relogin\n<\/code><\/pre>\n<ul>\n<li>Using the API:<\/li>\n<\/ul>\n<pre><code>import wandb\nwandb.login(relogin=True)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.2,
        "Solution_reading_time":3.16,
        "Solution_score_count":2.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":27.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":25.7594230556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi, I trained and deployed a ML model via Auto ML. The result looks like this:  <br \/>\n&quot;\\&quot;{\\\\&quot;result\\\\&quot;: [\\\\&quot;Test\\\\&quot;]}\\&quot;&quot;<\/p>\n<p>Once I did the same with an endpoint created with the Azure ML Designer my result looks like this:  <br \/>\n&quot;{\\&quot;Results\\&quot;: {\\&quot;WebServiceOutput0\\&quot;: [{\\&quot;Scored Labels\\&quot;: \\&quot;Test\\&quot;}]}}&quot;<\/p>\n<p>Is there a way to configure the response that it looks similar to the AutoML response?<\/p>\n<p>Thanks :)<\/p>",
        "Challenge_closed_time":1606912260000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606819526077,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a challenge in configuring the response of a ML model deployed via AutoML and an endpoint created with Azure ML Designer. The user wants to know if there is a way to configure the response of the endpoint to look similar to the AutoML response.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/181635\/how-to-configute-webserviceoutput",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":7.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":25.7594230556,
        "Challenge_title":"How to configute WebServiceOutput?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":65,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=fe5bc84e-425f-4db0-a234-78d2a8fbbae1\">@ID_27051995  <\/a> Unfortunately, AutoML and AML Designer currently generates 2 different swagger format automatically, and there is no way to configure the output format. We are working on to address this inconsistency, and the Designer swagger format will be the converged format. Cheers!<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.6,
        "Solution_reading_time":4.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":44.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1435766573232,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":10645.0,
        "Answerer_view_count":1173.0,
        "Challenge_adjusted_solved_time":6.5547283333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is that possible to use machine learning methods from Microsoft Azure Machine Learning  as an API from my own code (without ML Studio) with possibility to calculate everything on their side?<\/p>",
        "Challenge_closed_time":1450317613532,
        "Challenge_comment_count":0,
        "Challenge_created_time":1450294016510,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of using Azure ML methods as an API from their own code without using ML Studio and with the ability to perform calculations on Azure's side.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34320449",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":2.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":6.5547283333,
        "Challenge_title":"Use Azure ML methods like an API",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":180.0,
        "Challenge_word_count":37,
        "Platform":"Stack Overflow",
        "Poster_created_time":1336227824220,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":834.0,
        "Poster_view_count":122.0,
        "Solution_body":"<p>You can <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-publish-a-machine-learning-web-service\/\" rel=\"nofollow\">publish<\/a> an experiment (machine learning functions you hooked together in Azure ML Studio) as an API. When you call that API in your custom code you give it your data and all the computation runs in the cloud in Azure ML. <\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.8,
        "Solution_reading_time":4.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":46.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3261111111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nLet's consider an ML edge inference use-case on Greengrass-managed device. The model is unique to each device, however its architecture and invocation logic are the same for all devices. In other words, the same invocation Lambda could be the same for all devices, only the model parameters would need to change across devices. We'd like to deploy a unique inference Lambda to all devices, and load device-specific artifact to each device.\n\nCan this be achieved with Greengrass ML Inference? It seems that GG MLI requires each model to be associated with a specific Lambda. \n\nOtherwise, is the recommended pattern to self-manage the inference in Lambda? E.g. by loading a specific model from S3 unique a local config file or some env variable?",
        "Challenge_closed_time":1605020138000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605018964000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in deploying N models on N Greengrass devices with a unique Lambda for inference logic. They are unsure if this can be achieved with Greengrass ML Inference as it requires each model to be associated with a specific Lambda. They are seeking recommendations on how to self-manage the inference in Lambda by loading a specific model from S3 unique a local config file or some env variable.",
        "Challenge_last_edit_time":1667926501335,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUlVJHC1NaTTOquvDqs444oQ\/how-to-deploy-n-models-on-n-greengrass-devices-with-a-unique-lambda-for-inference-logic",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":10.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.3261111111,
        "Challenge_title":"How to deploy N models on N Greengrass devices with a unique Lambda for inference logic?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":51.0,
        "Challenge_word_count":138,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"In IoT Greengrass 1.x, the configuration is unique to each Greengrass Group. This includes Connectors, Lambdas and ML Resources.\n\nThe same Lambda can be referenced by multiple groups as a Greengrass function, which is likely what you want. This is similar to using one of the GG ML connectors (Object Detection or Image Classification).\n\nIn addition to your inference code, you'll also need to configure an ML Resource, which has a local name and a remote model. The local name would be the same for all Greengrass Groups, but in each group you will refer to a different remote object (the model) - either S3 or SageMaker job.\n\nEvery time a model changes, you will need to redeploy the corresponding Greengrass group for the changes to be deployed locally.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1613588986552,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":9.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":128.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1588516515763,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"UK",
        "Answerer_reputation_count":29087.0,
        "Answerer_view_count":3080.0,
        "Challenge_adjusted_solved_time":0.0127177778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am new to Amazon SageMaker and I am closely following this tutorial <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/<\/a> to create a machine learning-powered REST API with Amazon API Gateway mapping templates and Amazon SageMaker<\/p>\n\n<p>when I run the following command on terminal (<strong>Step 2 of the Tutorial<\/strong> )<\/p>\n\n<pre><code>aws sagemaker-runtime invoke-endpoint \\\n  --endpoint-name &lt;endpoint-name&gt; \\\n  --body '{\"instances\": [{\"in0\":[863],\"in1\":[882]}]}' \\\n  --content-type application\/json \\\n  --accept application\/json \\\n  results\n<\/code><\/pre>\n\n<p>I get the following <strong>Error:<\/strong> <code>Invalid base64: \"{\"instances\": [{\"in0\":[863],\"in1\":[882]}]}\"<\/code>\nMy endpoint is <code>InService<\/code> on the SageMaker console and the example Jupyter notebook run successfully. (I also substituted <code>&lt;endpoint-name&gt;<\/code> with the actual name - same error received with\/without quotations around the name) <\/p>\n\n<p>Using <strong>zsh<\/strong> here is the aws cli version:<\/p>\n\n<pre><code>aws --version\naws-cli\/2.0.15 Python\/3.7.4 Darwin\/19.4.0 botocore\/2.0.0dev19\n<\/code><\/pre>\n\n<p>Wondering what the problem could be. Any help is appreciated<\/p>",
        "Challenge_closed_time":1590314994627,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590313799700,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error \"Invalid base64\" while testing an Amazon SageMaker model endpoint using the AWS CLI. The user is following a tutorial to create a machine learning-powered REST API with Amazon API Gateway mapping templates and Amazon SageMaker. The endpoint is in service on the SageMaker console and the example Jupyter notebook runs successfully. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":1590314948843,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61984217",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":19.3,
        "Challenge_reading_time":21.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.3319241667,
        "Challenge_title":"Invalid base64: \"{\"instances\": [{\"in0\":[863],\"in1\":[882]}]}\" when testing Amazon SageMaker model endpoint using the AWS CLI",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1117.0,
        "Challenge_word_count":140,
        "Platform":"Stack Overflow",
        "Poster_created_time":1578221300168,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":117.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>The problem is that the body contents is being expected to be base 64 encoded, try base64 encoding the body before passing it to the invoke statement.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.6,
        "Solution_reading_time":1.92,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":27.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1439246522636,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":0.0595647222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I try to test my Azure ML model, I get the following error: \u201cError code: InternalError, Http status code: 500\u201d, so it appears something is failing inside of the machine learning service. How do I get around this error?<\/p>",
        "Challenge_closed_time":1439848076380,
        "Challenge_comment_count":0,
        "Challenge_created_time":1439847861947,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an internal error while testing their Azure ML model, resulting in an HTTP status code of 500. They are seeking guidance on how to resolve this issue.",
        "Challenge_last_edit_time":1439906222223,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/32060196",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":3.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.0595647222,
        "Challenge_title":"Azure ML Internal Error",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1408.0,
        "Challenge_word_count":43,
        "Platform":"Stack Overflow",
        "Poster_created_time":1439847618300,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>I've run into this error before, and unfortunately, the only workaround I found was to create a new ML workspace backed by a storage account that you know is online. Then copy your experiment over to the new workspace, and things should work. It can be a bit cumbersome, but it should get rid of your error message. With the service being relatively new, things sometimes get corrupted as updates are being made, so I recommend checking the box labeled \"disable updates\" within your experiment.  Hope that helps!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.0,
        "Solution_reading_time":6.33,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":88.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11.1486111111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,  \n  \nI am new to SageMaker and I am trying to deploy my model to an endpoint but am getting the following error:  \n  \n**Failure reason**  \nUnable to locate at least 2 availability zone(s) with the requested instance type ml.t2.medium that overlap with SageMaker subnets  \n  \nI have tried using different instance types but always the same error  \n  \nI was under the impression that SageMaker will create the required instances for me and I do not need to create the instances first? I am using the EU-WEST-1 zone and using the console to setup the endpoint",
        "Challenge_closed_time":1553556696000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1553516561000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to deploy their model to an endpoint in SageMaker. They are receiving an error message stating that at least 2 availability zones with the requested instance type ml.t2.medium are not available in SageMaker subnets. The user has tried using different instance types but the error persists. They are unsure if they need to create instances first as they were under the impression that SageMaker would create the required instances. The user is using the EU-WEST-1 zone and setting up the endpoint using the console.",
        "Challenge_last_edit_time":1668612166704,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUySs_fgNpSE6wuY-6W7MwqQ\/unable-to-create-endpoint",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":6.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":11.1486111111,
        "Challenge_title":"Unable to create endpoint",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":666.0,
        "Challenge_word_count":97,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello,  \n  \nSagemaker engineer here. I looked at the VpcConfig of your model and found only one subnet configured.   \n  \nThe error message \"Unable to locate at least 2 availability zone(s) with the requested instance type XYZ that overlap with SageMaker subnets\" usually indicates misconfigured VPCs. Sagemaker imposes mandatory requirement for at least 2 availability zones in your VPC subnets even if you only request one instance, to account for the potential use of auto-scaling in the future.   \n  \nIn order to create the endpoint, the number of subnets in your model needs to be at least 2 in distinct availability zones, and ideally as close to the total number of availability zones as possible in the region.   \n  \nHope it helps,   \nWenzhao",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1553556696000,
        "Solution_link_count":0.0,
        "Solution_readability":11.0,
        "Solution_reading_time":8.89,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":119.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":3.4251711111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I developed a machine learning model using Azure ML's clustering. Few of the requests made from the cluster are triggering 404 HTTP error. I followed the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script\" rel=\"nofollow noreferrer\">document<\/a> to do modifications in my swagger.json file. Finally ended up with &quot;list index out of range&quot; error. It seems to be having issue with the global parameter but I am no sure about it. I am using the API from postman with some default headers like mentioned in the body below<\/p>\n<pre><code>{\n    &quot;Inputs&quot;: {\n         &quot;input_1&quot; : &quot;content&quot;\n         &quot;input_2: : &quot;content&quot;\n         ......\n    },\n    &quot;GlobalParameters&quot;: 0\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1651111084096,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651094225790,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered a \"list index out of range\" error while modifying the swagger.json file as per the Azure ML documentation to resolve the 404 HTTP error triggered by some requests made from the cluster. The error seems to be related to the global parameter, but the user is unsure. The user is using the API from Postman with default headers.",
        "Challenge_last_edit_time":1651098753480,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72035391",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":10.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":4.6828627778,
        "Challenge_title":"AzureML schema \"list index out of range\" error",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":78.0,
        "Challenge_word_count":97,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651093614703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Netherland",
        "Poster_reputation_count":19.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>Change the &quot;GlobalParameter&quot; value to any floating number other than 1.0 or even you can remove it and execute. Sometimes, Global parameter will cause the issue. Check the below documentation.<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/746784\/azure-ml-studio-error-while-testing-real-time-endp.html\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/746784\/azure-ml-studio-error-while-testing-real-time-endp.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":22.0,
        "Solution_reading_time":6.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1296602388823,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA",
        "Answerer_reputation_count":1244.0,
        "Answerer_view_count":368.0,
        "Challenge_adjusted_solved_time":241.0489197222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a list of ML experiments which I have created in Azure Machine Learning Studio.  I have deployed them as web services (the new version, not classic).  <\/p>\n\n<p>How can I go into Azure Machine Learning Web Services, click on a web service (which was deployed from an experiment), then navigate back to the experiment \/ predictive model which feeds it?<\/p>\n\n<p>The only link I can find between the two is by updating the web service from the predictive experiment, which then confirms what the web service is. I can see that the \"ExperimentId\" is a GUID in the URL when in the experiment and the web service, so hopefully this is possible.<\/p>\n\n<p>My reasoning is that relying on matching naming conventions, etc., to select the appropriate model to update is subject to human error.<\/p>",
        "Challenge_closed_time":1490283526688,
        "Challenge_comment_count":0,
        "Challenge_created_time":1489415750577,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to navigate from a deployed web service back to the original experiment or predictive model in Azure Machine Learning Studio. The only link found is the \"ExperimentId\" in the URL, and the user is concerned about relying on naming conventions to select the correct model.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42766263",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":10.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":241.0489197222,
        "Challenge_title":"How to Find Azure ML Experiment based on Deployed Web Service",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":224.0,
        "Challenge_word_count":145,
        "Platform":"Stack Overflow",
        "Poster_created_time":1377703476328,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bristol, United Kingdom",
        "Poster_reputation_count":592.0,
        "Poster_view_count":43.0,
        "Solution_body":"<p>The <em>new<\/em> web service does not store any information about the experiment or workspace that was deployed (not all <em>new<\/em> web services are deployed from an experiment).<\/p>\n\n<p>Here are the options available to track the relationship between the experiment and a <em>new<\/em> web service.<\/p>\n\n<h2>last deployment<\/h2>\n\n<p>However, the experiment keeps track of the <strong>last<\/strong> <em>new<\/em> web service that was deployed from the experiment. each deployment to a <em>new<\/em> web service overwrites this value.<\/p>\n\n<p>The value is stored in the experiment graph. One way to get the graph is to use the powershell module <a href=\"http:\/\/aka.ms\/amlps\" rel=\"nofollow noreferrer\">amlps<\/a><\/p>\n\n<p><code>Export-AmlExperimentGraph -ExperimentId &lt;Experiment Id&gt; -OutputFile e.json<\/code><\/p>\n\n<p><strong>e.json<\/strong><\/p>\n\n<pre><code>{\n\"ExperimentId\":\"&lt;Experiment Id&gt;\",\n\/\/ . . .\n\"WebService\":{\n\/\/ . . .\n\"ArmWebServiceId\":\"&lt;Arm Id&gt;\"\n},\n\/\/ . . . \n}\n<\/code><\/pre>\n\n<h2>azure resource tags<\/h2>\n\n<p>The tags feature for Azure resources is supported by the <em>new<\/em> web services. Setting a <code>tag<\/code> on the web service programmatically, with powershell or via the azure portal UX can be used to store a reference to the experiment on the <em>new<\/em> web service.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.8,
        "Solution_reading_time":16.63,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":158.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":144.7355655556,
        "Challenge_answer_count":1,
        "Challenge_body":"I want to enable data capture for a specific endpoint (so far, only via the console). The endpoint works fine and also logs & returns the desired results. However, no files are written to the specified S3 location.\n\n### Endpoint Configuration ###\n\nThe endpoint is based on a training job with a scikit learn classifier. It has only one variant which is a `ml.m4.xlarge` instance type. Data Capture is enabled with a sampling percentage of 100%. As data capture storage locations I tried `s3:\/\/<bucket-name>` as well as `s3:\/\/<bucket-name>\/<some-other-path>`. With the \"Capture content type\" I tried leaving everything blank, setting `text\/csv` in \"CSV\/Text\" and `application\/json` in \"JSON\".\n\n### Endpoint Invokation ###\n\nThe endpoint is invoked in a Lambda function with a client. Here's the call:\n```\nsagemaker_body_source = {\n            \"segments\": segments,\n            \"language\": language\n        }\npayload = json.dumps(sagemaker_body_source).encode()\nresponse = self.client.invoke_endpoint(EndpointName=endpoint_name,\n                                       Body=payload,\n                                       ContentType='application\/json',\n                                       Accept='application\/json')\nresult = json.loads(response['Body'].read().decode())\nreturn result[\"predictions\"]\n```\nInternally, the endpoint uses a Flask API with an `\/invocation` path that returns the result.\n\n### Logs ###\n\nThe endpoint itself works fine and the Flask API is logging input and output:\n```\nINFO:api:body: {'segments': [<strings...>], 'language': 'de'}\n```\n\n```\nINFO:api:output: {'predictions': [{'text': 'some text', 'label': 'some_label'}, ....]}\n```",
        "Challenge_closed_time":1660656368966,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660135320930,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to enable data capture for a specific endpoint in Sagemaker via the console, but no files are being written to the specified S3 location. The endpoint is based on a training job with a scikit learn classifier and is invoked in a Lambda function with a client. The endpoint works fine and logs input and output, but data capture is not working.",
        "Challenge_last_edit_time":1668490449198,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUKWPP4eXTTZe5qIUDJAXnsQ\/sagemaker-data-capture-does-not-write-files",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":19.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":144.7355655556,
        "Challenge_title":"Sagemaker Data Capture does not write files",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":77.0,
        "Challenge_word_count":183,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"So the issue seemed to be related to the IAM role. The default role (`ModelEndpoint-Role`) does not have access to write S3 files. It worked via the SDK since it uses another role in the sagemaker studio. I did not receive any error message about this.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1660656368966,
        "Solution_link_count":0.0,
        "Solution_readability":5.4,
        "Solution_reading_time":3.04,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":46.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.1511888889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,  <\/p>\n<p>I am getting the following error message:  <br \/>\n&quot;WebserviceException: WebserviceException: Message: Service diabetes-service with the same name already exists, please use a different service name or delete the existing service. InnerException None ErrorResponse { &quot;error&quot;: { &quot;message&quot;: &quot;Service diabetes-service with the same name.&quot;  <\/p>\n<p>Please can you help with deleting the service in question?  <\/p>\n<p>Thanks,  <\/p>\n<p>Naveen  <\/p>",
        "Challenge_closed_time":1610680137287,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610661593007,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a WebserviceException error message while trying to delete an existing service. The error message suggests that a service with the same name already exists and the user needs to either use a different name or delete the existing service. The user is seeking help with deleting the service in question.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/231106\/webserviceexception-how-to-delete-an-existing-serv",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":6.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":5.1511888889,
        "Challenge_title":"WebserviceException: How to delete an existing service",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":65,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello, Naveen. This error message means that in your current AML workspace, there already exists a real-time endpoint(or service) whose name is &quot;diabetes-service&quot;, so you can't deploy a new service with this same name because it will cause duplication.   <\/p>\n<p>You can check your workspace in our portal <a href=\"https:\/\/ml.azure.com\/selectWorkspace\">https:\/\/ml.azure.com\/selectWorkspace<\/a> , in the sidebar you can find a &quot;Endpoints&quot; button, you can find all your &quot;real-time endpoint&quot; there. Then please delete the dup service, after deletion you can deploy your new service with the name &quot;diabetes-service&quot;.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.6,
        "Solution_reading_time":8.4,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":85.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1507660761310,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Madrid, Espa\u00f1a",
        "Answerer_reputation_count":492.0,
        "Answerer_view_count":22.0,
        "Challenge_adjusted_solved_time":10.7241647222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have pretty big (~200Gb, ~20M lines) raw jsonl dataset. I need to extract important properties from there and store the intermediate dataset in csv for further conversion into something like HDF5, parquet, etc. Obviously, I can't use <code>JSONDataSet<\/code> for loading raw dataset, because it utilizes <code>pandas.read_json<\/code> under the hood, and using pandas for the dataset of such size sounds like a bad idea. So I'm thinking about reading the raw dataset line by line, process and append processed data line by line to the intermediate dataset.<\/p>\n\n<p>What I can't understand is how to make this compatible with <code>AbstractDataSet<\/code> with its <code>_load<\/code> and <code>_save<\/code> methods.<\/p>\n\n<p>P.S. I understand I can move this out of kedro's context, and introduce preprocessed dataset as a raw one, but that kinda breaks the whole idea of complete pipelines. <\/p>",
        "Challenge_closed_time":1582275656936,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582237049943,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has a large raw jsonl dataset of around 200GB and 20 million lines that needs to be processed to extract important properties and stored in an intermediate dataset in CSV format for further conversion into HDF5, parquet, etc. However, using pandas for such a large dataset is not feasible. The user is considering reading the raw dataset line by line, processing it, and appending the processed data line by line to the intermediate dataset. The challenge is to make this compatible with AbstractDataSet's _load and _save methods while maintaining the idea of complete pipelines.",
        "Challenge_last_edit_time":1583417458307,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60329363",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":11.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":10.7241647222,
        "Challenge_title":"How to process huge datasets in kedro",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":656.0,
        "Challenge_word_count":141,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324477592580,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1315.0,
        "Poster_view_count":91.0,
        "Solution_body":"<p>Try to use pyspark to leverage lazy evaluation and batch execution. \nSparkDataSet is implemented in kedro.contib.io.spark_data_set<\/p>\n\n<p>Sample catalog config for jsonl:<\/p>\n\n<pre><code>your_dataset_name:   \n  type: kedro.contrib.io.pyspark.SparkDataSet\n  filepath: \"\\file_path\"\n  file_format: json\n  load_args:\n    multiline: True\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1582292099020,
        "Solution_link_count":0.0,
        "Solution_readability":13.5,
        "Solution_reading_time":4.42,
        "Solution_score_count":4.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":32.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10.8094663889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>As part of our MLOps flow, we need to retrain a machine learning model using the AML designer, and then update the AKS webservice with the new machine learning model (+ a couple of other supplementary training artifacts), also from the designer.  <\/p>\n<p>We have built an inference pipeline to do this, and are able to run it manually. However, the solution requirements require this process to be automated. We have previously successfully automated this through the python SDK and the akswebservice.update method, but this solution has a hard requirement to use the designer only (custom python code blocks would be allowed, however).  <\/p>\n<p>Is there a way, using any Azure services (eg Azure Data Factory, Azure DevOps), that we can kick off a designer real time inference update pipeline immediately after its associated training pipeline finishes executing, in order to get the latest model version into the webservice, without any manual intervention? To be clear though, manual intervention is acceptable to build the initial inference pipeline for version 1, but not on the retraining cycle.<\/p>",
        "Challenge_closed_time":1620691417852,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620652503773,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user needs to automate the process of retraining a machine learning model using Azure ML Designer and updating the AKS webservice with the new model and other training artifacts. They have successfully automated this process using the Python SDK and akswebservice.update method, but now have a requirement to use the designer only. The user is looking for a way to kick off a real-time inference update pipeline immediately after the associated training pipeline finishes executing, without any manual intervention. They are open to using Azure services such as Azure Data Factory or Azure DevOps.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/389170\/azure-ml-designer-automatically-update-aks-webserv",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.4,
        "Challenge_reading_time":14.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":10.8094663889,
        "Challenge_title":"Azure ML Designer: Automatically Update AKS Webservice After Training",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":183,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. Currently, you can only use the Azure Machine Learning SDK to automatically <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-update-web-service\">update the web service<\/a>. I'm inquiring from the product team whether there are plans to support this scenario (will share updates accordingly). Hope this helps.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.1,
        "Solution_reading_time":4.89,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":44.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1631803441500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"mexico",
        "Answerer_reputation_count":1258.0,
        "Answerer_view_count":685.0,
        "Challenge_adjusted_solved_time":23.1295969444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to set up mlops for Vertex AI, following <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/mlops-with-vertex-ai\/blob\/main\/01-dataset-management.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a>. It works until, near the end, I try:<\/p>\n<pre><code>vertex_ai.init(\nproject=PROJECT,\n    location=REGION)\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code> module 'google.cloud.aiplatform.constants' has no attribute 'SUPPORTED_REGIONS\n<\/code><\/pre>\n<p>I am using <code>us-central1<\/code> which is supported. I wondered if maybe <code>from google.cloud import aiplatform as vertex_ai<\/code> has been changed but don't know how to find out. Any help is much appreciated.<\/p>",
        "Challenge_closed_time":1652193787552,
        "Challenge_comment_count":2,
        "Challenge_created_time":1652110521003,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while setting up mlops for Vertex AI. The error message 'google.cloud.aiplatform.constants' has no attribute 'SUPPORTED_REGIONS' is displayed when trying to initialize Vertex AI with the project and location. The user is using a supported region, but is unsure if there have been any changes to the import statement.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72174602",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":11.2,
        "Challenge_reading_time":10.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":23.1295969444,
        "Challenge_title":"Why do I get 'google.cloud.aiplatform.constants' has no attribute 'SUPPORTED_REGIONS error for Vertex AI init?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":298.0,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>I followed the same Notebook as you, even though I didn't have any issue. What could be happening to you is that you are using an <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/releases\" rel=\"nofollow noreferrer\">older version of the library<\/a>.<\/p>\n<p>You can use the command to upgrade the library that is the following one: <code>pip3 install google-cloud-aiplatform --upgrade<\/code>.<\/p>\n<p>Sometimes this happens with the basic installation of the library; the problems could be in the <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform#installation\" rel=\"nofollow noreferrer\">dependencies, versions and indirectly permissions<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.6,
        "Solution_reading_time":8.59,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":76.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1546969667040,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":1689.0,
        "Answerer_view_count":170.0,
        "Challenge_adjusted_solved_time":0.3264786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm just testing out AWS Sagemaker notebook and created an endpoint using a partial script below:<\/p>\n\n<pre><code>endpoint_name = 'engine' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nendpoint_config_name = 'engine_config' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nmodel_name = 'engine_model' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n\nwhile status=='Creating':\n    time.sleep(60)\n    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n    status = resp['EndpointStatus']\n    print(\"Status: \" + status)\n\n<\/code><\/pre>\n\n<p>I'm trying to remove that endpoint by using:\nsm_client.delete_endpoint(EndpointName=endpoint_name)<\/p>\n\n<p>However, it didn't work because I naively used timestamps for the endpoint_name and I didn't remember them. The original variable values were overriden when I re-run the code. As a result, I can't delete the existing endpoint.\nI went to the Sagemaker management dashboard --> inference --> endpoints, but it's empty. I don't even know if I'm currently having any active endpoints or not. Please advise how to delete my endpoint in this case. Thank you in advance.<\/p>",
        "Challenge_closed_time":1574200291200,
        "Challenge_comment_count":1,
        "Challenge_created_time":1574199115877,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user created an endpoint in AWS SageMaker notebook using a script with timestamp-based variable names. The user is now unable to delete the endpoint as they do not remember the original variable values and the endpoint does not appear in the Sagemaker management dashboard. The user is seeking advice on how to delete the endpoint.",
        "Challenge_last_edit_time":1574275275427,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58943117",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.4,
        "Challenge_reading_time":14.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.3264786111,
        "Challenge_title":"Confirming endpoints were deleted in SageMaker notebook",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":661.0,
        "Challenge_word_count":129,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521994074812,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":338.0,
        "Poster_view_count":114.0,
        "Solution_body":"<p><strong>If there are no endpoints active under the \"Endpoints\" tab in the SageMaker service console, then you will not be incurring any charges for inference or endpoint infrastructure.<\/strong><\/p>\n\n<p>If this is the case, your Endpoints tab should look like the following:\n<a href=\"https:\/\/i.stack.imgur.com\/5QvEn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/5QvEn.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Endpoint Configurations<\/strong>, on the other hand, involve the metadata necessary for an endpoint deployment. This is just the metadata, and are stored (without cost) in your account, visible in the console under the \"Endpoint Configurations\" tab. You do not need to remove these configurations when tearing down an endpoint.<\/p>\n\n<p><strong>Important note:<\/strong> Double check that you are checking in the console for the <em>region you would have deployed to<\/em>. For example, if you ran the notebook and deployed an endpoint in <code>us-east-1<\/code>, but check the SageMaker console for <code>us-west-2<\/code>, it would not be displaying endpoints from the other region.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.7,
        "Solution_reading_time":14.48,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":148.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1359732456992,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":401.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":667.9050702778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I've successfully trained a LDA model with sagemaker, I've been able to set up an Inference API but it has a limit of how many records I can query at a time. <\/p>\n\n<p>I need to get predictions for a large file and have been trying to use Batch Transformation however am running against roadblock.<\/p>\n\n<p>My input date is in application\/x-recordio-protobuf content type, code is as follows:<\/p>\n\n<pre><code># Initialize the transformer object\ntransformer =sagemaker.transformer.Transformer(\n    base_transform_job_name='Batch-Transform',\n    model_name=model_name,\n    instance_count=1,\n    instance_type='ml.c4.xlarge',\n    output_path=output_location,\n    max_payload=20,\n    strategy='MultiRecord'\n    )\n# Start a transform job\ntransformer.transform(input_location, content_type='application\/x-recordio-protobuf',split_type=\"RecordIO\")\n# Then wait until the transform job has completed\ntransformer.wait()\n\n# Fetch validation result \ns3_client.download_file(bucket, 'topic_model_batch_transform\/output\/batch_tansform_part0.pbr.out', 'batch_tansform-result')\nwith open('batch_tansform-result') as f:\n    results = f.readlines()   \nprint(\"Sample transform result: {}\".format(results[0]))\n<\/code><\/pre>\n\n<p>I have chunked by input file into 10 files each around 19MB in size. I am attempting at first to run on a single chunk, therefore 19MB in total. I have tried changing strategy, trying SingleRecord. I have also tried different split_types, also trying None and \"Line\". <\/p>\n\n<p>I've read the documentation but its not clear what else I should try, also the error messages are very unclear.<\/p>\n\n<pre><code>2019-04-02T15:49:47.617:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=20, BatchStrategy=MULTI_RECORD\n#011at java.lang.Thread.run(Thread.java:748)2019-04-02T15:49:48.035:[sagemaker logs]: du-sagemaker\/data\/batch_transform\/batch_tansform_part0.pbr: Bad HTTP status returned from invoke: 413\n2019-04-02T15:49:48.036:[sagemaker logs]: du-sagemaker\/data\/batch_transform\/batch_tansform_part0.pbr:\n2019-04-02T15:49:48.036:[sagemaker logs]: du-sagemaker\/data\/batch_transform\/batch_tansform_part0.pbr: Message:\n2019-04-02T15:49:48.036:[sagemaker logs]: du-sagemaker\/data\/batch_transform\/batch_tansform_part0.pbr: &lt;!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 3.2 Final\/\/EN\"&gt;\n2019-04-02T15:49:48.036:[sagemaker logs]: du-sagemaker\/data\/batch_transform\/batch_tansform_part0.pbr: &lt;title&gt;413 Request Entity Too Large&lt;\/title&gt;\n2019-04-02T15:49:48.036:[sagemaker logs]: du-sagemaker\/data\/batch_transform\/batch_tansform_part0.pbr: &lt;h1&gt;Request Entity Too Large&lt;\/h1&gt;\n2019-04-02T15:49:48.036:[sagemaker logs]: du-sagemaker\/data\/batch_transform\/batch_tansform_part0.pbr: &lt;p&gt;The data value transmitted exceeds the capacity limit.&lt;\/p&gt;\n<\/code><\/pre>\n\n<p>The above is the last one I got with the above configuration, before that I was also getting a 400 HTTP error code.<\/p>\n\n<p>Any help or pointers would be greatly appreciated! Thank you<\/p>",
        "Challenge_closed_time":1556626383163,
        "Challenge_comment_count":0,
        "Challenge_created_time":1554221924910,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has successfully trained an LDA model with Sagemaker and set up an Inference API, but is facing a limit on the number of records that can be queried at a time. They are attempting to use Batch Transformation to get predictions for a large file, but are encountering errors with unclear messages. They have tried different strategies and split types, but are still unable to run the transformation on a single chunk of 19MB. The error message received is \"413 Request Entity Too Large\". The user is seeking help or pointers to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55479366",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":15.0,
        "Challenge_reading_time":39.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":667.9050702778,
        "Challenge_title":"Errors running Sagemaker Batch Transformation with LDA model",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":3622.0,
        "Challenge_word_count":281,
        "Platform":"Stack Overflow",
        "Poster_created_time":1359732456992,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, United Kingdom",
        "Poster_reputation_count":401.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>I managed to resolve the issue, it seemed the maxpayload I was using was too high. I set  <code>MaxPayloadInMB=1<\/code> and it now runs like a dream<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":1.9,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3166666667,
        "Challenge_answer_count":2,
        "Challenge_body":"I just want to clarify my understanding. I can use my own servers for calling webhooks correct (as long as they return the json structure required). The webhooks will essentially reach out another API service and return data for fulfillment. Thanks in advance for your time.",
        "Challenge_closed_time":1673513220000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1673512080000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking clarification on whether they can use their own servers to call webhooks, as long as the required JSON structure is returned. The webhooks will be used to reach out to another API service and return data for fulfillment.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Webhooks\/m-p\/509590#M1056",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.6,
        "Challenge_reading_time":3.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.3166666667,
        "Challenge_title":"Webhooks",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":97.0,
        "Challenge_word_count":46,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Exactly correct.\u00a0 During the processing of a conversation, if you have a Web Hook enabled, the Dialogflow engine will call-out to the target URL passing in a JSON payload and expecting a correctly formatted JSON response.\n\nSee the following for details:\n\nhttps:\/\/cloud.google.com\/dialogflow\/cx\/docs\/concept\/webhook\n\nTake care to notice that the target service MUST be callable through HTTPS which means that it has a valid SSL certificate.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.0,
        "Solution_reading_time":5.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":69.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":6.2657233333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm attempting to deploy a text classification model I trained with Vertex AI on the Google Cloud Platform using the Python SDK.<\/p>\n<pre><code>from google.cloud import aiplatform\n\nimport os\n\nos.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;] = &quot;&lt;key location&gt;&quot;\n\ndef create_endpoint(\n    project_id: str,\n    display_name: str,\n    location: str,\n    sync: bool = True,\n):\n    endpoint = aiplatform.Endpoint.create(\n        display_name=display_name, project=project_id, location=location,\n    )\n\n    print(endpoint.display_name)\n    print(endpoint.resource_name)\n    return endpoint\n\ndef deploy_model(project_id, location, model_id):\n    model_location = &quot;projects\/{}\/locations\/{}\/models\/{}&quot;.format(project_id, location, model_id)\n\n    print(&quot;Initializing Vertex AI&quot;)\n    aiplatform.init(project=project_id, location=location)\n\n    print(&quot;Getting model from {}&quot;.format(model_location))\n    model = aiplatform.Model(model_location)\n\n    print(&quot;Creating endpoint.&quot;)\n    endpoint = create_endpoint(project_id, &quot;{}_endpoint&quot;.format(model_id), location)\n\n    print(&quot;Deploying endpoint&quot;)\n    endpoint.deploy(\n        model,\n        machine_type=&quot;n1-standard-4&quot;,\n        min_replica_count=1,\n        max_replica_count=5,\n        accelerator_type='NVIDIA_TESLA_K80',\n        accelerator_count=1\n    )\n\n    return endpoint\n\nendpoint = deploy_model(\n    &quot;&lt;project name&gt;&quot;,\n    &quot;us-central1&quot;,\n    &quot;&lt;model id&gt;&quot;,\n)\n<\/code><\/pre>\n<p>Unfortunately, when I run this code, I receive this error after the endpoint.deploy is triggered:\n<code>google.api_core.exceptions.InvalidArgument: 400 'dedicated_resources' is not supported for Model...<\/code> followed by the model location.<\/p>\n<p>Note the places I've swapped my values with &lt;***&gt; to hide my local workspace variables.<\/p>",
        "Challenge_closed_time":1634271217372,
        "Challenge_comment_count":2,
        "Challenge_created_time":1634245049023,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while attempting to deploy a text classification model using Vertex AI on the Google Cloud Platform using the Python SDK. The error message states that 'dedicated_resources' is not supported for the model.",
        "Challenge_last_edit_time":1634248660768,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69577270",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":19.5,
        "Challenge_reading_time":24.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":7.2689858333,
        "Challenge_title":"Google Cloud Vertex AI - 400 'dedicated_resources' is not supported for Model",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":616.0,
        "Challenge_word_count":148,
        "Platform":"Stack Overflow",
        "Poster_created_time":1417013182680,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Boston, MA",
        "Poster_reputation_count":1256.0,
        "Poster_view_count":245.0,
        "Solution_body":"<p>You encounter the error since it is not possible to assign a custom machine to a Text Classification model. Only custom models and AutoML tabular models can use custom machines as per <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#specify\" rel=\"nofollow noreferrer\">documentation<\/a>. For AutoML models aside from tabular models, Vertex AI automatically configures the machine type.<\/p>\n<blockquote>\n<p>If you want to use a custom-trained model or an AutoML tabular model\nto serve online predictions, you must specify a machine type when you\ndeploy the Model resource as a DeployedModel to an Endpoint. <strong>For other<\/strong>\n<strong>types of AutoML models, Vertex AI configures the machine types<\/strong>\n<strong>automatically.<\/strong><\/p>\n<\/blockquote>\n<p>The workaround for this is to remove the machine related parameters on your <code>endpoint.deploy()<\/code> and you should be able to deploy the model.<\/p>\n<pre><code>print(&quot;Deploying endpoint&quot;)\nendpoint.deploy(model)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.5,
        "Solution_reading_time":13.4,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":127.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1417744681680,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":4211.0,
        "Answerer_view_count":716.0,
        "Challenge_adjusted_solved_time":74.6835766667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to invoke an AWS SageMaker endpoint through the following simple code using boto3<\/p>\n<pre><code>import boto3\n\nsession = boto3.Session(profile_name='mlacc',\n                        region_name='us-west-2')\n\nsagemaker_client = session.client('sagemaker-runtime')\n\nrequest_body = &quot;{\\n    \\&quot;requestSource\\&quot;: \\&quot;unittest\\&quot;,\\n    \\&quot;clusters\\&quot;: [{\\n        \\&quot;clusterMetadata\\&quot;: {\\n &quot;\n&quot;\\&quot;clusterId\\&quot;: \\&quot;id1\\&quot;,\\n            \\&quot;topic\\&quot;: [\\&quot;corona virus\\&quot;, \\&quot;Donald Trump\\&quot;],\\n            &quot;\n&quot;\\&quot;clusterSize\\&quot;: 2\\n        },\\n        \\&quot;documents\\&quot;: [{\\n            \\&quot;uid\\&quot;: \\&quot;1\\&quot;,\\n            &quot;\n&quot;\\&quot;content\\&quot;: \\&quot;content2\\&quot;,\\n            \\&quot;domain\\&quot;: \\&quot;CNN.com\\&quot;,\\n            \\&quot;title\\&quot;: \\&quot;This is a &quot;\n&quot;title\\&quot;,\\n            \\&quot;similarityScore\\&quot;: 1.3,\\n            \\&quot;published_at\\&quot;: 1566264017,\\n            &quot;\n&quot;\\&quot;domain_rank\\&quot;: 1,\\n            \\&quot;trust_domain_score\\&quot;: 1\\n        }, {\\n            \\&quot;uid\\&quot;: \\&quot;2\\&quot;,&quot;\n&quot;\\n            \\&quot;content\\&quot;: \\&quot;content2\\&quot;,\\n            \\&quot;domain\\&quot;: \\&quot;CNN.com\\&quot;,\\n            \\&quot;title\\&quot;: &quot;\n&quot;\\&quot;This is a title\\&quot;,\\n            \\&quot;similarityScore\\&quot;: 1.3,\\n            \\&quot;published_at\\&quot;: 1566264017,&quot;\n&quot;\\n            \\&quot;domain_rank\\&quot;: 1,\\n            \\&quot;trust_domain_score\\&quot;: 1\\n        }, {\\n            \\&quot;uid\\&quot;: &quot;\n&quot;\\&quot;2\\&quot;,\\n            \\&quot;content\\&quot;: \\&quot;content3\\&quot;,\\n            \\&quot;domain\\&quot;: \\&quot;CNN.com\\&quot;,\\n            \\&quot;title\\&quot;: &quot;\n&quot;\\&quot;This is a title\\&quot;,\\n            \\&quot;similarityScore\\&quot;: 1.3,\\n            \\&quot;published_at\\&quot;: 1566264017,&quot;\n&quot;\\n            \\&quot;domain_rank\\&quot;: 1,\\n            \\&quot;trust_domain_score\\&quot;: 1\\n        }]\\n    }]\\n}&quot;\n\n\nresponse = sagemaker_client.invoke_endpoint(\n    EndpointName='myEndpoint22',\n    Body=request_body,\n    ContentType='application\/json',\n)\n\nresponse_json = response['Body'].read().decode('utf-8')\n\nprint(response_json)\n<\/code><\/pre>\n<p>I get the following error when I run this code<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/Users\/rppatwa\/Desktop\/WorkDocs\/CodePlayground\/SimplePythonProject\/src\/PrototypeTesting\/SummarizationLocal.py&quot;, line 205, in &lt;module&gt;\n    main()\n  File &quot;\/Users\/rppatwa\/Desktop\/WorkDocs\/CodePlayground\/SimplePythonProject\/src\/PrototypeTesting\/SummarizationLocal.py&quot;, line 186, in main\n    ContentType='application\/json',\n  File &quot;\/Users\/rppatwa\/anaconda3\/lib\/python3.7\/site-packages\/botocore\/client.py&quot;, line 316, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File &quot;\/Users\/rppatwa\/anaconda3\/lib\/python3.7\/site-packages\/botocore\/client.py&quot;, line 635, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message &quot;Unable to parse data as JSON. Make sure the Content-Type header is set to &quot;application\/json&quot;&quot;. See https:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/KeyurshaASMLModel in account 753843489946 for more information.\n<\/code><\/pre>\n<p>If I inline the Body (not using the request_json) this call succeeds. Please let me know what I am missing.<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1598927909716,
        "Challenge_comment_count":3,
        "Challenge_created_time":1598659048840,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error when trying to invoke an AWS SageMaker endpoint using boto3. The error message states that the data cannot be parsed as JSON and suggests that the Content-Type header should be set to \"application\/json\". The user is unsure of what they are missing and is seeking assistance.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63642175",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":18.2,
        "Challenge_reading_time":47.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":74.6835766667,
        "Challenge_title":"Error when invoking AWS SageMaker endpoint using boto3 : \"Unable to parse data as JSON. Make sure the Content-Type header is set to \"application\/json\"",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1170.0,
        "Challenge_word_count":256,
        "Platform":"Stack Overflow",
        "Poster_created_time":1470101805440,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Santa Monica, CA, USA",
        "Poster_reputation_count":107.0,
        "Poster_view_count":21.0,
        "Solution_body":"<p>You need to remove the trailing comma after  <code>ContentType='application\/json',<\/code> and try below snippet for passing JSON to body field.<\/p>\n<pre><code>import json \njson.dumps(request_body) \ntest=json.dumps(request_body).encode()\n<\/code><\/pre>\n<p>This will also validate the JSON that you are passing.Now pass test to body for invoking the endopoint.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.1,
        "Solution_reading_time":4.7,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":42.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1454788607456,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Kirkland, WA",
        "Answerer_reputation_count":571.0,
        "Answerer_view_count":61.0,
        "Challenge_adjusted_solved_time":96.8292841667,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Azure ML Experiments provide ways to read and write CSV files to Azure blob storage through the <code>Reader<\/code> and <code>Writer<\/code> modules. However, I need to write a JSON file to blob storage. Since there is no module to do so, I'm trying to do so from within an <code>Execute Python Script<\/code> module.<\/p>\n\n<pre><code># Import the necessary items\nfrom azure.storage.blob import BlobService\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    account_name = 'mystorageaccount'\n    account_key='mykeyhere=='\n    json_string='{jsonstring here}'\n\n    blob_service = BlobService(account_name, account_key)\n\n    blob_service.put_block_blob_from_text(\"upload\",\"out.json\",json_string)\n\n    # Return value must be of a sequence of pandas.DataFrame\n    return dataframe1,\n<\/code><\/pre>\n\n<p>However, this results in an error: <code>ImportError: No module named azure.storage.blob<\/code><\/p>\n\n<p>This implies that the <code>azure-storage<\/code> Python package is not installed on Azure ML. <\/p>\n\n<p><em>How can I write to Azure blob storage from inside an Azure ML Experiment?<\/em><\/p>\n\n<p>Here's the fill error message:<\/p>\n\n<pre><code>Error 0085: The following error occurred during script evaluation, please view the output log for more information:\n---------- Start of error message from Python interpreter ----------\ndata:text\/plain,Caught exception while executing function: Traceback (most recent call last):\n  File \"C:\\server\\invokepy.py\", line 162, in batch\n    mod = import_module(moduleName)\n  File \"C:\\pyhome\\lib\\importlib\\__init__.py\", line 37, in import_module\n    __import__(name)\n  File \"C:\\temp\\azuremod.py\", line 19, in &lt;module&gt;\n    from azure.storage.blob import BlobService\nImportError: No module named azure.storage.blob\n\n---------- End of error message from Python  interpreter  ----------\nStart time: UTC 02\/06\/2016 17:59:47\nEnd time: UTC 02\/06\/2016 18:00:00`\n<\/code><\/pre>\n\n<p>Thanks, everyone!<\/p>\n\n<p>UPDATE: Thanks to Dan and Peter for the ideas below. This is the progress I've made using those recommendations.  I created a clean Python 2.7 virtual environment (in VS 2005), and did a <code>pip install azure-storage<\/code> to get the dependencies into my site-packages directory.  I then zipped the site-packages folder and uploaded as the Zip file, as per Dan's note below.  I then included the reference to the site-packages directory and successfully imported the required items.  This resulted in a time out error when writing to blog storage.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/wsrLn.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wsrLn.png\" alt=\"Failure to write to Blob storage\"><\/a><\/p>\n\n<p>Here is my code:<\/p>\n\n<pre><code># Get access to the uploaded Python packages    \nimport sys\npackages = \".\\Script Bundle\\site-packages\"\nsys.path.append(packages)\n\n# Import the necessary items from packages referenced above\nfrom azure.storage.blob import BlobService\nfrom azure.storage.queue import QueueService\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    account_name = 'mystorageaccount'\n    account_key='p8kSy3F...elided...3plQ=='\n\n    blob_service = BlobService(account_name, account_key)\n    blob_service.put_block_blob_from_text(\"upload\",\"out.txt\",\"Test to write\")\n\n    # All of the following also fail\n    #blob_service.create_container('images')\n    #blob_service.put_blob(\"upload\",\"testme.txt\",\"foo\",\"BlockBlob\")\n\n    #queue_service = QueueService(account_name, account_key)\n    #queue_service.create_queue('taskqueue')\n\n    # Return value must be of a sequence of pandas.DataFrame\n    return dataframe1,\n<\/code><\/pre>\n\n<p>And here is the new error log:<\/p>\n\n<pre><code>Error 0085: The following error occurred during script evaluation, please view the output log for more information:\n---------- Start of error message from Python interpreter ----------\ndata:text\/plain,C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\util\\ssl_.py:79: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https:\/\/urllib3.readthedocs.org\/en\/latest\/security.html#insecureplatformwarning.\n  InsecurePlatformWarning\nCaught exception while executing function: Traceback (most recent call last):   \n  File \"C:\\server\\invokepy.py\", line 169, in batch\n    odfs = mod.azureml_main(*idfs)\n  File \"C:\\temp\\azuremod.py\", line 44, in azureml_main\n    blob_service.put_blob(\"upload\",\"testme.txt\",\"foo\",\"BlockBlob\")\n  File \".\\Script Bundle\\site-packages\\azure\\storage\\blob\\blobservice.py\", line 883, in put_blob\n    self._perform_request(request)\n  File \".\\Script Bundle\\site-packages\\azure\\storage\\storageclient.py\", line 171, in _perform_request\n    resp = self._filter(request)\n  File \".\\Script Bundle\\site-packages\\azure\\storage\\storageclient.py\", line 160, in _perform_request_worker\n    return self._httpclient.perform_request(request)\n  File \".\\Script Bundle\\site-packages\\azure\\storage\\_http\\httpclient.py\", line 181, in perform_request\n    self.send_request_body(connection, request.body)\n  File \".\\Script Bundle\\site-packages\\azure\\storage\\_http\\httpclient.py\", line 143, in send_request_body\n    connection.send(request_body)\n  File \".\\Script Bundle\\site-packages\\azure\\storage\\_http\\requestsclient.py\", line 81, in send\n    self.response = self.session.request(self.method, self.uri, data=request_body, headers=self.headers, timeout=self.timeout)\n  File \"C:\\pyhome\\lib\\site-packages\\requests\\sessions.py\", line 464, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"C:\\pyhome\\lib\\site-packages\\requests\\sessions.py\", line 576, in send\n    r = adapter.send(request, **kwargs)\n  File \"C:\\pyhome\\lib\\site-packages\\requests\\adapters.py\", line 431, in send\n    raise SSLError(e, request=request)\nSSLError: The write operation timed out\n\n---------- End of error message from Python  interpreter  ----------\nStart time: UTC 02\/10\/2016 15:33:00\nEnd time: UTC 02\/10\/2016 15:34:18\n<\/code><\/pre>\n\n<p>Where my current exploration is leading is that there is a dependency on the <code>requests<\/code> Python package in <code>azure-storage<\/code>. <code>requests<\/code> has a known bug in Python 2.7 for calling newer SSL protocols. Not sure, but I'm digging around in that area now.  <\/p>\n\n<p>UPDATE 2: This code runs perfectly fine inside of a Python 3 Jupyter notebook.  Additionally, if I make the Blob Container open to public access, I can directly READ from the Container through a URL.  For instance: <code>df = pd.read_csv(\"https:\/\/mystorageaccount.blob.core.windows.net\/upload\/test.csv\")<\/code> easily loads the file from blob storage.  However, I cannot use the <code>azure.storage.blob.BlobService<\/code> to read from the same file.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/X4vpX.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/X4vpX.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>UPDATE 3: Dan, in a comment below, suggested I try from the Jupyter notebooks <em>hosted on Azure ML<\/em>.  I had been running it from a local Jupyter notebook (see update 2 above).  <strong>However<\/strong>, it fails when run from an Azure ML Notebook, and the errors point to the <code>requires<\/code> package again. I'll need to find the known issues with that package, but from my reading, the known issue is with urllib3 and only impacts Python 2.7 and NOT any Python 3.x versions.  And this was run in a Python 3.x notebook.  Grrr.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/jjbp2.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jjbp2.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>UPDATE 4: As Dan notes below, this may be an issue with Azure ML networking, as <code>Execute Python Script<\/code> is relatively new and just got networking support.  However, I have also tested this on an Azure App Service webjob, which is on an entirely different Azure platform. (It is also on an entirely different Python distribution and supports both Python 2.7 and 3.4\/5, but only at 32 bit - even on 64 bit machines.)  The code there also fails, with an <code>InsecurePlatformWarning<\/code> message.<\/p>\n\n<pre><code>[02\/08\/2016 15:53:54 &gt; b40783: SYS INFO] Run script 'ListenToQueue.py' with script host - 'PythonScriptHost'\n[02\/08\/2016 15:53:54 &gt; b40783: SYS INFO] Status changed to Running\n[02\/08\/2016 15:54:09 &gt; b40783: INFO] test.csv\n[02\/08\/2016 15:54:09 &gt; b40783: ERR ] D:\\home\\site\\wwwroot\\env\\Lib\\site-packages\\requests\\packages\\urllib3\\util\\ssl_.py:315: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. For more information, see https:\/\/urllib3.readthedocs.org\/en\/latest\/security.html#snimissingwarning.\n[02\/08\/2016 15:54:09 &gt; b40783: ERR ]   SNIMissingWarning\n[02\/08\/2016 15:54:09 &gt; b40783: ERR ] D:\\home\\site\\wwwroot\\env\\Lib\\site-packages\\requests\\packages\\urllib3\\util\\ssl_.py:120: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https:\/\/urllib3.readthedocs.org\/en\/latest\/security.html#insecureplatformwarning.\n[02\/08\/2016 15:54:09 &gt; b40783: ERR ]   InsecurePlatformWarning\n[02\/08\/2016 15:54:09 &gt; b40783: ERR ] D:\\home\\site\\wwwroot\\env\\Lib\\site-packages\\requests\\packages\\urllib3\\util\\ssl_.py:120: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https:\/\/urllib3.readthedocs.org\/en\/latest\/security.html#insecureplatformwarning.\n[02\/08\/2016 15:54:09 &gt; b40783: ERR ]   InsecurePlatformWarning\n<\/code><\/pre>",
        "Challenge_closed_time":1455143484176,
        "Challenge_comment_count":0,
        "Challenge_created_time":1454794898753,
        "Challenge_favorite_count":6.0,
        "Challenge_gpt_summary_original":"The user is trying to write a JSON file to Azure blob storage from within an Azure ML experiment using an Execute Python Script module. However, there is no module to do so, resulting in an error: \"ImportError: No module named azure.storage.blob\". The user has tried to install the necessary dependencies and import them, but it results in a time-out error when writing to blob storage. The user has also tried running the code from a Jupyter notebook hosted on Azure ML, but it fails with an InsecurePlatformWarning message.",
        "Challenge_last_edit_time":1634165631492,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35246826",
        "Challenge_link_count":11,
        "Challenge_participation_count":3,
        "Challenge_readability":11.1,
        "Challenge_reading_time":126.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":16.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":95,
        "Challenge_solved_time":96.8292841667,
        "Challenge_title":"Access Azure blob storage from within an Azure ML experiment",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":6083.0,
        "Challenge_word_count":1043,
        "Platform":"Stack Overflow",
        "Poster_created_time":1454788607456,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kirkland, WA",
        "Poster_reputation_count":571.0,
        "Poster_view_count":61.0,
        "Solution_body":"<p><strong>Bottom Line Up Front:<\/strong> Use HTTP instead of HTTPS for accessing Azure storage.<\/p>\n\n<p>When declaring BlobService pass in <code>protocol='http'<\/code> to force the service to communicate over HTTP. Note that you must have your container configured to allow requests over HTTP (which it does by default).<\/p>\n\n<p><code>client = BlobService(STORAGE_ACCOUNT, STORAGE_KEY, protocol=\"http\")<\/code><\/p>\n\n<p>History and credit:<\/p>\n\n<p>I posted a query on this topic to @AzureHelps and they opened a ticket on the MSDN forums: <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/46166b22-47ae-4808-ab87-402388dd7a5c\/trouble-writing-blob-storage-file-in-azure-ml-experiment?forum=MachineLearning&amp;prof=required\">https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/46166b22-47ae-4808-ab87-402388dd7a5c\/trouble-writing-blob-storage-file-in-azure-ml-experiment?forum=MachineLearning&amp;prof=required<\/a> <\/p>\n\n<p>Sudarshan Raghunathan replied with the magic.  Here are the steps to make it easy for everyone to duplicate my fix:<\/p>\n\n<ol>\n<li>Download azure.zip which provides the required libraries: <a href=\"https:\/\/azuremlpackagesupport.blob.core.windows.net\/python\/azure.zip\">https:\/\/azuremlpackagesupport.blob.core.windows.net\/python\/azure.zip<\/a><\/li>\n<li>Upload them as a DataSet to the Azure ML Studio<\/li>\n<li>Connect them to the Zip input on an <code>Execute Python Script<\/code> module<\/li>\n<li>Write your script as you would normally, being sure to create your <code>BlobService<\/code> object with <code>protocol='http'<\/code><\/li>\n<li>Run the Experiment - you should now be able to write to blob storage.<\/li>\n<\/ol>\n\n<p>Some example code can be found here: <a href=\"https:\/\/gist.github.com\/drdarshan\/92fff2a12ad9946892df\">https:\/\/gist.github.com\/drdarshan\/92fff2a12ad9946892df<\/a><\/p>\n\n<p>The code I used was the following, which doesn't first write the CSV to the file system, but sends as a text stream.<\/p>\n\n<pre><code>from azure.storage.blob import BlobService\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    account_name = 'mystorageaccount'\n    account_key='p8kSy3FACx...redacted...ebz3plQ=='\n    container_name = \"upload\"\n    json_output_file_name = 'testfromml.json'\n    json_orient = 'records' # Can be index, records, split, columns, values\n    json_force_ascii=False;\n\n    blob_service = BlobService(account_name, account_key, protocol='http')\n\n    blob_service.put_block_blob_from_text(container_name,json_output_file_name,dataframe1.to_json(orient=json_orient, force_ascii=json_force_ascii))\n\n    # Return value must be of a sequence of pandas.DataFrame\n    return dataframe1,\n<\/code><\/pre>\n\n<p>Some thoughts:<\/p>\n\n<ol>\n<li>I would prefer if the azure Python libraries were imported by default. Microsoft imports hundreds of 3rd party libraries into Azure ML as part of the Anaconda distribution. They should also include those necessary to work with Azure. We're in Azure, we've committed to Azure. Embrace it.<\/li>\n<li>I don't like that I have to use HTTP, instead of HTTPS. Granted, this is internal Azure communication, so it's likely no big deal. However, most of the documentation suggests the use of SSL \/ HTTPS when working with blob storage, so I'd prefer to be able to do that. <\/li>\n<li>I still get random timeout errors in the Experiment. Sometimes the Python code will execute in milliseconds, other times it runs for several 60 or seconds and then times out. This makes running it in an experiment very frustrating at times. However, when published as a Web Service I do not seem to have this problem.<\/li>\n<li>I would prefer that the experience from my local code matched more closely Azure ML. Locally, I can use HTTPS and never time out. It's blazing fast, and easy to write. But moving to an Azure ML experiment means some debugging, nearly every time. <\/li>\n<\/ol>\n\n<p>Huge props to Dan, Peter and Sudarshan, all from Microsoft, for their help in resolving this. I very much appreciate it!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":11.2,
        "Solution_reading_time":50.68,
        "Solution_score_count":5.0,
        "Solution_sentence_count":32.0,
        "Solution_word_count":454.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":188.1755805556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have big data that I need to pass it to already trained Machine Learning model on Azure and has been deployed as online endpoint, I realize that batch endpoints supports adding a reference to blob file as input, my question is: how to do the same for online enpdoints ?    <\/p>\n<p>So far all examples I see are passing the payload as json (Even in the test tab of the online enpoints) but i don't know how to simply pass a blob storage file's uri as the payload.<\/p>",
        "Challenge_closed_time":1656051612080,
        "Challenge_comment_count":1,
        "Challenge_created_time":1655374179990,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to pass a large amount of data to a pre-trained machine learning model on Azure through an online endpoint. While batch endpoints support adding a reference to a blob file as input, the user is unsure how to do the same for online endpoints. The examples seen so far only pass the payload as JSON, and the user is looking for a way to pass a blob storage file's URI as the payload.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/891865\/how-to-use-blob-storage-file-as-input-to-azure-ml",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":12.6,
        "Challenge_reading_time":6.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":188.1755805556,
        "Challenge_title":"How to use blob storage file as input to azure ml endpoint",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":99,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=a66ab675-b0e9-4f63-b57d-101663c4aaa0\">@Mostafa Mansour  <\/a> Thanks for the question. I have checked internally with the product team, Currently Online endpoints are for Realtime synchronous requests.    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.5,
        "Solution_reading_time":8.04,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":51.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1111111111,
        "Challenge_answer_count":1,
        "Challenge_body":"Does SageMaker Multi-Model Endpoint support SageMaker Model Monitor?",
        "Challenge_closed_time":1590501508000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590501108000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring whether SageMaker Multi-Model Endpoint supports SageMaker Model Monitor.",
        "Challenge_last_edit_time":1668554201782,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUq2z-BEt7TnmZ8vFYs-Hu7g\/does-sagemaker-multi-model-endpoint-support-sagemaker-model-monitor",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":19.2,
        "Challenge_reading_time":1.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":0.1111111111,
        "Challenge_title":"Does SageMaker Multi-Model Endpoint support SageMaker Model Monitor?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":248.0,
        "Challenge_word_count":15,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Amazon SageMaker Model Monitor currently supports only endpoints that host a single model and does not support monitoring multi-model endpoints. For information on using multi-model endpoints, see Host Multiple Models with Multi-Model Endpoints .  https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor.html",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925566336,
        "Solution_link_count":1.0,
        "Solution_readability":20.5,
        "Solution_reading_time":4.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":34.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":39.1253477778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>While opening ml studio (classic) , shows a pop up message like it will retire on 31 August 2024 . Couldn't close the message<\/p>",
        "Challenge_closed_time":1650491049672,
        "Challenge_comment_count":1,
        "Challenge_created_time":1650350198420,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to open Machine Learning Studio (classic) as a pop-up message appears stating that it will retire on August 31, 2024, and they are unable to close the message.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/817073\/cant-open-machine-learning-studio-(classic)",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.6,
        "Challenge_reading_time":2.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":39.1253477778,
        "Challenge_title":"Can't open Machine learning studio (classic)",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":27,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=065b67d4-0031-46e3-b16b-02416c45d955\">@Asheekha  <\/a>     <\/p>\n<p>Sorry about your experience again, I checked internally about the issue and my colleague confirmed that this is a known issue of Chrome browser, which can be fixed by cleaning the cookie.     <\/p>\n<p>Please try to clean the cookies and I hope this helps! Please let me know if you are still blocked by this issue.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p><em>-Please kindly accept the answer to help the community, thanks a lot.<\/em>    <\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.1,
        "Solution_reading_time":6.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":79.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.4894444444,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi, it seems that Sagemaker Batch Transform is limited to 100MB payloads\nI'd like to run preds against a 5GB csv file, what the recommended way to do so?",
        "Challenge_closed_time":1532628166000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1532619204000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge with Sagemaker Batch Transform as it has a limit of 100MB payloads. The user wants to run predictions against a 5GB csv file and is seeking recommendations on how to do so.",
        "Challenge_last_edit_time":1668425051631,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUlefH1ni4QOaulUT4870D5g\/sagemaker-batch-transform",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.0,
        "Challenge_reading_time":2.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":2.4894444444,
        "Challenge_title":"Sagemaker batch transform",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":376.0,
        "Challenge_word_count":31,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"SageMaker Batch Transform will automatically split your input file into whatever payload size is specified if you use `\"SplitType\": \"Line\"` and `\"BatchStrategy\": \"MultiRecord\"`. There's no need to split files yourself or to use large payload sizes unless you have very large single records.\n\nHope that helps!",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925544636,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":3.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":46.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":13.0756255556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We have developed and deployed machine learning models in AML Studio. The models were deployed using ACI and we have REST endpoints that we can make calls to successfully. Next thing that I need to do is to secure the endpoints using TLS. I am going through the following article:    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-secure-web-service#enable\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-secure-web-service#enable<\/a>    <\/p>\n<p>The article suggests that I need to get a domain and then update our DNS point to the IP address of scoring endpoint. I have a subdomain  ready to use but as for the IP address, I can't work out where I would get the IP address of the scoring endpoint and how I would even be able to map this to the endpoint as the current endpoint do not contain and IP address and look nothing like the example in the article.    <\/p>\n<p>URIs currently look like the following:    <br \/>\n<a href=\"http:\/\/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx.northeurope.azurecontainer.io\/score\">http:\/\/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx.northeurope.azurecontainer.io\/score<\/a>    <\/p>\n<p>Anyone able to help with this one please as it's a little confusing and I can't find any guidance online anywhere?<\/p>",
        "Challenge_closed_time":1616038303612,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615991231360,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has developed and deployed machine learning models in AML Studio using ACI and has REST endpoints that need to be secured using TLS. They are following an article that suggests getting a domain and updating DNS point to the IP address of the scoring endpoint. However, the user is unable to find the IP address of the scoring endpoint and is confused about how to map it to the endpoint as the current endpoint does not contain an IP address. They are seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/318807\/secure-azure-machine-learning-rest-endpoints-(depl",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":16.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":13.0756255556,
        "Challenge_title":"Secure Azure Machine Learning REST Endpoints (deployed in ACI) with TLS",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":179,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello,<\/p>\n<p>You can do it according to DNS.<\/p>\n<p>A \u201cURL\u201d is a full specification to a page. For example:<\/p>\n<p><a href=\"http:\/\/example.com\/this_is_example.html\">http:\/\/example.com\/this_is_example.html<\/a> is a URL. It has three parts:<\/p>\n<p>The protocol specifier: http:<\/p>\n<p>The domain name: example.com<\/p>\n<p>The page location: \/this_is_example.html<\/p>\n<p>The protocol specifies the port that will be used. http, for example, is  <br \/>\nport 80. ftp uses ports 20 and 21. SMTP, the mail sending protocol, is usually  <br \/>\non port 25. You can actually find the full list of \u201cofficial\u201d ports here.<\/p>\n<p>It\u2019s only the domain name that has an IP address associated with it. So that\u2019s what you would be looking up.<\/p>\n<p>My approach is to use the \u201cping\u201d command in a Windows command prompt. For  <br \/>\nexample:<\/p>\n<p>C:\\&gt;ping example.com<\/p>\n<p>Then you can get it.<\/p>\n<p>Regards,  <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":6.3,
        "Solution_reading_time":11.49,
        "Solution_score_count":0.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":131.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":158.096525,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>My Azure ML studio web service used to run fine but is unavailable for me to use now. Anybody know a fix?  <\/p>\n<p>Error Message: Could not authorize the request. Make sure the request has an Authorization header with a bearer token, of the form &quot;Authorization: Bearer [token]&quot;. See online help to find which tokens are valid for this request.  <br \/>\nSite Path: \/workspaces\/fde0912ad97d4a94b9b2baaafd54c3e1\/webservices\/378f095e8260497697790a6d65fe9ff8\/endpoints\/default  <br \/>\nActivity ID: 82e53138-b3b9-4a94-8695-0b8152c505ac  <br \/>\nRequest ID: e3e8fbe7-6161-411b-9c2c-e2a609436353  <br \/>\nWorkspace ID: fde0912ad97d4a94b9b2baaafd54c3e1  <br \/>\nWorkspace Type: Free  <br \/>\nUser Role: Owner  <br \/>\nTenant ID: f8cdef31-a31e-4b4a-93e4-5f571e91255a<\/p>",
        "Challenge_closed_time":1649047941240,
        "Challenge_comment_count":2,
        "Challenge_created_time":1648478793750,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an authorization error while trying to access their Azure ML studio web service. The error message suggests that the request needs to have an Authorization header with a bearer token. The user is seeking a fix for this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/790341\/authorization-bearer-(token)-error",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.2,
        "Challenge_reading_time":10.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":158.096525,
        "Challenge_title":"Authorization: Bearer [token] Error",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":91,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=4f826b11-7b14-4523-b00c-cbe2449313bf\">@dasa8  <\/a>     <\/p>\n<p>Update: The bug has been confirmed and the ETA is 2-3 weeks for the bug fixing. I am sorry for all the inconveniences.     <\/p>\n<p>The workaround for now is to use studio classic portal to manage the classic web service as below screenshot:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/189568-microsoftteams-image-9.png?platform=QnA\" alt=\"189568-microsoftteams-image-9.png\" \/>    <\/p>\n<p>Thanks for the understanding and sorry for the experience again.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p><em>-Please kindly accept the answer if you feel helpful, thanks a lot!<\/em>    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.9,
        "Solution_reading_time":8.87,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":81.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":237.2008941667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The Amazon documentation lists several approaches to evaluate a model (e.g. cross validation, etc.) however these methods does not seem to be available in the Sagemaker Java SDK. \nCurrently if we want to do 5-fold cross validation it seems the only option is to create 5 models (and also deploy 5 endpoints) one model for each subset of data and manually compute the performance metric (recall, precision, etc.). <\/p>\n\n<p>This approach is not very efficient and can also be expensive need to deploy k-endpoints, based on the number of folds in the k-fold validation.<\/p>\n\n<p>Is there another way to test the performance of a model?<\/p>",
        "Challenge_closed_time":1526758178327,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526673699553,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in evaluating a model in Sagemaker Java SDK as the available methods for model evaluation, such as cross-validation, are not present. The only option available is to create multiple models and endpoints for each subset of data, which is inefficient and expensive. The user is seeking an alternative method to test the performance of the model.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50418501",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":8.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":23.4663261111,
        "Challenge_title":"Sagemaker model evaluation",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1442.0,
        "Challenge_word_count":107,
        "Platform":"Stack Overflow",
        "Poster_created_time":1428454496052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Amazon SageMaker is a set of multiple components that you can choose which ones to use. <\/p>\n\n<p>The built-in algorithms are designed for (infinite) scale, which means that you can have huge datasets and be able to build a model with them quickly and with low cost. Once you have large datasets you usually don't need to use techniques such as cross-validation, and the recommendation is to have a clear split between training data and validation data. Each of these parts will be defined with an input channel when you are submitting a training job.  <\/p>\n\n<p>If you have a small amount of data and you want to train on all of it and use cross-validation to allow it, you can use a different part of the service (interactive notebook instance). You can bring your own algorithm or even container image to be used in the development, training or hosting. You can have any python code based on any machine learning library or framework, including scikit-learn, R, TensorFlow, MXNet etc. In your code, you can define cross-validation based on the training data that you copy from S3 to the worker instances. <\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1527527622772,
        "Solution_link_count":0.0,
        "Solution_readability":10.3,
        "Solution_reading_time":13.5,
        "Solution_score_count":3.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":192.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":25.6642,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>So I'm having an issue with setting up an endpoint for a machine learning model which was trained using Azure AutoML. When I try to test the deployed model, I get an error saying that the service is temporarily unavailable. After looking online, I found that this might happen because of an error in the run() function in the entry script.  <\/p>\n<p>When I try to test the entry script on a notebook in Azure ML studio, on a fresh compute instance, there are two problems:  <br \/>\nFirst I get the error: <em>AttributeError: 'MSIAuthentication' object has no attribute 'get_token'<\/em>  <br \/>\nWhich is solved by running: <em>pip install azureml-core<\/em>  <\/p>\n<p>Then I get the error: <em>ModuleNotFoundError: No module named 'azureml.automl.runtime'<\/em>  <br \/>\nWhich I try to solve using: <em>pip install azureml-automl-runtime<\/em>  <br \/>\nBut this throws a lot of incompatibility errors during the installation. When I then try to run the entry script I get an error with the message: &quot;<em>Failed while applying learned transformations.<\/em>&quot;  <\/p>\n<p>So I setup a new virtual environment on my local machine in which I only installed azure-automl-runtime. Using that setup the entry script works perfectly fine. So I created a custom environment in Azure ML studio using the conda file of that local virtual environment. Unfortunatly I still get the error &quot;service temporarily unavailable&quot; when trying to test the endpoint.  <\/p>\n<p>I have a feeling the default Azure ML containers are incompatible with azureml-automl-runtime, since installing this on a ML studio notebook also throws a lot of errors.   <\/p>\n<p>I feel like there should be an elegant way to deploy an AutoML model, am I doing something wrong here?   <\/p>\n<p><strong>Update:<\/strong> I found out I didn't change the environment for the endpoint, so that is why I was getting the same error probably. When using the custom environment I got errors from gunicorn, so I also added that package to the environment. Now I get the following error:  <\/p>\n<pre><code>      File &quot;\/var\/azureml-server\/entry.py&quot;, line 1, in &lt;module&gt;\n    import create_app\n  File &quot;\/var\/azureml-server\/create_app.py&quot;, line 4, in &lt;module&gt;\n    from routes_common import main\n  File &quot;\/var\/azureml-server\/routes_common.py&quot;, line 39, in &lt;module&gt;\n    from azure.ml.api.exceptions.ClientSideException import ClientSideException\nModuleNotFoundError: No module named 'azure.ml'\n<\/code><\/pre>\n<p>So what do I install to fix this? Is there a list somewhere of required packages for an ML model endpoint?  <\/p>",
        "Challenge_closed_time":1627463996087,
        "Challenge_comment_count":2,
        "Challenge_created_time":1627371604967,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with setting up an endpoint for a machine learning model trained using Azure AutoML. The deployed model is showing a \"service temporarily unavailable\" error. The user tried to solve the issue by testing the entry script on a notebook in Azure ML studio, but encountered errors related to the \"MSIAuthentication\" object and \"ModuleNotFoundError\". The user then created a custom environment in Azure ML studio using the conda file of a local virtual environment, but still faced the same error. The user suspects that the default Azure ML containers are incompatible with azureml-automl-runtime. The user is now trying to fix the error \"No module named 'azure.ml'\" and is looking for a list of required packages for an ML model",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/490809\/automated-machine-learning-model-deployment-issue",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.1,
        "Challenge_reading_time":32.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":25.6642,
        "Challenge_title":"Automated machine learning model deployment issue",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":384,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I managed to fix the issue with the environment by just adding everything that would throw an error. Then I found out the return value has to be a json\/dict object, which if not done throws the exact same 'service temporarily unavailable' error.     <\/p>\n<p>But my issue with the confusing curated environments and azureml-automl-runtime in ML studio notebooks remain. Maybe this is worth looking into <a href=\"\/users\/na\/?userid=0e711f59-976b-4899-a912-2f0dd680421a\">@Ramr-msft  <\/a> .<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":6.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":69.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1516367794196,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":1055.1677944445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>System information\nOS Platform and Distribution: Windows 10\nMLflow installed: using pip\nMLflow version: version 1.24.0\n**Python version: Python 3.9.7 **<\/p>\n<p>Describe the problem\nI have created a docker-compose system with a backend\/artifact storages, mlflow server and nginx to add an authentication layer.<\/p>\n<pre><code>...\nmlflow:\n        restart: always\n        build: .\n        environment:\n            - AWS_ACCESS_KEY_ID=${MINIO_USR}\n            - AWS_SECRET_ACCESS_KEY=${MINIO_PASS}       \n        expose:\n            - '5000'\n        networks:\n            - frontend\n            - backend\n        depends_on:\n            - storage                       \n        image: 'mlflow:Dockerfile'\n        container_name: mlflow_server_nginx\n\n    nginx:\n        restart: always\n        build: .\/nginx\n        container_name: mlflow_nginx\n        ports:\n            - 5043:443\n        links:\n            - mlflow:mlflow\n        volumes:\n            - 'path\/to\/nginx\/auth:\/etc\/nginx\/conf.d'\n            - 'path\/to\/nginx\/nginx.conf:\/etc\/nginx\/nginx.conf:ro'\n        networks:\n            - frontend\n        depends_on:\n            - mlflow\n<\/code><\/pre>\n<p>I have created an user\/password via htpasswd and a custom SSL CA (.pem\/.key) using openssl and my-mlflow.com server-name.<\/p>\n<p>When the docker-compose system is built i can access to mlflow UI via my browser. But when i try to create a new experiment using python trying diferent approaches, i get next errors:\nExecuted code 1:<\/p>\n<pre><code># Setting the requried environment variables\nos.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] = 'user'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'password'\n# Set username and password for added authentication\n#os.environ['MLFLOW_TRACKING_URI '] = 'https:\/\/localhost:5043\/'\n#os.environ['MLFLOW_TRACKING_USERNAME '] = 'user'\n#os.environ['MLFLOW_TRACKING_PASSWORD '] = 'password'\n#os.environ['MLFLOW_TRACKING_SERVER_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n#os.environ['MLFLOW_TRACKING_CLIENT_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n# MLflow enviroment\nremote_server_uri = &quot;https:\/\/user:password@localhost:5043\/&quot; # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\n\nmlflow.set_experiment(&quot;MLflow_demo&quot;)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>MlflowException: API request to https:\/\/user:password@localhost:5043\/api\/2.0\/mlflow\/experiments\/list failed with exception HTTPSConnectionPool(host='localhost', port=5043): Max retries exceeded with url: \/api\/2.0\/mlflow\/experiments\/list?view_type=ALL (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate (_ssl.c:1108)')))\n<\/code><\/pre>\n<p>After read some notes in the documentation and realated issues I tryed next<\/p>\n<pre><code># Setting the requried environment variables\nos.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] = 'user'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'password'\n# Set username and password for added authentication\n#os.environ['MLFLOW_TRACKING_URI '] = 'https:\/\/localhost:5043\/'\n#os.environ['MLFLOW_TRACKING_USERNAME '] = 'user'\n#os.environ['MLFLOW_TRACKING_PASSWORD '] = 'password'\n#os.environ['MLFLOW_TRACKING_SERVER_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\nos.environ['MLFLOW_TRACKING_CLIENT_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n# MLflow enviroment\nremote_server_uri = &quot;https:\/\/user:password@localhost:5043\/&quot; # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\n\nmlflow.set_experiment(&quot;MLflow_demo&quot;)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>MlflowException: API request to https:\/\/user:password@localhost:5043\/api\/2.0\/mlflow\/experiments\/list failed with exception HTTPSConnectionPool(host='localhost', port=5043): Max retries exceeded with url: \/api\/2.0\/mlflow\/experiments\/list?view_type=ALL (Caused by SSLError(SSLError(9, '[SSL] PEM lib (_ssl.c:4012)')))\n<\/code><\/pre>\n<p>Finally<\/p>\n<pre><code># Setting the requried environment variables\nos.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] = 'user'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'password'\n# Set username and password for added authentication\n#os.environ['MLFLOW_TRACKING_URI '] = 'https:\/\/localhost:5043\/'\n#os.environ['MLFLOW_TRACKING_USERNAME '] = 'user'\n#os.environ['MLFLOW_TRACKING_PASSWORD '] = 'password'\nos.environ['MLFLOW_TRACKING_SERVER_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n#os.environ['MLFLOW_TRACKING_CLIENT_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n# MLflow enviroment\nremote_server_uri = &quot;https:\/\/user:password@localhost:5043\/&quot; # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\n\nmlflow.set_experiment(&quot;MLflow_demo&quot;)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>MlflowException: API request to https:\/\/user:password@localhost:5043\/api\/2.0\/mlflow\/experiments\/list failed with exception HTTPSConnectionPool(host='localhost', port=5043): Max retries exceeded with url: \/api\/2.0\/mlflow\/experiments\/list?view_type=ALL (Caused by SSLError(SSLCertVerificationError(&quot;hostname 'localhost' doesn't match '*.my-mlflow.com'&quot;)))\n<\/code><\/pre>\n<p>Can you give me some hints about how to solve it?<\/p>\n<p>Thank you very much!\nFernando....<\/p>",
        "Challenge_closed_time":1652276299263,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648650339347,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to connect mlflow server via nginx ssl authentication. They have created a docker-compose system with a backend\/artifact storages, mlflow server, and nginx to add an authentication layer. The user has created a user\/password via htpasswd and a custom SSL CA (.pem\/.key) using openssl and my-mlflow.com server-name. They are able to access the mlflow UI via their browser, but when they try to create a new experiment using python, they get errors related to SSL certificate verification. The user has tried different approaches to solve the issue but has not been successful.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71679081",
        "Challenge_link_count":12,
        "Challenge_participation_count":1,
        "Challenge_readability":18.4,
        "Challenge_reading_time":68.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":45,
        "Challenge_solved_time":1007.2110877778,
        "Challenge_title":"How can I connect mlflow server via nginx ssl authentication?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":625.0,
        "Challenge_word_count":377,
        "Platform":"Stack Overflow",
        "Poster_created_time":1580841805372,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seville, Spain",
        "Poster_reputation_count":33.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>You can set:<\/p>\n<pre><code>os.environ['MLFLOW_TRACKING_INSECURE_TLS'] = 'true'\n<\/code><\/pre>\n<p>And then try to get your cert-chain straight from there for production use.<\/p>\n<p>Also see Documentation: <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#id19\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/tracking.html#id19<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1652448943407,
        "Solution_link_count":2.0,
        "Solution_readability":14.1,
        "Solution_reading_time":4.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":26.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1619177157943,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":835.0,
        "Answerer_view_count":516.0,
        "Challenge_adjusted_solved_time":139.5224691667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In a Vertex AI pipeline component,I try:<\/p>\n<pre><code>def my_comp(project_id: str, location: str, endpoint_id: str, endpoint: Output[Artifact]):\n    import google.cloud.aiplatform as aip\n    endpoints = aip.Endpoint.list()\n...\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code>'aiplatform.endpoints.list' denied on resource '\/\/aiplatform.googleapis.com\/projects\/...\n<\/code><\/pre>\n<p>My service account has owner permissions, and it works outside of the component. What do I need to do?<\/p>",
        "Challenge_closed_time":1663223342092,
        "Challenge_comment_count":6,
        "Challenge_created_time":1662721061203,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a permission issue while trying to use a Vertex AI pipeline component. The error message indicates that the 'aiplatform.endpoints.list' permission is denied on a specific resource. The user's service account has owner permissions, but the issue persists. The user is seeking guidance on how to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73661090",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":10.1,
        "Challenge_reading_time":7.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":139.5224691667,
        "Challenge_title":"How do I give Vertex AI pipeline component permissions?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":62.0,
        "Challenge_word_count":58,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>This permission denied on resource issue can be resolved by using import statement:<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as aiplatform\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":2.19,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":20.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1565376125572,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":31.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":10701.8852247222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a endpoint in Amazon SageMaker (Image-classification algorithm) in Jupyter notebook that works fine. In Lambda function works fine too, when I call the Lambda function from API Gateway, from test of API Gateway, works fine too.<\/p>\n<p>The problem is when I call the API from Postman according this answer: <a href=\"https:\/\/stackoverflow.com\/questions\/39660074\/post-image-data-using-postman\">&quot;Post Image data using POSTMAN&quot;<\/a><\/p>\n<p>The code in Lambda is:<\/p>\n<pre><code>import boto3\nimport json\nimport base64\n\nENDPOINT_NAME = &quot;DEMO-XGBoostEndpoint-Multilabel&quot;\nruntime= boto3.client(&quot;runtime.sagemaker&quot;)\nimagen_ = &quot;\/tmp\/imageToProcess.jpg&quot;\n\ndef write_to_file(save_path, data):\n    with open(save_path, &quot;wb&quot;) as f:\n        f.write(base64.b64decode(data))\n\ndef lambda_handler(event, context):\n    img_json = json.loads(json.dumps(event))\n\n    write_to_file(imagen_, json.dumps(event, indent=2))\n\n    with open(imagen_, &quot;rb&quot;) as image:\n        f = image.read()\n        b = bytearray(f)\n\n    payload = b\n\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                       ContentType=&quot;application\/x-image&quot;,\n                                       Body=payload)\n\n    #print(response)\n    result = json.loads(response[&quot;Body&quot;].read().decode())\n    print(result)\n    predicted_label=[]\n    classes = [&quot;chair&quot;, &quot;handbag&quot;, &quot;person&quot;, &quot;traffic light&quot;, &quot;clock&quot;]\n    for idx, val in enumerate(classes):\n        print(&quot;%s:%f &quot;%(classes[idx], result[idx]), end=&quot;&quot;)\n        predicted_label += (classes[idx], result[idx])\n\n    return {\n      &quot;statusCode&quot;: 200,\n      &quot;headers&quot;: { &quot;content-type&quot;: &quot;application\/json&quot;},\n      &quot;body&quot;:  predicted_label\n}\n<\/code><\/pre>\n<p>The error is:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/var\/task\/lambda_function.py&quot;, line 26, in lambda_handler\n    Body=payload)\n  File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 316, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 626, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message &quot;unable to evaluate payload provided&quot;. See https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/DEMO-XGBoostEndpoint-Multilabel in account 866341179300 for more information. ```\n<\/code><\/pre>",
        "Challenge_closed_time":1597003004372,
        "Challenge_comment_count":2,
        "Challenge_created_time":1595742856227,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while calling an Amazon SageMaker endpoint for an image-classification algorithm from Postman. The Lambda function and API Gateway work fine, but the error occurs when calling the API from Postman. The error message indicates that the payload provided is unable to be evaluated.",
        "Challenge_last_edit_time":1595997749087,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63096583",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":18.4,
        "Challenge_reading_time":35.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":350.0411513889,
        "Challenge_title":"SageMaker: An error occurred (ModelError) when calling the InvokeEndpoint operation: unable to evaluate payload provided",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":3996.0,
        "Challenge_word_count":218,
        "Platform":"Stack Overflow",
        "Poster_created_time":1565376125572,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I resolved with <a href=\"https:\/\/medium.com\/swlh\/upload-binary-files-to-s3-using-aws-api-gateway-with-aws-lambda-2b4ba8c70b8e\" rel=\"nofollow noreferrer\">this<\/a> post:<\/p>\n<p>Thank all<\/p>\n<p>Finally the code in lambda function is:<\/p>\n<pre><code>import os\nimport boto3\nimport json\nimport base64\n\nENDPOINT_NAME = os.environ['endPointName']\nCLASSES = &quot;[&quot;chair&quot;, &quot;handbag&quot;, &quot;person&quot;, &quot;traffic light&quot;, &quot;clock&quot;]&quot;\nruntime= boto3.client(&quot;runtime.sagemaker&quot;)\n\ndef lambda_handler(event, context):\n    file_content = base64.b64decode(event['content'])\n\n    payload = file_content\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType=&quot;application\/x-image&quot;, Body=payload)\n\n    result = json.loads(response[&quot;Body&quot;].read().decode())\n    print(result)\n    predicted_label=[]\n    classes = CLASSES\n    for idx, val in enumerate(classes):\n       print(&quot;%s:%f &quot;%(classes[idx], result[idx]), end=&quot;&quot;)\n       predicted_label += (classes[idx], result[idx])\n\n    return {\n      &quot;statusCode&quot;: 200,\n      &quot;headers&quot;: { &quot;content-type&quot;: &quot;application\/json&quot;},\n      &quot;body&quot;:  predicted_label\n    }\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1634524535896,
        "Solution_link_count":1.0,
        "Solution_readability":22.8,
        "Solution_reading_time":16.28,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":74.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1641848506127,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Mumbai, India",
        "Answerer_reputation_count":2252.0,
        "Answerer_view_count":131.0,
        "Challenge_adjusted_solved_time":1.7314472222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm currently developing some ETL for my ML model with AWS. The thing is that I want to <strong>trigger<\/strong> a Lambda when some Sagemaker Processing Job is finished. And the <strong>event<\/strong> passed to the Lambda, should be the configuration info (job name, arguments, etc..) of the Sagemaker Processing Job.<\/p>\n<p><strong>Q1<\/strong>: How can I do to <em>trigger the event<\/em> when the Processing Job is finished?<\/p>\n<p><strong>Q2<\/strong>: How can I do to pass the <em>Processing Job configurations as an event<\/em> for the Lambda?<\/p>",
        "Challenge_closed_time":1643913781760,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643907548550,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is developing ETL for their ML model with AWS and wants to trigger a Lambda function when a Sagemaker Processing Job is finished. They also want to pass the configuration information of the Sagemaker Processing Job as an event to the Lambda function. The user is seeking guidance on how to achieve these two objectives.",
        "Challenge_last_edit_time":1643931586140,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70975320",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.1,
        "Challenge_reading_time":7.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.7314472222,
        "Challenge_title":"EventBridge trigger: Sagemaker Processing Job finished",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":504.0,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1519511545083,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":43.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>You can use the following EventBridge rule pattern:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;source&quot;: [&quot;aws.sagemaker&quot;],\n  &quot;detail-type&quot;: [&quot;SageMaker Processing Job State Change&quot;],\n  &quot;detail&quot;: {\n    &quot;ProcessingJobStatus&quot;: [&quot;Failed&quot;, &quot;Completed&quot;, &quot;Stopped&quot;]\n  }\n}\n<\/code><\/pre>\n<p>The ProcessingJobStatus list can be modified based on which statuses you want to handle.<\/p>\n<p>You can set a Lambda function as the target of your EventBridge rule.<\/p>\n<p>Here is a sample event which will be passed to your Lambda, taken from AWS console:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;version&quot;: &quot;0&quot;,\n  &quot;id&quot;: &quot;0a15f67d-aa23-0123-0123-01a23w89r01t&quot;,\n  &quot;detail-type&quot;: &quot;SageMaker Processing Job State Change&quot;,\n  &quot;source&quot;: &quot;aws.sagemaker&quot;,\n  &quot;account&quot;: &quot;123456789012&quot;,\n  &quot;time&quot;: &quot;2019-05-31T21:49:54Z&quot;,\n  &quot;region&quot;: &quot;us-east-1&quot;,\n  &quot;resources&quot;: [&quot;arn:aws:sagemaker:us-west-2:012345678987:processing-job\/integ-test-analytics-algo-54ee3282-5899-4aa3-afc2-7ce1d02&quot;],\n  &quot;detail&quot;: {\n    &quot;ProcessingInputs&quot;: [{\n      &quot;InputName&quot;: &quot;InputName&quot;,\n      &quot;S3Input&quot;: {\n        &quot;S3Uri&quot;: &quot;s3:\/\/input\/s3\/uri&quot;,\n        &quot;LocalPath&quot;: &quot;\/opt\/ml\/processing\/input\/local\/path&quot;,\n        &quot;S3DataType&quot;: &quot;MANIFEST_FILE&quot;,\n        &quot;S3InputMode&quot;: &quot;PIPE&quot;,\n        &quot;S3DataDistributionType&quot;: &quot;FULLYREPLICATED&quot;\n      }\n    }],\n    &quot;ProcessingOutputConfig&quot;: {\n      &quot;Outputs&quot;: [{\n        &quot;OutputName&quot;: &quot;OutputName&quot;,\n        &quot;S3Output&quot;: {\n          &quot;S3Uri&quot;: &quot;s3:\/\/output\/s3\/uri&quot;,\n          &quot;LocalPath&quot;: &quot;\/opt\/ml\/processing\/output\/local\/path&quot;,\n          &quot;S3UploadMode&quot;: &quot;CONTINUOUS&quot;\n        }\n      }],\n      &quot;KmsKeyId&quot;: &quot;KmsKeyId&quot;\n    },\n    &quot;ProcessingJobName&quot;: &quot;integ-test-analytics-algo-54ee3282-5899-4aa3-afc2-7ce1d02&quot;,\n    &quot;ProcessingResources&quot;: {\n      &quot;ClusterConfig&quot;: {\n        &quot;InstanceCount&quot;: 3,\n        &quot;InstanceType&quot;: &quot;ml.c5.xlarge&quot;,\n        &quot;VolumeSizeInGB&quot;: 5,\n        &quot;VolumeKmsKeyId&quot;: &quot;VolumeKmsKeyId&quot;\n      }\n    },\n    &quot;StoppingCondition&quot;: {\n      &quot;MaxRuntimeInSeconds&quot;: 2000\n    },\n    &quot;AppSpecification&quot;: {\n      &quot;ImageUri&quot;: &quot;012345678901.dkr.ecr.us-west-2.amazonaws.com\/processing-uri:latest&quot;\n    },\n    &quot;NetworkConfig&quot;: {\n      &quot;EnableInterContainerTrafficEncryption&quot;: true,\n      &quot;EnableNetworkIsolation&quot;: false,\n      &quot;VpcConfig&quot;: {\n        &quot;SecurityGroupIds&quot;: [&quot;SecurityGroupId1&quot;, &quot;SecurityGroupId2&quot;, &quot;SecurityGroupId3&quot;],\n        &quot;Subnets&quot;: [&quot;Subnet1&quot;, &quot;Subnet2&quot;]\n      }\n    },\n    &quot;RoleArn&quot;: &quot;arn:aws:iam::012345678987:role\/SageMakerPowerUser&quot;,\n    &quot;ExperimentConfig&quot;: {},\n    &quot;ProcessingJobArn&quot;: &quot;arn:aws:sagemaker:us-west-2:012345678987:processing-job\/integ-test-analytics-algo-54ee3282-5899-4aa3-afc2-7ce1d02&quot;,\n    &quot;ProcessingJobStatus&quot;: &quot;Completed&quot;,\n    &quot;LastModifiedTime&quot;: 1589879735000,\n    &quot;CreationTime&quot;: 1589879735000\n  }\n}\n<\/code><\/pre>\n<p><strong>Edit:<\/strong><\/p>\n<p>If you want to match a ProcessingJobName with specific prefix:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;source&quot;: [&quot;aws.sagemaker&quot;],\n  &quot;detail-type&quot;: [&quot;SageMaker Processing Job State Change&quot;],\n  &quot;detail&quot;: {\n    &quot;ProcessingJobStatus&quot;: [&quot;Failed&quot;, &quot;Completed&quot;, &quot;Stopped&quot;],\n    &quot;ProcessingJobName&quot;: [{\n      &quot;prefix&quot;: &quot;standarize-data&quot;\n    }]\n  }\n}\n<\/code><\/pre>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1643920466910,
        "Solution_link_count":0.0,
        "Solution_readability":31.6,
        "Solution_reading_time":52.65,
        "Solution_score_count":3.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":193.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1568318861627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":486.0,
        "Answerer_view_count":75.0,
        "Challenge_adjusted_solved_time":90.3382358333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I trained an xgboost model on AWS-Sagemaker and created an endpoint. Now I want to call the endpoint using AWS Lambda and AWS API. I created an lambda function and added the below mentioned code for my xgboost model. When I try to test it, the function is throwing a ParamValidation error. Here is my code<\/p>\n\n<pre><code>import json\nimport os\nimport csv\nimport io\nimport boto3\nendpointname =os.environ['endpointname'] #name of the endpoint I created in sagemaker\nruntime = boto3.client('runtime.sagemaker')\ndef lambda_handler(event, context):\n    print(\"Recieved Event: \"+json.dumps(event,indent=2))\n    data=json.loads(json.dumps(event))\n    print(data)\n    response = runtime.invoke_endpoint(EndpointName=endpointname,ContentType='text\/csv',Body=data)\n    print(response)\n    result = json.loads(response['Body'].read().decode())\n    print(int(float(result))) #sagemaker xgb returns bytes type for the test case\n<\/code><\/pre>\n\n<p>The test event I created is dict type. The function is throwing  <code>Invalid type for parameter Body, value: {'Time':'7'}, type: &lt;class 'dict'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object<\/code>\nIt means I should pass either byte or bytearray instead of dict type into my event. But when I read this <a href=\"https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/python-programming-model-handler-types.html\" rel=\"nofollow noreferrer\">AWS Lambda doc<\/a> It says that my event type can only be dict,int,list,float,str, or None type. I followed the steps mentioned in <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">this<\/a> aws doc to create my lambda function. Can someone please explain why my code is throwing above mentioned error?<\/p>",
        "Challenge_closed_time":1574162755396,
        "Challenge_comment_count":0,
        "Challenge_created_time":1573829888267,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user trained an XGBoost model on AWS Sagemaker and created an endpoint. They are trying to call the endpoint using AWS Lambda and AWS API, but the Lambda function is throwing a ParamValidation error. The error message suggests that the user should pass either byte or bytearray instead of dict type into their event. However, according to AWS Lambda documentation, the event type can only be dict, int, list, float, str, or None type. The user followed the steps mentioned in an AWS blog to create their Lambda function and is seeking an explanation for the error.",
        "Challenge_last_edit_time":1573837537747,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58879596",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":24.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":92.4630913889,
        "Challenge_title":"Unable to call XG-Boost endpoint created in sagemaker using AWS-Lambda",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":519.0,
        "Challenge_word_count":210,
        "Platform":"Stack Overflow",
        "Poster_created_time":1568318861627,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hyderabad, Telangana, India",
        "Poster_reputation_count":486.0,
        "Poster_view_count":75.0,
        "Solution_body":"<p><code>data=json.loads(json.dumps(event))<\/code> is a redundant operation. <code>data=event<\/code> will return <code>True<\/code>. The event we provided for the test case is of type dict. It has a key value pair. key can be anything and the value should be a single string of all the predictor variables separated by comas. For predicting the output, we need value of the test case. So declare, for example, <code>payload=data['key']<\/code> then change <code>Body=payload<\/code> inside <code>response<\/code>. Then it will work.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":6.77,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":72.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1546566576912,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":25.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":49.4712094445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I built a chalice web-app that is hosted in an s3 bucket and calls an xgboost endpoint. I keep getting an error when I invoke the model through the web-app. When I looked into the Lambda log files I discovered my input is not properly decoding. <code>input_text = app.current_request.raw_body.decode()<\/code> What would be the correct code to decode the input from binary so I can pass in a regular string to my endpoint?<\/p>\n\n<p>Here is the error:<\/p>\n\n<p>botocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from model with message \"could not convert string to float: user_input=1%\". <\/p>\n\n<p>Here is my index.html file:<\/p>\n\n<pre><code>&lt;html&gt;\n&lt;head&gt;&lt;\/head&gt;\n&lt;body&gt;\n&lt;form method=\"post\" action=\"&lt;chalice_deployed_http&gt;\"&gt;\n\n&lt;input type=\"text\" name=\"user_input\"&gt;&lt;br&gt;\n\n&lt;input type=\"submit\" value=\"Submit\"&gt;\n&lt;\/form&gt;\n&lt;\/body&gt;\n&lt;\/html&gt;\n<\/code><\/pre>\n\n<p>Here is my app.py file:<\/p>\n\n<pre><code>try:\n    from StringIO import StringIO\nexcept ImportError:\n    from io import StringIO\n\nfrom io import BytesIO\nimport csv\nimport sys, os, base64, datetime, hashlib, hmac\nfrom chalice import Chalice, NotFoundError, BadRequestError\nimport boto3\n\n\napp = Chalice(app_name='&lt;name_of_chalice_app&gt;')\napp.debug = True\n\nsagemaker = boto3.client('sagemaker-runtime')\n\n@app.route('\/', methods=['POST'], content_types=['application\/x-www-form-urlencoded'])\ndef handle_data():\n    input_text = app.current_request.raw_body.decode()\n\n    res = sagemaker.invoke_endpoint(\n                    EndpointName='&lt;endpoint_name&gt;',\n                    Body=input_text,\n                    ContentType='text\/csv',\n                    Accept='Accept'\n                )\n    return res['Body'].read().decode()[0]\n<\/code><\/pre>\n\n<p>I should be able to pass in a string like this:<\/p>\n\n<p>'1,4,26,0.076923077,2,3,1,0.611940299,0.7818181820000001,0.40376569,0.571611506,0.12,12,1,0.0,2,1.0,1,2,6,3,1,1,1,1,1,3,1,0.000666667,1,1,2,2,-1.0,0.490196078,-1.0,0.633928571,6.0,145,2,2,1,3,2,2,1,3,2,3,3,-1.0,1,3,1,1,2,1,2,3,1,3,3,1,3,2,3,-1.0,3,3,1,2,2,1,3,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,0.3497921158934803,0'<\/p>\n\n<p>and get output like this:<\/p>\n\n<p>'5'<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/sb5Nw.jpg\" rel=\"nofollow noreferrer\">When I run it in a jupyter notebook it works.<\/a><\/p>",
        "Challenge_closed_time":1547657753670,
        "Challenge_comment_count":0,
        "Challenge_created_time":1547243089770,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error when invoking a SageMaker xgboost endpoint from a Chalice web-app hosted in an S3 bucket. The Lambda log files indicate that the input is not properly decoding, resulting in a client error (415) from the model. The user is seeking guidance on the correct code to decode the input from binary to pass in a regular string to the endpoint.",
        "Challenge_last_edit_time":1547479657316,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54154455",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":11.9,
        "Challenge_reading_time":32.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":115.1844166667,
        "Challenge_title":"How do you invoke a sagemaker xgboost endpoint from a chalice app?",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":805.0,
        "Challenge_word_count":223,
        "Platform":"Stack Overflow",
        "Poster_created_time":1546566576912,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":25.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>This worked:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>    input_text = app.current_request.raw_body\n    d = parse_qs(input_text)\n    lst = d[b'user_input'][0].decode()\n    res = sagemaker.invoke_endpoint(\n                    EndpointName='&lt;name-of-SageMaker-Endpoint&gt;',\n                    Body=lst,\n                    ContentType='text\/csv',\n                    Accept='Accept'\n                )\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":23.9,
        "Solution_reading_time":4.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405317204728,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":69.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":196.8378408333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have an endpoint running a trained SageMaker model on AWS, which expects the data on a specific format.<\/p>\n<p>Initially, the data has been processed on the client side of the application, it means, the <code>API Gateway<\/code> (which receives the POST API calls on AWS) used to receive pre-processed data, but now there's a change, the <code>API Gateway<\/code> will receive <strong>raw data<\/strong> from the client, and the job of pre-processing this data before sending to our SageMaker model is up to our workflow.<\/p>\n<p>What is the best way to create a pre-processing job on this workflow, without needing to re-train the model? My pre-process is just a bunch of dataframe transformations, no standardization or calculation with the training set required (it would not need to save any model file).<\/p>\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1602762793703,
        "Challenge_comment_count":0,
        "Challenge_created_time":1602162886373,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has an endpoint running a trained SageMaker model on AWS, which expects data on a specific format. The API Gateway used to receive pre-processed data, but now it will receive raw data from the client, and the job of pre-processing this data before sending it to the SageMaker model is up to the workflow. The user is looking for the best way to create a pre-processing job on this workflow without needing to re-train the model.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64263330",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.6,
        "Challenge_reading_time":10.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":166.640925,
        "Challenge_title":"Data Preprocessing on AWS SageMaker",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":450.0,
        "Challenge_word_count":134,
        "Platform":"Stack Overflow",
        "Poster_created_time":1405317204728,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":69.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>After some research, this is the solution I've followed:<\/p>\n<ul>\n<li>First I have created a <code>SKLearn<\/code> sagemaker model to do all the preprocess setup (I've built a Scikit-Learn custom class to handle all the preprocess steps, following this <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb\" rel=\"nofollow noreferrer\">AWS code<\/a>)<\/li>\n<li>Trained this preprocess model on my training data. My model, in specific, didn't need to be trained (it does not have any standardization or anything that would need to store training data parameters), but sagemaker requires the model to be trained.<\/li>\n<li>Loaded the trained legacy model that we had using the <code>Model<\/code> parameter.<\/li>\n<li>Created a <code>PipelineModel<\/code> with the preprocessing model and legacy model in cascade:<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>pipeline_model = PipelineModel(name=model_name,\n                               role=role,\n                               models=[\n                                    preprocess_model,\n                                    trained_model\n                               ])\n<\/code><\/pre>\n<ul>\n<li>Create a new endpoint, calling the <code>PipelineModel<\/code> and then changed the <code>Lambda<\/code> function to call this new endpoint. With this I could send the <strong>raw data<\/strong> directly for the same <code>API Gateway<\/code> and it would call only <strong>one<\/strong> endpoint, without needing to pay two endpoints 24\/7 to perform the entire process.<\/li>\n<\/ul>\n<p>I've found this to be a good and &quot;<em>economic<\/em>&quot; way to perform the preprocess outside the trained model, without having to do hard processing jobs on a <code>Lambda<\/code> function.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1602871502600,
        "Solution_link_count":1.0,
        "Solution_readability":18.0,
        "Solution_reading_time":22.23,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":200.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1505194585676,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":35.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":44.2415530556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to deploy a model to Kubernetes in Azure Machine Learning Studio, it was working for a while, but now, it fails during deployment, the error message is as follows:<\/p>\n<pre><code>Deploy: Failed on step WaitServiceCreating. Details: AzureML service API error. \nYour container application crashed. This may be caused by errors in your scoring file's init() function.\nPlease check the logs for your container instance: pipeline-created-on-07-28-2020-r.\nFrom the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\nYou can also try to run image viennaglobal.azurecr.io\/azureml\/azureml_6ae744633f749472feb283065055dc2c:latest locally.\nPlease refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\n<\/code><\/pre>\n<pre class=\"lang-js prettyprint-override\"><code>{\n    &quot;code&quot;: &quot;KubernetesDeploymentFailed&quot;,\n    &quot;statusCode&quot;: 400,\n    &quot;message&quot;: &quot;Kubernetes Deployment failed&quot;,\n    &quot;details&quot;: [\n        {\n            &quot;code&quot;: &quot;CrashLoopBackOff&quot;,\n            &quot;message&quot;: &quot;Your container application crashed. This may be caused by errors in your scoring file's init() function.\nPlease check the logs for your container instance: pipeline-created-on-07-28-2020-r. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image viennaglobal.azurecr.io\/azureml\/azureml_6ae744633f749472feb283065055dc2c:latest locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.&quot;\n        }\n    ]\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1596104635683,
        "Challenge_comment_count":2,
        "Challenge_created_time":1595913837627,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while deploying a model to Kubernetes in Azure Machine Learning Studio. The deployment fails with an error message indicating that the container application crashed, possibly due to errors in the scoring file's init() function. The user is advised to check the logs for the container instance and run the image locally for debugging.",
        "Challenge_last_edit_time":1595945366092,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63127521",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":10.5,
        "Challenge_reading_time":21.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":52.99946,
        "Challenge_title":"Deploying Model to Kubernetes",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":186.0,
        "Challenge_word_count":178,
        "Platform":"Stack Overflow",
        "Poster_created_time":1505194585676,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>It seems it was a bug, got corrected by itself today. Closing this question now<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.9,
        "Solution_reading_time":1.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":15.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1340063804276,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":298.0,
        "Answerer_view_count":38.0,
        "Challenge_adjusted_solved_time":0.3479,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>So our team created a new Azure <strong>Machine Learning<\/strong> resource, but whenever I try to add a new notebook and try to edit it using &quot;JUPYTERLAB&quot; i get <code>ERR_HTTP2_PROTOCOL_ERROR<\/code> error, but the same notebook, when edited using <code>EDIT IN JUPYTER<\/code> works perfectly.<\/p>\n<p>This is a blank and clean notebook, I also tried 2 different laptops and multiple browsers per laptop, same error. I also tried incognito and clearing cookies, but to no avail.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/IevSG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IevSG.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>update: I seem to have accidentally replicated the issue and I now know what is causing it, the situation is that Im using my work laptop and constantly switching VPN connections, and some times, connecting to the AZURE PORTAl OUTSIDE the VPN. So, when you've worked on a notebook while inside a VPN, then you disconnected, and tried loading the notebook sometime later, you will encounter this<\/p>",
        "Challenge_closed_time":1596643705983,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596642453543,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an ERR_HTTP2_PROTOCOL_ERROR when trying to edit a new notebook using JUPYTERLAB in Azure Machine Learning Studio. The error persists across different laptops and browsers, even after clearing cookies and using incognito mode. The issue was later found to be caused by switching VPN connections and accessing the Azure portal outside of the VPN.",
        "Challenge_last_edit_time":1597056501367,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63268849",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":12.2,
        "Challenge_reading_time":14.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.3479,
        "Challenge_title":"ERR_HTTP2_PROTOCOL_ERROR when opening Notebook in JUPYTERLAB Azure ML Studio",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":246.0,
        "Challenge_word_count":158,
        "Platform":"Stack Overflow",
        "Poster_created_time":1340063804276,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":298.0,
        "Poster_view_count":38.0,
        "Solution_body":"<p>This problem has stomped me for hours, but I was finally able to fix it. What I did was I opened a terminal and did a Jupyter lab rebuild &quot;jupyter lab build&quot;<\/p>\n<p><a href=\"https:\/\/imgur.com\/aRB8GWS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/imgur.com\/aRB8GWS.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/IceQO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IceQO.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1596669031916,
        "Solution_link_count":4.0,
        "Solution_readability":10.2,
        "Solution_reading_time":6.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":52.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1504001058088,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2101.0,
        "Answerer_view_count":41.0,
        "Challenge_adjusted_solved_time":243.1042716667,
        "Challenge_answer_count":2,
        "Challenge_body":"<blockquote>\n  <p><strong>System Details:<\/strong><\/p>\n  \n  <p>Operating System: Ubuntu 19.04<\/p>\n  \n  <p>Anaconda version: 2019.03<\/p>\n  \n  <p>Python version: 3.7.3<\/p>\n  \n  <p>mlflow version: 1.0.0<\/p>\n<\/blockquote>\n\n<p><strong>Steps to Reproduce:<\/strong> <a href=\"https:\/\/mlflow.org\/docs\/latest\/tutorial.html\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/tutorial.html<\/a><\/p>\n\n<p><strong>Error at line\/command:<\/strong> <code>mlflow models serve -m [path_to_model] -p 1234<\/code><\/p>\n\n<p><strong>Error:<\/strong>\nCommand 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1>&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 1<\/p>\n\n<p><strong>Terminal Log:<\/strong><\/p>\n\n<pre><code>(mlflow) root@user:\/home\/user\/mlflow\/mlflow\/examples\/sklearn_elasticnet_wine\/mlruns\/0\/e3dd02d5d84545ffab858db13ede7366\/artifacts\/model# mlflow models serve -m $(pwd) -p 1234\n2019\/06\/18 16:15:16 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\n2019\/06\/18 16:15:17 INFO mlflow.pyfunc.backend: === Running command 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1&gt;&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app'\nbash: activate: No such file or directory\nTraceback (most recent call last):\n  File \"\/root\/anaconda3\/envs\/mlflow\/bin\/mlflow\", line 10, in &lt;module&gt;\n    sys.exit(cli())\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/models\/cli.py\", line 43, in serve\n    host=host)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/backend.py\", line 76, in serve\n    command_env=command_env)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/backend.py\", line 147, in _execute_in_conda_env\n    command, rc\nException: Command 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1&gt;&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 1\n(mlflow) root@user:\/home\/user\/mlflow\/mlflow\/examples\/sklearn_elasticnet_wine\/mlruns\/0\/e3dd02d5d84545ffab858db13ede7366\/artifacts\/model# \n<\/code><\/pre>",
        "Challenge_closed_time":1561730574528,
        "Challenge_comment_count":0,
        "Challenge_created_time":1560855399150,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error while deploying the model to a local REST server using MLflow. The error occurred at the command \"mlflow models serve -m [path_to_model] -p 1234\" and returned a non-zero return code of 1. The terminal log shows that the command 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1>&2 && gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app' returned a non-zero return code as well.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56647549",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":14.2,
        "Challenge_reading_time":42.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":243.1042716667,
        "Challenge_title":"MLflow Error while deploying the Model to local REST server",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1840.0,
        "Challenge_word_count":223,
        "Platform":"Stack Overflow",
        "Poster_created_time":1504001058088,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2101.0,
        "Poster_view_count":41.0,
        "Solution_body":"<p>Following the steps mentioned in the GitHub Issue <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1507\" rel=\"nofollow noreferrer\">1507<\/a> (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1507\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/1507<\/a>) I was able to resolve this issue.<\/p>\n\n<p>In reference to this post, the \"<strong>anaconda\/bin\/<\/strong>\" directory is never added to the list of environment variables i.e. PATH variable. In order to resolve this issue, add the \"<strong>else<\/strong>\" part of conda initialize code block from ~\/.bashrc file to your PATH variable.<\/p>\n\n<pre><code># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$('\/home\/atulk\/anaconda3\/bin\/conda' 'shell.bash' 'hook' 2&gt; \/dev\/null)\"\nif [ $? -eq 0 ]; then\n    eval \"$__conda_setup\"\nelse\n    if [ -f \"\/home\/atulk\/anaconda3\/etc\/profile.d\/conda.sh\" ]; then\n        . \"\/home\/atulk\/anaconda3\/etc\/profile.d\/conda.sh\"\n    else\n        export PATH=\"\/home\/atulk\/anaconda3\/bin:$PATH\"\n    fi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\n<\/code><\/pre>\n\n<p>In this case, I added <strong>export PATH=\"\/home\/atulk\/anaconda3\/bin:$PATH\"<\/strong> to the PATH variable. However, this is just a temporary fix until the issue is fixed in the project.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":10.8,
        "Solution_reading_time":17.08,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":134.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":66.306625,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I've deployed an endpoint in sagemaker and was trying to invoke it through my python program. I had tested it using postman and it worked perfectly ok. Then I wrote the invocation code as follows<\/p>\n\n<pre><code>import boto3\nimport pandas as pd\nimport io\nimport numpy as np\n\ndef np2csv(arr):\n    csv = io.BytesIO()\n    np.savetxt(csv, arr, delimiter=',', fmt='%g')\n    return csv.getvalue().decode().rstrip()\n\n\nruntime= boto3.client('runtime.sagemaker')\npayload = np2csv(test_X)\n\nruntime.invoke_endpoint(\n    EndpointName='&lt;my-endpoint-name&gt;',\n    Body=payload,\n    ContentType='text\/csv',\n    Accept='Accept'\n)\n<\/code><\/pre>\n\n<p>Now whe I run this I get a validation error<\/p>\n\n<pre><code>ValidationError: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint &lt;my-endpoint-name&gt; of account &lt;some-unknown-account-number&gt; not found.\n<\/code><\/pre>\n\n<p>While using postman i had given my access key and secret key but I'm not sure how to pass it when using sagemaker apis. I'm not able to find it in the documentation also. <\/p>\n\n<p>So my question is, how can I use sagemaker api from my local machine to invoke my endpoint?<\/p>",
        "Challenge_closed_time":1517113913470,
        "Challenge_comment_count":0,
        "Challenge_created_time":1516867519790,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering a validation error while trying to invoke an endpoint in Sagemaker through their Python program. They had tested it using Postman and it worked fine. The user is unsure of how to pass their access key and secret key when using Sagemaker APIs and is seeking guidance on how to use Sagemaker API from their local machine to invoke their endpoint.",
        "Challenge_last_edit_time":1516875209620,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48438202",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.1,
        "Challenge_reading_time":15.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":68.4426888889,
        "Challenge_title":"Errors while using sagemaker api to invoke endpoints",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":4467.0,
        "Challenge_word_count":155,
        "Platform":"Stack Overflow",
        "Poster_created_time":1410972175307,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1124.0,
        "Poster_view_count":153.0,
        "Solution_body":"<p>When you are using any of the AWS SDK (including the one for Amazon SageMaker), you need to configure the credentials of your AWS account on the machine that you are using to run your code. If you are using your local machine, you can use the AWS CLI flow. You can find detailed instructions on the Python SDK page: <a href=\"https:\/\/aws.amazon.com\/developers\/getting-started\/python\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/developers\/getting-started\/python\/<\/a> <\/p>\n\n<p>Please note that when you are deploying the code to a different machine, you will have to make sure that you are giving the EC2, ECS, Lambda or any other target a role that will allow the call to this specific endpoint. While in your local machine it can be OK to give you admin rights or other permissive permissions, when you are deploying to a remote instance, you should restrict the permissions as much as possible. <\/p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"sagemaker:InvokeEndpoint\",\n            \"Resource\": \"arn:aws:sagemaker:*:1234567890:endpoint\/&lt;my-endpoint-name&gt;\"\n        }\n    ]\n}\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.0,
        "Solution_reading_time":14.44,
        "Solution_score_count":4.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":156.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.2849888889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>How do I get access to the Azure OpenAI service to evaluate it's capabilities?<\/p>",
        "Challenge_closed_time":1664001167767,
        "Challenge_comment_count":1,
        "Challenge_created_time":1663989341807,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information on how to access the Azure OpenAI service to evaluate its capabilities.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1021561\/azure-openai-service-capabilities",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.1,
        "Challenge_reading_time":1.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":3.2849888889,
        "Challenge_title":"Azure OpenAI service capabilities",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":17,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=71c0cf97-895c-43f1-ac54-98e1e9833ae4\">@A-4824  <\/a> Thanks for the question. It is a Limited Access service so you have to apply for it <a href=\"https:\/\/aka.ms\/oai\/access\">https:\/\/aka.ms\/oai\/access<\/a><\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":3.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":22.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":139.9193041667,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi Dears, \n\nHope this message finds you well\n\nI have a sagemaker model, buit by on demand notebook. \nI have been used batch transform jobs using lambda function, It take input inference json from s3 to create batch transform job and have finally predictions. \n\nThe question how can I make lambda to use last trained model automaticity ? \n      model_name = 'forecasting-deepar-2022-05-20-22-23-20-225',\n\nLambda code : \n\n if 'input_data_4' in file:\n\n                def batch_transform():\n                    transformer = Transformer(\n                        model_name = 'forecasting-deepar-2022-05-20-22-23-20-225',\n                        instance_count = 2,\n                        instance_type = 'ml.m5.xlarge',\n                        assemble_with = 'Line',\n                        output_path = output_data_path,\n                        base_transform_job_name ='daily-output-predictions-to-s3',\n                        #sagemaker_session = sagemaker.session.Session,\n                        accept = 'application\/jsonlines')\n                    transformer.transform(data = input_data_path, \n                                        content_type = 'application\/jsonlines', \n                                        split_type = 'Line',\n                                        wait=False, \n                                        logs=True)\n                        #Waits for the Pipeline Transform Job to finish.\n                    print('Batch Transform Job Created successfully!')\n                batch_transform()\n\nThanks\nBasem",
        "Challenge_closed_time":1655971465907,
        "Challenge_comment_count":1,
        "Challenge_created_time":1655467756412,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has a Sagemaker model built on an on-demand notebook and is using batch transform jobs with a lambda function to create predictions. The user wants to know how to make the lambda function automatically use the last trained model. The user has provided a code snippet for reference.",
        "Challenge_last_edit_time":1667925901872,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUF0u2FxOyTqK8-9hQG40i7g\/call-last-sagemaker-model-in-batch-transform-jobs",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.5,
        "Challenge_reading_time":14.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":139.9193041667,
        "Challenge_title":"Call last Sagemaker Model in Batch Transform Jobs",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":318.0,
        "Challenge_word_count":116,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi Basem,\n\nIf I understood correctly you'd like your Lambda function to automatically choose the latest SageMaker model when it runs, instead of hard-coding the model name.\n\nAlthough you *could* do this simply with [boto3.client(\"sagemaker\").list_models(...)](https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.list_models) (which can sort by creation time), I would not recommend it. The reason is that in general this lists *all* models present in SageMaker - which might include some for different use cases in future, even if you only have the one DeepAR forecasting use-case today. You'd have to manually filter after the API call.\n\nA **better approach** would probably be to register your forecasting models in [SageMaker Model Registry](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-registry.html) - which will allow you to register different versions and track extra metadata like metrics and approval status for each version if you need.\n\n- First (e.g. from your notebook) you can [create a model package group](https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_model_package_group) to track your forecasting models.\n- Then (when you create your SageMaker Model) you can **register** it as a new version in the group - via [Model.register()](https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html#sagemaker.model.Model.register).\n- At the point you want to look up which model to use, you can then [list_model_packages](https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.list_model_packages) which can filter to your specific group of models, and also by approval status if you like.\n\nSo for example you could set the model package group name as a configuration environment variable for your Lambda function, and have the function dynamically look up the latest version to use from the group when needed.\n\nOf course there are also many more custom ways to do this such as creating an SSM Parameter to track the name of your current accepted model, or creating your own model registry using a data store like DynamoDB... But SageMaker Model Registry seems like the most purpose-built tool for the job here to me.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1655971465908,
        "Solution_link_count":5.0,
        "Solution_readability":15.5,
        "Solution_reading_time":29.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":289.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1487450813192,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Lviv, Ukraine",
        "Answerer_reputation_count":1047.0,
        "Answerer_view_count":88.0,
        "Challenge_adjusted_solved_time":8.9721369445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an issue about acessing data in a BigQuery table in one project using a VertexAI in another project.<\/p>\n<p>Now, I own both project and have service accounts in both project, which implies that I also have the key (credential.json in the project containing the data) which I can use to define my client:<\/p>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\n\ncredentials = service_account.Credentials.from_service_account_file('credentials.json')\nproject_id = 'cloud-billing-XXXX'\nclient = bigquery.Client(credentials= credentials,project=project_id)\n<\/code><\/pre>\n<p>which should be enough to run:<\/p>\n<pre><code>%%bigquery\nSELECT * FROM `cloud-billing-XXXX.all_billing_detailed.gcp_billing_export` limit 100\n<\/code><\/pre>\n<p>but I get the error:<\/p>\n<pre><code>ERROR:\n 403 Access Denied: Table cloud-billing-XXXX:all_billing_detailed.gcp_billing_export: User does not have permission to query table cloud-billing-XXXX:all_billing_detailed.gcp_billing_export.\n<\/code><\/pre>\n<p>I can, from the project containing my notebook, query the table in BigQuery. This makes me think that the problem is a VertexAI permission issue. I read somewhere that the service account used when defining the notebook must match the service account in the project the data resides in. I tried to create a workbench notebook with the service account in the first project and it is created but when I try to open it it refuses to do so and get an error message.<\/p>\n<p>I've also tried to grant Editor and job user permissions across both project but that wouldn't work either.<\/p>\n<p>Any experiences and ideas on how to solve this would be greatly appreciated.<\/p>",
        "Challenge_closed_time":1659745527510,
        "Challenge_comment_count":1,
        "Challenge_created_time":1659713227817,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while accessing data in a BigQuery table in one project using a VertexAI in another project. The user owns both projects and has service accounts in both projects, but still gets a 403 Access Denied error. The user has tried granting Editor and job user permissions across both projects, but it did not work. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73252066",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.2,
        "Challenge_reading_time":22.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":8.9721369445,
        "Challenge_title":"Query BQ table with Jupyter Notebook across owned projects",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":67.0,
        "Challenge_word_count":237,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442929315876,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Stockholm, Sverige",
        "Poster_reputation_count":4598.0,
        "Poster_view_count":855.0,
        "Solution_body":"<ol>\n<li>As a mentioned in the comment: add your notebook service account to the first project (which contains your BigQuery data) and grant it with <strong>Bigquery Job User<\/strong> and <strong>BigQuery Data Viewer<\/strong> permissions.<\/li>\n<li>You can query data directly in you python code (without using &quot;magic&quot; %%bigquery). Just add the next two rows:<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>pct_overlap_terms_by_days_apart = client.query(&quot;SELECT * FROM `cloud-billing-XXXX.all_billing_detailed.gcp_billing_export` limit 100&quot;).to_dataframe()\npct_overlap_terms_by_days_apart.head()\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Check out table name. If table path is wrong, then you'll get the same error: <strong>403 Access Denied<\/strong>.<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.3,
        "Solution_reading_time":10.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":84.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1512301540763,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Milan, Italy",
        "Answerer_reputation_count":1427.0,
        "Answerer_view_count":113.0,
        "Challenge_adjusted_solved_time":24.03927,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currenlty trying to deploy a Vertex pipeline to achieve the following:<\/p>\n<ol>\n<li><p>Train a custom model (from a custom training python package) and dump model artifacts (trained model and data preprocessor that will be sed at prediction time). This is step is working fine as I can see new resources being created in the storage bucket.<\/p>\n<\/li>\n<li><p>Create a model resource via <code>ModelUploadOp<\/code>. This step fails for some reason when specifying <code>serving_container_environment_variables<\/code> and <code>serving_container_ports<\/code> with the error message in the <strong>errors<\/strong> section below. This is somewhat surprising as they are both needed by the prediction container and environment variables are passed as a dict as specified in the documentation.<br \/>\nThis step works just fine using <code>gcloud<\/code> commands:<\/p>\n<\/li>\n<\/ol>\n<pre class=\"lang-sh prettyprint-override\"><code>gcloud ai models upload \\\n    --region us-west1 \\\n    --display-name session_model_latest \\\n    --container-image-uri gcr.io\/and-reporting\/pred:latest \\\n    --container-env-vars=&quot;MODEL_BUCKET=ml_session_model&quot; \\\n    --container-health-route=\/\/health \\\n    --container-predict-route=\/\/predict \\\n    --container-ports=5000\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Create an endpoint.<\/li>\n<li>Deploy the model to the endpoint.<\/li>\n<\/ol>\n<p>There is clearly something that I am getting wrong with Vertex, the components <a href=\"https:\/\/google-cloud-pipeline-components.readthedocs.io\/en\/google-cloud-pipeline-components-0.2.2\/index.html\" rel=\"nofollow noreferrer\">documentation<\/a> doesn't help much in this case.<\/p>\n<h2>Pipeline<\/h2>\n<pre class=\"lang-py prettyprint-override\"><code>from datetime import datetime\n\nimport kfp\nfrom google.cloud import aiplatform\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom kfp.v2 import compiler\n\nPIPELINE_ROOT = &quot;gs:\/\/ml_model_bucket\/pipeline_root&quot;\n\n\n@kfp.dsl.pipeline(name=&quot;session-train-deploy&quot;, pipeline_root=PIPELINE_ROOT)\ndef pipeline():\n    training_op = gcc_aip.CustomPythonPackageTrainingJobRunOp(\n        project=&quot;my-project&quot;,\n        location=&quot;us-west1&quot;,\n        display_name=&quot;train_session_model&quot;,\n        model_display_name=&quot;session_model&quot;,\n        service_account=&quot;name@my-project.iam.gserviceaccount.com&quot;,\n        environment_variables={&quot;MODEL_BUCKET&quot;: &quot;ml_session_model&quot;},\n        python_module_name=&quot;trainer.train&quot;,\n        staging_bucket=&quot;gs:\/\/ml_model_bucket\/&quot;,\n        base_output_dir=&quot;gs:\/\/ml_model_bucket\/&quot;,\n        args=[\n            &quot;--gcs-data-path&quot;,\n            &quot;gs:\/\/ml_model_data\/2019-Oct_short.csv&quot;,\n            &quot;--gcs-model-path&quot;,\n            &quot;gs:\/\/ml_model_bucket\/model\/model.joblib&quot;,\n            &quot;--gcs-preproc-path&quot;,\n            &quot;gs:\/\/ml_model_bucket\/model\/preproc.pkl&quot;,\n        ],\n        container_uri=&quot;us-docker.pkg.dev\/vertex-ai\/training\/scikit-learn-cpu.0-23:latest&quot;,\n        python_package_gcs_uri=&quot;gs:\/\/ml_model_bucket\/trainer-0.0.1.tar.gz&quot;,\n        model_serving_container_image_uri=&quot;gcr.io\/my-project\/pred&quot;,\n        model_serving_container_predict_route=&quot;\/predict&quot;,\n        model_serving_container_health_route=&quot;\/health&quot;,\n        model_serving_container_ports=[5000],\n        model_serving_container_environment_variables={\n            &quot;MODEL_BUCKET&quot;: &quot;ml_model_bucket\/model&quot;\n        },\n    )\n\n    model_upload_op = gcc_aip.ModelUploadOp(\n        project=&quot;and-reporting&quot;,\n        location=&quot;us-west1&quot;,\n        display_name=&quot;session_model&quot;,\n        serving_container_image_uri=&quot;gcr.io\/my-project\/pred:latest&quot;,\n        # When passing the following 2 arguments this step fails...\n        serving_container_environment_variables={&quot;MODEL_BUCKET&quot;: &quot;ml_model_bucket\/model&quot;},\n        serving_container_ports=[5000],\n        serving_container_predict_route=&quot;\/predict&quot;,\n        serving_container_health_route=&quot;\/health&quot;,\n    )\n    model_upload_op.after(training_op)\n\n    endpoint_create_op = gcc_aip.EndpointCreateOp(\n        project=&quot;my-project&quot;,\n        location=&quot;us-west1&quot;,\n        display_name=&quot;pipeline_endpoint&quot;,\n    )\n\n    model_deploy_op = gcc_aip.ModelDeployOp(\n        model=model_upload_op.outputs[&quot;model&quot;],\n        endpoint=endpoint_create_op.outputs[&quot;endpoint&quot;],\n        deployed_model_display_name=&quot;session_model&quot;,\n        traffic_split={&quot;0&quot;: 100},\n        service_account=&quot;name@my-project.iam.gserviceaccount.com&quot;,\n    )\n    model_deploy_op.after(endpoint_create_op)\n\n\nif __name__ == &quot;__main__&quot;:\n    ts = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\n    compiler.Compiler().compile(pipeline, &quot;custom_train_pipeline.json&quot;)\n    pipeline_job = aiplatform.PipelineJob(\n        display_name=&quot;session_train_and_deploy&quot;,\n        template_path=&quot;custom_train_pipeline.json&quot;,\n        job_id=f&quot;session-custom-pipeline-{ts}&quot;,\n        enable_caching=True,\n    )\n    pipeline_job.submit()\n\n<\/code><\/pre>\n<h3>Errors and notes<\/h3>\n<ol>\n<li>When specifying <code>serving_container_environment_variables<\/code> and <code>serving_container_ports<\/code> the step fails with the following error:<\/li>\n<\/ol>\n<pre><code>{'code': 400, 'message': 'Invalid JSON payload received. Unknown name &quot;MODEL_BUCKET&quot; at \\'model.container_spec.env[0]\\': Cannot find field.\\nInvalid value at \\'model.container_spec.ports[0]\\' (type.googleapis.com\/google.cloud.aiplatform.v1.Port), 5000', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com\/google.rpc.BadRequest', 'fieldViolations': [{'field': 'model.container_spec.env[0]', 'description': 'Invalid JSON payload received. Unknown name &quot;MODEL_BUCKET&quot; at \\'model.container_spec.env[0]\\': Cannot find field.'}, {'field': 'model.container_spec.ports[0]', 'description': &quot;Invalid value at 'model.container_spec.ports[0]' (type.googleapis.com\/google.cloud.aiplatform.v1.Port), 5000&quot;}]}]}\n<\/code><\/pre>\n<p>When commenting out <code>serving_container_environment_variables<\/code> and <code>serving_container_ports<\/code>  the model resource gets created but deploying it manually to the endpoint results into a failed deployment with no output logs.<\/p>",
        "Challenge_closed_time":1643965835692,
        "Challenge_comment_count":2,
        "Challenge_created_time":1643879294320,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with the ModelUploadOp step in their Vertex pipeline. They are trying to create a model resource via ModelUploadOp, but the step fails when specifying serving_container_environment_variables and serving_container_ports. The error message indicates that the JSON payload received is invalid and cannot find the specified field. The user has tried deploying the model manually to the endpoint, but it results in a failed deployment with no output logs.",
        "Challenge_last_edit_time":1644239389000,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70968460",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":23.4,
        "Challenge_reading_time":83.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":42,
        "Challenge_solved_time":24.03927,
        "Challenge_title":"ModelUploadOp step failing with custom prediction container",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":273.0,
        "Challenge_word_count":381,
        "Platform":"Stack Overflow",
        "Poster_created_time":1512301540763,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Milan, Italy",
        "Poster_reputation_count":1427.0,
        "Poster_view_count":113.0,
        "Solution_body":"<p>After some time researching the problem I've stumbled upon <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/issues\/6848\" rel=\"nofollow noreferrer\">this<\/a> Github issue. The problem was originated by a mismatch between <a href=\"https:\/\/google-cloud-pipeline-components.readthedocs.io\/en\/google-cloud-pipeline-components-0.2.2\/index.html\" rel=\"nofollow noreferrer\"><code>google_cloud_pipeline_components<\/code><\/a> and <a href=\"https:\/\/kubernetes.io\/docs\/reference\/generated\/kubernetes-api\/v1.19\/#envvar-v1-core\" rel=\"nofollow noreferrer\"><code>kubernetes_api<\/code><\/a> docs. In this case, <code>serving_container_environment_variables<\/code> is typed as an <code>Optional[dict[str, str]]<\/code> whereas it should have been typed as a <code>Optional[list[dict[str, str]]]<\/code>. A similar mismatch can be found for <code>serving_container_ports<\/code> argument as well. Passing arguments following kubernetes documentation did the trick:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model_upload_op = gcc_aip.ModelUploadOp(\n    project=&quot;my-project&quot;,\n    location=&quot;us-west1&quot;,\n    display_name=&quot;session_model&quot;,\n    serving_container_image_uri=&quot;gcr.io\/my-project\/pred:latest&quot;,\n    serving_container_environment_variables=[\n        {&quot;name&quot;: &quot;MODEL_BUCKET&quot;, &quot;value&quot;: &quot;ml_session_model&quot;}\n    ],\n    serving_container_ports=[{&quot;containerPort&quot;: 5000}],\n    serving_container_predict_route=&quot;\/predict&quot;,\n    serving_container_health_route=&quot;\/health&quot;,\n)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":26.9,
        "Solution_reading_time":21.09,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":90.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1372408547912,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Farnborough, United Kingdom",
        "Answerer_reputation_count":7360.0,
        "Answerer_view_count":372.0,
        "Challenge_adjusted_solved_time":9.6629691666,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running some code in AWS Lambda that dynamically creates SageMaker models.\nI am locking Sagemaker's API version like so:<\/p>\n\n<p><code>const sagemaker = new AWS.SageMaker({apiVersion: '2017-07-24'});<\/code><\/p>\n\n<p>And here's the code to create the model:<\/p>\n\n<pre><code>await sagemaker.createModel({\n        ExecutionRoleArn: 'xxxxxx',\n        ModelName: sageMakerConfigId,\n        Containers: [{\n            Image: ecrUrl\n        }]\n    }).promise()\n<\/code><\/pre>\n\n<p>This code runs just fine locally with <code>aws-sdk<\/code> on <code>2.418.0<\/code>. <\/p>\n\n<p>However, when this code is deployed to Lambda, it doesn't work due to some validation errors upon creating the model:<\/p>\n\n<blockquote>\n  <ul>\n  <li>MissingRequiredParameter: Missing required key 'PrimaryContainer' in params<\/li>\n  <li>UnexpectedParameter: Unexpected key 'Containers' found in params<\/li>\n  <\/ul>\n<\/blockquote>\n\n<p>Is anyone aware of existing bugs in the <code>aws-sdk<\/code> for NodeJS using the SDK provided by AWS in the Lambda context? I believe the SDK available inside AWS Lambda is more up-to-date than <code>2.418.0<\/code> but apparently there are compatibility issues.<\/p>",
        "Challenge_closed_time":1553117574836,
        "Challenge_comment_count":0,
        "Challenge_created_time":1553074522987,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing compatibility issues while using SageMaker NodeJS's SDK in AWS Lambda. The code runs fine locally but fails to create a model in Lambda due to validation errors. The user suspects that the SDK provided by AWS in the Lambda context is more up-to-date than the version used locally, causing compatibility issues.",
        "Challenge_last_edit_time":1553082788147,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55257580",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":14.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":11.9588469444,
        "Challenge_title":"SageMaker NodeJS's SDK is not locking the API Version",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":277.0,
        "Challenge_word_count":142,
        "Platform":"Stack Overflow",
        "Poster_created_time":1548169897263,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":6496.0,
        "Poster_view_count":759.0,
        "Solution_body":"<p>As you've noticed the 'embedded' lambda version of the aws-sdk lags behind. It's actually on <code>2.290.0<\/code> (you can see the full details on the environment here: <a href=\"https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/current-supported-versions.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/current-supported-versions.html<\/a>)<\/p>\n\n<p>You can see here: <a href=\"https:\/\/github.com\/aws\/aws-sdk-js\/blame\/master\/clients\/sagemaker.d.ts\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/aws-sdk-js\/blame\/master\/clients\/sagemaker.d.ts<\/a> that it is not until <code>2.366.0<\/code> that the params for this method included <code>Containers<\/code> and did not require <code>PrimaryContainer<\/code>.<\/p>\n\n<p>As you've noted, the <em>workaround<\/em> is to deploy your lambda with the <code>aws-sdk<\/code> version that you're using. This is sometimes noted as a best practice, as it pins the <code>aws-sdk<\/code> on the functionality you've built and tested against.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":14.1,
        "Solution_reading_time":13.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":95.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1626973229036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":199.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":171.8247852778,
        "Challenge_answer_count":1,
        "Challenge_body":"<pre><code>import os\nimport io\nimport boto3\nimport json\nimport csv\n\n\n# grab environment variables\nENDPOINT_NAME = os.environ['ENDPOINT_NAME']\n# grab runtime client\nruntime = boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    # Load data from POST request\n    data = json.loads(json.dumps(event))\n    \n    # Grab the payload\n    payload = data['body']\n    \n    # Invoke the model. In this case the data type is a JSON but can be other things such as a CSV\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                   ContentType='application\/json',\n                                   Body=payload)\n    \n    # Get the body of the response from the model\n    result = response['Body'].read().decode()\n\n    # Return it along with the status code of 200 meaning this was succesful \n    return {\n        'statusCode': 200,\n        'body': result\n    }\n<\/code><\/pre>\n<p><strong>response from AWS Lambda<\/strong><\/p>\n<pre><code>{\n  &quot;errorMessage&quot;: &quot;'body'&quot;,\n  &quot;errorType&quot;: &quot;KeyError&quot;,\n  &quot;stackTrace&quot;: [\n    [\n      &quot;\/var\/task\/lambda_function.py&quot;,\n      18,\n      &quot;lambda_handler&quot;,\n      &quot;payload = data['body']&quot;\n    ]\n  ]\n}\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/h8wvA.png\" rel=\"nofollow noreferrer\">response from Postman 500 Internal Server Error<\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/cuknX.png\" rel=\"nofollow noreferrer\">but successfully invoke POST 200 in SageMaker Endpoint<\/a><\/p>",
        "Challenge_closed_time":1626975923230,
        "Challenge_comment_count":1,
        "Challenge_created_time":1626360722473,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with AWS Lambda where they are receiving an Internal Server Error 500 response when using Postman, but the \/invocations POST is successfully returning a 200 response in the SageMaker Endpoint. The error message indicates a KeyError related to the 'body' parameter in the Lambda function.",
        "Challenge_last_edit_time":1626360827356,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68396088",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":14.2,
        "Challenge_reading_time":19.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":170.8890991667,
        "Challenge_title":"Why AWS Lambda Internel Server Error 500 but successfully \/invocations POST 200 in Endpoint SageMaker?",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":277.0,
        "Challenge_word_count":141,
        "Platform":"Stack Overflow",
        "Poster_created_time":1512933739527,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Petaling Jaya, Selangor, Malaysia",
        "Poster_reputation_count":3.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>The issue is when you are trying to parse your payload with data['body']. The data is not being passed in the format that the endpoint is expecting. Use the following code snippet to properly format\/serialize your data for the endpoint. Also to make all this clearer make sure to check for your payload type to make sure you have not serialized again by accident.<\/p>\n<pre><code>    data = json.loads(json.dumps(event))\n    payload = json.dumps(data)\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                       ContentType='application\/json',\n                                       Body=payload)\n    result = json.loads(response['Body'].read().decode())\n<\/code><\/pre>\n<p>I work for AWS &amp; my opinions are my own<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1626979396583,
        "Solution_link_count":0.0,
        "Solution_readability":9.4,
        "Solution_reading_time":8.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":86.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1430203014072,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":432.0,
        "Answerer_view_count":54.0,
        "Challenge_adjusted_solved_time":1392.8864805556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'd like to connect to an AzureML Web Service. I have looked into the POST Method on the Arduino Homepage and also here <a href=\"https:\/\/iotguys.wordpress.com\/2014\/12\/25\/communicating-with-microsoft-azure-eventhub-using-arduino\/\" rel=\"nofollow\">https:\/\/iotguys.wordpress.com\/2014\/12\/25\/communicating-with-microsoft-azure-eventhub-using-arduino\/<\/a><\/p>\n\n<p>Here is my Setup method:<\/p>\n\n<pre><code>    void setup()\n    {\n      Serial.begin(9600);\n      while (!Serial) {\n      ; \/\/ wait for serial port to connect.\n      }\n\n     Serial.println(\"ethernet\");\n\n     if (Ethernet.begin(mac) == 0) {\n       Serial.println(\"ethernet failed\");\n       for (;;) ;\n     }\n    \/\/ give the Ethernet shield a second to initialize:\n    delay(1000);\n }\n<\/code><\/pre>\n\n<p>The Post Method is based on this: <a href=\"http:\/\/playground.arduino.cc\/Code\/WebClient\" rel=\"nofollow\">http:\/\/playground.arduino.cc\/Code\/WebClient<\/a><\/p>\n\n<p>I just added <code>sprintf(outBuf, \"Authorization: Bearer %s\\r\\n\", api_key);<\/code> to the header, with <code>char* api_key = \"the ML Web Service API KEY\"<\/code><\/p>\n\n<p>Also, unlike specified in the WebClient I use the whole WebService URI as url and do not specify a page name.<\/p>\n\n<p>This doesn't work.<\/p>\n\n<p>The Network to which I am connecting has Internet Access.<\/p>\n\n<p>What am I doing wrong?<\/p>",
        "Challenge_closed_time":1450337261483,
        "Challenge_comment_count":0,
        "Challenge_created_time":1445322870153,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to connect to an AzureML Web Service using an Arduino Uno and has followed the POST Method on the Arduino Homepage and other resources. The user has added the API key to the header and is using the whole WebService URI as the URL. However, the connection is not working, and the user is seeking help to identify the issue.",
        "Challenge_last_edit_time":1459290344956,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/33229576",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":16.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":1392.8864805556,
        "Challenge_title":"Arduino Uno - WebService (AzureML)",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":154.0,
        "Challenge_word_count":132,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369151239452,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":516.0,
        "Poster_view_count":57.0,
        "Solution_body":"<p>Machine Learning Studio services that you create needs to receive requests from a device that has SSL capabilities to perform HTTPS requests. AFAIK, Arduino doesn't support SSL capabilities.<\/p>\n\n<p>One usual scenario is to attach the Arduino to a third device like Raspberry Pi 2 etc to use it as a gateway and do the call from the Pi itself.<\/p>\n\n<p>Here's a sample <a href=\"https:\/\/github.com\/Azure\/connectthedots\/blob\/master\/GettingStarted.md\" rel=\"nofollow\">project<\/a> from Microsoft Open Technologies team that utilizes Arduino Uno, Raspberry pi and Azure stuff.<\/p>\n\n<p>Hope this helps!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1450346645647,
        "Solution_link_count":1.0,
        "Solution_readability":9.8,
        "Solution_reading_time":7.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":82.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.2522222222,
        "Challenge_answer_count":0,
        "Challenge_body":"<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\nIf you run \r\n\r\npy_test(\r\n name = \"sigopt_prior_beliefs_example\",\r\n size = \"medium\",\r\n srcs = [\"examples\/sigopt_prior_beliefs_example.py\"],\r\n deps = [\":tune_lib\"],\r\n tags = [\"exclusive\", \"example\"],\r\n args = [\"--smoke-test\"]\r\n)\r\n\r\nin python\/ray\/tune\/build (this part of the testing is commented out since you need a sigopt API key...)\r\nYou get an output that looks like this:\r\n\r\n\"\"\"\r\n...\r\n  File \"\/usr\/local\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 737, in _process_trial\r\n    self._validate_result_metrics(result)\r\n  File \"\/usr\/local\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 818, in _validate_result_metrics\r\n    elif search_metric and search_metric not in result:\r\nTypeError: unhashable type: 'list'\r\n...\r\n\"\"\"\r\nray 1.1.0.dev\r\n\r\n### Reproduction (REQUIRED)\r\nin python\/ray\/tune\/build  run the sigopt sections that are commented out.\r\n",
        "Challenge_closed_time":1603498330000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1603479422000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered an issue with Vertex AI endpoint call with JSON, where they are unable to generate the JSON payload for the input of the neural network. They have tried different methods, but all have resulted in an error 400, indicating an invalid JSON payload. The user is unsure if the array needs to be in b64 format and is seeking help to fix\/encode the payload.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ray-project\/ray\/issues\/11581",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.8,
        "Challenge_reading_time":12.71,
        "Challenge_repo_contributor_count":425.0,
        "Challenge_repo_fork_count":4048.0,
        "Challenge_repo_issue_count":30819.0,
        "Challenge_repo_star_count":23050.0,
        "Challenge_repo_watch_count":435.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":5.2522222222,
        "Challenge_title":"[Tune] Sigopt (multi-metric) api fails with 1.1.0 (tries to hash list)",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":102,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This is a consequence of search metric being able to be multi-metric. cc @krfricke \r\n\r\nAlso, let me ping the sigopt folks for a working API key... Should be fixed on #11583 . We'll pick this onto the release.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.4,
        "Solution_reading_time":2.45,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":37.0,
        "Tool":"SigOpt"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":5.2634725,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to determine how the Az ML model deployment works with AKS. If you have a single AKS cluster but attach from two separate workspaces, will models from both workspaces get deployed into different azureml-fe's with different IP addresses OR a single azureml-fe with a single IP address? Reason I ask is because I want to purchase a certificate but am unsure if all the models (regardless of workspace) will get exposed under the same IP Address OR multiple IP Addresses? If its the former, I can do it with one certificate...otherwise I have to do it with multiple certificates or SAN based certificates. So if anyone has experience with this, please let me know!<\/p>",
        "Challenge_closed_time":1655345708048,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655326759547,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to determine how Azure Machine Learning model deployment works with AKS. They are unsure if models from multiple workspaces will be deployed into different azureml-fe's with different IP addresses or a single azureml-fe with a single IP address. They want to purchase a certificate but are unsure if all the models will be exposed under the same IP address or multiple IP addresses. They are seeking advice from anyone with experience in this matter.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72637756",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":9.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":5.2634725,
        "Challenge_title":"Azure Machine Learning Model deployment as AKS web service from multiple workspaces",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":127.0,
        "Challenge_word_count":128,
        "Platform":"Stack Overflow",
        "Poster_created_time":1376577570772,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":301.0,
        "Poster_view_count":52.0,
        "Solution_body":"<p>By checking AKS webservice class, we can do the multiple services links to single AKS cluster. The endpoint management was described in <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.webservice.aks.akswebservice?view=azure-ml-py\" rel=\"nofollow noreferrer\">document<\/a>, but this is representing one-to-one cluster and service. For multiple workspaces refer <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.webservice.aksendpoint?view=azure-ml-py#azureml-core-webservice-aksendpoint-create-version\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>\n<p>Regarding <strong><code>azureml-fe<\/code><\/strong>. There will be one <strong>azureml-fe<\/strong> for one cluster. That means, when we are using different workspaces for deployment into one AKS, then only <strong>one<\/strong> <strong>azureml-fe<\/strong> and can be considered to take <strong>one certificate.<\/strong><\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/v1\/how-to-deploy-azure-kubernetes-service?tabs=python#azure-ml-router\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/v1\/how-to-deploy-azure-kubernetes-service?tabs=python#azure-ml-router<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":19.4,
        "Solution_reading_time":16.89,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":81.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":18.75,
        "Challenge_answer_count":2,
        "Challenge_body":"I have a model hosted on a Google Cloud endpoint and I would like to access it via the Java client.\u00a0 I've created a service account and a key for that service account with the , when I run my client code with the GOOGLE_APPLICATION_CREDENTIALS env var pointed to the key, I am able to call the service.\u00a0 When I try to authenticate explicitly using FixedCredentialProvider, it fails with an \"unauthenticated\" message.\u00a0\u00a0\n\nThe code is as follows\n\n```\n\nPredictionServiceSettings predictionServiceSettings =\n        PredictionServiceSettings.newBuilder().setEndpoint(location + \"-aiplatform.googleapis.com:443\")\n                .setCredentialsProvider(FixedCredentialsProvider.create(ServiceAccountCredentials.fromStream(new FileInputStream(\"\/Users\/ME\/Downloads\/XYZ.json\"))))\n                .build();\npredictionServiceClient = PredictionServiceClient.create(predictionServiceSettings);\nendpointName = EndpointName.of(project, location, endpointId);\nValue featureVal = Value.newBuilder().setStructValue(features).build();\nPredictResponse response =  predictionServiceClient.predict(\n        endpointName,\n        Collections.singletonList(featureVal),\n        Value.newBuilder().setNullValue(NullValue.NULL_VALUE).build());\n\n\n\n```",
        "Challenge_closed_time":1669109220000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1669041720000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to access a model hosted on a Google Cloud endpoint via the Java client. They have created a service account and a key for that service account, and when they run their client code with the GOOGLE_APPLICATION_CREDENTIALS env var pointed to the key, they are able to call the service. However, when they try to authenticate explicitly using FixedCredentialProvider, it fails with an \"unauthenticated\" message.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-can-I-explicitly-authenticate-to-the-ai-platform-using-the\/m-p\/491537#M833",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":20.5,
        "Challenge_reading_time":16.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":18.75,
        "Challenge_title":"How can I explicitly authenticate to the ai-platform using the java PredictionServiceClient",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":250.0,
        "Challenge_word_count":109,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi,\n\nUpon checking your code, FixedCredentialsProvider.create()\u00a0accepts\u00a0com.google.auth.Credentials\u00a0as a parameter. Can you try a Credentials object to\u00a0FixedCredentialsProvider.create()? See code below:\n\nGoogleCredentials credentials = GoogleCredentials.fromStream(new FileInputStream(\"\/Users\/ME\/Downloads\/XYZ.json\")).createScoped(Lists.newArrayList(\"https:\/\/www.googleapis.com\/auth\/cloud-platform\"));\n\n\u00a0If code above did not work, can you provide the stack trace of the error? Also what roles did you assign on your service account?\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.1,
        "Solution_reading_time":7.4,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":56.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1468951834403,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, United States",
        "Answerer_reputation_count":645.0,
        "Answerer_view_count":101.0,
        "Challenge_adjusted_solved_time":33.9211666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am deploying a machine learning image to Azure Container Instances from Azure Machine Learning services according to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml\" rel=\"nofollow noreferrer\">this article<\/a>, but am always stuck with the error message:<\/p>\n\n<blockquote>\n  <p>Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.<br>\n  Please check the logs for your container instance xxxxxxx'.<\/p>\n<\/blockquote>\n\n<p>I tried:<\/p>\n\n<ol>\n<li>increasing memory_gb=4 in aci_config.<\/li>\n<li>I did\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-troubleshoot-deployment#debug-the-docker-image-locally\" rel=\"nofollow noreferrer\">troubleshooting<\/a> locally, but I could not have found any.<\/li>\n<\/ol>\n\n<p>Below is my score.py<\/p>\n\n<pre><code>def init():\n    global model\n    model_path = Model.get_model_path('pofc_fc_model')\n    model = joblib.load(model_path)\n\ndef run(raw_data):\n    data = np.array(json.loads(raw_data)['data'])\n    y_hat = model.predict(data)\n    return y_hat.tolist()\n<\/code><\/pre>",
        "Challenge_closed_time":1553715289510,
        "Challenge_comment_count":0,
        "Challenge_created_time":1553593173310,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing issues while deploying a machine learning image to Azure Container Instances from Azure Machine Learning services. The deployment is failing with an error message indicating that the container application crashed, which may be caused by errors in the scoring file's init() function. The user has tried increasing memory_gb and troubleshooting locally but could not find any issues. The score.py file is also provided.",
        "Challenge_last_edit_time":1562618473092,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55353889",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":15.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":33.9211666667,
        "Challenge_title":"Azure container instances deployment failed",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":3020.0,
        "Challenge_word_count":110,
        "Platform":"Stack Overflow",
        "Poster_created_time":1510960409296,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangkok Thailand",
        "Poster_reputation_count":306.0,
        "Poster_view_count":32.0,
        "Solution_body":"<p>Have you registered the model <code>'pofc_fc_model'<\/code> in your workspace using the <code>register()<\/code> function on the model object? If not, there will be no model path and that can cause failure.<\/p>\n\n<p>See this section on model registration: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where#registermodel\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where#registermodel<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.8,
        "Solution_reading_time":6.8,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":40.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1515120748952,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":76.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":17.4135575,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to invoke the iris endpointfrom the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators\/tensorflow_iris_dnn_classifier_using_estimators.ipynb\" rel=\"nofollow noreferrer\">SageMaker example notebooks<\/a> using the aws cli. I've tried using the following command:<\/p>\n\n<pre><code>!aws sagemaker-runtime invoke-endpoint \\\n--endpoint-name sagemaker-tensorflow-py2-cpu-2018-03-19-21-27-52-956 \\\n--body \"[6.4, 3.2, 4.5, 1.5]\" \\\n--content-type \"application\/json\" \\\noutput.json\n<\/code><\/pre>\n\n<p>I get the following response:<\/p>\n\n<pre><code>{\n    \"InvokedProductionVariant\": \"AllTraffic\", \n    \"ContentType\": \"*\/*\"\n}\n<\/code><\/pre>\n\n<p>What am I doing wrong?<\/p>",
        "Challenge_closed_time":1521567781630,
        "Challenge_comment_count":0,
        "Challenge_created_time":1521505092823,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to call a SageMaker endpoint using the AWS CLI, but is receiving an unexpected response and is seeking guidance on what they may be doing wrong.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49374476",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":19.8,
        "Challenge_reading_time":10.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":17.4135575,
        "Challenge_title":"How do I call a SageMaker Endpoint using the AWS CLI (",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":4027.0,
        "Challenge_word_count":64,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443201378360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":749.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>If you've gotten that response, your request is successful. The output should be in the output file you specified - output.json :)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.4,
        "Solution_reading_time":1.7,
        "Solution_score_count":5.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":21.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1577817693600,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":329.4975816667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am aware that Sagemaker <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-model-endpoints.html#multi-model-support\" rel=\"nofollow noreferrer\">does not support multi-model endpoints for their built-in image classification algorithm<\/a>. However, in the documentation they hint at building a custom container to use &quot;any other framework or algorithm&quot; with the multi-model endpoint functionality:<\/p>\n<blockquote>\n<p>To use any other framework or algorithm, use the SageMaker inference toolkit to build a container that supports multi-model endpoints. For information, see <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html\" rel=\"nofollow noreferrer\">Build Your Own Container with Multi Model Server<\/a>.<\/p>\n<\/blockquote>\n<p>Ideally, I would like to deploy many (20+) image classification models I have already trained to a single endpoint to save on costs. However, after reading the &quot;Build Your Own Container&quot; guide it is still not exactly clear to me how to build a custom inference container for the models produced by a non-custom algorithm. Most of the tutorials and example notebooks refer to using Pytorch or Sklearn. It is not clear to me that I could make inferences using these libraries on the models I've created with the built-in image classification algorithm.<\/p>\n<p><em>Is<\/em> it possible to create a container to support multi-model endpoints for unsupported built-in Sagemaker algorithms? If so, would somebody be able to hint at how this might be done?<\/p>",
        "Challenge_closed_time":1612378891027,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611192699733,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to deploy multiple image classification models to a single endpoint using Sagemaker's multi-model endpoint functionality. However, Sagemaker does not support multi-model endpoints for their built-in image classification algorithm. The user is trying to build a custom inference container for the models produced by a non-custom algorithm, but is unsure how to do so as most tutorials and examples refer to Pytorch or Sklearn. The user is seeking guidance on whether it is possible to create a container to support multi-model endpoints for unsupported built-in Sagemaker algorithms and how to do so.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65819978",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.8,
        "Challenge_reading_time":20.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":329.4975816667,
        "Challenge_title":"Sagemaker multi-model endpoints with unsupported built-in algorithms",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":524.0,
        "Challenge_word_count":207,
        "Platform":"Stack Overflow",
        "Poster_created_time":1611191329712,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>yes, it is possible to deploy the built in image classification models as a SageMaker multi model endpoint. The key is that the image classification uses <a href=\"https:\/\/mxnet.apache.org\/versions\/1.7.0\/\" rel=\"nofollow noreferrer\">Apache MXNet<\/a>. You can extract the model artifacts (SageMaker stores them in a zip file named model.tar.gz in S3), then load them in to MXNet. The SageMaker MXNet container supports multi model endpoints, so you can use that to deploy the model.<\/p>\n<p>If you unzip the model.tar.gz from this algorithm, you'll find three files:<\/p>\n<p>image-classification-****.params<\/p>\n<p>image-classification-symbol.json<\/p>\n<p>model-shapes.json<\/p>\n<p>The MxNet container expects these files to be named <strong>image-classification-0000.params, model-symbol.json, and model-shapes.json<\/strong>. So I unzipped the zip file, renamed the files and rezipped them. For more information on the MXNet container check out the <a href=\"https:\/\/github.com\/aws\/sagemaker-mxnet-inference-toolkit\" rel=\"nofollow noreferrer\">GitHub repository<\/a>.<\/p>\n<p>After that you can deploy the model as a single MXNet endpoint using the SageMaker SDK with the following code:<\/p>\n<pre><code>from sagemaker import get_execution_role\nfrom sagemaker.mxnet.model import MXNetModel\n\nrole = get_execution_role()\n\nmxnet_model = MXNetModel(model_data=s3_model, role=role, \n                         entry_point='built_in_image_classifier.py', \n                         framework_version='1.4.1',\n                         py_version='py3')\n\npredictor = mxnet_model.deploy(instance_type='ml.c4.xlarge', initial_instance_count=1)\n<\/code><\/pre>\n<p>The entry point Python script can be an empty Python file for now. We will be using the default inference handling provided by the MXNet container.<\/p>\n<p>The default MXNet container only accepts JSON, CSV, and Numpy arrays as valid input. So you will have to format your input in to one of these three formats. The code below demonstrates how I did it with Numpy arrays:<\/p>\n<pre><code>import cv2\nimport io\n\nnp_array = cv2.imread(filename=img_filename)\nnp_array = np_array.transpose((2,0,1))\nnp_array = np.expand_dims(np_array, axis=0)\n\nbuffer = io.BytesIO()\nnp.save(buffer, np_array)\n\nresponse = sm.invoke_endpoint(EndpointName='Your_Endpoint_name', Body=buffer.getvalue(), ContentType='application\/x-npy')\n<\/code><\/pre>\n<p>Once you have a single endpoint working with MXNet container, you should be able to get it running in multi model endpoint using the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/multi_data_model.html\" rel=\"nofollow noreferrer\">SageMaker MultiDataModel constructor<\/a>.<\/p>\n<p>If you want to use a different input data type so you don't have to do the preprocessing in your application code, you can overwrite the input_fn method in the MxNet container by providing it in the entry_point script. <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/mxnet\/using_mxnet.html\" rel=\"nofollow noreferrer\">See here<\/a> for more information. If you do this, you could pass the image bytes directly to SageMaker, without formatting the numpy arrays.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":12.5,
        "Solution_reading_time":39.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":30.0,
        "Solution_word_count":347.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":6.4760219444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi,  <br \/>\nI have stored some ml model in my ADLS and I want to register the model to Azure ML using databricks.  <br \/>\nTried to use the following codes to register my ml model but keep encountering an error that the path cannot be found.<\/p>\n<p>import urllib.request  <br \/>\nfrom azureml.core.model import Model<\/p>\n<h1 id=\"register-a-model\">Register a model<\/h1>\n<p>model = Model.register(model_path = 'dbfs:\/mnt\/machinelearning\/classifier.joblib',  <br \/>\nmodel_name = &quot;pretrained-classifier&quot;,  <br \/>\ndescription = &quot;Pretrained Classifier&quot;,  <br \/>\nworkspace=ws)<\/p>",
        "Challenge_closed_time":1642438310976,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642414997297,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to import an ML model from ADLS to Azure ML using Databricks. They have attempted to register the model using the provided code but are encountering an error that the path cannot be found.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/697789\/import-ml-model-from-adls-to-azure-ml-using-databr",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.2,
        "Challenge_reading_time":8.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":6.4760219444,
        "Challenge_title":"Import ML Model from ADLS to Azure ML using Databricks",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":78,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=ad14abdc-d75c-4489-859e-28e2aba507a8\">@Yuzu  <\/a> Using the databricks file path for registering a model is not supported. When using the <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#azureml-core-model-model-register\">model.register()<\/a> you need to download the model locally and then use the path of the model or the folder in which the model is present to register the same.     <\/p>\n<blockquote>\n<p>model_path    <\/p>\n<p>The path on the local file system where the model assets are located. This can be a direct pointer to a single file or folder. If pointing to a folder, the child_paths parameter can be used to specify individual files to bundle together as the Model object, as opposed to using the entire contents of the folder.    <\/p>\n<\/blockquote>\n<p>This sample <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-to-cloud\/model-register-and-deploy.ipynb\">notebook<\/a> should help you with using the method.    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":14.5,
        "Solution_reading_time":18.8,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":149.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":19.7252202778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have two ML models A and B registered in my workspace and I want to deploy them to Azure Container Instance and perform A\/B testing or continuous rollout or Canary deployment. Can it be done without using AKS and only on ACI.<\/p>",
        "Challenge_closed_time":1648171136616,
        "Challenge_comment_count":3,
        "Challenge_created_time":1648100125823,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to perform A\/B testing or continuous rollout or Canary deployment of two ML models A and B registered in their workspace using Azure Container Instance without using AKS.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/785262\/a-b-testing-using-azure-container-instance",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":8.4,
        "Challenge_reading_time":3.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":19.7252202778,
        "Challenge_title":"A\/B testing using Azure Container Instance.",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":48,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=8d6b3f04-f6f3-4fa5-95d3-dbd939677f31\">@HARISH KUMAR  <\/a>     <\/p>\n<p>Thanks for reaching out to us. I can understand you want to deploy your ML models to ACI to do the A\/B test. For question can models to be deployed only ACI, the answer is yes, but please be aware that ACI has some limitation compared to AKS, please check if ACI is a good choice for your model:    <\/p>\n<p>About models:    <br \/>\nACI is suitable only for small models that are under 1 GB in size.    <br \/>\nWe recommend using single-node AKS to dev-test larger models.    <br \/>\nThe number of models to be deployed is limited to 1,000 models per deployment (per container).    <\/p>\n<p>About Vnet:    <br \/>\nWhen using Azure Container Instances in a virtual network, the virtual network must be in the same resource group as your Azure Machine Learning workspace.    <br \/>\nWhen using Azure Container Instances inside the virtual network, the Azure Container Registry (ACR) for your workspace cannot also be in the virtual network.    <\/p>\n<p>Other points:    <br \/>\nprefer not to manage your own Kubernetes cluster    <br \/>\nAre OK with having only a single replica of your service, which may impact uptime    <br \/>\nYou are advised to debug locally before deploying to the web service,    <\/p>\n<p>AKS Advantages for your reference as well:    <br \/>\nFast response time    <br \/>\nAutoscaling of the deployed service    <br \/>\nLogging    <br \/>\nModel data collection    <br \/>\nAuthentication    <br \/>\nTLS termination    <br \/>\nHardware acceleration options such as GPU and field-programmable gate arrays (FPGA)    <\/p>\n<p>If you feel like ACI fulfill your need, then you can follow below guidance to do the deployment:    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-container-instance#deploy-to-aci\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-container-instance#deploy-to-aci<\/a>    <\/p>\n<p>Hope this helps, please let me know if you have more questions.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p><em>-Please kindly accept the answer if you feel helpful, thanks.<\/em>    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.8,
        "Solution_reading_time":26.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":297.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1251699930780,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Santa Clara, CA, United States",
        "Answerer_reputation_count":1538.0,
        "Answerer_view_count":198.0,
        "Challenge_adjusted_solved_time":12.0071708333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I uploaded a pretrained scikit learn classification model to Vertex AI and ran a batch prediction on 5 samples. It just returned a list of false predictions with no confidence score. I don't see anywhere in the SDK documentation or Google console for how to get batch predictions to include the confidence scores. Is that something Vertex AI can do?<\/p>\n<p>My intent is to automate a batch prediction pipeline using the following code.<\/p>\n<pre><code># Predict\n# &quot;csv&quot;, &quot;&quot;bigquery&quot;, &quot;tf-record&quot;, &quot;tf-record-gzip&quot;, or &quot;file-list&quot;\nbatch_prediction_job = model.batch_predict(\n    job_display_name = job_display_name,\n    gcs_source = input_path,\n    instances_format = &quot;&quot;, # jsonl, csv, bigquery, \n    gcs_destination_prefix = output_path,\n    starting_replica_count = 1,\n    max_replica_count = 10,\n    sync = True,\n)\n\nbatch_prediction_job.wait()\n\nreturn batch_prediction_job.resource_name\n<\/code><\/pre>\n<p>I tried it out in google console as a test to make sure my input data was properly formatted.<\/p>",
        "Challenge_closed_time":1637647501512,
        "Challenge_comment_count":3,
        "Challenge_created_time":1637604275697,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user uploaded a pre-trained scikit learn classification model to Vertex AI and ran a batch prediction on 5 samples, but the predictions returned were false with no confidence score. The user is looking for a way to get batch predictions to include confidence scores, but cannot find any information in the SDK documentation or Google console. The user's intent is to automate a batch prediction pipeline.",
        "Challenge_last_edit_time":1637686316647,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70070421",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":10.8,
        "Challenge_reading_time":14.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":12.0071708333,
        "Challenge_title":"Return confidence score with custom model for Vertex AI batch predictions",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":319.0,
        "Challenge_word_count":133,
        "Platform":"Stack Overflow",
        "Poster_created_time":1417013182680,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Boston, MA",
        "Poster_reputation_count":1256.0,
        "Poster_view_count":245.0,
        "Solution_body":"<p>I don't think so; the stock sklearn container provided by vertex doesn't provide such a score I guess. You might need to write a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/use-custom-container\" rel=\"nofollow noreferrer\">custom container<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.8,
        "Solution_reading_time":3.5,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":29.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1261400320736,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Antwerp, Belgium",
        "Answerer_reputation_count":7876.0,
        "Answerer_view_count":924.0,
        "Challenge_adjusted_solved_time":0.6120011111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've installed <a href=\"https:\/\/mlflow.org\/\" rel=\"nofollow noreferrer\">MLflow<\/a> on Ubuntu Server 18.04 LTS, in a virtual environment (Python 3), using its <a href=\"https:\/\/mlflow.org\/docs\/latest\/quickstart.html\" rel=\"nofollow noreferrer\">Quickstart documentation<\/a>:<\/p>\n\n<pre><code>$ python3 -m venv mlflow\n$ source \/home\/emre\/mlflow\/bin\/activate\n$ pip install mlflow\n<\/code><\/pre>\n\n<p>that gave the following output during install:<\/p>\n\n<pre><code>Collecting mlflow\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e8\/b3\/cf358e182be34a62fcd6843e5df793f278bd9d24f78f565509cb927c6a22\/mlflow-0.1.0.tar.gz (4.3MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.3MB 323kB\/s\nCollecting Flask (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/7f\/e7\/08578774ed4536d3242b14dacb4696386634607af824ea997202cd0edb4b\/Flask-1.0.2-py2.py3-none-any.whl (91kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 9.4MB\/s\nCollecting awscli (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/ba\/32\/d6d254f6ccc2ed21f02d81f38709ff06feca9cbdb2e68ea90635fa483a73\/awscli-1.15.46-py2.py3-none-any.whl (1.3MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3MB 1.0MB\/s\nCollecting boto3 (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/24\/e0\/a98898b94d8093bbd8fd4576fb2e89620adac1e24a2bfc28d11c4ce29a5b\/boto3-1.7.46-py2.py3-none-any.whl (128kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 8.8MB\/s\nCollecting click&gt;=6.7 (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/34\/c1\/8806f99713ddb993c5366c362b2f908f18269f8d792aff1abfd700775a77\/click-6.7-py2.py3-none-any.whl (71kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 9.3MB\/s\nCollecting databricks-cli (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/58\/78\/4bda6f29a091ab7b0ad29efdba2491e5d0b56bd09d608857e6f0b799be48\/databricks-cli-0.7.2.tar.gz\nCollecting gitpython (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/ac\/c9\/96d7c86c623cb065976e58c0f4898170507724d6b4be872891d763d686f4\/GitPython-2.1.10-py2.py3-none-any.whl (449kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 450kB 2.9MB\/s\nCollecting numpy (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/68\/1e\/116ad560de97694e2d0c1843a7a0075cc9f49e922454d32f49a80eb6f1f2\/numpy-1.14.5-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.2MB 110kB\/s\nCollecting pandas (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/57\/eb\/6ab533ea8e35e7dd159af6922ac1123d4565d89f3926ad9a6aa46530978f\/pandas-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (11.8MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11.8MB 116kB\/s\nCollecting protobuf (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/fc\/f0\/db040681187496d10ac50ad167a8fd5f953d115b16a7085e19193a6abfd2\/protobuf-3.6.0-cp36-cp36m-manylinux1_x86_64.whl (7.1MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.1MB 177kB\/s\nCollecting pygal (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/5f\/b7\/201c9254ac0d2b8ffa3bb2d528d23a4130876d9ba90bc28e99633f323f17\/pygal-2.4.0-py2.py3-none-any.whl (127kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 9.7MB\/s\nCollecting python-dateutil (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/cf\/f5\/af2b09c957ace60dcfac112b669c45c8c97e32f94aa8b56da4c6d1682825\/python_dateutil-2.7.3-py2.py3-none-any.whl (211kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 215kB 6.0MB\/s\nCollecting pyyaml (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/10\/7d\/6efe0bd69580fecd40adf47ebaf8d807238308ccb851f0549881fa7605aa\/PyYAML-4.1.tar.gz (153kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153kB 7.8MB\/s\nCollecting querystring_parser (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/57\/64\/3086a9a991ff3aca7b769f5b0b51ff8445a06337ae2c58f215bcee48f527\/querystring_parser-1.2.3.tar.gz\nCollecting requests&gt;=2.17.3 (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/65\/47\/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda\/requests-2.19.1-py2.py3-none-any.whl (91kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 8.2MB\/s\nCollecting scikit-learn (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/3d\/2d\/9fbc7baa5f44bc9e88ffb7ed32721b879bfa416573e85031e16f52569bc9\/scikit_learn-0.19.1-cp36-cp36m-manylinux1_x86_64.whl (12.4MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.4MB 108kB\/s\nCollecting scipy (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/a8\/0b\/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730\/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 31.2MB 42kB\/s\nCollecting six&gt;=1.10.0 (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/67\/4b\/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a\/six-1.11.0-py2.py3-none-any.whl\nCollecting uuid (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/ce\/63\/f42f5aa951ebf2c8dac81f77a8edcc1c218640a2a35a03b9ff2d4aa64c3d\/uuid-1.30.tar.gz\nCollecting zipstream (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/1a\/a4\/58f0709cef999db1539960aa2ae77100dc800ebb8abb7afc97a1398dfb2f\/zipstream-1.1.4.tar.gz\nCollecting itsdangerous&gt;=0.24 (from Flask-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/dc\/b4\/a60bcdba945c00f6d608d8975131ab3f25b22f2bcfe1dab221165194b2d4\/itsdangerous-0.24.tar.gz (46kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 10.4MB\/s\nCollecting Werkzeug&gt;=0.14 (from Flask-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/20\/c4\/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243\/Werkzeug-0.14.1-py2.py3-none-any.whl (322kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 327kB 4.0MB\/s\nCollecting Jinja2&gt;=2.10 (from Flask-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/7f\/ff\/ae64bacdfc95f27a016a7bed8e8686763ba4d277a78ca76f32659220a731\/Jinja2-2.10-py2.py3-none-any.whl (126kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 8.2MB\/s\nCollecting rsa&lt;=3.5.0,&gt;=3.1.2 (from awscli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e1\/ae\/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e\/rsa-3.4.2-py2.py3-none-any.whl (46kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 10.5MB\/s\nCollecting botocore==1.10.46 (from awscli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b4\/04\/ddaad5574f70a539d106e8d53b4685e3de4387de7a16884a95459f8c7691\/botocore-1.10.46-py2.py3-none-any.whl (4.4MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.4MB 314kB\/s\nCollecting s3transfer&lt;0.2.0,&gt;=0.1.12 (from awscli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/d7\/14\/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d\/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 10.6MB\/s\nCollecting colorama&lt;=0.3.9,&gt;=0.2.5 (from awscli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/db\/c8\/7dcf9dbcb22429512708fe3a547f8b6101c0d02137acbd892505aee57adf\/colorama-0.3.9-py2.py3-none-any.whl\nCollecting docutils&gt;=0.10 (from awscli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/36\/fa\/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d\/docutils-0.14-py3-none-any.whl (543kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 552kB 2.5MB\/s\nCollecting jmespath&lt;1.0.0,&gt;=0.7.1 (from boto3-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b7\/31\/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365\/jmespath-0.9.3-py2.py3-none-any.whl\nCollecting configparser&gt;=0.3.5 (from databricks-cli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/7c\/69\/c2ce7e91c89dc073eb1aa74c0621c3eefbffe8216b3f9af9d3885265c01c\/configparser-3.5.0.tar.gz\nCollecting tabulate&gt;=0.7.7 (from databricks-cli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/12\/c2\/11d6845db5edf1295bc08b2f488cf5937806586afe42936c3f34c097ebdc\/tabulate-0.8.2.tar.gz (45kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 7.9MB\/s\nCollecting gitdb2&gt;=2.0.0 (from gitpython-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e0\/95\/c772c13b7c5740ec1a0924250e6defbf5dfdaee76a50d1c47f9c51f1cabb\/gitdb2-2.0.3-py2.py3-none-any.whl (63kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 11.2MB\/s\nCollecting pytz&gt;=2011k (from pandas-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/dc\/83\/15f7833b70d3e067ca91467ca245bae0f6fe56ddc7451aa0dc5606b120f2\/pytz-2018.4-py2.py3-none-any.whl (510kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 512kB 421kB\/s\nRequirement already satisfied: setuptools in .\/mlflow\/lib\/python3.6\/site-packages (from protobuf-&gt;mlflow)\nCollecting chardet&lt;3.1.0,&gt;=3.0.2 (from requests&gt;=2.17.3-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/bc\/a9\/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8\/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 143kB 8.7MB\/s\nCollecting idna&lt;2.8,&gt;=2.5 (from requests&gt;=2.17.3-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/4b\/2a\/0276479a4b3caeb8a8c1af2f8e4355746a97fab05a372e4a2c6a6b876165\/idna-2.7-py2.py3-none-any.whl (58kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 10.3MB\/s\nCollecting urllib3&lt;1.24,&gt;=1.21.1 (from requests&gt;=2.17.3-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/bd\/c9\/6fdd990019071a4a32a5e7cb78a1d92c53851ef4f56f62a3486e6a7d8ffb\/urllib3-1.23-py2.py3-none-any.whl (133kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 143kB 8.3MB\/s\nCollecting certifi&gt;=2017.4.17 (from requests&gt;=2.17.3-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/7c\/e6\/92ad559b7192d846975fc916b65f667c7b8c3a32bea7372340bfe9a15fa5\/certifi-2018.4.16-py2.py3-none-any.whl (150kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153kB 8.0MB\/s\nCollecting MarkupSafe&gt;=0.23 (from Jinja2&gt;=2.10-&gt;Flask-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/4d\/de\/32d741db316d8fdb7680822dd37001ef7a448255de9699ab4bfcbdf4172b\/MarkupSafe-1.0.tar.gz\nCollecting pyasn1&gt;=0.1.3 (from rsa&lt;=3.5.0,&gt;=3.1.2-&gt;awscli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/a0\/70\/2c27740f08e477499ce19eefe05dbcae6f19fdc49e9e82ce4768be0643b9\/pyasn1-0.4.3-py2.py3-none-any.whl (72kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 10.9MB\/s\nCollecting smmap2&gt;=2.0.0 (from gitdb2&gt;=2.0.0-&gt;gitpython-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e3\/59\/4e22f692e65f5f9271252a8e63f04ce4ad561d4e06192478ee48dfac9611\/smmap2-2.0.3-py2.py3-none-any.whl\nBuilding wheels for collected packages: mlflow, databricks-cli, pyyaml, querystring-parser, uuid, zipstream, itsdangerous, configparser, tabulate, MarkupSafe\n  Running setup.py bdist_wheel for mlflow ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/mlflow\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmp10fdrz2ypip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for mlflow\n  Running setup.py clean for mlflow\n  Running setup.py bdist_wheel for databricks-cli ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/databricks-cli\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmpy_2acqi3pip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for databricks-cli\n  Running setup.py clean for databricks-cli\n  Running setup.py bdist_wheel for pyyaml ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/pyyaml\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmp4bs2fwrtpip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for pyyaml\n  Running setup.py clean for pyyaml\n  Running setup.py bdist_wheel for querystring-parser ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/querystring-parser\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmp_cnm9w_tpip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for querystring-parser\n  Running setup.py clean for querystring-parser\n  Running setup.py bdist_wheel for uuid ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/uuid\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmpenr2igaxpip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for uuid\n  Running setup.py clean for uuid\n  Running setup.py bdist_wheel for zipstream ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/zipstream\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmpnzsjh5e2pip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for zipstream\n  Running setup.py clean for zipstream\n  Running setup.py bdist_wheel for itsdangerous ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/itsdangerous\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmp7imi3zv2pip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for itsdangerous\n  Running setup.py clean for itsdangerous\n  Running setup.py bdist_wheel for configparser ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/configparser\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmpyk9qtmi1pip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for configparser\n  Running setup.py clean for configparser\n  Running setup.py bdist_wheel for tabulate ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/tabulate\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmpjim2qr00pip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for tabulate\n  Running setup.py clean for tabulate\n  Running setup.py bdist_wheel for MarkupSafe ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/MarkupSafe\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmpsdpdd8ulpip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for MarkupSafe\n  Running setup.py clean for MarkupSafe\nFailed to build mlflow databricks-cli pyyaml querystring-parser uuid zipstream itsdangerous configparser tabulate MarkupSafe\nInstalling collected packages: click, itsdangerous, Werkzeug, MarkupSafe, Jinja2, Flask, pyasn1, rsa, jmespath, six, python-dateutil, docutils, botocore, s3transfer, colorama, pyyaml, awscli, boto3, configparser, chardet, idna, urllib3, certifi, requests, tabulate, databricks-cli, smmap2, gitdb2, gitpython, numpy, pytz, pandas, protobuf, pygal, querystring-parser, scikit-learn, scipy, uuid, zipstream, mlflow\n  Running setup.py install for itsdangerous ... done\n  Running setup.py install for MarkupSafe ... done\n  Running setup.py install for pyyaml ... done\n  Running setup.py install for configparser ... done\n  Running setup.py install for tabulate ... done\n  Running setup.py install for databricks-cli ... done\n  Running setup.py install for querystring-parser ... done\n  Running setup.py install for uuid ... done\n  Running setup.py install for zipstream ... done\n  Running setup.py install for mlflow ... done\nSuccessfully installed Flask-1.0.2 Jinja2-2.10 MarkupSafe-1.0 Werkzeug-0.14.1 awscli-1.15.46 boto3-1.7.46 botocore-1.10.46 certifi-2018.4.16 chardet-3.0.4 click-6.7 colorama-0.3.9 configparser-3.5.0 databricks-cli-0.7.2 docutils-0.14 gitdb2-2.0.3 gitpython-2.1.10 idna-2.7 itsdangerous-0.24 jmespath-0.9.3 mlflow-0.1.0 numpy-1.14.5 pandas-0.23.1 protobuf-3.6.0 pyasn1-0.4.3 pygal-2.4.0 python-dateutil-2.7.3 pytz-2018.4 pyyaml-4.1 querystring-parser-1.2.3 requests-2.19.1 rsa-3.4.2 s3transfer-0.1.13 scikit-learn-0.19.1 scipy-1.1.0 six-1.11.0 smmap2-2.0.3 tabulate-0.8.2 urllib3-1.23 uuid-1.30 zipstream-1.1.4\n<\/code><\/pre>\n\n<p>After that I checked the following didn't give any errors:<\/p>\n\n<pre><code>import os\nfrom mlflow import log_metric, log_param, log_artifact\n<\/code><\/pre>\n\n<p>But when I try to run the web-based user interface, I get the following errors:<\/p>\n\n<pre><code>$ mlflow ui\nTraceback (most recent call last):\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 574, in _build_master\n    ws.require(__requires__)\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 892, in require\n    needed = self.resolve(parse_requirements(requirements))\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 783, in resolve\n    raise VersionConflict(dist, req).with_context(dependent_req)\npkg_resources.ContextualVersionConflict: (PyYAML 4.1 (\/home\/emre\/mlflow\/lib\/python3.6\/site-packages), Requirement.parse('PyYAML&lt;=3.12,&gt;=3.10'), {'awscli'})\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/home\/emre\/mlflow\/bin\/mlflow\", line 6, in &lt;module&gt;\n    from pkg_resources import load_entry_point\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 3088, in &lt;module&gt;\n    @_call_aside\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 3072, in _call_aside\n    f(*args, **kwargs)\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 3101, in _initialize_master_working_set\n    working_set = WorkingSet._build_master()\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 576, in _build_master\n    return cls._build_from_requirements(__requires__)\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 589, in _build_from_requirements\n    dists = ws.resolve(reqs, Environment())\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 783, in resolve\n    raise VersionConflict(dist, req).with_context(dependent_req)\npkg_resources.ContextualVersionConflict: (PyYAML 4.1 (\/home\/emre\/mlflow\/lib\/python3.6\/site-packages), Requirement.parse('PyYAML&lt;=3.12,&gt;=3.10'), {'awscli'})\n<\/code><\/pre>\n\n<p>Any ideas how I can fix this?<\/p>",
        "Challenge_closed_time":1530109158067,
        "Challenge_comment_count":0,
        "Challenge_created_time":1530106954863,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"the user encountered challenges while attempting to run a web-based user interface on an ubuntu server 18.04 lts in a virtual environment, resulting in an error related to pyyaml.",
        "Challenge_last_edit_time":1530112131487,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51064366",
        "Challenge_link_count":42,
        "Challenge_participation_count":1,
        "Challenge_readability":15.2,
        "Challenge_reading_time":293.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":213,
        "Challenge_solved_time":0.6120011111,
        "Challenge_title":"Can't run MLflow web-based user interface",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":791.0,
        "Challenge_word_count":1336,
        "Platform":"Stack Overflow",
        "Poster_created_time":1261400320736,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Antwerp, Belgium",
        "Poster_reputation_count":7876.0,
        "Poster_view_count":924.0,
        "Solution_body":"<p>Apparently I had to install the <code>wheel<\/code> module inside my virtual environment. I deleted the virtual environment, re-created it, and then installed the <code>wheel<\/code> module:<\/p>\n\n<pre><code>pip install wheel\n<\/code><\/pre>\n\n<p>after that <code>pip install mlflow<\/code>, as well as <code>mlflow ui<\/code> worked successfully.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.8,
        "Solution_reading_time":4.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":41.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":24.0835044444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed a model as a Webservice in Azure ML.Its a simple one and all it does is do a linear Regression .The underlying code is python . Now i need to pass which all columns have to selected as independent variables, dynamically, from the client side . How may i do this in Azure ML studio?<\/p>",
        "Challenge_closed_time":1458632118243,
        "Challenge_comment_count":0,
        "Challenge_created_time":1458567955050,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has deployed a simple linear regression model as a webservice in Azure ML, but needs to dynamically select which columns to use as independent variables from the client side. They are seeking guidance on how to accomplish this in Azure ML studio.",
        "Challenge_last_edit_time":1458612577147,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36132719",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.1,
        "Challenge_reading_time":4.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":17.8231091667,
        "Challenge_title":"Select columns dynamically in Azure ML model",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":238.0,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1422821109972,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":1238.0,
        "Poster_view_count":172.0,
        "Solution_body":"<p>Based on my understanding, I think you want to dynamically get the selected columns data via request the Azure ML webservice with some parameters on the client.<\/p>\n\n<p>You can refer to the offical document <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-web-service-parameters\/\" rel=\"nofollow\">Use Azure Machine Learning Web Service Parameters<\/a> and the blog <a href=\"https:\/\/blogs.technet.microsoft.com\/machinelearning\/2014\/11\/25\/azureml-web-service-parameters\/\" rel=\"nofollow\">AzureML Web Service Parameters<\/a> to know how to set and use the web service parameters to implement your needs via add the selected column names as array into the json parameter <code>GlobalParameters<\/code>.<\/p>\n\n<p>Meanwhile, there is a client sample on GitHub <a href=\"https:\/\/github.com\/nk773\/AzureML_RRSApp\" rel=\"nofollow\">https:\/\/github.com\/nk773\/AzureML_RRSApp<\/a>. Althought it was writen in Java, I think it is easy to understand, then you can rewrite in Python with <code>requests<\/code> package.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1458699277763,
        "Solution_link_count":4.0,
        "Solution_readability":16.5,
        "Solution_reading_time":13.49,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":113.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1227171471292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":17500.0,
        "Answerer_view_count":1561.0,
        "Challenge_adjusted_solved_time":233.4898177778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Invoking a multimodel Sagemaker Endpoint, I get an error that it is not multimodel. I create it like this.<\/p>\n<pre><code>create_endpoint_config_response = client.create_endpoint_config(\n    EndpointConfigName=endpoint_config_name,\n    ProductionVariants=[\n        {\n            &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n            &quot;InitialVariantWeight&quot;: 0.5,\n            &quot;InitialInstanceCount&quot;: 1,\n            &quot;ModelName&quot;: model_name1,\n            &quot;VariantName&quot;: model_name1,\n        },\n         {\n            &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n            &quot;InitialVariantWeight&quot;: 0.5,\n            &quot;InitialInstanceCount&quot;: 1,\n            &quot;ModelName&quot;: model_name2,\n            &quot;VariantName&quot;: model_name2,\n        }\n    ]\n)\n<\/code><\/pre>\n<p>I confirm in the GUI that it in fact has multiple models. I invoke it like this:<\/p>\n<pre><code>response = client.invoke_endpoint(\n    EndpointName=endpoint_name, \n    TargetModel=model_name1,\n    ContentType=&quot;text\/x-libsvm&quot;, \n    Body=payload\n)\n<\/code><\/pre>\n<p>and get this error:<\/p>\n<blockquote>\n<p>ValidationError: An error occurred (ValidationError) when calling the\nInvokeEndpoint operation: Endpoint\nmy-endpoint1 is not a multi-model endpoint\nand does not support target model header.<\/p>\n<\/blockquote>\n<p>The same problem was discussed <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1026\" rel=\"nofollow noreferrer\">here<\/a> with no resolution.<\/p>\n<p>How can I invoke a multimodel endpoint?<\/p>",
        "Challenge_closed_time":1629283623048,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629114933853,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while invoking a multimodel Sagemaker Endpoint, despite having created it with multiple models. The error message states that the endpoint is not multimodel and does not support target model header. The user has confirmed that the endpoint has multiple models in the GUI and is seeking a solution to invoke the multimodel endpoint.",
        "Challenge_last_edit_time":1629119102992,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68802388",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":16.4,
        "Challenge_reading_time":19.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":46.8581097222,
        "Challenge_title":"Why do I get an error that Sagemaker Endpoint does not have multiple models when it does?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":247.0,
        "Challenge_word_count":133,
        "Platform":"Stack Overflow",
        "Poster_created_time":1227171471292,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Israel",
        "Poster_reputation_count":17500.0,
        "Poster_view_count":1561.0,
        "Solution_body":"<p>The answer (see <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1026\" rel=\"nofollow noreferrer\">GitHub<\/a> discussion) is that this error message is simply false.<\/p>\n<p>To avoid this error, the model's local filename (usually for the form <code>model_filename.tar.gz<\/code>) must be used, not the model name.<\/p>\n<p>The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/invoke-multi-model-endpoint.html\" rel=\"nofollow noreferrer\">documentation<\/a> does say this, though it lacks essential detail.<\/p>\n<p>I found <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb\" rel=\"nofollow noreferrer\">this to be the best example<\/a>.  See the last part  of that Notebook, in which <code>invoke_endpoint<\/code> is used (rather than a predictor as used earlier in the Notebook).<\/p>\n<p>As to the location of that model file: This <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">Notebook<\/a> says:<\/p>\n<blockquote>\n<p>When creating the Model entity for multi-model endpoints, the container's ModelDataUrl is the S3 prefix where the model\nartifacts that are invokable by the endpoint are located. The rest of the S3 path will be specified when invoking the model.<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1629959666336,
        "Solution_link_count":4.0,
        "Solution_readability":17.2,
        "Solution_reading_time":19.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":138.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1342685175156,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":12103.0,
        "Answerer_view_count":1451.0,
        "Challenge_adjusted_solved_time":12.8997730556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am able to deploy a Azure Machine learning prediction service in my workspace <code>ws<\/code> using the syntax<\/p>\n\n<pre><code>aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n                                               memory_gb=8, \n                                               tags={\"method\" : \"some method\"}, \n                                               description='Predict something')\n<\/code><\/pre>\n\n<p>and then<\/p>\n\n<pre><code>service = Webservice.deploy_from_image(deployment_config = aciconfig,\n                                       image = image,\n                                       name = service_name,\n                                       workspace = ws)\n<\/code><\/pre>\n\n<p>as described in the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where#aci\" rel=\"nofollow noreferrer\">documentation<\/a>.<br>\nHowever, this exposes a service publicly and this is not really optimal.<\/p>\n\n<p>What's the easiest way to shield the ACI service? I understand that passing an <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.webservice.aciwebservice?view=azure-ml-py#deploy-configuration-cpu-cores-none--memory-gb-none--tags-none--properties-none--description-none--location-none--auth-enabled-none--ssl-enabled-none--enable-app-insights-none--ssl-cert-pem-file-none--ssl-key-pem-file-none--ssl-cname-none-\" rel=\"nofollow noreferrer\"><code>auth_enabled=True<\/code><\/a> parameter may do the job, but then how can I instruct a client (say, using <code>curl<\/code> or Postman) to use the service afterwards? <\/p>",
        "Challenge_closed_time":1556182170523,
        "Challenge_comment_count":0,
        "Challenge_created_time":1556135731340,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has successfully deployed an Azure Machine Learning prediction service in their workspace using the given syntax. However, the service is publicly exposed, and the user is looking for the easiest way to shield the ACI service. They are considering passing an \"auth_enabled=True\" parameter but are unsure how to instruct a client to use the service afterward.",
        "Challenge_last_edit_time":1556186547292,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55837639",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":18.8,
        "Challenge_reading_time":19.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":12.8997730556,
        "Challenge_title":"How to enable authentication for an ACI webservice in Azure Machine Learning service?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":676.0,
        "Challenge_word_count":113,
        "Platform":"Stack Overflow",
        "Poster_created_time":1415722650716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Verona, VR, Italy",
        "Poster_reputation_count":4811.0,
        "Poster_view_count":713.0,
        "Solution_body":"<p>See <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-consume-web-service#call-the-service-c\" rel=\"nofollow noreferrer\">here<\/a> for an example (in C#). When you enable auth, you will need to send the API key in the \"Authorization\" header in the HTTP request:<\/p>\n\n<pre><code>client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Bearer\", authKey);\n<\/code><\/pre>\n\n<p>See <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-consume-web-service#authentication-key\" rel=\"nofollow noreferrer\">here<\/a> how to retrieve the key.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1556183204092,
        "Solution_link_count":2.0,
        "Solution_readability":19.8,
        "Solution_reading_time":8.26,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1221810788500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paderborn, North-Rhine-Westphalia, Germany",
        "Answerer_reputation_count":68522.0,
        "Answerer_view_count":7896.0,
        "Challenge_adjusted_solved_time":380.8509777778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>It is not clear to me if one could use mlflow to serve a model that is evolving continuously based on its previous predictions.<\/p>\n<p>I need to be able to query a model in order to make a prediction on a sample of data which is the basic use of mlflow serve. However I also want the model to be updated internaly now that it has seen new data.<\/p>\n<p>Is it possible or does it need a FR ?<\/p>",
        "Challenge_closed_time":1618124640630,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616753577110,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unsure if mlflow can be used to continuously update a model based on its previous predictions while also being able to query the model for predictions on new data. They are seeking clarification on whether this is possible or if it requires a feature request.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66814885",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":5.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":380.8509777778,
        "Challenge_title":"Serve online learning models with mlflow",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":360.0,
        "Challenge_word_count":84,
        "Platform":"Stack Overflow",
        "Poster_created_time":1515156959092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Fairbanks, AK, United States",
        "Poster_reputation_count":76.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>I think that you should be able to do that by implementing the custom python model or custom flavor, as it's described in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">documentation<\/a>.  In this case you need to create a class that is inherited from <code>mlflow.pyfunc.PythonModel<\/code>, and implement the <code>predict<\/code> method, and inside that method you're free to do anything.  Here is just simple example from documentation:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class AddN(mlflow.pyfunc.PythonModel):\n\n    def __init__(self, n):\n        self.n = n\n\n    def predict(self, context, model_input):\n        return model_input.apply(lambda column: column + self.n)\n<\/code><\/pre>\n<p>and this model is then could be saved &amp; loaded again just as normal models:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># Construct and save the model\nmodel_path = &quot;add_n_model&quot;\nadd5_model = AddN(n=5)\nmlflow.pyfunc.save_model(path=model_path, python_model=add5_model)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(model_path)\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.9,
        "Solution_reading_time":14.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":120.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.4533425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained a linear regression model in AzureML studio which was created in designer as pipeline.    <\/p>\n<p>I could not able to see R square and adj-R square metric in Evaluate Model step.     <\/p>\n<p>Could any throw thoughts how can I add these 2 metrics to my trained model     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/89088-image.png?platform=QnA\" alt=\"89088-image.png\" \/>    <\/p>\n<p>Thanks    <br \/>\nBhaskar<\/p>",
        "Challenge_closed_time":1618877959740,
        "Challenge_comment_count":1,
        "Challenge_created_time":1618843927707,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a challenge in adding R square and adj-R square metric to their trained linear regression model in AzureML Studio. They are seeking suggestions on how to add these two metrics to their model.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/362850\/how-to-add-r2-and-adj-r2-metric-in-linear-regressi",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.8,
        "Challenge_reading_time":6.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":9.4533425,
        "Challenge_title":"How to add r2 and adj r2 metric in linear regression model - AzureML Studio?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":71,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello,    <\/p>\n<p>Sorry  for the confusing. Actually, Coefficient of determination, often referred to as R2, represents the predictive power of the model as a value between 0 and 1. Zero means the model is random (explains nothing); 1 means there is a perfect fit. However, caution should be used in interpreting R2 values, as low values can be entirely normal and high values can be suspect in Azure Machine Learning Designer.    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/evaluate-model#metrics-for-regression-models\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/evaluate-model#metrics-for-regression-models<\/a>    <\/p>\n<p>Regards,    <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.6,
        "Solution_reading_time":9.74,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":78.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1443426419048,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation_count":842.0,
        "Answerer_view_count":219.0,
        "Challenge_adjusted_solved_time":125.3573088889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am new to Azure ML Studio. I tried creating an experiment that takes a numeric value as input and a gives a data table type output. I works fine when I run it in the portal , but not when I run it as a web service. It shows a single value numeric output , when it has to be a data table type.<\/p>\n\n<p>Is there a way to change the output type of web service output? <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/oq5Xb.png\" rel=\"nofollow noreferrer\">Visualizing output in portal<\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/wUmN7.png\" rel=\"nofollow noreferrer\">Test RRS output(web service)<\/a><\/p>",
        "Challenge_closed_time":1486009486972,
        "Challenge_comment_count":6,
        "Challenge_created_time":1485555724117,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with the output type of a web service in Azure ML Studio. The experiment works fine in the portal but shows a single value numeric output instead of a data table type when run as a web service. The user is seeking a solution to change the output type of the web service output.",
        "Challenge_last_edit_time":1485558200660,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41903982",
        "Challenge_link_count":2,
        "Challenge_participation_count":7,
        "Challenge_readability":7.5,
        "Challenge_reading_time":7.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":126.0452375,
        "Challenge_title":"Web service output - Azure ML Studio",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":587.0,
        "Challenge_word_count":96,
        "Platform":"Stack Overflow",
        "Poster_created_time":1449123268407,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"East Newark, NJ, United States",
        "Poster_reputation_count":33.0,
        "Poster_view_count":43.0,
        "Solution_body":"<p>Make is a classic web service and see the JSON output getting from it. If it's providing all data you need.. go for it.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.9,
        "Solution_reading_time":1.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":24.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":136.3803758333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created an experiment in Machine Learning Studio and deployed it as a web service. I've got a request-response API in my workspace that works. Can it be also used by other people?<\/p>",
        "Challenge_closed_time":1597984478140,
        "Challenge_comment_count":1,
        "Challenge_created_time":1597493508787,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created an experiment in Machine Learning Studio and deployed it as a web service. They are wondering if the web service can be used by other people.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63425902",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.0,
        "Challenge_reading_time":3.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":136.3803758333,
        "Challenge_title":"Are Machine Learning Studios's web services public?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":39.0,
        "Challenge_word_count":40,
        "Platform":"Stack Overflow",
        "Poster_created_time":1575044869560,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":21.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>When you deploy a model, a Webservice object is returned with information about the service.<\/p>\n<pre><code>from azureml.core.webservice import AciWebservice, Webservice\nfrom azureml.core.model import Model\n\ndeployment_config = AciWebservice.deploy_configuration(cpu_cores = 3, memory_gb = 15, location = &quot;centralus&quot;)\nservice = Model.deploy(ws, &quot;aciservice&quot;, [model], inference_config, deployment_config)\nservice.wait_for_deployment(show_output = True)\nprint(service.state)\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/AvVgY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AvVgY.png\" alt=\"enter image description here\" \/><\/a>\nPlease follow the below to Consume an Azure Machine Learning model deployed as a web service\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-consume-web-service\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-consume-web-service<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":20.0,
        "Solution_reading_time":13.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":71.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1369787017728,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Atlanta, Georgia",
        "Answerer_reputation_count":55.0,
        "Answerer_view_count":49.0,
        "Challenge_adjusted_solved_time":115.0280841667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>This is what I use to deploy an Auto-ML model:<\/p>\n<pre><code>            MachineSpec machineSpec = MachineSpec.newBuilder().setMachineType(&quot;n1-standard-2&quot;).build();\n            DedicatedResources dedicatedResources =\n                    DedicatedResources.newBuilder().setMinReplicaCount(1).setMachineSpec(machineSpec).build();            \n            String model = ModelName.of(project, location, modelId).toString();\n            DeployedModel deployedModel =\n                    DeployedModel.newBuilder()\n                            .setModel(model)\n                            .setDisplayName(deployedModelDisplayName)\n                            .setDedicatedResources(dedicatedResources)\n                            .build();\n            Map&lt;String, Integer&gt; trafficSplit = new HashMap&lt;&gt;();\n            trafficSplit.put(&quot;0&quot;, 100);\n            EndpointName endpoint = EndpointName.of(project, location, endpointId);\n            OperationFuture&lt;DeployModelResponse, DeployModelOperationMetadata&gt; response =\n                    client.deployModelAsync(endpoint, deployedModel, trafficSplit);\n            response.getInitialFuture().get().getName());\n<\/code><\/pre>\n<p>The error appears when I hit this line <code>response.getInitialFuture().get().getName());<\/code><\/p>\n<p>Here is the error:\n<code>INVALID_ARGUMENT: 'dedicated_resources' is not supported for Model projects\/***\/locations\/us-central1\/models\/***<\/code><\/p>\n<p>I can deploy the model using cloud console but not programmatically using java 8. It is a new model and the endpoint is also new without any assigned model to it.<\/p>",
        "Challenge_closed_time":1640820603630,
        "Challenge_comment_count":8,
        "Challenge_created_time":1640406502527,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue when deploying an Auto-ML model using Java. The error message \"INVALID_ARGUMENT: 'dedicated_resources' is not supported for Model projects\/***\/locations\/us-central1\/models\/***\" appears when the user hits the line \"response.getInitialFuture().get().getName());\". The user is able to deploy the model using cloud console but not programmatically using Java 8.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70477987",
        "Challenge_link_count":0,
        "Challenge_participation_count":9,
        "Challenge_readability":21.9,
        "Challenge_reading_time":18.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":115.0280841667,
        "Challenge_title":"Vertex Ai issue when deploying a model using Java",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":244.0,
        "Challenge_word_count":109,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369787017728,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Atlanta, Georgia",
        "Poster_reputation_count":55.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>I am sorry everyone, I was implementing the wrong section of the documentation. I had to follow AutoML image, not Custom-trained one.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/a4xPR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/a4xPR.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.9,
        "Solution_reading_time":4.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":32.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1434295027120,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Lusaka, Zambia",
        "Answerer_reputation_count":951.0,
        "Answerer_view_count":157.0,
        "Challenge_adjusted_solved_time":164.0830111111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I need to use the <a href=\"https:\/\/aws.amazon.com\/marketplace\/pp\/prodview-7y6xdiukxucr2\" rel=\"nofollow noreferrer\">WireframeToCode<\/a> model from the AWS Marketplace, I used Nodejs to read and send the file data to the model like this:<\/p>\n\n<pre><code>var sageMakerRuntime = new AWS.SageMakerRuntime();\n\nvar bitmap = fs.readFileSync(\"sample.jpeg\", \"utf8\");\nvar buffer = new Buffer.from(bitmap, \"base64\");\n\nvar params = {\n  Body: buffer.toJSON(),\n  EndpointName: \"wireframe-to-code\",\n  Accept: \"image\/jpeg\",\n  ContentType: \"application\/json\"\n};\n\nsageMakerRuntime.invokeEndpoint(params, function(err, data) {\n  if (err) console.log(err, err.stack);\n  else console.log(data);\n});\n<\/code><\/pre>\n\n<p>but i get this error:<\/p>\n\n<blockquote>\n  <p>message: 'Expected params.Body to be a string, Buffer, Stream, Blob,\n  or typed array object',   code: 'InvalidParameterType',   time:\n  2020-03-30T11:06:27.535Z<\/p>\n<\/blockquote>\n\n<p>From the documentation, the supported content type for input is  <code>image\/jpeg<\/code> output is <code>application\/json<\/code>.<\/p>\n\n<p>when I try to convert the Body to a string like this: <code>JSON.stringify(buffer.toJSON())<\/code> I get this error:<\/p>\n\n<blockquote>\n  <p>Received client error (415) from model with message \"This predictor\n  only supports JSON formatted data\"<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1586158824067,
        "Challenge_comment_count":0,
        "Challenge_created_time":1585568125227,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use the WireframeToCode model from AWS Marketplace by passing an image to the SageMaker endpoint using Nodejs. However, they are encountering an error message stating that the expected parameter Body should be a string, Buffer, Stream, Blob, or typed array object. The supported content type for input is image\/jpeg and output is application\/json. When the user tries to convert the Body to a string, they receive a client error message stating that the predictor only supports JSON formatted data.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60929678",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":11.9,
        "Challenge_reading_time":17.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":164.0830111111,
        "Challenge_title":"How to pass image to AWS SageMaker endpoint",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":2170.0,
        "Challenge_word_count":142,
        "Platform":"Stack Overflow",
        "Poster_created_time":1434295027120,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lusaka, Zambia",
        "Poster_reputation_count":951.0,
        "Poster_view_count":157.0,
        "Solution_body":"<p>I had to pass in bitmap and change <code>ContentType<\/code> to <code>\"image\/jpeg\"<\/code><\/p>\n\n<pre><code>const AWS = require(\"aws-sdk\");\nconst fs = require(\"fs\");\n\nconst sageMakerRuntime = new AWS.SageMakerRuntime({\n  region: \"us-east-1\",\n  accessKeyId: \"XXXXXXXXXXXX\",\n  secretAccessKey: \"XXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n});\n\nconst bitmap = fs.readFileSync(\"sample.jpeg\");\n\nvar params = {\n  Body: bitmap,\n  EndpointName: \"wireframe-to-code\",\n  ContentType: \"image\/jpeg\"\n};\n\nsageMakerRuntime.invokeEndpoint(params, function(err, data) {\n  if (err) {\n    console.log(err, err.stack);\n  } else {\n    responseData = JSON.parse(Buffer.from(data.Body).toString());\n    console.log(responseData);\n  }\n});\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.8,
        "Solution_reading_time":9.0,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":50.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":65.0577777778,
        "Challenge_answer_count":1,
        "Challenge_body":"Is it possible to deploy a model created by SageMaker in one account to an IoT Greengrass device in a different account?",
        "Challenge_closed_time":1556529654000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1556295446000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of deploying a SageMaker model from one account to an IoT Greengrass device in a different account.",
        "Challenge_last_edit_time":1667925988660,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUJha7KbxOTXuhMRGbMYGC0g\/deploy-sagemaker-model-to-iot-greengrass-in-different-account",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":2.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":65.0577777778,
        "Challenge_title":"Deploy SageMaker model to IoT Greengrass in different account?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":82.0,
        "Challenge_word_count":30,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"For IoT Greengrass 1.x, this is possible but not trivial. From the console this is not possible, as you can only select buckets or SageMaker jobs from the same account, but you can refer to resources in other accounts if you use the CLI or the API. \n\nYou have to create a new Resource Definition Version with the correct data specifying the model resource and then add it to your group definition. For permissions in the source account, you must set up the S3 bucket policy to allow access from the destination account. For permissions in the destination account, you must update the IoT Greengrass service role policy to access the model resource in the source account.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1611605697668,
        "Solution_link_count":0.0,
        "Solution_readability":9.7,
        "Solution_reading_time":8.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":117.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":123.0932055556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a code R (manipulateXY.R) taking parameters X (from picklist), Y (a \"not constrained\" value) from a text file (parameter.txt) and producing n images.\nI want to put this code as a \"R script\" in Azure ML, and to produce a web service pointing to that logic (manipulateXY). The question is: how can I pass parameters to the Azure code? I need it because I want a web app with the following outfit<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/dq0Kr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/dq0Kr.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>such that I choose the X and Y and press \"Run\", it calls the logic in Azure ML, it takes the generated images and put them on the web-app. <\/p>",
        "Challenge_closed_time":1466963216443,
        "Challenge_comment_count":0,
        "Challenge_created_time":1466520080903,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to put an R script in Azure ML and produce a web service pointing to that logic. However, the user is facing challenges in passing parameters to the Azure code to create a web app that allows the user to choose X and Y parameters and generate images by calling the logic in Azure ML.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37947524",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":9.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":123.0932055556,
        "Challenge_title":"AzureML: taking parameters as input",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":91.0,
        "Challenge_word_count":117,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436432728608,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Colleferro, Italy",
        "Poster_reputation_count":809.0,
        "Poster_view_count":361.0,
        "Solution_body":"<p>You can use web service parameters as shown here - <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-web-service-parameters\/\" rel=\"nofollow\">https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-web-service-parameters<\/a>\/<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":52.8,
        "Solution_reading_time":3.97,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":12.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1415722650716,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Verona, VR, Italy",
        "Answerer_reputation_count":4811.0,
        "Answerer_view_count":713.0,
        "Challenge_adjusted_solved_time":72.4820763889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Azure Machine Learning Service to deploy a ML model as web service.<\/p>\n<p>I <a href=\"https:\/\/stackoverflow.com\/a\/55281703\/4240413\">registered a <code>model<\/code><\/a> and now would like to deploy it as an ACI web service as in <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where#aci\" rel=\"nofollow noreferrer\">the guide<\/a>.<\/p>\n<p>To do so I define<\/p>\n<pre><code>from azureml.core.webservice import Webservice, AciWebservice\nfrom azureml.core.image import ContainerImage\n\naciconfig = AciWebservice.deploy_configuration(cpu_cores=4, \n                      memory_gb=32, \n                      tags={&quot;data&quot;: &quot;text&quot;,  &quot;method&quot; : &quot;NB&quot;}, \n                      description='Predict something')\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>image_config = ContainerImage.image_configuration(execution_script=&quot;score.py&quot;, \n                      docker_file=&quot;Dockerfile&quot;,\n                      runtime=&quot;python&quot;, \n                      conda_file=&quot;myenv.yml&quot;)\n<\/code><\/pre>\n<p>and create an image with<\/p>\n<pre><code>image = ContainerImage.create(name = &quot;scorer-image&quot;,\n                      models = [model],\n                      image_config = image_config,\n                      workspace = ws\n                      )\n<\/code><\/pre>\n<p>Image creation succeeds with<\/p>\n<blockquote>\n<p>Creating image Image creation operation finished for image\nscorer-image:5, operation &quot;Succeeded&quot;<\/p>\n<\/blockquote>\n<p>Also, troubleshooting the image by running it locally on an Azure VM with<\/p>\n<pre><code>sudo docker run -p 8002:5001 myscorer0588419434.azurecr.io\/scorer-image:5\n<\/code><\/pre>\n<p>allows me to run (locally) queries successfully against <code>http:\/\/localhost:8002\/score<\/code>.<\/p>\n<p>However, deployment with<\/p>\n<pre><code>service_name = 'scorer-svc'\nservice = Webservice.deploy_from_image(deployment_config = aciconfig,\n                                        image = image,\n                                        name = service_name,\n                                        workspace = ws)\n<\/code><\/pre>\n<p>fails with<\/p>\n<blockquote>\n<p>Creating service<br \/>\nRunning.<br \/>\nFailedACI service creation operation finished, operation &quot;Failed&quot;<br \/>\nService creation polling reached terminal state, current service state: Transitioning<br \/>\nService creation polling reached terminal state, unexpected response received. Transitioning<\/p>\n<\/blockquote>\n<p>I tried setting in the <code>aciconfig<\/code> more generous <code>memory_gb<\/code>, but to no avail: the deployment stays in a <em>transitioning<\/em> state (like in the image below if monitored on the Azure portal):\n<a href=\"https:\/\/i.stack.imgur.com\/gCjI3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gCjI3.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Also, running <code>service.get_logs()<\/code> gives me<\/p>\n<blockquote>\n<p>WebserviceException: Received bad response from Model Management\nService: Response Code: 404<\/p>\n<\/blockquote>\n<p>What could possibly be the culprit?<\/p>",
        "Challenge_closed_time":1553818018048,
        "Challenge_comment_count":2,
        "Challenge_created_time":1553557082573,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy a machine learning model as a web service using Azure Machine Learning Service and Azure Container Instance. The image creation is successful and the model runs locally, but the deployment fails with a \"transitioning\" state and a 404 error when trying to retrieve logs. The user has tried increasing the memory but the issue persists. The cause of the problem is unknown.",
        "Challenge_last_edit_time":1597618502196,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55347910",
        "Challenge_link_count":5,
        "Challenge_participation_count":3,
        "Challenge_readability":15.5,
        "Challenge_reading_time":38.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":72.4820763889,
        "Challenge_title":"Why does my ML model deployment in Azure Container Instance still fail with \"current service state: Transitioning\"?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":3489.0,
        "Challenge_word_count":261,
        "Platform":"Stack Overflow",
        "Poster_created_time":1415722650716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Verona, VR, Italy",
        "Poster_reputation_count":4811.0,
        "Poster_view_count":713.0,
        "Solution_body":"<p>If ACI deployment fails, one solution is trying to allocate <em>less<\/em> resources, e.g.<\/p>\n\n<pre><code>aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n                  memory_gb=8, \n                  tags={\"data\": \"text\",  \"method\" : \"NB\"}, \n                  description='Predict something')\n<\/code><\/pre>\n\n<p>While the error messages thrown are not particularly informative, this is actually clearly stated in the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/container-instances\/container-instances-region-availability\" rel=\"nofollow noreferrer\">documentation<\/a>:<\/p>\n\n<blockquote>\n  <p>When a region is under heavy load, you may experience a failure when\n  deploying instances. To mitigate such a deployment failure, try\n  deploying instances with lower resource settings [...]<\/p>\n<\/blockquote>\n\n<p>The documentation also states which are the maximum values of the CPU\/RAM resources available in the different regions (at the time of writing, requiring a deployment with <code>memory_gb=32<\/code> would likely fail in all regions because of insufficient resources).<\/p>\n\n<p>Upon requiring less resources, deployment should succeed with <\/p>\n\n<blockquote>\n  <p>Creating service<br>\n  Running......................................................<br>\n  SucceededACI service creation operation finished, operation<br>\n  \"Succeeded\" Healthy<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1554115865523,
        "Solution_link_count":1.0,
        "Solution_readability":15.7,
        "Solution_reading_time":17.17,
        "Solution_score_count":3.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":134.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1285219808283,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Perth WA, Australia",
        "Answerer_reputation_count":6770.0,
        "Answerer_view_count":1127.0,
        "Challenge_adjusted_solved_time":0.0213536111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm currently trying to integrate an ML model currently deployed as a webservice on AzureML with PowerBI.<\/p>\n<p>I see that it can be <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#invoking-the-azure-ml-model-in-power-bi\" rel=\"nofollow noreferrer\">integrated<\/a> but the model requires the addition of a schema file when it is <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#schema-discovery-for-machine-learning-models\" rel=\"nofollow noreferrer\">being deployed as a webservice<\/a>. Without this, the model can't be viewed in PowerBI.<\/p>\n<p>The problem that I have come up against is that I use MLflow to log ML model performances and subsequently to deploy a selected model onto AzureML as a webservice using MLflow's AzureML integration - mlflow.azureml.deploy(). This unfortunately doesn't have the option to define a schema file before the model is deployed, thus resulting in no model being available in PowerBI as it lacks the required schema file.<\/p>\n<p>My options seem to be:<\/p>\n<ol>\n<li>Find a workaround, possibly using the working <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/mlflow\/model-serving\" rel=\"nofollow noreferrer\">REST api of the model in a power query<\/a>.<\/li>\n<li>Rewrite the deployment code and handle the webservice deployment steps in Azure instead of MLflow.<\/li>\n<\/ol>\n<p>I thought I would ask to see if I am maybe missing something as I can't find a workaround using my current code to define a schema file in MLflow when deploying with mlflow.azureml.deploy().<\/p>",
        "Challenge_closed_time":1600604920243,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600261190477,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to integrate an ML model deployed as a webservice on AzureML with PowerBI, but the model requires a schema file to be viewed in PowerBI. The user uses MLflow to deploy the model onto AzureML, but MLflow's AzureML integration does not have the option to define a schema file before deployment, resulting in no model being available in PowerBI. The user is considering finding a workaround using the REST API or rewriting the deployment code to handle the webservice deployment steps in Azure instead of MLflow.",
        "Challenge_last_edit_time":1600855880503,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63920599",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":21.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":95.4804905556,
        "Challenge_title":"PowerBI and MLflow integration (through AzureML)",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":405.0,
        "Challenge_word_count":204,
        "Platform":"Stack Overflow",
        "Poster_created_time":1600260166047,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Point number 2 is the way we solved this issue. Instead of using MLflow to deploy to a scoring service on Azure, we wrote a custom code which load MLflow model when container is initialised.<\/p>\n<p>Scoring code is something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nfrom mlflow.pyfunc import load_model\n\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n\ndef init():\n    global model\n    model = load_model(os.path.join(os.environ.get(&quot;AZUREML_MODEL_DIR&quot;), &quot;awesome_model&quot;))\n\n@input_schema('data', NumpyParameterType(input_sample))\n@output_schema(NumpyParameterType(output_sample))\n\ndef run(data):\n    return model.predict(data)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1600855957376,
        "Solution_link_count":0.0,
        "Solution_readability":16.9,
        "Solution_reading_time":10.68,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":75.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1280527017200,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3035.0,
        "Answerer_view_count":129.0,
        "Challenge_adjusted_solved_time":114.9383547222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>After setting up an endpoint for my model on Amazon SageMaker, I am trying to invoke it with a POST request which contains a file with a key <code>image<\/code> &amp; content type as <code>multipart\/form-data<\/code>.<\/p>\n\n<p>My AWS CLI command is like this:<\/p>\n\n<pre><code>aws sagemaker-runtime invoke-endpoint --endpoint-name &lt;endpoint-name&gt; --body image=@\/local\/file\/path\/dummy.jpg --content-type multipart\/form-data output.json --region us-east-1\n<\/code><\/pre>\n\n<p>which should be an equivalent of:<\/p>\n\n<pre><code>curl -X POST -F \"image=@\/local\/file\/path\/dummy.jpg\" http:\/\/&lt;endpoint&gt;\n<\/code><\/pre>\n\n<p>After running the <code>aws<\/code> command, the file is not transferred via the request, and my model is receiving the request without any file in it.<\/p>\n\n<p>Can someone please tell me what should be the correct format of the <code>aws<\/code> command in order to achieve this?<\/p>",
        "Challenge_closed_time":1533918119980,
        "Challenge_comment_count":2,
        "Challenge_created_time":1533504341903,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to invoke an endpoint for their model on Amazon SageMaker using a POST request with a file as a multipart\/form-data body. They are using an AWS CLI command to transfer the file, but the file is not being transferred via the request, and the model is receiving the request without any file in it. The user is seeking guidance on the correct format of the AWS command to transfer the file successfully.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51698373",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":12.3,
        "Challenge_reading_time":12.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":114.9383547222,
        "Challenge_title":"Amazon SageMaker: Invoke endpoint with file as multipart\/form-data",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":2346.0,
        "Challenge_word_count":119,
        "Platform":"Stack Overflow",
        "Poster_created_time":1472843515756,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>The first problem is that you're using 'http' for your CURL request. Virtually all AWS services strictly use 'https' as their protocol, SageMaker included. <a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/rande.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/rande.html<\/a>. I'm going to assume this was a typo though.<\/p>\n\n<p>You can check the verbose output of the AWS CLI by passing the '--debug' argument to your call. I re-ran a similar experiment with my favorite duck.jpg image:<\/p>\n\n<pre><code>aws --debug sagemaker-runtime invoke-endpoint --endpoint-name MyEndpoint --body image=@\/duck.jpg --content-type multipart\/form-data  &gt;(cat)\n<\/code><\/pre>\n\n<p>Looking at the output, I see:<\/p>\n\n<pre><code>2018-08-10 08:42:20,870 - MainThread - botocore.endpoint - DEBUG - Making request for OperationModel(name=InvokeEndpoint) (verify_ssl=True) with params: {'body': 'image=@\/duck.jpg', 'url': u'https:\/\/sagemaker.us-west-2.amazonaws.com\/endpoints\/MyEndpoint\/invocations', 'headers': {u'Content-Type': 'multipart\/form-data', 'User-Agent': 'aws-cli\/1.15.14 Python\/2.7.10 Darwin\/16.7.0 botocore\/1.10.14'}, 'context': {'auth_type': None, 'client_region': 'us-west-2', 'has_streaming_input': True, 'client_config': &lt;botocore.config.Config object at 0x109a58ed0&gt;}, 'query_string': {}, 'url_path': u'\/endpoints\/MyEndpoint\/invocations', 'method': u'POST'}\n<\/code><\/pre>\n\n<p>It looks like the AWS CLI is using the string literal '@\/duck.jpg', not the file contents.<\/p>\n\n<p>Trying again with curl and the \"--verbose\" flag:<\/p>\n\n<pre><code>curl --verbose -X POST -F \"image=@\/duck.jpg\" https:\/\/sagemaker.us-west-2.amazonaws.com\/endpoints\/MyEndpoint\/invocations\n<\/code><\/pre>\n\n<p>I see the following:<\/p>\n\n<pre><code>Content-Length: 63097\n<\/code><\/pre>\n\n<p>Much better. The '@' operator is a CURL specific feature. The AWS CLI does have a way to pass files though: <\/p>\n\n<pre><code>--body fileb:\/\/\/duck.jpg\n<\/code><\/pre>\n\n<p>There is also a 'file' for non-binary files such as JSON. Unfortunately you cannot have the prefix. That is, you cannot say:<\/p>\n\n<pre><code> --body image=fileb:\/\/\/duck.jpg\n<\/code><\/pre>\n\n<p>You can prepend the string 'image=' to your file with a command such as the following. (You'll probably need to be more clever if your images are really big; this is really inefficient.)<\/p>\n\n<pre><code> echo -e \"image=$(cat \/duck.jpg)\" &gt; duck_with_prefix\n<\/code><\/pre>\n\n<p>Your final command would then be:<\/p>\n\n<pre><code> aws sagemaker-runtime invoke-endpoint --endpoint-name MyEndpoint --body fileb:\/\/\/duck_with_prefix --content-type multipart\/form-data  &gt;(cat)\n<\/code><\/pre>\n\n<p>Another note: Using raw curl with AWS services is extremely difficult due to the AWS Auth signing requirements - <a href=\"https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/API\/sig-v4-authenticating-requests.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/API\/sig-v4-authenticating-requests.html<\/a> <\/p>\n\n<p>It can be done, but you'll likely be more productive by using the AWS CLI or a pre-existing tool such as Postman - <a href=\"https:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/how-to-use-postman-to-call-api.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/how-to-use-postman-to-call-api.html<\/a> <\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":8.0,
        "Solution_readability":14.9,
        "Solution_reading_time":43.72,
        "Solution_score_count":3.0,
        "Solution_sentence_count":28.0,
        "Solution_word_count":323.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":1.9786919444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The results can be written to SQL Azure using the writer module in the experiment but after publishing the web service the output comes in the Json Structure and it doesn't go to the writer module <\/p>",
        "Challenge_closed_time":1430897690128,
        "Challenge_comment_count":0,
        "Challenge_created_time":1430890566837,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in writing the results of an Azure ML web service to an Azure SQL database because the output is in JSON structure and cannot be processed by the writer module.",
        "Challenge_last_edit_time":1431065815883,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/30068341",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":4.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":1.9786919444,
        "Challenge_title":"How to write the results of Azure ML web service to the azure sql database (The output of Azure ML web service is in Json structure)",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":990.0,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403541426412,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, India",
        "Poster_reputation_count":191.0,
        "Poster_view_count":22.0,
        "Solution_body":"<p>Don't set output port and use Batch execution service - details are provided here - <a href=\"http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-publish-a-machine-learning-web-service\/\" rel=\"nofollow\">Publish web service<\/a> and <a href=\"http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-consume-web-services\/\" rel=\"nofollow\">consume web service<\/a><\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":28.8,
        "Solution_reading_time":5.51,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":24.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1367358004727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA",
        "Answerer_reputation_count":6421.0,
        "Answerer_view_count":642.0,
        "Challenge_adjusted_solved_time":11.8625777778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have one live AWS Sagemaker endpoint where we have auto scaled enabled. \nNow I want to updated it from 'ml.t2.xlarge' to 'ml.t2.2xlarge' but it is showing this error <\/p>\n\n<pre><code>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the \nUpdateEndpoint operation: The variant(s) \"[config1]\" must be deregistered as scalable targets with \nApplication Auto Scaling before they can be removed or have their instance type updated.\n<\/code><\/pre>\n\n<p>I believe we need to first de-register auto-scaling using this link \n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-delete.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-delete.html<\/a><\/p>\n\n<p>but I doubt if will take our application down and the new model with training will take multiple hours. We can't afford this so please let me know if there are any better way to do it.<\/p>",
        "Challenge_closed_time":1582835892307,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582793187027,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while updating the instance type of a live AWS Sagemaker endpoint with auto-scaling enabled. The error message suggests that the auto-scaling needs to be de-registered before updating the instance type. However, the user is concerned that de-registering auto-scaling may take the application down and the new model with training will take multiple hours. The user is seeking a better way to update the instance type without affecting the application.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60429339",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":13.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":11.8625777778,
        "Challenge_title":"Update live AWS Sagemaker auto scaled endpoint instance type without putting it down",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":869.0,
        "Challenge_word_count":129,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501403168107,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":1370.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>You should have no problem updating your Endpoint instance type without taking the availability hit. The basic method looks like this when you have an active autoscaling policy:<\/p>\n\n<ol>\n<li>Create a new EndpointConfig that uses the new instance type, <code>ml.t2.2xlarge<\/code>\n\n<ol>\n<li>Do this by calling <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateEndpointConfig.html\" rel=\"nofollow noreferrer\"><code>CreateEndpointConfig<\/code><\/a>.<\/li>\n<li>Pass in the same values you used for your previous Endpoint config. You can point to the same <code>ModelName<\/code> that you did as well. By reusing the same model, you don't have to retrain it or anything<\/li>\n<\/ol><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-delete.html\" rel=\"nofollow noreferrer\">Delete the existing autoscaling policy<\/a>\n\n<ol>\n<li>Depending on your autoscaling, you might want to increase the desired count of your Endpoint in the event it needs to scale while you are doing this.<\/li>\n<li>If you are experience a spike in traffic while you are making these API calls, you risk an outage of your model if it can't keep up with traffic. Just keep this in mind and possibly scale in advance for this possibility.<\/li>\n<\/ol><\/li>\n<li>Call <code>UpdateEndpoint<\/code> like you did previously and specify this new <code>EndpointConfigName<\/code><\/li>\n<li>Wait for your Endpoint status to be <code>InService<\/code>. This should take 10-20 mins.<\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-add-policy.html\" rel=\"nofollow noreferrer\">Create a new autoscaling policy<\/a> for this new Endpoint and production variant<\/li>\n<\/ol>\n\n<p>You should be good to go without sacrificing availability.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":11.7,
        "Solution_reading_time":22.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":215.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1565528932887,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"United Kingdom",
        "Answerer_reputation_count":1579.0,
        "Answerer_view_count":91.0,
        "Challenge_adjusted_solved_time":17.8840719444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am deploying 50 NLP models on Azure Container Instances via the Azure Machine Learning service. All 50 models are quite similar and have the same input\/output format with just the model implementation changing slightly. <\/p>\n\n<p>I want to write a generic score.py entry file and pass in the model name as a parameter. The interface method signature does not allow a parameter in the init() method of score.py, so I moved the model loading into the run method. I am assuming the init() method gets run once whereas Run(data) will get executed on every invocation, so this is possibly not ideal (the models are 1 gig in size)<\/p>\n\n<p>So how can I pass in some value to the init() method of my container to tell it what model to load? <\/p>\n\n<p>Here is my current, working code:<\/p>\n\n<pre><code>def init():\n\ndef loadModel(model_name):\n    model_path = Model.get_model_path(model_name)  \n    return fasttext.load_model(model_path)\n\ndef run(raw_data):\n    # extract model_name from raw_data omitted...\n    model = loadModel(model_name)\n\n    ...\n<\/code><\/pre>\n\n<p>but this is what I would like to do (which breaks the interface)<\/p>\n\n<pre><code>def init(model_name):\n    model = loadModel(model_name)\n\ndef loadModel(model_name):\n    model_path = Model.get_model_path(model_name)  \n    return fasttext.load_model(model_path)\n\ndef run(raw_data):\n    ...\n<\/code><\/pre>",
        "Challenge_closed_time":1572381894847,
        "Challenge_comment_count":0,
        "Challenge_created_time":1572325608637,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to deploy 50 NLP models on Azure Container Instances via the Azure Machine Learning service and is trying to write a generic score.py entry file to pass in the model name as a parameter. However, the interface method signature does not allow a parameter in the init() method of score.py, so the user moved the model loading into the run method. The user is looking for a way to pass in some value to the init() method of the container to tell it what model to load.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58601697",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":17.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":15.6350583334,
        "Challenge_title":"How to pass in the model name during init in Azure Machine Learning Service?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":851.0,
        "Challenge_word_count":195,
        "Platform":"Stack Overflow",
        "Poster_created_time":1256089885500,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sydney, Australia",
        "Poster_reputation_count":4947.0,
        "Poster_view_count":531.0,
        "Solution_body":"<p>If you're looking to use the same deployed container and switch models between requests; it's not the preferred design choice for Azure machine learning service, we need to specify the model name to load during build\/deploy.<\/p>\n\n<p>Ideally, each deployed web-service endpoint should allow inference of one model only; with the model name defined before the container the image starts building\/deploying. <\/p>\n\n<p>It is mandatory that the entry script has both <code>init()<\/code> and <code>run(raw_data)<\/code> with those <strong>exact<\/strong> signatures. <\/p>\n\n<p>At the moment, we can't change the signature of <code>init()<\/code> method to take a parameter like in <code>init(model_name)<\/code>.  <\/p>\n\n<p>The only dynamic user input you'd ever get to pass into this web-service is via <code>run(raw_data)<\/code> method. As you have tried, given the size of your model passing it via run is not feasible. <\/p>\n\n<p><code>init()<\/code> is run first and only <strong>once<\/strong> after your web-service deploy. Even if <code>init()<\/code> took the <code>model_name<\/code> parameter, there isn't a straight forward way to call this method directly and pass your desired model name.<\/p>\n\n<hr>\n\n<p>But, one possible solution is: <\/p>\n\n<p>You can create params file like below and store the file in azure blob storage.<\/p>\n\n<p>Example runtime parameters generation script:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pickle\n\nparams = {'model_name': 'YOUR_MODEL_NAME_TO_USE'}\n\nwith open('runtime_params.pkl', 'wb') as file:\n    pickle.dump(params, file)\n\n<\/code><\/pre>\n\n<p>You'll need to use <a href=\"https:\/\/github.com\/Azure\/azure-storage-python\" rel=\"nofollow noreferrer\">Azure Storage Python SDK<\/a> to write code that can read from your blob storage account. This also mentioned in the official docs <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where#prepare-to-deploy\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Then you can access this from <code>init()<\/code> function in your score script. <\/p>\n\n<p>Example <code>score.py<\/code> script:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azure.storage.blob import BlockBlobService\nimport pickle\n\ndef init():\n\n  global model\n\n  block_blob_service = BlockBlobService(connection_string='your_connection_string')\n\n  blob_item = block_blob_service.get_blob_to_bytes('your-container-name','runtime_params.pkl')\n\n  params = pickle.load(blob_item.content)\n\n  model = loadModel(params['model_name'])\n<\/code><\/pre>\n\n<p>You can store connection strings in Azure KeyVault for secure access. Azure ML Workspaces comes with built-in KeyVault integration. More info <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.keyvault.keyvault?view=azure-ml-py#get-secret-name-\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>With this approach, you're abstracting runtime params config to another cloud location rather than the container itself. So you wouldn't need to re-build the image or deploy the web-service again. Simply restarting the container will work.<\/p>\n\n<hr>\n\n<p>If you're looking to simply re-use <code>score.py<\/code> (not changing code) for <strong>multiple model deployments in multiple containers<\/strong> then here's another possible solution.<\/p>\n\n<p>You can define your model name to use in web-service in a text file and read it in score.py. You'll need to pass this text file as a dependency when setting up the image config.<\/p>\n\n<p>This would, however, need multiple params files for each container deployment.<\/p>\n\n<p>Passing 'runtime_params.pkl' in <code>dependencies<\/code> to your image config (More detail example <a href=\"https:\/\/github.com\/rithinch\/heartfulness-similar-content-service\/blob\/master\/experiments\/notebooks\/Deploy%20Model%20-%20Azure.ipynb\" rel=\"nofollow noreferrer\">here<\/a>):<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>image_config = ContainerImage.image_configuration(execution_script=\"score.py\", \n                                                  runtime=\"python\", \n                                                  conda_file=\"myenv.yml\",\n                                                  dependencies=[\"runtime_params.pkl\"],\n                                                  docker_file=\"Dockerfile\")\n<\/code><\/pre>\n\n<p>Reading this in your score.py <code>init()<\/code> function:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def init():\n\n  global model\n\n  with open('runtime_params.pkl', 'rb') as file:\n    params = pickle.load(file)\n\n  model = loadModel(params['model_name'])\n\n<\/code><\/pre>\n\n<p>Since your creating a new image config with this approach, you'll need to build the image and re-deploy the service.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1572389991296,
        "Solution_link_count":4.0,
        "Solution_readability":12.7,
        "Solution_reading_time":58.32,
        "Solution_score_count":3.0,
        "Solution_sentence_count":40.0,
        "Solution_word_count":476.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.0071163889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to pass parameters to an Azure ML Web Service with Postman? I created an R web service endpoint that runs in an Azure Container Instance. My run function has one argument (&quot;data&quot;). I can call the web service using Azure ML R SDK (using invoke_webservice()) and the input parameter is read successfully from the request content. The input is constructed like:  <\/p>\n<pre><code>toJSON(data.frame(data=&quot;This is my test string&quot;))\n<\/code><\/pre>\n<p>Result:  <\/p>\n<pre><code>[{&quot;data&quot;: &quot;This is my test string&quot;}]\n<\/code><\/pre>\n<p>If I create a Postman request and copy the input to the request body, the input parameter is not passed to the web service. The web service can return a static output to Postman but the variable data is always empty. Is this a property of the ML Web Service? If not, how can I set up the request body so that the argument is read successfully? I have tried many variations, but none have worked.  <\/p>\n<p>I have set content-type header to application\/json. I don't have authentication in the web service, since it is just a test instance.  <\/p>\n<p>Ultimately, we need to call the web service with C# from Azure Function. I know that we can use the C# template from documentation and it can probably pass the parameter to the web service, but it would be nice to be able to test the web service with Postman.<\/p>",
        "Challenge_closed_time":1603976126076,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603965300457,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in passing parameters to an Azure ML Web Service with Postman. The user has created an R web service endpoint that runs in an Azure Container Instance and can call the web service using Azure ML R SDK. However, when creating a Postman request and copying the input to the request body, the input parameter is not passed to the web service. The user has tried many variations but none have worked. The user has set the content-type header to application\/json and does not have authentication in the web service. The user ultimately needs to call the web service with C# from Azure Function.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/144230\/consume-azure-ml-web-service-with-postman-how-to-p",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":17.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":3.0071163889,
        "Challenge_title":"Consume Azure ML Web Service with Postman: how to pass arguments?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":240,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Try this in postman.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/35960-postman.png?platform=QnA\" alt=\"35960-postman.png\" \/>    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":23.9,
        "Solution_reading_time":2.22,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":9.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":24.8431438889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>hello.  <\/p>\n<p>I am currently using Machine Learning Studio (classic).  <\/p>\n<p>'From now through 31 August 2024, you can continue to use the existing Machine Learning Studio (classic) experiments and web services. Beginning 1 December 2021, new creation of Machine Learning Studio (classic) resources will not be available.  <\/p>\n<p>Is the following interpretation correct?  <\/p>\n<p>Things you can't do from 1 December 2021  <br \/>\n-Creating a workspace for Machine Learning Studio (classic)  <br \/>\n-Creating a web service plan for Machine Learning Studio (classic)  <\/p>\n<p>What you can do until 1 December 2021  <br \/>\n-Creating new Machine Learning Studio (classic) experiments  <br \/>\n-Creating new Machine Learning Studio (classic) trained models  <br \/>\n-Creating a new Machine Learning Studio (classic) web service<\/p>",
        "Challenge_closed_time":1636209342128,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636119906810,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is currently using Machine Learning Studio (classic) and has received a notification that new creation of resources for this platform will not be available from 1 December 2021. The user can continue to use existing experiments and web services until 31 August 2024. The user can still create new experiments, trained models, and web services until 1 December 2021, but will not be able to create a workspace or web service plan for Machine Learning Studio (classic) after this date.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/616952\/about-the-end-of-machine-learning-studio-(classic)",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":10.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":24.8431438889,
        "Challenge_title":"About the end of Machine Learning Studio (classic)#2",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":126,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, customers will not be able to create new ML Studio(classic) workspaces after Dec 1, 2021. However, customers can create or update experiments\/web services in existing workspaces until Aug 31, 2024.<\/p>\n<hr \/>\n<p>--- *Kindly <em><strong>Accept Answer<\/strong><\/em> if the information helps. Thanks.*<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.8,
        "Solution_reading_time":3.94,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":41.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":34.21275,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained a model using azure AutoML and downloaded the model. Then I created a new conda env using the conda file and tried to execute the scoring_file_v_1_0_0.py which is in the zip. I receive this error:  <\/p>\n<p>&gt; WARNING - Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception cannot import name 'convert_inputs'.  <\/p>\n<p>Is this still some dependency problem or am I doing something unexpected? I did expect the script to open a web server to serve the model.<\/p>",
        "Challenge_closed_time":1591288865200,
        "Challenge_comment_count":1,
        "Challenge_created_time":1591165699300,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to execute the scoring file of an Azure AutoML model that was downloaded and deployed locally. The error message indicates a failure in loading the azureml_run_type_providers due to an exception of not being able to import the name 'convert_inputs'. The user is unsure if this is a dependency problem or if they are missing something in their execution.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/31601\/deploy-azureml-model-locally-cannot-import-name-co",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.4,
        "Challenge_reading_time":8.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":34.21275,
        "Challenge_title":"Deploy AzureML Model locally: cannot import name 'convert_inputs'",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":91,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I am sorry, the problem was something path related that got mixed up within my jupyter notebook setup and a moved directory. So actually the pickle was not where the framework expect it. Can be closed.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":2.54,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":36.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":141.7067575,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>With an <code>sagemaker.estimator.Estimator<\/code>, I want to re-<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html#sagemaker.estimator.EstimatorBase.deploy\" rel=\"nofollow noreferrer\">deploy<\/a> a model after retraining (calling <code>fit<\/code> with new data).<\/p>\n<p>When I call this<\/p>\n<pre><code>estimator.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')\n<\/code><\/pre>\n<p>I get an error<\/p>\n<pre><code>botocore.exceptions.ClientError: An error occurred (ValidationException) \nwhen calling the CreateEndpoint operation: \nCannot create already existing endpoint &quot;arn:aws:sagemaker:eu-east- \n1:1776401913911:endpoint\/zyx&quot;.\n<\/code><\/pre>\n<p>Apparently I want to use functionality like <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_UpdateEndpoint.html\" rel=\"nofollow noreferrer\">UpdateEndpoint<\/a>. How do I access that functionality from this API?<\/p>",
        "Challenge_closed_time":1601891469683,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601630651783,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to re-deploy a re-trained Sagemaker model using an estimator. The error message indicates that the endpoint already exists, and the user is seeking guidance on how to access the UpdateEndpoint functionality from the API.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64169189",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":20.5,
        "Challenge_reading_time":13.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":72.4494166667,
        "Challenge_title":"How can I deploy a re-trained Sagemaker model to an endpoint?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1958.0,
        "Challenge_word_count":78,
        "Platform":"Stack Overflow",
        "Poster_created_time":1227171471292,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Israel",
        "Poster_reputation_count":17500.0,
        "Poster_view_count":1561.0,
        "Solution_body":"<p>Yes, under the hood the <code>model.deploy<\/code> creates a model, an endpoint configuration and an endpoint. When you call again the method from an already-deployed, trained estimator it will create an error because a similarly-configured endpoint is already deployed. What I encourage you to try:<\/p>\n<ul>\n<li><p>use the <code>update_endpoint=True<\/code> parameter. From the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\" rel=\"noreferrer\">SageMaker SDK doc<\/a>:\n<em>&quot;Additionally, it is possible to deploy a different endpoint configuration, which links to your model, to an already existing\nSageMaker endpoint. This can be done by specifying the existing\nendpoint name for the <code>endpoint_name<\/code> parameter along with the\n<code>update_endpoint<\/code> parameter as True within your <code>deploy()<\/code> call.&quot;<\/em><\/p>\n<\/li>\n<li><p>Alternatively, if you want to create a separate model you can specify a new <code>model_name<\/code> in your <code>deploy<\/code><\/p>\n<\/li>\n<\/ul>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1602140796110,
        "Solution_link_count":1.0,
        "Solution_readability":14.0,
        "Solution_reading_time":13.16,
        "Solution_score_count":6.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":123.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":457.1169444444,
        "Challenge_answer_count":0,
        "Challenge_body":"The bucket of processed data does not exist (src\/sagemaker\/FD_SL_Training_BYO_Codes.ipynb)\r\n\r\n\r\n### Reproduction Steps\r\n\r\naws s3 ls s3:\/\/fraud-detection-solution\/processed_data\r\n\r\n\r\n\r\n### Error Log\r\n\r\nAn error occurred (NoSuchBucket) when calling the ListObjectsV2 operation: The specified bucket does not exist\r\n\r\n\r\n\r\n### Environment\r\n\r\n  - **CDK CLI Version:** 1.75.0 (build 7708242)\r\n  - **Framework Version:** not installed\r\n  - **Node.js Version:**  not installed\r\n  - **OS               :**\r\n\r\n### Other\r\n\r\n<!-- e.g. detailed explanation, stacktraces, related issues, suggestions on how to fix, links for us to have context, eg. associated pull-request, stackoverflow, gitter, etc -->\r\n\r\n\r\n\r\n--- \r\n\r\nThis is :bug: Bug Report",
        "Challenge_closed_time":1621931178000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620285557000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue with their Sagemaker endpoint failing to deploy or timing out with a server error (0) bug. The error log shows that the backend worker process died due to a parameter conflict between the Sagemaker endpoint deployment code and the model training code on n-hidden and hidden_size. The user provided reproduction steps and environment details.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/realtime-fraud-detection-with-gnn-on-dgl\/issues\/103",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":12.7,
        "Challenge_reading_time":9.06,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":25.0,
        "Challenge_repo_issue_count":1008.0,
        "Challenge_repo_star_count":130.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":457.1169444444,
        "Challenge_title":"The data path inside sagemaker notebook does not work",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":85,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":572.4141666667,
        "Challenge_answer_count":0,
        "Challenge_body":"In particular:\r\n- Experiments needs to be renamed to Jobs\r\n- Datasets needs to be renamed to Data\r\n\r\nFurther changes probably aren't absolutely necessary right now, but should be considered as well. See #616.",
        "Challenge_closed_time":1663956142000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661895451000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The AzureML extension in VS Code (Insiders) prompts users to login twice when the user is signed out and reloads VS Code. This behavior is disruptive and unnecessary, and no other extensions for AWS or Azure do this.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1691",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.1,
        "Challenge_reading_time":3.19,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":572.4141666667,
        "Challenge_title":"Update Treeview asset labels to match Azure ML Studio.",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":40,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1626973229036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":199.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":23.1600519444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a pytorch model that i have tested as a real-time endpoint in sagemaker, now i want to test it with batch inference. I am using jsonl data, and setting up a batch transform job as documented in aws documentation, in addition, i'm using my own inference.py (see sample below). I'm getting a json decode error inside the input_fn , function, when i try =&gt; json.loads(request_body).<\/p>\n<p>the error is =&gt; raise JSONDecodeError(&quot;Extra data&quot;, s, end)<\/p>\n<p>has anyone tried this? I sucessfully tested this model and json input with a real time endpoint in sagemaker, but now i'm trying to switch to batch and it is erroring it out.<\/p>\n<p>inference.py<\/p>\n<pre><code>def model_fn(model_dir):\n   ....\n\n\ndef input_fn(request_body, request_content_type):\n    data = json.loads(request_body)\n    return data\n\ndef predict_fn(data, model)\n  ...\n<\/code><\/pre>\n<p>set up for batch job via lambda<\/p>\n<pre><code>response = client.create_transform_job(\n    TransformJobName='some-job',\n    ModelName='mypytorchmodel',\n    ModelClientConfig={\n        'InvocationsTimeoutInSeconds': 3600,\n        'InvocationsMaxRetries': 1\n    },\n    BatchStrategy='MultiRecord',\n    TransformInput={\n        'DataSource': {\n            'S3DataSource': {\n                'S3DataType': 'S3Prefix',\n                'S3Uri': 's3:\/\/inputpath'\n            }\n        },\n        'ContentType': 'application\/json',\n        'SplitType': 'Line'\n    },\n    TransformOutput={\n        'S3OutputPath': 's3:\/\/outputpath',\n        'Accept': 'application\/json',\n        'AssembleWith': 'Line',\n    },\n    TransformResources={\n        'InstanceType': 'ml.g4dn.xlarge'\n        'InstanceCount': 1\n    }\n)\n<\/code><\/pre>\n<p>input file<\/p>\n<pre><code>{&quot;input&quot; : &quot;input line one&quot;}\n{&quot;input&quot; : &quot;input line two&quot;}\n{&quot;input&quot; : &quot;input line three&quot;}\n{&quot;input&quot; : &quot;input line four&quot;}\n{&quot;input&quot; : &quot;input line five&quot;}\n...\n\n<\/code><\/pre>",
        "Challenge_closed_time":1661785882720,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661702506533,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to test a PyTorch model with batch inference using jsonl data in Sagemaker. They have set up a batch transform job and are using their own inference.py file. However, they are encountering a JSON decode error inside the input_fn function when trying to load the request body. The user has successfully tested the model and json input with a real-time endpoint in Sagemaker, but it is failing for batch inference.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73520188",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":23.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":23.1600519444,
        "Challenge_title":"How to get batch predictions with jsonl data in sagemaker?",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":52.0,
        "Challenge_word_count":195,
        "Platform":"Stack Overflow",
        "Poster_created_time":1584308275360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":365.0,
        "Poster_view_count":94.0,
        "Solution_body":"<p>What is your client side code where you are invoking the endpoint? You should also be properly serializing the data on the client side and handling it in your inference script. Example:<\/p>\n<pre><code>import json\ndata = json.loads(json.dumps(request_body))\npayload = json.dumps(data)\nresponse = client.invoke_endpoint(\n    EndpointName=endpoint_name,\n    ContentType=content_type,\n    Body=payload)\nresult = json.loads(response['Body'].read().decode())['Output']\nresult\n<\/code><\/pre>\n<p>Make sure to also specify your content_type appropriately &quot;application\/jsonlines&quot;.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.7,
        "Solution_reading_time":7.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1623879163643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":224.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":0.6793758333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've created my own model on a AWS SageMaker instance, with my own training and inference loops. I want to deploy it so that I can call the model for inference from AWS Lambda.<\/p>\n<p>I didn't use the SageMaker package to develop at all, but every tutorial (here is <a href=\"https:\/\/towardsdatascience.com\/using-aws-sagemaker-and-lambda-function-to-build-a-serverless-ml-platform-f14b3ec5854a%3E\" rel=\"nofollow noreferrer\">one<\/a>) I've looked at does so.<\/p>\n<p>How do I create an endpoint without using the SageMaker package.<\/p>",
        "Challenge_closed_time":1657053745440,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657051299687,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a model on AWS SageMaker without using the SageMaker SDK and wants to set up an endpoint to call the model for inference from AWS Lambda without using the SageMaker package. They are seeking guidance on how to create an endpoint without using the SageMaker package.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72874937",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":8.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.6793758333,
        "Challenge_title":"Is it possible set up an endpoint for a model I created in AWS SageMaker without using the SageMaker SDK",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":66.0,
        "Challenge_word_count":88,
        "Platform":"Stack Overflow",
        "Poster_created_time":1657050754840,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>You can use the boto3 library to do this.<\/p>\n<p>Here is an example of pseudo code for this -<\/p>\n<pre><code>import boto3\nsm_client = boto3.client('sagemaker')\ncreate_model_respose = sm_client.create_model(ModelName=model_name, ExecutionRoleArn=role, Containers=[container] )\n\ncreate_endpoint_config_response = sm_client.create_endpoint_config(EndpointConfigName=endpoint_config_name)\n\ncreate_endpoint_response = sm_client.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":25.5,
        "Solution_reading_time":7.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1525449880547,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Madrid, Spain",
        "Answerer_reputation_count":81.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":68.6075986111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am starting to use aws sagemaker on the development of my machine learning model and I'm trying to build a lambda function to process the responses of a sagemaker labeling job. I already created my own lambda function but when I try to read the event contents I can see that the event dict is completely empty, so I'm not getting any data to read.<\/p>\n\n<p>I have already given enough permissions to the role of the lambda function. Including:\n- AmazonS3FullAccess.\n- AmazonSagemakerFullAccess.\n- AWSLambdaBasicExecutionRole<\/p>\n\n<p>I've tried using this code for the Post-annotation Lambda (adapted for python 3.6):<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step2-demo1.html#sms-custom-templates-step2-demo1-post-annotation\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step2-demo1.html#sms-custom-templates-step2-demo1-post-annotation<\/a><\/p>\n\n<p>As well as this one in this git repository:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/aws-sagemaker-ground-truth-recipe\/blob\/master\/aws_sagemaker_ground_truth_sample_lambda\/annotation_consolidation_lambda.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/aws-sagemaker-ground-truth-recipe\/blob\/master\/aws_sagemaker_ground_truth_sample_lambda\/annotation_consolidation_lambda.py<\/a><\/p>\n\n<p>But none of them seemed to work.<\/p>\n\n<p>For creating the labeling job I'm using boto3's functions for sagemaker:\n<a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_labeling_job\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_labeling_job<\/a><\/p>\n\n<p>This is the code i have for creating the labeling job:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def create_labeling_job(client,bucket_name ,labeling_job_name, manifest_uri, output_path):\n\n    print(\"Creating labeling job with name: %s\"%(labeling_job_name))\n\n    response = client.create_labeling_job(\n        LabelingJobName=labeling_job_name,\n        LabelAttributeName='annotations',\n        InputConfig={\n            'DataSource': {\n                'S3DataSource': {\n                    'ManifestS3Uri': manifest_uri\n                }\n            },\n            'DataAttributes': {\n                'ContentClassifiers': [\n                    'FreeOfAdultContent',\n                ]\n            }\n        },\n        OutputConfig={\n            'S3OutputPath': output_path\n        },\n        RoleArn='arn:aws:myrolearn',\n        LabelCategoryConfigS3Uri='s3:\/\/'+bucket_name+'\/config.json',\n        StoppingConditions={\n            'MaxPercentageOfInputDatasetLabeled': 100,\n        },\n        LabelingJobAlgorithmsConfig={\n            'LabelingJobAlgorithmSpecificationArn': 'arn:image-classification'\n        },\n        HumanTaskConfig={\n            'WorkteamArn': 'arn:my-private-workforce-arn',\n            'UiConfig': {\n                'UiTemplateS3Uri':'s3:\/\/'+bucket_name+'\/templatefile'\n            },\n            'PreHumanTaskLambdaArn': 'arn:aws:lambda:us-east-1:432418664414:function:PRE-BoundingBox',\n            'TaskTitle': 'Title',\n            'TaskDescription': 'Description',\n            'NumberOfHumanWorkersPerDataObject': 1,\n            'TaskTimeLimitInSeconds': 600,\n            'AnnotationConsolidationConfig': {\n                'AnnotationConsolidationLambdaArn': 'arn:aws:my-custom-post-annotation-lambda'\n            }\n        }\n    )\n\n    return response\n<\/code><\/pre>\n\n<p>And this is the one i have for the lambda function:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>    print(\"Received event: \" + json.dumps(event, indent=2))\n    print(\"event: %s\"%(event))\n    print(\"context: %s\"%(context))\n    print(\"event headers: %s\"%(event[\"headers\"]))\n\n    parsed_url = urlparse(event['payload']['s3Uri']);\n    print(\"parsed_url: \",parsed_url)\n\n    labeling_job_arn = event[\"labelingJobArn\"]\n    label_attribute_name = event[\"labelAttributeName\"]\n\n    label_categories = None\n    if \"label_categories\" in event:\n        label_categories = event[\"labelCategories\"]\n        print(\" Label Categories are : \" + label_categories)\n\n    payload = event[\"payload\"]\n    role_arn = event[\"roleArn\"]\n\n    output_config = None # Output s3 location. You can choose to write your annotation to this location\n    if \"outputConfig\" in event:\n        output_config = event[\"outputConfig\"]\n\n    # If you specified a KMS key in your labeling job, you can use the key to write\n    # consolidated_output to s3 location specified in outputConfig.\n    kms_key_id = None\n    if \"kmsKeyId\" in event:\n        kms_key_id = event[\"kmsKeyId\"]\n\n    # Create s3 client object\n    s3_client = S3Client(role_arn, kms_key_id)\n\n    # Perform consolidation\n    return do_consolidation(labeling_job_arn, payload, label_attribute_name, s3_client)\n<\/code><\/pre>\n\n<p>I've tried debugging the event object with:<\/p>\n\n<pre><code>    print(\"Received event: \" + json.dumps(event, indent=2))\n<\/code><\/pre>\n\n<p>But it just prints an empty dictionary: <code>Received event: {}<\/code><\/p>\n\n<p>I expect the output to be something like:<\/p>\n\n<pre><code>    #Content of an example event:\n    {\n        \"version\": \"2018-10-16\",\n        \"labelingJobArn\": &lt;labelingJobArn&gt;,\n        \"labelCategories\": [&lt;string&gt;],  # If you created labeling job using aws console, labelCategories will be null\n        \"labelAttributeName\": &lt;string&gt;,\n        \"roleArn\" : \"string\",\n        \"payload\": {\n            \"s3Uri\": &lt;string&gt;\n        }\n        \"outputConfig\":\"s3:\/\/&lt;consolidated_output configured for labeling job&gt;\"\n    }\n<\/code><\/pre>\n\n<p>Lastly, when I try yo get the labeling job ARN with:<\/p>\n\n<pre><code>    labeling_job_arn = event[\"labelingJobArn\"]\n<\/code><\/pre>\n\n<p>I just get a KeyError (which makes sense because the dictionary is empty).<\/p>",
        "Challenge_closed_time":1564741689012,
        "Challenge_comment_count":0,
        "Challenge_created_time":1564494701657,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with an empty dictionary on AnnotationConsolidation lambda event for AWS Sagemaker. They have created their own lambda function and given enough permissions to the role of the lambda function, but when they try to read the event contents, they can see that the event dict is completely empty, so they are not getting any data to read. They have tried using different codes for the Post-annotation Lambda, but none of them seemed to work. The user is also unable to get the labeling job ARN as they are getting a KeyError because the dictionary is empty.",
        "Challenge_last_edit_time":1575462189052,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57273357",
        "Challenge_link_count":6,
        "Challenge_participation_count":2,
        "Challenge_readability":23.8,
        "Challenge_reading_time":70.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":68.6075986111,
        "Challenge_title":"Empty dictionary on AnnotationConsolidation lambda event for aws Sagemaker",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":846.0,
        "Challenge_word_count":425,
        "Platform":"Stack Overflow",
        "Poster_created_time":1525449880547,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Madrid, Spain",
        "Poster_reputation_count":81.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I found the problem, I needed to add the ARN of the role used by my Lamda function as a Trusted Entity on the Role used for the Sagemaker Labeling Job.<\/p>\n\n<p>I just went to <code>Roles &gt; MySagemakerExecutionRole &gt; Trust Relationships<\/code> and added:<\/p>\n\n<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": [\n          \"arn:aws:iam::xxxxxxxxx:role\/My-Lambda-Role\",\n           ...\n        ],\n        \"Service\": [\n          \"lambda.amazonaws.com\",\n          \"sagemaker.amazonaws.com\",\n           ...\n        ]\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n<\/code><\/pre>\n\n<p>This made it work for me.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.0,
        "Solution_reading_time":7.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1254957460063,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"North Carolina, USA",
        "Answerer_reputation_count":2484.0,
        "Answerer_view_count":362.0,
        "Challenge_adjusted_solved_time":27.4381997222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>To use Azure Machine Learning Web service <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/consume-web-services\" rel=\"nofollow noreferrer\">here<\/a> you can find some sample code in C#, R, Python and JavaScript. I want to use it in PowerShell.\nI found <a href=\"https:\/\/www.sepago.com\/blog\/2015\/11\/30\/zugriff-mit-powershell-auf-azure-machine-learning-api-azureml\" rel=\"nofollow noreferrer\">this<\/a> tutorial, but when I am running bellow line of code, it will return error that it is not recognized:<\/p>\n\n<pre><code>Set-AzureMLWebServiceConnection -URI $Url -APIKey $API_key\n\nOutput:\nSet-AzureMLWebServiceConnection : The term 'Set-AzureMLWebServiceConnection' is not recognized as the name of a cmdlet, function, script file, or operable \nprogram. Check the spelling of the name, or if a path was included, verify that the path is correct and try again.\nAt C:\\Users\\Reza\\Desktop\\ndbench\\Azure\\Automation\\01_get_metrics\\add_target_to_tables - runbook_01.ps1:33 char:1\n+ Set-AzureMLWebServiceConnection -URI $Url -APIKey $API_key\n+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    + CategoryInfo          : ObjectNotFound: (Set-AzureMLWebServiceConnection:String) [], CommandNotFoundException\n    + FullyQualifiedErrorId : CommandNotFoundException\n<\/code><\/pre>\n\n<p>I can't found <code>Set-AzureMLWebServiceConnection<\/code> in my PowerShell command-list and I don't know how I can enable\/install it.\n<a href=\"https:\/\/i.stack.imgur.com\/YMso7.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YMso7.jpg\" alt=\"enter image description here\"><\/a>\nCan you please guide me, how I can connect to Azure Machine Learning Web service using PowerShell?<\/p>",
        "Challenge_closed_time":1523724686212,
        "Challenge_comment_count":1,
        "Challenge_created_time":1523625908693,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to connect to Azure Machine Learning Web service using PowerShell but is encountering an error that the command 'Set-AzureMLWebServiceConnection' is not recognized. The user is seeking guidance on how to enable\/install the command and connect to the service using PowerShell.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49818134",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":15.2,
        "Challenge_reading_time":22.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":27.4381997222,
        "Challenge_title":"how connect to Azure Machine Learning Web service using PowerShell?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":303.0,
        "Challenge_word_count":169,
        "Platform":"Stack Overflow",
        "Poster_created_time":1433870950220,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tehran, Iran",
        "Poster_reputation_count":1316.0,
        "Poster_view_count":201.0,
        "Solution_body":"<p>The comment @gvee mentioned may be the best to use going forward though it is in beta.<\/p>\n\n<p>However, to answer your question, use the <code>Install-Module -Name AzureML<\/code> <a href=\"https:\/\/www.powershellgallery.com\/packages\/AzureML\/1.0.1\" rel=\"nofollow noreferrer\">command<\/a> to get access to the Azure ML commands.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ZerMp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZerMp.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.3,
        "Solution_reading_time":6.58,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":48.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":96.8504455556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using sagemaker batch transform, with json input files. see below for sample input\/output files. i have custom inference code below, and i'm using json.dumps to return prediction, but it's not returning json. I tried to use =&gt;    &quot;DataProcessing&quot;: {&quot;JoinSource&quot;: &quot;string&quot;,  }, to match input and output. but i'm getting error that &quot;unable to marshall ...&quot; . I think because , the output_fn is returning array of list or just list and not json , that is why it is unable to match input with output.any suggestions on how should i return the data?<\/p>\n<p>infernce code<\/p>\n<pre><code>def model_fn(model_dir):\n...\ndef input_fn(data, content_type):\n...\ndef predict_fn(data, model):\n...\ndef output_fn(prediction, accept):\n    if accept == &quot;application\/json&quot;:\n        return json.dumps(prediction), mimetype=accept)\n    raise RuntimeException(&quot;{} accept type is not supported by this script.&quot;.format(accept))\n<\/code><\/pre>\n<p>input file<\/p>\n<pre><code>{&quot;data&quot; : &quot;input line  one&quot; }\n{&quot;data&quot; : &quot;input line  two&quot; }\n....\n<\/code><\/pre>\n<p>output file<\/p>\n<pre><code>[&quot;output line  one&quot; ]\n[&quot;output line  two&quot; ]\n<\/code><\/pre>\n<pre><code>{\n   &quot;BatchStrategy&quot;: SingleRecord,\n   &quot;DataProcessing&quot;: { \n      &quot;JoinSource&quot;: &quot;string&quot;,\n   },\n   &quot;MaxConcurrentTransforms&quot;: 3,\n   &quot;MaxPayloadInMB&quot;: 6,\n   &quot;ModelClientConfig&quot;: { \n      &quot;InvocationsMaxRetries&quot;: 1,\n      &quot;InvocationsTimeoutInSeconds&quot;: 3600\n   },\n   &quot;ModelName&quot;: &quot;some-model&quot;,\n   &quot;TransformInput&quot;: { \n      &quot;ContentType&quot;: &quot;string&quot;,\n      &quot;DataSource&quot;: { \n         &quot;S3DataSource&quot;: { \n            &quot;S3DataType&quot;: &quot;string&quot;,\n            &quot;S3Uri&quot;: &quot;s3:\/\/bucket-sample&quot;\n         }\n      },\n      &quot;SplitType&quot;: &quot;Line&quot;\n   },\n   &quot;TransformJobName&quot;: &quot;transform-job&quot;\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1654129068707,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653780407103,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in matching input\/output with Sagemaker batch transform. The custom inference code is using json.dumps to return predictions, but it's not returning JSON. The user tried to use \"DataProcessing\" to match input and output, but it resulted in an error. The output_fn is returning an array of lists or just a list, which is why it's unable to match input with output. The user is seeking suggestions on how to return the data.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72419908",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.7,
        "Challenge_reading_time":26.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":96.8504455556,
        "Challenge_title":"How to match input\/output with sagemaker batch transform?",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":358.0,
        "Challenge_word_count":178,
        "Platform":"Stack Overflow",
        "Poster_created_time":1590797441983,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":525.0,
        "Poster_view_count":98.0,
        "Solution_body":"<p><code>json.dumps<\/code> will not convert your array to a dict structure and serialize it to a JSON String.<\/p>\n<p>What data type is <code>prediction<\/code> ? Have you tested making sure <code>prediction<\/code> is a dict?<\/p>\n<p>You can confirm the data type by adding <code>print(type(prediction))<\/code> to see the data type in the CloudWatch Logs.<\/p>\n<p>If prediction is a <code>list<\/code> you can test the following:<\/p>\n<pre><code>def output_fn(prediction, accept):\n    if accept == &quot;application\/json&quot;:\n\n        my_dict = {'output': prediction}\n        return json.dumps(my_dict), mimetype=accept)\n\n    raise RuntimeException(&quot;{} accept type is not supported by this script.&quot;.format(accept))\n<\/code><\/pre>\n<p><code>DataProcessing<\/code> and <code>JoinSource<\/code> are used to associate the data that is relevant to the prediction results in the output. It is not meant to be used to match the input and output format.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.3,
        "Solution_reading_time":11.87,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":115.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.8375316667,
        "Challenge_answer_count":1,
        "Challenge_body":"I want to use an Amazon Sagemaker endpoint for a custom classification model. The endpoint should only handle sporadic input (say a few times a week). \nFor this purpose I want to employ autoscaling that scales the number of instances down to 0 when the endpoint is not used. \n\nAre there any costs associated with having an endpoint with 0 instances? \n\nThanks!",
        "Challenge_closed_time":1666371029704,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666360814590,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user wants to use an Amazon SageMaker endpoint for a custom classification model that only handles sporadic input. They want to use autoscaling to scale the number of instances down to 0 when the endpoint is not used. The user is asking if there are any costs associated with having an endpoint with 0 instances.",
        "Challenge_last_edit_time":1668547705502,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU0VGYdZe8TRivmtGHoiDDHw\/cost-of-autoscaling-endpoint-amazon-sagemaker-endpoint-to-zero",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":5.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2.8375316667,
        "Challenge_title":"Cost of autoscaling endpoint Amazon SageMaker endpoint to zero",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":491.0,
        "Challenge_word_count":70,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You dont pay any compute costs for the duration when the endpoint size scales down to 0. But i think you can design it better. There are few other options for you to use in SageMaker Endpoint(assuming you are using realtime endpoint)\n\n1. Try using [SageMaker Serverless Inference](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints.html) instead. Its purely serverless in nature so you pay only when the endpoint is serving inference. i think that would fit your requirement better.\n2. You can think of using Lambda as well which will reduce your hosting costs. but you have to do more work in setting up the inference stack all by yourself.\n3. There is also an option of [SageMaker asynchronous inference](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/async-inference.html) but its mostly useful for inference which require longer time to process each request. The reason i mention this is it also support scale to 0 when no traffic is coming.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1666796875366,
        "Solution_link_count":2.0,
        "Solution_readability":8.4,
        "Solution_reading_time":12.05,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":144.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":54.3866655556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Because of a faulty score.py file in my InferenceConfig, a Model.Deploy failed to Azure Machine Learning, using ACI.  I wanted to create the endpoint in the cloud, but the only state I can see in the portal is Unhealthy.  My local script to deploy the model (using ) keeps running, until it times out. (using the <code>service.wait_for_deployment(show_output=True)<\/code>statement).<\/p>\n\n<p>Is there an option to get more insights in the actual reason\/error message of the deployment turning \"Unhealthy\"?<\/p>",
        "Challenge_closed_time":1584133364176,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583937572180,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an issue with a faulty score.py file in their InferenceConfig, causing a Model.Deploy failure to Azure Machine Learning using ACI. The endpoint created in the cloud shows an Unhealthy state, and the local script to deploy the model keeps running until it times out. The user is seeking options to get more insights into the actual reason or error message of the deployment turning Unhealthy.",
        "Challenge_last_edit_time":1584215688008,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60638587",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.4,
        "Challenge_reading_time":7.35,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":54.3866655556,
        "Challenge_title":"How to get insights in exceptions and logging of AzureML endpoint deployment",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":364.0,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Poster_created_time":1360655430743,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belgium",
        "Poster_reputation_count":2947.0,
        "Poster_view_count":355.0,
        "Solution_body":"<p>Usually the timeout is caused by an error in init() function in scoring script. You can get the detailed logs using <code>print(service.get_logs())<\/code> to find the Python error.<\/p>\n\n<p>For more comprehensive troubleshooting guide, see:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.3,
        "Solution_reading_time":6.32,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":37.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1553882107003,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":294.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":2777.7798802778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am deploying a model onto AWS via Sagemaker:<\/p>\n\n<p>I set up my JSON schema as follow:<\/p>\n\n<pre><code>import json\nschema = {\n    \"input\": [\n        {\n            \"name\": \"V1\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V2\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V3\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V4\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V5\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V6\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V7\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V8\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V9\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V10\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V11\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V12\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V13\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V14\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V15\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V16\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V17\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V18\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V19\",\n            \"type\": \"double\"\n        }, \n                {\n            \"name\": \"V20\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V21\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V22\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V23\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V24\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V25\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V26\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V27\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V28\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"Amount\",\n            \"type\": \"double\"\n        },         \n    ],\n    \"output\": \n        {\n            \"name\": \"features\",\n            \"type\": \"double\",\n            \"struct\": \"vector\"\n        }\n}\nschema_json = json.dumps(schema)\nprint(schema_json)\n<\/code><\/pre>\n\n<p>And deployed as:<\/p>\n\n<pre><code>from sagemaker.model import Model\nfrom sagemaker.pipeline import PipelineModel\nfrom sagemaker.sparkml.model import SparkMLModel\n\nsparkml_data = 's3:\/\/{}\/{}\/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')\n# passing the schema defined above by using an environment variable that sagemaker-sparkml-serving understands\nsparkml_model = SparkMLModel(model_data=sparkml_data, env={'SAGEMAKER_SPARKML_SCHEMA' : schema_json})\nxgb_model = Model(model_data=xgb_model.model_data, image=training_image)\n\nmodel_name = 'inference-pipeline-' + timestamp_prefix\nsm_model = PipelineModel(name=model_name, role=role, models=[sparkml_model, xgb_model])\n\n    endpoint_name = 'inference-pipeline-ep-' + timestamp_prefix\nsm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)\n<\/code><\/pre>\n\n<p>I got the error as below:<\/p>\n\n<p>ClientError: An error occurred (ValidationException) when calling the CreateModel operation: 1 validation error detected: Value '{SAGEMAKER_SPARKML_SCHEMA={\"input\": [{\"type\": \"double\", \"name\": \"V1\"}, {\"type\": \"double\", \"name\": \"V2\"}, {\"type\": \"double\", \"name\": \"V3\"}, {\"type\": \"double\", \"name\": \"V4\"}, {\"type\": \"double\", \"name\": \"V5\"}, {\"type\": \"double\", \"name\": \"V6\"}, {\"type\": \"double\", \"name\": \"V7\"}, {\"type\": \"double\", \"name\": \"V8\"}, {\"type\": \"double\", \"name\": \"V9\"}, {\"type\": \"double\", \"name\": \"V10\"}, {\"type\": \"double\", \"name\": \"V11\"}, {\"type\": \"double\", \"name\": \"V12\"}, {\"type\": \"double\", \"name\": \"V13\"}, {\"type\": \"double\", \"name\": \"V14\"}, {\"type\": \"double\", \"name\": \"V15\"}, {\"type\": \"double\", \"name\": \"V16\"}, {\"type\": \"double\", \"name\": \"V17\"}, {\"type\": \"double\", \"name\": \"V18\"}, {\"type\": \"double\", \"name\": \"V19\"}, {\"type\": \"double\", \"name\": \"V20\"}, {\"type\": \"double\", \"name\": \"V21\"}, {\"type\": \"double\", \"name\": \"V22\"}, {\"type\": \"double\", \"name\": \"V23\"}, {\"type\": \"double\", \"name\": \"V24\"}, {\"type\": \"double\", \"name\": \"V25\"}, {\"type\": \"double\", \"name\": \"V26\"}, {\"type\": \"double\", \"name\": \"V27\"}, {\"type\": \"double\", \"name\": \"V28\"}, {\"type\": \"double\", \"name\": \"Amount\"}], \"output\": {\"type\": \"double\", \"name\": \"features\", \"struct\": \"vector\"}}}' at 'containers.1**.<strong>member.environment' failed to satisfy constraint: Map value must satisfy constraint: [Member must have length less than or equal to 1024<\/strong>,** Member must have length greater than or equal to 0, Member must satisfy regular expression pattern: [\\S\\s]*]<\/p>\n\n<p>I try to reduce my features to 20 and it able to deploy. Just wondering how can I Pass the schema with 29 attributes?<\/p>",
        "Challenge_closed_time":1600448991412,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590448983843,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error while deploying a model onto AWS via Sagemaker. The error occurred due to the JSON schema exceeding the maximum length allowed by AWS. The user was able to deploy the model successfully after reducing the number of features to 20. The user is seeking a solution to pass the schema with 29 attributes.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62012264",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":17.0,
        "Challenge_reading_time":51.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":2777.7798802778,
        "Challenge_title":"AWS SageMaker SparkML Schema Eroor: member.environment' failed to satisfy constraint",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":226.0,
        "Challenge_word_count":418,
        "Platform":"Stack Overflow",
        "Poster_created_time":1359061977540,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"New York, NY, USA",
        "Poster_reputation_count":427.0,
        "Poster_view_count":62.0,
        "Solution_body":"<p>I do not think the environment length of 1024 limit will be increased in a short time. To work around this, you could try to rebuild the spark ml container with the <code>SAGEMAKER_SPARKML_SCHEMA<\/code> env var:<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-sparkml-serving-container\/blob\/master\/README.md#running-the-image-locally\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-sparkml-serving-container\/blob\/master\/README.md#running-the-image-locally<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.8,
        "Solution_reading_time":6.48,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":39.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.7565502778,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm trying to deploy my own GreengrassV2 components. It's a SageMaker ML model (optimized with SageMakerNeo and packaged as a Greengrass component) and the according inference app. I was trying to deploy it to my core device with SageMaker Edge Manager component. But it is always stuck in the status \"In progress\".\n\nMy logs show this error:\ncom.aws.greengrass.tes.CredentialRequestHandler: Error in retrieving AwsCredentials from TES. {iotCredentialsPath=\/role-aliases\/edgedevicerolealias\/credentials, credentialData=TES responded with status code: 403. Caching response. {\"message\":\"Access Denied\"}}\n\nBut how do I know which policies are missing?",
        "Challenge_closed_time":1682713832572,
        "Challenge_comment_count":0,
        "Challenge_created_time":1682711108991,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with deploying their own GreengrassV2 components, specifically a SageMaker ML model and inference app, to their core device using SageMaker Edge Manager component. The deployment is stuck in \"In progress\" status and the logs show an error related to missing policies, but the user is unsure which policies are missing.",
        "Challenge_last_edit_time":1683057468614,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUlDu9VAj4Qx-O7cbAXDz28w\/greengrass-own-component-deployment-stuck-in-progress",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":9.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.7565502778,
        "Challenge_title":"Greengrass own component deployment stuck \"in progress\"",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":65.0,
        "Challenge_word_count":90,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello, please refer to https:\/\/docs.aws.amazon.com\/greengrass\/v2\/developerguide\/troubleshooting.html#token-exchange-service-credentials-http-403 for troubleshooting,  you'll need `iot:AssumeRoleWithCertificate` permissions on your core device's AWS IoT role alias",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1682713832572,
        "Solution_link_count":1.0,
        "Solution_readability":21.2,
        "Solution_reading_time":3.58,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":2.5969138889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to follow <a href=\"https:\/\/azure.github.io\/azureml-sdk-for-r\/articles\/train-and-deploy-first-model.html\" rel=\"nofollow noreferrer\">this post<\/a> to deploy a &quot;model&quot; in Azure.<\/p>\n<p>A code snipet is as follows and the model, which is simply a function adding 2 numbers, seems to register fine. I don't even use the model to isolate the problem after 1000s of attempts as this scoring code shows:<\/p>\n<pre><code>library(jsonlite)\n\ninit &lt;- function()\n{\n  message(&quot;hello world&quot;)\n  \n  function(data)\n  {\n    vars &lt;- as.data.frame(fromJSON(data))\n    prediction &lt;- 2\n    toJSON(prediction)\n  }\n}\n<\/code><\/pre>\n<p>Should be fine shouldn't it? Any way I run this code snippet:<\/p>\n<pre><code>r_env &lt;- r_environment(name = &quot;basic_env&quot;)\ninference_config &lt;- inference_config(\n  entry_script = &quot;score.R&quot;,\n  source_directory = &quot;.&quot;,\n  environment = r_env)\n\naci_config &lt;- aci_webservice_deployment_config(cpu_cores = 1, memory_gb = 0.5)\n\naci_service &lt;- deploy_model(ws, \n                            'xxxxx', \n                            list(model), \n                            inference_config, \n                            aci_config)\n\nwait_for_deployment(aci_service, show_output = TRUE)\n<\/code><\/pre>\n<p>Which produces this (after a looooong time):<\/p>\n<pre><code>Running.....................................................................\nFailed\nService deployment polling reached non-successful terminal state, current service state: Failed\nOperation ID: 14c35064-7ff4-46aa-9bfa-ab8a63218a2c\nMore information can be found using '.get_logs()'\nError:\n{\n  &quot;code&quot;: &quot;AciDeploymentFailed&quot;,\n  &quot;statusCode&quot;: 400,\n  &quot;message&quot;: &quot;Aci Deployment failed with exception: Error in entry script, RuntimeError: Error in file(filename, \\&quot;r\\&quot;, encoding = encoding) : , please run print(service.get_logs()) to get details.&quot;,\n  &quot;details&quot;: [\n    {\n      &quot;code&quot;: &quot;CrashLoopBackOff&quot;,\n      &quot;message&quot;: &quot;Error in entry script, RuntimeError: Error in file(filename, \\&quot;r\\&quot;, encoding = encoding) : , please run print(service.get_logs()) to get details.&quot;\n    }\n  ]\n}\n<\/code><\/pre>\n<p>It does not tell me much. Not sure how to debug this further? How can I run this:<\/p>\n<pre><code>print(service.get_logs())\n<\/code><\/pre>\n<p>and where please? Guess this is a Python artifact? Any other input very much welcome.<\/p>\n<p>PS:<\/p>\n<p>At this point in time, I have my suspicion that the above R entry file definition is not what is expected these days. Looking at the Python equivalent taken from <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=azcli\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<pre><code>import json\n\ndef init():\n    print(&quot;This is init&quot;)\n\ndef run(data):\n    test = json.loads(data)\n    print(f&quot;received data {test}&quot;)\n    return f&quot;test is {test}&quot;\n<\/code><\/pre>\n<p>Would something like this not be more suitable (tried it without success).<\/p>\n<pre><code>library(jsonlite)\n\ninit &lt;- function()\n{\n    message(&quot;hello world&quot;)\n}\n\ninit &lt;- function()\n{\n    return(42)\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1621007625123,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620998276233,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy a model in Azure using Azure Machine Learning and AzureMLSDK in R. They have registered the model and written the scoring code, but the deployment fails with a non-successful terminal state error. The error message does not provide much information, and the user is unsure how to debug it further. They suspect that the R entry file definition may not be what is expected and wonder if a Python equivalent would be more suitable.",
        "Challenge_last_edit_time":1621008123540,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67535014",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":40.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":2.5969138889,
        "Challenge_title":"deploy model and expose model as web service via azure machine learning + azuremlsdk in R",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":282.0,
        "Challenge_word_count":319,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>Great to see people putting the R SDK through it's paces!<\/p>\n<p>The vignette you're using is obviously a great way to get started. It seems you're almost all the way through without a hitch.<\/p>\n<p>Deployment is always tricky, and I'm not expert myself. I'd point you to this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment-local?WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">guide on troubleshooting deployment locally<\/a>. Similar functionality exists for the R SDK, namely: <a href=\"https:\/\/azure.github.io\/azureml-sdk-for-r\/reference\/local_webservice_deployment_config.html\" rel=\"nofollow noreferrer\"><code>local_webservice_deployment_config()<\/code><\/a>.<\/p>\n<p>So I think you change your example to this:<\/p>\n<pre class=\"lang-r prettyprint-override\"><code>deployment_config &lt;- local_webservice_deployment_config(port = 8890)\n<\/code><\/pre>\n<p>Once you know the service is working locally, the issue you're having with the ACI webservice becomes a lot easier to narrow down.<\/p>",
        "Solution_comment_count":8.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.8,
        "Solution_reading_time":13.74,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":107.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1585590244876,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":55.0,
        "Answerer_view_count":12.0,
        "Challenge_adjusted_solved_time":0.0392458333,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>We created a webservice endpoint and tested it with the following code, and also with POSTMAN.<\/p>\n\n<p>We deployed the service to an AKS in the same resource group and subscription as the AML resource.<\/p>\n\n<p><strong>UPDATE: the attached AKS had a custom networking configuration and rejected external connections.<\/strong><\/p>\n\n<pre><code>import numpy\nimport os, json, datetime, sys\nfrom operator import attrgetter\nfrom azureml.core import Workspace\nfrom azureml.core.model import Model\nfrom azureml.core.image import Image\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.authentication import AzureCliAuthentication\n\ncli_auth = AzureCliAuthentication()\n# Get workspace\nws = Workspace.from_config(auth=cli_auth)\n\n# Get the AKS Details\ntry:\n    with open(\"..\/aml_config\/aks_webservice.json\") as f:\n        config = json.load(f)\nexcept:\n    print(\"No new model, thus no deployment on AKS\")\n    # raise Exception('No new model to register as production model perform better')\n    sys.exit(0)\n\nservice_name = config[\"aks_service_name\"]\n# Get the hosted web service\nservice = Webservice(workspace=ws, name=service_name)\n\n# Input for Model with all features\ninput_j = [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]]\nprint(input_j)\ntest_sample = json.dumps({\"data\": input_j})\ntest_sample = bytes(test_sample, encoding=\"utf8\")\ntry:\n    prediction = service.run(input_data=test_sample)\n    print(prediction)\nexcept Exception as e:\n    result = str(e)\n    print(result)\n    raise Exception(\"AKS service is not working as expected\")\n<\/code><\/pre>\n\n<p>In AML Studio, the deployment state is \"Healthy\".<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/RTB10.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RTB10.png\" alt=\"Endpoint attributes\"><\/a><\/p>\n\n<p>We get the following error when testing:<\/p>\n\n<pre><code>Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'\n<\/code><\/pre>\n\n<p><strong>Log just after deploying the AKS Webservice <a href=\"http:\/\/t.ly\/t79b\" rel=\"nofollow noreferrer\">here<\/a>.<\/strong><\/p>\n\n<p><strong>Log after running the test script <a href=\"http:\/\/t.ly\/79k5\" rel=\"nofollow noreferrer\">here<\/a>.<\/strong><\/p>\n\n<p>How can we know what is causing this problem and fix it?<\/p>",
        "Challenge_closed_time":1592590202852,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592508291480,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user created a webservice endpoint and tested it with code and POSTMAN. The service was deployed to an AKS in the same resource group and subscription as the AML resource. However, the user encountered a TimeoutError when testing the service and is seeking help to identify and fix the issue. The AKS had a custom networking configuration and rejected external connections.",
        "Challenge_last_edit_time":1592590061567,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62457880",
        "Challenge_link_count":4,
        "Challenge_participation_count":3,
        "Challenge_readability":12.7,
        "Challenge_reading_time":30.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":22.7531588889,
        "Challenge_title":"AML - Web service TimeoutError",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":332.0,
        "Challenge_word_count":275,
        "Platform":"Stack Overflow",
        "Poster_created_time":1585590244876,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>We checked the AKS networking configuration and realized it has an Azure CNI profile.<\/p>\n\n<p>In order to test the webservice we need to do it from inside the created virtual network.\nIt worked well!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":2.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1285219808283,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Perth WA, Australia",
        "Answerer_reputation_count":6770.0,
        "Answerer_view_count":1127.0,
        "Challenge_adjusted_solved_time":0.0213536111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm currently trying to integrate an ML model currently deployed as a webservice on AzureML with PowerBI.<\/p>\n<p>I see that it can be <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#invoking-the-azure-ml-model-in-power-bi\" rel=\"nofollow noreferrer\">integrated<\/a> but the model requires the addition of a schema file when it is <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#schema-discovery-for-machine-learning-models\" rel=\"nofollow noreferrer\">being deployed as a webservice<\/a>. Without this, the model can't be viewed in PowerBI.<\/p>\n<p>The problem that I have come up against is that I use MLflow to log ML model performances and subsequently to deploy a selected model onto AzureML as a webservice using MLflow's AzureML integration - mlflow.azureml.deploy(). This unfortunately doesn't have the option to define a schema file before the model is deployed, thus resulting in no model being available in PowerBI as it lacks the required schema file.<\/p>\n<p>My options seem to be:<\/p>\n<ol>\n<li>Find a workaround, possibly using the working <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/mlflow\/model-serving\" rel=\"nofollow noreferrer\">REST api of the model in a power query<\/a>.<\/li>\n<li>Rewrite the deployment code and handle the webservice deployment steps in Azure instead of MLflow.<\/li>\n<\/ol>\n<p>I thought I would ask to see if I am maybe missing something as I can't find a workaround using my current code to define a schema file in MLflow when deploying with mlflow.azureml.deploy().<\/p>",
        "Challenge_closed_time":1600604920243,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600261190477,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to integrate an ML model deployed as a webservice on AzureML with PowerBI, but the model requires a schema file to be viewed in PowerBI. The user uses MLflow to deploy the model onto AzureML, but MLflow's AzureML integration does not have the option to define a schema file before deployment, resulting in no model being available in PowerBI. The user is considering finding a workaround using the REST API or rewriting the deployment code to handle the webservice deployment steps in Azure instead of MLflow.",
        "Challenge_last_edit_time":1600855880503,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63920599",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":21.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":95.4804905556,
        "Challenge_title":"PowerBI and MLflow integration (through AzureML)",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":405.0,
        "Challenge_word_count":204,
        "Platform":"Stack Overflow",
        "Poster_created_time":1600260166047,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Point number 2 is the way we solved this issue. Instead of using MLflow to deploy to a scoring service on Azure, we wrote a custom code which load MLflow model when container is initialised.<\/p>\n<p>Scoring code is something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nfrom mlflow.pyfunc import load_model\n\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n\ndef init():\n    global model\n    model = load_model(os.path.join(os.environ.get(&quot;AZUREML_MODEL_DIR&quot;), &quot;awesome_model&quot;))\n\n@input_schema('data', NumpyParameterType(input_sample))\n@output_schema(NumpyParameterType(output_sample))\n\ndef run(data):\n    return model.predict(data)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1600855957376,
        "Solution_link_count":0.0,
        "Solution_readability":16.9,
        "Solution_reading_time":10.68,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":75.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1554298968016,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"wondeland",
        "Answerer_reputation_count":1540.0,
        "Answerer_view_count":118.0,
        "Challenge_adjusted_solved_time":554.3479247222,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>As I am logging my entire models and params into mlflow I thought it will be a good idea to have  it protected under a user name and password.<\/p>\n\n<p>I use the following code to run the mlflow server<\/p>\n\n<p><code>mlflow server --host 0.0.0.0 --port 11111<\/code>\nworks perfect,in mybrowser i type <code>myip:11111<\/code> and i see everything (which eventually is the problem)<\/p>\n\n<p>If I understood the documentation and the following <a href=\"https:\/\/groups.google.com\/forum\/#!topic\/mlflow-users\/E9QW4HdS8a8\" rel=\"noreferrer\">https:\/\/groups.google.com\/forum\/#!topic\/mlflow-users\/E9QW4HdS8a8<\/a> link here correct, I should use nginx to create the authentication.<\/p>\n\n<p>I installed <code>nginx open sourcre<\/code>  and <code>apache2-utils<\/code><\/p>\n\n<p>created <code>sudo htpasswd -c \/etc\/apache2\/.htpasswd user1<\/code> user and passwords.<\/p>\n\n<p>I edited my <code>\/etc\/nginx\/nginx.conf<\/code> to the following:<\/p>\n\n<pre><code>server {\n        listen 80;\n        listen 443 ssl;\n\n        server_name my_ip;\n        root NOT_SURE_WHICH_PATH_TO_PUT_HERE, THE VENV?;\n        location \/ {\n            proxy_pass                      my_ip:11111\/;\n            auth_basic                      \"Restricted Content\";\n            auth_basic_user_file \/home\/path to the password file\/.htpasswd;\n        }\n    }\n<\/code><\/pre>\n\n<p><strong>but no authentication appears.<\/strong><\/p>\n\n<p>if I change the conf to listen to  <code>listen 11111<\/code>\nI get an error that the port is already in use ( of course, by the mlflow server....)<\/p>\n\n<p>my wish is to have a authentication window before anyone can enter by the mlflow with a browser.<\/p>\n\n<p>would be happy to hear any suggestions.<\/p>",
        "Challenge_closed_time":1576255052616,
        "Challenge_comment_count":1,
        "Challenge_created_time":1574259400087,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary_original":"The user is trying to add authentication to their mlFlow server to protect their models and parameters with a username and password. They have tried using nginx and apache2-utils to create authentication, but it is not working. They are seeking suggestions to add an authentication window before anyone can enter the mlFlow server with a browser.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58956459",
        "Challenge_link_count":2,
        "Challenge_participation_count":5,
        "Challenge_readability":9.9,
        "Challenge_reading_time":20.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":11.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":554.3479247222,
        "Challenge_title":"How to run authentication on a mlFlow server?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":13870.0,
        "Challenge_word_count":193,
        "Platform":"Stack Overflow",
        "Poster_created_time":1554298968016,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"wondeland",
        "Poster_reputation_count":1540.0,
        "Poster_view_count":118.0,
        "Solution_body":"<p>the problem here is that both <code>mlflow<\/code> and <code>nginx<\/code> are trying to run on the <strong>same port<\/strong>... <\/p>\n\n<ol>\n<li><p>first lets deal with nginx:<\/p>\n\n<p>1.1 in \/etc\/nginx\/sites-enable make a new file <code>sudo nano mlflow<\/code> and delete the exist default.<\/p>\n\n<p>1.2 in mlflow file:<\/p><\/li>\n<\/ol>\n\n<pre><code>server {\n    listen YOUR_PORT;\n    server_name YOUR_IP_OR_DOMAIN;\n    auth_basic           \u201cAdministrator\u2019s Area\u201d;\n    auth_basic_user_file \/etc\/apache2\/.htpasswd; #read the link below how to set username and pwd in nginx\n\n    location \/ {\n        proxy_pass http:\/\/localhost:8000;\n        include \/etc\/nginx\/proxy_params;\n        proxy_redirect off;\n    }\n}\n<\/code><\/pre>\n\n<p>1.3.  restart nginx <code>sudo systemctl restart nginx<\/code><\/p>\n\n<ol start=\"2\">\n<li>on your server run mlflow  <code>mlflow server --host localhost --port 8000<\/code><\/li>\n<\/ol>\n\n<p>Now if you try access the YOUR_IP_OR_DOMAIN:YOUR_PORT within your browser an auth popup should appear, enter your host and pass and now you in mlflow<\/p>\n\n<ol start=\"3\">\n<li><p>now there are 2 options to tell the mlflow server about it:<\/p>\n\n<p>3.1 set username and pwd as environment variable \n<code>export MLFLOW_TRACKING_USERNAME=user export MLFLOW_TRACKING_PASSWORD=pwd<\/code><\/p>\n\n<p>3.2 edit in your <code>\/venv\/lib\/python3.6\/site-packages\/mlflowpackages\/mlflow\/tracking\/_tracking_service\/utils.py<\/code> the function <\/p><\/li>\n<\/ol>\n\n<pre><code>def _get_rest_store(store_uri, **_):\n    def get_default_host_creds():\n        return rest_utils.MlflowHostCreds(\n            host=store_uri,\n            username=replace with nginx user\n            password=replace with nginx pwd\n            token=os.environ.get(_TRACKING_TOKEN_ENV_VAR),\n            ignore_tls_verification=os.environ.get(_TRACKING_INSECURE_TLS_ENV_VAR) == 'true',\n        )\n<\/code><\/pre>\n\n<p>in your .py file where you work with mlflow:<\/p>\n\n<pre><code>import mlflow\nremote_server_uri = \"YOUR_IP_OR_DOMAIN:YOUR_PORT\" # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\nmlflow.set_experiment(\"\/my-experiment\")\nwith mlflow.start_run():\n    mlflow.log_param(\"a\", 1)\n    mlflow.log_metric(\"b\", 2)\n<\/code><\/pre>\n\n<p>A link to nginx authentication doc <a href=\"https:\/\/docs.nginx.com\/nginx\/admin-guide\/security-controls\/configuring-http-basic-authentication\/\" rel=\"noreferrer\">https:\/\/docs.nginx.com\/nginx\/admin-guide\/security-controls\/configuring-http-basic-authentication\/<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":14.7,
        "Solution_reading_time":30.69,
        "Solution_score_count":8.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":211.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1550779047856,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":363.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":4538.5667388889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am getting the below error.\nDoes anyone have any idea how to solve it?<\/p>\n<pre><code>Failed to create pipeline job. Error: Vertex AI Service Agent \n'XXXXX@gcp-sa-aiplatform-cc.iam.gserviceaccount.com' should be granted\n access to the image gcr.io\/gcp-project-id\/application:latest\n<\/code><\/pre>",
        "Challenge_closed_time":1659424830192,
        "Challenge_comment_count":3,
        "Challenge_created_time":1643025892210,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to create a pipeline job in GCP Vertex AI Service Agent. The error message indicates that the service agent needs to be granted access to the GCR image 'gcr.io\/gcp-project-id\/application:latest'.",
        "Challenge_last_edit_time":1643085989932,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70833594",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":8.5,
        "Challenge_reading_time":4.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":4555.2605505556,
        "Challenge_title":"GCP Vertex AI Service Agent access to GCR image Error",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":707.0,
        "Challenge_word_count":44,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501131989640,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":329.0,
        "Poster_view_count":88.0,
        "Solution_body":"<p><code>{PROJECT_NUMBER}@gcp-sa-aiplatform-cc.iam.gserviceaccount.com<\/code> is google's <a href=\"https:\/\/cloud.google.com\/iam\/docs\/service-agents\" rel=\"nofollow noreferrer\">AI Platform service agent<\/a>.\nThis Service agent requires access to read\/pull the docker image from your project's gcr to create container for pipeline run.<\/p>\n<p>If You have permission to edit <a href=\"https:\/\/cloud.google.com\/iam\/docs\/understanding-roles\" rel=\"nofollow noreferrer\">IAM roles<\/a>, You can try adding <a href=\"https:\/\/cloud.google.com\/artifact-registry\/docs\/access-control#roles\" rel=\"nofollow noreferrer\">Artifact Registry roles<\/a> to the above service agent.\nYou can start with adding <code>roles\/artifactregistry.reader<\/code>.<\/p>\n<p>Hope this helps :)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":14.8,
        "Solution_reading_time":10.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":66.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1263294862568,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":183045.0,
        "Answerer_view_count":13691.0,
        "Challenge_adjusted_solved_time":0.3125277778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to invoke a SageMaker enpoint from AWS Lambda using a lambda function.<\/p>\n<p>This is a sample API call to the endpoint from SageMaker Studio, working as expected:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3iTPN.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3iTPN.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>here's my Lambda function (<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">inspired from documentation<\/a>):<\/p>\n<pre><code>import os\nimport io\nimport boto3\nimport json\n\n\nENDPOINT_NAME = 'iris-autoscale-6'\nruntime= boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    # print(&quot;Received event: &quot; + json.dumps(event, indent=2))\n    payload = json.loads(json.dumps(event))\n    print(payload)\n    \n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\n    print(response)\n    result = json.loads(response['Body'].read().decode())\n    print(result)\n    \n    return result\n<\/code><\/pre>\n<p>My error message:<\/p>\n<pre><code>Test Event Name\nProperTest\n\nResponse\n{\n  &quot;errorMessage&quot;: &quot;Parameter validation failed:\\nInvalid type for parameter Body, value: {'sepal_length': [5.1, 4.9, 4.7, 4.6, 5], 'sepal_width': [3.5, 3, 3.2, 3.1, 3.6], 'petal_length': [1.4, 1.4, 1.3, 1.5, 1.4], 'petal_width': [0.2, 0.2, 0.2, 0.2, 0.2]}, type: &lt;class 'dict'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object&quot;,\n  &quot;errorType&quot;: &quot;ParamValidationError&quot;,\n  &quot;stackTrace&quot;: [\n    &quot;  File \\&quot;\/var\/task\/lambda_function.py\\&quot;, line 17, in lambda_handler\\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/client.py\\&quot;, line 386, in _api_call\\n    return self._make_api_call(operation_name, kwargs)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/client.py\\&quot;, line 678, in _make_api_call\\n    api_params, operation_model, context=request_context)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/client.py\\&quot;, line 726, in _convert_to_request_dict\\n    api_params, operation_model)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/validate.py\\&quot;, line 319, in serialize_to_request\\n    raise ParamValidationError(report=report.generate_report())\\n&quot;\n  ]\n}\n\nFunction Logs\nSTART RequestId: 70278b9f-f75e-4ac9-a827-7ad35d162512 Version: $LATEST\n{'sepal_length': [5.1, 4.9, 4.7, 4.6, 5], 'sepal_width': [3.5, 3, 3.2, 3.1, 3.6], 'petal_length': [1.4, 1.4, 1.3, 1.5, 1.4], 'petal_width': [0.2, 0.2, 0.2, 0.2, 0.2]}\n[ERROR] ParamValidationError: Parameter validation failed:\nInvalid type for parameter Body, value: {'sepal_length': [5.1, 4.9, 4.7, 4.6, 5], 'sepal_width': [3.5, 3, 3.2, 3.1, 3.6], 'petal_length': [1.4, 1.4, 1.3, 1.5, 1.4], 'petal_width': [0.2, 0.2, 0.2, 0.2, 0.2]}, type: &lt;class 'dict'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object\nTraceback (most recent call last):\n\u00a0\u00a0File &quot;\/var\/task\/lambda_function.py&quot;, line 17, in lambda_handler\n\u00a0\u00a0\u00a0\u00a0response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 386, in _api_call\n\u00a0\u00a0\u00a0\u00a0return self._make_api_call(operation_name, kwargs)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 678, in _make_api_call\n\u00a0\u00a0\u00a0\u00a0api_params, operation_model, context=request_context)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 726, in _convert_to_request_dict\n\u00a0\u00a0\u00a0\u00a0api_params, operation_model)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/validate.py&quot;, line 319, in serialize_to_request\n\u00a0\u00a0\u00a0\u00a0raise ParamValidationError(report=report.generate_report())\nEND RequestId: 70278b9f-f75e-4ac9-a827-7ad35d162512\nREPORT RequestId: 70278b9f-f75e-4ac9-a827-7ad35d162512  Duration: 26.70 ms  Billed Duration: 27 ms  Memory Size: 128 MB Max Memory Used: 76 MB  Init Duration: 343.10 ms\n<\/code><\/pre>\n<p>Here's the policy attached to the lambda function:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;VisualEditor0&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:sagemaker:ap-south-1:&lt;my-account-id&gt;:endpoint\/iris-autoscale-6&quot;\n        }\n    ]\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1629024434667,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629022235437,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to invoke a SageMaker endpoint from AWS Lambda using a lambda function. The error message indicates that the parameter validation failed due to an invalid type for the parameter Body. The user has provided the lambda function code, error message, and policy attached to the lambda function.",
        "Challenge_last_edit_time":1629023309567,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68790568",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":16.0,
        "Challenge_reading_time":60.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":44,
        "Challenge_solved_time":0.6108972222,
        "Challenge_title":"\"errorMessage\": \"Parameter validation failed in Lambda calling SageMaker endpoint",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":499.0,
        "Challenge_word_count":370,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559910246180,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":2046.0,
        "Poster_view_count":369.0,
        "Solution_body":"<p>The issue is that your <code>payload<\/code> has invalid format. It should be one of:<\/p>\n<pre><code>&lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object\n<\/code><\/pre>\n<p>The following should address the error (note: you may have many other issues in your code):<\/p>\n<pre><code>    payload = json.dumps(event)\n    print(payload)\n    \n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload.encode())\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.7,
        "Solution_reading_time":6.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":46.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1254957460063,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"North Carolina, USA",
        "Answerer_reputation_count":2484.0,
        "Answerer_view_count":362.0,
        "Challenge_adjusted_solved_time":2.6018694445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I used Machine learning tutorial: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/create-experiment\" rel=\"nofollow noreferrer\">Create your first data science experiment in Azure Machine Learning Studio<\/a> to create an <code>Experiment<\/code> and then converted it to a <code>predictive experiment<\/code>. Now I'm trying to deploy it as a Web Service by following this article that was referenced in the above article: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/publish-a-machine-learning-web-service#deploy-it-as-a-web-service\" rel=\"nofollow noreferrer\">Deploy it as a web service<\/a>. But when I click on <code>Run<\/code> and then on <code>Deploy Web Service<\/code>, I don't see the <code>Price Plan<\/code> dropdown and <code>Plan Name<\/code> input box etc as mentioned in the section <code>Machine Learning Web Service portal Deploy Experiment Page<\/code> of the second article above. After I clicked on Deploy Web Service link in ML studio, I got the page shown below.<strong>Question<\/strong>: What I may be doing wrong?<\/p>\n\n<p>Note: You can click on the picture to get a larger view.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/G3TKo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/G3TKo.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":1526322971967,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526313605237,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy an Azure ML experiment as a web service by following a tutorial, but is unable to see the Price Plan dropdown and Plan Name input box as mentioned in the tutorial. The user is seeking help to understand what they may be doing wrong.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50334563",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":12.1,
        "Challenge_reading_time":18.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":2.6018694445,
        "Challenge_title":"Deployment of an Azure ML Experiment as a Web Service through Azure Machine Learning Studio",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":330.0,
        "Challenge_word_count":162,
        "Platform":"Stack Overflow",
        "Poster_created_time":1330144099340,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":19815.0,
        "Poster_view_count":2272.0,
        "Solution_body":"<p>I think it depends on what workspace you're in. If you're in the free one then you get the screen that you already get, but if you create a workspace in the Azure portal and use that one, then you will get a screen like below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/drRpa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/drRpa.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>To create a new workspace, in the Azure Portal, create a new \"Machine Learning Studio Workspace\" and when you go to Azure ML Studio select the new workspace from the top right.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.7,
        "Solution_reading_time":7.3,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":87.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1320061998252,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":778.0,
        "Answerer_view_count":89.0,
        "Challenge_adjusted_solved_time":135.5520369445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>same input is used in two cases, but different result is returned from python module<\/p>\n\n<p>here is the python script that return the result to the webservice:<\/p>\n\n<pre><code>import pandas as pd\nimport sys\n\n\n  def get_segments(dataframe):\n     dataframe['segment']=dataframe['segment'].astype('str')\n     segments = dataframe.loc[~dataframe['segment'].duplicated()]['segment']\n     return segments\n\n\n  def azureml_main(dataframe1 = None, dataframe2 = None):\n\n   df = dataframe1\n   segments = get_segments(df)\n   segmentCount =segments.size\n\n   if (segmentCount &gt; 0) :\n      res = pd.DataFrame(columns=['segmentId','recommendation'],index=[range(segmentCount)])\n    i=0    \n    for seg in segments:\n        d= df.query('segment ==[\"{}\"]'.format(seg)).sort(['count'],ascending=[0])\n\n        res['segmentId'][i]=seg\n        recommendation='['\n        for index, x in d.iterrows():\n            item=str(x['ItemId'])\n            recommendation = recommendation + item + ','\n        recommendation = recommendation[:-1] + ']'\n        res['recommendation'][i]= recommendation\n        i=i+1\n   else:\n\n      res = pd.DataFrame(columns=[seg,pdver],index=[range(segmentCount)])\n\nreturn res,\n<\/code><\/pre>\n\n<p>when in experiment it returnd the actual itemIds, when in webservice it returns some numbers<\/p>\n\n<p>the purpose of this code is to pivot some table by segment column for recommendation<\/p>",
        "Challenge_closed_time":1468139423743,
        "Challenge_comment_count":2,
        "Challenge_created_time":1467651436410,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing an issue where the same input is used in two cases, but different results are returned from the Python module. The Python script is used to pivot some table by segment column for recommendation. In the experiment, it returns the actual itemIds, but in the webservice, it returns some numbers.",
        "Challenge_last_edit_time":1468140771380,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38189399",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":14.6,
        "Challenge_reading_time":17.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":135.5520369445,
        "Challenge_title":"azure ml experiment return different results than webservice",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":264.0,
        "Challenge_word_count":124,
        "Platform":"Stack Overflow",
        "Poster_created_time":1320061998252,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":778.0,
        "Poster_view_count":89.0,
        "Solution_body":"<p>After discussion with the product team from Microsoft. the issue was resolved.\nthe product team rolled out an update to the web service first, and only later to the ML-Studio, which fixed an issue with categorical attributes in \"Execute python script\".\nthe issue was in a earlier stage of the flow and has nothing to do with the python code above.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.9,
        "Solution_reading_time":4.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":61.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5810352778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>hello azure, is there anywhere azure will post the plan of the supportability and the current status? I went through Open AI document but I found nothing helpful. How should we start to use it in Azure? Where we can get more details? What\u2019s the pricing? Can you provide some details to these questions?<\/p>",
        "Challenge_closed_time":1677360042060,
        "Challenge_comment_count":0,
        "Challenge_created_time":1677357950333,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information about the supportability plan and current status of Open AI in Azure. They are also looking for details on how to use it, where to get more information, and pricing. They have not found helpful information in the Open AI document.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1184290\/open-ai-support-supportability",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.3,
        "Challenge_reading_time":4.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.5810352778,
        "Challenge_title":"Open AI support supportability",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":57,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=4d395bc2-5cc4-4ca4-a2c9-4cb20730b092\">Starfall<\/a> <\/p>\n<p>Thanks for reaching out to us here, you need to check on Azure Open AI models document for the information you are looking for - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/openai\/concepts\/models#gpt-3-models-1\">https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/openai\/concepts\/models#gpt-3-models-1<\/a><\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/89541da1-ce9d-443d-812c-16db4a698845?platform=QnA\" alt=\"User's image\" \/><\/p>\n<p>Pricing - <a href=\"https:\/\/azure.microsoft.com\/en-us\/pricing\/details\/cognitive-services\/openai-service\/\">https:\/\/azure.microsoft.com\/en-us\/pricing\/details\/cognitive-services\/openai-service\/<\/a><\/p>\n<p>For quick start, please refer to this document - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/openai\/quickstart?pivots=programming-language-studio\">https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/openai\/quickstart?pivots=programming-language-studio<\/a> <\/p>\n<p>Prerequisites<\/p>\n<ul>\n<li> An Azure subscription - <a href=\"https:\/\/azure.microsoft.com\/free\/cognitive-services\">Create one for free<\/a>.<\/li>\n<li> Access granted to Azure OpenAI in the desired Azure subscription.\n   Currently, access to this service is granted only by application. You can apply for access to Azure OpenAI by completing the form at <a href=\"https:\/\/aka.ms\/oai\/access\">https:\/\/aka.ms\/oai\/access<\/a>. Open an issue on this repo to contact us if you have an issue.<\/li>\n<li> An Azure OpenAI resource with a model deployed. For more information about model deployment, see the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/openai\/how-to\/create-resource\">resource deployment guide<\/a>.<\/li>\n<\/ul>\n<p>I hope you can start smoothly, please let me know if you need more help.<\/p>\n<p>Regards,<\/p>\n<p>Yutong <\/p>\n<p>-Please kindly accept the answer and vote 'Yes' if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":8.0,
        "Solution_readability":15.7,
        "Solution_reading_time":27.85,
        "Solution_score_count":1.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":169.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1619174589310,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bangalore, Karnataka, India",
        "Answerer_reputation_count":1200.0,
        "Answerer_view_count":804.0,
        "Challenge_adjusted_solved_time":254.5180547222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have a Vertex AI model deployed on an endpoint and want to do some prediction from my app in Golang.<\/p>\n<p>To do this I create code inspired by this example : <a href=\"https:\/\/cloud.google.com\/go\/docs\/reference\/cloud.google.com\/go\/aiplatform\/latest\/apiv1?hl=en\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/go\/docs\/reference\/cloud.google.com\/go\/aiplatform\/latest\/apiv1?hl=en<\/a><\/p>\n<pre><code>const file = &quot;MY_BASE64_IMAGE&quot;\n\nfunc main() {\n\n    ctx := context.Background()\n\n    c, err := aiplatform.NewPredictionClient(cox)\n    if err != nil {\n        log.Printf(&quot;QueryVertex NewPredictionClient - Err:%s&quot;, err)\n    }\n    defer c.Close()\n\n    parameters, err := structpb.NewValue(map[string]interface{}{\n        &quot;confidenceThreshold&quot;: 0.2,\n        &quot;maxPredictions&quot;:      5,\n    })\n    if err != nil {\n        log.Printf(&quot;QueryVertex structpb.NewValue parameters - Err:%s&quot;, err)\n    }\n\n    instance, err := structpb.NewValue(map[string]interface{}{\n        &quot;content&quot;: file,\n    })\n    if err != nil {\n        log.Printf(&quot;QueryVertex structpb.NewValue instance - Err:%s&quot;, err)\n    }\n\n    reqP := &amp;aiplatformpb.PredictRequest{\n        Endpoint:   &quot;projects\/PROJECT_ID\/locations\/LOCATION_ID\/endpoints\/ENDPOINT_ID&quot;,\n        Instances:  []*structpb.Value{instance},\n        Parameters: parameters,\n    }\n\n    resp, err := c.Predict(cox, reqP)\n    if err != nil {\n        log.Printf(&quot;QueryVertex Predict - Err:%s&quot;, err)\n    }\n\n    log.Printf(&quot;QueryVertex Res:%+v&quot;, resp)\n}\n<\/code><\/pre>\n<p>I put the path to my service account JSON file on GOOGLE_APPLICATION_CREDENTIALS environment variable.\nBut when I run my test app I obtain this error message:<\/p>\n<pre><code>QueryVertex Predict - Err:rpc error: code = Unimplemented desc = unexpected HTTP status code received from server: 404 (Not Found); transport: received unexpected content-type &quot;text\/html; charset=UTF-8&quot;\nQueryVertex Res:&lt;nil&gt;\n<\/code><\/pre>",
        "Challenge_closed_time":1652647881720,
        "Challenge_comment_count":3,
        "Challenge_created_time":1651731616723,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to use Golang to make predictions from a deployed Vertex AI model on an endpoint. They have created code based on a Google Cloud example, but when they run their test app, they receive an error message stating \"unexpected HTTP status code received from server: 404 (Not Found).\" The user has set the path to their service account JSON file on the GOOGLE_APPLICATION_CREDENTIALS environment variable.",
        "Challenge_last_edit_time":1654669807780,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72122744",
        "Challenge_link_count":2,
        "Challenge_participation_count":6,
        "Challenge_readability":15.0,
        "Challenge_reading_time":26.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":254.5180547222,
        "Challenge_title":"Google Cloud Vertex AI with Golang: rpc error: code = Unimplemented desc = unexpected HTTP status code received from server: 404 (Not Found)",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":455.0,
        "Challenge_word_count":177,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651730962056,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>As @DazWilkin suggested, configure the client option to specify the specific <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest#service-endpoint\" rel=\"noreferrer\">regional endpoint<\/a> with a port 443:<\/p>\n<pre><code>option.WithEndpoint(&quot;&lt;region&gt;-aiplatform.googleapis.com:443&quot;)\n<\/code><\/pre>\n<p>Try like below:<\/p>\n<pre><code>func main() {\n \n   ctx := context.Background()\n   c, err := aiplatform.NewPredictionClient(\n       ctx,\n       option.WithEndpoint(&quot;&lt;region&gt;-aiplatform.googleapis.com:443&quot;),\n   )\n   if err != nil {\n       log.Printf(&quot;QueryVertex NewPredictionClient - Err:%s&quot;, err)\n   }\n   defer c.Close()\n       .\n       .\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1652712010567,
        "Solution_link_count":1.0,
        "Solution_readability":18.6,
        "Solution_reading_time":8.78,
        "Solution_score_count":5.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":43.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1341655118270,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Egypt",
        "Answerer_reputation_count":505.0,
        "Answerer_view_count":53.0,
        "Challenge_adjusted_solved_time":19.6138405556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a deployed model on sagemaker with two production variants. I was wondering if you get charged for both variants even if I set all the traffic to just go through one of them.<\/p>\n<p>The docs on pricing are found below but I couldn't seem to find the answer to this.<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/sagemaker\/pricing\/<\/a><\/p>",
        "Challenge_closed_time":1622110871903,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622040262077,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has a deployed model on Sagemaker with two production variants and is unsure if they will be charged for both variants even if all the traffic is directed to just one of them. They have checked the pricing documentation but could not find a clear answer.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67707288",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.9,
        "Challenge_reading_time":6.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":19.6138405556,
        "Challenge_title":"Do you get charged for production variants on sagemaker that have no traffic going through them?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":40.0,
        "Challenge_word_count":71,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521737124656,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Milton Keynes",
        "Poster_reputation_count":738.0,
        "Poster_view_count":69.0,
        "Solution_body":"<p>You will be charged as long as the model is running on an instance regardless of whether the traffic is going through it or not as still the model is running on an instance.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.2,
        "Solution_reading_time":2.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":34.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":150.3343644444,
        "Challenge_answer_count":11,
        "Challenge_body":"<p>I can't use ml real-time inference endpoint becouse it's stuck on transitioning status (more than 20 hours). Could you help me with that?<\/p>",
        "Challenge_closed_time":1600780351532,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600239147820,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with Azure ML real-time inference endpoint deployment as it is stuck on transitioning status for more than 20 hours. They are seeking assistance to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/96645\/azure-ml-real-time-inference-endpoint-deloyment-st",
        "Challenge_link_count":0,
        "Challenge_participation_count":11,
        "Challenge_readability":8.6,
        "Challenge_reading_time":2.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":150.3343644444,
        "Challenge_title":"Azure ML real-time inference endpoint deloyment stuck on transitioning status",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":32,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I've chacked in on some different algorithms and the issue appears when i'm using n-grams block for getting features. When i'm using feature hashing for example it looks like working well.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":2.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":31.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1342685175156,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":12103.0,
        "Answerer_view_count":1451.0,
        "Challenge_adjusted_solved_time":1.8368638889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When we deploy a model as an ACIWebService in Azure Machine Learning Service, we do not need to specify any <code>deployment_target<\/code>.<\/p>\n<p>According to the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#deploy-workspace--name--models--inference-config-none--deployment-config-none--deployment-target-none--overwrite-false-\" rel=\"nofollow noreferrer\">AzureML documentation<\/a> for <code>azureml.core.model.model<\/code> class,<\/p>\n<pre><code>deployment_target\nComputeTarget\ndefault value: None\nA ComputeTarget to deploy the Webservice to. As Azure Container Instances has no associated ComputeTarget, leave this parameter as None to deploy to Azure Container Instances.\n<\/code><\/pre>\n<p>What does Microsoft mean by<\/p>\n<blockquote>\n<p>As Azure Container Instances has no associated ComputeTarget<\/p>\n<\/blockquote>\n<p>In which &quot;Compute Target&quot; is an ACIWebService deployed?<\/p>",
        "Challenge_closed_time":1610958655960,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610952043250,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is confused about where the Azure Machine ACI Webservice deploys and what is meant by \"Compute Target\" in the context of deploying an ACIWebService. The AzureML documentation states that as Azure Container Instances has no associated ComputeTarget, the deployment_target parameter should be left as None to deploy to Azure Container Instances.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65769868",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.0,
        "Challenge_reading_time":13.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.8368638889,
        "Challenge_title":"Where does the Azure Machine ACI Webservice deploy?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":317.0,
        "Challenge_word_count":94,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/container-instances\/container-instances-overview\" rel=\"nofollow noreferrer\">Azure Container Instances<\/a> itself is the compute platform. It spins up a container in a serverless-fashion.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.6,
        "Solution_reading_time":3.28,
        "Solution_score_count":3.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":2.5398080556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to read a json file from S3 into a sagemaker notebook.<\/p>\n\n<p>I can do this with pandas with this code, and this works without error :<\/p>\n\n<pre><code>import json\nimport pandas as pd\nimport boto3\n\n\nprefix_source = 'folder'\n\ns3 = boto3.resource('s3')\nmy_bucket_source = s3.Bucket('bucket_source')\n\nfor obj in my_bucket_source.objects.filter(Prefix=prefix_source):\n        data_location = 's3:\/\/{}\/{}'.format(obj.bucket_name, obj.key)\n        data = pd.read_json(data_location, lines = True )\n        display(data.head())\n<\/code><\/pre>\n\n<p>but I don't want to use pandas, I want to use Python <\/p>\n\n<p>I tried this code<\/p>\n\n<pre><code>for obj in my_bucket_source.objects.filter(Prefix=prefix_source):\n        data_location = 's3:\/\/{}\/{}'.format(obj.bucket_name, obj.key)\n        with open(data_location, 'r') as f:\n            array = json.load(f)\n            display(array) \n<\/code><\/pre>\n\n<p>I got this error :<\/p>\n\n<p><strong>IOError: [Errno 2] No such file or directory<\/strong><\/p>",
        "Challenge_closed_time":1555526691136,
        "Challenge_comment_count":1,
        "Challenge_created_time":1555517547827,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to read a JSON file from S3 into a Sagemaker notebook using Python. They were able to do it using pandas, but they want to use Python instead. However, when they tried to read the file using Python, they encountered an error \"IOError: [Errno 2] No such file or directory\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55731954",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.4,
        "Challenge_reading_time":12.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":2.5398080556,
        "Challenge_title":"read json file with Python from S3 into sagemaker notebook",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":7094.0,
        "Challenge_word_count":110,
        "Platform":"Stack Overflow",
        "Poster_created_time":1445771281667,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":737.0,
        "Poster_view_count":63.0,
        "Solution_body":"<p>Json.load() expect a local file system path \"\/...\", not an \"s3:\/\/\" URI.<br>\nSee answer here: <a href=\"https:\/\/stackoverflow.com\/a\/47121263\">https:\/\/stackoverflow.com\/a\/47121263<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.9,
        "Solution_reading_time":2.51,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":16.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1965.6686111111,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nTrainer.fit fails with a pickle error when the logger is MLFlowLogger, and distributed_backend='ddp' on GPUs but without SLURM.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Instantiate MLFlowLogger in Pytorch 0.5.3.2 with Pytorch 1.3.1 and MLFlow 1.4.0. The execution environment has environment variables MLFLOW_TRACKING_URI, and also MLFLOW_TRACKING_USERNAME and MLFLOW_TRACKING_PASSWORD to connect to the MLflow tracking server with HTTP Basic Authentication. The MLflow tracking server is also v1.4.0.\r\n2. Instantiate Trainer with MLFlowLogger instance as logger, distributed_backend='ddp' and with the gpus parameter on a machine with NVIDIA GPUs but without SLURM.\r\n3. Run Trainer.fit\r\n\r\nFrom the error output, it looks like multiprocessing is attempting to pickle the nested function in MLflow function [_get_rest_store](https:\/\/github.com\/mlflow\/mlflow\/blob\/v1.4.0\/mlflow\/tracking\/_tracking_service\/utils.py#L81):\r\n```\r\nayla.khan@gpu12:~\/photosynthetic$ python test_mlflow.py\r\nTraceback (most recent call last):\r\n  File \"test_mlflow.py\", line 71, in <module>\r\n    trainer.fit(model)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 343, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_gpus, args=(model,))\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 162, in spawn\r\n    process.start()\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object '_get_rest_store.<locals>.get_default_host_creds'\r\n```\r\n\r\n#### Code sample\r\nSample code tested with a very simple test model ([gist](https:\/\/gist.github.com\/a-y-khan\/8693d2b186227561a4baf4d03ce75c34)):\r\n\r\n```\r\ntest_hparams = Namespace()\r\nmodel = XORGateModel(test_hparams)\r\n\r\nlogger = MLFlowLogger(experiment_name='test_lightning_logger',\r\n                                          tracking_uri=os.environ['MLFLOW_TRACKING_URI'])\r\ntrainer = pl.Trainer(logger=logger, distributed_backend='ddp', gpus='-1')\r\ntrainer.fit(model)\r\n```\r\n\r\n### Expected behavior\r\n\r\nTrainer.fit runs without error.\r\n\r\n### Environment\r\n\r\n```\r\n(photosynthetic) ayla.khan@gpu12:~\/photosynthetic$ python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 1.3.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 440.33.01\r\ncuDNN version: \/usr\/local\/cuda-10.0\/lib64\/libcudnn.so.7\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] pytorch-lightning==0.5.3.2\r\n[pip] pytorch-toolbelt==0.2.1\r\n[pip] torch==1.3.1\r\n[pip] torchsummary==1.5.1\r\n[pip] torchvision==0.4.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.3.0            py36he904b0f_0\r\n[conda] mkl_fft                   1.0.15           py36ha843d7b_0\r\n[conda] mkl_random                1.1.0            py36hd6b4f25_0\r\n[conda] pytorch                   1.3.1           py3.6_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] pytorch-lightning         0.5.3.2                  pypi_0    pypi\r\n[conda] pytorch-toolbelt          0.2.1                    pypi_0    pypi\r\n[conda] torchsummary              1.5.1                    pypi_0    pypi\r\n[conda] torchvision               0.4.2                py36_cu101    pytorch\r\n```",
        "Challenge_closed_time":1583540837000,
        "Challenge_comment_count":9,
        "Challenge_created_time":1576464430000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered an issue with mlflow byom predictor where arbitrary URLs can be created without checking if an actual mlflow model is served at that URL. The user suggests implementing a check to ensure the validity of the URL before creating or linking the model.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/630",
        "Challenge_link_count":2,
        "Challenge_participation_count":9,
        "Challenge_readability":14.0,
        "Challenge_reading_time":59.35,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":64,
        "Challenge_solved_time":1965.6686111111,
        "Challenge_title":"Pickle error from Trainer.fit when using MLFlowLogger and distributed data parallel without SLURM",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":409,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Will investigate. We have a test that is supposed to prevent these problems from sneaking back in, but apparently it's not doing it's job. I imagine everyone is busy with the build failures - but for the record, I am  having a similar problem. Essentially, I cannot get a logger to work using ddp. It's gving me one of those days when I wonder why I ever wanted to write software ;)\r\n\r\nThis is Ubuntu 18.04.2LTS, on a 14 core, 7 gpu machine. Python 3.6.8, pytorch 1.3.1, pytorch-lightning 0.5.3.2, Tensorboard 2.1.0. Everything else standard except pillow isis 6.2.2 due to known bug in 7.0.\r\n\r\nI am working with a tried and true model and hyperparameters. The model and logging work fine as cpu, gpu, or dp - and ddp if I don't log. But not ddp with logging. I am not using SLURM.\r\n\r\nI have tried to get around this several ways: passing a custom logger, not using the logger created by Trainer(), etc. They either fail when called from one of the new processes, with an attribute error in Tensorboard TTDummyFileWriter.get_logdir(), or they fail with a pickle error about thread.locks when being copied to a new process\r\n\r\nI will detail these in a bug report if you think they are NOT due to the recent build issues.\r\n\r\nBut thought you'd want to know ...\r\n\r\ns\r\n @dbczumar,  @smurching? @neggert is this fixed now? Can this issue be re-opened? I'm currently working with Pytorch-Lightning==0.7.6 and am getting an identical pickle issue when using DDP with the MLFLowLogger.\r\n\r\n**Reproducing**\r\n\r\nUsing the script the OP gave led to some other errors (mostly to do with lightning version differences), so a new gist to reproduce in Pytorch-Lightning 0.7.6 can be found [here](https:\/\/gist.github.com\/Polyphenolx\/39424e5673fc029567f7f3ae3551fffb).\r\n\r\nThis is easily reproducible in other projects as well.\r\n\r\n**Error Output**\r\n\r\n```\r\n\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/utilities\/distributed.py:23: DeprecationWarning: `logging` package has been renamed to `loggers` since v0.7.0 The deprecated package name will be removed in v0.9.0.\r\n  warnings.warn(*args, **kwargs)\r\n\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/utilities\/distributed.py:23: DeprecationWarning: `mlflow_logger` module has been renamed to `mlflow` since v0.6.0. The deprecated module name will be removed in v0.8.0.\r\n  warnings.warn(*args, **kwargs)\r\n\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/utilities\/distributed.py:23: DeprecationWarning: `data_loader` decorator deprecated in v0.7.0. Will be removed v0.9.0\r\n  warnings.warn(*args, **kwargs)\r\nGPU available: True, used: True\r\nNo environment variable for node rank defined. Set as 0.\r\nCUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nTraceback (most recent call last):\r\n  File \"mlflow_test.py\", line 65, in <module>\r\n    trainer.fit(model)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 844, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 200, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 149, in start_processes\r\n    process.start()\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object '_get_rest_store.<locals>.get_default_host_creds'\r\n```\r\n\r\n**Environment**\r\n```\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- GeForce GTX 1080 Ti\r\n\t\t- GeForce GTX 1080 Ti\r\n\t\t- GeForce GTX 1080 Ti\r\n\t\t- GeForce GTX 1080 Ti\r\n\t- available:         True\r\n\t- version:           10.2\r\n* Packages:\r\n\t- numpy:             1.18.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.5.0\r\n\t- pytorch-lightning: 0.7.6\r\n\t- tensorboard:       2.2.2\r\n\t- tqdm:              4.46.1\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.6.8\r\n\t- version:           #102-Ubuntu SMP Mon May 11 10:07:26 UTC 2020\r\n``` To add to this, it appears to be a greater issue with MLFLow and how their tracking utilities are coded. They use a higher order function that causes issues with pickling in torches DDP backend. I've created an issue on MLFLow git, and submitted a PR to remedy the problem. \r\n\r\nIn the interim, feel free to implement the fix described in the issue in the MLFlow git as a temporary fix until\/if they review\/merge mine Following up on this: The pickling fix was merged into the master branch of MLFlow a couple days ago (see the bug mention above). Training using DDP is now functional on MLFLow versions installed from master, but it may take them some time to release the fix to PyPi Running into this same issue as are a few others here:\r\nhttps:\/\/github.com\/minimaxir\/aitextgen\/issues\/135\r\n![image](https:\/\/user-images.githubusercontent.com\/4674698\/121708545-8923f780-ca8c-11eb-9483-56740fd6d401.png)\r\n Hi,\r\n I am still getting the below error:\r\n![image](https:\/\/user-images.githubusercontent.com\/57705684\/131129141-fa483cb4-cb95-43a1-b1d3-62bf78711de2.png)\r\n\r\nI am using DP strategy and PT version '1.8.1+cu111' and PL version '1.3.8'.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":9.3,
        "Solution_reading_time":75.14,
        "Solution_score_count":null,
        "Solution_sentence_count":72.0,
        "Solution_word_count":675.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":285.6506852778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an endpoint in <code>us-east<\/code> which serves a custom imported model (docker image).<\/p>\n<p>This endpoint uses <code>min replicas = 1<\/code> and <code>max replicas = 100<\/code>.<\/p>\n<p>Sometimes, Vertex AI will require the model to scale from 1 to 2.<\/p>\n<p>However, there seems to be an issue causing the number of replicas to go from <code>1 -&gt; 0 -&gt; 2<\/code> instead of <code>1 -&gt; 2<\/code>.<\/p>\n<p>This causes several 504 (Gateway Timeout) errors in my API and the way to solve that was setting <code>min replicas &gt; 1<\/code>, highly impacting the monthly cost of the application.<\/p>\n<p>Is this some known issue to Vertex AI\/GCP services, is there anyway to fix it?<\/p>",
        "Challenge_closed_time":1641201707430,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640176354140,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has encountered an issue with Vertex AI Endpoints where the number of replicas scales from 1 to 0 before increasing to 2, causing 504 errors in their API. The user has had to set the minimum replicas to a higher number to solve the issue, which has increased the monthly cost of the application. The user is seeking a solution to this problem and wondering if it is a known issue with Vertex AI\/GCP services.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70449117",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.6,
        "Challenge_reading_time":9.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":284.8203583334,
        "Challenge_title":"Vertex AI Endpoints scales to 0 before increasing number of replicas",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":77.0,
        "Challenge_word_count":117,
        "Platform":"Stack Overflow",
        "Poster_created_time":1400036379907,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Rio de Janeiro - RJ, Brasil",
        "Poster_reputation_count":490.0,
        "Poster_view_count":65.0,
        "Solution_body":"<p>The intermittent <code>504<\/code> errors could be a result of an endpoint that is under-provisioned to handle the load. It can also happen if too many prediction requests are sent to the endpoint before it has a chance to scale up.<\/p>\n<p>Traffic splitting of the incoming prediction requests is done randomly. So, multiple requests may end up on the same model server at the same time. This can happen even if the overall Queries Per Second (QPS) is low, and especially when the QPS is spiky. This contributes to the requests being queued up if the model server isn't able to handle the load. This is what results in a 504 error.<\/p>\n<p><strong>Recommendations to mitigate the <code>504<\/code> errors are as follows:<\/strong><\/p>\n<ul>\n<li><p>Improve the container's ability to use all resources in the container. One thing to keep in mind about resource utilization is whether the model server is single-threaded or multi-threaded. The container may not be using up all the cores and\/or requests may be queuing up, hence are served only one-at-a-time.<\/p>\n<\/li>\n<li><p>Autoscaling is happening, it just might need to be tuned to the prediction workload and expectations. A lower utilization threshold would trigger autoscaling sooner.<\/p>\n<\/li>\n<li><p>Perform an exponential backoff while the deployment is scaling. This way, there is a retry mechanism to handle failed requests.<\/p>\n<\/li>\n<li><p>Provision a higher minimum replica count for the endpoint, which you have already implemented.<\/p>\n<\/li>\n<\/ul>\n<p>If the above recommendations do not solve the problem or in general require further investigation of these errors, please reach out to <a href=\"https:\/\/cloud.google.com\/support-hub#section-2\" rel=\"nofollow noreferrer\">GCP support<\/a> in case you have a <a href=\"https:\/\/cloud.google.com\/support\/\" rel=\"nofollow noreferrer\">support plan<\/a>. Otherwise, please open an issue in the <a href=\"https:\/\/issuetracker.google.com\/issues\/new?component=1134259&amp;template=1640573\" rel=\"nofollow noreferrer\">issue tracker<\/a>.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1641204696607,
        "Solution_link_count":3.0,
        "Solution_readability":10.0,
        "Solution_reading_time":25.75,
        "Solution_score_count":1.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":283.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1586263306992,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Krak\u00f3w, Poland",
        "Answerer_reputation_count":1622.0,
        "Answerer_view_count":654.0,
        "Challenge_adjusted_solved_time":9.5230127778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How can I find IP for vertex AI managed notebook instance? The service is differing from user managed notebooks in certain sense. The creation of an instance doesn't create a compute instance, so it's all managed by itself.<\/p>\n<p>My purpose is to whitelist the set of IPs in Mongo atlas. Set of IPs being of all the notebooks in that region. I'm using google-managed networks in this case.<\/p>\n<p>I've a few doubts here:<\/p>\n<ul>\n<li>Since within managed nb, I can change CPU consumption, will this reinstantiate a new cluster, with entirely new IP, or it will be 1 from among a group of IPs?<\/li>\n<li>Is it possible to add a custom init script?<\/li>\n<\/ul>",
        "Challenge_closed_time":1635870534303,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635836251457,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble finding the IP for their Vertex AI managed notebook instance, which is different from user-managed notebooks. They need to whitelist a set of IPs in Mongo Atlas for all notebooks in the region, but have doubts about whether changing CPU consumption will create a new cluster with a new IP or if it will be one from a group of IPs. They also want to know if it's possible to add a custom init script.",
        "Challenge_last_edit_time":1636016613528,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69806432",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.1,
        "Challenge_reading_time":8.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":9.5230127778,
        "Challenge_title":"Vertex AI Managed Notebook, get subnet\/IP",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":665.0,
        "Challenge_word_count":119,
        "Platform":"Stack Overflow",
        "Poster_created_time":1353151867408,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":2513.0,
        "Poster_view_count":251.0,
        "Solution_body":"<p>If you want to connect to a database service on GCP, create a network (or use the default) and instantiate the notebook using this network (<code>Advanced options<\/code>) and create the white list for this entire network . It's required because the managed notebook creates a peering network on the network you will use, you can check you in <code>VPC Network<\/code> \u279e <code>VPC Network Peering<\/code>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Pd1ui.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Pd1ui.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>If you want an external IP, it will not work. Google managed notebooks <strong>does not use external ips<\/strong>, they basically access the internet via NAT gateways (does not matter if you use google or own managed networks) so you will not be able to do what you want. Move for user managed notebooks (where you can assign a fixed external ip) or white list any IP on your Mongo db service if you are not in a production environment.<\/p>\n<p>About yous doubts:<\/p>\n<blockquote>\n<p>Since within managed nb, I can change CPU consumption, will this instantiate a new cluster, with entirely new IP, or it will be 1 from among a group of IPs<\/p>\n<\/blockquote>\n<p>For the internal network it may change when you restart or recreate the notebook instance. For an external network, it does not exists and explained.<\/p>\n<blockquote>\n<p>Is it possible to add a custom init script?<\/p>\n<\/blockquote>\n<p>Basically not. But you can provide custom docker images for the notebook.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1635872119870,
        "Solution_link_count":2.0,
        "Solution_readability":9.7,
        "Solution_reading_time":19.36,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":234.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1298044626600,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1476.0,
        "Answerer_view_count":105.0,
        "Challenge_adjusted_solved_time":2.4293805556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there an API to receive list of published webservices?<\/p>\n<p>I have workspace id and auth token, I can get list of projects and experiments, but I can't get list of services created from experiments. Specifically I need the URL in order to post requests.\n<a href=\"https:\/\/i.stack.imgur.com\/6YHP1.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6YHP1.png\" alt=\"\" \/><\/a><br \/>\n<sub>(source: <a href=\"https:\/\/msdnshared.blob.core.windows.net\/media\/TNBlogsFS\/prod.evol.blogs.technet.com\/CommunityServer.Blogs.Components.WeblogFiles\/00\/00\/01\/02\/52\/JupRay-2.png\" rel=\"nofollow noreferrer\">windows.net<\/a>)<\/sub><\/p>\n<p>In client api I see if we publish a new service we can get it, but do we have more options?<\/p>",
        "Challenge_closed_time":1462276416390,
        "Challenge_comment_count":0,
        "Challenge_created_time":1462267670620,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for an API to receive a list of published webservices in Azure ML, as they have the workspace id and auth token but cannot get the list of services created from experiments. They specifically need the URL to post requests and are wondering if there are more options available besides publishing a new service.",
        "Challenge_last_edit_time":1639596690136,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37000397",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":10.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":2.4293805556,
        "Challenge_title":"How to get list of services in Azure ML?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":42.0,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369681858647,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":464.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>The R Azure ML API has that. Excerpt from <a href=\"https:\/\/htmlpreview.github.io\/?https:\/\/github.com\/RevolutionAnalytics\/AzureML\/blob\/master\/vignettes\/getting_started.html\" rel=\"nofollow\">https:\/\/htmlpreview.github.io\/?https:\/\/github.com\/RevolutionAnalytics\/AzureML\/blob\/master\/vignettes\/getting_started.html<\/a> : <\/p>\n\n<p><code>\n(webservices &lt;- services(ws, name = \"AzureML-vignette-silly\"))\n<\/code><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":37.8,
        "Solution_reading_time":5.71,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":20.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":46.8695180555,
        "Challenge_answer_count":2,
        "Challenge_body":"I want to allowlist the Sagemaker studio IP so people can access certain allowlisted services from Sagemaker. I created a sagemaker domain in my private subnet of my VPC, so theoretically it should use the IP of the associated NAT gateway, right? But I see a different IP \ud83e\udd14",
        "Challenge_closed_time":1675200089343,
        "Challenge_comment_count":0,
        "Challenge_created_time":1674683552396,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user wants to allowlist the Sagemaker studio IP for accessing certain services, but is facing challenges as the IP address being used is different from the associated NAT gateway.",
        "Challenge_last_edit_time":1675031393875,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUX7n9V0osTAmdmLYM21vmKQ\/how-to-allowlist-sagemaker-ip",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.5,
        "Challenge_reading_time":3.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":143.4824852778,
        "Challenge_title":"How to allowlist sagemaker IP?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":131.0,
        "Challenge_word_count":52,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I should've read through my terraform code that created Sagemaker more carefully, I specified a VPC so I thought it would be *in* the VPC but it turns out I needed to specify the AppNetworkAccessType too.\n\nbad:\n```hcl\nresource \"aws_sagemaker_domain\" \"my_domain\" {\n  domain_name = var.domain_name\n  auth_mode   = \"IAM\"\n  vpc_id      = var.vpc_id\n  subnet_ids  = var.subnet_ids\n```\n\ngood:\n```hcl\nresource \"aws_sagemaker_domain\" \"my_domain\" {\n  domain_name = var.domain_name\n  auth_mode   = \"IAM\"\n  vpc_id      = var.vpc_id\n  subnet_ids  = var.subnet_ids\n  app_network_access_type = \"VpcOnly\"\n```",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1675200124140,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":7.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1347347916327,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":271.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":25.0828377778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When publishing an Azure ML Web Service and preloading data in our R model we see inconsistent performance. First calls are slow but following calls are fast, waiting a bit (couple of minutes) for the next call ends up showing longer response times.<\/p>",
        "Challenge_closed_time":1466599962023,
        "Challenge_comment_count":0,
        "Challenge_created_time":1466509663807,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is experiencing inconsistent performance when using Azure ML Web Service for R models. The first calls are slow, but subsequent calls are faster. Waiting a few minutes before making the next call results in longer response times.",
        "Challenge_last_edit_time":1466610116060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37943572",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":3.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":25.0828377778,
        "Challenge_title":"Azure ML Web Service for R models shows unpredictable",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":69.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_created_time":1466503688920,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":64.0,
        "Solution_body":"<p>The way Azure ML Web Services work in the background means that instances hosting the models are provisioned and moved in a very dynamic multi-tenant environment. Caching data (warming up) can be helpful but this doesn't mean all subsequent calls will land on the same instance with the same data available in the cache. <\/p>\n\n<p>For models that need a lot of in-memory data there is a limit to what the Azure ML Web Services hosting layer can offer at this point. Microsoft R server could be an alternative to host these big ML workloads and looking at Service Fabric to scale <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.0,
        "Solution_reading_time":7.08,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":103.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1421596186347,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":812.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":59.8581247222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In the SageMaker documentation, both <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-model-endpoints.html\" rel=\"nofollow noreferrer\">Multi-Model Endpoints<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-container-direct.html\" rel=\"nofollow noreferrer\">Multi-Container Endpoints with Direct Invocation<\/a> are described as very similar methods to host multiple models on a single endpoint. The given use cases appear identical except that <strong>Multi-Model Endpoints<\/strong> include many more advanced features.<\/p>\n<p>For example, <strong>Multi-Model Endpoints<\/strong> can host <em>n<\/em> number of models and support features such as resource sharing and model caching while <strong>Multi-Container Endpoints with Direct Invocation<\/strong> are limited to hosting only 5 models and lack model caching.<\/p>\n<p>When does it make sense to use <strong>Multi-Container Endpoints with Direct Invocation<\/strong> instead of <strong>Multi-Model Endpoints<\/strong>?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/2EQAA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2EQAA.png\" alt=\"Multi-Model Endpoint\" \/><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/A6jyS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/A6jyS.png\" alt=\"Multi-Container Endpoint with Direct Invocation\" \/><\/a><\/p>",
        "Challenge_closed_time":1630054299816,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629838066340,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking to understand the differences between Multi-Model Endpoints and Multi-Container Endpoints with Direct Invocation in SageMaker documentation. While both methods allow hosting multiple models on a single endpoint, Multi-Model Endpoints offer more advanced features such as hosting more models and supporting resource sharing and model caching. The user is questioning when it is appropriate to use Multi-Container Endpoints with Direct Invocation instead of Multi-Model Endpoints.",
        "Challenge_last_edit_time":1629838810567,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68913914",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":18.0,
        "Challenge_reading_time":18.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":60.0648544444,
        "Challenge_title":"Why Use Multi-Container Endpoints instead of Multi-Model Endpoints?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":224.0,
        "Challenge_word_count":127,
        "Platform":"Stack Overflow",
        "Poster_created_time":1531231343652,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"St. Louis, MO, USA",
        "Poster_reputation_count":676.0,
        "Poster_view_count":70.0,
        "Solution_body":"<p>If you want to serve multiple models from the same framework using the same endpoint then you can use multi-model endpoints. Due to using the same framework (e.g. only sklearn models), multi-model endpoints make it to the endpoint when they are called. You can have thousands of those models under one endpoint. Multi-container endpoints on the other hand allow serving models from multiple frameworks, e.g. one TensorFlow, one XGBoost and so on, with direct invocation again. However in this case there's <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-container-direct.html\" rel=\"nofollow noreferrer\">limit of 5 different models<\/a> on a single endpoint.<\/p>\n<p>So depending on the problem you are working, if you need to use multiple frameworks on a single endpoint then you will need to use multi-container endpoint with direct invocation. Otherwise you can use the multi-model endpoint.<\/p>\n<p><a href=\"https:\/\/towardsdatascience.com\/deploy-thousands-of-models-on-sagemaker-real-time-endpoints-with-automatic-retraining-pipelines-4eef7521d5a3\" rel=\"nofollow noreferrer\">Reference<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.4,
        "Solution_reading_time":14.35,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":134.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":10.5155730556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>After I've trained and deployed the model with AWS SageMaker, I want to evaluate it on several csv files:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>- category-1-eval.csv (~700000 records)\n- category-2-eval.csv (~500000 records)\n- category-3-eval.csv (~800000 records)\n...\n<\/code><\/pre>\n\n<p>The right way to do this is with using <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/estimator\/Estimator#evaluate\" rel=\"nofollow noreferrer\">Estimator.evaluate()<\/a> method, as it is fast.<\/p>\n\n<p>The problem is - I cannot find the way to restore SageMaker model into Tensorflow Estimator, is it possible?<\/p>\n\n<p>I've tried to restore a model like this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>tf.estimator.DNNClassifier(\n    feature_columns=...,\n    hidden_units=[...],\n    model_dir=\"s3:\/\/&lt;bucket_name&gt;\/checkpoints\",\n)\n<\/code><\/pre>\n\n<p>In AWS SageMaker documentation a different approach is described - <a href=\"https:\/\/docs.aws.amazon.com\/en_us\/sagemaker\/latest\/dg\/how-it-works-model-validation.html\" rel=\"nofollow noreferrer\">to test the actual endpoint from the Notebook<\/a> - but it takes to much time and requires a lot of API calls to the endpoint.<\/p>",
        "Challenge_closed_time":1562591149540,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562553970617,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained and deployed a model with AWS SageMaker and wants to evaluate it on several CSV files using the Estimator.evaluate() method. However, they are unable to restore the SageMaker model into Tensorflow Estimator and are seeking a solution. The AWS SageMaker documentation suggests testing the actual endpoint from the Notebook, but this approach is time-consuming and requires a lot of API calls to the endpoint.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56927813",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":16.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":10.3274786111,
        "Challenge_title":"Using of Estamator.evaluate() on trained sagemaker tensorflow model",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":155.0,
        "Challenge_word_count":126,
        "Platform":"Stack Overflow",
        "Poster_created_time":1530642335903,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Vancouver, BC, Canada",
        "Poster_reputation_count":53.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>if you used the built-in Tensorflow container, your model has been saved in Tensorflow Serving format, e.g.:<\/p>\n\n<pre><code>$ tar tfz model.tar.gz\nmodel\/\nmodel\/1\/\nmodel\/1\/saved_model.pb\nmodel\/1\/variables\/\nmodel\/1\/variables\/variables.index\nmodel\/1\/variables\/variables.data-00000-of-00001\n<\/code><\/pre>\n\n<p>You can easily load it with Tensorflow Serving on your local machine, and send it samples to predict. More info at <a href=\"https:\/\/www.tensorflow.org\/tfx\/guide\/serving\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/tfx\/guide\/serving<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1562591826680,
        "Solution_link_count":2.0,
        "Solution_readability":12.2,
        "Solution_reading_time":7.39,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":53.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11.9014691667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>hello.  <\/p>\n<p>I am currently using Machine Learning Studio (classic).  <\/p>\n<p>'From now through 31 August 2024, you can continue to use the existing Machine Learning Studio (classic) experiments and web services. Beginning 1 December 2021, new creation of Machine Learning Studio (classic) resources will not be available.'  <\/p>\n<p>As mentioned above, what do resources mean?  <br \/>\nIs it possible to continue creating experiments and web sales, and using APIs from outside until 2024?<\/p>",
        "Challenge_closed_time":1635733766196,
        "Challenge_comment_count":1,
        "Challenge_created_time":1635690920907,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is currently using Machine Learning Studio (classic) and has learned that new creation of resources will not be available starting December 1, 2021. The user is seeking clarification on what \"resources\" means and if they can continue creating experiments, web services, and using APIs until 2024.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/610432\/about-the-end-of-machine-learning-studio-(classic)",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.6,
        "Challenge_reading_time":6.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":11.9014691667,
        "Challenge_title":"About the end of Machine Learning Studio (classic)",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":81,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=6fde402b-b49f-41d7-8640-5316dd9a5cd2\">@gatsby53  <\/a> ,    <\/p>\n<p>Thanks for reaching out to us here. There are several important dates.    <\/p>\n<p>Beginning 1 December 2021, you will <strong>not be able to create new<\/strong> Machine Learning Studio (classic) resources. You can still work on your <strong>existing resource<\/strong> from 1 December 2021 to 31 August 2024.    <\/p>\n<p>Support for Machine Learning Studio (classic) will end on 31 August 2024. We recommend you transition to Azure Machine Learning by that date.    <\/p>\n<p>Please refer to this guidance for how to migrate your project for better experience.    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/migrate-overview\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/migrate-overview<\/a>    <\/p>\n<p>Hope this will help. Please let us know if any further queries.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/543261\/index.html\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":11.9,
        "Solution_reading_time":25.33,
        "Solution_score_count":0.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":201.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":524.4738888889,
        "Challenge_answer_count":2,
        "Challenge_body":"Hello,  \n  \nI have trained my model on sagemaker. I have deleted the endpoint, but I am keeping the model and the endpoint configuration which points to the model.  \n  \nFrom the sagemaker dashboard I am able to recreate the endpoint using the existing endpoint configuration. However I don't want to keep the endpoint on all the time, as I will use it only once a day for a few minutes.  \n  \nIs it possible to create in on demand from a Python script? I would assume that it is possible, but can't find how. Can someone point me in the right direction?  \n  \nRegards.",
        "Challenge_closed_time":1626971777000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625083671000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a model on sagemaker and deleted the endpoint, but kept the model and endpoint configuration. They want to recreate the endpoint on demand from a Python script instead of keeping it on all the time, but are unsure how to do so and are seeking guidance.",
        "Challenge_last_edit_time":1668599351248,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUTyUMHH4QRDaMa6L24rhOMg\/create-endpoint-from-python",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.0,
        "Challenge_reading_time":6.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":524.4738888889,
        "Challenge_title":"Create endpoint from Python",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":125.0,
        "Challenge_word_count":104,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello hugoflores,   \n  \nYou can use SageMaker APIs - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DeleteEndpoint.html to delete the endpoint and https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateEndpoint.html. to create an endpoint. This an be automated either using SageMaker Pipelines or a Lambda function.  \n  \nHere are a few resources towards that:  \n  \nhttps:\/\/awsfeed.com\/whats-new\/machine-learning\/build-a-ci-cd-pipeline-for-deploying-custom-machine-learning-models-using-aws-services  \nhttps:\/\/github.com\/aws-samples\/aws-lambda-layer-create-script  \nhttps:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1200  \nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/  \nhttps:\/\/www.sagemakerworkshop.com\/step\/deploymodel\/  \n  \nHTH,   \n  \nChaitanya",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1626971777000,
        "Solution_link_count":7.0,
        "Solution_readability":32.1,
        "Solution_reading_time":11.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":44.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1403541426412,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bengaluru, India",
        "Answerer_reputation_count":191.0,
        "Answerer_view_count":22.0,
        "Challenge_adjusted_solved_time":2355.5938513889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created the Azure ML experiment with R script module \nit works fine while we run the experiment but\n when we publish the web service it throws error http 500 \n ( I believe the error is causing in the R script module because other modules are running fine in web service but i can't debug the problem<\/p>\n\n<blockquote>\n  <p>Http status code: 500, Timestamp: Fri, 08 May 2015 04:23:14 GMT<\/p>\n<\/blockquote>\n\n<p>Also is there any limitation in r e.g. some function which wont work in web service<\/p>",
        "Challenge_closed_time":1439539745912,
        "Challenge_comment_count":0,
        "Challenge_created_time":1431059608047,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has created an Azure Machine Learning experiment with an R script module that works fine when run, but encounters an HTTP 500 error when publishing the web service. The user suspects that the error is caused by the R script module, but is unable to debug the problem. The user also asks if there are any limitations in R that may prevent certain functions from working in the web service.",
        "Challenge_last_edit_time":1446192965568,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/30115812",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.7,
        "Challenge_reading_time":7.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":2355.5938513889,
        "Challenge_title":"Error while running Azure Machine Learning web service but the experiment works fine",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":415.0,
        "Challenge_word_count":98,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403541426412,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, India",
        "Poster_reputation_count":191.0,
        "Poster_view_count":22.0,
        "Solution_body":"<p>I found the problem. I was facing this error because the R module in the Azure ML was was taking variable as the other data type and not producing any outputs results which i was checking through for loop which is why i was getting this error.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.9,
        "Solution_reading_time":3.0,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":47.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.6422222222,
        "Challenge_answer_count":1,
        "Challenge_body":"I have a customer asking me about the [Rendezvous architecture](https:\/\/towardsdatascience.com\/rendezvous-architecture-for-data-science-in-production-79c4d48f12b). What I'm thinking is, we could implement this in a number of ways, all using endpoint variants:\n\n- Lambda (and probably SQS) around the endpoint;\n- A custom monitoring job;\n- Step Functions\n\nWithout going into details of the above options or of how the evaluation and SLA check will be done, it looks like the several models would fit very well as variants of an endpoint. The thing is, the architecture expects to call them all. Is there a way to directly call all variants of a model, or will a wrapper to identify the variants, call them all and process the results be needed?",
        "Challenge_closed_time":1604506964000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604486652000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is exploring the implementation of Rendezvous architecture using endpoint variants, such as Lambda, SQS, custom monitoring job, and Step Functions. However, the architecture expects to call all variants of a model, and the user is unsure if there is a way to directly call all variants or if a wrapper is needed to identify the variants, call them all, and process the results.",
        "Challenge_last_edit_time":1667925743687,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU6bm-EMtOQV6robgbTXClLQ\/running-a-request-against-all-variants-in-an-endpoint",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":9.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":5.6422222222,
        "Challenge_title":"Running a request against all variants in an endpoint",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":40.0,
        "Challenge_word_count":121,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"When I last looked into it, it was not possible to query all versions\/variants of the model automatically. You can specify what variant to use when using the `invoke_endpoint` method. I would therefore write a lambda function to invoke each of the endpoints one-by-one (see here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_runtime_InvokeEndpoint.html). To be especially rigorous about it, you can add a function in your lambda code that first retrieves all the endpoint variants (see here: https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.describe_endpoint) then queries them one-by-one, and returns all the results.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1607685345047,
        "Solution_link_count":2.0,
        "Solution_readability":14.3,
        "Solution_reading_time":9.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":81.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1467237684900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":613.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":1.5730691667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I've trained a model on sagemaker and have created the endpoint. I'm trying to invoke the endpoint using postman. But when training the model and even after that, I have not specified any header for the training data. I'm at a loss as to how to create payload while sending a post request to sagemaker<\/p>",
        "Challenge_closed_time":1522946255432,
        "Challenge_comment_count":0,
        "Challenge_created_time":1522940592383,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user has trained a model on sagemaker and created an endpoint, but is having trouble invoking the endpoint using postman. They are unsure how to create a payload while sending a post request to sagemaker as they have not specified any header for the training data.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49675637",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.0,
        "Challenge_reading_time":4.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.5730691667,
        "Challenge_title":"How to pass a request to sagemaker using postman",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":6746.0,
        "Challenge_word_count":63,
        "Platform":"Stack Overflow",
        "Poster_created_time":1410972175307,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1124.0,
        "Poster_view_count":153.0,
        "Solution_body":"<p>Once the endpoint is created, you can invoke it as any other restful service, with credentials and payload. <\/p>\n\n<p>I am guessing, there could be two places where might be stuck. \nOne could be, sending an actual PostMan Request with all the headers and everything. \nNewer version of Postman has AWS Signature as one of the Authorization types. You can use that to invoke the service. There are no other spacial headers required. Note that there is a bug in Postman still open (<a href=\"https:\/\/github.com\/postmanlabs\/postman-app-support\/issues\/1663\" rel=\"noreferrer\">issue-1663<\/a>) that only affects if you are a AWS federated account. Individual accounts should not be affected by this issue. <\/p>\n\n<p>Or, you could be stuck at the actual payload. When you invoke the SageMaker endpoint, the payload is passed as is to the model. If you want to preprocess the input before feeding it to the model, you'd have to implement an input_fn method and specify that when instantiating the model. <\/p>\n\n<p>You might also be able to invoke SageMaker endpoint using AWS SDK boto3 as follows <\/p>\n\n<pre><code>import boto3\nruntime= boto3.client('runtime.sagemaker')\n\npayload = getImageData()\n\n\nresult  = runtime.invoke_endpoint(\n    EndpointName='my_endpoint_name',\n    Body=payload,\n    ContentType='image\/jpeg'\n)\n<\/code><\/pre>\n\n<p>Hope this helps.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.4,
        "Solution_reading_time":16.63,
        "Solution_score_count":11.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":189.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1451855177167,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Crici\u00fama - State of Santa Catarina, Brazil",
        "Answerer_reputation_count":68.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":37.9991222222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have some data in S3 and I want to create a lambda function to predict the output with my deployed aws sagemaker endpoint then I put the outputs in S3 again. Is it necessary in this case to create an api gateway like decribed in this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"noreferrer\">link<\/a> ? and in the lambda function what I have to put. I expect to put (where to find the data, how to invoke the endpoint, where to put the data) <\/p>\n\n<pre><code>import boto3\nimport io\nimport json\nimport csv\nimport os\n\n\nclient = boto3.client('s3') #low-level functional API\n\nresource = boto3.resource('s3') #high-level object-oriented API\nmy_bucket = resource.Bucket('demo-scikit-byo-iris') #subsitute this for your s3 bucket name. \n\nobj = client.get_object(Bucket='demo-scikit-byo-iris', Key='foo.csv')\nlines= obj['Body'].read().decode('utf-8').splitlines()\nreader = csv.reader(lines)\n\nimport io\nfile = io.StringIO(lines)\n\n# grab environment variables\nruntime= boto3.client('runtime.sagemaker')\n\nresponse = runtime.invoke_endpoint(\n    EndpointName= 'nilm2',\n    Body = file.getvalue(),\n    ContentType='*\/*',\n    Accept = 'Accept')\n\noutput = response['Body'].read().decode('utf-8')\n<\/code><\/pre>\n\n<p>my data is a csv file of 2 columns of floats with no headers, the problem is that lines return a list of strings(each row is an element of this list:['11.55,65.23', '55.68,69.56'...]) the invoke work well but the response is also a string: output = '65.23\\n,65.23\\n,22.56\\n,...' <\/p>\n\n<p>So how to save this output to S3 as a csv file <\/p>\n\n<p>Thanks<\/p>",
        "Challenge_closed_time":1550022296023,
        "Challenge_comment_count":1,
        "Challenge_created_time":1549885499183,
        "Challenge_favorite_count":4.0,
        "Challenge_gpt_summary_original":"The user wants to create a lambda function to predict the output with their deployed AWS Sagemaker endpoint and put the outputs in S3 again. They are unsure if it is necessary to create an API gateway and what to put in the lambda function. The user has provided a code snippet that successfully invokes the endpoint, but the response is a string that needs to be saved to S3 as a CSV file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54629890",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.0,
        "Challenge_reading_time":21.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":37.9991222222,
        "Challenge_title":"Invoke aws sagemaker endpoint",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":3146.0,
        "Challenge_word_count":200,
        "Platform":"Stack Overflow",
        "Poster_created_time":1502010899808,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tunis, Tunisia",
        "Poster_reputation_count":109.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>If your Lambda function is scheduled, then you won't need an API Gateway. But if the predict action will be triggered by a user, by an application, for example, you will need.<\/p>\n\n<p>When you call the invoke endpoint, actually you are calling a SageMaker endpoint, which is not the same as an API Gateway endpoint. <\/p>\n\n<p>A common architecture with SageMaker is:<\/p>\n\n<ol>\n<li>API Gateway with receives a request then calls an authorizer, then\ninvoke your Lambda; <\/li>\n<li>A Lambda with does some parsing in your input data, then calls your SageMaker prediction endpoint, then, handles the result and returns to your application.<\/li>\n<\/ol>\n\n<p>By the situation you describe, I can't say if your task is some academic stuff or a production one.<\/p>\n\n<p>So, how you can save the data as a CSV file from your Lambda? <\/p>\n\n<p>I believe you can just parse the output, then just upload the file to S3. Here you will parse manually or with a lib, with boto3 you can upload the file. The output of your model depends on your implementation on SageMaker image. So, if you need the response data in another format, maybe you will need to use a <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">custom image<\/a>. I normally use a custom image, which I can define how I want to handle my data on requests\/responses.<\/p>\n\n<p>In terms of a production task, I certainly recommend you check Batch transform jobs from SageMaker. You can provide an input file (the S3 path) and also a destination file (another S3 path). The SageMaker will run the batch predictions and will persist a file with the results. Also, you won't need to deploy your model to an endpoint, when this job run, will create an instance of your endpoint, download your data to predict, do the predictions, upload the output, and shut down the instance. You only need a trained model.<\/p>\n\n<p>Here some info about Batch transform jobs:<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-batch.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-batch.html<\/a><\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-batch-transform.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-batch-transform.html<\/a><\/p>\n\n<p>I hope it helps, let me know if need more info.<\/p>\n\n<p>Regards.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":10.5,
        "Solution_reading_time":30.91,
        "Solution_score_count":4.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":341.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1446238644288,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bothell, WA, USA",
        "Answerer_reputation_count":1080.0,
        "Answerer_view_count":155.0,
        "Challenge_adjusted_solved_time":17662.8095425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We have serval <a href=\"http:\/\/luis.ai\" rel=\"nofollow noreferrer\">LUIS<\/a> models that work fine and returns desired <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/luis\/luis-concept-intent\" rel=\"nofollow noreferrer\">Intents<\/a> and <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/luis\/luis-concept-entity-types\" rel=\"nofollow noreferrer\">Entities<\/a>.<\/p>\n\n<p>Models are separated based on content and target business domain so we do not want to merge them.\nStill there are some <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/luis\/luis-quickstart-intents-regex-entity\" rel=\"nofollow noreferrer\">Regex entities<\/a> that are the same in each of the model.<\/p>\n\n<hr>\n\n<p>If we'd like to have one Regex definition at one place could we eventually share such definition among multiple LUIS models?<\/p>\n\n<p>Right now we proceed with Ctrl+C and Ctrl+V<\/p>",
        "Challenge_closed_time":1529098914963,
        "Challenge_comment_count":0,
        "Challenge_created_time":1529053600037,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has multiple LUIS models that work well and return the desired Intents and Entities. However, they have some Regex entities that are the same in each model, and they want to know if it's possible to share the definition of these entities among multiple models instead of copying and pasting them.",
        "Challenge_last_edit_time":1529058260707,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50872338",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":12.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":12.5874794445,
        "Challenge_title":"Could LUIS share entity definition among multiple models?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":64.0,
        "Challenge_word_count":96,
        "Platform":"Stack Overflow",
        "Poster_created_time":1483649190768,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Prague-Prague 1, Czechia",
        "Poster_reputation_count":507.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>We could share the entire app by <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/luis\/luis-how-to-manage-versions\" rel=\"noreferrer\">cloning the app<\/a>. And the only way to use specific entities would be to delete the others.<\/p>\n<blockquote>\n<p>If we'd like to have one Regex definition at one place could we eventually share such definition among multiple LUIS models?<\/p>\n<p>Right now we proceed with Ctrl+C and Ctrl+V<\/p>\n<\/blockquote>\n<p>This is the only way to do it right now.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1592644375060,
        "Solution_link_count":1.0,
        "Solution_readability":8.5,
        "Solution_reading_time":6.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":67.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":6.3710825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there any way to connected google cloud platform service vertex ai endpoint through .Net code ? I am new to gcp vertex. any help is really apricated.<\/p>",
        "Challenge_closed_time":1648004091847,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647981155950,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking assistance in connecting a Google Cloud Platform service, Vertex AI endpoint, through .Net code and is new to GCP Vertex.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71578582",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.2,
        "Challenge_reading_time":2.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":6.3710825,
        "Challenge_title":"connect vertex ai endpoint through .Net",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":128.0,
        "Challenge_word_count":32,
        "Platform":"Stack Overflow",
        "Poster_created_time":1462469556836,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":509.0,
        "Poster_view_count":65.0,
        "Solution_body":"<p>You could refer to <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\" rel=\"nofollow noreferrer\">Google.Cloud.AIPlatform.V1 documentation<\/a> for reference for dot net. You can start by:<\/p>\n<ol>\n<li>Installing the package from <a href=\"https:\/\/www.nuget.org\/packages\/Google.Cloud.AIPlatform.V1\/\" rel=\"nofollow noreferrer\">Google AI Platform nuget<\/a><\/li>\n<li>If you don't have an endpoint yet you can check out <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\/Google.Cloud.AIPlatform.V1.EndpointServiceClient\" rel=\"nofollow noreferrer\">Google.Cloud.AIPlatform.V1.EndpointServiceClient<\/a>. You can use this class to manage endpoints like create endpoint, delete endpoint, deploy endpoint, etc.\n<ul>\n<li>Check this <a href=\"https:\/\/github.com\/googleapis\/google-cloud-dotnet\/blob\/main\/apis\/Google.Cloud.AIPlatform.V1\/Google.Cloud.AIPlatform.V1.Snippets\/IndexEndpointServiceClientSnippets.g.cs\" rel=\"nofollow noreferrer\">EndpointServiceClient code sample<\/a> for usage.<\/li>\n<\/ul>\n<\/li>\n<li>If you have an endpoint and you want to run predictions using it, you can check out <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\/Google.Cloud.AIPlatform.V1.PredictionServiceClient\" rel=\"nofollow noreferrer\">Google.Cloud.AIPlatform.V1.PredictionServiceClient<\/a>. You can use this class to perform prediction using your endpoint.\n<ul>\n<li>Specifically <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\/Google.Cloud.AIPlatform.V1.PredictionServiceClient#Google_Cloud_AIPlatform_V1_PredictionServiceClient_Predict_Google_Cloud_AIPlatform_V1_EndpointName_System_Collections_Generic_IEnumerable_Google_Protobuf_WellKnownTypes_Value__Google_Protobuf_WellKnownTypes_Value_Google_Api_Gax_Grpc_CallSettings_\" rel=\"nofollow noreferrer\">Predict(EndpointName, IEnumerable, Value, CallSettings)<\/a> method where it accepts an endpoint as parameter.<\/li>\n<li>Check this <a href=\"https:\/\/github.com\/googleapis\/google-cloud-dotnet\/blob\/main\/apis\/Google.Cloud.AIPlatform.V1\/Google.Cloud.AIPlatform.V1.Snippets\/PredictionServiceClientSnippets.g.cs\" rel=\"nofollow noreferrer\">PredictionServiceClient code sample<\/a> for usage.<\/li>\n<\/ul>\n<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":26.5,
        "Solution_reading_time":31.63,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":136.0,
        "Tool":"Vertex AI"
    }
]