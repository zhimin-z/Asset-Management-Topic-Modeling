[
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10.8978944445,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Is there any way to return a custom HTTP status code from R Web Service in Azure ML?  <\/p>\n<p>All the examples of entry scripts in documentation return the response body from the scoring function. In Python Web Service, it is possible to return a HTTP response object with a custom status code. However, R's httr library does not seem to have any function to create response objects directly (only via HTTP method objects such as POST, which call a given URL).  <\/p>\n<p>I would like to implement a custom exception handling scheme in R Web Service. Is there any way to return a custom HTTP code from the entry script?  <\/p>\n<p>EDIT: Found this idea on the feedback forum, which suggests that the option is not available in Python Web Service either:  <br \/>\n<a href=\"https:\/\/feedback.azure.com\/forums\/257792-machine-learning\/suggestions\/40122838-make-http-status-codes-controllable-from-your-scor\">https:\/\/feedback.azure.com\/forums\/257792-machine-learning\/suggestions\/40122838-make-http-status-codes-controllable-from-your-scor<\/a>  <\/p>",
        "Challenge_closed_time":1612394092687,
        "Challenge_comment_count":0,
        "Challenge_created_time":1612354860267,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to implement a custom exception handling scheme in R Web Service in Azure ML and wants to know if there is any way to return a custom HTTP status code from the entry script. However, the httr library in R does not seem to have any function to create response objects directly. The user also found an idea on the feedback forum suggesting that the option is not available in Python Web Service either.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/257156\/how-to-specify-http-response-status-code-in-aml-r",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":12.9,
        "Challenge_reading_time":13.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":10.8978944445,
        "Challenge_title":"How to specify HTTP response status code in AML R Web Service",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":148,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello Lauri,  <\/p>\n<p>Thanks for the feedback. Yes, we have this product idea in our backlog. I will help to bump up this idea to product group again. ^^  <\/p>\n<p>Regards,  <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.9,
        "Solution_reading_time":2.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":32.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"custom HTTP status code"
    },
    {
        "Answerer_created_time":1535490052056,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":81.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":76.4050927778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have ~5MB json string that I want to send to my endpoint. I am using boto3.client to invoke the endpoint from my python client. It throws ConnectionResetError. <\/p>\n\n<pre><code>    File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\urllib3\\connectionpool.py\", line 600, in urlopen\n    chunked=chunked)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\urllib3\\connectionpool.py\", line 354, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\", line 1229, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\botocore\\awsrequest.py\", line 92, in _send_request\n    method, url, body, headers, *args, **kwargs)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\", line 1275, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\", line 1224, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\botocore\\awsrequest.py\", line 119, in _send_output\n    self.send(msg)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\botocore\\awsrequest.py\", line 203, in send\n    return super(AWSConnection, self).send(str)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\", line 977, in send\n    self.sock.sendall(data)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\ssl.py\", line 1012, in sendall\n    v = self.send(byte_view[count:])\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\ssl.py\", line 981, in send\n    return self._sslobj.write(data)\nConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n<\/code><\/pre>\n\n<p>Looking at the trace, I am guessing it is due to json string size. Could someone please help me how to get around this? <\/p>",
        "Challenge_closed_time":1586421549190,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586140541570,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering a ConnectionResetError while trying to send a 5MB JSON string to an AWS Sagemaker endpoint using boto3.client from their Python client. The user suspects that the error is due to the size of the JSON string and is seeking help to resolve the issue.",
        "Challenge_last_edit_time":1586146490856,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61052173",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.4,
        "Challenge_reading_time":29.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":78.0576722223,
        "Challenge_title":"Is there a limit on input json string for aws sagemaker endpoint?",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1281.0,
        "Challenge_word_count":174,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1584461369092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Exceeding the payload size limit does result in a connection reset from the SageMaker Runtime service.<\/p>\n\n<p>From the SageMaker <a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html\" rel=\"nofollow noreferrer\">documentation<\/a>:<\/p>\n\n<blockquote>\n  <p>Maximum payload size for endpoint invocation |    5 MB<\/p>\n<\/blockquote>\n\n<p>There are likely more space-efficient data formats than JSON that you could use to transmit the payload, but the available options will depend on the type of data and what model image you are using (i.e. whether Amazon-provided or a custom implementation).<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.3,
        "Solution_reading_time":7.73,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":75.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"ConnectionResetError"
    },
    {
        "Answerer_created_time":1294155598400,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"M\u00fcnchen, Deutschland",
        "Answerer_reputation_count":112558.0,
        "Answerer_view_count":21355.0,
        "Challenge_adjusted_solved_time":1.8225491667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to have a lambda calling a Sagemaker instance in another region. If both are in the same region, everything works fine. If they are not, I get the following error:<\/p>\n\n<pre><code>The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\n\nThe Canonical String for this request should have been\n'POST\n\/endpoints\/foo-endpoint\/invocations\n\nhost:runtime.sagemaker.us-east-1.amazonaws.com\nx-amz-date:20180406T082536Z\n\nhost;x-amz-date\n1234567890foobarfoobarfoobarboofoobarfoobarfoobarfoobarfoobarfoo'\n\nThe String-to-Sign should have been\n'AWS4-HMAC-SHA256\n20180406T082536Z\n20180406\/us-east-1\/sagemaker\/aws4_request\n987654321abcdeffoobarfoobarfoobarfoobarfoobarfoobarfoobarfoobarf'\n<\/code><\/pre>\n\n<p>I use <a href=\"https:\/\/github.com\/DavidMuller\/aws-requests-auth\" rel=\"nofollow noreferrer\"><code>aws-requests-auth<\/code><\/a> (0.4.1) with boto3 (1.5.15 - updating to 1.7.1 didn't change anything, <a href=\"https:\/\/github.com\/boto\/boto3\/blob\/develop\/CHANGELOG.rst\" rel=\"nofollow noreferrer\">changelog<\/a>) like this:<\/p>\n\n<pre><code>import requests\nfrom aws_requests_auth.aws_auth import AWSRequestsAuth\nauth = AWSRequestsAuth(aws_access_key=config['AWS']['ACCESS_KEY'],\n                       aws_secret_access_key=(\n                           config['AWS']['SECRET_ACCESS_KEY']),\n                       aws_host=config['AWS']['HOST'],\n                       aws_region=config['AWS']['REGION'],\n                       aws_service=config['AWS']['SERVICE'])\n\npayload = {'foo': 'bar'}\nresponse = requests.post(post_url,\n                         data=json.dumps(payload),\n                         headers={'content-type': 'application\/json'},\n                         auth=auth)\n<\/code><\/pre>\n\n<p>printing <code>auth<\/code> only gives <code>&lt;aws_requests_auth.aws_auth.AWSRequestsAuth object at 0x7f9d00c98390&gt;<\/code>.<\/p>\n\n<p>Is there a way to print the \"Canonical String\" mentioned in the error message?<\/p>\n\n<p>(Any other ideas how to fix this are appreciated as well)<\/p>",
        "Challenge_closed_time":1523012141480,
        "Challenge_comment_count":0,
        "Challenge_created_time":1523005580303,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to call a Sagemaker instance in another region using a lambda function, but is encountering an error related to the request signature. The error message includes a Canonical String that should have been used for the request. The user is using aws-requests-auth and boto3 libraries for authentication and is seeking a way to print the Canonical String mentioned in the error message.",
        "Challenge_last_edit_time":1531211963023,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49689216",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":15.3,
        "Challenge_reading_time":26.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":1.8225491667,
        "Challenge_title":"How can I print the Canonical String which aws-requests-auth sends?",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":827.0,
        "Challenge_word_count":171,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1294155598400,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"M\u00fcnchen, Deutschland",
        "Poster_reputation_count":112558.0,
        "Poster_view_count":21355.0,
        "Solution_body":"<p>A work-around for the asked question:<\/p>\n\n<pre><code>req = requests.request('POST', 'http:\/\/httpbin.org\/get')\nreq.body = b''\nreq.method = ''\nprint(auth.get_aws_request_headers(req,\n                                   aws_access_key=auth.aws_access_key,\n                                   aws_secret_access_key=auth.aws_secret_access_key,\n                                   aws_token=auth.aws_token))\n<\/code><\/pre>\n\n<p>The problem is not solved, though. And now I wonder what the first argument of <code>auth.get_aws_request_headers<\/code> is.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.7,
        "Solution_reading_time":5.95,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":34.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"request signature error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":6.5138261111,
        "Challenge_answer_count":1,
        "Challenge_body":"We deployed a LighGBM Regression model and endpoint using Sagemaker Jumpstart.\nWe have attempted to configure this endpoint as 'asynchronous' via the console.\nReceiving Error: ValidationException-Network Isolation is not supported when specifying an AsyncInferenceConfig.\n\nLooking at the model's network details the model has Enable Network Isolation set as 'True'.\nThis was default output setting set by JumpStart.\n\nHow can we diasble Network Isolation to in order to make this endpoint asynchronous?",
        "Challenge_closed_time":1653023938004,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653000488230,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user deployed a LighGBM Regression model and endpoint using Sagemaker Jumpstart and attempted to configure it as 'asynchronous' via the console. However, they received an error stating that Network Isolation is not supported when specifying an AsyncInferenceConfig. The model's network details show that Enable Network Isolation is set as 'True', which was the default output setting set by JumpStart. The user is seeking guidance on how to disable Network Isolation to make the endpoint asynchronous.",
        "Challenge_last_edit_time":1668013027784,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUZNbZZQHhSl2RYUtLU8zpSQ\/sagemaker-asynchronous-endpoint-configuration",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":6.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":6.5138261111,
        "Challenge_title":"Sagemaker Asynchronous Endpoint Configuration",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":123.0,
        "Challenge_word_count":74,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Vanilla SageMaker \"Models\" (as opposed to versioned ModelPackages) are immutable in the API with no \"UpdateModel\" action... But I think you should be able to create a new Model copying the settings of the current one.\n\nI'd suggest to:\n\n1. Use [DescribeModel](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeModel.html) (via [boto3.client(\"sagemaker\").describe_model()](https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.describe_model), assuming you're using Python) to fetch all the parameters of the existing JumpStart model such as the S3 artifact location and other settings\n2. Use [CreateModel](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html) ([create_model()](https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_model)) to create a new model with same configuration but network isolation disabled\n3. Use your new model to try and deploy an async endpoint\n\nProbably you'd find the low-level boto3 SDK more intuitive for this task than the high-level `sagemaker` SDK's [Model class](https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html) - because the latter does some magic that makes typical build\/train\/deploy workflows easier but can be less natural for hacking around with existing model definitions. For example, creating an SMSDK `Model` object doesn't actually create a Model in the SageMaker API, because deployment instance type affects choice of container image so that gets deferred until a `.deploy()` call or similar later.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1653023938006,
        "Solution_link_count":5.0,
        "Solution_readability":18.3,
        "Solution_reading_time":21.3,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":174.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"Network Isolation error"
    },
    {
        "Answerer_created_time":1631803441500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"mexico",
        "Answerer_reputation_count":1258.0,
        "Answerer_view_count":685.0,
        "Challenge_adjusted_solved_time":26.1469897223,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to un-deploy model from an endpoint following <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform.Endpoint#google_cloud_aiplatform_Endpoint_undeploy\" rel=\"nofollow noreferrer\">this documentation<\/a>.<\/p>\n<pre><code>Endpoint.undeploy(deployed_model_id=model_id)\n<\/code><\/pre>\n<p>I even tried <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.endpoints\/undeployModel\" rel=\"nofollow noreferrer\">google api<\/a>. Same Issue with this as well.<\/p>\n<p>Getting 404 error<\/p>\n<blockquote>\n<p>The Deployed Model with ID <code>2367889687867<\/code> is missing.<\/p>\n<\/blockquote>\n<p><strong>INFO:<\/strong><\/p>\n<ol>\n<li>Both model and Endpoint are in same region.<\/li>\n<li>There is a single model deployed in the endpoint with <code>traffic_percentage=100<\/code>.<\/li>\n<\/ol>",
        "Challenge_closed_time":1655314821510,
        "Challenge_comment_count":4,
        "Challenge_created_time":1655220692347,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to un-deploy a model from an endpoint using GCP AI Platform Vertex and Google API, but is encountering a 404 error stating that the deployed model with the given ID is missing. The model and endpoint are in the same region and there is only one model deployed with a traffic percentage of 100.",
        "Challenge_last_edit_time":1655954014260,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72619696",
        "Challenge_link_count":2,
        "Challenge_participation_count":5,
        "Challenge_readability":16.2,
        "Challenge_reading_time":12.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":26.1469897223,
        "Challenge_title":"GCP AI Platform Vertex endpoint model undeploy : 404 The DeployedModel with ID `2367889687867` is missing",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":271.0,
        "Challenge_word_count":80,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1550779047856,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":363.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>The <code>deployed_model_id<\/code> is different from the <code>model_id<\/code>.That\u2019s why you are getting the Error 404, it is searching for something that is not the same.<\/p>\n<p>You can get the <code>deployed_model_id<\/code> by:<\/p>\n<ul>\n<li>list_models()<\/li>\n<li>list()<\/li>\n<\/ul>\n<p>Using <code>list_models()<\/code> brings you a list of all the deployed models ids, while using <code>list()<\/code> only brings one, you can add filters such as <code>display_name<\/code>, <code>model_id<\/code>, <code>region<\/code>, etc.<\/p>\n<pre><code>list(\n    filter= \u2018display_name= \u201cdisplay_name\u201d\u2019,\n)\n<\/code><\/pre>\n<p>You also can get the <code>deployed_model_id<\/code> using the Cloud SDK.<\/p>\n<pre><code>gcloud ai models list --region=$REGION --filter=&quot;DISPLAY_NAME: $NAME&quot; | grep &quot;MODEL_ID&quot; | cut -f2 -d: | sed 's\/\\s\/\/'\n<\/code><\/pre>\n<p>Additionally, you can specify the <code>deployed_model_id<\/code> when you are deploying your model using Cloud SDK the command should look like:<\/p>\n<pre><code>gcloud ai endpoints deploy-model $endpoint --project=$project --region=$region --model=$model_id --display-name=$model_name --deployed-model-id=$deployed_model_id\n<\/code><\/pre>\n<p>There are some flags that are required when you deploy a model such as endpoint, project, region, model and display name. And there are others that are <a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/ai\/endpoints\/deploy-model#:%7E:text=the%20uploaded%20model.-,OPTIONAL%20FLAGS,-%2D%2Daccelerator%3D%5Bcount\" rel=\"nofollow noreferrer\">optional flags<\/a> that you can use deployed_model_id is one of them.(I don\u2019t know if this is possible but you could set the deployed_model_id as the same as the model_id).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.7,
        "Solution_reading_time":22.36,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":182.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing deployed model"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":32.9444641667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have been used Azure for the first time, and I am overwelmed by the huge quantity of information about Azure.    <\/p>\n<p>I think that the information about security on Azure is not unified.    <\/p>\n<p>For example, in <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/security\/fundamentals\/identity-management-best-practices\">Identity Management and access control security best practices<\/a> page, sometimes there are multiple best practices per one section header.    <br \/>\nHowever, in <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/storage\/blobs\/security-recommendations?toc=\/azure\/security\/fundamentals\/toc.json&amp;bc=\/azure\/security\/breadcrumb\/toc.json\">Security recommendations for Blob storage<\/a> page,security recommendations are documented in the form of table, one issue per one row.    <\/p>\n<p>I wish there was a cross-sectional, unified security check list for Azure as follows.    <\/p>\n<ul>\n<li> We could select Azure services we use.    <\/li>\n<li> When we select the services, the security check list are displayed or could be downloaded as text file.    <\/li>\n<li> The security check list are documented so that we can easily understand what we should do. (where on the Azure portal UI, which item, or how to do set the item which is related to security, etc)    <\/li>\n<\/ul>\n<p>I have used Azure services as follows.    <\/p>\n<ul>\n<li> Azure Data Factory    <\/li>\n<li> Azure Data Lake Storage Gen2    <\/li>\n<li> Azure Functions (App Service)    <\/li>\n<li> Azure Database for MySQL    <\/li>\n<li> Azure Machine Learning    <\/li>\n<li> Azure Monitor (for Application Insights)    <\/li>\n<\/ul>\n<p>Even if I take one service (for example, Azure Data Lake Storage Gen2), I think that I have to check at least two pages (<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/storage\/blobs\/security-recommendations?toc=\/azure\/security\/fundamentals\/toc.json&amp;bc=\/azure\/security\/breadcrumb\/toc.json\">here<\/a> and <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/security\/fundamentals\/paas-applications-using-storage\">here<\/a> ).    <br \/>\nHowever, I'm not sure if it's covered. Do you have any good ideas?    <\/p>\n<p>Regards.<\/p>",
        "Challenge_closed_time":1638553979008,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638435378937,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is overwhelmed by the amount of information available on Azure and finds that the security information is not unified. They wish for a cross-sectional, unified security checklist for Azure that would allow them to select the services they use and display or download the relevant security checklist. The user has used several Azure services and finds that they have to check at least two pages for each service to ensure security. They are seeking suggestions for a better approach.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/648921\/is-there-a-cross-sectional-unified-security-check",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":13.3,
        "Challenge_reading_time":27.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":32.9444641667,
        "Challenge_title":"Is there a cross-sectional, unified security check list for Azure?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":267,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=c624669d-08b4-4372-b158-6b43fc05d41a\">@Makoto Oda  <\/a>,    <\/p>\n<p>Thanks for using Microsoft Q&amp;A!!    <\/p>\n<p>I do not think that we have a single document which can provide you a consolidated view of security across all Azure services.  You may need to go through the documentation available for individual services to get the required information.  However, you can try checking - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/security\/\">Azure security documentation<\/a> and <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/architecture\/framework\/security\/overview\">Security considerations for Azure Architecture center<\/a> if this helps you getting anything specific you are looking in Azure at higher level.     <\/p>\n<p>Hope this helps.    <\/p>\n<p>Thanks    <br \/>\nSaurabh    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.8,
        "Solution_reading_time":10.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":93.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"unified security checklist"
    },
    {
        "Answerer_created_time":1435766573232,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":10645.0,
        "Answerer_view_count":1173.0,
        "Challenge_adjusted_solved_time":6.5547283333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is that possible to use machine learning methods from Microsoft Azure Machine Learning  as an API from my own code (without ML Studio) with possibility to calculate everything on their side?<\/p>",
        "Challenge_closed_time":1450317613532,
        "Challenge_comment_count":0,
        "Challenge_created_time":1450294016510,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of using Azure ML methods as an API from their own code without using ML Studio and with the ability to perform calculations on Azure's side.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34320449",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":2.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":6.5547283333,
        "Challenge_title":"Use Azure ML methods like an API",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":180.0,
        "Challenge_word_count":37,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1336227824220,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":834.0,
        "Poster_view_count":122.0,
        "Solution_body":"<p>You can <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-publish-a-machine-learning-web-service\/\" rel=\"nofollow\">publish<\/a> an experiment (machine learning functions you hooked together in Azure ML Studio) as an API. When you call that API in your custom code you give it your data and all the computation runs in the cloud in Azure ML. <\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.8,
        "Solution_reading_time":4.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":46.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"use Azure ML as API"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":39.1253477778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>While opening ml studio (classic) , shows a pop up message like it will retire on 31 August 2024 . Couldn't close the message<\/p>",
        "Challenge_closed_time":1650491049672,
        "Challenge_comment_count":1,
        "Challenge_created_time":1650350198420,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to open Machine Learning Studio (classic) as a pop-up message appears stating that it will retire on August 31, 2024, and they are unable to close the message.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/817073\/cant-open-machine-learning-studio-(classic)",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.6,
        "Challenge_reading_time":2.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":39.1253477778,
        "Challenge_title":"Can't open Machine learning studio (classic)",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":27,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=065b67d4-0031-46e3-b16b-02416c45d955\">@Asheekha  <\/a>     <\/p>\n<p>Sorry about your experience again, I checked internally about the issue and my colleague confirmed that this is a known issue of Chrome browser, which can be fixed by cleaning the cookie.     <\/p>\n<p>Please try to clean the cookies and I hope this helps! Please let me know if you are still blocked by this issue.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p><em>-Please kindly accept the answer to help the community, thanks a lot.<\/em>    <\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.1,
        "Solution_reading_time":6.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":79.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to open ML Studio"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":18.75,
        "Challenge_answer_count":2,
        "Challenge_body":"I have a model hosted on a Google Cloud endpoint and I would like to access it via the Java client.\u00a0 I've created a service account and a key for that service account with the , when I run my client code with the GOOGLE_APPLICATION_CREDENTIALS env var pointed to the key, I am able to call the service.\u00a0 When I try to authenticate explicitly using FixedCredentialProvider, it fails with an \"unauthenticated\" message.\u00a0\u00a0\n\nThe code is as follows\n\n```\n\nPredictionServiceSettings predictionServiceSettings =\n        PredictionServiceSettings.newBuilder().setEndpoint(location + \"-aiplatform.googleapis.com:443\")\n                .setCredentialsProvider(FixedCredentialsProvider.create(ServiceAccountCredentials.fromStream(new FileInputStream(\"\/Users\/ME\/Downloads\/XYZ.json\"))))\n                .build();\npredictionServiceClient = PredictionServiceClient.create(predictionServiceSettings);\nendpointName = EndpointName.of(project, location, endpointId);\nValue featureVal = Value.newBuilder().setStructValue(features).build();\nPredictResponse response =  predictionServiceClient.predict(\n        endpointName,\n        Collections.singletonList(featureVal),\n        Value.newBuilder().setNullValue(NullValue.NULL_VALUE).build());\n\n\n\n```",
        "Challenge_closed_time":1669109220000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1669041720000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to access a model hosted on a Google Cloud endpoint via the Java client. They have created a service account and a key for that service account, and when they run their client code with the GOOGLE_APPLICATION_CREDENTIALS env var pointed to the key, they are able to call the service. However, when they try to authenticate explicitly using FixedCredentialProvider, it fails with an \"unauthenticated\" message.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-can-I-explicitly-authenticate-to-the-ai-platform-using-the\/m-p\/491537#M833",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":20.5,
        "Challenge_reading_time":16.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":18.75,
        "Challenge_title":"How can I explicitly authenticate to the ai-platform using the java PredictionServiceClient",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":250.0,
        "Challenge_word_count":109,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi,\n\nUpon checking your code, FixedCredentialsProvider.create()\u00a0accepts\u00a0com.google.auth.Credentials\u00a0as a parameter. Can you try a Credentials object to\u00a0FixedCredentialsProvider.create()? See code below:\n\nGoogleCredentials credentials = GoogleCredentials.fromStream(new FileInputStream(\"\/Users\/ME\/Downloads\/XYZ.json\")).createScoped(Lists.newArrayList(\"https:\/\/www.googleapis.com\/auth\/cloud-platform\"));\n\n\u00a0If code above did not work, can you provide the stack trace of the error? Also what roles did you assign on your service account?\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.1,
        "Solution_reading_time":7.4,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":56.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"authentication failure"
    },
    {
        "Answerer_created_time":1421596186347,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":812.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":59.8581247222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In the SageMaker documentation, both <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-model-endpoints.html\" rel=\"nofollow noreferrer\">Multi-Model Endpoints<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-container-direct.html\" rel=\"nofollow noreferrer\">Multi-Container Endpoints with Direct Invocation<\/a> are described as very similar methods to host multiple models on a single endpoint. The given use cases appear identical except that <strong>Multi-Model Endpoints<\/strong> include many more advanced features.<\/p>\n<p>For example, <strong>Multi-Model Endpoints<\/strong> can host <em>n<\/em> number of models and support features such as resource sharing and model caching while <strong>Multi-Container Endpoints with Direct Invocation<\/strong> are limited to hosting only 5 models and lack model caching.<\/p>\n<p>When does it make sense to use <strong>Multi-Container Endpoints with Direct Invocation<\/strong> instead of <strong>Multi-Model Endpoints<\/strong>?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/2EQAA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2EQAA.png\" alt=\"Multi-Model Endpoint\" \/><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/A6jyS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/A6jyS.png\" alt=\"Multi-Container Endpoint with Direct Invocation\" \/><\/a><\/p>",
        "Challenge_closed_time":1630054299816,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629838066340,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking to understand the differences between Multi-Model Endpoints and Multi-Container Endpoints with Direct Invocation in SageMaker documentation. While both methods allow hosting multiple models on a single endpoint, Multi-Model Endpoints offer more advanced features such as hosting more models and supporting resource sharing and model caching. The user is questioning when it is appropriate to use Multi-Container Endpoints with Direct Invocation instead of Multi-Model Endpoints.",
        "Challenge_last_edit_time":1629838810567,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68913914",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":18.0,
        "Challenge_reading_time":18.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":60.0648544444,
        "Challenge_title":"Why Use Multi-Container Endpoints instead of Multi-Model Endpoints?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":224.0,
        "Challenge_word_count":127,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1531231343652,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"St. Louis, MO, USA",
        "Poster_reputation_count":676.0,
        "Poster_view_count":70.0,
        "Solution_body":"<p>If you want to serve multiple models from the same framework using the same endpoint then you can use multi-model endpoints. Due to using the same framework (e.g. only sklearn models), multi-model endpoints make it to the endpoint when they are called. You can have thousands of those models under one endpoint. Multi-container endpoints on the other hand allow serving models from multiple frameworks, e.g. one TensorFlow, one XGBoost and so on, with direct invocation again. However in this case there's <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-container-direct.html\" rel=\"nofollow noreferrer\">limit of 5 different models<\/a> on a single endpoint.<\/p>\n<p>So depending on the problem you are working, if you need to use multiple frameworks on a single endpoint then you will need to use multi-container endpoint with direct invocation. Otherwise you can use the multi-model endpoint.<\/p>\n<p><a href=\"https:\/\/towardsdatascience.com\/deploy-thousands-of-models-on-sagemaker-real-time-endpoints-with-automatic-retraining-pipelines-4eef7521d5a3\" rel=\"nofollow noreferrer\">Reference<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.4,
        "Solution_reading_time":14.35,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":134.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"compare endpoint types"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.4533425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained a linear regression model in AzureML studio which was created in designer as pipeline.    <\/p>\n<p>I could not able to see R square and adj-R square metric in Evaluate Model step.     <\/p>\n<p>Could any throw thoughts how can I add these 2 metrics to my trained model     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/89088-image.png?platform=QnA\" alt=\"89088-image.png\" \/>    <\/p>\n<p>Thanks    <br \/>\nBhaskar<\/p>",
        "Challenge_closed_time":1618877959740,
        "Challenge_comment_count":1,
        "Challenge_created_time":1618843927707,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a challenge in adding R square and adj-R square metric to their trained linear regression model in AzureML Studio. They are seeking suggestions on how to add these two metrics to their model.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/362850\/how-to-add-r2-and-adj-r2-metric-in-linear-regressi",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.8,
        "Challenge_reading_time":6.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":9.4533425,
        "Challenge_title":"How to add r2 and adj r2 metric in linear regression model - AzureML Studio?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":71,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello,    <\/p>\n<p>Sorry  for the confusing. Actually, Coefficient of determination, often referred to as R2, represents the predictive power of the model as a value between 0 and 1. Zero means the model is random (explains nothing); 1 means there is a perfect fit. However, caution should be used in interpreting R2 values, as low values can be entirely normal and high values can be suspect in Azure Machine Learning Designer.    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/evaluate-model#metrics-for-regression-models\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/evaluate-model#metrics-for-regression-models<\/a>    <\/p>\n<p>Regards,    <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.6,
        "Solution_reading_time":9.74,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":78.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"add R square metric"
    },
    {
        "Answerer_created_time":1520413126203,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":37123.0,
        "Answerer_view_count":4058.0,
        "Challenge_adjusted_solved_time":47.4339141667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I trained a model in AZURE ML. Now i want to use that model in my ios app to predict the output\u00a0.<\/p>\n\n<p>How to download the model from AZURE and use it my swift code.<\/p>",
        "Challenge_closed_time":1525849618928,
        "Challenge_comment_count":0,
        "Challenge_created_time":1525678856837,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a model in AZURE ML and wants to use it in their iOS app to predict output. They are seeking guidance on how to download the model from AZURE and use it in their Swift code.",
        "Challenge_last_edit_time":1558224843256,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50209284",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":3.0,
        "Challenge_reading_time":2.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":47.4339141667,
        "Challenge_title":"How to use the trained model developed in AZURE ML",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":516.0,
        "Challenge_word_count":45,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1510206999776,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":81.0,
        "Poster_view_count":41.0,
        "Solution_body":"<p>As far as I know, the model could run in <strong>Azure Machine Learning Studio<\/strong>.It seems that you are unable to download it, the model could do nothing outside of Azure ML. <\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/41236871\/how-to-download-the-trained-models-from-azure-machine-studio\">Here<\/a> is a similar post for you to refer, I have also tried @Ahmet's \nmethod, but result is like @mrjrdnthms says.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1525850503192,
        "Solution_link_count":1.0,
        "Solution_readability":8.6,
        "Solution_reading_time":5.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"use Azure ML model in iOS"
    },
    {
        "Answerer_created_time":1488711530187,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":29.0,
        "Answerer_view_count":12.0,
        "Challenge_adjusted_solved_time":21.4251527778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using a Classic Web Service with a non-default endpoint for a Update Resource activity on the Azure Data Factory. This is the error I get:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/shK0R.png\" rel=\"nofollow noreferrer\">Screenshot of Error<\/a><\/p>\n\n<p>I didn't find any info on the web and couldn't figure it out myself. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/data-factory\/data-factory-azure-ml-update-resource-activity\" rel=\"nofollow noreferrer\">This<\/a> website shows an example that I used by just filling in my values for mlEndpoint, apiKey and updateRessourceEndpoint:<\/p>\n\n<pre><code>{\n    \"name\": \"updatableScoringEndpoint2\",\n    \"properties\": {\n        \"type\": \"AzureML\",\n        \"typeProperties\": {\n            \"mlEndpoint\": \"https:\/\/ussouthcentral.services.azureml.net\/workspaces\/xxx\/services\/--scoring experiment--\/jobs\",\n            \"apiKey\": \"endpoint2Key\",\n            \"updateResourceEndpoint\": \"https:\/\/management.azureml.net\/workspaces\/xxx\/webservices\/--scoring experiment--\/endpoints\/endpoint2\"\n        }\n    }\n}\n<\/code><\/pre>\n\n<p>There is no mention of a token that needs to be passed...<\/p>",
        "Challenge_closed_time":1503393687923,
        "Challenge_comment_count":0,
        "Challenge_created_time":1503316557373,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while using a Classic Web Service with a non-default endpoint for an Update Resource activity on the Azure Data Factory. The error message is displayed in the provided screenshot, and the user has not been able to find any information on the web or figure it out themselves. The user has followed an example from a website by filling in their values for mlEndpoint, apiKey, and updateResourceEndpoint, but there is no mention of a token that needs to be passed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45796489",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":14.9,
        "Challenge_reading_time":14.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":21.4251527778,
        "Challenge_title":"Azure Machine Learning: What error is this?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":253.0,
        "Challenge_word_count":103,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1476806455803,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Holzkirchen, Deutschland",
        "Poster_reputation_count":3068.0,
        "Poster_view_count":386.0,
        "Solution_body":"<p>this error is basically saying the apiKey you provided is invalid to perform the update resource operation. Here is some posts for your reference: <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/3bb77e37-8860-43c6-bcaa-d6ebd70617b8\/retrain-predictive-web-service-programmatically-when-do-not-have-access-to-managementazuremlnet?forum=MachineLearning\" rel=\"nofollow noreferrer\">https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/3bb77e37-8860-43c6-bcaa-d6ebd70617b8\/retrain-predictive-web-service-programmatically-when-do-not-have-access-to-managementazuremlnet?forum=MachineLearning<\/a><\/p>\n\n<p>Please also be noted that if you modified your linked service in ADF, remember to re-deploy the pipeline as well to reflect your change in time.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":25.1,
        "Solution_reading_time":10.4,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"error in non-default endpoint"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1111111111,
        "Challenge_answer_count":1,
        "Challenge_body":"Does SageMaker Multi-Model Endpoint support SageMaker Model Monitor?",
        "Challenge_closed_time":1590501508000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590501108000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring whether SageMaker Multi-Model Endpoint supports SageMaker Model Monitor.",
        "Challenge_last_edit_time":1668554201782,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUq2z-BEt7TnmZ8vFYs-Hu7g\/does-sagemaker-multi-model-endpoint-support-sagemaker-model-monitor",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":19.2,
        "Challenge_reading_time":1.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":0.1111111111,
        "Challenge_title":"Does SageMaker Multi-Model Endpoint support SageMaker Model Monitor?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":248.0,
        "Challenge_word_count":15,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Amazon SageMaker Model Monitor currently supports only endpoints that host a single model and does not support monitoring multi-model endpoints. For information on using multi-model endpoints, see Host Multiple Models with Multi-Model Endpoints .  https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor.html",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925566336,
        "Solution_link_count":1.0,
        "Solution_readability":20.5,
        "Solution_reading_time":4.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":34.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"support for Model Monitor"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.6422222222,
        "Challenge_answer_count":1,
        "Challenge_body":"I have a customer asking me about the [Rendezvous architecture](https:\/\/towardsdatascience.com\/rendezvous-architecture-for-data-science-in-production-79c4d48f12b). What I'm thinking is, we could implement this in a number of ways, all using endpoint variants:\n\n- Lambda (and probably SQS) around the endpoint;\n- A custom monitoring job;\n- Step Functions\n\nWithout going into details of the above options or of how the evaluation and SLA check will be done, it looks like the several models would fit very well as variants of an endpoint. The thing is, the architecture expects to call them all. Is there a way to directly call all variants of a model, or will a wrapper to identify the variants, call them all and process the results be needed?",
        "Challenge_closed_time":1604506964000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604486652000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is exploring the implementation of Rendezvous architecture using endpoint variants, such as Lambda, SQS, custom monitoring job, and Step Functions. However, the architecture expects to call all variants of a model, and the user is unsure if there is a way to directly call all variants or if a wrapper is needed to identify the variants, call them all, and process the results.",
        "Challenge_last_edit_time":1667925743687,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU6bm-EMtOQV6robgbTXClLQ\/running-a-request-against-all-variants-in-an-endpoint",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":9.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":5.6422222222,
        "Challenge_title":"Running a request against all variants in an endpoint",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":40.0,
        "Challenge_word_count":121,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"When I last looked into it, it was not possible to query all versions\/variants of the model automatically. You can specify what variant to use when using the `invoke_endpoint` method. I would therefore write a lambda function to invoke each of the endpoints one-by-one (see here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_runtime_InvokeEndpoint.html). To be especially rigorous about it, you can add a function in your lambda code that first retrieves all the endpoint variants (see here: https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.describe_endpoint) then queries them one-by-one, and returns all the results.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1607685345047,
        "Solution_link_count":2.0,
        "Solution_readability":14.3,
        "Solution_reading_time":9.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":81.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"call all variants"
    },
    {
        "Answerer_created_time":1517578984080,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2090.0,
        "Answerer_view_count":163.0,
        "Challenge_adjusted_solved_time":1.8957083334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been playing with Amazon Sagemaker. They have amazing sample notebooks in different areas. However, for testing purposes, I want to create an endpoint that returns the result from a function. From what I have seen so far, my understanding is that we can deploy only models but I would like to clarify it.<\/p>\n\n<p>Let's say I want to invoke the endpoint and it should give me the square of the input value. So, I will first create a function:<\/p>\n\n<pre><code>def my_square(x):\n    return x**2\n<\/code><\/pre>\n\n<p>Can we deploy this simple function in Amazon Sagemaker?<\/p>",
        "Challenge_closed_time":1534153182207,
        "Challenge_comment_count":0,
        "Challenge_created_time":1534146357657,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user wants to know if it is possible to deploy a simple function, such as a square function, to Amazon Sagemaker as an endpoint for testing purposes. They are unsure if Sagemaker only allows for the deployment of models.",
        "Challenge_last_edit_time":1534368379667,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51817494",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":7.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.8957083334,
        "Challenge_title":"deploy a simple function to amazon sagemaker",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":216.0,
        "Challenge_word_count":101,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1429147641928,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2282.0,
        "Poster_view_count":264.0,
        "Solution_body":"<p>Yes this is possible but it will need some overhead:\nYou can pass your own docker images for training and inference to sagemaker.<\/p>\n\n<p>Inside this containers you can do anything you want including return your <code>my_square<\/code> function. Keep in mind that you have to write your own flask microservice including proxy and wsgi server(if needed).<\/p>\n\n<p>In my opinion <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">this example<\/a> is the most helpfull one.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.1,
        "Solution_reading_time":7.77,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":68.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"deploy function to Sagemaker"
    },
    {
        "Answerer_created_time":1351154914716,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2564.0,
        "Answerer_view_count":451.0,
        "Challenge_adjusted_solved_time":25.1791094445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>i am trying to deploy a model in Vertex AI pipeline component using <code>DeployModelRequest<\/code>. I try to get the model using <code>GetModelRequest<\/code><\/p>\n<pre><code>    model_name = f'projects\/{project}\/locations\/{location}\/models\/{model_id}'\n    model_request = aiplatform_v1.types.GetModelRequest(name=model_name)\n    model_info = client_model.get_model(request=model_request)       \n    \n    deploy_request = aiplatform_v1.types.DeployModelRequest(endpoint=end_point, \n                                                                deployed_model=model_info)\n    client.deploy_model(request=deploy_request)\n<\/code><\/pre>\n<p>but this gives:<\/p>\n<pre><code>TypeError: Parameter to MergeFrom() must be instance of same class: expected \ngoogle.cloud.aiplatform.v1.DeployedModel got Model\n<\/code><\/pre>\n<p>I have also tried <code>deployed_model=model_info.deployed_models[0]<\/code> but this gave:<\/p>\n<pre><code>TypeError: Parameter to MergeFrom() must be instance of same class: expected\n google.cloud.aiplatform.v1.DeployedModel got DeployedModelRef.\n<\/code><\/pre>\n<p>So what do I use for <code>deployed_model<\/code>?<\/p>",
        "Challenge_closed_time":1663344904267,
        "Challenge_comment_count":6,
        "Challenge_created_time":1663254259473,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while trying to deploy a model in Vertex AI pipeline component using DeployModelRequest. They are trying to get the model using GetModelRequest, but it is giving a TypeError stating that the parameter to MergeFrom() must be an instance of the same class. The user has also tried using deployed_model=model_info.deployed_models[0], but it is giving another TypeError stating that the expected class is DeployedModel, but it got DeployedModelRef. The user is seeking guidance on what to use for deployed_model.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73733464",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":20.8,
        "Challenge_reading_time":15.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":25.1791094445,
        "Challenge_title":"What should be used for deployed_model in DeployModelRequest in Vertex AI pipeline?",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":39.0,
        "Challenge_word_count":93,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>As simple as:<\/p>\n<pre><code>machine_spec = MachineSpec(machine_type=&quot;n1-standard-2&quot;)\ndedicated_resources = DedicatedResources(machine_spec=machine_spec, \n                                         min_replica_count=1, \n                                         max_replica_count=1)\ndepmodel= DeployedModel(model=model_name, dedicated_resources=dedicated_resources) \n\ndeploy_request = aiplatform_v1.types.DeployModelRequest(\n                   endpoint=end_point, deployed_model=depmodel, \n                   traffic_split={'0':100})\nclient.deploy_model(request=deploy_request)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":49.5,
        "Solution_reading_time":6.68,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":19.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"TypeError in deploying model"
    },
    {
        "Answerer_created_time":1300443398976,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Switzerland",
        "Answerer_reputation_count":681.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":101.0100211111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to deploy a TorchServe instance on Google Vertex AI platform but as per their documentation (<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#response_requirements\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#response_requirements<\/a>), it requires the responses to be of the following shape:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;predictions&quot;: PREDICTIONS\n}\n<\/code><\/pre>\n<p>Where <strong>PREDICTIONS<\/strong> is an array of JSON values representing the predictions that your container has generated.<\/p>\n<p>Unfortunately, when I try to return such a shape in the <code>postprocess()<\/code> method of my custom handler, as such:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def postprocess(self, data):\n    return {\n        &quot;predictions&quot;: data\n    }\n<\/code><\/pre>\n<p>TorchServe returns:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;code&quot;: 503,\n  &quot;type&quot;: &quot;InternalServerException&quot;,\n  &quot;message&quot;: &quot;Invalid model predict output&quot;\n}\n<\/code><\/pre>\n<p>Please note that <code>data<\/code> is a list of lists, for example: [[1, 2, 1], [2, 3, 3]]. (Basically, I am generating embeddings from sentences)<\/p>\n<p>Now if I simply return <code>data<\/code> (and not a Python dictionary), it works with TorchServe but when I deploy the container on Vertex AI, it returns the following error:  <code>ModelNotFoundException<\/code>. I assumed Vertex AI throws this error since the return shape does not match what's expected (c.f. documentation).<\/p>\n<p>Did anybody successfully manage to deploy a TorchServe instance with custom handler on Vertex AI?<\/p>",
        "Challenge_closed_time":1633278542092,
        "Challenge_comment_count":2,
        "Challenge_created_time":1632907406933,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while deploying a TorchServe instance on Google Vertex AI platform. The platform requires the responses to be in a specific shape, which is not being generated by the user's custom handler. The user is trying to return the required shape in the postprocess() method of the custom handler, but it is resulting in an error. When the user returns the data without a Python dictionary, it works with TorchServe, but it throws a ModelNotFoundException error on Vertex AI. The user is seeking help to deploy a TorchServe instance with a custom handler on Vertex AI.",
        "Challenge_last_edit_time":1632914906016,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69373666",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":18.5,
        "Challenge_reading_time":23.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":103.0930997222,
        "Challenge_title":"Deployment with customer handler on Google Cloud Vertex AI",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":433.0,
        "Challenge_word_count":191,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1300443398976,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Switzerland",
        "Poster_reputation_count":681.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>Actually, making sure that the TorchServe processes correctly the input dictionary (instances) solved the issue. It seems like what's on the <a href=\"https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/pytorch-google-cloud-how-deploy-pytorch-models-vertex-ai\" rel=\"nofollow noreferrer\">article<\/a> did not work for me.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.6,
        "Solution_reading_time":4.5,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":30.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"deployment error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":150.3343644444,
        "Challenge_answer_count":11,
        "Challenge_body":"<p>I can't use ml real-time inference endpoint becouse it's stuck on transitioning status (more than 20 hours). Could you help me with that?<\/p>",
        "Challenge_closed_time":1600780351532,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600239147820,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with Azure ML real-time inference endpoint deployment as it is stuck on transitioning status for more than 20 hours. They are seeking assistance to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/96645\/azure-ml-real-time-inference-endpoint-deloyment-st",
        "Challenge_link_count":0,
        "Challenge_participation_count":11,
        "Challenge_readability":8.6,
        "Challenge_reading_time":2.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":150.3343644444,
        "Challenge_title":"Azure ML real-time inference endpoint deloyment stuck on transitioning status",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":32,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I've chacked in on some different algorithms and the issue appears when i'm using n-grams block for getting features. When i'm using feature hashing for example it looks like working well.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":2.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":31.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"stuck deployment status"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.7795147222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hallo, <\/p>\n<p>I have a student version of azure account. So, I want to classify images (e.g. cat vs dog) with Azure Machine Learning. I used data labeling and automatedML for the task. I registered my model and I tried to deploy it by means of Real-time endpoint. I use for that the standard_F2s__v2 VM, because for other VMs I don't have enough quota.<\/p>\n<p>During deployment I get this error (see below). Do you know what can I do? what's the problem? the VM or scripts (docker etc.) which are generated by azure? <\/p>\n<p>Thanks for answers!<\/p>\n<blockquote>\n<p>Instance status:\nSystemSetup: Succeeded\nUserContainerImagePull: Succeeded\nModelDownload: Succeeded\nUserContainerStart: InProgress\nContainer events:\nKind: Pod, Name: Downloading, Type: Normal, Time: 2023-05-05T18:52:51.640736Z, Message: Start downloading models\nKind: Pod, Name: Pulling, Type: Normal, Time: 2023-05-05T18:52:51.939809Z, Message: Start pulling container image\nKind: Pod, Name: Pulled, Type: Normal, Time: 2023-05-05T18:53:18.896965Z, Message: Container image is pulled successfully\nKind: Pod, Name: Downloaded, Type: Normal, Time: 2023-05-05T18:53:18.896965Z, Message: Models are downloaded successfully\nKind: Pod, Name: Created, Type: Normal, Time: 2023-05-05T18:53:18.983732Z, Message: Created container inference-server\nKind: Pod, Name: Started, Type: Normal, Time: 2023-05-05T18:53:19.121257Z, Message: Started container inference-server\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-05-05T18:53:33.609235Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-05-05T18:53:44.184435Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-05-05T18:53:53.609086Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-05-05T18:54:03.608893Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-05-05T18:54:13.608775Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-05-05T18:54:23.608705Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\nContainer logs:\n2023-05-05T18:53:19,317469780+00:00 - rsyslog\/run \n2023-05-05T18:53:19,320703107+00:00 - gunicorn\/run \n2023-05-05T18:53:19,322375521+00:00 | gunicorn\/run | \n2023-05-05T18:53:19,324133736+00:00 | gunicorn\/run | ###############################################\n2023-05-05T18:53:19,325868850+00:00 | gunicorn\/run | AzureML Container Runtime Information\n2023-05-05T18:53:19,328116769+00:00 | gunicorn\/run | ###############################################\n2023-05-05T18:53:19,331154294+00:00 | gunicorn\/run | \n2023-05-05T18:53:19,333284612+00:00 | gunicorn\/run | \n2023-05-05T18:53:19,341554081+00:00 | gunicorn\/run | AzureML image information: mlflow-ubuntu20.04-py38-cpu-inference:20230404.v14\n2023-05-05T18:53:19,343354796+00:00 | gunicorn\/run | \n2023-05-05T18:53:19,345147111+00:00 | gunicorn\/run | \n2023-05-05T18:53:19,346924426+00:00 | gunicorn\/run | PATH environment variable: \/opt\/miniconda\/envs\/amlenv\/bin:\/opt\/miniconda\/bin:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\n2023-05-05T18:53:19,348632240+00:00 | gunicorn\/run | PYTHONPATH environment variable: \n2023-05-05T18:53:19,350929659+00:00 | gunicorn\/run | \n2023-05-05T18:53:19,358940326+00:00 - nginx\/run \nnginx: [warn] the &quot;user&quot; directive makes sense only if the master process runs with super-user privileges, ignored in \/etc\/nginx\/nginx.conf:1\n2023-05-05T18:53:21,047186621+00:00 | gunicorn\/run | CONDAPATH environment variable: \/opt\/miniconda<\/p>\n<h1 id=\"conda-environments\">conda environments:<\/h1>\n<h1 id=\"section\"><\/h1>\n<p>base                     \/opt\/miniconda\namlenv                   \/opt\/miniconda\/envs\/amlenv\n2023-05-05T18:53:22,109803493+00:00 | gunicorn\/run | \n2023-05-05T18:53:22,111660209+00:00 | gunicorn\/run | Pip Dependencies (before dynamic installation)\nazure-core==1.26.3\nazure-identity==1.12.0\nazureml-inference-server-http==0.8.3\ncachetools==5.3.0\ncertifi==2022.12.7\ncffi==1.15.1\ncharset-normalizer==3.1.0\nclick==8.1.3\ncryptography==40.0.1\nFlask==2.2.3\nFlask-Cors==3.0.10\ngoogle-api-core==2.11.0\ngoogle-auth==2.17.1\ngoogleapis-common-protos==1.59.0\ngunicorn==20.1.0\nidna==3.4\nimportlib-metadata==6.1.0\ninference-schema==1.5.1\nitsdangerous==2.1.2\nJinja2==3.1.2\nMarkupSafe==2.1.2\nmsal==1.21.0\nmsal-extensions==1.0.0\nopencensus==0.11.2\nopencensus-context==0.1.3\nopencensus-ext-azure==1.1.9\nportalocker==2.7.0\nprotobuf==4.22.1\npsutil==5.9.4\npyasn1==0.4.8\npyasn1-modules==0.2.8\npycparser==2.21\npydantic==1.10.7\nPyJWT==2.6.0\npython-dateutil==2.8.2\npytz==2023.3\nrequests==2.28.2\nrsa==4.9\nsix==1.16.0\ntyping_extensions==4.5.0\nurllib3==1.26.15\nWerkzeug==2.2.3\nwrapt==1.12.1\nzipp==3.15.0\n2023-05-05T18:53:23,247714179+00:00 | gunicorn\/run | \n2023-05-05T18:53:23,249556992+00:00 | gunicorn\/run | Entry script directory: \/var\/mlflow_resources\/.\n2023-05-05T18:53:23,251367404+00:00 | gunicorn\/run | \n2023-05-05T18:53:23,253148416+00:00 | gunicorn\/run | ###############################################\n2023-05-05T18:53:23,254978929+00:00 | gunicorn\/run | Dynamic Python Package Installation\n2023-05-05T18:53:23,256700340+00:00 | gunicorn\/run | ###############################################\n2023-05-05T18:53:23,258536553+00:00 | gunicorn\/run | \n2023-05-05T18:53:23,260471566+00:00 | gunicorn\/run | Updating conda environment from \/var\/azureml-app\/azureml-models\/trained_05052023\/1\/mlflow-model\/conda.yaml !\nRetrieving notices: ...working... done\n.\/run: line 152:    62 Killed                  conda env create -n userenv -f &quot;${CONDA_FILENAME}&quot;\nCollecting package metadata (repodata.json): ...working... Error occurred. Sleeping to send error logs.\n2023-05-05T18:54:29,187641958+00:00 - gunicorn\/finish 95 0\n2023-05-05T18:54:29,189598769+00:00 - Exit code 95 is not normal. Killing image.<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1683360869436,
        "Challenge_comment_count":0,
        "Challenge_created_time":1683350863183,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while deploying their ML model using Real-time endpoint on Azure Machine Learning. The error message indicates that the readiness probe failed with status code 502, and the logs show an error occurred while updating the conda environment from the conda.yaml file. The user is unsure whether the issue is with the VM or the scripts generated by Azure.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1276938\/ml-model-deployment-(endpoints)-throws-error",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.0,
        "Challenge_reading_time":81.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":2.7795147222,
        "Challenge_title":"ML Model Deployment (Endpoints) throws error",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":504,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello @<strong>student2023 !<\/strong><\/p>\n<p>it seems that the Container has some errors<\/p>\n<p>Can you post the steps of the Process as you did it ?<\/p>\n<p>Also check on Azure , is the Container Healthy ?<\/p>\n<p>Is this a lab you found or your own ? <\/p>\n<p>Come back to see your feedback !<\/p>\n<hr \/>\n<p>Kindly mark the answer as accepted in case it helped or post your feedback to help !<\/p>\n<p>Regards<\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.1,
        "Solution_reading_time":5.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":69.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"failed readiness probe"
    },
    {
        "Answerer_created_time":1254957460063,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"North Carolina, USA",
        "Answerer_reputation_count":2484.0,
        "Answerer_view_count":362.0,
        "Challenge_adjusted_solved_time":2.6018694445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I used Machine learning tutorial: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/create-experiment\" rel=\"nofollow noreferrer\">Create your first data science experiment in Azure Machine Learning Studio<\/a> to create an <code>Experiment<\/code> and then converted it to a <code>predictive experiment<\/code>. Now I'm trying to deploy it as a Web Service by following this article that was referenced in the above article: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/publish-a-machine-learning-web-service#deploy-it-as-a-web-service\" rel=\"nofollow noreferrer\">Deploy it as a web service<\/a>. But when I click on <code>Run<\/code> and then on <code>Deploy Web Service<\/code>, I don't see the <code>Price Plan<\/code> dropdown and <code>Plan Name<\/code> input box etc as mentioned in the section <code>Machine Learning Web Service portal Deploy Experiment Page<\/code> of the second article above. After I clicked on Deploy Web Service link in ML studio, I got the page shown below.<strong>Question<\/strong>: What I may be doing wrong?<\/p>\n\n<p>Note: You can click on the picture to get a larger view.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/G3TKo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/G3TKo.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":1526322971967,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526313605237,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy an Azure ML experiment as a web service by following a tutorial, but is unable to see the Price Plan dropdown and Plan Name input box as mentioned in the tutorial. The user is seeking help to understand what they may be doing wrong.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50334563",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":12.1,
        "Challenge_reading_time":18.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":2.6018694445,
        "Challenge_title":"Deployment of an Azure ML Experiment as a Web Service through Azure Machine Learning Studio",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":330.0,
        "Challenge_word_count":162,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1330144099340,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":19815.0,
        "Poster_view_count":2272.0,
        "Solution_body":"<p>I think it depends on what workspace you're in. If you're in the free one then you get the screen that you already get, but if you create a workspace in the Azure portal and use that one, then you will get a screen like below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/drRpa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/drRpa.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>To create a new workspace, in the Azure Portal, create a new \"Machine Learning Studio Workspace\" and when you go to Azure ML Studio select the new workspace from the top right.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.7,
        "Solution_reading_time":7.3,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":87.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing dropdown and input box"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":46.8695180555,
        "Challenge_answer_count":2,
        "Challenge_body":"I want to allowlist the Sagemaker studio IP so people can access certain allowlisted services from Sagemaker. I created a sagemaker domain in my private subnet of my VPC, so theoretically it should use the IP of the associated NAT gateway, right? But I see a different IP \ud83e\udd14",
        "Challenge_closed_time":1675200089343,
        "Challenge_comment_count":0,
        "Challenge_created_time":1674683552396,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user wants to allowlist the Sagemaker studio IP for accessing certain services, but is facing challenges as the IP address being used is different from the associated NAT gateway.",
        "Challenge_last_edit_time":1675031393875,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUX7n9V0osTAmdmLYM21vmKQ\/how-to-allowlist-sagemaker-ip",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.5,
        "Challenge_reading_time":3.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":143.4824852778,
        "Challenge_title":"How to allowlist sagemaker IP?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":131.0,
        "Challenge_word_count":52,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I should've read through my terraform code that created Sagemaker more carefully, I specified a VPC so I thought it would be *in* the VPC but it turns out I needed to specify the AppNetworkAccessType too.\n\nbad:\n```hcl\nresource \"aws_sagemaker_domain\" \"my_domain\" {\n  domain_name = var.domain_name\n  auth_mode   = \"IAM\"\n  vpc_id      = var.vpc_id\n  subnet_ids  = var.subnet_ids\n```\n\ngood:\n```hcl\nresource \"aws_sagemaker_domain\" \"my_domain\" {\n  domain_name = var.domain_name\n  auth_mode   = \"IAM\"\n  vpc_id      = var.vpc_id\n  subnet_ids  = var.subnet_ids\n  app_network_access_type = \"VpcOnly\"\n```",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1675200124140,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":7.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"IP address mismatch"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":22.6043166667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm following a tutorial (<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python<\/a>) on how to deploy a model to Azure, and I had a few questions that have had confused a bit. I had a ready model that I trained using a notebook in Azure ML and have saved the model in a folder (as .h5) in my compute directory (Users\/username\/projectname\/models).    <\/p>\n<p>1- Can I deploy from the Azure ML Notebook section? So I create a .py file (or can I do it in a .ipynb notebook?), connect to my workspace, and register the model through there? I have my model stored in the models folder, so can I just reference that from an <code>azureml.core.Run<\/code> object?    <\/p>\n<p>2- When I create my entry scripts and inference and deployment configurations, do they have to be in separate files or does that not matter? Same for the code to deploy the model.    <\/p>\n<p>3- What model extensions are supported? Is .h5 fine?    <\/p>\n<p>4- When I deploy successfully, do I get an endpoint or uri I can connect to from anywhere?    <\/p>\n<p>I know this is a bit all over the place, but any clarifications would be appreciated. <\/p>",
        "Challenge_closed_time":1633027344787,
        "Challenge_comment_count":0,
        "Challenge_created_time":1632945969247,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is following a tutorial on how to deploy a model to Azure ML and has some questions regarding the deployment process. They are unsure if they can deploy from the Azure ML Notebook section, whether the entry scripts and deployment configurations need to be in separate files, what model extensions are supported, and if they will get an endpoint or URI after successful deployment.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/571518\/deploying-model-in-azure-ml-confusion",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.4,
        "Challenge_reading_time":15.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":22.6043166667,
        "Challenge_title":"Deploying model in Azure ML confusion",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":196,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. Here's the workflow for deploying a model:    <\/p>\n<ol>\n<li> Register the model    <\/li>\n<li> Prepare an entry script    <\/li>\n<li> Prepare an inference configuration    <\/li>\n<li> Deploy the model locally to ensure everything works    <\/li>\n<li> Choose a compute target    <\/li>\n<li> Re-deploy the model to the cloud    <\/li>\n<li> Test the resulting web service    <\/li>\n<\/ol>\n<p>You can perform the above steps through AML notebooks. However, you <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python#define-a-dummy-entry-script\">entry script<\/a> and <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python#define-an-inference-configuration\">deployment configuration<\/a> need to be in separate files. After <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python#deploy-your-machine-learning-model\">deployment<\/a>, you obtain an endpoint for <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-consume-web-service?tabs=python\">calling the webservice<\/a>. Model with extension .h5 is supported.    <\/p>\n<p>You can create new or reference an existing environment in your config, here's information on how to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments\">create\/use software environments<\/a>. Also, here's another <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/ml-frameworks\/tensorflow\/train-hyperparameter-tune-deploy-with-tensorflow\/train-hyperparameter-tune-deploy-with-tensorflow.ipynb\">example<\/a> (Deploy the model in ACI section) of how to create a scoring script. Please review the following <a href=\"https:\/\/www.tensorflow.org\/guide\/keras\/save_and_serialize\">document<\/a> for details on how to save and load Keras models.    <\/p>\n<pre><code>%%writefile score.py  \nimport json  \nimport numpy as np  \nimport os  \nimport tensorflow as tf  \n  \nfrom azureml.core.model import Model  \n  \ndef init():  \n    global tf_model  \n    model_root = os.getenv('AZUREML_MODEL_DIR')  \n    # the name of the folder in which to look for tensorflow model files  \n    tf_model_folder = 'model'  \n      \n    tf_model = tf.saved_model.load(os.path.join(model_root, tf_model_folder))  \n  \ndef run(raw_data):  \n    data = np.array(json.loads(raw_data)['data'], dtype=np.float32)  \n      \n    # make prediction  \n    out = tf_model(data)  \n    y_hat = np.argmax(out, axis=1)  \n  \n    return y_hat.tolist()  \n<\/code><\/pre>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":16.4,
        "Solution_reading_time":32.7,
        "Solution_score_count":1.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":219.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"model deployment questions"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":65.0577777778,
        "Challenge_answer_count":1,
        "Challenge_body":"Is it possible to deploy a model created by SageMaker in one account to an IoT Greengrass device in a different account?",
        "Challenge_closed_time":1556529654000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1556295446000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of deploying a SageMaker model from one account to an IoT Greengrass device in a different account.",
        "Challenge_last_edit_time":1667925988660,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUJha7KbxOTXuhMRGbMYGC0g\/deploy-sagemaker-model-to-iot-greengrass-in-different-account",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":2.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":65.0577777778,
        "Challenge_title":"Deploy SageMaker model to IoT Greengrass in different account?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":82.0,
        "Challenge_word_count":30,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"For IoT Greengrass 1.x, this is possible but not trivial. From the console this is not possible, as you can only select buckets or SageMaker jobs from the same account, but you can refer to resources in other accounts if you use the CLI or the API. \n\nYou have to create a new Resource Definition Version with the correct data specifying the model resource and then add it to your group definition. For permissions in the source account, you must set up the S3 bucket policy to allow access from the destination account. For permissions in the destination account, you must update the IoT Greengrass service role policy to access the model resource in the source account.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1611605697668,
        "Solution_link_count":0.0,
        "Solution_readability":9.7,
        "Solution_reading_time":8.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":117.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"deploy SageMaker to IoT Greengrass"
    },
    {
        "Answerer_created_time":1512770138847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":1.5377747222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I've been wrapped around this problem for a while and can't seem to understand where this issue is coming from. I'm deploying a model on Sagemaker and I get the error on this line of code:<\/p>\n<p><code>sm_model.deploy(initial_instance_count=1, instance_type='ml.m4.2xlarge', endpoint_name=endpoint_name)<\/code><\/p>\n<p>Jupyter Notebook outputs the error below. Note: Line 269 isn't code in my Notebook, it is just a reference I get as a result of my model.deploy command above.<\/p>\n<pre><code>---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n\n    267             return self.image\n    268 \n--&gt; 269         region_name = self.sagemaker_session.boto_region_name\n    270         return create_image_uri(\n    271             region_name,\n\nAttributeError: 'NoneType' object has no attribute 'boto_region_name'\n<\/code><\/pre>\n<p>Edit: This is just an example dataset that I'm using to create this pipeline. This is on a sagemaker notebook instance. I'm adding the entire code for clarification below.<\/p>\n<pre><code>from sagemaker.model import Model\nfrom sagemaker.pipeline import PipelineModel\nimport boto3\nfrom time import gmtime, strftime\n\nimport boto3\nimport s3fs\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\nsns.set()\n\n\n\nimport boto3\nimport sagemaker\nfrom sagemaker import get_execution_role\n\n# Using Amazon S3\ns3 = boto3.client('s3')\nsage = boto3.client('sagemaker')\n\nsession = boto3.session.Session()    \nsagemaker_session = sagemaker.Session()\n\n# Get a SageMaker-compatible role used by this Notebook Instance.\nrole = get_execution_role()\n\n\n#Upload file using AWS session\n# S3 prefix\n\nprefix = 'Scikit-keras-NLP-pipeline-Boston-Housing-example-June08-test1'\n\n\ntrain_input = sagemaker_session.upload_data(\n    path='housing.csv', \n    bucket=bucket,\n    key_prefix='{}\/{}'.format(prefix, 'train'))\n\n\n\nfrom sagemaker.sklearn.estimator import SKLearn\noutput_dir = 's3:\/\/sagemaker-us-east-1-819182027957\/Scikit-keras-NLP-pipeline-Boston-Housing-example-July08-test1\/train'\nmodel_dir = 's3:\/\/sagemaker-us-east-1-819182027957\/Scikit-keras-NLP-pipeline-Boston-Housing-example-June08-test1\/train'\n\nscript_path = 'Boston.py'\n\nsklearn_preprocessor = SKLearn(\n    entry_point=script_path,\n    role=role,\n    train_instance_type=&quot;ml.c4.xlarge&quot;,\n    sagemaker_session=sagemaker_session,\n    output_path=output_dir)\n\n\nsklearn_preprocessor.fit({'train': train_input,'model-dir':model_dir,'output-data-dir':output_dir})\n\n\n\n\n\n\nfrom sagemaker.tensorflow.serving import Model\nsagemaker_estimator = Model(model_data = 's3:\/\/' + sagemaker_session.default_bucket() + '\/Scikit-keras-NLP-pipeline-Boston-Housing-example-June08-test1\/train\/Bostonmodel.tar.gz',\n                                  role = role)\n\n\n#####\n\n\nscikit_learn_inference_model = sklearn_preprocessor.create_model()\n#sagemaker_model = sagemaker_estimator.create_model()                     # Does Not have create_model method\nsagemaker_model = sagemaker_estimator\n\n\n\n\nmodel_name = 'Boston-inf-pipeline-July08-model' \nendpoint_name = 'Boston-inf-pipeline-July08-endpoint'\n\n\n#Build Inference Pipeline\nsm_model = PipelineModel(\n    name=model_name, \n    role=role, \n    models=[\n        scikit_learn_inference_model, \n        sagemaker_model],\n    sagemaker_session=sagemaker_session)\n\nsm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)\n<\/code><\/pre>",
        "Challenge_closed_time":1594239276336,
        "Challenge_comment_count":4,
        "Challenge_created_time":1593007959690,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while deploying a model on Sagemaker. The error message indicates that the 'NoneType' object has no attribute 'boto_region_name'. The user has provided the code snippet and the error message for reference.",
        "Challenge_last_edit_time":1594233740347,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62557113",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":16.0,
        "Challenge_reading_time":44.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":342.0324016667,
        "Challenge_title":"'NoneType' object has no attribute 'boto_region_name'",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1169.0,
        "Challenge_word_count":273,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1489614351972,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Washington, DC, United States",
        "Poster_reputation_count":379.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>The error is caused by your <code>sagemaker.tensorflow.serving.Model<\/code> not having a <code>sagemaker.session.Session<\/code> associated with it.<\/p>\n<p>Add <code>sagemaker_session=sagemaker_session<\/code> to your Model instantiation:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.tensorflow.serving import Model\nsagemaker_model = Model(model_data='s3:\/\/' + sagemaker_session.default_bucket() + '\/Scikit-keras-NLP-pipeline-Boston-Housing-example-June08-test1\/train\/Bostonmodel.tar.gz',\n                        role=role,\n                        sagemaker_session=sagemaker_session)\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":29.0,
        "Solution_reading_time":7.9,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"attribute error"
    },
    {
        "Answerer_created_time":1520413126203,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":37123.0,
        "Answerer_view_count":4058.0,
        "Challenge_adjusted_solved_time":29.5944916667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I deployed a web-service from an experiment in ML studio. I tested the API, and everything was working fine. I tested it in Postman. After 2 hours, I got an authentication error when I sent a request using the same API. So to resolve this, I republished my Web Service and got new authentication code, so the API is working fine for now. I have two questions:<\/p>\n\n<p>1) Does the primary key automatically expire after a while or by signing out from ML studio? \n2) What is the application of the second key in ML Studio APIs? Where do we need the second key? <\/p>",
        "Challenge_closed_time":1535450343787,
        "Challenge_comment_count":0,
        "Challenge_created_time":1535343803617,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user deployed a web-service from an experiment in ML Studio and tested the API successfully in Postman. However, after 2 hours, the user encountered an authentication error when sending a request using the same API. To resolve this, the user republished the Web Service and obtained a new authentication code. The user has two questions: 1) Does the primary key expire automatically or by signing out from ML studio? 2) What is the application of the second key in ML Studio APIs?",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52032535",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":7.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":29.5944916667,
        "Challenge_title":"Does the primary key of Web Service API in ML Studio expire?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":228.0,
        "Challenge_word_count":116,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501114346136,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Australia",
        "Poster_reputation_count":37.0,
        "Poster_view_count":8.0,
        "Solution_body":"<blockquote>\n  <p>1) Does the primary key automatically expire after a while or by signing out from ML studio?<\/p>\n<\/blockquote>\n\n<p>I could not find any limit of the primary key in the office docs. Per my test, my primary key does not expire more than two hours or sign out from ML studio.<\/p>\n\n<blockquote>\n  <p>2) What is the application of the second key in ML Studio APIs? Where do we need the second key?<\/p>\n<\/blockquote>\n\n<p>The second key is the same usage of the primary key, like a backup of the primary key. Also, the primary key equals the API key in the ML studio.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.7,
        "Solution_reading_time":6.93,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":104.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"authentication key usage"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":2.5969138889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to follow <a href=\"https:\/\/azure.github.io\/azureml-sdk-for-r\/articles\/train-and-deploy-first-model.html\" rel=\"nofollow noreferrer\">this post<\/a> to deploy a &quot;model&quot; in Azure.<\/p>\n<p>A code snipet is as follows and the model, which is simply a function adding 2 numbers, seems to register fine. I don't even use the model to isolate the problem after 1000s of attempts as this scoring code shows:<\/p>\n<pre><code>library(jsonlite)\n\ninit &lt;- function()\n{\n  message(&quot;hello world&quot;)\n  \n  function(data)\n  {\n    vars &lt;- as.data.frame(fromJSON(data))\n    prediction &lt;- 2\n    toJSON(prediction)\n  }\n}\n<\/code><\/pre>\n<p>Should be fine shouldn't it? Any way I run this code snippet:<\/p>\n<pre><code>r_env &lt;- r_environment(name = &quot;basic_env&quot;)\ninference_config &lt;- inference_config(\n  entry_script = &quot;score.R&quot;,\n  source_directory = &quot;.&quot;,\n  environment = r_env)\n\naci_config &lt;- aci_webservice_deployment_config(cpu_cores = 1, memory_gb = 0.5)\n\naci_service &lt;- deploy_model(ws, \n                            'xxxxx', \n                            list(model), \n                            inference_config, \n                            aci_config)\n\nwait_for_deployment(aci_service, show_output = TRUE)\n<\/code><\/pre>\n<p>Which produces this (after a looooong time):<\/p>\n<pre><code>Running.....................................................................\nFailed\nService deployment polling reached non-successful terminal state, current service state: Failed\nOperation ID: 14c35064-7ff4-46aa-9bfa-ab8a63218a2c\nMore information can be found using '.get_logs()'\nError:\n{\n  &quot;code&quot;: &quot;AciDeploymentFailed&quot;,\n  &quot;statusCode&quot;: 400,\n  &quot;message&quot;: &quot;Aci Deployment failed with exception: Error in entry script, RuntimeError: Error in file(filename, \\&quot;r\\&quot;, encoding = encoding) : , please run print(service.get_logs()) to get details.&quot;,\n  &quot;details&quot;: [\n    {\n      &quot;code&quot;: &quot;CrashLoopBackOff&quot;,\n      &quot;message&quot;: &quot;Error in entry script, RuntimeError: Error in file(filename, \\&quot;r\\&quot;, encoding = encoding) : , please run print(service.get_logs()) to get details.&quot;\n    }\n  ]\n}\n<\/code><\/pre>\n<p>It does not tell me much. Not sure how to debug this further? How can I run this:<\/p>\n<pre><code>print(service.get_logs())\n<\/code><\/pre>\n<p>and where please? Guess this is a Python artifact? Any other input very much welcome.<\/p>\n<p>PS:<\/p>\n<p>At this point in time, I have my suspicion that the above R entry file definition is not what is expected these days. Looking at the Python equivalent taken from <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=azcli\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<pre><code>import json\n\ndef init():\n    print(&quot;This is init&quot;)\n\ndef run(data):\n    test = json.loads(data)\n    print(f&quot;received data {test}&quot;)\n    return f&quot;test is {test}&quot;\n<\/code><\/pre>\n<p>Would something like this not be more suitable (tried it without success).<\/p>\n<pre><code>library(jsonlite)\n\ninit &lt;- function()\n{\n    message(&quot;hello world&quot;)\n}\n\ninit &lt;- function()\n{\n    return(42)\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1621007625123,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620998276233,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy a model in Azure using Azure Machine Learning and AzureMLSDK in R. They have registered the model and written the scoring code, but the deployment fails with a non-successful terminal state error. The error message does not provide much information, and the user is unsure how to debug it further. They suspect that the R entry file definition may not be what is expected and wonder if a Python equivalent would be more suitable.",
        "Challenge_last_edit_time":1621008123540,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67535014",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":40.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":2.5969138889,
        "Challenge_title":"deploy model and expose model as web service via azure machine learning + azuremlsdk in R",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":282.0,
        "Challenge_word_count":319,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>Great to see people putting the R SDK through it's paces!<\/p>\n<p>The vignette you're using is obviously a great way to get started. It seems you're almost all the way through without a hitch.<\/p>\n<p>Deployment is always tricky, and I'm not expert myself. I'd point you to this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment-local?WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">guide on troubleshooting deployment locally<\/a>. Similar functionality exists for the R SDK, namely: <a href=\"https:\/\/azure.github.io\/azureml-sdk-for-r\/reference\/local_webservice_deployment_config.html\" rel=\"nofollow noreferrer\"><code>local_webservice_deployment_config()<\/code><\/a>.<\/p>\n<p>So I think you change your example to this:<\/p>\n<pre class=\"lang-r prettyprint-override\"><code>deployment_config &lt;- local_webservice_deployment_config(port = 8890)\n<\/code><\/pre>\n<p>Once you know the service is working locally, the issue you're having with the ACI webservice becomes a lot easier to narrow down.<\/p>",
        "Solution_comment_count":8.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.8,
        "Solution_reading_time":13.74,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":107.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"deployment failure"
    },
    {
        "Answerer_created_time":1464811778510,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":196.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":55.3733275,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to build a model monitoring container for an ml project in aws sagemaker. Where I'm use processing jobs for the pre and post processing and model metric part. Can I incorporate model monitoring in that; without creating an endpoint?<\/p>",
        "Challenge_closed_time":1660323034192,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660123690213,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to build a model monitoring container for an ML project in AWS Sagemaker using processing jobs for pre and post-processing and model metrics. They are wondering if it is possible to incorporate model monitoring without creating an endpoint.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73303849",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.6,
        "Challenge_reading_time":4.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":55.3733275,
        "Challenge_title":"Is there any possible ways to do model monitoring in aws without an endpoint",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":36.0,
        "Challenge_word_count":54,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1660119311500,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>Not at the moment. Model Monitoring is tied to SageMaker RealTime endpoints.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":1.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":12.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"model monitoring without endpoint"
    },
    {
        "Answerer_created_time":1337759214688,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pune India",
        "Answerer_reputation_count":1036.0,
        "Answerer_view_count":124.0,
        "Challenge_adjusted_solved_time":810.2160711111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have implemented machine learning algorithms through sagemaker.<\/p>\n\n<p>I have installed SDK for .net, and tried by executing below code.<\/p>\n\n<pre><code>Uri sagemakerEndPointURI = new Uri(\"https:\/\/runtime.sagemaker.us-east-2.amazonaws.com\/endpoints\/MyEndpointName\/invocations\");\nAmazon.SageMakerRuntime.Model.InvokeEndpointRequest request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest();\nrequest.EndpointName = \"MyEndpointName\";\nAmazonSageMakerRuntimeClient aawsClient = new AmazonSageMakerRuntimeClient(myAwsAccessKey,myAwsSecreteKey);            \nAmazon.SageMakerRuntime.Model.InvokeEndpointResponse resposnse= aawsClient.InvokeEndpoint(request);\n<\/code><\/pre>\n\n<p>By executing this, I am getting validation error as \"<code>1 validation error detected: Value at 'body' failed to satisfy constraint: Member must not be null<\/code>\"<\/p>\n\n<p>Can anyone guide me on how and what more input data I need to pass to call the given API?<\/p>\n\n<p>EDIT<\/p>\n\n<p>Further I'd tried by provinding body parameter which contains a MemoryStream written by a '.gz' or '.pkl' file, and it giving me error as : \"Error unmarshalling response back from AWS,  HTTP content length exceeded 5246976 bytes.\"<\/p>\n\n<p>EDIT 1\/23\/2018<\/p>\n\n<p>Further I came up with the error message as <\/p>\n\n<blockquote>\n  <p>ERROR - model server - 'TypeError' object has no attribute 'message'<\/p>\n<\/blockquote>\n\n<p>Thanks<\/p>",
        "Challenge_closed_time":1519637555372,
        "Challenge_comment_count":4,
        "Challenge_created_time":1516531050743,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to call a Sagemaker training model endpoint API in C# using the SDK for .net. However, they are encountering a validation error stating that the member must not be null. They have tried passing a MemoryStream written by a '.gz' or '.pkl' file, but it is giving them an error stating that the HTTP content length exceeded 5246976 bytes. They have also encountered an error message stating that the 'TypeError' object has no attribute 'message'.",
        "Challenge_last_edit_time":1516720777516,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48365866",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":14.2,
        "Challenge_reading_time":18.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":862.9179525,
        "Challenge_title":"How to call Sagemaker training model endpoint API in C#",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":2093.0,
        "Challenge_word_count":153,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1337759214688,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune India",
        "Poster_reputation_count":1036.0,
        "Poster_view_count":124.0,
        "Solution_body":"<p>Later solved it by <code>Encoding.ASCII.GetBytes<\/code>as in below code.<\/p>\n\n<pre><code> byte[] bytes = System.IO.File.ReadAllBytes(@\"EXCEL_FILE_PATH\");\n    string listA = \"\";\n    while (!reader.EndOfStream)\n        {\n            var line = reader.ReadLine();\n            listA = listA + line + \"\\n\";\n        }\n    byte[] bytes = Encoding.ASCII.GetBytes(listA);\n    request.Body = new MemoryStream(bytes);\n    InvokeEndpointResponse response = sagemakerRunTimeClient.InvokeEndpoint(request);\n    string predictions = Encoding.UTF8.GetString(response.Body.ToArray());\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.8,
        "Solution_reading_time":6.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":36.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"validation error"
    },
    {
        "Answerer_created_time":1467005064900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":16.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":310.5718647222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've got the following data structure in an Azure DB Table:<\/p>\n\n<pre><code>Client_ID | Customer_ID | Item | Preference_Score\n<\/code><\/pre>\n\n<p>The table can contain different datasets from different clients but the data structure is always the same. Then, the table is imported in Azure ML.<\/p>\n\n<p>What I need is to repeat the same sequence of tasks in Azure ML for all the Client_ID in the above mentioned table.<\/p>\n\n<p>So that in the end I will train a single model for each client and score the data of each single client individually and append the scored data and store it again in Azure SQL.<\/p>\n\n<p>Is there any for each task in Azure ML like in SSIS? What's the best way to do this? <\/p>\n\n<p>Thanks.<\/p>",
        "Challenge_closed_time":1467005967403,
        "Challenge_comment_count":0,
        "Challenge_created_time":1465887908690,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to repeat a sequence of tasks in Azure ML for all the Client_IDs in a table containing different datasets from different clients. The goal is to train a single model for each client and score their data individually, then append and store the scored data in Azure SQL. The user is seeking advice on the best way to accomplish this and if there is a \"for each\" task in Azure ML similar to SSIS.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37805113",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":9.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":310.5718647222,
        "Challenge_title":"Azure ML Loops through the different tasks",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":575.0,
        "Challenge_word_count":126,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1427804300743,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":983.0,
        "Poster_view_count":259.0,
        "Solution_body":"<p>You can start from Azure Data Factory for automating batch scoring. In your model instead of web service output, you can use DataWriter exporter module to write the output directly into an Azure Table etc. You can check Microsoft MyDriving reference guide (<a href=\"http:\/\/aka.ms\/mydrivingdocs\" rel=\"nofollow\">http:\/\/aka.ms\/mydrivingdocs<\/a>) at page 107-8 where the machine learning section starts at page 100.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.0,
        "Solution_reading_time":5.32,
        "Solution_score_count":-1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":57.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"repeat tasks for each client"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":158.096525,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>My Azure ML studio web service used to run fine but is unavailable for me to use now. Anybody know a fix?  <\/p>\n<p>Error Message: Could not authorize the request. Make sure the request has an Authorization header with a bearer token, of the form &quot;Authorization: Bearer [token]&quot;. See online help to find which tokens are valid for this request.  <br \/>\nSite Path: \/workspaces\/fde0912ad97d4a94b9b2baaafd54c3e1\/webservices\/378f095e8260497697790a6d65fe9ff8\/endpoints\/default  <br \/>\nActivity ID: 82e53138-b3b9-4a94-8695-0b8152c505ac  <br \/>\nRequest ID: e3e8fbe7-6161-411b-9c2c-e2a609436353  <br \/>\nWorkspace ID: fde0912ad97d4a94b9b2baaafd54c3e1  <br \/>\nWorkspace Type: Free  <br \/>\nUser Role: Owner  <br \/>\nTenant ID: f8cdef31-a31e-4b4a-93e4-5f571e91255a<\/p>",
        "Challenge_closed_time":1649047941240,
        "Challenge_comment_count":2,
        "Challenge_created_time":1648478793750,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an authorization error while trying to access their Azure ML studio web service. The error message suggests that the request needs to have an Authorization header with a bearer token. The user is seeking a fix for this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/790341\/authorization-bearer-(token)-error",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.2,
        "Challenge_reading_time":10.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":158.096525,
        "Challenge_title":"Authorization: Bearer [token] Error",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":91,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=4f826b11-7b14-4523-b00c-cbe2449313bf\">@dasa8  <\/a>     <\/p>\n<p>Update: The bug has been confirmed and the ETA is 2-3 weeks for the bug fixing. I am sorry for all the inconveniences.     <\/p>\n<p>The workaround for now is to use studio classic portal to manage the classic web service as below screenshot:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/189568-microsoftteams-image-9.png?platform=QnA\" alt=\"189568-microsoftteams-image-9.png\" \/>    <\/p>\n<p>Thanks for the understanding and sorry for the experience again.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p><em>-Please kindly accept the answer if you feel helpful, thanks a lot!<\/em>    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.9,
        "Solution_reading_time":8.87,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":81.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"authorization error"
    },
    {
        "Answerer_created_time":1505194585676,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":35.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":44.2415530556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to deploy a model to Kubernetes in Azure Machine Learning Studio, it was working for a while, but now, it fails during deployment, the error message is as follows:<\/p>\n<pre><code>Deploy: Failed on step WaitServiceCreating. Details: AzureML service API error. \nYour container application crashed. This may be caused by errors in your scoring file's init() function.\nPlease check the logs for your container instance: pipeline-created-on-07-28-2020-r.\nFrom the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\nYou can also try to run image viennaglobal.azurecr.io\/azureml\/azureml_6ae744633f749472feb283065055dc2c:latest locally.\nPlease refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\n<\/code><\/pre>\n<pre class=\"lang-js prettyprint-override\"><code>{\n    &quot;code&quot;: &quot;KubernetesDeploymentFailed&quot;,\n    &quot;statusCode&quot;: 400,\n    &quot;message&quot;: &quot;Kubernetes Deployment failed&quot;,\n    &quot;details&quot;: [\n        {\n            &quot;code&quot;: &quot;CrashLoopBackOff&quot;,\n            &quot;message&quot;: &quot;Your container application crashed. This may be caused by errors in your scoring file's init() function.\nPlease check the logs for your container instance: pipeline-created-on-07-28-2020-r. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image viennaglobal.azurecr.io\/azureml\/azureml_6ae744633f749472feb283065055dc2c:latest locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.&quot;\n        }\n    ]\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1596104635683,
        "Challenge_comment_count":2,
        "Challenge_created_time":1595913837627,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while deploying a model to Kubernetes in Azure Machine Learning Studio. The deployment fails with an error message indicating that the container application crashed, possibly due to errors in the scoring file's init() function. The user is advised to check the logs for the container instance and run the image locally for debugging.",
        "Challenge_last_edit_time":1595945366092,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63127521",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":10.5,
        "Challenge_reading_time":21.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":52.99946,
        "Challenge_title":"Deploying Model to Kubernetes",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":186.0,
        "Challenge_word_count":178,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1505194585676,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>It seems it was a bug, got corrected by itself today. Closing this question now<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.9,
        "Solution_reading_time":1.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":15.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"deployment failure"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":10.5155730556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>After I've trained and deployed the model with AWS SageMaker, I want to evaluate it on several csv files:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>- category-1-eval.csv (~700000 records)\n- category-2-eval.csv (~500000 records)\n- category-3-eval.csv (~800000 records)\n...\n<\/code><\/pre>\n\n<p>The right way to do this is with using <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/estimator\/Estimator#evaluate\" rel=\"nofollow noreferrer\">Estimator.evaluate()<\/a> method, as it is fast.<\/p>\n\n<p>The problem is - I cannot find the way to restore SageMaker model into Tensorflow Estimator, is it possible?<\/p>\n\n<p>I've tried to restore a model like this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>tf.estimator.DNNClassifier(\n    feature_columns=...,\n    hidden_units=[...],\n    model_dir=\"s3:\/\/&lt;bucket_name&gt;\/checkpoints\",\n)\n<\/code><\/pre>\n\n<p>In AWS SageMaker documentation a different approach is described - <a href=\"https:\/\/docs.aws.amazon.com\/en_us\/sagemaker\/latest\/dg\/how-it-works-model-validation.html\" rel=\"nofollow noreferrer\">to test the actual endpoint from the Notebook<\/a> - but it takes to much time and requires a lot of API calls to the endpoint.<\/p>",
        "Challenge_closed_time":1562591149540,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562553970617,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained and deployed a model with AWS SageMaker and wants to evaluate it on several CSV files using the Estimator.evaluate() method. However, they are unable to restore the SageMaker model into Tensorflow Estimator and are seeking a solution. The AWS SageMaker documentation suggests testing the actual endpoint from the Notebook, but this approach is time-consuming and requires a lot of API calls to the endpoint.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56927813",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":16.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":10.3274786111,
        "Challenge_title":"Using of Estamator.evaluate() on trained sagemaker tensorflow model",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":155.0,
        "Challenge_word_count":126,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1530642335903,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Vancouver, BC, Canada",
        "Poster_reputation_count":53.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>if you used the built-in Tensorflow container, your model has been saved in Tensorflow Serving format, e.g.:<\/p>\n\n<pre><code>$ tar tfz model.tar.gz\nmodel\/\nmodel\/1\/\nmodel\/1\/saved_model.pb\nmodel\/1\/variables\/\nmodel\/1\/variables\/variables.index\nmodel\/1\/variables\/variables.data-00000-of-00001\n<\/code><\/pre>\n\n<p>You can easily load it with Tensorflow Serving on your local machine, and send it samples to predict. More info at <a href=\"https:\/\/www.tensorflow.org\/tfx\/guide\/serving\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/tfx\/guide\/serving<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1562591826680,
        "Solution_link_count":2.0,
        "Solution_readability":12.2,
        "Solution_reading_time":7.39,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":53.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to restore model"
    },
    {
        "Answerer_created_time":1394703217223,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cologne, Germany",
        "Answerer_reputation_count":486.0,
        "Answerer_view_count":26.0,
        "Challenge_adjusted_solved_time":166.9819036111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to enable data capture for a specific endpoint (so far, only via the console). The endpoint works fine and also logs &amp; returns the desired results. However, no files are written to the specified S3 location.<\/p>\n<h3>Endpoint Configuration<\/h3>\n<p>The endpoint is based on a training job with a scikit learn classifier. It has only one variant which is a <code>ml.m4.xlarge<\/code> instance type. Data Capture is enabled with a sampling percentage of 100%. As data capture storage locations I tried <code>s3:\/\/&lt;bucket-name&gt;<\/code> as well as <code>s3:\/\/&lt;bucket-name&gt;\/&lt;some-other-path&gt;<\/code>. With the &quot;Capture content type&quot; I tried leaving everything blank, setting <code>text\/csv<\/code> in &quot;CSV\/Text&quot; and <code>application\/json<\/code> in &quot;JSON&quot;.<\/p>\n<h3>Endpoint Invokation<\/h3>\n<p>The endpoint is invoked in a Lambda function with a client. Here's the call:<\/p>\n<pre><code>sagemaker_body_source = {\n            &quot;segments&quot;: segments,\n            &quot;language&quot;: language\n        }\npayload = json.dumps(sagemaker_body_source).encode()\nresponse = self.client.invoke_endpoint(EndpointName=endpoint_name,\n                                       Body=payload,\n                                       ContentType='application\/json',\n                                       Accept='application\/json')\nresult = json.loads(response['Body'].read().decode())\nreturn result[&quot;predictions&quot;]\n<\/code><\/pre>\n<p>Internally, the endpoint uses a Flask API with an <code>\/invocation<\/code> path that returns the result.<\/p>\n<h3>Logs<\/h3>\n<p>The endpoint itself works fine and the Flask API is logging input and output:<\/p>\n<pre><code>INFO:api:body: {'segments': [&lt;strings...&gt;], 'language': 'de'}\n<\/code><\/pre>\n<pre><code>INFO:api:output: {'predictions': [{'text': 'some text', 'label': 'some_label'}, ....]}\n<\/code><\/pre>",
        "Challenge_closed_time":1660654017400,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660052882547,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to enable data capture for a specific endpoint in Sagemaker, but no files are being written to the specified S3 location. The endpoint is based on a training job with a scikit learn classifier and is invoked in a Lambda function with a client. The endpoint works fine and logs input and output, but data capture is not working.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73292975",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.3,
        "Challenge_reading_time":23.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":166.9819036111,
        "Challenge_title":"Sagemaker Data Capture does not write files",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":65.0,
        "Challenge_word_count":187,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1394703217223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cologne, Germany",
        "Poster_reputation_count":486.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>So the issue seemed to be related to the IAM role. The default role (<code>ModelEndpoint-Role<\/code>) does not have access to write S3 files. It worked via the SDK since it uses another role in the sagemaker studio. I did not receive any error message about this.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.4,
        "Solution_reading_time":3.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":46.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"data capture not working"
    },
    {
        "Answerer_created_time":1364896706347,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Noida, India",
        "Answerer_reputation_count":6584.0,
        "Answerer_view_count":962.0,
        "Challenge_adjusted_solved_time":5.9270777778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to train a linear learner model in Sagemaker. My training set is 422 rows split into 4 files on AWS S3. The mini-batch size that I set is 50. <\/p>\n\n<p>I keep on getting this error in Sagemaker.<\/p>\n\n<blockquote>\n  <p>Customer Error: No training data processed. Either the training\n  channel is empty or the mini-batch size is too high. Verify that\n  training data contains non-empty files and the mini-batch size is less\n  than the number of records per training host.<\/p>\n<\/blockquote>\n\n<p>I am using this InputDataConfig<\/p>\n\n<pre><code>InputDataConfig=[\n            {\n                'ChannelName': 'train',\n                'DataSource': {\n                    'S3DataSource': {\n                        'S3DataType': 'S3Prefix',\n                        'S3Uri': 's3:\/\/MY_S3_BUCKET\/REST_OF_PREFIX\/exported\/',\n                        'S3DataDistributionType': 'FullyReplicated'\n                    }\n                },\n                'ContentType': 'text\/csv',\n                'CompressionType': 'Gzip'\n            }\n        ],\n<\/code><\/pre>\n\n<p>I am not sure what I am doing wrong here. I tried increasing the number of records to 5547495 split across 6 files. The same error. That makes me think that somehow the config itself has something missing. Due to which it seems to think training channel is just not present. I tried changing 'train' to 'training' as that is what the erorr message is saying. But then I got <\/p>\n\n<blockquote>\n  <p>Customer Error: Unable to initialize the algorithm. Failed to validate\n  input data configuration. (caused by ValidationError)<\/p>\n  \n  <p>Caused by: {u'training': {u'TrainingInputMode': u'Pipe',\n  u'ContentType': u'text\/csv', u'RecordWrapperType': u'None',\n  u'S3DistributionType': u'FullyReplicated'}} is not valid under any of\n  the given schemas<\/p>\n<\/blockquote>\n\n<p>I went back to train as that seems to be what is needed. But what am I doing wrong with that? <\/p>",
        "Challenge_closed_time":1559566976623,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559544607823,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while training a linear learner model in AWS Sagemaker. The error message suggests that either the training channel is empty or the mini-batch size is too high. The user has tried increasing the number of records and changing the channel name but is still facing the same issue. The user is unsure about what is wrong with the InputDataConfig.",
        "Challenge_last_edit_time":1559545639143,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56422325",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":22.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":6.2135555556,
        "Challenge_title":"AWS Sagemaker - Either the training channel is empty or the mini-batch size is too high",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":599.0,
        "Challenge_word_count":249,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1364896706347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Noida, India",
        "Poster_reputation_count":6584.0,
        "Poster_view_count":962.0,
        "Solution_body":"<p>Found the problem. The CompressionType was mentioned as 'Gzip' but I had changed the actual file to be not compressed when doing the exports. As soon as I changed it to be 'None' the training went smoothly.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.5,
        "Solution_reading_time":2.6,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":37.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"training error"
    },
    {
        "Answerer_created_time":1434295027120,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Lusaka, Zambia",
        "Answerer_reputation_count":951.0,
        "Answerer_view_count":157.0,
        "Challenge_adjusted_solved_time":164.0830111111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I need to use the <a href=\"https:\/\/aws.amazon.com\/marketplace\/pp\/prodview-7y6xdiukxucr2\" rel=\"nofollow noreferrer\">WireframeToCode<\/a> model from the AWS Marketplace, I used Nodejs to read and send the file data to the model like this:<\/p>\n\n<pre><code>var sageMakerRuntime = new AWS.SageMakerRuntime();\n\nvar bitmap = fs.readFileSync(\"sample.jpeg\", \"utf8\");\nvar buffer = new Buffer.from(bitmap, \"base64\");\n\nvar params = {\n  Body: buffer.toJSON(),\n  EndpointName: \"wireframe-to-code\",\n  Accept: \"image\/jpeg\",\n  ContentType: \"application\/json\"\n};\n\nsageMakerRuntime.invokeEndpoint(params, function(err, data) {\n  if (err) console.log(err, err.stack);\n  else console.log(data);\n});\n<\/code><\/pre>\n\n<p>but i get this error:<\/p>\n\n<blockquote>\n  <p>message: 'Expected params.Body to be a string, Buffer, Stream, Blob,\n  or typed array object',   code: 'InvalidParameterType',   time:\n  2020-03-30T11:06:27.535Z<\/p>\n<\/blockquote>\n\n<p>From the documentation, the supported content type for input is  <code>image\/jpeg<\/code> output is <code>application\/json<\/code>.<\/p>\n\n<p>when I try to convert the Body to a string like this: <code>JSON.stringify(buffer.toJSON())<\/code> I get this error:<\/p>\n\n<blockquote>\n  <p>Received client error (415) from model with message \"This predictor\n  only supports JSON formatted data\"<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1586158824067,
        "Challenge_comment_count":0,
        "Challenge_created_time":1585568125227,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use the WireframeToCode model from AWS Marketplace by passing an image to the SageMaker endpoint using Nodejs. However, they are encountering an error message stating that the expected parameter Body should be a string, Buffer, Stream, Blob, or typed array object. The supported content type for input is image\/jpeg and output is application\/json. When the user tries to convert the Body to a string, they receive a client error message stating that the predictor only supports JSON formatted data.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60929678",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":11.9,
        "Challenge_reading_time":17.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":164.0830111111,
        "Challenge_title":"How to pass image to AWS SageMaker endpoint",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":2170.0,
        "Challenge_word_count":142,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1434295027120,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lusaka, Zambia",
        "Poster_reputation_count":951.0,
        "Poster_view_count":157.0,
        "Solution_body":"<p>I had to pass in bitmap and change <code>ContentType<\/code> to <code>\"image\/jpeg\"<\/code><\/p>\n\n<pre><code>const AWS = require(\"aws-sdk\");\nconst fs = require(\"fs\");\n\nconst sageMakerRuntime = new AWS.SageMakerRuntime({\n  region: \"us-east-1\",\n  accessKeyId: \"XXXXXXXXXXXX\",\n  secretAccessKey: \"XXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n});\n\nconst bitmap = fs.readFileSync(\"sample.jpeg\");\n\nvar params = {\n  Body: bitmap,\n  EndpointName: \"wireframe-to-code\",\n  ContentType: \"image\/jpeg\"\n};\n\nsageMakerRuntime.invokeEndpoint(params, function(err, data) {\n  if (err) {\n    console.log(err, err.stack);\n  } else {\n    responseData = JSON.parse(Buffer.from(data.Body).toString());\n    console.log(responseData);\n  }\n});\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.8,
        "Solution_reading_time":9.0,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":50.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unsupported input format"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.7565502778,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm trying to deploy my own GreengrassV2 components. It's a SageMaker ML model (optimized with SageMakerNeo and packaged as a Greengrass component) and the according inference app. I was trying to deploy it to my core device with SageMaker Edge Manager component. But it is always stuck in the status \"In progress\".\n\nMy logs show this error:\ncom.aws.greengrass.tes.CredentialRequestHandler: Error in retrieving AwsCredentials from TES. {iotCredentialsPath=\/role-aliases\/edgedevicerolealias\/credentials, credentialData=TES responded with status code: 403. Caching response. {\"message\":\"Access Denied\"}}\n\nBut how do I know which policies are missing?",
        "Challenge_closed_time":1682713832572,
        "Challenge_comment_count":0,
        "Challenge_created_time":1682711108991,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with deploying their own GreengrassV2 components, specifically a SageMaker ML model and inference app, to their core device using SageMaker Edge Manager component. The deployment is stuck in \"In progress\" status and the logs show an error related to missing policies, but the user is unsure which policies are missing.",
        "Challenge_last_edit_time":1683057468614,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUlDu9VAj4Qx-O7cbAXDz28w\/greengrass-own-component-deployment-stuck-in-progress",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":9.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.7565502778,
        "Challenge_title":"Greengrass own component deployment stuck \"in progress\"",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":65.0,
        "Challenge_word_count":90,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello, please refer to https:\/\/docs.aws.amazon.com\/greengrass\/v2\/developerguide\/troubleshooting.html#token-exchange-service-credentials-http-403 for troubleshooting,  you'll need `iot:AssumeRoleWithCertificate` permissions on your core device's AWS IoT role alias",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1682713832572,
        "Solution_link_count":1.0,
        "Solution_readability":21.2,
        "Solution_reading_time":3.58,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"deployment stuck, missing policies"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":29.32571,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I read that there is a way to train and host multiple models using a single endpoint for a single dataset in AWS Sagemaker. But I have 2 different datasets in S3 and have to train a model for each dataset. Can these 2 different models be hosted using a single endpoint? <\/p>",
        "Challenge_closed_time":1578755333996,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578649761440,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to know if it is possible to host two different models, each trained on a different dataset, using a single endpoint in AWS Sagemaker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59679192",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.1,
        "Challenge_reading_time":4.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":29.32571,
        "Challenge_title":"Hosting multiple models for multiple datasets in aws sagemaker",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":411.0,
        "Challenge_word_count":61,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1550756471932,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":67.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>Yes, this is called a multi-model endpoint. You can use a large number of models on the same endpoint. They get loaded and unloaded dynamically as needed, and you simply have to pass the model name in your prediction request.<\/p>\n\n<p>Here are some resources:<\/p>\n\n<ul>\n<li><p>Blog post + example : <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/<\/a><\/p><\/li>\n<li><p>Video explaining model deployment scenarios on SageMaker: <a href=\"https:\/\/youtu.be\/dT8jmdF-ZWw\" rel=\"nofollow noreferrer\">https:\/\/youtu.be\/dT8jmdF-ZWw<\/a><\/p><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":17.5,
        "Solution_reading_time":10.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"hosting multiple models"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":1.3582819445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In my AML pipeline, I've got a model built and deployed to the AciWebservice. I now have a need to include some additional data that would be used by score.py. This data is in json format (~1mb) and is specific to the model that's built. To accomplish this, I was thinking of sticking this file in blob store and updating some \"placholder\" vars in the score.py during deployment, but it seems hacky. <\/p>\n\n<p>Here are some options I was contemplating but wasn't sure on the practicality<\/p>\n\n<p><strong>Option 1:<\/strong>\nIs it possible to include this file, during the model deployment itself so that it's part of the docker image? <\/p>\n\n<p><strong>Option 2:<\/strong>\nAnother possibility I was contemplating, would it be possible to include this json data part of the Model artifacts?<\/p>\n\n<p><strong>Option 3:<\/strong>\nHow about registering it as a dataset and pull that in the score file?<\/p>\n\n<p>What is the recommended way to deploy dependent files in a model deployment scenario?<\/p>",
        "Challenge_closed_time":1589484511663,
        "Challenge_comment_count":0,
        "Challenge_created_time":1589475408817,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has built and deployed a model to AciWebservice using Azure ML pipeline. They now need to include additional data in json format that would be used by score.py. The user is contemplating different options such as including the file during model deployment, including the json data as part of the model artifacts, or registering it as a dataset. The user is seeking recommendations on the best way to deploy dependent files in a model deployment scenario.",
        "Challenge_last_edit_time":1589479621848,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61803031",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.8,
        "Challenge_reading_time":12.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":2.5285683334,
        "Challenge_title":"Azure ML: Include additional files during model deployment",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":866.0,
        "Challenge_word_count":167,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1330016065408,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1704.0,
        "Poster_view_count":232.0,
        "Solution_body":"<p>There are few ways to accomplish this:<\/p>\n\n<ol>\n<li><p>Put the additional file in the same folder as your model file, and <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none--sample-input-dataset-none--sample-output-dataset-none--resource-configuration-none-\" rel=\"nofollow noreferrer\">register<\/a> the whole folder as the model. In this approach the file is stored alongside the model.<\/p><\/li>\n<li><p>Put the file in a local folder, and specify that folder as source_directory in <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.inferenceconfig?view=azure-ml-py\" rel=\"nofollow noreferrer\">InferenceConfig<\/a>. In this approach the file is re-uploaded every time you deploy a new endpoint.<\/p><\/li>\n<li><p>Use custom base image in InferenceConfig to bake the file into Docker image itself.<\/p><\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.8,
        "Solution_reading_time":14.5,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":88.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"deploy dependent files"
    },
    {
        "Answerer_created_time":1320061998252,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":778.0,
        "Answerer_view_count":89.0,
        "Challenge_adjusted_solved_time":135.5520369445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>same input is used in two cases, but different result is returned from python module<\/p>\n\n<p>here is the python script that return the result to the webservice:<\/p>\n\n<pre><code>import pandas as pd\nimport sys\n\n\n  def get_segments(dataframe):\n     dataframe['segment']=dataframe['segment'].astype('str')\n     segments = dataframe.loc[~dataframe['segment'].duplicated()]['segment']\n     return segments\n\n\n  def azureml_main(dataframe1 = None, dataframe2 = None):\n\n   df = dataframe1\n   segments = get_segments(df)\n   segmentCount =segments.size\n\n   if (segmentCount &gt; 0) :\n      res = pd.DataFrame(columns=['segmentId','recommendation'],index=[range(segmentCount)])\n    i=0    \n    for seg in segments:\n        d= df.query('segment ==[\"{}\"]'.format(seg)).sort(['count'],ascending=[0])\n\n        res['segmentId'][i]=seg\n        recommendation='['\n        for index, x in d.iterrows():\n            item=str(x['ItemId'])\n            recommendation = recommendation + item + ','\n        recommendation = recommendation[:-1] + ']'\n        res['recommendation'][i]= recommendation\n        i=i+1\n   else:\n\n      res = pd.DataFrame(columns=[seg,pdver],index=[range(segmentCount)])\n\nreturn res,\n<\/code><\/pre>\n\n<p>when in experiment it returnd the actual itemIds, when in webservice it returns some numbers<\/p>\n\n<p>the purpose of this code is to pivot some table by segment column for recommendation<\/p>",
        "Challenge_closed_time":1468139423743,
        "Challenge_comment_count":2,
        "Challenge_created_time":1467651436410,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing an issue where the same input is used in two cases, but different results are returned from the Python module. The Python script is used to pivot some table by segment column for recommendation. In the experiment, it returns the actual itemIds, but in the webservice, it returns some numbers.",
        "Challenge_last_edit_time":1468140771380,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38189399",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":14.6,
        "Challenge_reading_time":17.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":135.5520369445,
        "Challenge_title":"azure ml experiment return different results than webservice",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":264.0,
        "Challenge_word_count":124,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1320061998252,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":778.0,
        "Poster_view_count":89.0,
        "Solution_body":"<p>After discussion with the product team from Microsoft. the issue was resolved.\nthe product team rolled out an update to the web service first, and only later to the ML-Studio, which fixed an issue with categorical attributes in \"Execute python script\".\nthe issue was in a earlier stage of the flow and has nothing to do with the python code above.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.9,
        "Solution_reading_time":4.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":61.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"inconsistent results"
    },
    {
        "Answerer_created_time":1467451434136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":111.0,
        "Answerer_view_count":36.0,
        "Challenge_adjusted_solved_time":21.3031752778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><strong>PROBLEM<\/strong> <\/p>\n\n<p>I deployed my experiments in Azure Machine Learning as a Web Service. The experiments ran without error. <\/p>\n\n<p>But when testing using <code>REQUEST\/RESPONSE<\/code>, I'm getting the error below:<\/p>\n\n<blockquote>\n  <p>Execute R Script Piped (RPackage) : The following error occurred during evaluation of R script: R_tryEval: return error: Error in split(df, list(df$PRO_NAME, df$Illness_Code))[Ind] : invalid subscript type 'list'<\/p>\n<\/blockquote>\n\n<p>This is the code:<\/p>\n\n<pre><code># Loop through the dataframe and apply model\nInd &lt;- sapply(split(df, list(df$PRO_NAME,df$Illness_Code)), \n              function(x)nrow(x)&gt;1)\n\nout &lt;- lapply(\n    split(df, list(df$PRO_NAME, df$Illness_Code))[Ind],\n    function(c){\n        m &lt;- lm(formula = COUNT ~ YEAR, data = c)\n        coef(m)\n    })\n<\/code><\/pre>",
        "Challenge_closed_time":1544578308848,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544501617417,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has encountered an error while testing their deployed experiments in Azure Machine Learning as a Web Service using REQUEST\/RESPONSE. The error message indicates an invalid subscript type 'list' in the R script. The provided code shows a loop through a dataframe and applying a model.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53717284",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":11.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":21.3031752778,
        "Challenge_title":"invalid subscript type 'list' Azure Machine Learning",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":53.0,
        "Challenge_word_count":97,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1467451434136,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":111.0,
        "Poster_view_count":36.0,
        "Solution_body":"<p><strong>FIXED<\/strong><\/p>\n\n<p><strong>Problem:<\/strong><\/p>\n\n<p>Some R codes don't work if input data is limited (e.g 1-2 rows only)<\/p>\n\n<p><strong>Solution:<\/strong><\/p>\n\n<p>Load data by <code>Batch<\/code> instead of <code>REQUEST\/RESPONSE<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.5,
        "Solution_reading_time":3.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":24.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"invalid subscript type"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.1917047222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using model versioning and would like to have different model versions accessible via the same endpoint. Any best practices to access the multiple models from the same endpoint.<\/p>",
        "Challenge_closed_time":1653550999480,
        "Challenge_comment_count":1,
        "Challenge_created_time":1653539509343,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is using model versioning and wants to access different model versions from the same endpoint. They are seeking best practices for accessing multiple models from a single endpoint.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/864579\/accessing-different-model-versions-from-same-endpo",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.2,
        "Challenge_reading_time":3.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":3.1917047222,
        "Challenge_title":"Accessing different model versions from same endpoint",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":36,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=71c0cf97-895c-43f1-ac54-98e1e9833ae4\">@A-4824  <\/a> Thanks, You may deploy \u201clocally\u201d to a Azure Machine Learning compute instance, by specifying different port # for each version. They are converted to a URL according to the format https:\/\/&lt;compute instance\u2019s name&gt;-port.region.instances.azureml.ms\/score    <\/p>\n<p>Model v1: service_url = <a href=\"https:\/\/azure-ml-compute-instance-name-8001.westeurope.instances.azureml.ms\/score\">https:\/\/azure-ml-compute-instance-name-8001.westeurope.instances.azureml.ms\/score<\/a>    <br \/>\nModel v2: service_url = https:\/\/ azure-ml-compute-instance-name-8002.westeurope.instances.azureml.ms\/score    <\/p>\n<p>There\u2019s sample code in <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-local-container-notebook-vm\">documentation<\/a>. You can specify port to deploy with the following parameter.    <br \/>\ndeployment_config = LocalWebservice.deploy_configuration(port=8001)    <\/p>\n<p>We recommend using the new ML Endpoints (Preview) <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-endpoints\">What are endpoints (preview) - Azure Machine Learning | Microsoft Learn<\/a>.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":18.9,
        "Solution_reading_time":16.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":87.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"access multiple models"
    },
    {
        "Answerer_created_time":1446238644288,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bothell, WA, USA",
        "Answerer_reputation_count":1080.0,
        "Answerer_view_count":155.0,
        "Challenge_adjusted_solved_time":17662.8095425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We have serval <a href=\"http:\/\/luis.ai\" rel=\"nofollow noreferrer\">LUIS<\/a> models that work fine and returns desired <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/luis\/luis-concept-intent\" rel=\"nofollow noreferrer\">Intents<\/a> and <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/luis\/luis-concept-entity-types\" rel=\"nofollow noreferrer\">Entities<\/a>.<\/p>\n\n<p>Models are separated based on content and target business domain so we do not want to merge them.\nStill there are some <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/luis\/luis-quickstart-intents-regex-entity\" rel=\"nofollow noreferrer\">Regex entities<\/a> that are the same in each of the model.<\/p>\n\n<hr>\n\n<p>If we'd like to have one Regex definition at one place could we eventually share such definition among multiple LUIS models?<\/p>\n\n<p>Right now we proceed with Ctrl+C and Ctrl+V<\/p>",
        "Challenge_closed_time":1529098914963,
        "Challenge_comment_count":0,
        "Challenge_created_time":1529053600037,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has multiple LUIS models that work well and return the desired Intents and Entities. However, they have some Regex entities that are the same in each model, and they want to know if it's possible to share the definition of these entities among multiple models instead of copying and pasting them.",
        "Challenge_last_edit_time":1529058260707,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50872338",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":12.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":12.5874794445,
        "Challenge_title":"Could LUIS share entity definition among multiple models?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":64.0,
        "Challenge_word_count":96,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1483649190768,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Prague-Prague 1, Czechia",
        "Poster_reputation_count":507.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>We could share the entire app by <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/luis\/luis-how-to-manage-versions\" rel=\"noreferrer\">cloning the app<\/a>. And the only way to use specific entities would be to delete the others.<\/p>\n<blockquote>\n<p>If we'd like to have one Regex definition at one place could we eventually share such definition among multiple LUIS models?<\/p>\n<p>Right now we proceed with Ctrl+C and Ctrl+V<\/p>\n<\/blockquote>\n<p>This is the only way to do it right now.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1592644375060,
        "Solution_link_count":1.0,
        "Solution_readability":8.5,
        "Solution_reading_time":6.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":67.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"share Regex entities"
    },
    {
        "Answerer_created_time":1296602388823,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA",
        "Answerer_reputation_count":1244.0,
        "Answerer_view_count":368.0,
        "Challenge_adjusted_solved_time":241.0489197222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a list of ML experiments which I have created in Azure Machine Learning Studio.  I have deployed them as web services (the new version, not classic).  <\/p>\n\n<p>How can I go into Azure Machine Learning Web Services, click on a web service (which was deployed from an experiment), then navigate back to the experiment \/ predictive model which feeds it?<\/p>\n\n<p>The only link I can find between the two is by updating the web service from the predictive experiment, which then confirms what the web service is. I can see that the \"ExperimentId\" is a GUID in the URL when in the experiment and the web service, so hopefully this is possible.<\/p>\n\n<p>My reasoning is that relying on matching naming conventions, etc., to select the appropriate model to update is subject to human error.<\/p>",
        "Challenge_closed_time":1490283526688,
        "Challenge_comment_count":0,
        "Challenge_created_time":1489415750577,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to navigate from a deployed web service back to the original experiment or predictive model in Azure Machine Learning Studio. The only link found is the \"ExperimentId\" in the URL, and the user is concerned about relying on naming conventions to select the correct model.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42766263",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":10.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":241.0489197222,
        "Challenge_title":"How to Find Azure ML Experiment based on Deployed Web Service",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":224.0,
        "Challenge_word_count":145,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1377703476328,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bristol, United Kingdom",
        "Poster_reputation_count":592.0,
        "Poster_view_count":43.0,
        "Solution_body":"<p>The <em>new<\/em> web service does not store any information about the experiment or workspace that was deployed (not all <em>new<\/em> web services are deployed from an experiment).<\/p>\n\n<p>Here are the options available to track the relationship between the experiment and a <em>new<\/em> web service.<\/p>\n\n<h2>last deployment<\/h2>\n\n<p>However, the experiment keeps track of the <strong>last<\/strong> <em>new<\/em> web service that was deployed from the experiment. each deployment to a <em>new<\/em> web service overwrites this value.<\/p>\n\n<p>The value is stored in the experiment graph. One way to get the graph is to use the powershell module <a href=\"http:\/\/aka.ms\/amlps\" rel=\"nofollow noreferrer\">amlps<\/a><\/p>\n\n<p><code>Export-AmlExperimentGraph -ExperimentId &lt;Experiment Id&gt; -OutputFile e.json<\/code><\/p>\n\n<p><strong>e.json<\/strong><\/p>\n\n<pre><code>{\n\"ExperimentId\":\"&lt;Experiment Id&gt;\",\n\/\/ . . .\n\"WebService\":{\n\/\/ . . .\n\"ArmWebServiceId\":\"&lt;Arm Id&gt;\"\n},\n\/\/ . . . \n}\n<\/code><\/pre>\n\n<h2>azure resource tags<\/h2>\n\n<p>The tags feature for Azure resources is supported by the <em>new<\/em> web services. Setting a <code>tag<\/code> on the web service programmatically, with powershell or via the azure portal UX can be used to store a reference to the experiment on the <em>new<\/em> web service.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.8,
        "Solution_reading_time":16.63,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":158.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"navigate to original model"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":1796.2886094445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>My <code>API_ENDPOINT<\/code> is set to <code>europe-west1-aiplatform.googleapis.com<\/code>.<\/p>\n<p>I define a pipeline:<\/p>\n<pre><code>def pipeline(project: str = PROJECT_ID, region: str = REGION, api_endpoint: str = API_ENDPOINT):\n<\/code><\/pre>\n<p>when I run it:<\/p>\n<pre><code>job = aip.PipelineJob(\ndisplay_name=DISPLAY_NAME,\ntemplate_path=&quot;image classification_pipeline.json&quot;.replace(&quot; &quot;, &quot;_&quot;),)\njob.run()\n<\/code><\/pre>\n<p>it is always created in USandA:<\/p>\n<pre><code>INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. \nResource name: projects\/my_proj_id\/locations\/us-central1\/pipelineJobs\/automl-image-training-v2-anumber\n<\/code><\/pre>\n<p>How do I get it into Europe?<\/p>",
        "Challenge_closed_time":1641305260580,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641293452093,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has set the API_ENDPOINT to \"europe-west1-aiplatform.googleapis.com\" in their GCP Vertex pipeline, but when they run the pipeline, it is always created in the US. The user is seeking a solution to get the pipeline created in Europe.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70577610",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":14.9,
        "Challenge_reading_time":10.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3.2801352778,
        "Challenge_title":"Why is my GCP Vertex pipeline api_endpoint not right?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":92.0,
        "Challenge_word_count":61,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>The <code>location<\/code> parameter in the <code>aip.PipelineJob()<\/code> class can be used to specify in which region the pipeline will be deployed. Refer to this <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/aiplatform#class-googlecloudaiplatformpipelinejobdisplayname-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-templatepath-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-jobid-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-pipelineroot-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-parametervalues-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-any--none-enablecaching-optionalboolhttpspythonreadthedocsioenlatestlibraryfunctionshtmlbool--none-encryptionspeckeyname-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-labels-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-credentials-optionalgoogleauthcredentialscredentialshttpsgoogleapisdevpythongoogle-authlatestreferencegoogleauthcredentialshtmlgoogleauthcredentialscredentials--none-project-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-location-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none\" rel=\"nofollow noreferrer\">documentation<\/a> for more information about the <code>PipelineJob()<\/code> method.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>REGION = &quot;europe-west1&quot;\n\njob = aip.PipelineJob(\n          display_name=DISPLAY_NAME,\n          template_path=&quot;image classification_pipeline.json&quot;.replace(&quot; &quot;, &quot;_&quot;),\n          location=REGION)\n\njob.run()\n<\/code><\/pre>\n<p>The above code will deploy a pipeline in the <code>europe-west1<\/code> region. The code returns the following output. The job is now deployed in the specified region.<\/p>\n<pre><code>INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\nINFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects\/&lt;project-id&gt;\/locations\/europe-west1\/pipelineJobs\/hello-world-pipeline\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1647760091087,
        "Solution_link_count":1.0,
        "Solution_readability":50.8,
        "Solution_reading_time":30.44,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":82.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"pipeline created in wrong region"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":23.8292419444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have multiple models in Google Vertex AI and I want to create an endpoint to serve my predictions.\nI need to run aggregation algorithms, like the Voting algorithm on the output of my models.\nI have not found any ways of using the models together so that I can run the voting algorithms on the results.\nDo I have to create a new model, curl my existing models and then run my algorithms on the results?<\/p>",
        "Challenge_closed_time":1647950025928,
        "Challenge_comment_count":3,
        "Challenge_created_time":1647864699137,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has multiple models in Google Vertex AI and wants to create an endpoint to serve predictions by running aggregation algorithms like the Voting algorithm on the output of the models. However, the user has not found any ways to use the models together to run the voting algorithms on the results. The user is unsure if they have to create a new model, curl the existing models, and then run the algorithms on the results.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71557442",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":8.8,
        "Challenge_reading_time":5.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":23.7018863889,
        "Challenge_title":"How combine results from multiple models in Google Vertex AI?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":253.0,
        "Challenge_word_count":86,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1372407778700,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Oslo, Norway",
        "Poster_reputation_count":134.0,
        "Poster_view_count":74.0,
        "Solution_body":"<p>There is no in-built provision to implement aggregation algorithms in Vertex AI. To <code>curl<\/code> results from the models then aggregate them, we would need to deploy all of them to individual endpoints. Instead, I would suggest the below method to deploy the models and the meta-model(aggregate model) to a single endpoint using <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/use-custom-container\" rel=\"nofollow noreferrer\">custom containers for prediction<\/a>. The custom container requirements can be found <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p>You can load the model artifacts from GCS into a custom container. If the same set of models are used (i.e) the input models to the meta-model do not change, you can package them inside the container to reduce load time. Then, a custom HTTP logic can be used to return the aggregation output like so. This is a sample custom flask server logic.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def get_models_from_gcs():\n    ## Pull the required model artifacts from GCS and load them here.\n    models = [model_1, model_2, model_3]\n    return models\n\ndef aggregate_predictions(predictions):\n    ## Your aggregation algorithm here\n    return aggregated_result\n\n\n@app.post(os.environ['AIP_PREDICT_ROUTE'])\nasync def predict(request: Request):\n    body = await request.json()\n    instances = body[&quot;instances&quot;]\n    inputs = np.asarray(instances)\n    preprocessed_inputs = _preprocessor.preprocess(inputs)\n\n    models = get_models_from_gcs()\n    predictions = []\n    \n    for model in models:\n        predictions.append(model.predict(preprocessed_inputs))\n\n    aggregated_result = aggregate_predictions(predictions)\n\n    return {&quot;aggregated_predictions&quot;: aggregated_result}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1647950484408,
        "Solution_link_count":2.0,
        "Solution_readability":14.5,
        "Solution_reading_time":23.43,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":191.0,
        "Tool":"Vertex AI",
        "Challenge_type":"inquiry",
        "Challenge_summary":"run voting algorithm on multiple models"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":90.3942838889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>My workflow is running perfect on Experimentation, but after deployed to web service, I receive this error while post.<\/p>\n\n<p>Python Code:<\/p>\n\n<pre><code># -*- coding: utf-8 -*-\n\n#import sys\nimport pickle\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree \n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    print('input dataframe1 ',dataframe1)\n    decision_tree_pkl_predictive_maint = r'.\\Script Bundle\\decision_tree_pkl_predictive_maint.pkl'\n\n    #sys.path.insert(0,\".\\Script Bundle\")\n    #model = pickle.load(open(\".\\Script Bundle\\decision_tree_pkl_predictive_maint.pkl\", 'rb'))\n\n    modle_file = open(decision_tree_pkl_predictive_maint,\"rb\")\n    model = pickle.load(modle_file)\n\n    #return the mode of prediciton\n    result = model.predict(dataframe1)\n    print(result)\n    result_df = pd.DataFrame({'prediction_class':result})\n    return result_df,\n<\/code><\/pre>\n\n<p>ERROR:<\/p>\n\n<p>Execute Python Script RRS : Error 0085: The following error occurred during script evaluation, please view the output log for more information: ---------- Start of error message from Python interpreter ---------- Caught exception while executing function: Traceback (most recent call last): File \"\\server\\InvokePy.py\", line 120, in executeScript outframe = mod.azureml_main(*inframes) File \"\\temp-1036260731852293620.py\", line 46, in azureml_main modle_file = open(decision_tree_pkl_predictive_maint,\"rb\") FileNotFoundError: [Errno 2] No such file or directory: '.\\Script Bundle\\decision_tree_pkl_predictive_maint.pkl' ---------- End of error message from Python interpreter ----------<\/p>\n\n<p>Please, Advice.<\/p>",
        "Challenge_closed_time":1580091525612,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579766106190,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering Error 0085 while executing a Python script in Azure Web service, but not in ML Experiment. The error message indicates that the file 'decision_tree_pkl_predictive_maint.pkl' cannot be found in the specified directory. The user is seeking advice on how to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59873804",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.1,
        "Challenge_reading_time":22.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":90.3942838889,
        "Challenge_title":"Error 0085 while executing python script in Azure Web service but not in ML Experiment",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":215.0,
        "Challenge_word_count":163,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1554724240183,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation_count":29.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>The issue has to do with your file path. Ensure that you have included the correct path.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.1,
        "Solution_reading_time":1.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"file not found"
    },
    {
        "Answerer_created_time":1577817693600,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":329.4975816667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am aware that Sagemaker <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-model-endpoints.html#multi-model-support\" rel=\"nofollow noreferrer\">does not support multi-model endpoints for their built-in image classification algorithm<\/a>. However, in the documentation they hint at building a custom container to use &quot;any other framework or algorithm&quot; with the multi-model endpoint functionality:<\/p>\n<blockquote>\n<p>To use any other framework or algorithm, use the SageMaker inference toolkit to build a container that supports multi-model endpoints. For information, see <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html\" rel=\"nofollow noreferrer\">Build Your Own Container with Multi Model Server<\/a>.<\/p>\n<\/blockquote>\n<p>Ideally, I would like to deploy many (20+) image classification models I have already trained to a single endpoint to save on costs. However, after reading the &quot;Build Your Own Container&quot; guide it is still not exactly clear to me how to build a custom inference container for the models produced by a non-custom algorithm. Most of the tutorials and example notebooks refer to using Pytorch or Sklearn. It is not clear to me that I could make inferences using these libraries on the models I've created with the built-in image classification algorithm.<\/p>\n<p><em>Is<\/em> it possible to create a container to support multi-model endpoints for unsupported built-in Sagemaker algorithms? If so, would somebody be able to hint at how this might be done?<\/p>",
        "Challenge_closed_time":1612378891027,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611192699733,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to deploy multiple image classification models to a single endpoint using Sagemaker's multi-model endpoint functionality. However, Sagemaker does not support multi-model endpoints for their built-in image classification algorithm. The user is trying to build a custom inference container for the models produced by a non-custom algorithm, but is unsure how to do so as most tutorials and examples refer to Pytorch or Sklearn. The user is seeking guidance on whether it is possible to create a container to support multi-model endpoints for unsupported built-in Sagemaker algorithms and how to do so.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65819978",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.8,
        "Challenge_reading_time":20.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":329.4975816667,
        "Challenge_title":"Sagemaker multi-model endpoints with unsupported built-in algorithms",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":524.0,
        "Challenge_word_count":207,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1611191329712,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>yes, it is possible to deploy the built in image classification models as a SageMaker multi model endpoint. The key is that the image classification uses <a href=\"https:\/\/mxnet.apache.org\/versions\/1.7.0\/\" rel=\"nofollow noreferrer\">Apache MXNet<\/a>. You can extract the model artifacts (SageMaker stores them in a zip file named model.tar.gz in S3), then load them in to MXNet. The SageMaker MXNet container supports multi model endpoints, so you can use that to deploy the model.<\/p>\n<p>If you unzip the model.tar.gz from this algorithm, you'll find three files:<\/p>\n<p>image-classification-****.params<\/p>\n<p>image-classification-symbol.json<\/p>\n<p>model-shapes.json<\/p>\n<p>The MxNet container expects these files to be named <strong>image-classification-0000.params, model-symbol.json, and model-shapes.json<\/strong>. So I unzipped the zip file, renamed the files and rezipped them. For more information on the MXNet container check out the <a href=\"https:\/\/github.com\/aws\/sagemaker-mxnet-inference-toolkit\" rel=\"nofollow noreferrer\">GitHub repository<\/a>.<\/p>\n<p>After that you can deploy the model as a single MXNet endpoint using the SageMaker SDK with the following code:<\/p>\n<pre><code>from sagemaker import get_execution_role\nfrom sagemaker.mxnet.model import MXNetModel\n\nrole = get_execution_role()\n\nmxnet_model = MXNetModel(model_data=s3_model, role=role, \n                         entry_point='built_in_image_classifier.py', \n                         framework_version='1.4.1',\n                         py_version='py3')\n\npredictor = mxnet_model.deploy(instance_type='ml.c4.xlarge', initial_instance_count=1)\n<\/code><\/pre>\n<p>The entry point Python script can be an empty Python file for now. We will be using the default inference handling provided by the MXNet container.<\/p>\n<p>The default MXNet container only accepts JSON, CSV, and Numpy arrays as valid input. So you will have to format your input in to one of these three formats. The code below demonstrates how I did it with Numpy arrays:<\/p>\n<pre><code>import cv2\nimport io\n\nnp_array = cv2.imread(filename=img_filename)\nnp_array = np_array.transpose((2,0,1))\nnp_array = np.expand_dims(np_array, axis=0)\n\nbuffer = io.BytesIO()\nnp.save(buffer, np_array)\n\nresponse = sm.invoke_endpoint(EndpointName='Your_Endpoint_name', Body=buffer.getvalue(), ContentType='application\/x-npy')\n<\/code><\/pre>\n<p>Once you have a single endpoint working with MXNet container, you should be able to get it running in multi model endpoint using the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/multi_data_model.html\" rel=\"nofollow noreferrer\">SageMaker MultiDataModel constructor<\/a>.<\/p>\n<p>If you want to use a different input data type so you don't have to do the preprocessing in your application code, you can overwrite the input_fn method in the MxNet container by providing it in the entry_point script. <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/mxnet\/using_mxnet.html\" rel=\"nofollow noreferrer\">See here<\/a> for more information. If you do this, you could pass the image bytes directly to SageMaker, without formatting the numpy arrays.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":12.5,
        "Solution_reading_time":39.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":30.0,
        "Solution_word_count":347.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"create custom inference container"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":17.1430497222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We have built numerous diagnostic models which can be reduced to equations and code that will allow us to repeat the work. We have the code physically available to us, so it can be installed in our own software.  <\/p>\n<p>Now I would like to use artificial neural networks to build a prediction model. After I build that model, will I be able to take that model and transfer it to our own software environment? My concern is that the prediction model will just be a black box. Thanks<\/p>",
        "Challenge_closed_time":1591950718436,
        "Challenge_comment_count":1,
        "Challenge_created_time":1591889003457,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has built diagnostic models using code and equations that can be installed in their own software. They now want to build a prediction model using artificial neural networks and transfer it to their own software environment, but are concerned that the model will be a black box.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/34890\/access-to-neural-network-model",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.0,
        "Challenge_reading_time":6.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":17.1430497222,
        "Challenge_title":"Access to neural network model",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":93,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a>@OliverBathe-8330<\/a> Please follow the below Deployment scenarios. If possible can you please add more details about the use case.<\/p>\n<p>Option A: Use the DevOps pipeline integration to rollout to production Using same approach as in the <a href=\"https:\/\/github.com\/Microsoft\/MLOpsPython\">MLOps repo<\/a>, set up a <a href=\"https:\/\/github.com\/microsoft\/MLOpsPython\/blob\/master\/docs\/getting_started.md#set-up-build-release-trigger-and-release-multi-stage-pipeline\">release trigger for your DevOps release pipeline<\/a> listening from your dev workspace model registry but then deploy to your production workspace (requires registering again in Prod model registry, call model.deploy() in the Prod workspace<\/p>\n<p>Option B: Use the AML pipeline to rollout to production Following same example as above, add additional PythonScriptStep in your AML pipeline to register and deploy model in the Production workspace<\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where<\/a><\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":16.3,
        "Solution_reading_time":14.62,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":110.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"neural network transparency"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":123.0932055556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a code R (manipulateXY.R) taking parameters X (from picklist), Y (a \"not constrained\" value) from a text file (parameter.txt) and producing n images.\nI want to put this code as a \"R script\" in Azure ML, and to produce a web service pointing to that logic (manipulateXY). The question is: how can I pass parameters to the Azure code? I need it because I want a web app with the following outfit<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/dq0Kr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/dq0Kr.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>such that I choose the X and Y and press \"Run\", it calls the logic in Azure ML, it takes the generated images and put them on the web-app. <\/p>",
        "Challenge_closed_time":1466963216443,
        "Challenge_comment_count":0,
        "Challenge_created_time":1466520080903,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to put an R script in Azure ML and produce a web service pointing to that logic. However, the user is facing challenges in passing parameters to the Azure code to create a web app that allows the user to choose X and Y parameters and generate images by calling the logic in Azure ML.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37947524",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":9.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":123.0932055556,
        "Challenge_title":"AzureML: taking parameters as input",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":91.0,
        "Challenge_word_count":117,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436432728608,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Colleferro, Italy",
        "Poster_reputation_count":809.0,
        "Poster_view_count":361.0,
        "Solution_body":"<p>You can use web service parameters as shown here - <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-web-service-parameters\/\" rel=\"nofollow\">https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-web-service-parameters<\/a>\/<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":52.8,
        "Solution_reading_time":3.97,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":12.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"difficulty passing parameters"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.5504875,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi, I'm going to use the canvas by connecting to S3.\nWhen using sagemaker canvas, should the canvas region and S3 region be the same?\nThank you.",
        "Challenge_closed_time":1658234155411,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658221373656,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring whether the SageMaker Canvas region and S3 region should be the same when using the Canvas to connect to S3.",
        "Challenge_last_edit_time":1667926015744,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QULHZtj6HwQReouXor72UuSg\/should-sagemaker-canvas-region-and-s3-region-be-the-same",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":2.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":3.5504875,
        "Challenge_title":"Should SageMaker Canvas region and S3 region be the same?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":112.0,
        "Challenge_word_count":36,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi, S3 does not have to be in the same region as SageMaker Canvas, but make sure your user has the correct permissions to access the bucket!",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1658234155411,
        "Solution_link_count":0.0,
        "Solution_readability":11.5,
        "Solution_reading_time":1.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":27.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"SageMaker Canvas and S3 region"
    },
    {
        "Answerer_created_time":1254957460063,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"North Carolina, USA",
        "Answerer_reputation_count":2484.0,
        "Answerer_view_count":362.0,
        "Challenge_adjusted_solved_time":27.4381997222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>To use Azure Machine Learning Web service <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/consume-web-services\" rel=\"nofollow noreferrer\">here<\/a> you can find some sample code in C#, R, Python and JavaScript. I want to use it in PowerShell.\nI found <a href=\"https:\/\/www.sepago.com\/blog\/2015\/11\/30\/zugriff-mit-powershell-auf-azure-machine-learning-api-azureml\" rel=\"nofollow noreferrer\">this<\/a> tutorial, but when I am running bellow line of code, it will return error that it is not recognized:<\/p>\n\n<pre><code>Set-AzureMLWebServiceConnection -URI $Url -APIKey $API_key\n\nOutput:\nSet-AzureMLWebServiceConnection : The term 'Set-AzureMLWebServiceConnection' is not recognized as the name of a cmdlet, function, script file, or operable \nprogram. Check the spelling of the name, or if a path was included, verify that the path is correct and try again.\nAt C:\\Users\\Reza\\Desktop\\ndbench\\Azure\\Automation\\01_get_metrics\\add_target_to_tables - runbook_01.ps1:33 char:1\n+ Set-AzureMLWebServiceConnection -URI $Url -APIKey $API_key\n+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    + CategoryInfo          : ObjectNotFound: (Set-AzureMLWebServiceConnection:String) [], CommandNotFoundException\n    + FullyQualifiedErrorId : CommandNotFoundException\n<\/code><\/pre>\n\n<p>I can't found <code>Set-AzureMLWebServiceConnection<\/code> in my PowerShell command-list and I don't know how I can enable\/install it.\n<a href=\"https:\/\/i.stack.imgur.com\/YMso7.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YMso7.jpg\" alt=\"enter image description here\"><\/a>\nCan you please guide me, how I can connect to Azure Machine Learning Web service using PowerShell?<\/p>",
        "Challenge_closed_time":1523724686212,
        "Challenge_comment_count":1,
        "Challenge_created_time":1523625908693,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to connect to Azure Machine Learning Web service using PowerShell but is encountering an error that the command 'Set-AzureMLWebServiceConnection' is not recognized. The user is seeking guidance on how to enable\/install the command and connect to the service using PowerShell.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49818134",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":15.2,
        "Challenge_reading_time":22.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":27.4381997222,
        "Challenge_title":"how connect to Azure Machine Learning Web service using PowerShell?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":303.0,
        "Challenge_word_count":169,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1433870950220,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tehran, Iran",
        "Poster_reputation_count":1316.0,
        "Poster_view_count":201.0,
        "Solution_body":"<p>The comment @gvee mentioned may be the best to use going forward though it is in beta.<\/p>\n\n<p>However, to answer your question, use the <code>Install-Module -Name AzureML<\/code> <a href=\"https:\/\/www.powershellgallery.com\/packages\/AzureML\/1.0.1\" rel=\"nofollow noreferrer\">command<\/a> to get access to the Azure ML commands.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ZerMp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZerMp.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.3,
        "Solution_reading_time":6.58,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":48.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unrecognized command"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":0.6330175,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am attempting to pass json data into my sagemaker model through a lambda function. Currently, I am using a testing model that makes relatively quick inferences and returns them to the lambda function through the invoke_endpoint call. However, eventually a more advanced model will be implemented which might take longer than a lambda function can fun for (15 minutes maximum) to produce inferences. In the case that I call invoke_endpoint in one lambda function, can I return the response to another lambda function which is invoked by the sagemaker endpoint response? Even better, can I shut down the current lambda function after sending the data to sagemaker, and re-invoke it upon a response? I need to store the inference in DynamoDB, which is why I need a response (Unless I can update the saved model to store inferences directly, in which case I need the lambda function to not expect a response from invoke_endpoint). Sorry for my ignorance, I am a bit new to sagemaker.<\/p>",
        "Challenge_closed_time":1623878015320,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623875736457,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to pass JSON data into a SageMaker model through a Lambda function. They are concerned that a more advanced model may take longer than the maximum 15 minutes a Lambda function can run for. They are wondering if they can return the response to another Lambda function or shut down the current Lambda function after sending the data to SageMaker and re-invoke it upon a response. They need a response to store the inference in DynamoDB.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68009703",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.7,
        "Challenge_reading_time":12.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.6330175,
        "Challenge_title":"Can \"Invoke_endpoint\" calls timeout a lambda function?",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":181.0,
        "Challenge_word_count":172,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1622063222448,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":5.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>When calling <code>invoke_endpoint<\/code>, the underlying model invocation must take <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html#your-algorithms-inference-code-how-containe-serves-requests\" rel=\"nofollow noreferrer\">less than 1 minute<\/a>. If a single model execution needs more time to execute, consider running the model in Lambda itself, in SageMaker Training API (if its coldstart is acceptable) or in a custom service. If the invocation is made of several shorter calls you can also chain multiple services together with Step Functions.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":18.0,
        "Solution_reading_time":7.82,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"handling long-running Lambda function"
    },
    {
        "Answerer_created_time":1460437080990,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":386.0,
        "Answerer_view_count":42.0,
        "Challenge_adjusted_solved_time":1.0301077778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have the following data:<\/p>\n\n<ul>\n<li>Identifier of a person<\/li>\n<li>Days in location (starts at 1 and runs until event)<\/li>\n<li>Age of person in months at that time (so this increases as the days in location increase too).<\/li>\n<li>Smoker (boolean), doesn't change over time in our case<\/li>\n<li>Sex, doesn't change over time<\/li>\n<li>Fall (boolean) this is an event that may never happen, or can happen multiple times during the complete period for a certain person<\/li>\n<li>Number of wounds: (this can go from 0 to 8), a wound mostly doesn't heal immediately so it mostly stays open for a certain period of time<\/li>\n<li>Event we want to predict (boolean), only the last row of a person will have value true for this<\/li>\n<\/ul>\n\n<p>I have this data for 1500 people (in total 1500000 records so on average about 1000 records per person). For some people the event I want to predict takes place after a couple of days, for some after 10 years.  For everybody in the dataset the event will take place, so the last record for a certain identifier will always have the event we want to predict as 1.<\/p>\n\n<p>I'm new to this and all the documentation I have found so far doesn't demonstrate time series for multiple persons or objects. When I for example split the data in the machine learning studio, I want to keep records of the same person over time together.<\/p>\n\n<p>Would it be possible to feed the system after the model is trained with new records and for each day that passes it would give the estimate of the event taking place in the next 5 days?<\/p>\n\n<p>Edit: sample data of 2 persons: <a href=\"http:\/\/pastebin.com\/KU4bjKwJ\" rel=\"nofollow\">http:\/\/pastebin.com\/KU4bjKwJ<\/a><\/p>",
        "Challenge_closed_time":1477404012396,
        "Challenge_comment_count":2,
        "Challenge_created_time":1477382328947,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has a dataset of 1500 people with various properties such as age, smoker, sex, number of wounds, and fall events. They want to create a model that predicts an event based on other time series events and properties of an object. The user is struggling to find documentation on time series for multiple persons or objects and wants to keep records of the same person over time together. They also want to know if it's possible to feed the system with new records and get an estimate of the event taking place in the next 5 days.",
        "Challenge_last_edit_time":1477400304008,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40234432",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":13.9,
        "Challenge_reading_time":21.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":6.0231802778,
        "Challenge_title":"Create a model that predicts an event based on other time series events and properties of an object",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":596.0,
        "Challenge_word_count":300,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1345413556180,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belgium",
        "Poster_reputation_count":1946.0,
        "Poster_view_count":211.0,
        "Solution_body":"<p>sounds like very similar to this sample:<\/p>\n\n<p><a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/df7c518dcba7407fb855377339d6589f\" rel=\"nofollow\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/df7c518dcba7407fb855377339d6589f<\/a><\/p>\n\n<p>Unfortunately there is going to be a bit of R code involved. Yes you should be able to retrain the model with new data.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.8,
        "Solution_reading_time":5.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"time series modeling and prediction"
    },
    {
        "Answerer_created_time":1565376125572,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":31.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":10701.8852247222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a endpoint in Amazon SageMaker (Image-classification algorithm) in Jupyter notebook that works fine. In Lambda function works fine too, when I call the Lambda function from API Gateway, from test of API Gateway, works fine too.<\/p>\n<p>The problem is when I call the API from Postman according this answer: <a href=\"https:\/\/stackoverflow.com\/questions\/39660074\/post-image-data-using-postman\">&quot;Post Image data using POSTMAN&quot;<\/a><\/p>\n<p>The code in Lambda is:<\/p>\n<pre><code>import boto3\nimport json\nimport base64\n\nENDPOINT_NAME = &quot;DEMO-XGBoostEndpoint-Multilabel&quot;\nruntime= boto3.client(&quot;runtime.sagemaker&quot;)\nimagen_ = &quot;\/tmp\/imageToProcess.jpg&quot;\n\ndef write_to_file(save_path, data):\n    with open(save_path, &quot;wb&quot;) as f:\n        f.write(base64.b64decode(data))\n\ndef lambda_handler(event, context):\n    img_json = json.loads(json.dumps(event))\n\n    write_to_file(imagen_, json.dumps(event, indent=2))\n\n    with open(imagen_, &quot;rb&quot;) as image:\n        f = image.read()\n        b = bytearray(f)\n\n    payload = b\n\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                       ContentType=&quot;application\/x-image&quot;,\n                                       Body=payload)\n\n    #print(response)\n    result = json.loads(response[&quot;Body&quot;].read().decode())\n    print(result)\n    predicted_label=[]\n    classes = [&quot;chair&quot;, &quot;handbag&quot;, &quot;person&quot;, &quot;traffic light&quot;, &quot;clock&quot;]\n    for idx, val in enumerate(classes):\n        print(&quot;%s:%f &quot;%(classes[idx], result[idx]), end=&quot;&quot;)\n        predicted_label += (classes[idx], result[idx])\n\n    return {\n      &quot;statusCode&quot;: 200,\n      &quot;headers&quot;: { &quot;content-type&quot;: &quot;application\/json&quot;},\n      &quot;body&quot;:  predicted_label\n}\n<\/code><\/pre>\n<p>The error is:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/var\/task\/lambda_function.py&quot;, line 26, in lambda_handler\n    Body=payload)\n  File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 316, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 626, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message &quot;unable to evaluate payload provided&quot;. See https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/DEMO-XGBoostEndpoint-Multilabel in account 866341179300 for more information. ```\n<\/code><\/pre>",
        "Challenge_closed_time":1597003004372,
        "Challenge_comment_count":2,
        "Challenge_created_time":1595742856227,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while calling an Amazon SageMaker endpoint for an image-classification algorithm from Postman. The Lambda function and API Gateway work fine, but the error occurs when calling the API from Postman. The error message indicates that the payload provided is unable to be evaluated.",
        "Challenge_last_edit_time":1595997749087,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63096583",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":18.4,
        "Challenge_reading_time":35.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":350.0411513889,
        "Challenge_title":"SageMaker: An error occurred (ModelError) when calling the InvokeEndpoint operation: unable to evaluate payload provided",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":3996.0,
        "Challenge_word_count":218,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1565376125572,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I resolved with <a href=\"https:\/\/medium.com\/swlh\/upload-binary-files-to-s3-using-aws-api-gateway-with-aws-lambda-2b4ba8c70b8e\" rel=\"nofollow noreferrer\">this<\/a> post:<\/p>\n<p>Thank all<\/p>\n<p>Finally the code in lambda function is:<\/p>\n<pre><code>import os\nimport boto3\nimport json\nimport base64\n\nENDPOINT_NAME = os.environ['endPointName']\nCLASSES = &quot;[&quot;chair&quot;, &quot;handbag&quot;, &quot;person&quot;, &quot;traffic light&quot;, &quot;clock&quot;]&quot;\nruntime= boto3.client(&quot;runtime.sagemaker&quot;)\n\ndef lambda_handler(event, context):\n    file_content = base64.b64decode(event['content'])\n\n    payload = file_content\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType=&quot;application\/x-image&quot;, Body=payload)\n\n    result = json.loads(response[&quot;Body&quot;].read().decode())\n    print(result)\n    predicted_label=[]\n    classes = CLASSES\n    for idx, val in enumerate(classes):\n       print(&quot;%s:%f &quot;%(classes[idx], result[idx]), end=&quot;&quot;)\n       predicted_label += (classes[idx], result[idx])\n\n    return {\n      &quot;statusCode&quot;: 200,\n      &quot;headers&quot;: { &quot;content-type&quot;: &quot;application\/json&quot;},\n      &quot;body&quot;:  predicted_label\n    }\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1634524535896,
        "Solution_link_count":1.0,
        "Solution_readability":22.8,
        "Solution_reading_time":16.28,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":74.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to evaluate payload"
    },
    {
        "Answerer_created_time":1374169767267,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":548.0,
        "Answerer_view_count":70.0,
        "Challenge_adjusted_solved_time":15.3127302778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>A bit confused with automatisation of Sagemaker retraining the model.<\/p>\n<p>Currently I have a notebook instance with Sagemaker <code>LinearLerner<\/code> model making the classification task. So using <code>Estimator<\/code> I'm making training, then deploying the model creating <code>Endpoint<\/code>. Afterwards using <code>Lambda<\/code> function for invoke this endpoint, I add it to the <code>API Gateway<\/code> receiving the api endpoint which can be used for POST requests and sending back response with class.<\/p>\n<p>Now I'm facing with the problem of retraining. For that I use <code>serverless<\/code> approach and <code>lambda<\/code> function getting environment variables for training_jobs. But the problem that Sagemaker not allow to rewrite training job and you can only create new one. My goal is to automatise the part when the new training job and the new endpoint config will apply to the existing endpoint that I don't need to change anything in API gateway. Is that somehow possible to automatically attach new endpoint config with existing endpoint?<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1594485516256,
        "Challenge_comment_count":0,
        "Challenge_created_time":1594430390427,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges with automating the retraining of a Sagemaker LinearLerner model. They have created an endpoint using an Estimator and deployed it, and are using a Lambda function to invoke the endpoint and add it to an API Gateway. However, they are unable to rewrite the training job and can only create a new one, which is causing issues with automating the process. The user is seeking a solution to automatically attach a new endpoint configuration with the existing endpoint without having to change anything in the API Gateway.",
        "Challenge_last_edit_time":1594502576883,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62844211",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.7,
        "Challenge_reading_time":14.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":15.3127302778,
        "Challenge_title":"Updating Sagemaker Endpoint with new Endpoint Configuration",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":3443.0,
        "Challenge_word_count":161,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1386491614716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2778.0,
        "Poster_view_count":352.0,
        "Solution_body":"<p>If I am understanding the question correctly, you should be able to use <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateEndpointConfig.html\" rel=\"nofollow noreferrer\">CreateEndpointConfig<\/a> near the end of the training job, then use <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_UpdateEndpoint.html\" rel=\"nofollow noreferrer\">UpdateEndpoint<\/a>:<\/p>\n<p><code>Deploys the new EndpointConfig specified in the request, switches to using newly created endpoint, and then deletes resources provisioned for the endpoint using the previous EndpointConfig (there is no availability loss).<\/code><\/p>\n<p>If the API Gateway \/ Lambda is routed via the endpoint ARN, that should not change after using <code>UpdateEndpoint<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.1,
        "Solution_reading_time":10.31,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":79.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to rewrite training job"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.1511888889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,  <\/p>\n<p>I am getting the following error message:  <br \/>\n&quot;WebserviceException: WebserviceException: Message: Service diabetes-service with the same name already exists, please use a different service name or delete the existing service. InnerException None ErrorResponse { &quot;error&quot;: { &quot;message&quot;: &quot;Service diabetes-service with the same name.&quot;  <\/p>\n<p>Please can you help with deleting the service in question?  <\/p>\n<p>Thanks,  <\/p>\n<p>Naveen  <\/p>",
        "Challenge_closed_time":1610680137287,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610661593007,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a WebserviceException error message while trying to delete an existing service. The error message suggests that a service with the same name already exists and the user needs to either use a different name or delete the existing service. The user is seeking help with deleting the service in question.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/231106\/webserviceexception-how-to-delete-an-existing-serv",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":6.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":5.1511888889,
        "Challenge_title":"WebserviceException: How to delete an existing service",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":65,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello, Naveen. This error message means that in your current AML workspace, there already exists a real-time endpoint(or service) whose name is &quot;diabetes-service&quot;, so you can't deploy a new service with this same name because it will cause duplication.   <\/p>\n<p>You can check your workspace in our portal <a href=\"https:\/\/ml.azure.com\/selectWorkspace\">https:\/\/ml.azure.com\/selectWorkspace<\/a> , in the sidebar you can find a &quot;Endpoints&quot; button, you can find all your &quot;real-time endpoint&quot; there. Then please delete the dup service, after deletion you can deploy your new service with the name &quot;diabetes-service&quot;.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.6,
        "Solution_reading_time":8.4,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":85.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"service deletion error"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":111.6768886111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have some data in S3 and I want to create a lambda function to predict the output with my deployed aws sagemaker endpoint then I put the outputs in S3 again. Is it necessary in this case to create an api gateway like decribed in this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">link<\/a> ? and in the lambda function what I have to put. I expect to put (where to find the data, how to invoke the endpoint, where to put the data) <\/p>\n\n<p>Thanks<\/p>",
        "Challenge_closed_time":1549874448016,
        "Challenge_comment_count":0,
        "Challenge_created_time":1549472411217,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user wants to create a lambda function to predict the output using their deployed AWS Sagemaker endpoint and store the outputs in S3. They are unsure if they need to create an API gateway and what to include in the lambda function. They expect to include information on where to find the data, how to invoke the endpoint, and where to put the data.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54558832",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":7.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":111.6768886111,
        "Challenge_title":"call sagemaker endpoint using lambda function",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":5259.0,
        "Challenge_word_count":87,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1502010899808,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tunis, Tunisia",
        "Poster_reputation_count":109.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>you definitely don't have to create an API in API Gateway. You can invoke the endpoint directly using the invoke_endpoint() API, passing the endpoint name, the content type, and the payload.<\/p>\n\n<p>For example:<\/p>\n\n<pre><code>import boto3\n\nendpoint_name = &lt;INSERT_ENDPOINT_NAME&gt;\nruntime = boto3.Session().client(service_name='sagemaker-runtime',region_name='us-east-1')\n\nresponse = runtime.invoke_endpoint(EndpointName=endpoint_name, ContentType='application\/x-image', Body=payload)\nprint(response['Body'].read())\n<\/code><\/pre>\n\n<p>More examples here using a Lambda function: <a href=\"https:\/\/medium.com\/@julsimon\/using-chalice-to-serve-sagemaker-predictions-a2015c02b033\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/@julsimon\/using-chalice-to-serve-sagemaker-predictions-a2015c02b033<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":20.6,
        "Solution_reading_time":10.89,
        "Solution_score_count":4.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"create lambda function"
    },
    {
        "Answerer_created_time":1605283363407,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":31.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":289.2941775,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>This seems to be a tricky thing to do, as I haven't found too much documentation for it. I'm trying to deploy a Huggingface pre-trained model for NLU to a SageMaker endpoint. Naturally, I don't want to do this manually, I'd like to automate it through CloudFormation. I found a somewhat <a href=\"https:\/\/faun.pub\/mastering-the-mystical-art-of-model-deployment-part-2-deploying-amazon-sagemaker-endpoints-with-cf9539dc2579\" rel=\"nofollow noreferrer\">useful article<\/a> on how to deploy, but the name of the training model is confusing and I don't know where I would find the right name for the model I want to deploy or where I would put that name (I want to deploy an <a href=\"https:\/\/huggingface.co\/sentence-transformers\/all-MiniLM-L6-v2\" rel=\"nofollow noreferrer\">all-MiniLM-L6-v2<\/a> model).<\/p>\n<p>Is this possible to do? Do I need to deploy a container? If so, how do I set up the container to process requests and return the text embeddings from the model? I've looked into doing this with just a lambda (which would satisfy the automated deployment process), but the packages I need to use greatly exceed the 250MB limit for lambda+layers.<\/p>\n<p>How do I deploy an endpoint from CloudFormation? Does anyone have experience doing this? If so, please share your wisdom.<\/p>",
        "Challenge_closed_time":1659976220936,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658934967237,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy a Huggingface pre-trained model for NLU to a SageMaker endpoint through CloudFormation, but is having difficulty finding the correct name for the model and setting up the container to process requests and return the text embeddings from the model. They have looked into using a lambda, but the required packages exceed the 250MB limit. The user is seeking advice on how to deploy an endpoint from CloudFormation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73140531",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":9.9,
        "Challenge_reading_time":17.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":289.2371386111,
        "Challenge_title":"Deploy Pre-Trained model to SageMaker Endpoint from CloudFormation",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":103.0,
        "Challenge_word_count":193,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1605283363407,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>To anyone curious, this is how I ended up solving this issue:<\/p>\n<p>I ran a Jupyter notebook locally to create the model artifact. Once complete, I zipped the model artifact into a tar.gz file.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoModel, AutoTokenizer\nfrom os import makedirs\n\nsaved_model_dir = 'saved_model_dir'\nmakedirs(saved_model_dir, exist_ok=True)\n\n# models were obtained from https:\/\/huggingface.co\/models\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers\/multi-qa-MiniLM-L6-cos-v1')\nmodel = AutoModel.from_pretrained('sentence-transformers\/multi-qa-MiniLM-L6-cos-v1')\n\ntokenizer.save_pretrained(saved_model_dir)\nmodel.save_pretrained(saved_model_dir)\n<\/code><\/pre>\n<pre class=\"lang-bash prettyprint-override\"><code>cd saved_model_dir &amp;&amp; tar czvf ..\/model.tar.gz *\n<\/code><\/pre>\n<p>I included a script in my pipeline to then upload that artifact to S3.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>aws s3 cp path\/to\/model.tar.gz s3:\/\/bucket-name\/prefix\n<\/code><\/pre>\n<p>I also created a CloudFormation template that would stand up my SageMaker resources. The tricky part of this was finding a container image to use, and a colleague was able to point me to <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">this repo<\/a> that contained a massive list of AWS-maintained container images for deep learning and inference. From there, I just needed to select the one that fit my needs.<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>Resources:\n  SageMakerModel:\n    Type: AWS::SageMaker::Model\n    Properties:\n      PrimaryContainer:\n        Image: 763104351884.dkr.ecr.us-east-1.amazonaws.com\/pytorch-inference:1.12.0-cpu-py38-ubuntu20.04-sagemaker # image resource found at https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\n        Mode: SingleModel\n        ModelDataUrl: s3:\/\/path\/to\/model.tar.gz\n      ExecutionRole: \n      ModelName: inference-model\n\n  SageMakerEndpointConfig:\n    Type: AWS::SageMaker::EndpointConfig\n    Properties:\n      EndpointConfigName: endpoint-config-name\n      ProductionVariants:\n        - ModelName: inference-model\n          InitialInstanceCount: 1\n          InstanceType: ml.t2.medium\n          VariantName: dev\n  \n  SageMakerEndpoint:\n    Type: AWS::SageMaker::Endpoint\n    Properties:\n      EndpointName: endpoint-name\n      EndpointConfigName: !GetAtt SageMakerEndpointConfig.EndpointConfigName\n<\/code><\/pre>\n<p>Once the PyTorch model is created locally, this solution essentially automates the process of provisioning and deploying a SageMaker endpoint for inference. If I need to switch the model, I just need to run my notebook code and it will overwrite my existing model artifact. Then I can redeploy and my pipeline will re-upload the artifact to S3, modify the existing SageMaker resources, and the solution will begin operating using the new model.<\/p>\n<p>This may not be the most elegant solution out there, so any suggestions or pointers would be much appreciated!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1659976426276,
        "Solution_link_count":3.0,
        "Solution_readability":17.0,
        "Solution_reading_time":39.05,
        "Solution_score_count":2.0,
        "Solution_sentence_count":21.0,
        "Solution_word_count":297.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"deploy SageMaker endpoint"
    },
    {
        "Answerer_created_time":1435766573232,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":10645.0,
        "Answerer_view_count":1173.0,
        "Challenge_adjusted_solved_time":13.1109433333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I try to use a trained model from Microsoft Azure Machine Learning Studio in Azure Stream Analytics.\nBefore I start work with my IoT-Stream sensor data, I try this sample: \n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/stream-analytics\/stream-analytics-machine-learning-integration-tutorial\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/stream-analytics\/stream-analytics-machine-learning-integration-tutorial<\/a><\/p>\n\n<p>I can deploy the web service and it works fine with a console application.\nThe result from web service:<\/p>\n\n<pre><code>{\n    \"Results\": {\n        \"output1\": {\n            \"type\": \"table\",\n            \"value\": {\n                \"ColumnNames\": [\"Sentiment\", \"Score\"],\n                \"ColumnTypes\": [\"String\", \"Double\"],\n                \"Values\": [\n                    [\"neutral\", \"0.564501523971558\"]\n                ]\n            }\n        }\n    }\n}\n<\/code><\/pre>\n\n<p>The T-SQL in Stream Analytics from tutorial looks like:<\/p>\n\n<pre><code>WITH subquery AS (  \n    SELECT text, sentiment(text) as result from input  \n)  \n\nSelect text, result.[Scored Labels]  \nInto output  \nFrom subquery\n<\/code><\/pre>\n\n<p>Unfortunately it does not work. Can someone explain <code>result.[Scored Labels]<\/code><\/p>\n\n<p>Is it possible to debug my Stream Analytic job?\nI get no output. No result-file, no warning, no exception...<\/p>",
        "Challenge_closed_time":1481449233936,
        "Challenge_comment_count":0,
        "Challenge_created_time":1481402034540,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use a trained model from Microsoft Azure Machine Learning Studio in Azure Stream Analytics to work with IoT-Stream sensor data. They have followed a tutorial and deployed the web service, which works fine with a console application. However, the T-SQL in Stream Analytics from the tutorial does not work, and the user is unable to get any output or debug their Stream Analytic job. They are seeking an explanation for the term \"result.[Scored Labels]\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41080045",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.6,
        "Challenge_reading_time":16.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":13.1109433333,
        "Challenge_title":"How can I use ML function in Azure Stream Analytics?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":752.0,
        "Challenge_word_count":135,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1400791038563,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":2437.0,
        "Poster_view_count":234.0,
        "Solution_body":"<p>It is not currently possible to test your query when you use a function to call out to Azure ML. The test query functionality runs in the web browser window so I guess they haven't implemented that feature yet. <\/p>\n\n<p>I expect if you start the job it will actually work. However you may need to change <code>result.[Scored Labels]<\/code> to match the columns in the Azure ML API output by saying <code>result.Sentiment<\/code> and <code>result.Score<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":5.73,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":74.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"T-SQL not working"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":12.2603875,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I've searched for hours for this and can't find a single thing that answers the question. I've created and published a new Azure Machine Learning service, and have created an endpoint. I can call the service using the Postman REST CLient, but accessing it via a JavaScript webpage returns a console log saying that CORS is enabled for the service. Now, for the life of me, I can't figure out how to disable CORS for Azure Machine Learning services. Any help would be much appreciated, thanks!<\/p>",
        "Challenge_closed_time":1454387492627,
        "Challenge_comment_count":4,
        "Challenge_created_time":1421424031317,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created and published an Azure Machine Learning service with an endpoint. While the service can be called using Postman REST Client, accessing it via a JavaScript webpage returns a console log saying that CORS is enabled for the service. The user is seeking help to disable CORS for Azure Machine Learning services.",
        "Challenge_last_edit_time":1454343355232,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/27987910",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":6.7,
        "Challenge_reading_time":6.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":12,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":9156.5170305556,
        "Challenge_title":"Azure Machine Learning - CORS",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":3242.0,
        "Challenge_word_count":89,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403948655636,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":135.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>Currently, we don't support disabling CORS on API side but you can either use the above option or you can use the API management service to disable CORS. The links below should help you with this<\/p>\n\n<p>Here are the links: <a href=\"http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/api-management-get-started\/\" rel=\"noreferrer\">step by step<\/a> guide, also this <a href=\"http:\/\/channel9.msdn.com\/Blogs\/AzureApiMgmt\/Last-mile-Security\" rel=\"noreferrer\">video<\/a> on setting headers, and <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn894084.aspx#JSONP\" rel=\"noreferrer\">this doc<\/a> on policies.<\/p>\n\n<p>API Management service allow CORS by enabling it in the API configuration page<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":11.6,
        "Solution_reading_time":9.27,
        "Solution_score_count":4.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":74.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"CORS enabled"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.1831,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I understand Machine Learning studio will be on deprecation soon, I have already got a workspace of it and currently it works well. My questions is when the studio will be not useable anymore.<\/p>",
        "Challenge_closed_time":1680137203723,
        "Challenge_comment_count":0,
        "Challenge_created_time":1680125744563,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is concerned about the deprecation of Machine Learning Studio and wants to know when it will no longer be usable.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1194471\/machine-learning-studio-deprecation-plan",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":2.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":3.1831,
        "Challenge_title":"Machine Learning Studio deprecation plan",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":38,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=d5d1581d-d5c6-4a48-a88d-16b3f134edc8\">@sukharev  <\/a><\/p>\n<p>Support for Machine Learning Studio (classic) will end on <strong>31 August 2024.<\/strong> We recommend you transition to <strong><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/overview-what-is-azure-machine-learning\">Azure Machine Learning<\/a><\/strong> by that date.<\/p>\n<p>Beginning 1 December 2021, you will not be able to create new Machine Learning Studio (classic) resources. Through <strong>31 August 2024<\/strong>, you can continue to use the existing Machine Learning Studio (classic) resources.<\/p>\n<p>ML Studio (classic) documentation is being retired and may not be updated in the future.<\/p>\n<p>We would recommend you to migrate to Azure Machine Learning Service - <\/p>\n<p>The <strong>designer<\/strong> feature in Azure Machine Learning provides a similar drag-and-drop experience to Studio (classic). However, Azure Machine Learning also provides robust <strong><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-model-management-and-deployment\">code-first workflows<\/a><\/strong> as an alternative. This migration series focuses on the designer, since it's most similar to the Studio (classic) experience.<\/p>\n<p>I would invite you to take a look on the differences between classic and designer - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/v1\/migrate-overview#step-1-assess-azure-machine-learning\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/v1\/migrate-overview#step-1-assess-azure-machine-learning<\/a><\/p>\n<p>I hope this helps.<\/p>\n<p>Regards,<\/p>\n<p>Yutong<\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.8,
        "Solution_reading_time":23.18,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":171.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"ML Studio deprecation date"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.7637902778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Assume a data scientist who is coding inside a Synapse notebook, aims to submit his AutoML job to Azure ML. Also assume that we already created the Azure ML workspace, and linked it to Synapse, and also gave Synapse workspace the contributor access to Azure ML workspace. Also the data scientist has the Azure reader role at the synapse workspace level. Data scientist run the following code according to this link (<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/synapse-analytics\/spark\/apache-spark-azure-machine-learning-tutorial\">https:\/\/learn.microsoft.com\/en-us\/azure\/synapse-analytics\/spark\/apache-spark-azure-machine-learning-tutorial<\/a>)    <\/p>\n<p>from azureml.core import Workspace    <\/p>\n<p>subscription_id = &quot;xxxxxx&quot; #you should be owner or contributor    <br \/>\nresource_group = &quot;xxxxx&quot; #you should be owner or contributor    <br \/>\nworkspace_name = &quot;xxxxx&quot; #your workspace name    <br \/>\nworkspace_region = &quot;xxxxx&quot; #your region    <\/p>\n<p>ws = Workspace(workspace_name = workspace_name,    <br \/>\n               subscription_id = subscription_id,  <br \/>\n               resource_group = resource_group)  <\/p>\n<p>However, he receives an error that says he does not have the required contributor\/owner roles at the subscription and resource group level. But we (as the synapse administrators) we don't want to give him the contributor\/owner role at the subscription and resource group name    <\/p>\n<p>Question: How the data scientist can submit his job without letting him to have the required contributor\/owner role. Can he use the managed identity of the Synapse workspace to connect to the Azure ML workspace?    <\/p>\n<p>Thank you<\/p>",
        "Challenge_closed_time":1648620290572,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648588740927,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"A data scientist is trying to submit an AutoML job to Azure ML from a Synapse notebook, but is receiving an error stating that they do not have the required contributor\/owner roles at the subscription and resource group level. The Synapse administrators do not want to give the data scientist these roles. The question is whether the data scientist can use the managed identity of the Synapse workspace to connect to the Azure ML workspace and submit the job without the required roles.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/792681\/submitting-a-job-to-azure-ml-from-synapse-workspac",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.6,
        "Challenge_reading_time":21.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":8.7637902778,
        "Challenge_title":"Submitting a job to Azure ML from Synapse workspace",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":209,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <em>anonymous user<\/em>,    <\/p>\n<p>Thanks for the question and using MS Q&amp;A platform.    <\/p>\n<blockquote>\n<p>Make sure your <code>Service principal<\/code> or <code>Managed Service Identity (MSI)<\/code> must have &quot;<strong>Contributor<\/strong>&quot; access to the AML workspace.    <\/p>\n<\/blockquote>\n<p> If the model is registered in Azure Machine Learning, then you can choose either of the following two supported ways of authentication.    <\/p>\n<ul>\n<li> <strong>Through service principal:<\/strong> You can use service principal client ID and secret directly to authenticate to AML workspace. Service principal must have &quot;Contributor&quot; access to the AML workspace.    <\/li>\n<li> <strong>Through linked service:<\/strong> You can use linked service to authenticate to AML workspace. Linked service can use &quot;service principal&quot; or Synapse workspace's &quot;Managed Service Identity (MSI)&quot; for authentication. &quot;Service principal&quot; or &quot;Managed Service Identity (MSI)&quot; must have &quot;Contributor&quot; access to the AML workspace.    <\/li>\n<\/ul>\n<p>Here is the complete walkthrough of authenticating AML workspace with Azure Synapse Analytics:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/188130-synapse-aml-predict.gif?platform=QnA\" alt=\"188130-synapse-aml-predict.gif\" \/>      <\/p>\n<p>For more details, refer to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool\">Tutorial: Score machine learning models with PREDICT in serverless Apache Spark pools<\/a>.    <\/p>\n<p>Hope this will help. Please let us know if any further queries.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/support\/community-champions-program\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":13.0,
        "Solution_reading_time":35.73,
        "Solution_score_count":1.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":280.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"submit AutoML job without required roles"
    },
    {
        "Answerer_created_time":1395422283667,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1411.0,
        "Answerer_view_count":45.0,
        "Challenge_adjusted_solved_time":0.6555202778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed an AzureML published experiment with deployed web service. I tried to use the <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-consume-web-services\/\" rel=\"nofollow\">sample code provided in the configuration page<\/a>, but universal apps do not implement Http.Formatting yet, thus I couldn't use <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/hh944521(v=vs.118).aspx\" rel=\"nofollow\">postasjsonasync<\/a>.<\/p>\n\n<p>I tried to follow the sample code as much as possible, but I'm getting statuscode of 415 \"Unsupported Media Type\", What's the mistake I'm doing?<\/p>\n\n<pre><code>var client = new HttpClient();\nclient.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Bearer\", apiKey);\n\/\/ client.BaseAddress = uri;\n\nvar scoreRequest = new\n{\n            Inputs = new Dictionary&lt;string, StringTable&gt;() {\n                    {\n                        \"dataInput\",\n                        new StringTable()\n                        {\n                            ColumnNames = new [] {\"Direction\", \"meanX\", \"meanY\", \"meanZ\"},\n                            Values = new [,] {  { \"\", x.ToString(), y.ToString(), z.ToString() },  }\n                        }\n                    },\n                },\n            GlobalParameters = new Dictionary&lt;string, string&gt;() { }\n };\n var stringContent = new StringContent(scoreRequest.ToString());\n HttpResponseMessage response = await client.PostAsync(uri, stringContent);\n<\/code><\/pre>\n\n<p>Many Thanks<\/p>",
        "Challenge_closed_time":1452007973623,
        "Challenge_comment_count":0,
        "Challenge_created_time":1452005613750,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while trying to send a request as JSON on UWP for an AzureML published experiment with a deployed web service. The sample code provided in the configuration page is not working as universal apps do not implement Http.Formatting yet, resulting in a status code of 415 \"Unsupported Media Type\". The user is seeking help to identify the mistake in the code.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34614582",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":16.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.6555202778,
        "Challenge_title":"Send request as Json on UWP",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":3194.0,
        "Challenge_word_count":117,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1352139399460,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cyprus",
        "Poster_reputation_count":820.0,
        "Poster_view_count":256.0,
        "Solution_body":"<p>You'll need to serialize the object to a JSON string (I recommend using NewtonSoft.Json to make it easier) and set the content type accordingly. Here's an implementation I'm using in my UWP apps (note that <code>_client<\/code> is an <code>HttpClient<\/code>):<\/p>\n\n<pre><code>    public async Task&lt;HttpResponseMessage&gt; PostAsJsonAsync&lt;T&gt;(Uri uri, T item)\n    {\n        var itemAsJson = JsonConvert.SerializeObject(item);\n        var content = new StringContent(itemAsJson);\n        content.Headers.ContentType = new MediaTypeHeaderValue(\"application\/json\");\n\n        return await _client.PostAsync(uri, content);\n    }\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.2,
        "Solution_reading_time":7.86,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":62.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unsupported media type"
    },
    {
        "Answerer_created_time":1504757520827,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":19041.0,
        "Answerer_view_count":968.0,
        "Challenge_adjusted_solved_time":90.3055055556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to compare the service with a list of available service names, if service is found in the list then do update the service otherwise deploy the service.\nBut below condition only deploying new service even when service available in list variable?<\/p>\n<pre><code>SERVNAME=ner\nSERVICE=$(az ml service list -g $(ml_rg) --workspace-name $(ml_ws) --model-name $(model_name) --query &quot;[].name&quot;)\n\nif [[ &quot;$SERVNAME&quot; == &quot;$SERVICE&quot; ]];\nthen\n   echo &quot;Service Found: $(SERVNAME) and updating the service&quot;\n   az ml service update --name $(AKS_DEPLOYMENT_NAME) \\\n          --model '$(MODEL_NAME):$(MODEL_VERSION)' \\\n          --dc aksDeploymentConfig.json \\\n          --ic inferenceConfig.json \\\n          -e $(ml_env_name) --ev $(ml_env_version) \\\n          -g $(ml_rg) --workspace-name $(ml_ws) -v ;\nelse\n   echo &quot;Service Not found and starting deploying new service&quot;\n   az ml model deploy --name $(AKS_DEPLOYMENT_NAME) --model \\\n   '$(MODEL_NAME):$(MODEL_VERSION)' \\\n          --compute-target $(ml_aks_name) \\\n          --ic inferenceConfig.json \\\n          -e $(ml_env_name) --ev $(ml_env_version) \\\n          --dc aksDeploymentConfig.json \\\n          -g $(ml_rg) --workspace-name $(ml_ws) \\\n          --overwrite -v ;\nfi\n<\/code><\/pre>\n<p>Example list<\/p>\n<pre><code>SERVNAME=&quot;ner&quot;\nSERVICE=[ &quot;ner&quot;, &quot;aks-gpu-ner-0306210907&quot;, &quot;aks-gpu-ner-30012231&quot;, &quot;aks-gpu-ner-1305211336&quot;]\n<\/code><\/pre>",
        "Challenge_closed_time":1634000233752,
        "Challenge_comment_count":2,
        "Challenge_created_time":1633936791707,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to compare a service name with a list of available service names in bash. However, the code is only deploying a new service even when the service is available in the list variable. The user has provided an example list of service names.",
        "Challenge_last_edit_time":1633953732907,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69522401",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":14.2,
        "Challenge_reading_time":18.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":17.6227902778,
        "Challenge_title":"Compare String with list of strings in bash",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1153.0,
        "Challenge_word_count":142,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1527066589807,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Assuming the <code>az ml<\/code> command returns a json array string and you want to\ncheck if the array includes the value of variable <code>SERVNAME<\/code>, would you\nplease try:<\/p>\n<pre><code>SERVNAME=&quot;ner&quot;\nSERVICE='[ &quot;ner&quot;, &quot;aks-gpu-ner-0306210907&quot;, &quot;aks-gpu-ner-30012231&quot;, &quot;aks-gpu-ner-1305211336&quot;]'\n\nif [[ $SERVICE =~ &quot;\\&quot;$SERVNAME\\&quot;&quot; ]]; then\n    echo &quot;Service Found&quot;\n    # put your command here to update the service\nelse\n    echo &quot;Service Not Found&quot;\n    # put your command here to deploy new service\nfi\n<\/code><\/pre>\n<p>The regex operator <code>$SERVICE =~ &quot;\\&quot;$SERVNAME\\&quot;&quot;<\/code> matches if the string <code>$SERVICE<\/code>\ncontains the substring <code>$SERVNAME<\/code> enclosed with double quotes.<\/p>\n<p>If <code>jq<\/code> is available, you could also say:<\/p>\n<pre><code>result=$(echo &quot;$SERVICE&quot; | jq --arg var &quot;$SERVNAME&quot; '. | index($var)')\nif [[ $result != &quot;null&quot; ]]; then\n    echo &quot;Service Found&quot;\nelse\n    echo &quot;Service Not Found&quot;\nfi\n<\/code><\/pre>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1634278832727,
        "Solution_link_count":0.0,
        "Solution_readability":16.5,
        "Solution_reading_time":14.34,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":111.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"deploying duplicate service"
    },
    {
        "Answerer_created_time":1500629225150,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5939.0,
        "Answerer_view_count":886.0,
        "Challenge_adjusted_solved_time":133.8006258333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to create a module for Sagemaker endpoints. There's an optional object variable called <code>async_inference_config<\/code>. If you omit it, the endpoint being deployed is synchronous, but if you include it, the endpoint deployed is asynchronous. To satisfy both of these usecases, the <code>async_inference_config<\/code> needs to be an optional block.<\/p>\n<p>I am unsure of how to make this block optional though.<br \/>\nAny guidance would be greatly appreciated. See example below of structure of the optional parameter.<\/p>\n<p><strong>Example:<\/strong><\/p>\n<pre><code>resource &quot;aws_sagemaker_endpoint_configuration&quot; &quot;sagemaker_endpoint_configuration&quot; {\n  count = var.create ? 1 : 0\n\n  name = var.endpoint_configuration_name\n  production_variants {\n    instance_type          = var.instance_type\n    initial_instance_count = var.instance_count\n    model_name             = var.model_name\n    variant_name           = var.variant_name\n  }\n  async_inference_config {\n    output_config {\n      s3_output_path = var.s3_output_path\n    }\n    client_config {\n      max_concurrent_invocations_per_instance = var.max_concurrent_invocations_per_instance\n    }\n  }\n  lifecycle {\n    create_before_destroy = true\n    ignore_changes        = [&quot;name&quot;]\n  }\n\n  tags = var.tags\n\n  depends_on = [aws_sagemaker_model.sagemaker_model]\n}\n<\/code><\/pre>\n<p><strong>Update:<\/strong> What I tried based on the below suggestion, which seemed to work<\/p>\n<pre><code>dynamic &quot;async_inference_config&quot; {\n    for_each = var.async_inference_config == null ? [] : [true]\n    content {\n      output_config {\n        s3_output_path = lookup(var.async_inference_config, &quot;s3_output_path&quot;, null)\n      }\n      client_config {\n        max_concurrent_invocations_per_instance = lookup(var.async_inference_config, &quot;max_concurrent_invocations_per_instance&quot;, null)\n      }\n    }\n  }\n<\/code><\/pre>",
        "Challenge_closed_time":1658387166296,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658386596203,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a module for Sagemaker endpoints with an optional object variable called \"async_inference_config\". The endpoint being deployed is synchronous if the variable is omitted, and asynchronous if it is included. The user is unsure of how to make this block optional and is seeking guidance. The user has provided an example of the structure of the optional parameter and an update on what they have tried based on a suggestion.",
        "Challenge_last_edit_time":1658442017223,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73061907",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":17.7,
        "Challenge_reading_time":23.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":0.1583591667,
        "Challenge_title":"Terraform - Optional Nested Variable",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":55.0,
        "Challenge_word_count":146,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443225809767,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Vancouver, BC, Canada",
        "Poster_reputation_count":2332.0,
        "Poster_view_count":560.0,
        "Solution_body":"<p>You could use a <code>dynamic<\/code> block [1] in combination with <code>for_each<\/code> meta-argument [2]. It would look something like:<\/p>\n<pre><code>dynamic &quot;async_inference_config&quot; {\n    for_each = var.s3_output_path != null &amp;&amp; var.max_concurrent_invocations_per_instance != null ? [1] : []\n    content {\n    output_config {\n      s3_output_path = var.s3_output_path\n    }\n    client_config {\n      max_concurrent_invocations_per_instance = var.max_concurrent_invocations_per_instance\n    }\n  }\n}\n<\/code><\/pre>\n<p>Of course, you could come up with a different variable, say <code>enable_async_inference_config<\/code> (probalby of type <code>bool<\/code>) and base the <code>for_each<\/code> on that, e.g.:<\/p>\n<pre><code>dynamic &quot;async_inference_config&quot; {\n    for_each = var.enable_async_inference_config ? [1] : []\n    content {\n    output_config {\n      s3_output_path = var.s3_output_path\n    }\n    client_config {\n      max_concurrent_invocations_per_instance = var.max_concurrent_invocations_per_instance\n    }\n  }\n}\n<\/code><\/pre>\n<hr \/>\n<p>[1] <a href=\"https:\/\/www.terraform.io\/language\/expressions\/dynamic-blocks\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/language\/expressions\/dynamic-blocks<\/a><\/p>\n<p>[2] <a href=\"https:\/\/www.terraform.io\/language\/meta-arguments\/for_each\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/language\/meta-arguments\/for_each<\/a><\/p>",
        "Solution_comment_count":13.0,
        "Solution_last_edit_time":1658923699476,
        "Solution_link_count":4.0,
        "Solution_readability":23.4,
        "Solution_reading_time":18.04,
        "Solution_score_count":2.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":82.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"make block optional"
    },
    {
        "Answerer_created_time":1285219808283,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Perth WA, Australia",
        "Answerer_reputation_count":6770.0,
        "Answerer_view_count":1127.0,
        "Challenge_adjusted_solved_time":0.0213536111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm currently trying to integrate an ML model currently deployed as a webservice on AzureML with PowerBI.<\/p>\n<p>I see that it can be <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#invoking-the-azure-ml-model-in-power-bi\" rel=\"nofollow noreferrer\">integrated<\/a> but the model requires the addition of a schema file when it is <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#schema-discovery-for-machine-learning-models\" rel=\"nofollow noreferrer\">being deployed as a webservice<\/a>. Without this, the model can't be viewed in PowerBI.<\/p>\n<p>The problem that I have come up against is that I use MLflow to log ML model performances and subsequently to deploy a selected model onto AzureML as a webservice using MLflow's AzureML integration - mlflow.azureml.deploy(). This unfortunately doesn't have the option to define a schema file before the model is deployed, thus resulting in no model being available in PowerBI as it lacks the required schema file.<\/p>\n<p>My options seem to be:<\/p>\n<ol>\n<li>Find a workaround, possibly using the working <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/mlflow\/model-serving\" rel=\"nofollow noreferrer\">REST api of the model in a power query<\/a>.<\/li>\n<li>Rewrite the deployment code and handle the webservice deployment steps in Azure instead of MLflow.<\/li>\n<\/ol>\n<p>I thought I would ask to see if I am maybe missing something as I can't find a workaround using my current code to define a schema file in MLflow when deploying with mlflow.azureml.deploy().<\/p>",
        "Challenge_closed_time":1600604920243,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600261190477,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to integrate an ML model deployed as a webservice on AzureML with PowerBI, but the model requires a schema file to be viewed in PowerBI. The user uses MLflow to deploy the model onto AzureML, but MLflow's AzureML integration does not have the option to define a schema file before deployment, resulting in no model being available in PowerBI. The user is considering finding a workaround using the REST API or rewriting the deployment code to handle the webservice deployment steps in Azure instead of MLflow.",
        "Challenge_last_edit_time":1600855880503,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63920599",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":21.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":95.4804905556,
        "Challenge_title":"PowerBI and MLflow integration (through AzureML)",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":405.0,
        "Challenge_word_count":204,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1600260166047,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Point number 2 is the way we solved this issue. Instead of using MLflow to deploy to a scoring service on Azure, we wrote a custom code which load MLflow model when container is initialised.<\/p>\n<p>Scoring code is something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nfrom mlflow.pyfunc import load_model\n\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n\ndef init():\n    global model\n    model = load_model(os.path.join(os.environ.get(&quot;AZUREML_MODEL_DIR&quot;), &quot;awesome_model&quot;))\n\n@input_schema('data', NumpyParameterType(input_sample))\n@output_schema(NumpyParameterType(output_sample))\n\ndef run(data):\n    return model.predict(data)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1600855957376,
        "Solution_link_count":0.0,
        "Solution_readability":16.9,
        "Solution_reading_time":10.68,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":75.0,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"no schema file available"
    },
    {
        "Answerer_created_time":1626973229036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":199.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":171.8247852778,
        "Challenge_answer_count":1,
        "Challenge_body":"<pre><code>import os\nimport io\nimport boto3\nimport json\nimport csv\n\n\n# grab environment variables\nENDPOINT_NAME = os.environ['ENDPOINT_NAME']\n# grab runtime client\nruntime = boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    # Load data from POST request\n    data = json.loads(json.dumps(event))\n    \n    # Grab the payload\n    payload = data['body']\n    \n    # Invoke the model. In this case the data type is a JSON but can be other things such as a CSV\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                   ContentType='application\/json',\n                                   Body=payload)\n    \n    # Get the body of the response from the model\n    result = response['Body'].read().decode()\n\n    # Return it along with the status code of 200 meaning this was succesful \n    return {\n        'statusCode': 200,\n        'body': result\n    }\n<\/code><\/pre>\n<p><strong>response from AWS Lambda<\/strong><\/p>\n<pre><code>{\n  &quot;errorMessage&quot;: &quot;'body'&quot;,\n  &quot;errorType&quot;: &quot;KeyError&quot;,\n  &quot;stackTrace&quot;: [\n    [\n      &quot;\/var\/task\/lambda_function.py&quot;,\n      18,\n      &quot;lambda_handler&quot;,\n      &quot;payload = data['body']&quot;\n    ]\n  ]\n}\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/h8wvA.png\" rel=\"nofollow noreferrer\">response from Postman 500 Internal Server Error<\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/cuknX.png\" rel=\"nofollow noreferrer\">but successfully invoke POST 200 in SageMaker Endpoint<\/a><\/p>",
        "Challenge_closed_time":1626975923230,
        "Challenge_comment_count":1,
        "Challenge_created_time":1626360722473,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with AWS Lambda where they are receiving an Internal Server Error 500 response when using Postman, but the \/invocations POST is successfully returning a 200 response in the SageMaker Endpoint. The error message indicates a KeyError related to the 'body' parameter in the Lambda function.",
        "Challenge_last_edit_time":1626360827356,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68396088",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":14.2,
        "Challenge_reading_time":19.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":170.8890991667,
        "Challenge_title":"Why AWS Lambda Internel Server Error 500 but successfully \/invocations POST 200 in Endpoint SageMaker?",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":277.0,
        "Challenge_word_count":141,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1512933739527,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Petaling Jaya, Selangor, Malaysia",
        "Poster_reputation_count":3.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>The issue is when you are trying to parse your payload with data['body']. The data is not being passed in the format that the endpoint is expecting. Use the following code snippet to properly format\/serialize your data for the endpoint. Also to make all this clearer make sure to check for your payload type to make sure you have not serialized again by accident.<\/p>\n<pre><code>    data = json.loads(json.dumps(event))\n    payload = json.dumps(data)\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                       ContentType='application\/json',\n                                       Body=payload)\n    result = json.loads(response['Body'].read().decode())\n<\/code><\/pre>\n<p>I work for AWS &amp; my opinions are my own<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1626979396583,
        "Solution_link_count":0.0,
        "Solution_readability":9.4,
        "Solution_reading_time":8.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":86.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"KeyError in Lambda function"
    },
    {
        "Answerer_created_time":1369207318272,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":35.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":84.9611705556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to take the environment variables as parameters for the template:\n<a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-properties-sagemaker-model-containerdefinition.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-properties-sagemaker-model-containerdefinition.html<\/a><\/p>\n<p>The type seems to be Json in the template and I dont understand how to populate it.<\/p>\n<p>It seems like I can define this if i hardcode environment variables as below:<\/p>\n<pre><code>Resources:\n  SageMakerModel:\n    Type: 'AWS::SageMaker::Model'\n    Properties:\n      ExecutionRoleArn: \n        Ref: ExecutionRoleArn\n      EnableNetworkIsolation: false\n      PrimaryContainer:\n        Environment:\n          REQUEST_KEEP_ALIVE_TIME_SEC: '90'\n        Image: \n          Ref: ImageURI\n<\/code><\/pre>\n<p>However, there doesnt seem to be a way pass this in ? Anyone figured this out or any recommended way to do this ?<\/p>",
        "Challenge_closed_time":1635277709907,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634971849693,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble understanding how to populate a Json type parameter for a cloudformation template that takes environment variables as parameters. They have tried hardcoding the environment variables but are unsure if there is a way to pass them in. They are seeking advice on how to solve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69685819",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":19.0,
        "Challenge_reading_time":12.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":84.9611705556,
        "Challenge_title":"Taking Json type as parameter for cloudformation template",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":415.0,
        "Challenge_word_count":96,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369207318272,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seattle, WA, USA",
        "Poster_reputation_count":35.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>I was able to get this to work by using AWS:Include and Fn:Transform and storing my environment variables as json in passed s3 file.<\/p>\n<p>My cfn template looks like:<\/p>\n<pre><code>Resources:\n  SageMakerModel:\n    Type: 'AWS::SageMaker::Model'\n    Properties:\n      ExecutionRoleArn: \n        Ref: ExecutionRoleArn\n      EnableNetworkIsolation: false\n      PrimaryContainer:\n        Environment:\n          Fn::Transform:\n            Name: AWS::Include\n            Parameters:\n              Location: &lt;your S3 file&gt;\n        Image: \n          Ref: ImageURI\n<\/code><\/pre>\n<p>My s3 file looks like:<\/p>\n<pre><code>{\n  &quot;REQUEST_KEEP_ALIVE_TIME_SEC&quot;: &quot;90&quot;\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":20.1,
        "Solution_reading_time":7.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"pass environment variables in Json parameter"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1965.6686111111,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nTrainer.fit fails with a pickle error when the logger is MLFlowLogger, and distributed_backend='ddp' on GPUs but without SLURM.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Instantiate MLFlowLogger in Pytorch 0.5.3.2 with Pytorch 1.3.1 and MLFlow 1.4.0. The execution environment has environment variables MLFLOW_TRACKING_URI, and also MLFLOW_TRACKING_USERNAME and MLFLOW_TRACKING_PASSWORD to connect to the MLflow tracking server with HTTP Basic Authentication. The MLflow tracking server is also v1.4.0.\r\n2. Instantiate Trainer with MLFlowLogger instance as logger, distributed_backend='ddp' and with the gpus parameter on a machine with NVIDIA GPUs but without SLURM.\r\n3. Run Trainer.fit\r\n\r\nFrom the error output, it looks like multiprocessing is attempting to pickle the nested function in MLflow function [_get_rest_store](https:\/\/github.com\/mlflow\/mlflow\/blob\/v1.4.0\/mlflow\/tracking\/_tracking_service\/utils.py#L81):\r\n```\r\nayla.khan@gpu12:~\/photosynthetic$ python test_mlflow.py\r\nTraceback (most recent call last):\r\n  File \"test_mlflow.py\", line 71, in <module>\r\n    trainer.fit(model)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 343, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_gpus, args=(model,))\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 162, in spawn\r\n    process.start()\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object '_get_rest_store.<locals>.get_default_host_creds'\r\n```\r\n\r\n#### Code sample\r\nSample code tested with a very simple test model ([gist](https:\/\/gist.github.com\/a-y-khan\/8693d2b186227561a4baf4d03ce75c34)):\r\n\r\n```\r\ntest_hparams = Namespace()\r\nmodel = XORGateModel(test_hparams)\r\n\r\nlogger = MLFlowLogger(experiment_name='test_lightning_logger',\r\n                                          tracking_uri=os.environ['MLFLOW_TRACKING_URI'])\r\ntrainer = pl.Trainer(logger=logger, distributed_backend='ddp', gpus='-1')\r\ntrainer.fit(model)\r\n```\r\n\r\n### Expected behavior\r\n\r\nTrainer.fit runs without error.\r\n\r\n### Environment\r\n\r\n```\r\n(photosynthetic) ayla.khan@gpu12:~\/photosynthetic$ python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 1.3.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 440.33.01\r\ncuDNN version: \/usr\/local\/cuda-10.0\/lib64\/libcudnn.so.7\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] pytorch-lightning==0.5.3.2\r\n[pip] pytorch-toolbelt==0.2.1\r\n[pip] torch==1.3.1\r\n[pip] torchsummary==1.5.1\r\n[pip] torchvision==0.4.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.3.0            py36he904b0f_0\r\n[conda] mkl_fft                   1.0.15           py36ha843d7b_0\r\n[conda] mkl_random                1.1.0            py36hd6b4f25_0\r\n[conda] pytorch                   1.3.1           py3.6_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] pytorch-lightning         0.5.3.2                  pypi_0    pypi\r\n[conda] pytorch-toolbelt          0.2.1                    pypi_0    pypi\r\n[conda] torchsummary              1.5.1                    pypi_0    pypi\r\n[conda] torchvision               0.4.2                py36_cu101    pytorch\r\n```",
        "Challenge_closed_time":1583540837000,
        "Challenge_comment_count":9,
        "Challenge_created_time":1576464430000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered an issue with mlflow byom predictor where arbitrary URLs can be created without checking if an actual mlflow model is served at that URL. The user suggests implementing a check to ensure the validity of the URL before creating or linking the model.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/630",
        "Challenge_link_count":2,
        "Challenge_participation_count":9,
        "Challenge_readability":14.0,
        "Challenge_reading_time":59.35,
        "Challenge_repo_contributor_count":444.0,
        "Challenge_repo_fork_count":2922.0,
        "Challenge_repo_issue_count":15116.0,
        "Challenge_repo_star_count":23576.0,
        "Challenge_repo_watch_count":234.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":64,
        "Challenge_solved_time":1965.6686111111,
        "Challenge_title":"Pickle error from Trainer.fit when using MLFlowLogger and distributed data parallel without SLURM",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":409,
        "Discussion_body":"Will investigate. We have a test that is supposed to prevent these problems from sneaking back in, but apparently it's not doing it's job. I imagine everyone is busy with the build failures - but for the record, I am  having a similar problem. Essentially, I cannot get a logger to work using ddp. It's gving me one of those days when I wonder why I ever wanted to write software ;)\r\n\r\nThis is Ubuntu 18.04.2LTS, on a 14 core, 7 gpu machine. Python 3.6.8, pytorch 1.3.1, pytorch-lightning 0.5.3.2, Tensorboard 2.1.0. Everything else standard except pillow isis 6.2.2 due to known bug in 7.0.\r\n\r\nI am working with a tried and true model and hyperparameters. The model and logging work fine as cpu, gpu, or dp - and ddp if I don't log. But not ddp with logging. I am not using SLURM.\r\n\r\nI have tried to get around this several ways: passing a custom logger, not using the logger created by Trainer(), etc. They either fail when called from one of the new processes, with an attribute error in Tensorboard TTDummyFileWriter.get_logdir(), or they fail with a pickle error about thread.locks when being copied to a new process\r\n\r\nI will detail these in a bug report if you think they are NOT due to the recent build issues.\r\n\r\nBut thought you'd want to know ...\r\n\r\ns\r\n @dbczumar,  @smurching? @neggert is this fixed now? Can this issue be re-opened? I'm currently working with Pytorch-Lightning==0.7.6 and am getting an identical pickle issue when using DDP with the MLFLowLogger.\r\n\r\n**Reproducing**\r\n\r\nUsing the script the OP gave led to some other errors (mostly to do with lightning version differences), so a new gist to reproduce in Pytorch-Lightning 0.7.6 can be found [here](https:\/\/gist.github.com\/Polyphenolx\/39424e5673fc029567f7f3ae3551fffb).\r\n\r\nThis is easily reproducible in other projects as well.\r\n\r\n**Error Output**\r\n\r\n```\r\n\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/utilities\/distributed.py:23: DeprecationWarning: `logging` package has been renamed to `loggers` since v0.7.0 The deprecated package name will be removed in v0.9.0.\r\n  warnings.warn(*args, **kwargs)\r\n\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/utilities\/distributed.py:23: DeprecationWarning: `mlflow_logger` module has been renamed to `mlflow` since v0.6.0. The deprecated module name will be removed in v0.8.0.\r\n  warnings.warn(*args, **kwargs)\r\n\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/utilities\/distributed.py:23: DeprecationWarning: `data_loader` decorator deprecated in v0.7.0. Will be removed v0.9.0\r\n  warnings.warn(*args, **kwargs)\r\nGPU available: True, used: True\r\nNo environment variable for node rank defined. Set as 0.\r\nCUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nTraceback (most recent call last):\r\n  File \"mlflow_test.py\", line 65, in <module>\r\n    trainer.fit(model)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 844, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 200, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 149, in start_processes\r\n    process.start()\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object '_get_rest_store.<locals>.get_default_host_creds'\r\n```\r\n\r\n**Environment**\r\n```\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- GeForce GTX 1080 Ti\r\n\t\t- GeForce GTX 1080 Ti\r\n\t\t- GeForce GTX 1080 Ti\r\n\t\t- GeForce GTX 1080 Ti\r\n\t- available:         True\r\n\t- version:           10.2\r\n* Packages:\r\n\t- numpy:             1.18.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.5.0\r\n\t- pytorch-lightning: 0.7.6\r\n\t- tensorboard:       2.2.2\r\n\t- tqdm:              4.46.1\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.6.8\r\n\t- version:           #102-Ubuntu SMP Mon May 11 10:07:26 UTC 2020\r\n``` To add to this, it appears to be a greater issue with MLFLow and how their tracking utilities are coded. They use a higher order function that causes issues with pickling in torches DDP backend. I've created an issue on MLFLow git, and submitted a PR to remedy the problem. \r\n\r\nIn the interim, feel free to implement the fix described in the issue in the MLFlow git as a temporary fix until\/if they review\/merge mine Following up on this: The pickling fix was merged into the master branch of MLFlow a couple days ago (see the bug mention above). Training using DDP is now functional on MLFLow versions installed from master, but it may take them some time to release the fix to PyPi Running into this same issue as are a few others here:\r\nhttps:\/\/github.com\/minimaxir\/aitextgen\/issues\/135\r\n![image](https:\/\/user-images.githubusercontent.com\/4674698\/121708545-8923f780-ca8c-11eb-9483-56740fd6d401.png)\r\n Hi,\r\n I am still getting the below error:\r\n![image](https:\/\/user-images.githubusercontent.com\/57705684\/131129141-fa483cb4-cb95-43a1-b1d3-62bf78711de2.png)\r\n\r\nI am using DP strategy and PT version '1.8.1+cu111' and PL version '1.3.8'.",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"security vulnerability"
    },
    {
        "Answerer_created_time":1432829415467,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":501.0,
        "Answerer_view_count":76.0,
        "Challenge_adjusted_solved_time":523.8942158334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm posting this more as a 'probe' question and plan to expand the discussion in case some interest shows up. The reason behind this is that in my experience, the SO community on <code>azure-ml<\/code> (and related) is still developing and there is not much feedback - but I would be happy to help it grow stronger. <\/p>\n\n<p>My situation is as follows: I have an experiment in Azure ML which does all its work inside an <code>R<\/code> module. I published this as a web service and set the 'max concurrent calls' slider to 10 - which I believe guarantees me that there will be at most 10 instances of my web service up and running at any time, to serve requests (please correct me if i am wrong). <\/p>\n\n<p>Now, I am trying to do some performance testing by firing 10 parallel calls to my webservice, but get unexpected results...<\/p>\n\n<p>I am trying to run the load tests and log where each of them actually goes to (which instance). My idea is to get a glimpse into how these calls are actually distributed to the instances by the load balancer, under certain max number of concurrent calls = X. I am doing this by firing a call to \"bot.whatismyipaddress.com\" from inside the <code>R<\/code> script. Here is the important snip of the code:<\/p>\n\n<pre><code>library(rjson)\nmachine.ip &lt;- readLines(\"http:\/\/bot.whatismyipaddress.com\/\", warn=F)\nresult$MachineIP &lt;- machine.ip\n<\/code><\/pre>\n\n<p>Additionally, I am using the sample <code>R<\/code> code from the web service RRS help page to fire up to 70 (sequential) calls to my web service. This sample code returns some info back to the console : the results of my web service as well as some info on to which hostname the call goes through. Here is a sample :<\/p>\n\n<pre><code>* Hostname was NOT found in DNS cache\n*   Trying 40.114.242.9...\n* Connected to europewest.services.azureml.net (40.114.242.9) port 443 (#0)\n<\/code><\/pre>\n\n<p>The difficulty that I am facing is that I cannot <strong>uniquely identify<\/strong> the different instances of my web service. The info out to console from the call (the second snippet) often shows a different IP address than the one from inside-<code>R<\/code>-code logs (<code>result$MachineIP<\/code>)...<\/p>\n\n<p>Can someone point out what am i doing wrong, and how could i uniquely identify the different instances that are serving the calls? Any help would be really appreciated. Thanks!<\/p>\n\n<p>P.S. I've tried <a href=\"https:\/\/stackoverflow.com\/questions\/14357219\/function-for-retrieving-own-ip-address-from-within-r\">this<\/a> as well, but the first apporach does not work when calling it from inside the <code>R<\/code> script and I'm using a modified version of the second apporach (the one suggested there does not work). <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/93f07abf-f0ec-4baa-8225-1ca1a072ca2d\/system-call-from-inside-r-script-does-not-work?forum=MachineLearning\" rel=\"nofollow noreferrer\">Here<\/a> are also my <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/ee6ff5a6-2995-4f3f-b4db-0229b1d9d1d3\/lifetime-of-azure-ml-web-service-container?forum=MachineLearning\" rel=\"nofollow noreferrer\">questions<\/a> on the Azure forum, in case someone is interested.<\/p>\n\n<p>If anyone could help or point me to some source of info I would be really grateful! <\/p>",
        "Challenge_closed_time":1452244028967,
        "Challenge_comment_count":0,
        "Challenge_created_time":1450358009790,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing difficulty in uniquely identifying different instances of their web service in Azure ML, which is causing unexpected results during load testing. They are trying to log where each load test goes to and how the calls are distributed to the instances by the load balancer. The user has tried different approaches but has not been successful in identifying the different instances. They are seeking help to resolve the issue.",
        "Challenge_last_edit_time":1495540319592,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34335483",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":42.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":523.8942158334,
        "Challenge_title":"Uniquely identify instances of VMs (Azure ML - web services)",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":61.0,
        "Challenge_word_count":464,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432829415467,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":501.0,
        "Poster_view_count":76.0,
        "Solution_body":"<p>This question was resolved thanks to some people on the Azure ML forum so \nI'm going to post an answer for anyone landing here in search for some answers...<\/p>\n\n<p>The short answer is no, this is not possible. The more detailed version is:<br>\n\"From within the R script you cannot identify the internal AzureML IP addresses or the unique web service instances. When you make an external network call from the R script to an outside URL, that URL will see one of the AzureML public virtual IP's as the source IP. These are IP's of the load balancers, and not of the machines that are physically running the web service. AzureML dynamically allocates the instances of R engine in the backend, handles failures, and uses multiple nodes for running the web service for high availability. The exact layout of these for a given web service is not programmatically discoverable.\"<br>\nHere is also the <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/dd1f0658-7b0b-46d8-8e32-3fe4e96ec4be\/uniquely-identify-instances-of-vms-web-services?forum=MachineLearning#cde28631-828d-4d83-9c93-1a1cf0dfb6fb\" rel=\"nofollow\">link<\/a> to the original discussion. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.5,
        "Solution_reading_time":14.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":162.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"difficulty identifying instances"
    },
    {
        "Answerer_created_time":1372408547912,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Farnborough, United Kingdom",
        "Answerer_reputation_count":7360.0,
        "Answerer_view_count":372.0,
        "Challenge_adjusted_solved_time":9.6629691666,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running some code in AWS Lambda that dynamically creates SageMaker models.\nI am locking Sagemaker's API version like so:<\/p>\n\n<p><code>const sagemaker = new AWS.SageMaker({apiVersion: '2017-07-24'});<\/code><\/p>\n\n<p>And here's the code to create the model:<\/p>\n\n<pre><code>await sagemaker.createModel({\n        ExecutionRoleArn: 'xxxxxx',\n        ModelName: sageMakerConfigId,\n        Containers: [{\n            Image: ecrUrl\n        }]\n    }).promise()\n<\/code><\/pre>\n\n<p>This code runs just fine locally with <code>aws-sdk<\/code> on <code>2.418.0<\/code>. <\/p>\n\n<p>However, when this code is deployed to Lambda, it doesn't work due to some validation errors upon creating the model:<\/p>\n\n<blockquote>\n  <ul>\n  <li>MissingRequiredParameter: Missing required key 'PrimaryContainer' in params<\/li>\n  <li>UnexpectedParameter: Unexpected key 'Containers' found in params<\/li>\n  <\/ul>\n<\/blockquote>\n\n<p>Is anyone aware of existing bugs in the <code>aws-sdk<\/code> for NodeJS using the SDK provided by AWS in the Lambda context? I believe the SDK available inside AWS Lambda is more up-to-date than <code>2.418.0<\/code> but apparently there are compatibility issues.<\/p>",
        "Challenge_closed_time":1553117574836,
        "Challenge_comment_count":0,
        "Challenge_created_time":1553074522987,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing compatibility issues while using SageMaker NodeJS's SDK in AWS Lambda. The code runs fine locally but fails to create a model in Lambda due to validation errors. The user suspects that the SDK provided by AWS in the Lambda context is more up-to-date than the version used locally, causing compatibility issues.",
        "Challenge_last_edit_time":1553082788147,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55257580",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":14.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":11.9588469444,
        "Challenge_title":"SageMaker NodeJS's SDK is not locking the API Version",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":277.0,
        "Challenge_word_count":142,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1548169897263,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":6496.0,
        "Poster_view_count":759.0,
        "Solution_body":"<p>As you've noticed the 'embedded' lambda version of the aws-sdk lags behind. It's actually on <code>2.290.0<\/code> (you can see the full details on the environment here: <a href=\"https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/current-supported-versions.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/current-supported-versions.html<\/a>)<\/p>\n\n<p>You can see here: <a href=\"https:\/\/github.com\/aws\/aws-sdk-js\/blame\/master\/clients\/sagemaker.d.ts\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/aws-sdk-js\/blame\/master\/clients\/sagemaker.d.ts<\/a> that it is not until <code>2.366.0<\/code> that the params for this method included <code>Containers<\/code> and did not require <code>PrimaryContainer<\/code>.<\/p>\n\n<p>As you've noted, the <em>workaround<\/em> is to deploy your lambda with the <code>aws-sdk<\/code> version that you're using. This is sometimes noted as a best practice, as it pins the <code>aws-sdk<\/code> on the functionality you've built and tested against.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":14.1,
        "Solution_reading_time":13.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":95.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"compatibility issues"
    },
    {
        "Answerer_created_time":1606378353316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Huskvarna, Sverige",
        "Answerer_reputation_count":275.0,
        "Answerer_view_count":36.0,
        "Challenge_adjusted_solved_time":3.7086241667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The yaml template <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-properties-sagemaker-model-containerdefinition.html\" rel=\"nofollow noreferrer\">documentation<\/a> for the AWS Cloudformation AWS::SageMaker::Model ContainerDefinition specifies that &quot;Environment&quot; is of type Json. I can't work out how to submit json in my yaml template that does not cause a &quot;CREATE_FAILED    Internal Failure&quot; after running a deploy with the below command.<\/p>\n<pre><code>aws cloudformation deploy --stack-name test1 --template-file test-template-export.yml\n<\/code><\/pre>\n<p>test-template-export.yml<\/p>\n<pre><code>Description: Example yaml\n\nResources:\n  Model:\n    Type: AWS::SageMaker::Model\n    Properties:\n      Containers:\n      - ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n      - Environment: '{&quot;SAGEMAKER_CONTAINER_LOG_LEVEL&quot;: &quot;20&quot;}'\n      ExecutionRoleArn: arn:aws:iam::123456789123:role\/service-role\/AmazonSageMakerServiceCatalogProductsUseRole\n<\/code><\/pre>\n<p>I have also tried the below formats as well and still no luck.<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n  Environment: '{&quot;SAGEMAKER_CONTAINER_LOG_LEVEL&quot;: &quot;20&quot;}'\n<\/code><\/pre>\n<p>--<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n- Environment: | \n         {\n            &quot;SAGEMAKER_CONTAINER_LOG_LEVEL&quot;: &quot;20&quot;\n          }\n<\/code><\/pre>\n<p>--<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n- Environment:\n  - SAGEMAKER_CONTAINER_LOG_LEVEL: &quot;20&quot;\n<\/code><\/pre>\n<p>Running without Environment deploys fine.<\/p>\n<p>I have tried everything in <a href=\"https:\/\/stackoverflow.com\/questions\/39041209\/how-to-specify-json-formatted-string-in-cloudformation\">this answer.<\/a>\nHow do I format this Environment argument?<\/p>\n<p>My version of aws cli is &quot;aws-cli\/2.4.10 Python\/3.8.8&quot;<\/p>",
        "Challenge_closed_time":1643284402368,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643283742143,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble formatting the parameter of data type JSON in an AWS CloudFormation YAML template for the AWS::SageMaker::Model ContainerDefinition. The user has tried various formats but has not been successful in deploying the template without encountering a \"CREATE_FAILED Internal Failure\" error. The user is seeking guidance on how to properly format the Environment argument.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70877982",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":24.0,
        "Challenge_reading_time":28.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.1833958333,
        "Challenge_title":"How to format parameter of data type json in a aws cloudformation yaml template?",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":391.0,
        "Challenge_word_count":151,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1452981020536,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"UK",
        "Poster_reputation_count":311.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p>Hi when you see json format think more dict.\nSo write it like this:<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n  Environment:\n     SAGEMAKER_CONTAINER_LOG_LEVEL: 20\n<\/code><\/pre>\n<p>For IAM Policies the PolicyDocument is json type and this is how AWS do it in their exempel:<\/p>\n<pre><code>Type: 'AWS::IAM::Policy'\nProperties:\n  PolicyName: CFNUsers\n  PolicyDocument:\n    Version: &quot;2012-10-17&quot;\n    Statement:\n      - Effect: Allow\n        Action:\n          - 'cloudformation:Describe*'\n          - 'cloudformation:List*'\n          - 'cloudformation:Get*'\n        Resource: '*'\n  Groups:\n    - !Ref CFNUserGroup\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1643297093190,
        "Solution_link_count":0.0,
        "Solution_readability":18.1,
        "Solution_reading_time":8.26,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":59.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"deployment error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":32.5692297222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi,  <\/p>\n<p>I  was deploying a real-time inference pipeline into an AKS compute in East US region today. The endpoint deployment state was stuck at Transitioning for over 2 hours and never finished and I had to delete it. A separate deployment to region East US 2 got stuck as well.  I was able to deploy the same pipeline to East US  the day before yesterday.  <\/p>\n<p> I wonder if this is likely an error related to my account\/resources or a system wide issue? Did anyone else encounter the similar issue?  <\/p>\n<p>thanks in advance!  <\/p>",
        "Challenge_closed_time":1591940540980,
        "Challenge_comment_count":6,
        "Challenge_created_time":1591823291753,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue while deploying a real-time inference pipeline into an AKS compute in East US region. The deployment state was stuck at Transitioning for over 2 hours and never finished, and a separate deployment to region East US 2 got stuck as well. The user wonders if this is an error related to their account\/resources or a system-wide issue and if anyone else has encountered a similar issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/34653\/azure-ml-real-time-inference-endpoint-deloyment-st",
        "Challenge_link_count":0,
        "Challenge_participation_count":8,
        "Challenge_readability":7.6,
        "Challenge_reading_time":7.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":32.5692297222,
        "Challenge_title":"Azure ML real-time inference endpoint deloyment stuck - with deployment state as Transitioning for over 2 hours.",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":112,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello All,  <\/p>\n<p>We have deployed a fix now to all regions and this should be fixed. Could you please retry and let us know if there are any issues.  <\/p>\n<p>-Rohit<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.8,
        "Solution_reading_time":2.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":32.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"stuck deployment"
    },
    {
        "Answerer_created_time":1524658535683,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":31.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":189.9530194445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The sentiment analysis sample at <a href=\"https:\/\/gallery.azure.ai\/Collection\/Twitter-Sentiment-Analysis-Collection-1\" rel=\"nofollow noreferrer\">https:\/\/gallery.azure.ai\/Collection\/Twitter-Sentiment-Analysis-Collection-1<\/a> shows use of Filter Based Feature Selection in the training experiment, which is used to generate a SelectColumnsTransform to be saved and used in the predictive experiment, alongside the trained model. The article at <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/create-models-and-endpoints-with-powershell\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/create-models-and-endpoints-with-powershell<\/a> explains how you can programmatically train multiple models on different datasets, save those models and create then patch multiple new endpoints, so that each can be used for scoring using a different model. The same technique can also be used to create and save multiple SelectColumnsTransform outputs, for feature selection specific to a given set of training data. However, the Patch-AmlWebServiceEndpoint does not appear to allow a SelectColumnsTransform in a scoring web service to be amended to use the relevant itransform saved during training. An 'EditableResourcesNotAvailable' message is returned, along with a list of resources that can be edited which includes models but not transformations. In addition, unlike (say) ImportData, a SelectColumnsTransform does not offer any parameters that can be exposed as web service parameters. <\/p>\n\n<p>So, how is it possible to create multiple web service endpoints programmatically that each use different SelectColumnsTransform itransform blobs, such as for a document classification service where each endpoint is based on a different set of training data?<\/p>\n\n<p>Any information much appreciated.<\/p>",
        "Challenge_closed_time":1527865890430,
        "Challenge_comment_count":0,
        "Challenge_created_time":1527182059560,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in creating multiple web service endpoints programmatically that use different SelectColumnsTransform itransform blobs, specifically for a document classification service where each endpoint is based on a different set of training data. The Patch-AmlWebServiceEndpoint does not allow a SelectColumnsTransform in a scoring web service to be amended to use the relevant itransform saved during training, and a SelectColumnsTransform does not offer any parameters that can be exposed as web service parameters.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50514817",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":16.0,
        "Challenge_reading_time":25.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":189.9530194445,
        "Challenge_title":"Azure Machine Learning Studio SelectColumnsTransform - how to patch or set web service input parameter?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":74.0,
        "Challenge_word_count":228,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1524658535683,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>Never mind. I got rid of the SelectColumnsTransform altogether (departing from the example experiment), instead using a R script in the training experiment to save the names of the columns selected, then another R script in the predictive experiment to load those names and remove any other feature columns.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":23.6,
        "Solution_reading_time":3.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":49.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to create multiple endpoints"
    },
    {
        "Answerer_created_time":1273856732636,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London",
        "Answerer_reputation_count":20277.0,
        "Answerer_view_count":615.0,
        "Challenge_adjusted_solved_time":1183.6423358333,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Permanent deletion of an experiment isn't documented anywhere. I'm using Mlflow w\/ backend postgres db<\/p>\n\n<p>Here's what I've run: <\/p>\n\n<pre><code>client = MlflowClient(tracking_uri=server)\nclient.delete_experiment(1)\n<\/code><\/pre>\n\n<p>This deletes the the experiment, but when I run a new experiment with the same name as the one I just deleted, it will return this error:<\/p>\n\n<pre><code>mlflow.exceptions.MlflowException: Cannot set a deleted experiment 'cross-sell' as the active experiment. You can restore the experiment, or permanently delete the  experiment to create a new one.\n<\/code><\/pre>\n\n<p>I cannot find anywhere in the documentation that shows how to permanently delete everything.<\/p>",
        "Challenge_closed_time":1585231513452,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580970401043,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in permanently deleting an experiment in Mlflow with a backend postgres db. Although the user has tried deleting the experiment using the MlflowClient, they are still unable to create a new experiment with the same name due to an error. The user is unable to find any documentation on how to permanently delete everything.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60088889",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":11.2,
        "Challenge_reading_time":9.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":20,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1183.6423358333,
        "Challenge_title":"How Do You \"Permanently\" Delete An Experiment In Mlflow?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":13984.0,
        "Challenge_word_count":100,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443225809767,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Vancouver, BC, Canada",
        "Poster_reputation_count":2332.0,
        "Poster_view_count":560.0,
        "Solution_body":"<p>Unfortunately it seems there is no way to do this via the UI or CLI at the moment :-\/<\/p>\n\n<p>The way to do it depends on the type of backend file store that you are using.<\/p>\n\n<p><strong>Filestore<\/strong>:<\/p>\n\n<p>If you are using the filesystem as a storage mechanism (the default) then it is easy. The 'deleted' experiments are moved to a <code>.trash<\/code> folder. You just need to clear that out:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>rm -rf mlruns\/.trash\/*\n<\/code><\/pre>\n\n<p>As of the current version of the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/cli.html#mlflow-experiments-delete\" rel=\"noreferrer\">documentation<\/a> (1.7.2), they remark:<\/p>\n\n<blockquote>\n  <p>It is recommended to use a cron job or an alternate workflow mechanism to clear <code>.trash<\/code> folder.<\/p>\n<\/blockquote>\n\n<p><strong>SQL Database:<\/strong><\/p>\n\n<p>This is more tricky, as there are dependencies that need to be deleted. I am using MySQL, and these commands work for me:<\/p>\n\n<pre class=\"lang-sql prettyprint-override\"><code>USE mlflow_db;  # the name of your database\nDELETE FROM experiment_tags WHERE experiment_id=ANY(\n    SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n);\nDELETE FROM latest_metrics WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n    )\n);\nDELETE FROM metrics WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n    )\n);\nDELETE FROM tags WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n    )\n);\nDELETE FROM runs WHERE experiment_id=ANY(\n    SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n);\nDELETE FROM experiments where lifecycle_stage=\"deleted\";\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.3,
        "Solution_reading_time":24.34,
        "Solution_score_count":22.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":220.0,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to permanently delete experiment"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":20.3918425,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I created a new azure automl experiment and deployed it to an endpoint and can access the scoring URI via postman but how do I consume it in excel? Classic ml studio had the excel addin you can use but I don't see the same for URIs created and deployed from an automl experiment.   <\/p>\n<p>This Microsoft Developer video has a demo of exactly what I'm looking to do around the 32 min mark.  <br \/>\n<a href=\"https:\/\/youtu.be\/9FGuf55_Xtk?t=1915\">https:\/\/youtu.be\/9FGuf55_Xtk?t=1915<\/a>  <\/p>",
        "Challenge_closed_time":1611147165400,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611073754767,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has created an Azure AutoML experiment and deployed it to an endpoint, and can access the scoring URI via Postman. However, they are unsure how to consume it in Excel as they cannot find the Excel add-in that was available in Classic ML Studio. They are seeking guidance on how to achieve this and have provided a video demo for reference.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/236781\/consume-scoreing-api-in-excel",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":6.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":20.3918425,
        "Challenge_title":"Consume scoreing api in excel",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":82,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"#\">@Anonymous  <\/a>  Thanks for the question, Have a look here:    <br \/>\n<a href=\"https:\/\/github.com\/retkowsky\/AzureML_Excel\">https:\/\/github.com\/retkowsky\/AzureML_Excel<\/a>    <\/p>\n<p>There is an Excel macro in the Excel file that call an Azure ML service deployed model.    <br \/>\nThere is a quick description of the process in the Word document available in this repo.    <br \/>\nYou can find here as well the Python notebook for creating &amp; deploying the model. No autoML in it but not a big deal to adapt.    <\/p>\n<p>Please try the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-consume-web-service?tabs=python\">Consume web services portion<\/a> Azure ML documentation? That could help you get started.     <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.9,
        "Solution_reading_time":9.36,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":95.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"consume endpoint in Excel"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":7.1241191666,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>similar question to\n<a href=\"https:\/\/stackoverflow.com\/a\/66683538\/6896705\">AWS Lambda send image file to Amazon Sagemaker<\/a><\/p>\n<p>I try to make simple-mnist work (the model was built by referring to <a href=\"https:\/\/sagemaker-immersionday.workshop.aws\/en\/lab3\/option1.html\" rel=\"nofollow noreferrer\">aws tutorial<\/a>)<\/p>\n<p>Then I am using API gateway (REST API w\/ proxy integration) to post image data to lambda, and would like to send it to sagemaker endpoint and make an inference.<\/p>\n<p>In lambda function, I wrote the code(.py) like this.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>runtime = boto3.Session().client('sagemaker-runtime')\n\nendpoint_name = 'tensorflow-training-YYYY-mm-dd-...'\nres = runtime.invoke_endpoint(EndpointName=endpoint_name,\n                              Body=Image,\n                              ContentType='image\/jpeg',\n                              Accept='image\/jpeg')\n<\/code><\/pre>\n<p>However, when I send image to lambda via API gateway, this error occurs.<\/p>\n<blockquote>\n<p>[ERROR] ModelError: An error occurred (ModelError) when calling the\nInvokeEndpoint operation: Received client error (415) from model with\nmessage &quot; {\n&quot;error&quot;: &quot;Unsupported Media Type: image\/jpeg&quot; }<\/p>\n<\/blockquote>\n<p>I think I need to do something referring to <a href=\"https:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/api-gateway-payload-encodings.html\" rel=\"nofollow noreferrer\">Working with binary media types for REST APIs\n<\/a><\/p>\n<p>But since I am very new, I have no idea about the appropriate thing to do, on which page (maybe API Gateway page?) or how...<\/p>\n<p>I need some clues to solve this problem. Thank you in advance.<\/p>",
        "Challenge_closed_time":1626967022752,
        "Challenge_comment_count":0,
        "Challenge_created_time":1626929018517,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to send image data from API Gateway to Lambda and then to Sagemaker endpoint for inference. However, they are encountering an error related to unsupported media type (415) when invoking the endpoint. The user suspects that they need to work with binary media types for REST APIs but is unsure about the appropriate steps to take. They are seeking guidance to resolve the issue.",
        "Challenge_last_edit_time":1626941375923,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68479297",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":12.6,
        "Challenge_reading_time":21.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":10.5567319444,
        "Challenge_title":"AWS send image to Sagemaker from Lambda: how to set content handling?",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":390.0,
        "Challenge_word_count":191,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1475109151950,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":43.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>Looking <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/adapt-inference-container.html\" rel=\"nofollow noreferrer\">here<\/a> you can see that only some specific content types are supported by default, and images are not in this list. I think you have to either implement your <code>input_fn<\/code> function or adapt your data to one of the supported content types.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.8,
        "Solution_reading_time":4.89,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":46.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unsupported media type"
    },
    {
        "Answerer_created_time":1227171471292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":17500.0,
        "Answerer_view_count":1561.0,
        "Challenge_adjusted_solved_time":233.4898177778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Invoking a multimodel Sagemaker Endpoint, I get an error that it is not multimodel. I create it like this.<\/p>\n<pre><code>create_endpoint_config_response = client.create_endpoint_config(\n    EndpointConfigName=endpoint_config_name,\n    ProductionVariants=[\n        {\n            &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n            &quot;InitialVariantWeight&quot;: 0.5,\n            &quot;InitialInstanceCount&quot;: 1,\n            &quot;ModelName&quot;: model_name1,\n            &quot;VariantName&quot;: model_name1,\n        },\n         {\n            &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n            &quot;InitialVariantWeight&quot;: 0.5,\n            &quot;InitialInstanceCount&quot;: 1,\n            &quot;ModelName&quot;: model_name2,\n            &quot;VariantName&quot;: model_name2,\n        }\n    ]\n)\n<\/code><\/pre>\n<p>I confirm in the GUI that it in fact has multiple models. I invoke it like this:<\/p>\n<pre><code>response = client.invoke_endpoint(\n    EndpointName=endpoint_name, \n    TargetModel=model_name1,\n    ContentType=&quot;text\/x-libsvm&quot;, \n    Body=payload\n)\n<\/code><\/pre>\n<p>and get this error:<\/p>\n<blockquote>\n<p>ValidationError: An error occurred (ValidationError) when calling the\nInvokeEndpoint operation: Endpoint\nmy-endpoint1 is not a multi-model endpoint\nand does not support target model header.<\/p>\n<\/blockquote>\n<p>The same problem was discussed <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1026\" rel=\"nofollow noreferrer\">here<\/a> with no resolution.<\/p>\n<p>How can I invoke a multimodel endpoint?<\/p>",
        "Challenge_closed_time":1629283623048,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629114933853,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while invoking a multimodel Sagemaker Endpoint, despite having created it with multiple models. The error message states that the endpoint is not multimodel and does not support target model header. The user has confirmed that the endpoint has multiple models in the GUI and is seeking a solution to invoke the multimodel endpoint.",
        "Challenge_last_edit_time":1629119102992,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68802388",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":16.4,
        "Challenge_reading_time":19.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":46.8581097222,
        "Challenge_title":"Why do I get an error that Sagemaker Endpoint does not have multiple models when it does?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":247.0,
        "Challenge_word_count":133,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1227171471292,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Israel",
        "Poster_reputation_count":17500.0,
        "Poster_view_count":1561.0,
        "Solution_body":"<p>The answer (see <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1026\" rel=\"nofollow noreferrer\">GitHub<\/a> discussion) is that this error message is simply false.<\/p>\n<p>To avoid this error, the model's local filename (usually for the form <code>model_filename.tar.gz<\/code>) must be used, not the model name.<\/p>\n<p>The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/invoke-multi-model-endpoint.html\" rel=\"nofollow noreferrer\">documentation<\/a> does say this, though it lacks essential detail.<\/p>\n<p>I found <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb\" rel=\"nofollow noreferrer\">this to be the best example<\/a>.  See the last part  of that Notebook, in which <code>invoke_endpoint<\/code> is used (rather than a predictor as used earlier in the Notebook).<\/p>\n<p>As to the location of that model file: This <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">Notebook<\/a> says:<\/p>\n<blockquote>\n<p>When creating the Model entity for multi-model endpoints, the container's ModelDataUrl is the S3 prefix where the model\nartifacts that are invokable by the endpoint are located. The rest of the S3 path will be specified when invoking the model.<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1629959666336,
        "Solution_link_count":4.0,
        "Solution_readability":17.2,
        "Solution_reading_time":19.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":138.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"endpoint not multimodel"
    },
    {
        "Answerer_created_time":1341655118270,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Egypt",
        "Answerer_reputation_count":505.0,
        "Answerer_view_count":53.0,
        "Challenge_adjusted_solved_time":19.6138405556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a deployed model on sagemaker with two production variants. I was wondering if you get charged for both variants even if I set all the traffic to just go through one of them.<\/p>\n<p>The docs on pricing are found below but I couldn't seem to find the answer to this.<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/sagemaker\/pricing\/<\/a><\/p>",
        "Challenge_closed_time":1622110871903,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622040262077,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has a deployed model on Sagemaker with two production variants and is unsure if they will be charged for both variants even if all the traffic is directed to just one of them. They have checked the pricing documentation but could not find a clear answer.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67707288",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.9,
        "Challenge_reading_time":6.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":19.6138405556,
        "Challenge_title":"Do you get charged for production variants on sagemaker that have no traffic going through them?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":40.0,
        "Challenge_word_count":71,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521737124656,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Milton Keynes",
        "Poster_reputation_count":738.0,
        "Poster_view_count":69.0,
        "Solution_body":"<p>You will be charged as long as the model is running on an instance regardless of whether the traffic is going through it or not as still the model is running on an instance.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.2,
        "Solution_reading_time":2.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":34.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"pricing for multiple variants"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":6.2657233333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm attempting to deploy a text classification model I trained with Vertex AI on the Google Cloud Platform using the Python SDK.<\/p>\n<pre><code>from google.cloud import aiplatform\n\nimport os\n\nos.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;] = &quot;&lt;key location&gt;&quot;\n\ndef create_endpoint(\n    project_id: str,\n    display_name: str,\n    location: str,\n    sync: bool = True,\n):\n    endpoint = aiplatform.Endpoint.create(\n        display_name=display_name, project=project_id, location=location,\n    )\n\n    print(endpoint.display_name)\n    print(endpoint.resource_name)\n    return endpoint\n\ndef deploy_model(project_id, location, model_id):\n    model_location = &quot;projects\/{}\/locations\/{}\/models\/{}&quot;.format(project_id, location, model_id)\n\n    print(&quot;Initializing Vertex AI&quot;)\n    aiplatform.init(project=project_id, location=location)\n\n    print(&quot;Getting model from {}&quot;.format(model_location))\n    model = aiplatform.Model(model_location)\n\n    print(&quot;Creating endpoint.&quot;)\n    endpoint = create_endpoint(project_id, &quot;{}_endpoint&quot;.format(model_id), location)\n\n    print(&quot;Deploying endpoint&quot;)\n    endpoint.deploy(\n        model,\n        machine_type=&quot;n1-standard-4&quot;,\n        min_replica_count=1,\n        max_replica_count=5,\n        accelerator_type='NVIDIA_TESLA_K80',\n        accelerator_count=1\n    )\n\n    return endpoint\n\nendpoint = deploy_model(\n    &quot;&lt;project name&gt;&quot;,\n    &quot;us-central1&quot;,\n    &quot;&lt;model id&gt;&quot;,\n)\n<\/code><\/pre>\n<p>Unfortunately, when I run this code, I receive this error after the endpoint.deploy is triggered:\n<code>google.api_core.exceptions.InvalidArgument: 400 'dedicated_resources' is not supported for Model...<\/code> followed by the model location.<\/p>\n<p>Note the places I've swapped my values with &lt;***&gt; to hide my local workspace variables.<\/p>",
        "Challenge_closed_time":1634271217372,
        "Challenge_comment_count":2,
        "Challenge_created_time":1634245049023,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while attempting to deploy a text classification model using Vertex AI on the Google Cloud Platform using the Python SDK. The error message states that 'dedicated_resources' is not supported for the model.",
        "Challenge_last_edit_time":1634248660768,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69577270",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":19.5,
        "Challenge_reading_time":24.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":7.2689858333,
        "Challenge_title":"Google Cloud Vertex AI - 400 'dedicated_resources' is not supported for Model",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":616.0,
        "Challenge_word_count":148,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1417013182680,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Boston, MA",
        "Poster_reputation_count":1256.0,
        "Poster_view_count":245.0,
        "Solution_body":"<p>You encounter the error since it is not possible to assign a custom machine to a Text Classification model. Only custom models and AutoML tabular models can use custom machines as per <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#specify\" rel=\"nofollow noreferrer\">documentation<\/a>. For AutoML models aside from tabular models, Vertex AI automatically configures the machine type.<\/p>\n<blockquote>\n<p>If you want to use a custom-trained model or an AutoML tabular model\nto serve online predictions, you must specify a machine type when you\ndeploy the Model resource as a DeployedModel to an Endpoint. <strong>For other<\/strong>\n<strong>types of AutoML models, Vertex AI configures the machine types<\/strong>\n<strong>automatically.<\/strong><\/p>\n<\/blockquote>\n<p>The workaround for this is to remove the machine related parameters on your <code>endpoint.deploy()<\/code> and you should be able to deploy the model.<\/p>\n<pre><code>print(&quot;Deploying endpoint&quot;)\nendpoint.deploy(model)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.5,
        "Solution_reading_time":13.4,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":127.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unsupported resource type"
    },
    {
        "Answerer_created_time":1342685175156,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":12103.0,
        "Answerer_view_count":1451.0,
        "Challenge_adjusted_solved_time":12.8997730556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am able to deploy a Azure Machine learning prediction service in my workspace <code>ws<\/code> using the syntax<\/p>\n\n<pre><code>aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n                                               memory_gb=8, \n                                               tags={\"method\" : \"some method\"}, \n                                               description='Predict something')\n<\/code><\/pre>\n\n<p>and then<\/p>\n\n<pre><code>service = Webservice.deploy_from_image(deployment_config = aciconfig,\n                                       image = image,\n                                       name = service_name,\n                                       workspace = ws)\n<\/code><\/pre>\n\n<p>as described in the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where#aci\" rel=\"nofollow noreferrer\">documentation<\/a>.<br>\nHowever, this exposes a service publicly and this is not really optimal.<\/p>\n\n<p>What's the easiest way to shield the ACI service? I understand that passing an <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.webservice.aciwebservice?view=azure-ml-py#deploy-configuration-cpu-cores-none--memory-gb-none--tags-none--properties-none--description-none--location-none--auth-enabled-none--ssl-enabled-none--enable-app-insights-none--ssl-cert-pem-file-none--ssl-key-pem-file-none--ssl-cname-none-\" rel=\"nofollow noreferrer\"><code>auth_enabled=True<\/code><\/a> parameter may do the job, but then how can I instruct a client (say, using <code>curl<\/code> or Postman) to use the service afterwards? <\/p>",
        "Challenge_closed_time":1556182170523,
        "Challenge_comment_count":0,
        "Challenge_created_time":1556135731340,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has successfully deployed an Azure Machine Learning prediction service in their workspace using the given syntax. However, the service is publicly exposed, and the user is looking for the easiest way to shield the ACI service. They are considering passing an \"auth_enabled=True\" parameter but are unsure how to instruct a client to use the service afterward.",
        "Challenge_last_edit_time":1556186547292,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55837639",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":18.8,
        "Challenge_reading_time":19.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":12.8997730556,
        "Challenge_title":"How to enable authentication for an ACI webservice in Azure Machine Learning service?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":676.0,
        "Challenge_word_count":113,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1415722650716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Verona, VR, Italy",
        "Poster_reputation_count":4811.0,
        "Poster_view_count":713.0,
        "Solution_body":"<p>See <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-consume-web-service#call-the-service-c\" rel=\"nofollow noreferrer\">here<\/a> for an example (in C#). When you enable auth, you will need to send the API key in the \"Authorization\" header in the HTTP request:<\/p>\n\n<pre><code>client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Bearer\", authKey);\n<\/code><\/pre>\n\n<p>See <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-consume-web-service#authentication-key\" rel=\"nofollow noreferrer\">here<\/a> how to retrieve the key.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1556183204092,
        "Solution_link_count":2.0,
        "Solution_readability":19.8,
        "Solution_reading_time":8.26,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"secure ACI service"
    },
    {
        "Answerer_created_time":1460664823627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":81.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":170.4113630556,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I wanted to know if there is a way to call the Azure Machine Learning webservice using JavaScript Ajax.<\/p>\n\n<p>The Azure ML gives sample code for C#, Python and R.<\/p>\n\n<p>I did try out to call the webservice using JQuery Ajax but it returns a failure.<\/p>\n\n<p>I am able to call the same service using a python script.<\/p>\n\n<p>Here is my Ajax code : <\/p>\n\n<pre><code>  $.ajax({\n        url: webserviceurl,\n        type: \"POST\",           \n        data: sampleData,            \n        dataType:'jsonp',                        \n        headers: {\n        \"Content-Type\":\"application\/json\",            \n        \"Authorization\":\"Bearer \" + apiKey                       \n        },\n        success: function (data) {\n          console.log('Success');\n        },\n        error: function (data) {\n           console.log('Failure ' +  data.statusText + \" \" + data.status);\n        },\n  });\n<\/code><\/pre>",
        "Challenge_closed_time":1464718210400,
        "Challenge_comment_count":2,
        "Challenge_created_time":1464104729493,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in calling the Azure Machine Learning webservice using JavaScript Ajax. They have tried using JQuery Ajax but it returns a failure, while they are able to call the same service using a python script. They have provided their Ajax code for reference.",
        "Challenge_last_edit_time":1526047792276,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37418265",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":7.9,
        "Challenge_reading_time":9.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":170.4113630556,
        "Challenge_title":"Azure Machine Learning using Javascript Ajax call",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1607.0,
        "Challenge_word_count":94,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1460664823627,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":81.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>Well after a lot of RnD, I was able to finally call Azure ML using some workarounds.<\/p>\n\n<p>Wrapping Azure ML webservice on Azure API is one option.<\/p>\n\n<p>But, what I did was that I created a python webservice which calls the Azure webservice.<\/p>\n\n<p>So now my HTML App calls the python webservice which calls Azure ML and returns data to the HTML App.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.9,
        "Solution_reading_time":4.38,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"failure in calling webservice"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.2653166667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have 2 experiments A and B in Azure MLS classic. I need the web service output of experiment A as one of the web service inputs for experiment B.  Please let me know if it is possible and if yes, how I can do it.<\/p>",
        "Challenge_closed_time":1592433417623,
        "Challenge_comment_count":2,
        "Challenge_created_time":1592407262483,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has two experiments in Azure MLS classic and needs to use the web service output of experiment A as one of the web service inputs for experiment B. They are seeking guidance on whether this is possible and how to do it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/37128\/connect-2-separate-experiments-via-webservice-azur",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.4,
        "Challenge_reading_time":3.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":7.2653166667,
        "Challenge_title":"Connect 2 separate experiments via webservice - Azure MLS Classic",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":54,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I used export module in experiment A and import module in experiment B to transfer the output of A as input of B.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.3,
        "Solution_reading_time":1.44,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":23.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"use output as input"
    },
    {
        "Answerer_created_time":1343167997556,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":191.0,
        "Answerer_view_count":27.0,
        "Challenge_adjusted_solved_time":132.4537347222,
        "Challenge_answer_count":2,
        "Challenge_body":"<h2><strong>ASKING THIS HERE AT THE EXPLICIT REQUEST OF THE MICROSOFT AZURE SUPPORT TEAM.<\/strong><\/h2>\n\n<p>I've been attempting to call the MS Luis.ai <em>programmatic<\/em> API (bit.ly\/2iev01n) and have been receiving a 401 unauthorized response to every request. Here's a simple GET example: <code>https:\/\/api.projectoxford.ai\/luis\/v1.0\/prog\/apps\/{appId}\/entities?subscription-key={subscription_key}<\/code>.  <\/p>\n\n<p>I am providing my appId from the Luis.ai GUI (as specified by the API docs), here:<br>\n<img src=\"https:\/\/i.stack.imgur.com\/Cg2Fw.png\" alt=\"Luis.ai App Settings App Id\"><\/p>\n\n<p>I am providing my subscription key from Azure (as specified by the API docs), here:<br>\n<img src=\"https:\/\/i.stack.imgur.com\/GS2Fe.png\" alt=\"Azure Console\"><\/p>\n\n<p>The app ID and subscription key, sourced from above, are the exact same as what I'm using to hit the query API successfully (see note at bottom). My account is pay-as-you-go (not free).<\/p>\n\n<p><strong><em>Am I doing something wrong here? Is this API deprecated, moved, down, or out-of-sync with the docs?<\/em><\/strong><\/p>\n\n<p><strong>NOTE:<\/strong> I can manipulate my model through the online GUI but that approach will be far too manual for our business needs where our model will need to be programmatically updated as new business entities come into existence.  <\/p>\n\n<p><strong>NOTE:<\/strong> The programmatic API is different from the query API which has this request URL, which is working fine for me:<br>\n<code>https:\/\/api.projectoxford.ai\/luis\/v2.0\/apps\/{appId}?subscription-key={subscription_key}&amp;verbose=true&amp;q={utterance}<\/code>  <\/p>\n\n<p><strong>NOTE:<\/strong> There doesn't seem to be a Luis.ai programmatic API for v2.0--which is why the URLs from the query and programmatic APIs have different versions.  <\/p>",
        "Challenge_closed_time":1484669845332,
        "Challenge_comment_count":2,
        "Challenge_created_time":1484180085280,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering a 401 unauthorized response when attempting to call the Microsoft Luis.ai programmatic API, despite providing the correct app ID and subscription key. The user is unsure if they are doing something wrong or if the API is deprecated, moved, down, or out-of-sync with the documentation. The user needs to programmatically update their model, but manipulating it through the online GUI is too manual for their business needs. The programmatic API is different from the query API, which is working fine for the user. There doesn't seem to be a Luis.ai programmatic API for v2.0.",
        "Challenge_last_edit_time":1484193011887,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41603082",
        "Challenge_link_count":4,
        "Challenge_participation_count":4,
        "Challenge_readability":10.4,
        "Challenge_reading_time":23.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":136.0444588889,
        "Challenge_title":"401 Errors Calling the Microsoft Luis.ai Programmatic API",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1280.0,
        "Challenge_word_count":229,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1343167997556,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":191.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>Answering my own question here:<\/p>\n\n<p>I have found my LUIS.ai programmatic API key. It is found by:\nLUIS.ai dashboard -> username (upper-right) -> settings in dropdown -> Subscription Keys tab -> Programmatic API Key<\/p>\n\n<p>It was not immediately obvious since it's found nowhere else: not alongside any of the other key listings in cognitive services or the LUIS.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":4.63,
        "Solution_score_count":7.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unauthorized response"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3166666667,
        "Challenge_answer_count":2,
        "Challenge_body":"I just want to clarify my understanding. I can use my own servers for calling webhooks correct (as long as they return the json structure required). The webhooks will essentially reach out another API service and return data for fulfillment. Thanks in advance for your time.",
        "Challenge_closed_time":1673513220000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1673512080000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking clarification on whether they can use their own servers to call webhooks, as long as the required JSON structure is returned. The webhooks will be used to reach out to another API service and return data for fulfillment.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Webhooks\/m-p\/509590#M1056",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.6,
        "Challenge_reading_time":3.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.3166666667,
        "Challenge_title":"Webhooks",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":97.0,
        "Challenge_word_count":46,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Exactly correct.\u00a0 During the processing of a conversation, if you have a Web Hook enabled, the Dialogflow engine will call-out to the target URL passing in a JSON payload and expecting a correctly formatted JSON response.\n\nSee the following for details:\n\nhttps:\/\/cloud.google.com\/dialogflow\/cx\/docs\/concept\/webhook\n\nTake care to notice that the target service MUST be callable through HTTPS which means that it has a valid SSL certificate.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.0,
        "Solution_reading_time":5.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":69.0,
        "Tool":"Vertex AI",
        "Challenge_type":"inquiry",
        "Challenge_summary":"using own servers for webhooks"
    },
    {
        "Answerer_created_time":1608712056580,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":496.0,
        "Answerer_view_count":23.0,
        "Challenge_adjusted_solved_time":231.5724452778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to submit batch prediction job for a custom model (in my case it is torch model, but I think this is irrelevant in this case). So I read the documentation:\n<a href=\"https:\/\/i.stack.imgur.com\/WGa7T.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WGa7T.png\" alt=\"batch prediction from file-list\" \/><\/a><\/p>\n<p>But as there are no examples I cannot be sure what the schema of the json object which vertex ai will send to my model will be. Does someone have made this work ?<\/p>\n<p>My best guess is that the request will be with the following body:<\/p>\n<pre><code>{'instance' : &lt;b64-encoded-content-of-the-file&gt;}\n<\/code><\/pre>\n<p>But when I read the documentation (for other 'features' of vertex ai) I could imagine the following body as well:<\/p>\n<pre><code>{'instance': {'b64' : &lt;b64-encoded-content-of-the-file&gt;}}\n<\/code><\/pre>\n<p>Does somebody actually know ?<\/p>\n<p>Another thing I did is to make a 'fake-model' which returns the request it gets ... when I submit the batch-prediction job it actually finishes successfully but when I check the output file it is empty ... so ... I actually need help\/more time to think of other ways to decipher vertex ai docs.<\/p>\n<p>Thanks in advance!<\/p>",
        "Challenge_closed_time":1626781802423,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625948141620,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to submit a batch prediction job for a custom model on Vertex AI, but is unsure of the schema of the JSON object that Vertex AI will send to the model. The user has tried to create a \"fake-model\" to decipher the documentation, but the output file is empty. The user is seeking help to understand the Vertex AI documentation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68331232",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":16.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":231.5724452778,
        "Challenge_title":"Vertex AI batch predictions from file-list",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":611.0,
        "Challenge_word_count":180,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1455786472727,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Varna, Bulgaria",
        "Poster_reputation_count":405.0,
        "Poster_view_count":100.0,
        "Solution_body":"<p>Vertex AI <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#request_requirements\" rel=\"nofollow noreferrer\">custom container<\/a> should wrap a service with an endpoint (predict) for receiving a list of instances, each is a json serializable object<\/p>\n<pre><code>{'instances': [{'b64' : &lt;b64-encoded-content-of-the-file1&gt;}, {'b64' : &lt;b64-encoded-content-of-the-file1&gt;}, ...]}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":21.5,
        "Solution_reading_time":6.02,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":33.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"empty output file"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":188.0286397222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using one of the images listed here <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md<\/a>, to create an sagemaker endpoint, but I keep getting &quot;failed reason: Image size 15136109518 is greater that suppported size 1073741824&quot; .<\/p>\n<p>is there a way to find out the size of images provided <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md<\/a> or any aws managed images?<\/p>",
        "Challenge_closed_time":1645569051340,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644892148237,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble creating a sagemaker endpoint using an image from the AWS deep learning containers repository due to the image size being greater than the supported size. The user is seeking a way to determine the size of images provided by AWS managed images or the deep learning containers repository.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71120471",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":18.6,
        "Challenge_reading_time":9.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":188.0286397222,
        "Challenge_title":"How to determine size of images available in aws?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":153.0,
        "Challenge_word_count":62,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1590797441983,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":525.0,
        "Poster_view_count":98.0,
        "Solution_body":"<p>I suspect you are trying to deploy a serverless endpoint provisioned with 1GB of memory. As discussed <a href=\"https:\/\/repost.aws\/questions\/QU35dVp2D9SKKUnnVYGw9Z7A\/how-to-check-determine-image-container-size-for-aws-managed-images\" rel=\"nofollow noreferrer\">here<\/a> &quot;You can increase the memory size of your endpoint with the MemorySizeInMB parameter, more info in this documentation: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config%22\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config&quot;<\/a><\/p>\n<p>In order to view the uncompressed size of an image you can use the following example command:<\/p>\n<pre><code>$ docker pull 763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-training:1.15.2-cpu-py27-ubuntu18.04\n\n$ docker inspect -f &quot;{{ .Size }}&quot; 763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-training:1.15.2-cpu-py27-ubuntu18.04\n<\/code><\/pre>\n<p>Kindly also note that you will need to provision enough memory to accommodate your model as well. Please see this <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints.html#serverless-endpoints-how-it-works-memory\" rel=\"nofollow noreferrer\">link<\/a> for more information.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":20.1,
        "Solution_reading_time":18.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":99.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"image size too large"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11.1486111111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,  \n  \nI am new to SageMaker and I am trying to deploy my model to an endpoint but am getting the following error:  \n  \n**Failure reason**  \nUnable to locate at least 2 availability zone(s) with the requested instance type ml.t2.medium that overlap with SageMaker subnets  \n  \nI have tried using different instance types but always the same error  \n  \nI was under the impression that SageMaker will create the required instances for me and I do not need to create the instances first? I am using the EU-WEST-1 zone and using the console to setup the endpoint",
        "Challenge_closed_time":1553556696000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1553516561000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to deploy their model to an endpoint in SageMaker. They are receiving an error message stating that at least 2 availability zones with the requested instance type ml.t2.medium are not available in SageMaker subnets. The user has tried using different instance types but the error persists. They are unsure if they need to create instances first as they were under the impression that SageMaker would create the required instances. The user is using the EU-WEST-1 zone and setting up the endpoint using the console.",
        "Challenge_last_edit_time":1668612166704,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUySs_fgNpSE6wuY-6W7MwqQ\/unable-to-create-endpoint",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":6.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":11.1486111111,
        "Challenge_title":"Unable to create endpoint",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":666.0,
        "Challenge_word_count":97,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello,  \n  \nSagemaker engineer here. I looked at the VpcConfig of your model and found only one subnet configured.   \n  \nThe error message \"Unable to locate at least 2 availability zone(s) with the requested instance type XYZ that overlap with SageMaker subnets\" usually indicates misconfigured VPCs. Sagemaker imposes mandatory requirement for at least 2 availability zones in your VPC subnets even if you only request one instance, to account for the potential use of auto-scaling in the future.   \n  \nIn order to create the endpoint, the number of subnets in your model needs to be at least 2 in distinct availability zones, and ideally as close to the total number of availability zones as possible in the region.   \n  \nHope it helps,   \nWenzhao",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1553556696000,
        "Solution_link_count":0.0,
        "Solution_readability":11.0,
        "Solution_reading_time":8.89,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":119.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unavailable instance type"
    },
    {
        "Answerer_created_time":1430203014072,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":432.0,
        "Answerer_view_count":54.0,
        "Challenge_adjusted_solved_time":1392.8864805556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'd like to connect to an AzureML Web Service. I have looked into the POST Method on the Arduino Homepage and also here <a href=\"https:\/\/iotguys.wordpress.com\/2014\/12\/25\/communicating-with-microsoft-azure-eventhub-using-arduino\/\" rel=\"nofollow\">https:\/\/iotguys.wordpress.com\/2014\/12\/25\/communicating-with-microsoft-azure-eventhub-using-arduino\/<\/a><\/p>\n\n<p>Here is my Setup method:<\/p>\n\n<pre><code>    void setup()\n    {\n      Serial.begin(9600);\n      while (!Serial) {\n      ; \/\/ wait for serial port to connect.\n      }\n\n     Serial.println(\"ethernet\");\n\n     if (Ethernet.begin(mac) == 0) {\n       Serial.println(\"ethernet failed\");\n       for (;;) ;\n     }\n    \/\/ give the Ethernet shield a second to initialize:\n    delay(1000);\n }\n<\/code><\/pre>\n\n<p>The Post Method is based on this: <a href=\"http:\/\/playground.arduino.cc\/Code\/WebClient\" rel=\"nofollow\">http:\/\/playground.arduino.cc\/Code\/WebClient<\/a><\/p>\n\n<p>I just added <code>sprintf(outBuf, \"Authorization: Bearer %s\\r\\n\", api_key);<\/code> to the header, with <code>char* api_key = \"the ML Web Service API KEY\"<\/code><\/p>\n\n<p>Also, unlike specified in the WebClient I use the whole WebService URI as url and do not specify a page name.<\/p>\n\n<p>This doesn't work.<\/p>\n\n<p>The Network to which I am connecting has Internet Access.<\/p>\n\n<p>What am I doing wrong?<\/p>",
        "Challenge_closed_time":1450337261483,
        "Challenge_comment_count":0,
        "Challenge_created_time":1445322870153,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to connect to an AzureML Web Service using an Arduino Uno and has followed the POST Method on the Arduino Homepage and other resources. The user has added the API key to the header and is using the whole WebService URI as the URL. However, the connection is not working, and the user is seeking help to identify the issue.",
        "Challenge_last_edit_time":1459290344956,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/33229576",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":16.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":1392.8864805556,
        "Challenge_title":"Arduino Uno - WebService (AzureML)",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":154.0,
        "Challenge_word_count":132,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369151239452,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":516.0,
        "Poster_view_count":57.0,
        "Solution_body":"<p>Machine Learning Studio services that you create needs to receive requests from a device that has SSL capabilities to perform HTTPS requests. AFAIK, Arduino doesn't support SSL capabilities.<\/p>\n\n<p>One usual scenario is to attach the Arduino to a third device like Raspberry Pi 2 etc to use it as a gateway and do the call from the Pi itself.<\/p>\n\n<p>Here's a sample <a href=\"https:\/\/github.com\/Azure\/connectthedots\/blob\/master\/GettingStarted.md\" rel=\"nofollow\">project<\/a> from Microsoft Open Technologies team that utilizes Arduino Uno, Raspberry pi and Azure stuff.<\/p>\n\n<p>Hope this helps!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1450346645647,
        "Solution_link_count":1.0,
        "Solution_readability":9.8,
        "Solution_reading_time":7.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":82.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"connection not working"
    },
    {
        "Answerer_created_time":1623879163643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":224.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":180.3498686111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>i am trying to understand how input_fn, predict_fn and outout_fn work? I am able to understand what they are, but I am not able to understand  how they are called (invoked), can anyone help me understand the same<\/p>",
        "Challenge_closed_time":1661363649820,
        "Challenge_comment_count":1,
        "Challenge_created_time":1660714390293,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking help in understanding how input_fn, predict_fn, and output_fn work in AWS Sagemaker script mode, specifically how they are called or invoked.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73383302",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":3.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":180.3498686111,
        "Challenge_title":"how does input_fn, predict_fn and output_fn work in aws sagemaker script mode?",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":39.0,
        "Challenge_word_count":49,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1573543021663,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":17.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>When your endpoint comes up, <code>model_fn<\/code> is invoked so that your model is loaded. When you invoke the endpoint, <code>input_fn<\/code> is called so that your input payload is parsed, immediately after that, <code>predict_fn<\/code> is called so that a prediction is generated, and then <code>output_fn<\/code> is called to parse the prediction before returning it to the caller.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.2,
        "Solution_reading_time":4.95,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"understanding input\/output functions"
    },
    {
        "Answerer_created_time":1491898605956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sydney NSW, Australia",
        "Answerer_reputation_count":161.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":19.0323811111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am looking at <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/sklearn_abalone_featurizer.py\" rel=\"nofollow noreferrer\">this example<\/a> to implement the data processing of incoming raw data for a sagemaker endpoint prior to model inference\/scoring. This is all great but I have 2 questions:<\/p>\n<ul>\n<li>How can one debug this (e.g can I invoke endpoint without it being exposed as restful API and then use Sagemaker debugger)<\/li>\n<li>Sagemaker can be used &quot;remotely&quot; - e.g. via VSC. Can such a script be uploaded programatically?<\/li>\n<\/ul>\n<p>Thanks.<\/p>",
        "Challenge_closed_time":1651199956772,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651131440200,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to implement data processing for a SageMaker endpoint prior to model inference\/scoring using an example from GitHub. They have two questions: how to debug the implementation and whether a script can be uploaded programmatically for remote use via VSC.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72039744",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.0,
        "Challenge_reading_time":9.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":19.0323811111,
        "Challenge_title":"debug and deploy featurizer (data processor for imodel inference) of sagemaker endpoint",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":33.0,
        "Challenge_word_count":86,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>Sagemaker Debugger is only to monitor the training jobs.<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/train-debugger.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/train-debugger.html<\/a><\/p>\n<p>I dont think you can use it on Endpoints.<\/p>\n<p>The script that you have provided is used both for training and inference. The container used by the estimator will take care of what functions to run. So it is not possible to debug the script directly. But what are you debugging in the code ? Training part or the inference part ?<\/p>\n<p>While creating the estimator we need to give either the entry_point or the source directory. If you are using the &quot;entry_point&quot; then the value should be relative path to the file, if you are using &quot;source_dir&quot; then you should be able to give an S3 path. So before running the estimator, you can programmatically tar the files and upload it to S3 and then use the S3 path in the estimator.<\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.2,
        "Solution_reading_time":12.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":145.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"debug and upload script"
    },
    {
        "Answerer_created_time":1340833876128,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":751.0,
        "Answerer_view_count":73.0,
        "Challenge_adjusted_solved_time":1.1074488889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to call an Azure Machine Learning Pipeline Endpoint I've set up using C# &amp; the Machine Learning REST api.<\/p>\n<p>I am certain that I have the Service Principal configured correctly, as I can successfully authenticate &amp; hit the endpoint using the <code>azureml-core<\/code> python sdk:<\/p>\n<pre><code>sp = ServicePrincipalAuthentication(\n    tenant_id=tenant_id,\n    service_principal_id=service_principal_id,\n    service_principal_password=service_principal_password)\nws =Workspace.get(\n    name=workspace_name, \n    resource_group=resource_group, \n    subscription_id=subscription_id, \n    auth=sp)\n\nendpoint = PipelineEndpoint.get(ws, name='MyEndpoint')\nendpoint.submit('Test_Experiment')\n<\/code><\/pre>\n<p>I'm using the following example in C# to attempt to run my endpoint: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-pipelines#run-a-published-pipeline-using-c\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-pipelines#run-a-published-pipeline-using-c<\/a><\/p>\n<p>I'm attempting to fill <code>auth_key<\/code> with the following code:<\/p>\n<pre><code>var clientId = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_ID&quot;);\nvar clientSecret = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_SECRET&quot;);\nvar tenantId = Environment.GetEnvironmentVariable(&quot;AZURE_TENANT_ID&quot;);\n\nvar cred = new ClientSecretCredential(tenantId, clientId, clientSecret);\nvar auth_key = cred.GetToken(new Azure.Core.TokenRequestContext(new string[] {&quot;.default&quot; }));\n<\/code><\/pre>\n<p>I receive a 401 (unauthorized).<\/p>\n<p>What am I am doing wrong?<\/p>\n<ul>\n<li>UPDATE *<\/li>\n<\/ul>\n<p>I changed the 'scopes' param in the <code>TokenRequestContext<\/code> to look like:<\/p>\n<pre><code>var auth_key = cred.GetToken(new Azure.Core.TokenRequestContext(new string[] { &quot;http:\/\/DataTriggerApp\/.default&quot; }));\n<\/code><\/pre>\n<p><code>http:\/\/DataTriggerApp<\/code> is one of the <code>servicePrincipalNames<\/code> that shows up when i query my Service Principal from the azure CLI.<\/p>\n<p>Now, when I attempt to use the returned token to call the Machine Learning Pipeline Endpoint, I receive a 403 instead of a 401.  Maybe some progress?<\/p>",
        "Challenge_closed_time":1634160031172,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634153827710,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to call an Azure Machine Learning Pipeline Endpoint using C# and the Machine Learning REST API. They have configured the Service Principal correctly and can successfully authenticate and hit the endpoint using the azureml-core python SDK. However, when attempting to run the endpoint in C#, they receive a 401 unauthorized error. They have tried to fill the auth_key with the correct code but still receive the same error. They have updated the scopes param in the TokenRequestContext and now receive a 403 error instead of a 401.",
        "Challenge_last_edit_time":1634156473112,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69561386",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":15.7,
        "Challenge_reading_time":30.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":1.7231838889,
        "Challenge_title":"How do I use Service Principal authentication with an Azure Machine Learning Pipeline Endpoint in C#?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":752.0,
        "Challenge_word_count":204,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1340833876128,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":751.0,
        "Poster_view_count":73.0,
        "Solution_body":"<p>Ok, through a lot of trial-and-error I was able to come up with two ways of acquiring a token that allows me to hit my Azure Machine Learning Pipeline Endpoint through the REST api.  One uses Microsoft.Identity.Client &amp; one uses Azure.Identity.<\/p>\n<pre><code>using Microsoft.Identity.Client;\n\n...\n\npublic static async Task&lt;string&gt; GetAccessToken()\n{\n      var clientId = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_ID&quot;);\n      var clientSecret = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_SECRET&quot;);\n      var tenantId = Environment.GetEnvironmentVariable(&quot;AZURE_TENANT_ID&quot;);\n\n   \n      var app = ConfidentialClientApplicationBuilder.Create(clientId)\n                                                .WithClientSecret(clientSecret)                                                \n                                                .WithAuthority(AzureCloudInstance.AzurePublic, tenantId)\n                                                .Build();\n      var result = await app.AcquireTokenForClient(new string[] { &quot;https:\/\/ml.azure.com\/.default&quot; }).ExecuteAsync();\n      return result.AccessToken;\n}\n<\/code><\/pre>\n<p>Or:<\/p>\n<pre><code>using Azure.Identity;\n...\n\npublic static async Task&lt;string&gt; GetAccessToken()\n{\n      var clientId = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_ID&quot;);\n      var clientSecret = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_SECRET&quot;);\n      var tenantId = Environment.GetEnvironmentVariable(&quot;AZURE_TENANT_ID&quot;);\n\n\n      var cred = new ClientSecretCredential(tenantId, clientId, clientSecret);\n      var token =  await cred.GetTokenAsync(new Azure.Core.TokenRequestContext(new string[] { &quot;https:\/\/ml.azure.com\/.default&quot; }));\n      return token.Token;\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1634160459928,
        "Solution_link_count":2.0,
        "Solution_readability":22.1,
        "Solution_reading_time":20.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":107.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"authentication error"
    },
    {
        "Answerer_created_time":1406731060412,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Washington, USA",
        "Answerer_reputation_count":139.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":406.3169388889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>SageMaker provides a full machine learning development environment on AWS. It works with the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/\" rel=\"nofollow noreferrer\">Amazon SageMaker Python SDK<\/a>, which allows Jupyter Notebooks to interact with the functionality. This also provides the path to using the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_featurestore.html\" rel=\"nofollow noreferrer\">Amazon SageMaker Feature Store<\/a>.<\/p>\n<p>Is there any REST API available for SageMaker? Say one wanted to create their own custom UI, but still use SageMaker features, is this possible?<\/p>\n<p>Can it be done using the <a href=\"https:\/\/aws.amazon.com\/api-gateway\/\" rel=\"nofollow noreferrer\">Amazon API Gateway<\/a>?<\/p>",
        "Challenge_closed_time":1626469400220,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625006659240,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the availability of a REST API for SageMaker and whether it is possible to interact with SageMaker through the Amazon API Gateway to create a custom UI while still utilizing SageMaker features.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68186468",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":12.0,
        "Challenge_reading_time":11.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":406.3169388889,
        "Challenge_title":"Is there a REST API available for SageMaker, or is it possible to interact with SageMaker over the Amazon API Gateway?",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1164.0,
        "Challenge_word_count":102,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1346443720088,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":11650.0,
        "Poster_view_count":977.0,
        "Solution_body":"<p>Amazon API Gateway currently does not provide first-class integration for SageMaker. But you can use these services via AWS SDK. If you wish, you can embed the AWS SDK calls into a service, host on AWS (e.g. running on EC2 or as lambda functions) and use API gateway to expose your REST API.<\/p>\n<p>Actually, SageMaker is not fundamentally different from any other AWS service from this aspect.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.3,
        "Solution_reading_time":4.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"SageMaker REST API"
    },
    {
        "Answerer_created_time":1443426419048,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation_count":842.0,
        "Answerer_view_count":219.0,
        "Challenge_adjusted_solved_time":125.3573088889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am new to Azure ML Studio. I tried creating an experiment that takes a numeric value as input and a gives a data table type output. I works fine when I run it in the portal , but not when I run it as a web service. It shows a single value numeric output , when it has to be a data table type.<\/p>\n\n<p>Is there a way to change the output type of web service output? <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/oq5Xb.png\" rel=\"nofollow noreferrer\">Visualizing output in portal<\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/wUmN7.png\" rel=\"nofollow noreferrer\">Test RRS output(web service)<\/a><\/p>",
        "Challenge_closed_time":1486009486972,
        "Challenge_comment_count":6,
        "Challenge_created_time":1485555724117,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with the output type of a web service in Azure ML Studio. The experiment works fine in the portal but shows a single value numeric output instead of a data table type when run as a web service. The user is seeking a solution to change the output type of the web service output.",
        "Challenge_last_edit_time":1485558200660,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41903982",
        "Challenge_link_count":2,
        "Challenge_participation_count":7,
        "Challenge_readability":7.5,
        "Challenge_reading_time":7.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":126.0452375,
        "Challenge_title":"Web service output - Azure ML Studio",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":587.0,
        "Challenge_word_count":96,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1449123268407,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"East Newark, NJ, United States",
        "Poster_reputation_count":33.0,
        "Poster_view_count":43.0,
        "Solution_body":"<p>Make is a classic web service and see the JSON output getting from it. If it's providing all data you need.. go for it.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.9,
        "Solution_reading_time":1.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":24.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"incorrect output type"
    },
    {
        "Answerer_created_time":1342685175156,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":12103.0,
        "Answerer_view_count":1451.0,
        "Challenge_adjusted_solved_time":257.7152338889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm completely new to Azure ML, but I wanted to try out their automated ML UX. So I've followed the instructions to finally deploy my app (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-portal-experiments#deploy-your-model\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-portal-experiments#deploy-your-model<\/a>). Now I've got my \"Scoring URI\", but I don't know how to use it? <strong>How can I test an input and get an output - can I do it with Postman?<\/strong><\/p>\n\n<ul>\n<li>the tutorial doesn't tell me what to do with this \"Scoring URI\", and so I am stuck<\/li>\n<\/ul>",
        "Challenge_closed_time":1565163075267,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565145529467,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is new to Azure ML and has followed the instructions to deploy their app. They have obtained the \"Scoring URI\" but are unsure how to use it to test input and get output. The tutorial does not provide guidance on how to use the \"Scoring URI\".",
        "Challenge_last_edit_time":1565163086470,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57386269",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":9.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":4.8738333334,
        "Challenge_title":"How to use Azure ML Scoring URI?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":826.0,
        "Challenge_word_count":85,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1526863814910,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":45.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>On the bottom of the page that you have linked above, there is a link:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-consume-web-service\" rel=\"nofollow noreferrer\">Learn how to consume a web service.<\/a><\/p>\n\n<p>This is exactly on that topic on how to use the deployed web service for scoring (sending an input and getting an output).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1566090861312,
        "Solution_link_count":1.0,
        "Solution_readability":10.7,
        "Solution_reading_time":4.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":48.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"use Scoring URI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10.8094663889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>As part of our MLOps flow, we need to retrain a machine learning model using the AML designer, and then update the AKS webservice with the new machine learning model (+ a couple of other supplementary training artifacts), also from the designer.  <\/p>\n<p>We have built an inference pipeline to do this, and are able to run it manually. However, the solution requirements require this process to be automated. We have previously successfully automated this through the python SDK and the akswebservice.update method, but this solution has a hard requirement to use the designer only (custom python code blocks would be allowed, however).  <\/p>\n<p>Is there a way, using any Azure services (eg Azure Data Factory, Azure DevOps), that we can kick off a designer real time inference update pipeline immediately after its associated training pipeline finishes executing, in order to get the latest model version into the webservice, without any manual intervention? To be clear though, manual intervention is acceptable to build the initial inference pipeline for version 1, but not on the retraining cycle.<\/p>",
        "Challenge_closed_time":1620691417852,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620652503773,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user needs to automate the process of retraining a machine learning model using Azure ML Designer and updating the AKS webservice with the new model and other training artifacts. They have successfully automated this process using the Python SDK and akswebservice.update method, but now have a requirement to use the designer only. The user is looking for a way to kick off a real-time inference update pipeline immediately after the associated training pipeline finishes executing, without any manual intervention. They are open to using Azure services such as Azure Data Factory or Azure DevOps.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/389170\/azure-ml-designer-automatically-update-aks-webserv",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.4,
        "Challenge_reading_time":14.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":10.8094663889,
        "Challenge_title":"Azure ML Designer: Automatically Update AKS Webservice After Training",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":183,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. Currently, you can only use the Azure Machine Learning SDK to automatically <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-update-web-service\">update the web service<\/a>. I'm inquiring from the product team whether there are plans to support this scenario (will share updates accordingly). Hope this helps.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.1,
        "Solution_reading_time":4.89,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":44.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"automate model retraining and updating"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":524.4738888889,
        "Challenge_answer_count":2,
        "Challenge_body":"Hello,  \n  \nI have trained my model on sagemaker. I have deleted the endpoint, but I am keeping the model and the endpoint configuration which points to the model.  \n  \nFrom the sagemaker dashboard I am able to recreate the endpoint using the existing endpoint configuration. However I don't want to keep the endpoint on all the time, as I will use it only once a day for a few minutes.  \n  \nIs it possible to create in on demand from a Python script? I would assume that it is possible, but can't find how. Can someone point me in the right direction?  \n  \nRegards.",
        "Challenge_closed_time":1626971777000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625083671000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a model on sagemaker and deleted the endpoint, but kept the model and endpoint configuration. They want to recreate the endpoint on demand from a Python script instead of keeping it on all the time, but are unsure how to do so and are seeking guidance.",
        "Challenge_last_edit_time":1668599351248,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUTyUMHH4QRDaMa6L24rhOMg\/create-endpoint-from-python",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.0,
        "Challenge_reading_time":6.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":524.4738888889,
        "Challenge_title":"Create endpoint from Python",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":125.0,
        "Challenge_word_count":104,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello hugoflores,   \n  \nYou can use SageMaker APIs - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DeleteEndpoint.html to delete the endpoint and https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateEndpoint.html. to create an endpoint. This an be automated either using SageMaker Pipelines or a Lambda function.  \n  \nHere are a few resources towards that:  \n  \nhttps:\/\/awsfeed.com\/whats-new\/machine-learning\/build-a-ci-cd-pipeline-for-deploying-custom-machine-learning-models-using-aws-services  \nhttps:\/\/github.com\/aws-samples\/aws-lambda-layer-create-script  \nhttps:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1200  \nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/  \nhttps:\/\/www.sagemakerworkshop.com\/step\/deploymodel\/  \n  \nHTH,   \n  \nChaitanya",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1626971777000,
        "Solution_link_count":7.0,
        "Solution_readability":32.1,
        "Solution_reading_time":11.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":44.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"recreate endpoint on demand"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":18.0674716667,
        "Challenge_answer_count":1,
        "Challenge_body":"I am trying to do some inference on some CSV file saved on s3 using BatchTransform with `strategy ='MultiRecord'` and `assemble_with='Line'`. The same system works with `strategy ='SingleRecord'`, however I need it to be as efficient as possible. The main issue comes when I switch to MultiRecord with a small csv composed of two columns, both texts.\nWith a `max_payload = 6`, the process is succesfull with a CSV of 181 samples (609.7KB), but with the same CSV with 182 samples (621.6KB), the process fails with a `\"message\": \"Worker died.\"`. I imagined it had something to do with the memory limit of the instance I am using, so I switched to a `ml.m5.2xlarge` with 32GB of memory. \nWhen I switch to a `max_payload = 7`, suddenly the process works with the 182 samples of CSV (621.6KB), but it fails with anything bigger than that. Any ideas of what could be causing this?\n\nThe logs look like this\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,218 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 0%| | 0.00\/8.68M [00:00<?, ?B\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,327 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 0%| | 20.0k\/8.68M [00:00<00:47, 191kB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,434 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 1%| | 100k\/8.68M [00:00<00:17, 526kB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,543 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 3%|\u258e | 228k\/8.68M [00:00<00:10, 841kB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,651 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 6%|\u258c | 509k\/8.68M [00:00<00:05, 1.56MB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,759 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 12%|\u2588\u258f | 1.04M\/8.68M [00:00<00:02, 2.91MB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,867 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 25%|\u2588\u2588\u258d | 2.14M\/8.68M [00:00<00:01, 5.54MB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,975 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 50%|\u2588\u2588\u2588\u2588\u2589 | 4.31M\/8.68M [00:00<00:00, 10.6MB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,976 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 8.65M\/8.68M [00:00<00:00, 20.7MB\/s]\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,129 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8.68M\/8.68M [00:00<00:00, 10.5MB\/s]\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,129 [WARN ] W-9007-model_1.0-stderr MODEL_LOG -\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,131 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 0%| | 0.00\/615 [00:00<?, ?B\/s]\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,894 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13647\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,895 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13459\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,895 [INFO ] W-9006-model_1.0 TS_METRICS - W-9006-model_1.0.ms:15108|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,896 [INFO ] W-9006-model_1.0 TS_METRICS - WorkerThreadTime.ms:56|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,897 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1684489793897\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,898 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13705\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,898 [INFO ] W-9007-model_1.0 TS_METRICS - W-9007-model_1.0.ms:15111|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,899 [INFO ] W-9007-model_1.0 TS_METRICS - WorkerThreadTime.ms:42|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,896 [INFO ] W-9005-model_1.0 TS_METRICS - W-9005-model_1.0.ms:15111|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,902 [INFO ] W-9005-model_1.0 TS_METRICS - WorkerThreadTime.ms:108|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,906 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13399\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,906 [INFO ] W-9000-model_1.0 TS_METRICS - W-9000-model_1.0.ms:15148|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,907 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:73|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,907 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Backend received inference at: 1684489793\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:53,989 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13784\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:53,991 [INFO ] W-9002-model_1.0 TS_METRICS - W-9002-model_1.0.ms:15231|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:53,992 [INFO ] W-9002-model_1.0 TS_METRICS - WorkerThreadTime.ms:56|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,009 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13663\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,009 [INFO ] W-9001-model_1.0 TS_METRICS - W-9001-model_1.0.ms:15249|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489794\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,010 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:103|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489794\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,038 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13293\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,038 [INFO ] W-9003-model_1.0 TS_METRICS - W-9003-model_1.0.ms:15277|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489794\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,039 [INFO ] W-9003-model_1.0 TS_METRICS - WorkerThreadTime.ms:52|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489794\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,187 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13751\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,187 [INFO ] W-9004-model_1.0 TS_METRICS - W-9004-model_1.0.ms:15426|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489794\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,188 [INFO ] W-9004-model_1.0 TS_METRICS - WorkerThreadTime.ms:87|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489794\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,971 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:66.7|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,972 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:46.74238204956055|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,972 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:9.122749328613281|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,973 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:16.3|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,973 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:17504.14453125|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,974 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:13734.4453125|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,974 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:44.8|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,909 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60000\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,910 [ERROR] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Number or consecutive unsuccessful inference 1\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,910 [ERROR] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error\n\n2023-05-19T12:50:54.009+03:00\torg.pytorch.serve.wlm.WorkerInitializationException: Backend worker did not respond in given time\n\n2023-05-19T12:50:54.009+03:00\t#011at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:199) [model-server.jar:?]\n\n2023-05-19T12:50:54.009+03:00\t#011at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]\n\n2023-05-19T12:50:54.009+03:00\t#011at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]\n\n2023-05-19T12:50:54.009+03:00\t#011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\n\n2023-05-19T12:50:54.009+03:00\t#011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\n\n2023-05-19T12:50:54.009+03:00\t#011at java.lang.Thread.run(Thread.java:829) [?:?]\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,945 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_MODEL_LOADED\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,946 [INFO ] W-9006-model_1.0 ACCESS_LOG - \/169.254.255.130:54478 \"POST \/invocations HTTP\/1.1\" 500 72597\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,947 [INFO ] W-9006-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489779\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,947 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stderr\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,948 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stdout\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,949 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds.\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,995 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1684489853995\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,997 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Backend received inference at: 1684489853\n\n2023-05-19T12:50:55.010+03:00\t2023-05-19T09:50:54,024 [INFO ] W-9006-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stdout\n\n2023-05-19T12:50:55.010+03:00\t2023-05-19T09:50:54,024 [INFO ] W-9006-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stderr\n\n2023-05-19T12:50:57.010+03:00\t2023-05-19T09:50:56,227 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Listening on port: \/home\/model-server\/tmp\/.ts.sock.9006\n\n2023-05-19T12:50:57.010+03:00\t2023-05-19T09:50:56,235 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Successfully loaded \/opt\/conda\/lib\/python3.8\/site-packages\/ts\/configs\/metrics.yaml.\n\n2023-05-19T12:50:57.010+03:00\t2023-05-19T09:50:56,235 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - [PID]428\n\n2023-05-19T12:50:57.011+03:00\t2023-05-19T09:50:56,236 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: \/home\/model-server\/tmp\/.ts.sock.9006\n\n2023-05-19T12:50:57.011+03:00\t2023-05-19T09:50:56,235 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Torch worker started.\n\n2023-05-19T12:50:57.011+03:00\t2023-05-19T09:50:56,237 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\n\n2023-05-19T12:50:57.011+03:00\t2023-05-19T09:50:56,238 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Connection accepted: \/home\/model-server\/tmp\/.ts.sock.9006.\n\n2023-05-19T12:50:57.011+03:00\t2023-05-19T09:50:56,238 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1684489856238\n\n2023-05-19T12:50:57.011+03:00\t2023-05-19T09:50:56,269 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\n\n2023-05-19T12:51:07.014+03:00\t2023-05-19T09:51:06,351 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 10083\n\n2023-05-19T12:51:07.014+03:00\t2023-05-19T09:51:06,351 [INFO ] W-9006-model_1.0 TS_METRICS - W-9006-model_1.0.ms:87564|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489866\n\n2023-05-19T12:51:07.014+03:00\t2023-05-19T09:51:06,352 [INFO ] W-9006-model_1.0 TS_METRICS - WorkerThreadTime.ms:30|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489866\n\n2023-05-19T12:51:40.023+03:00\t2023-05-19T09:51:39,971 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:40.023+03:00\t2023-05-19T09:51:39,972 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:46.74235534667969|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:40.023+03:00\t2023-05-19T09:51:39,972 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:9.12277603149414|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:40.023+03:00\t2023-05-19T09:51:39,972 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:16.3|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:40.023+03:00\t2023-05-19T09:51:39,973 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:17432.91796875|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:40.024+03:00\t2023-05-19T09:51:39,974 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:13805.67578125|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:40.024+03:00\t2023-05-19T09:51:39,974 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:45.0|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:54.028+03:00\t2023-05-19T09:51:53,996 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60000\n\n2023-05-19T12:51:54.028+03:00\t2023-05-19T09:51:53,997 [ERROR] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Number or consecutive unsuccessful inference 2\n\n2023-05-19T12:51:54.028+03:00\t2023-05-19T09:51:53,997 [ERROR] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error\n\n2023-05-19T12:51:54.028+03:00\torg.pytorch.serve.wlm.WorkerInitializationException: Backend worker did not respond in given time\n\n2023-05-19T12:51:54.028+03:00\t#011at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:199) [model-server.jar:?]\n\n2023-05-19T12:51:54.028+03:00\t#011at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)",
        "Challenge_closed_time":1684555351304,
        "Challenge_comment_count":0,
        "Challenge_created_time":1684490308406,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while using SageMaker Batch Transform with `strategy ='MultiRecord'` and `assemble_with='Line'` to perform inference on CSV files saved on S3. The process works with `strategy ='SingleRecord'`, but fails with a `\"message\": \"Worker died.\"` error when using `strategy ='MultiRecord'` with a CSV file of 182 samples (621.6KB) and a `max_payload` of 6. The user suspects that the issue is related to the memory limit of the instance being used and switches to a `ml.m5.2xlarge` instance with 32GB of memory. However, the process only works with a `max_payload` of 7 and fails with anything bigger than that. The user is seeking ideas on what could be causing this issue.",
        "Challenge_last_edit_time":1684836402615,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUAI7z2SCLT06bUOu1ULKV-Q\/sagemaker-batch-transform-multirecord-fail-with-csv-as-input",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":205.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":183,
        "Challenge_solved_time":18.0674716667,
        "Challenge_title":"SageMaker Batch Transform MultiRecord Fail with CSV as Input",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":65.0,
        "Challenge_word_count":837,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"There could be two possible reasons for the error message : \"message\": \"Worker died.\".\n\n1. This is commonly noticed when the instance exhausts the Memory usage. (This can be verified if you can check the Cloudwatch Metrics for the job run.)\n2. Model server timeouts during the job. \n\nYou can either use a larger instance for the job or try to set the higher value of following environment variables - worker and timeout in your script. \n\n\tmodel_server_workers = int(os.environ.get(_params.MODEL_SERVER_WORKERS_ENV, num_cpus())) \n\tmodel_server_timeout = int(os.environ.get(_params.MODEL_SERVER_TIMEOUT_ENV, <Enter value here>))",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1684555351304,
        "Solution_link_count":0.0,
        "Solution_readability":9.0,
        "Solution_reading_time":7.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":82.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"Batch Transform error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":457.1169444444,
        "Challenge_answer_count":0,
        "Challenge_body":"The bucket of processed data does not exist (src\/sagemaker\/FD_SL_Training_BYO_Codes.ipynb)\r\n\r\n\r\n### Reproduction Steps\r\n\r\naws s3 ls s3:\/\/fraud-detection-solution\/processed_data\r\n\r\n\r\n\r\n### Error Log\r\n\r\nAn error occurred (NoSuchBucket) when calling the ListObjectsV2 operation: The specified bucket does not exist\r\n\r\n\r\n\r\n### Environment\r\n\r\n  - **CDK CLI Version:** 1.75.0 (build 7708242)\r\n  - **Framework Version:** not installed\r\n  - **Node.js Version:**  not installed\r\n  - **OS               :**\r\n\r\n### Other\r\n\r\n<!-- e.g. detailed explanation, stacktraces, related issues, suggestions on how to fix, links for us to have context, eg. associated pull-request, stackoverflow, gitter, etc -->\r\n\r\n\r\n\r\n--- \r\n\r\nThis is :bug: Bug Report",
        "Challenge_closed_time":1621931178000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620285557000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue with their Sagemaker endpoint failing to deploy or timing out with a server error (0) bug. The error log shows that the backend worker process died due to a parameter conflict between the Sagemaker endpoint deployment code and the model training code on n-hidden and hidden_size. The user provided reproduction steps and environment details.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/realtime-fraud-detection-with-gnn-on-dgl\/issues\/103",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":12.7,
        "Challenge_reading_time":9.06,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":32.0,
        "Challenge_repo_issue_count":1277.0,
        "Challenge_repo_star_count":169.0,
        "Challenge_repo_watch_count":22.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":457.1169444444,
        "Challenge_title":"The data path inside sagemaker notebook does not work",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":85,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"endpoint deployment error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":524.3688888889,
        "Challenge_answer_count":0,
        "Challenge_body":"Hello,\r\n\r\nWhen running the experiment, the error message **Environment name can not start with the prefix AzureML** was displayed. How can I set the name of the environment? I'm following the GitHub tutorial and haven't found anything about it.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117882725-01767d80-b281-11eb-8df5-36d8683523e7.png)\r\n\r\nCode used:\r\n\r\n- Registering Dataset\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883192-81044c80-b281-11eb-9dec-d73431948061.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883230-8c577800-b281-11eb-8445-060839369fe5.png)\r\n\r\n- Training Pipeline\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883313-a5602900-b281-11eb-818d-3972111d7f9c.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883356-b315ae80-b281-11eb-99d9-1ac6c0989186.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883429-c7f24200-b281-11eb-88de-3979570adb55.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883465-d2144080-b281-11eb-8b9c-f74756bedd01.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883535-e48e7a00-b281-11eb-9d31-e035f44a0871.png)\r\n\r\nReferences:\r\n\r\n- https:\/\/github.com\/microsoft\/solution-accelerator-many-models\/tree\/master\/Automated_ML\/02_AutoML_Training_Pipeline\r\n- https:\/\/github.com\/MicrosoftDocs\/azure-docs\/issues\/65770\r\n\r\nBest regards,\r\nCristina\r\n\r\n\r\n---\r\n#### Detalhes do documento\r\n\r\n\u26a0 *N\u00e3o edite esta se\u00e7\u00e3o. \u00c9 necess\u00e1rio para a vincula\u00e7\u00e3o do problema do docs.microsoft.com \u279f GitHub.*\r\n\r\n* ID: 49399a7d-d4e8-370e-ce62-d60a6b64e412\r\n* Version Independent ID: 782d8ba4-75dd-27c3-5a46-a921c3ead4bf\r\n* Content: [azureml.contrib.automl.pipeline.steps.AutoMLPipelineBuilder class - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/pt-br\/python\/api\/azureml-contrib-automl-pipeline-steps\/azureml.contrib.automl.pipeline.steps.automlpipelinebuilder?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/stable\/docs-ref-autogen\/azureml-contrib-automl-pipeline-steps\/azureml.contrib.automl.pipeline.steps.AutoMLPipelineBuilder.yml](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr.pt-BR\/blob\/live\/AzureML-Docset\/stable\/docs-ref-autogen\/azureml-contrib-automl-pipeline-steps\/azureml.contrib.automl.pipeline.steps.AutoMLPipelineBuilder.yml)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @DebFro\r\n* Microsoft Alias: **debfro**",
        "Challenge_closed_time":1622654301000,
        "Challenge_comment_count":10,
        "Challenge_created_time":1620766573000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to use `azureml.exceptions.WebserviceException` to effectively manage errors in REST API calls for Azure Machine Learning Service's AKS Webservice Endpoint. They want to know how to raise exceptions to provide proper feedback to end-users in case of unsuccessful API calls.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1468",
        "Challenge_link_count":12,
        "Challenge_participation_count":10,
        "Challenge_readability":29.1,
        "Challenge_reading_time":35.07,
        "Challenge_repo_contributor_count":58.0,
        "Challenge_repo_fork_count":2387.0,
        "Challenge_repo_issue_count":1906.0,
        "Challenge_repo_star_count":3704.0,
        "Challenge_repo_watch_count":2001.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":524.3688888889,
        "Challenge_title":"AutoMLPipelineBuilder.get_many_models_train_steps - Error \"Environment name can not start with the prefix AzureML...\"",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":112,
        "Discussion_body":"Hi,\r\n\r\nI have some updates:\r\n\r\n- I put the code below to set the environment.\r\n\r\nfrom azureml.core.environment import Environment\r\n\r\nenv = Environment.get(workspace=ws, name=\"AzureML-Tutorial\")\r\nmyenv = env.clone(\"automl_env\")\r\n\r\ntrain_steps = AutoMLPipelineBuilder.get_many_models_train_steps(experiment=experiment,\r\n                                                                automl_settings=automl_settings,\r\n                                                                train_data=dataset_input,\r\n                                                                compute_target=compute_target,\r\n                                                                partition_column_names=partition_column_names,\r\n                                                                node_count=1,\r\n                                                                process_count_per_node=2,\r\n                                                                run_invocation_timeout=3700,\r\n                                                                train_env=myenv)\r\n\r\n- The environment problem has been resolved, but now the process displays the message **ValueError: None is not in list**. I don't know what this means.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/118014360-82894f80-b329-11eb-8e6a-558d6606d7b1.png)\r\n\r\nFailure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (pyarrow 3.0.0 (\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages), Requirement.parse('pyarrow<2.0.0,>=0.17.0'), {'azureml-dataset-runtime'}).\r\nTraceback (most recent call last):\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/runtime\/_many_models\/train_model.py\", line 212, in <module>\r\n    logs = run(data_file_path, args, automl_settings, current_step_run)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/runtime\/_many_models\/train_model.py\", line 100, in run\r\n    data = pd.read_csv(file_path, parse_dates=[timestamp_column])\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 676, in parser_f\r\n    return _read(filepath_or_buffer, kwds)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 448, in _read\r\n    parser = TextFileReader(fp_or_buf, **kwds)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 880, in __init__\r\n    self._make_engine(self.engine)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 1114, in _make_engine\r\n    self._engine = CParserWrapper(self.f, **self.options)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 1949, in __init__\r\n    self._set_noconvert_columns()\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 2015, in _set_noconvert_columns\r\n    _set(val)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 2005, in _set\r\n    x = names.index(x)\r\nValueError: None is not in list\r\n\r\nBest regards,\r\nCristina @crisansou is there any error surfaced in 70_driver_log? \r\nHi @shbijlan ,\r\n\r\nI deleted the workspace. I tried to reproduce the steps again but I couldn't even create the experiment, below is the error message. Can you tell which is the recommended version to use this solution?\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/119876912-c752e000-befe-11eb-9a18-c8f03afae48c.png)\r\n\r\nI don't know if it's related, but I realized that now compute instance is using version 1.29.\r\n\r\n!pip install --upgrade azureml-sdk[automl]\r\n!pip install azureml-contrib-automl-pipeline-steps\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/119877097-03864080-beff-11eb-8134-07d22a243284.png)\r\n\r\n\r\n\r\n\r\n> @crisansou is there any error surfaced in 70_driver_log?\r\n\r\n from azureml.core. import Environment\r\ntrain_env = Environment.get(workspace = ws, name = 'AzureML-AutoML')\r\n\r\nCan you please pass train_env like above for the workaround? There is a bug in our code that needs to be fixed. we will fix it in our next release.\r\n\r\nAlso this solution only supports 'forecasting' and needs a time_column_name passed in automl settings Hi @deeptim123 ,\r\n\r\nThanks for the instructions! After including the environment I was able to run the cell, but the pipeline ended with an error because I am using a regression model.\r\n\r\nIn the next release, in addition to fixing the bug, will it be possible to use regression?\r\n\r\n> from azureml.core. import Environment\r\n> train_env = Environment.get(workspace = ws, name = 'AzureML-AutoML')\r\n> \r\n> Can you please pass train_env like above for the workaround? There is a bug in our code that needs to be fixed. we will fix it in our next release.\r\n> \r\n> Also this solution only supports 'forecasting' and needs a time_column_name passed in automl settings\r\n\r\n There are currently no plans to support regression. @cartacioS  for visibility of this ask @deeptim123 ,\r\n\r\nThanks for the info. I think it's important to add this functionality for regression and classification as well.\r\n\r\n> There are currently no plans to support regression. @cartacioS for visibility of this ask\r\n\r\n @crisansou - this is currently not on our roadmap, and purposefully unprioritized as 90% of our customer base, especially customers investing in thousands+ models are leveraging only forecasting scenarios. Priorities change from one semester to the next, and it may be supported at a later date but is not in scope right now. If you are or have a direct customer who is blocked by the lack of many model support for regression and classification please contact me at sabina.cartacio@microsoft.com and we can further discuss.\r\n\r\nThanks! Hi @cartacioS ,\r\n\r\nGot it, thanks for the info! The project is starting now, but if it is really necessary to use the multiple models solution for regression I'll send you an email.\r\n\r\n> @crisansou - this is currently not on our roadmap, and purposefully unprioritized as 90% of our customer base, especially customers investing in thousands+ models are leveraging only forecasting scenarios. Priorities change from one semester to the next, and it may be supported at a later date but is not in scope right now. If you are or have a direct customer who is blocked by the lack of many model support for regression and classification please contact me at [sabina.cartacio@microsoft.com](mailto:sabina.cartacio@microsoft.com) and we can further discuss.\r\n> \r\n> Thanks!\r\n\r\n Closing as this is being tracked offline as a feature request for MANY MODELS, by Sabina.",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"manage REST API errors"
    },
    {
        "Answerer_created_time":1553882107003,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":294.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":2777.7798802778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am deploying a model onto AWS via Sagemaker:<\/p>\n\n<p>I set up my JSON schema as follow:<\/p>\n\n<pre><code>import json\nschema = {\n    \"input\": [\n        {\n            \"name\": \"V1\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V2\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V3\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V4\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V5\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V6\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V7\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V8\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V9\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V10\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V11\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V12\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V13\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V14\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V15\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V16\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V17\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V18\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V19\",\n            \"type\": \"double\"\n        }, \n                {\n            \"name\": \"V20\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V21\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V22\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V23\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V24\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V25\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V26\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V27\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V28\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"Amount\",\n            \"type\": \"double\"\n        },         \n    ],\n    \"output\": \n        {\n            \"name\": \"features\",\n            \"type\": \"double\",\n            \"struct\": \"vector\"\n        }\n}\nschema_json = json.dumps(schema)\nprint(schema_json)\n<\/code><\/pre>\n\n<p>And deployed as:<\/p>\n\n<pre><code>from sagemaker.model import Model\nfrom sagemaker.pipeline import PipelineModel\nfrom sagemaker.sparkml.model import SparkMLModel\n\nsparkml_data = 's3:\/\/{}\/{}\/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')\n# passing the schema defined above by using an environment variable that sagemaker-sparkml-serving understands\nsparkml_model = SparkMLModel(model_data=sparkml_data, env={'SAGEMAKER_SPARKML_SCHEMA' : schema_json})\nxgb_model = Model(model_data=xgb_model.model_data, image=training_image)\n\nmodel_name = 'inference-pipeline-' + timestamp_prefix\nsm_model = PipelineModel(name=model_name, role=role, models=[sparkml_model, xgb_model])\n\n    endpoint_name = 'inference-pipeline-ep-' + timestamp_prefix\nsm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)\n<\/code><\/pre>\n\n<p>I got the error as below:<\/p>\n\n<p>ClientError: An error occurred (ValidationException) when calling the CreateModel operation: 1 validation error detected: Value '{SAGEMAKER_SPARKML_SCHEMA={\"input\": [{\"type\": \"double\", \"name\": \"V1\"}, {\"type\": \"double\", \"name\": \"V2\"}, {\"type\": \"double\", \"name\": \"V3\"}, {\"type\": \"double\", \"name\": \"V4\"}, {\"type\": \"double\", \"name\": \"V5\"}, {\"type\": \"double\", \"name\": \"V6\"}, {\"type\": \"double\", \"name\": \"V7\"}, {\"type\": \"double\", \"name\": \"V8\"}, {\"type\": \"double\", \"name\": \"V9\"}, {\"type\": \"double\", \"name\": \"V10\"}, {\"type\": \"double\", \"name\": \"V11\"}, {\"type\": \"double\", \"name\": \"V12\"}, {\"type\": \"double\", \"name\": \"V13\"}, {\"type\": \"double\", \"name\": \"V14\"}, {\"type\": \"double\", \"name\": \"V15\"}, {\"type\": \"double\", \"name\": \"V16\"}, {\"type\": \"double\", \"name\": \"V17\"}, {\"type\": \"double\", \"name\": \"V18\"}, {\"type\": \"double\", \"name\": \"V19\"}, {\"type\": \"double\", \"name\": \"V20\"}, {\"type\": \"double\", \"name\": \"V21\"}, {\"type\": \"double\", \"name\": \"V22\"}, {\"type\": \"double\", \"name\": \"V23\"}, {\"type\": \"double\", \"name\": \"V24\"}, {\"type\": \"double\", \"name\": \"V25\"}, {\"type\": \"double\", \"name\": \"V26\"}, {\"type\": \"double\", \"name\": \"V27\"}, {\"type\": \"double\", \"name\": \"V28\"}, {\"type\": \"double\", \"name\": \"Amount\"}], \"output\": {\"type\": \"double\", \"name\": \"features\", \"struct\": \"vector\"}}}' at 'containers.1**.<strong>member.environment' failed to satisfy constraint: Map value must satisfy constraint: [Member must have length less than or equal to 1024<\/strong>,** Member must have length greater than or equal to 0, Member must satisfy regular expression pattern: [\\S\\s]*]<\/p>\n\n<p>I try to reduce my features to 20 and it able to deploy. Just wondering how can I Pass the schema with 29 attributes?<\/p>",
        "Challenge_closed_time":1600448991412,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590448983843,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error while deploying a model onto AWS via Sagemaker. The error occurred due to the JSON schema exceeding the maximum length allowed by AWS. The user was able to deploy the model successfully after reducing the number of features to 20. The user is seeking a solution to pass the schema with 29 attributes.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62012264",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":17.0,
        "Challenge_reading_time":51.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":2777.7798802778,
        "Challenge_title":"AWS SageMaker SparkML Schema Eroor: member.environment' failed to satisfy constraint",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":226.0,
        "Challenge_word_count":418,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1359061977540,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"New York, NY, USA",
        "Poster_reputation_count":427.0,
        "Poster_view_count":62.0,
        "Solution_body":"<p>I do not think the environment length of 1024 limit will be increased in a short time. To work around this, you could try to rebuild the spark ml container with the <code>SAGEMAKER_SPARKML_SCHEMA<\/code> env var:<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-sparkml-serving-container\/blob\/master\/README.md#running-the-image-locally\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-sparkml-serving-container\/blob\/master\/README.md#running-the-image-locally<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.8,
        "Solution_reading_time":6.48,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":39.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"JSON schema too long"
    },
    {
        "Answerer_created_time":1394703217223,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cologne, Germany",
        "Answerer_reputation_count":486.0,
        "Answerer_view_count":26.0,
        "Challenge_adjusted_solved_time":75.2264213889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I would like to host a model on Sagemaker using the new <a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2021\/12\/amazon-sagemaker-serverless-inference\/?nc1=h_ls\" rel=\"nofollow noreferrer\">Serverless Inference<\/a>.<\/p>\n<p>I wrote my own container for inference and handler following several guides. These are the requirements:<\/p>\n<pre><code>mxnet\nmulti-model-server\nsagemaker-inference\nretrying\nnltk\ntransformers==4.12.4\ntorch==1.10.0\n<\/code><\/pre>\n<p>On non-serverless endpoints, this container works perfectly well. However, with the serverless version I get the following error message when loading the model:<\/p>\n<pre><code>ERROR - \/.sagemaker\/mms\/models\/model already exists.\n<\/code><\/pre>\n<p>The error is thrown by the following subprocess<\/p>\n<pre><code>['model-archiver', '--model-name', 'model', '--handler', '\/home\/model-server\/handler_service.py:handle', '--model-path', '\/opt\/ml\/model', '--export-path', '\/.sagemaker\/mms\/models', '--archive-format', 'no-archive']\n<\/code><\/pre>\n<p>So something that has to do with the <code>model-archiver<\/code> (which I guess is a process from the MMS package?).<\/p>",
        "Challenge_closed_time":1639671069487,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639400254370,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to host a model on Sagemaker using the new Serverless Inference. They have written their own container for inference and handler, but when loading the model, they receive an error message stating that the model already exists. The error is thrown by the subprocess 'model-archiver', which is a process from the MMS package.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70335049",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":14.5,
        "Challenge_reading_time":15.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":75.2264213889,
        "Challenge_title":"Sagemaker Serverless Inference & custom container: Model archiver subprocess fails",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":373.0,
        "Challenge_word_count":116,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1394703217223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cologne, Germany",
        "Poster_reputation_count":486.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>So the issue really was related to hosting the model using the sagemaker inference toolkit and MMS which always uses the multi-model scenario which is not supported by serverless inference.<\/p>\n<p>I ended up writing my own Flask API which actually is nearly as easy and more customizable. Ping me for details if you're interested.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.5,
        "Solution_reading_time":4.17,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":54.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"model already exists error"
    },
    {
        "Answerer_created_time":1340063804276,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":298.0,
        "Answerer_view_count":38.0,
        "Challenge_adjusted_solved_time":0.3479,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>So our team created a new Azure <strong>Machine Learning<\/strong> resource, but whenever I try to add a new notebook and try to edit it using &quot;JUPYTERLAB&quot; i get <code>ERR_HTTP2_PROTOCOL_ERROR<\/code> error, but the same notebook, when edited using <code>EDIT IN JUPYTER<\/code> works perfectly.<\/p>\n<p>This is a blank and clean notebook, I also tried 2 different laptops and multiple browsers per laptop, same error. I also tried incognito and clearing cookies, but to no avail.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/IevSG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IevSG.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>update: I seem to have accidentally replicated the issue and I now know what is causing it, the situation is that Im using my work laptop and constantly switching VPN connections, and some times, connecting to the AZURE PORTAl OUTSIDE the VPN. So, when you've worked on a notebook while inside a VPN, then you disconnected, and tried loading the notebook sometime later, you will encounter this<\/p>",
        "Challenge_closed_time":1596643705983,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596642453543,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an ERR_HTTP2_PROTOCOL_ERROR when trying to edit a new notebook using JUPYTERLAB in Azure Machine Learning Studio. The error persists across different laptops and browsers, even after clearing cookies and using incognito mode. The issue was later found to be caused by switching VPN connections and accessing the Azure portal outside of the VPN.",
        "Challenge_last_edit_time":1597056501367,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63268849",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":12.2,
        "Challenge_reading_time":14.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.3479,
        "Challenge_title":"ERR_HTTP2_PROTOCOL_ERROR when opening Notebook in JUPYTERLAB Azure ML Studio",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":246.0,
        "Challenge_word_count":158,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1340063804276,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":298.0,
        "Poster_view_count":38.0,
        "Solution_body":"<p>This problem has stomped me for hours, but I was finally able to fix it. What I did was I opened a terminal and did a Jupyter lab rebuild &quot;jupyter lab build&quot;<\/p>\n<p><a href=\"https:\/\/imgur.com\/aRB8GWS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/imgur.com\/aRB8GWS.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/IceQO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IceQO.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1596669031916,
        "Solution_link_count":4.0,
        "Solution_readability":10.2,
        "Solution_reading_time":6.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":52.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"HTTP2 protocol error"
    },
    {
        "Answerer_created_time":1324988509368,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Moscow, Russia",
        "Answerer_reputation_count":1593.0,
        "Answerer_view_count":93.0,
        "Challenge_adjusted_solved_time":812.6895558333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to Invoke Endpoint, previously deployed on Amazon SageMaker.\nHere is my code:<\/p>\n\n<pre><code>import numpy as np\nimport boto3\n\nclient = boto3.client('sagemaker-runtime')\n\ndef np2csv(arr):\n    csv = io.BytesIO()\n    np.savetxt(csv, arr, delimiter=',', fmt='%g')\n    return csv.getvalue().decode().rstrip()\n\nendpoint_name = 'DEMO-XGBoostEndpoint-2018-12-12-22-07-28'\ntest_vector = np.array([3.60606061e+00, \n                        3.91395664e+00, \n                        1.34200000e+03, \n                        4.56100000e+03,\n                        2.00000000e+02, \n                        2.00000000e+02]) \ncsv_test_vector = np2csv(test_vector)\n\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                               ContentType='text\/csv',\n                               Body=csv_test_vector)\n<\/code><\/pre>\n\n<p>And here is the error I get:<\/p>\n\n<blockquote>\n  <p>ModelErrorTraceback (most recent call last)\n   in ()\n        1 response = client.invoke_endpoint(EndpointName=endpoint_name,\n        2                                    ContentType='text\/csv',\n  ----> 3                                    Body=csv_test_vector)<\/p>\n  \n  <p>\/home\/ec2-user\/anaconda3\/envs\/python2\/lib\/python2.7\/site-packages\/botocore\/client.pyc\n  in _api_call(self, *args, **kwargs)\n      318                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n      319             # The \"self\" in this scope is referring to the BaseClient.\n  --> 320             return self._make_api_call(operation_name, kwargs)\n      321 \n      322         _api_call.<strong>name<\/strong> = str(py_operation_name)<\/p>\n  \n  <p>\/home\/ec2-user\/anaconda3\/envs\/python2\/lib\/python2.7\/site-packages\/botocore\/client.pyc\n  in _make_api_call(self, operation_name, api_params)\n      621             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n      622             error_class = self.exceptions.from_code(error_code)\n  --> 623             raise error_class(parsed_response, operation_name)\n      624         else:\n      625             return parsed_response<\/p>\n  \n  <p>ModelError: An error occurred (ModelError) when calling the\n  InvokeEndpoint operation: Received client error (415) from model with\n  message \"setting an array element with a sequence.\". See\n  <a href=\"https:\/\/us-east-1.console.aws.amazon.com\/cloudwatch\/home?region=us-east-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/DEMO-XGBoostEndpoint-2018-12-12-22-07-28\" rel=\"nofollow noreferrer\">https:\/\/us-east-1.console.aws.amazon.com\/cloudwatch\/home?region=us-east-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/DEMO-XGBoostEndpoint-2018-12-12-22-07-28<\/a>\n  in account 249707424405 for more information.<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1547665192368,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544739509967,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to invoke an endpoint previously deployed on Amazon SageMaker. The error message indicates that there is an issue with setting an array element with a sequence. The error code received is 415 and the user is advised to check the provided link for more information.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53770876",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":17.3,
        "Challenge_reading_time":31.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":812.6895558333,
        "Challenge_title":"AWS Sagemaker, InvokeEndpoint operation, Model error: \"setting an array element with a sequence.\"",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":2561.0,
        "Challenge_word_count":171,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324988509368,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Moscow, Russia",
        "Poster_reputation_count":1593.0,
        "Poster_view_count":93.0,
        "Solution_body":"<p>This works:<\/p>\n\n<pre><code>import numpy as np\nimport boto3\n\nclient = boto3.client('sagemaker-runtime')\nendpoint_name = 'DEMO-XGBoostEndpoint-2018-12-12-22-07-28'\ntest_vector = [3.60606061e+00, \n               3.91395664e+00, \n               1.34200000e+03, \n               4.56100000e+03,\n               2.00000000e+02, \n               2.00000000e+02]) \n\nbody = ',',join([str(item) for item in test_vector])\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                               ContentType='text\/csv',\n                               Body=body)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.1,
        "Solution_reading_time":5.95,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":30.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"error setting array element"
    },
    {
        "Answerer_created_time":1302088276340,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Dubai, United Arab Emirates",
        "Answerer_reputation_count":5263.0,
        "Answerer_view_count":637.0,
        "Challenge_adjusted_solved_time":1.5271127778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Working on a IoT telemetry project that receives humidity and weather pollution data from different sites on the field. I will then apply Machine Learning on the collected data. I'm using Event Hubs and Stream Analytics. Is there a way of pulling the data to Azure Machine Learning without the hassle of writing an application to get it from Stream Analytics and push to AML web service?<\/p>",
        "Challenge_closed_time":1467284456456,
        "Challenge_comment_count":0,
        "Challenge_created_time":1467278958850,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is working on an IoT telemetry project that collects humidity and weather pollution data from different sites using Event Hubs and Stream Analytics. They are looking for a way to pull the data to Azure Machine Learning without having to write an application to get it from Stream Analytics and push it to AML web service.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38119062",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.1,
        "Challenge_reading_time":5.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.5271127778,
        "Challenge_title":"Pulling data from Stream Analytics to Azure Machine Learning",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":627.0,
        "Challenge_word_count":75,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1311017514580,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Beirut, Lebanon",
        "Poster_reputation_count":408.0,
        "Poster_view_count":32.0,
        "Solution_body":"<p>Stream Analytics has a functionality called the \u201c<a href=\"https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/12\/10\/azure-ml-now-available-as-a-function-in-azure-stream-analytics\/\" rel=\"nofollow\">Functions<\/a>\u201d. You can call any web service you\u2019ve published using AML from within Stream Analytics and apply it within your Stream Analytics query. Check this <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/stream-analytics-machine-learning-integration-tutorial\/\" rel=\"nofollow\">link for a tutorial<\/a>.\nExample workflow in your case would be like the following;<\/p>\n\n<ul>\n<li>Telemetry arrives and reaches Stream Analytics<\/li>\n<li>Streaming Analytics (SA) calls the Machine Learning function to apply it on the data<\/li>\n<li>SA redirects it to the output accordingly, here you can use the PowerBI to create a predictions dashboards.<\/li>\n<\/ul>\n\n<p>Another way would be using R, and here\u2019s a good tutorial showing that <a href=\"https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/12\/10\/azure-ml-now-available-as-a-function-in-azure-stream-analytics\/\" rel=\"nofollow\">https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/12\/10\/azure-ml-now-available-as-a-function-in-azure-stream-analytics\/<\/a> . \nIt is more work of course but can give you more control as you control the code.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":19.6,
        "Solution_reading_time":17.45,
        "Solution_score_count":3.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":123.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"pull data to AML"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.8375316667,
        "Challenge_answer_count":1,
        "Challenge_body":"I want to use an Amazon Sagemaker endpoint for a custom classification model. The endpoint should only handle sporadic input (say a few times a week). \nFor this purpose I want to employ autoscaling that scales the number of instances down to 0 when the endpoint is not used. \n\nAre there any costs associated with having an endpoint with 0 instances? \n\nThanks!",
        "Challenge_closed_time":1666371029704,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666360814590,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user wants to use an Amazon SageMaker endpoint for a custom classification model that only handles sporadic input. They want to use autoscaling to scale the number of instances down to 0 when the endpoint is not used. The user is asking if there are any costs associated with having an endpoint with 0 instances.",
        "Challenge_last_edit_time":1668547705502,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU0VGYdZe8TRivmtGHoiDDHw\/cost-of-autoscaling-endpoint-amazon-sagemaker-endpoint-to-zero",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":5.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2.8375316667,
        "Challenge_title":"Cost of autoscaling endpoint Amazon SageMaker endpoint to zero",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":491.0,
        "Challenge_word_count":70,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You dont pay any compute costs for the duration when the endpoint size scales down to 0. But i think you can design it better. There are few other options for you to use in SageMaker Endpoint(assuming you are using realtime endpoint)\n\n1. Try using [SageMaker Serverless Inference](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints.html) instead. Its purely serverless in nature so you pay only when the endpoint is serving inference. i think that would fit your requirement better.\n2. You can think of using Lambda as well which will reduce your hosting costs. but you have to do more work in setting up the inference stack all by yourself.\n3. There is also an option of [SageMaker asynchronous inference](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/async-inference.html) but its mostly useful for inference which require longer time to process each request. The reason i mention this is it also support scale to 0 when no traffic is coming.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1666796875366,
        "Solution_link_count":2.0,
        "Solution_readability":8.4,
        "Solution_reading_time":12.05,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":144.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"cost of 0 instances"
    },
    {
        "Answerer_created_time":1585590244876,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":55.0,
        "Answerer_view_count":12.0,
        "Challenge_adjusted_solved_time":0.0392458333,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>We created a webservice endpoint and tested it with the following code, and also with POSTMAN.<\/p>\n\n<p>We deployed the service to an AKS in the same resource group and subscription as the AML resource.<\/p>\n\n<p><strong>UPDATE: the attached AKS had a custom networking configuration and rejected external connections.<\/strong><\/p>\n\n<pre><code>import numpy\nimport os, json, datetime, sys\nfrom operator import attrgetter\nfrom azureml.core import Workspace\nfrom azureml.core.model import Model\nfrom azureml.core.image import Image\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.authentication import AzureCliAuthentication\n\ncli_auth = AzureCliAuthentication()\n# Get workspace\nws = Workspace.from_config(auth=cli_auth)\n\n# Get the AKS Details\ntry:\n    with open(\"..\/aml_config\/aks_webservice.json\") as f:\n        config = json.load(f)\nexcept:\n    print(\"No new model, thus no deployment on AKS\")\n    # raise Exception('No new model to register as production model perform better')\n    sys.exit(0)\n\nservice_name = config[\"aks_service_name\"]\n# Get the hosted web service\nservice = Webservice(workspace=ws, name=service_name)\n\n# Input for Model with all features\ninput_j = [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]]\nprint(input_j)\ntest_sample = json.dumps({\"data\": input_j})\ntest_sample = bytes(test_sample, encoding=\"utf8\")\ntry:\n    prediction = service.run(input_data=test_sample)\n    print(prediction)\nexcept Exception as e:\n    result = str(e)\n    print(result)\n    raise Exception(\"AKS service is not working as expected\")\n<\/code><\/pre>\n\n<p>In AML Studio, the deployment state is \"Healthy\".<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/RTB10.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RTB10.png\" alt=\"Endpoint attributes\"><\/a><\/p>\n\n<p>We get the following error when testing:<\/p>\n\n<pre><code>Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'\n<\/code><\/pre>\n\n<p><strong>Log just after deploying the AKS Webservice <a href=\"http:\/\/t.ly\/t79b\" rel=\"nofollow noreferrer\">here<\/a>.<\/strong><\/p>\n\n<p><strong>Log after running the test script <a href=\"http:\/\/t.ly\/79k5\" rel=\"nofollow noreferrer\">here<\/a>.<\/strong><\/p>\n\n<p>How can we know what is causing this problem and fix it?<\/p>",
        "Challenge_closed_time":1592590202852,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592508291480,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user created a webservice endpoint and tested it with code and POSTMAN. The service was deployed to an AKS in the same resource group and subscription as the AML resource. However, the user encountered a TimeoutError when testing the service and is seeking help to identify and fix the issue. The AKS had a custom networking configuration and rejected external connections.",
        "Challenge_last_edit_time":1592590061567,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62457880",
        "Challenge_link_count":4,
        "Challenge_participation_count":3,
        "Challenge_readability":12.7,
        "Challenge_reading_time":30.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":22.7531588889,
        "Challenge_title":"AML - Web service TimeoutError",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":332.0,
        "Challenge_word_count":275,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1585590244876,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>We checked the AKS networking configuration and realized it has an Azure CNI profile.<\/p>\n\n<p>In order to test the webservice we need to do it from inside the created virtual network.\nIt worked well!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":2.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"TimeoutError when testing service"
    },
    {
        "Answerer_created_time":1539125320752,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":71.7034436111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I've trained a tensorflow.keras model using SageMaker Script Mode like this:<\/p>\n\n<pre><code>import os\n\nimport sagemaker\nfrom sagemaker.tensorflow import TensorFlow\n\nestimator = TensorFlow(entry_point='train.py',\n                       source_dir='src',\n                       train_instance_type=train_instance_type,\n                       train_instance_count=1,\n                       hyperparameters=hyperparameters,\n                       role=sagemaker.get_execution_role(),\n                       framework_version='1.12.0',\n                       py_version='py3', \n                       script_mode=True)\n<\/code><\/pre>\n\n<p>However, how do I specify what the serving code is when I call <code>estimator.deploy()<\/code>? And what is it by default? Also is there any way to modify the nginx.conf using Script Mode?<\/p>",
        "Challenge_closed_time":1548883525400,
        "Challenge_comment_count":1,
        "Challenge_created_time":1548625393003,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a tensorflow.keras model using SageMaker Script Mode and is now facing challenges in specifying the serving code when calling estimator.deploy(). The user is also unsure about the default serving code and if there is a way to modify the nginx.conf using Script Mode.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54393158",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":14.4,
        "Challenge_reading_time":8.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":71.7034436111,
        "Challenge_title":"SageMaker Script Mode Serving",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1352.0,
        "Challenge_word_count":65,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>With script mode the default serving method is the TensorFlow Serving-based one:\n<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/estimator.py#L393\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/estimator.py#L393<\/a>\nCustom script is not allowed with the TFS based container. You can use serving_input_receiver_fn to specify how the input data is processed as described here: <a href=\"https:\/\/www.tensorflow.org\/guide\/saved_model\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/guide\/saved_model<\/a><\/p>\n\n<p>As for modifying the ngnix.conf, there are no supported ways of doing that. Depends on what you want to change in the config file you can hack the sagemaker-python-sdk to pass in different values for these environment variables: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/blob\/3fd736aac4b0d97df5edaea48d37c49a1688ad6e\/container\/sagemaker\/serve.py#L29\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/blob\/3fd736aac4b0d97df5edaea48d37c49a1688ad6e\/container\/sagemaker\/serve.py#L29<\/a><\/p>\n\n<p>Here is where you can override the environment variables: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/serving.py#L130\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/serving.py#L130<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":8.0,
        "Solution_readability":22.2,
        "Solution_reading_time":20.15,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":100.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"specify serving code"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":23.80566,
        "Challenge_answer_count":1,
        "Challenge_body":"1. We trained custom model with manually annotated data set of documents but the accuracy is low, we want to annotate and train again. When I create a new version will it learn from the current set of inputs and also preserve the old training ? Do I need to give all the data for every incremental training ? \n\n2. Since there is some low accuracy issue, I want to add a2i . How to do it in console UI for batch processing ?\nWhen the people make additional annotation in a2i, will the comprehend learn incrementally ?\nOr do we need to make run the training job again ?",
        "Challenge_closed_time":1677047264656,
        "Challenge_comment_count":0,
        "Challenge_created_time":1676615139900,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a custom model with a manually annotated dataset of documents, but the accuracy is low. They want to re-annotate and train the model again, but are unsure if the new version will learn from the current set of inputs and preserve the old training. They also want to add Amazon Augmented AI (a2i) to improve accuracy and are unsure if the model will learn incrementally from additional annotations or if they need to run the training job again.",
        "Challenge_last_edit_time":1676961564280,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUuwusX1xNQO6J-M-AkCFlig\/comprehend-incremental-training",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":7.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":120.0346544444,
        "Challenge_title":"Comprehend incremental training",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":53.0,
        "Challenge_word_count":106,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You will need to provide all of the data for each incremental training. \n\nComprehend is not integrated with A2I at this time, so you would need to re-submit a new training job with all the annotations.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1677047264656,
        "Solution_link_count":0.0,
        "Solution_readability":9.1,
        "Solution_reading_time":2.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":36.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"re-annotate and use a2i"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":10.4660813889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>For AzureML we\u2019re using the REST api provided in published pipelines to launch them as part of scheduled jobs.<\/p>\n<p>It looks like if we republish an endpoint the GUID at the end of the URL changes.\nDo you have any recommendations for how to alias this so the URL can remain the same for a caller or keep it constant?<\/p>",
        "Challenge_closed_time":1640952460163,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640914782270,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with republishing an endpoint in AzureML, as the GUID at the end of the URL changes. They are seeking recommendations on how to alias the endpoint to keep the URL constant for the caller.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70538271",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":4.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":10.4660813889,
        "Challenge_title":"Any recommendation on republish an endpoint to remain same",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":124.0,
        "Challenge_word_count":68,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1632461310820,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":107.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>These are static, unique URLs that can be associated with multiple published pipeline versions (you can make one pipeline the default).<\/p>\n<p>Pipeline Endpoints:<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.pipelineendpoint?view=azure-ml-py\" rel=\"nofollow noreferrer\">azureml.pipeline.core.PipelineEndpoint class - Azure Machine Learning Python | Microsoft Docs<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.2,
        "Solution_reading_time":5.73,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":33.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"alias endpoint URL"
    },
    {
        "Answerer_created_time":1582182357612,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":155.0,
        "Answerer_view_count":27.0,
        "Challenge_adjusted_solved_time":392.2279305556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an ML model (trained locally) in python. Previously the model has been deployed to a Windows IIS server and it's working fine.<\/p>\n<p>Now, I am trying to deploy it as a service on Azure container instance (ACI) with 1 core, and 1 GB of memory. I took references from <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python\" rel=\"nofollow noreferrer\">one<\/a> and <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-existing-model\" rel=\"nofollow noreferrer\">two<\/a> Microsoft docs. The docs use SDK for all the steps, but <strong>I am using the GUI feature from the Azure portal<\/strong>.<\/p>\n<p>After registering the model, I created an entry script and a conda environment YAML file (see below), and uploaded both to &quot;Custom deployment asset&quot; (at Deploy model area).<\/p>\n<p>Unfortunately, after hitting deploy, the Deployment state is stuck at Transitioning state. Even after 4 hours, the state remains the same and there were no Deployment logs too, so I am unable to find what I am doing wrong here.<\/p>\n<blockquote>\n<p>NOTE: below is just an excerpt of the entry script<\/p>\n<\/blockquote>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport pickle\nimport re, json\nimport numpy as np\nimport sklearn\n\ndef init():\n    global model \n    global classes\n    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'randomForest50.pkl')\n    model = pickle.load(open(model_path, &quot;rb&quot;))\n\n    classes = lambda x : [&quot;F&quot;, &quot;M&quot;][x]\n\ndef run(data):\n    try:\n        namesList = json.loads(data)[&quot;data&quot;][&quot;names&quot;]\n        pred = list(map(classes, model.predict(preprocessing(namesList))))\n        return str(pred[0])\n    except Exception as e:\n        error = str(e)\n        return error\n<\/code><\/pre>\n<pre class=\"lang-yaml prettyprint-override\"><code>name: gender_prediction\ndependencies:\n- python\n- numpy\n- scikit-learn\n- pip:\n    - pandas\n    - pickle\n    - re\n    - json\n<\/code><\/pre>",
        "Challenge_closed_time":1612491964670,
        "Challenge_comment_count":1,
        "Challenge_created_time":1611073723990,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy a locally trained ML model as a service on Azure container instance (ACI) using the GUI feature from the Azure portal. After registering the model, creating an entry script, and a conda environment YAML file, the user uploaded both to \"Custom deployment asset\" but the Deployment state is stuck at Transitioning state, and there were no Deployment logs too.",
        "Challenge_last_edit_time":1611079944120,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65795579",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":11.3,
        "Challenge_reading_time":25.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":393.9557444445,
        "Challenge_title":"Debugging AML Model Deployment",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":106.0,
        "Challenge_word_count":231,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1582182357612,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":155.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>The issue was in the YAML file. <strong>The dependencies\/libraries in the YAML should be according to conda environment<\/strong>. So, I changed everything accordingly, and it worked.<\/p>\n<p>Modified YAML file:<\/p>\n<pre><code>name: gender_prediction\ndependencies:\n- python=3.7\n- numpy\n- scikit-learn\n- pip:\n    - azureml-defaults\n    - pandas\n    - pickle4\n    - regex\n    - inference-schema[numpy-support]   \n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.1,
        "Solution_reading_time":5.17,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":42.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"stuck at Transitioning state"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":6.3710825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there any way to connected google cloud platform service vertex ai endpoint through .Net code ? I am new to gcp vertex. any help is really apricated.<\/p>",
        "Challenge_closed_time":1648004091847,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647981155950,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking assistance in connecting a Google Cloud Platform service, Vertex AI endpoint, through .Net code and is new to GCP Vertex.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71578582",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.2,
        "Challenge_reading_time":2.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":6.3710825,
        "Challenge_title":"connect vertex ai endpoint through .Net",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":128.0,
        "Challenge_word_count":32,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1462469556836,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":509.0,
        "Poster_view_count":65.0,
        "Solution_body":"<p>You could refer to <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\" rel=\"nofollow noreferrer\">Google.Cloud.AIPlatform.V1 documentation<\/a> for reference for dot net. You can start by:<\/p>\n<ol>\n<li>Installing the package from <a href=\"https:\/\/www.nuget.org\/packages\/Google.Cloud.AIPlatform.V1\/\" rel=\"nofollow noreferrer\">Google AI Platform nuget<\/a><\/li>\n<li>If you don't have an endpoint yet you can check out <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\/Google.Cloud.AIPlatform.V1.EndpointServiceClient\" rel=\"nofollow noreferrer\">Google.Cloud.AIPlatform.V1.EndpointServiceClient<\/a>. You can use this class to manage endpoints like create endpoint, delete endpoint, deploy endpoint, etc.\n<ul>\n<li>Check this <a href=\"https:\/\/github.com\/googleapis\/google-cloud-dotnet\/blob\/main\/apis\/Google.Cloud.AIPlatform.V1\/Google.Cloud.AIPlatform.V1.Snippets\/IndexEndpointServiceClientSnippets.g.cs\" rel=\"nofollow noreferrer\">EndpointServiceClient code sample<\/a> for usage.<\/li>\n<\/ul>\n<\/li>\n<li>If you have an endpoint and you want to run predictions using it, you can check out <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\/Google.Cloud.AIPlatform.V1.PredictionServiceClient\" rel=\"nofollow noreferrer\">Google.Cloud.AIPlatform.V1.PredictionServiceClient<\/a>. You can use this class to perform prediction using your endpoint.\n<ul>\n<li>Specifically <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\/Google.Cloud.AIPlatform.V1.PredictionServiceClient#Google_Cloud_AIPlatform_V1_PredictionServiceClient_Predict_Google_Cloud_AIPlatform_V1_EndpointName_System_Collections_Generic_IEnumerable_Google_Protobuf_WellKnownTypes_Value__Google_Protobuf_WellKnownTypes_Value_Google_Api_Gax_Grpc_CallSettings_\" rel=\"nofollow noreferrer\">Predict(EndpointName, IEnumerable, Value, CallSettings)<\/a> method where it accepts an endpoint as parameter.<\/li>\n<li>Check this <a href=\"https:\/\/github.com\/googleapis\/google-cloud-dotnet\/blob\/main\/apis\/Google.Cloud.AIPlatform.V1\/Google.Cloud.AIPlatform.V1.Snippets\/PredictionServiceClientSnippets.g.cs\" rel=\"nofollow noreferrer\">PredictionServiceClient code sample<\/a> for usage.<\/li>\n<\/ul>\n<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":26.5,
        "Solution_reading_time":31.63,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":136.0,
        "Tool":"Vertex AI",
        "Challenge_type":"inquiry",
        "Challenge_summary":"connect to Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0783333333,
        "Challenge_answer_count":1,
        "Challenge_body":"Is it possible to train and deploy ML models in Greengrass? Or is Greengrass limited to inference while training is done using SageMaker in cloud?",
        "Challenge_closed_time":1556295681000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1556295399000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is questioning whether it is possible to train and deploy ML models in Greengrass or if it is limited to inference while training is done using SageMaker in the cloud.",
        "Challenge_last_edit_time":1668529586004,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU2FEvRboNRL2Ipn1yE7IBvg\/greengrass-for-data-processing-and-ml-model-training",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":2.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.0783333333,
        "Challenge_title":"Greengrass for data processing and ML model training",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":78.0,
        "Challenge_word_count":32,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"As a native service offering, Greengrass has support for deploying models to the edge and running *inference* code against those models. Nothing prevents you from deploying your own code to the edge that would train a model, but I suspect you wouldn't be able to store it as a Greengrass local resource for later inferences without doing a round trip to the cloud and redeploying to GG.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1606141115182,
        "Solution_link_count":0.0,
        "Solution_readability":15.2,
        "Solution_reading_time":4.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"train and deploy in Greengrass"
    },
    {
        "Answerer_created_time":1439246522636,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":0.0595647222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I try to test my Azure ML model, I get the following error: \u201cError code: InternalError, Http status code: 500\u201d, so it appears something is failing inside of the machine learning service. How do I get around this error?<\/p>",
        "Challenge_closed_time":1439848076380,
        "Challenge_comment_count":0,
        "Challenge_created_time":1439847861947,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an internal error while testing their Azure ML model, resulting in an HTTP status code of 500. They are seeking guidance on how to resolve this issue.",
        "Challenge_last_edit_time":1439906222223,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/32060196",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":3.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.0595647222,
        "Challenge_title":"Azure ML Internal Error",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1408.0,
        "Challenge_word_count":43,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1439847618300,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>I've run into this error before, and unfortunately, the only workaround I found was to create a new ML workspace backed by a storage account that you know is online. Then copy your experiment over to the new workspace, and things should work. It can be a bit cumbersome, but it should get rid of your error message. With the service being relatively new, things sometimes get corrupted as updates are being made, so I recommend checking the box labeled \"disable updates\" within your experiment.  Hope that helps!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.0,
        "Solution_reading_time":6.33,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":88.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"internal error"
    },
    {
        "Answerer_created_time":1263294862568,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":183045.0,
        "Answerer_view_count":13691.0,
        "Challenge_adjusted_solved_time":0.3125277778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to invoke a SageMaker enpoint from AWS Lambda using a lambda function.<\/p>\n<p>This is a sample API call to the endpoint from SageMaker Studio, working as expected:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3iTPN.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3iTPN.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>here's my Lambda function (<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">inspired from documentation<\/a>):<\/p>\n<pre><code>import os\nimport io\nimport boto3\nimport json\n\n\nENDPOINT_NAME = 'iris-autoscale-6'\nruntime= boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    # print(&quot;Received event: &quot; + json.dumps(event, indent=2))\n    payload = json.loads(json.dumps(event))\n    print(payload)\n    \n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\n    print(response)\n    result = json.loads(response['Body'].read().decode())\n    print(result)\n    \n    return result\n<\/code><\/pre>\n<p>My error message:<\/p>\n<pre><code>Test Event Name\nProperTest\n\nResponse\n{\n  &quot;errorMessage&quot;: &quot;Parameter validation failed:\\nInvalid type for parameter Body, value: {'sepal_length': [5.1, 4.9, 4.7, 4.6, 5], 'sepal_width': [3.5, 3, 3.2, 3.1, 3.6], 'petal_length': [1.4, 1.4, 1.3, 1.5, 1.4], 'petal_width': [0.2, 0.2, 0.2, 0.2, 0.2]}, type: &lt;class 'dict'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object&quot;,\n  &quot;errorType&quot;: &quot;ParamValidationError&quot;,\n  &quot;stackTrace&quot;: [\n    &quot;  File \\&quot;\/var\/task\/lambda_function.py\\&quot;, line 17, in lambda_handler\\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/client.py\\&quot;, line 386, in _api_call\\n    return self._make_api_call(operation_name, kwargs)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/client.py\\&quot;, line 678, in _make_api_call\\n    api_params, operation_model, context=request_context)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/client.py\\&quot;, line 726, in _convert_to_request_dict\\n    api_params, operation_model)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/validate.py\\&quot;, line 319, in serialize_to_request\\n    raise ParamValidationError(report=report.generate_report())\\n&quot;\n  ]\n}\n\nFunction Logs\nSTART RequestId: 70278b9f-f75e-4ac9-a827-7ad35d162512 Version: $LATEST\n{'sepal_length': [5.1, 4.9, 4.7, 4.6, 5], 'sepal_width': [3.5, 3, 3.2, 3.1, 3.6], 'petal_length': [1.4, 1.4, 1.3, 1.5, 1.4], 'petal_width': [0.2, 0.2, 0.2, 0.2, 0.2]}\n[ERROR] ParamValidationError: Parameter validation failed:\nInvalid type for parameter Body, value: {'sepal_length': [5.1, 4.9, 4.7, 4.6, 5], 'sepal_width': [3.5, 3, 3.2, 3.1, 3.6], 'petal_length': [1.4, 1.4, 1.3, 1.5, 1.4], 'petal_width': [0.2, 0.2, 0.2, 0.2, 0.2]}, type: &lt;class 'dict'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object\nTraceback (most recent call last):\n\u00a0\u00a0File &quot;\/var\/task\/lambda_function.py&quot;, line 17, in lambda_handler\n\u00a0\u00a0\u00a0\u00a0response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 386, in _api_call\n\u00a0\u00a0\u00a0\u00a0return self._make_api_call(operation_name, kwargs)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 678, in _make_api_call\n\u00a0\u00a0\u00a0\u00a0api_params, operation_model, context=request_context)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 726, in _convert_to_request_dict\n\u00a0\u00a0\u00a0\u00a0api_params, operation_model)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/validate.py&quot;, line 319, in serialize_to_request\n\u00a0\u00a0\u00a0\u00a0raise ParamValidationError(report=report.generate_report())\nEND RequestId: 70278b9f-f75e-4ac9-a827-7ad35d162512\nREPORT RequestId: 70278b9f-f75e-4ac9-a827-7ad35d162512  Duration: 26.70 ms  Billed Duration: 27 ms  Memory Size: 128 MB Max Memory Used: 76 MB  Init Duration: 343.10 ms\n<\/code><\/pre>\n<p>Here's the policy attached to the lambda function:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;VisualEditor0&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:sagemaker:ap-south-1:&lt;my-account-id&gt;:endpoint\/iris-autoscale-6&quot;\n        }\n    ]\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1629024434667,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629022235437,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to invoke a SageMaker endpoint from AWS Lambda using a lambda function. The error message indicates that the parameter validation failed due to an invalid type for the parameter Body. The user has provided the lambda function code, error message, and policy attached to the lambda function.",
        "Challenge_last_edit_time":1629023309567,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68790568",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":16.0,
        "Challenge_reading_time":60.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":44,
        "Challenge_solved_time":0.6108972222,
        "Challenge_title":"\"errorMessage\": \"Parameter validation failed in Lambda calling SageMaker endpoint",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":499.0,
        "Challenge_word_count":370,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559910246180,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":2046.0,
        "Poster_view_count":369.0,
        "Solution_body":"<p>The issue is that your <code>payload<\/code> has invalid format. It should be one of:<\/p>\n<pre><code>&lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object\n<\/code><\/pre>\n<p>The following should address the error (note: you may have many other issues in your code):<\/p>\n<pre><code>    payload = json.dumps(event)\n    print(payload)\n    \n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload.encode())\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.7,
        "Solution_reading_time":6.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":46.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"parameter validation failed"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":25.7594230556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi, I trained and deployed a ML model via Auto ML. The result looks like this:  <br \/>\n&quot;\\&quot;{\\\\&quot;result\\\\&quot;: [\\\\&quot;Test\\\\&quot;]}\\&quot;&quot;<\/p>\n<p>Once I did the same with an endpoint created with the Azure ML Designer my result looks like this:  <br \/>\n&quot;{\\&quot;Results\\&quot;: {\\&quot;WebServiceOutput0\\&quot;: [{\\&quot;Scored Labels\\&quot;: \\&quot;Test\\&quot;}]}}&quot;<\/p>\n<p>Is there a way to configure the response that it looks similar to the AutoML response?<\/p>\n<p>Thanks :)<\/p>",
        "Challenge_closed_time":1606912260000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606819526077,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a challenge in configuring the response of a ML model deployed via AutoML and an endpoint created with Azure ML Designer. The user wants to know if there is a way to configure the response of the endpoint to look similar to the AutoML response.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/181635\/how-to-configute-webserviceoutput",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":7.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":25.7594230556,
        "Challenge_title":"How to configute WebServiceOutput?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":65,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=fe5bc84e-425f-4db0-a234-78d2a8fbbae1\">@ID_27051995  <\/a> Unfortunately, AutoML and AML Designer currently generates 2 different swagger format automatically, and there is no way to configure the output format. We are working on to address this inconsistency, and the Designer swagger format will be the converged format. Cheers!<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.6,
        "Solution_reading_time":4.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":44.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"configure endpoint response"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.2849888889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>How do I get access to the Azure OpenAI service to evaluate it's capabilities?<\/p>",
        "Challenge_closed_time":1664001167767,
        "Challenge_comment_count":1,
        "Challenge_created_time":1663989341807,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information on how to access the Azure OpenAI service to evaluate its capabilities.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1021561\/azure-openai-service-capabilities",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.1,
        "Challenge_reading_time":1.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":3.2849888889,
        "Challenge_title":"Azure OpenAI service capabilities",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":17,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=71c0cf97-895c-43f1-ac54-98e1e9833ae4\">@A-4824  <\/a> Thanks for the question. It is a Limited Access service so you have to apply for it <a href=\"https:\/\/aka.ms\/oai\/access\">https:\/\/aka.ms\/oai\/access<\/a><\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":3.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":22.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"access OpenAI service"
    },
    {
        "Answerer_created_time":1567512169150,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3129.0,
        "Answerer_view_count":781.0,
        "Challenge_adjusted_solved_time":39.4909888889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm attempting to create a kubernetes pod that will run MLflow tracker to store the mlflow artifacts in a designated s3 location. Below is what I'm attempting to deploy with<\/p>\n\n<p>Dockerfile:<\/p>\n\n<pre><code>FROM python:3.7.0\n\nRUN pip install mlflow==1.0.0\nRUN pip install boto3\nRUN pip install awscli --upgrade --user\n\nENV AWS_MLFLOW_BUCKET aws_mlflow_bucket\nENV AWS_ACCESS_KEY_ID aws_access_key_id\nENV AWS_SECRET_ACCESS_KEY aws_secret_access_key\n\nCOPY run.sh \/\n\nENTRYPOINT [\"\/run.sh\"]\n\n# docker build -t seedjeffwan\/mlflow-tracking-server:1.0.0 .\n# 1.0.0 is current mlflow version\n<\/code><\/pre>\n\n<p>run.sh:<\/p>\n\n<pre><code>#!\/bin\/sh\n\nset -e\n\nif [ -z $FILE_DIR ]; then\n  echo &gt;&amp;2 \"FILE_DIR must be set\"\n  exit 1\nfi\n\nif [ -z $AWS_MLFLOW_BUCKET ]; then\n  echo &gt;&amp;2 \"AWS_MLFLOW_BUCKET must be set\"\n  exit 1\nfi\n\nif [ -z $AWS_ACCESS_KEY_ID ]; then\n  echo &gt;&amp;2 \"AWS_ACCESS_KEY_ID must be set\"\n  exit 1\nfi\n\nif [ -z $AWS_SECRET_ACCESS_KEY ]; then\n  echo &gt;&amp;2 \"AWS_SECRET_ACCESS_KEY must be set\"\n  exit 1\nfi\n\nmkdir -p $FILE_DIR &amp;&amp; mlflow server \\\n    --backend-store-uri $FILE_DIR \\\n    --default-artifact-root s3:\/\/${AWS_MLFLOW_BUCKET} \\\n    --host 0.0.0.0 \\\n    --port 5000\n<\/code><\/pre>\n\n<p>mlflow.yaml:<\/p>\n\n<pre><code>---\napiVersion: apps\/v1\nkind: Deployment\nmetadata:\n  name: mlflow-tracking-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: mlflow-tracking-server\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: mlflow-tracking-server\n    spec:\n      containers:\n      - name: mlflow-tracking-server\n        image: seedim\/mlflow-tracker-service:v1\n        ports:\n        - containerPort: 5000\n        env:\n        # FILE_DIR can not be mount dir, MLFLOW need a empty dir but mount dir has lost+found\n        - name: FILE_DIR\n          value: \/mnt\/mlflow\/manifest\n        - name: AWS_MLFLOW_BUCKET\n          value: &lt;aws_s3_bucket&gt;\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: AWS_SECRET_ACCESS_KEY\n        volumeMounts:\n        - mountPath: \/mnt\/mlflow\n          name: mlflow-manifest-storage\n      volumes:\n        - name: mlflow-manifest-storage\n          persistentVolumeClaim:\n            claimName: mlflow-manifest-pvc\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mlflow-tracking-server\n  namespace: default\n  labels:\n    app: mlflow-tracking-server\nspec:\n  ports:\n  - port: 5000\n    protocol: TCP\n  selector:\n    app: mlflow-tracking-server\n\n---\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: mlflow-manifest-pvc\n  namespace: default\nspec:\n  storageClassName: gp2\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 2Gi\n<\/code><\/pre>\n\n<p>I am then building the docker image, saving it to the minikube environment, and then attempting to run the docker image on a kubernetes pod. <\/p>\n\n<p>When I try this, I get CrashLoopBackOff error for the image pod and 'pod has unbound immediate PersistentVolumeClaims' for the pod created with the yaml. <\/p>\n\n<p>I'm attempting to follow the information here (<a href=\"https:\/\/github.com\/aws-samples\/eks-kubeflow-workshop\/blob\/master\/notebooks\/07_Experiment_Tracking\/07_02_MLFlow.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/eks-kubeflow-workshop\/blob\/master\/notebooks\/07_Experiment_Tracking\/07_02_MLFlow.ipynb<\/a>). <\/p>\n\n<p>Is there anything noticeable that I'm doing wrong in this situation?<\/p>\n\n<p>Thank you <\/p>",
        "Challenge_closed_time":1586433992303,
        "Challenge_comment_count":5,
        "Challenge_created_time":1586291824743,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is attempting to create a Kubernetes pod to run MLflow tracker and store artifacts in a designated S3 location. They have created a Dockerfile and a YAML file for deployment, but when attempting to run the pod, they encounter a CrashLoopBackOff error for the image pod and 'pod has unbound immediate PersistentVolumeClaims' for the pod created with the YAML. The user is seeking assistance in identifying any issues with their approach.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61089001",
        "Challenge_link_count":2,
        "Challenge_participation_count":6,
        "Challenge_readability":15.3,
        "Challenge_reading_time":42.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":39.4909888889,
        "Challenge_title":"MLflow Kubernetes Pod Deployment",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":2919.0,
        "Challenge_word_count":346,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1417012835812,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":945.0,
        "Poster_view_count":148.0,
        "Solution_body":"<p>The issue here is related to Persistent Volume Claim that is not provisioned by Your minikube cluster.<\/p>\n\n<p>You will need to make a decision to switch to platform managed kubernetes service or to stick with minikube and manually satisfy the Persistent Volume Claim or \nwith alternative solutions.<\/p>\n\n<p>The simplest option would be to use <a href=\"https:\/\/helm.sh\/\" rel=\"nofollow noreferrer\">helm<\/a> charts for mflow installation like <a href=\"https:\/\/hub.helm.sh\/charts\/cetic\/mlflow\" rel=\"nofollow noreferrer\">this<\/a> or <a href=\"https:\/\/hub.helm.sh\/charts\/larribas\/mlflow\" rel=\"nofollow noreferrer\">this<\/a>.<\/p>\n\n<p>The first helm <a href=\"https:\/\/hub.helm.sh\/charts\/cetic\/mlflow\" rel=\"nofollow noreferrer\">chart<\/a> has listed requirements:<\/p>\n\n<blockquote>\n  <h2>Prerequisites<\/h2>\n  \n  <ul>\n  <li>Kubernetes cluster 1.10+<\/li>\n  <li>Helm 2.8.0+<\/li>\n  <li>PV provisioner support in the underlying infrastructure.<\/li>\n  <\/ul>\n<\/blockquote>\n\n<p>Just like in the guide You followed this one requires PV provisioner support.<\/p>\n\n<p>So by switching to EKS You most likely will have easier time deploying mflow with artifact storing with s3.<\/p>\n\n<p>If You wish to stay on minikube, You will need to modify the helm chart values or the yaml files from the guide You linked to be compatible with You manual configuration of PV. It might also need permissions configuration for s3.<\/p>\n\n<p>The second helm <a href=\"https:\/\/hub.helm.sh\/charts\/larribas\/mlflow\" rel=\"nofollow noreferrer\">chart<\/a> has the following limitation\/feature:<\/p>\n\n<blockquote>\n  <h2>Known limitations of this Chart<\/h2>\n  \n  <p>I've created this Chart to use it in a production-ready environment in my company. We are using MLFlow with a Postgres backend store.<\/p>\n  \n  <p>Therefore, the following capabilities have been left out of the Chart:<\/p>\n  \n  <ul>\n  <li>Using persistent volumes as a backend store.<\/li>\n  <li>Using other database engines like MySQL or SQLServer.<\/li>\n  <\/ul>\n<\/blockquote>\n\n<p>You can try to install it on minikube. This setup would result in artifacts being stored on remote a database. It would still need tweaking in order to connect to s3.<\/p>\n\n<p>Anyway minikube still is a lightweight distribution of kubernetes targeted mainly for learning, so You will eventually reach another limitation if You stick to it for too long.<\/p>\n\n<p>Hope it helps.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":9.4,
        "Solution_reading_time":29.6,
        "Solution_score_count":2.0,
        "Solution_sentence_count":24.0,
        "Solution_word_count":304.0,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"deployment errors"
    },
    {
        "Answerer_created_time":1360013608220,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bolton, United Kingdom",
        "Answerer_reputation_count":65842.0,
        "Answerer_view_count":4569.0,
        "Challenge_adjusted_solved_time":4.4584758333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>After deploying an Azure ML model to a container instance, call to the model fails when using the code provided in the &quot;Consume&quot; section of the endpoint (Python and C#).<\/p>\n<p>I have trained a model in Azure Auto-ML and deployed the model to a container instance.<\/p>\n<p><strong>Now when I am try to use the Python code provided in the Endpoint's &quot;Consume&quot; section I get the following error:<\/strong><\/p>\n<pre><code>The request failed with status code: 502\nContent-Length: 55\nContent-Type: text\/html; charset=utf-8\nDate: Mon, 07 Mar 2022 12:32:07 GMT\nServer: nginx\/1.14.0 (Ubuntu)\nX-Ms-Request-Id: 768c2eb5-10f3-4e8a-9412-3fcfc0f6d648\nX-Ms-Run-Function-Failed: True\nConnection: close\n\n---------------------------------------------------------------------------\nJSONDecodeError Traceback (most recent call last)\n&lt;ipython-input-1-6eeff158e915&gt; in &lt;module&gt;\n48 # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n49 print(error.info())\n---&gt; 50 print(json.loads(error.read().decode(&quot;utf8&quot;, 'ignore')))\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/json\/init.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n352 parse_int is None and parse_float is None and\n353 parse_constant is None and object_pairs_hook is None and not kw):\n--&gt; 354 return _default_decoder.decode(s)\n355 if cls is None:\n356 cls = JSONDecoder\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/json\/decoder.py in decode(self, s, _w)\n337\n338 &quot;&quot;&quot;\n--&gt; 339 obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n340 end = _w(s, end).end()\n341 if end != len(s):\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/json\/decoder.py in raw_decode(self, s, idx)\n355 obj, end = self.scan_once(s, idx)\n356 except StopIteration as err:\n--&gt; 357 raise JSONDecodeError(&quot;Expecting value&quot;, s, err.value) from None\n358 return obj, end\n\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\n<\/code><\/pre>\n<p><strong>If I use C# code provided in the Endpoint's &quot;Consume&quot; section I get the following error:<\/strong><\/p>\n<pre><code>The request failed with status code: BadGateway\nConnection: keep-alive\nX-Ms-Request-Id: 5c3543cf-29ac-46a3-a9fb-dcb6a0041b08\nX-Ms-Run-Function-Failed: True\nDate: Mon, 07 Mar 2022 12:38:32 GMT\nServer: nginx\/1.14.0 (Ubuntu)\n\n'&lt;=' not supported between instances of 'str' and 'int'\n<\/code><\/pre>\n<p><strong>The Python code I am using:<\/strong><\/p>\n<pre><code> import urllib.request\n import json\n import os\n import ssl\n    \n def allowSelfSignedHttps(allowed):\n     # bypass the server certificate verification on client side\n     if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n         ssl._create_default_https_context = ssl._create_unverified_context\n    \n allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n    \n data = {\n     &quot;Inputs&quot;: {\n         &quot;data&quot;:\n         [\n             {\n                 &quot;SaleDate&quot;: &quot;2022-02-08T00:00:00.000Z&quot;,\n                 &quot;OfferingGroupId&quot;: &quot;0&quot;,\n                 &quot;week_of_year&quot;: &quot;7&quot;,\n                 &quot;month_of_year&quot;: &quot;2&quot;,\n                 &quot;day_of_week&quot;: &quot;1&quot;\n             },\n         ]\n     },\n     &quot;GlobalParameters&quot;: {\n         &quot;quantiles&quot;: &quot;0.025,0.975&quot;\n     }\n }\n    \n body = str.encode(json.dumps(data))\n    \n url = 'http:\/\/4a0427c2-30d4-477e-85f5-dfdfdfdfdsfdff623f.uksouth.azurecontainer.io\/score'\n api_key = '' # Replace this with the API key for the web service\n headers = {'Content-Type':'application\/json', 'Authorization':('Bearer '+ api_key)}\n    \n req = urllib.request.Request(url, body, headers)\n    \n try:\n     response = urllib.request.urlopen(req)\n    \n     result = response.read()\n     print(result)\n except urllib.error.HTTPError as error:\n     print(&quot;The request failed with status code: &quot; + str(error.code))\n    \n     # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n     print(error.info())\n     print(json.loads(error.read().decode(&quot;utf8&quot;, 'ignore')))\n<\/code><\/pre>\n<p><strong>The C# code I have tried<\/strong>:<\/p>\n<pre><code> using System;\n using System.Collections.Generic;\n using System.IO;\n using System.Net.Http;\n using System.Net.Http.Headers;\n using System.Text;\n using System.Threading.Tasks;\n using Newtonsoft.Json;\n    \n namespace MLModelAPICall\n {\n     class Program\n     {\n         static void Main(string[] args)\n         {\n             InvokeRequestResponseService().Wait();\n         }\n    \n         static async Task InvokeRequestResponseService()\n         {\n             var handler = new HttpClientHandler()\n             {\n                 ClientCertificateOptions = ClientCertificateOption.Manual,\n                 ServerCertificateCustomValidationCallback =\n                         (httpRequestMessage, cert, cetChain, policyErrors) =&gt; { return true; }\n             };\n             using (var client = new HttpClient(handler))\n             {\n                 \/\/ Request data goes here\n                 var scoreRequest = new\n                 {\n                     Inputs = new Dictionary&lt;string, List&lt;Dictionary&lt;string, string&gt;&gt;&gt;()\n                     {\n                         {\n                             &quot;data&quot;,\n                             new List&lt;Dictionary&lt;string, string&gt;&gt;()\n                             {\n                                 new Dictionary&lt;string, string&gt;()\n                                 {\n                                     {\n                                         &quot;SaleDate&quot;, &quot;2022-02-08T00:00:00.000Z&quot;\n                                     },\n                                     {\n                                         &quot;OfferingGroupId&quot;, &quot;0&quot;\n                                     },\n                                     {\n                                         &quot;week_of_year&quot;, &quot;7&quot;\n                                     },\n                                     {\n                                         &quot;month_of_year&quot;, &quot;2&quot;\n                                     },\n                                     {\n                                         &quot;day_of_week&quot;, &quot;1&quot;\n                                     }\n                                 }\n                             }\n                         }\n                     },\n                     GlobalParameters = new Dictionary&lt;string, string&gt;()\n                     {\n                         {\n                             &quot;quantiles&quot;, &quot;0.025,0.975&quot;\n                         }\n                     }\n                 };\n    \n    \n                 const string apiKey = &quot;&quot;; \/\/ Replace this with the API key for the web service\n                 client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(&quot;Bearer&quot;, apiKey);\n                 client.BaseAddress = new Uri(&quot;http:\/\/4a0427c2-30d4-477e-85f5-xxxxxxxxxxxxx.uksouth.azurecontainer.io\/score&quot;);\n    \n                 \/\/ WARNING: The 'await' statement below can result in a deadlock\n                 \/\/ if you are calling this code from the UI thread of an ASP.Net application.\n                 \/\/ One way to address this would be to call ConfigureAwait(false)\n                 \/\/ so that the execution does not attempt to resume on the original context.\n                 \/\/ For instance, replace code such as:\n                 \/\/      result = await DoSomeTask()\n                 \/\/ with the following:\n                 \/\/      result = await DoSomeTask().ConfigureAwait(false)\n    \n                 var requestString = JsonConvert.SerializeObject(scoreRequest);\n                 var content = new StringContent(requestString);\n    \n                 content.Headers.ContentType = new MediaTypeHeaderValue(&quot;application\/json&quot;);\n    \n                 HttpResponseMessage response = await client.PostAsync(&quot;&quot;, content);\n    \n                 if (response.IsSuccessStatusCode)\n                 {\n                     string result = await response.Content.ReadAsStringAsync();\n                     Console.WriteLine(&quot;Result: {0}&quot;, result);\n                 }\n                 else\n                 {\n                     Console.WriteLine(string.Format(&quot;The request failed with status code: {0}&quot;, response.StatusCode));\n    \n                     \/\/ Print the headers - they include the requert ID and the timestamp,\n                     \/\/ which are useful for debugging the failure\n                     Console.WriteLine(response.Headers.ToString());\n    \n                     string responseContent = await response.Content.ReadAsStringAsync();\n                     Console.WriteLine(responseContent);\n                     Console.ReadLine();\n                 }\n             }\n         }\n     }\n }\n<\/code><\/pre>\n<p>Could you please help me with this issue? I am not sure what do to if Microsoft's provided code is erroring out, don't know what else to do.<\/p>",
        "Challenge_closed_time":1646835298180,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646819247667,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error when trying to call an Azure ML model deployed to a container instance using the Python and C# code provided in the \"Consume\" section of the endpoint. The Python code returns a 502 error with a JSONDecodeError, while the C# code returns a BadGateway error with a message indicating a problem with the code. The user is seeking assistance in resolving the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71407308",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":95.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":58,
        "Challenge_solved_time":4.4584758333,
        "Challenge_title":"Azure ML model to a container instance, call to the model fails when using the code provided in the \"Consume\" section of the endpoint (Python and C#)",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":89.0,
        "Challenge_word_count":697,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1360013608220,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bolton, United Kingdom",
        "Poster_reputation_count":65842.0,
        "Poster_view_count":4569.0,
        "Solution_body":"<p>After much more digging I found out that the &quot;Consume&quot; scripts provided with the endpoint are wrong (Python and C#) .<\/p>\n<p>When making a call to the endpoint the GlobalParameters expects an integer value, but the provided scripts have wrapped the values in double quotes hence making it a string:<\/p>\n<pre><code> },\n &quot;GlobalParameters&quot;: {\n     &quot;quantiles&quot;: &quot;0.025,0.975&quot;\n }\n<\/code><\/pre>\n<p>If you are using Python to consume the model, when making call to the endpoint your GlobalParameters should be define as this:<\/p>\n<pre><code> },\n &quot;GlobalParameters&quot;: {\n     &quot;quantiles&quot;: [0.025,0.975]\n }\n<\/code><\/pre>\n<p>wrapped in square brackets<\/p>\n<blockquote>\n<p>[0.025,0.975]<\/p>\n<\/blockquote>\n<p>and not in double quotes &quot;<\/p>\n<blockquote>\n<p><em>I have also opened a ticket with microsoft so hopefully they will fix the code provided in the &quot;consume&quot; section of every endpoint<\/em><\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.8,
        "Solution_reading_time":12.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":120.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"error calling deployed model"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":93.4837152778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>So, I created an experiment, based on one of many examples and picked a dataset that required some transformations, columns choosing and categorical features. The model creation worked just fine, with only smaller hiccups. However as I deployed the webservice, the API is requesting the new column data set (created after the feature transformation) and not the original data set. This does not serve my purpose, as my aim for creating a service that would adhere to the original dataset features.  <\/p>\n<p>What am I doing wrong? Or shall I be required to implement the data transformation. Also, the feature I am trying to predict is also showing up as a input feature, and that does not make sense.  <\/p>\n<p>Any pointers?  <\/p>",
        "Challenge_closed_time":1602859603928,
        "Challenge_comment_count":4,
        "Challenge_created_time":1602523062553,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue while deploying an Azure ML classic webservice. The API is requesting the new column data set instead of the original data set, which does not serve the user's purpose. The user is unsure if they are doing something wrong or if they need to implement data transformation. Additionally, the feature they are trying to predict is showing up as an input feature, which does not make sense. The user is seeking pointers to resolve these issues.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/123854\/azure-ml-classic-webservices-deploy-error-when-doi",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":9.0,
        "Challenge_reading_time":9.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":93.4837152778,
        "Challenge_title":"Azure ML classic webservices deploy - Error When Doing Test Request-Response",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":132,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>After further investigation, it has been determined that by design, studio (classic) will echo this error when using convert to indicator values transformation, hence, we suggest not using convert to indicator values block in the inference pipeline. However, in <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/convert-to-indicator-values\">designer<\/a>, indicator values transformation is saved so that this module can be used in the inference pipeline. Sorry for the inconvenience, but hope this helps!    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.6,
        "Solution_reading_time":7.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"incorrect API request"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":279.3938888889,
        "Challenge_answer_count":2,
        "Challenge_body":"The parameters of a big neural network model can be huge. But the largest storage size of a host instance is only 30G, according to https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/host-instance-storage.html. Is there a way to increase the storage volume? I have a model (embeddings) that is very close to 30G and caused a no space error when deploying.   \n  \nThanks!",
        "Challenge_closed_time":1577746997000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1576741179000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge with the storage capacity of a host instance while deploying a big neural network model. The largest storage size of a host instance is only 30G, which is not sufficient for the user's model that is very close to 30G and caused a no space error. The user is seeking a way to increase the storage volume.",
        "Challenge_last_edit_time":1668610792135,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUeVh4VvD6R-eIhRYY6K8dsw\/how-to-increase-the-storage-of-host-instance",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":5.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":279.3938888889,
        "Challenge_title":"how to increase the storage of host instance",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":769.0,
        "Challenge_word_count":62,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The disk size is currently not configurable for SageMaker Endpoints with EBS backed volumes. As a workaround, please use instances with ephemeral storage for your SageMaker endpoint.  \n  \nExample instance types with ephemeral storage:  \n- m5d instances: https:\/\/aws.amazon.com\/ec2\/instance-types\/m5\/  \n- c5d instances: https:\/\/aws.amazon.com\/ec2\/instance-types\/c5\/  \n- r5d instances: https:\/\/aws.amazon.com\/ec2\/instance-types\/r5\/  \n  \nThe full list of Amazon SageMaker instance types can be accessed here: https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1577746997000,
        "Solution_link_count":4.0,
        "Solution_readability":12.8,
        "Solution_reading_time":7.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"insufficient storage capacity"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.2522222222,
        "Challenge_answer_count":0,
        "Challenge_body":"<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\nIf you run \r\n\r\npy_test(\r\n name = \"sigopt_prior_beliefs_example\",\r\n size = \"medium\",\r\n srcs = [\"examples\/sigopt_prior_beliefs_example.py\"],\r\n deps = [\":tune_lib\"],\r\n tags = [\"exclusive\", \"example\"],\r\n args = [\"--smoke-test\"]\r\n)\r\n\r\nin python\/ray\/tune\/build (this part of the testing is commented out since you need a sigopt API key...)\r\nYou get an output that looks like this:\r\n\r\n\"\"\"\r\n...\r\n  File \"\/usr\/local\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 737, in _process_trial\r\n    self._validate_result_metrics(result)\r\n  File \"\/usr\/local\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 818, in _validate_result_metrics\r\n    elif search_metric and search_metric not in result:\r\nTypeError: unhashable type: 'list'\r\n...\r\n\"\"\"\r\nray 1.1.0.dev\r\n\r\n### Reproduction (REQUIRED)\r\nin python\/ray\/tune\/build  run the sigopt sections that are commented out.\r\n",
        "Challenge_closed_time":1603498330000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1603479422000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered an issue with Vertex AI endpoint call with JSON, where they are unable to generate the JSON payload for the input of the neural network. They have tried different methods, but all have resulted in an error 400, indicating an invalid JSON payload. The user is unsure if the array needs to be in b64 format and is seeking help to fix\/encode the payload.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ray-project\/ray\/issues\/11581",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.8,
        "Challenge_reading_time":12.71,
        "Challenge_repo_contributor_count":422.0,
        "Challenge_repo_fork_count":4491.0,
        "Challenge_repo_issue_count":35793.0,
        "Challenge_repo_star_count":25903.0,
        "Challenge_repo_watch_count":446.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":5.2522222222,
        "Challenge_title":"[Tune] Sigopt (multi-metric) api fails with 1.1.0 (tries to hash list)",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":102,
        "Discussion_body":"This is a consequence of search metric being able to be multi-metric. cc @krfricke \r\n\r\nAlso, let me ping the sigopt folks for a working API key... Should be fixed on #11583 . We'll pick this onto the release.",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"SigOpt",
        "Challenge_type":"anomaly",
        "Challenge_summary":"invalid JSON payload"
    },
    {
        "Answerer_created_time":1342685175156,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":12103.0,
        "Answerer_view_count":1451.0,
        "Challenge_adjusted_solved_time":1.8368638889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When we deploy a model as an ACIWebService in Azure Machine Learning Service, we do not need to specify any <code>deployment_target<\/code>.<\/p>\n<p>According to the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#deploy-workspace--name--models--inference-config-none--deployment-config-none--deployment-target-none--overwrite-false-\" rel=\"nofollow noreferrer\">AzureML documentation<\/a> for <code>azureml.core.model.model<\/code> class,<\/p>\n<pre><code>deployment_target\nComputeTarget\ndefault value: None\nA ComputeTarget to deploy the Webservice to. As Azure Container Instances has no associated ComputeTarget, leave this parameter as None to deploy to Azure Container Instances.\n<\/code><\/pre>\n<p>What does Microsoft mean by<\/p>\n<blockquote>\n<p>As Azure Container Instances has no associated ComputeTarget<\/p>\n<\/blockquote>\n<p>In which &quot;Compute Target&quot; is an ACIWebService deployed?<\/p>",
        "Challenge_closed_time":1610958655960,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610952043250,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is confused about where the Azure Machine ACI Webservice deploys and what is meant by \"Compute Target\" in the context of deploying an ACIWebService. The AzureML documentation states that as Azure Container Instances has no associated ComputeTarget, the deployment_target parameter should be left as None to deploy to Azure Container Instances.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65769868",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.0,
        "Challenge_reading_time":13.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.8368638889,
        "Challenge_title":"Where does the Azure Machine ACI Webservice deploy?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":317.0,
        "Challenge_word_count":94,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/container-instances\/container-instances-overview\" rel=\"nofollow noreferrer\">Azure Container Instances<\/a> itself is the compute platform. It spins up a container in a serverless-fashion.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.6,
        "Solution_reading_time":3.28,
        "Solution_score_count":3.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"ACI deployment target"
    },
    {
        "Answerer_created_time":1559910246180,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bengaluru, Karnataka, India",
        "Answerer_reputation_count":2046.0,
        "Answerer_view_count":369.0,
        "Challenge_adjusted_solved_time":1095.2252230556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>based on the aws documentation, maximum timeout limit is less that 30 seconds in api gateway.so hooking up an sagemaker endpoint with api gateway wouldn't make sense, if the request\/response is going to take more than 30 seconds. is there any workaround ? adding a lambda in between api gateway and sagemaker endpoint is going to add more time to process request\/response, which i would like to avoid. also, there will be added time for lambda cold starts and sagemaker serverless endpoints are built on top of lambda so that will also add cold start time. is there a way to invoke the serverless sagemaker endpoints , without these overhead?<\/p>",
        "Challenge_closed_time":1645795642143,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645755688313,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in accessing\/invoking a sagemaker endpoint without using lambda due to the maximum timeout limit of less than 30 seconds in API Gateway. Adding a lambda in between would add more time to process request\/response and also add cold start time. The user is looking for a workaround to invoke the serverless sagemaker endpoints without these overheads.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71260306",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":8.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":11.0982861111,
        "Challenge_title":"How to access\/invoke a sagemaker endpoint without lambda?",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1122.0,
        "Challenge_word_count":115,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1590797441983,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":525.0,
        "Poster_view_count":98.0,
        "Solution_body":"<p>It is indeed possible to invoke sagemaker endpoints from sagemaker without using any other AWS services and that is also manifested by the fact that they have invocation URLs.<\/p>\n<p>Here's how you set it up:<\/p>\n<ol>\n<li>create an user with only programmatic access and attach a policy json that should look something like below:<\/li>\n<\/ol>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;VisualEditor0&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:sagemaker:&lt;region&gt;:&lt;account-id&gt;:endpoint\/&lt;endpoint-name&gt;&quot;\n        }\n    ]\n} \n<\/code><\/pre>\n<p>you can replace <code>&lt;endpoint-name&gt;<\/code> with <code>*<\/code> to let this user invoke all endpoints.<\/p>\n<ol start=\"2\">\n<li><p>use the ACCESS-KEY and SECRET-ACCESS-KEY to configure authorisation in postman like shown in this screenshot. also add the parameters in advanced tab like shown in the screenshot.\n<a href=\"https:\/\/i.stack.imgur.com\/cYkTf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/cYkTf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>then fill up your body with the relevant content type.<\/p>\n<\/li>\n<li><p>then add or remove additional headers like variant-name or model-name, if you have them set up and the headers should look like shown in this screenshot: <a href=\"https:\/\/i.stack.imgur.com\/NLqkV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NLqkV.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>send the request to receive reponse like this\n<a href=\"https:\/\/i.stack.imgur.com\/uA4kF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uA4kF.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<\/ol>\n<p><em><strong>URL and credentials in the above screenshots doesn't work anymore, duh!<\/strong><\/em><\/p>\n<p>and if you want code to invoke the endpoint directly using some back-end language, <a href=\"https:\/\/stackoverflow.com\/a\/70803026\/11814996\">here's code for python<\/a>.<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1649698499116,
        "Solution_link_count":7.0,
        "Solution_readability":13.4,
        "Solution_reading_time":27.72,
        "Solution_score_count":3.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":219.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"maximum timeout limit"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.5253416667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Anaconda announced that commercial users should purchase the licenses on APR, 20, 2020.  <br \/>\nHowever, Azure Machine Learning heavily depends on this anaconda packages; developing models on computing instance and deploy container environment.  <\/p>\n<p>Do commercial developers have to pay to anaconda to continue usage of Azure Machine Learning with anaconda?  <\/p>",
        "Challenge_closed_time":1605602646227,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605597154997,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is questioning whether commercial developers need to purchase licenses from Anaconda to continue using Azure Machine Learning, which heavily relies on Anaconda packages for developing and deploying models. Anaconda announced that commercial users should purchase licenses on April 20, 2020.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/165312\/anaconda-commercial-use-on-azure-machine-learning",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":5.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.5253416667,
        "Challenge_title":"Anaconda commercial use on Azure Machine Learning",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":58,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=55ef7a9f-c4d6-4e64-b536-02ebed405538\">@Eisuke Yonezawa  <\/a> If you are have the commercial version of Anaconda you can configure the same for your experiments to deploy packages that are available under license but in most cases you can simply use conda to install available python packages without paying for the commercial license. There is no restriction to have a commercial license to continue using Azure Machine Learning with anaconda. I hope this helps!!<\/p>\n",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.4,
        "Solution_reading_time":6.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":70.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"Anaconda license for commercial use"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":51.6131461111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to use Azure ML to host an image classification model trained in lobe.ai (externally trained model).     <\/p>\n<p>I've used the 'no code' model deployment approach described <a href=\"https:\/\/learn.microsoft.com\/en-gb\/azure\/machine-learning\/how-to-deploy-no-code-deployment\">here<\/a>     <\/p>\n<p>I've been able to authenticate my workspace and register my TensorFlow model, but the endpoint is stuck on transitioning for over 2 hours.     <\/p>\n<p>Any ideas?    <\/p>\n<pre><code>from azureml.core import Model  \n  \nmodel = Model.register(workspace=ws,  \n                       model_name='cxr',                            # Name of the registered model in your workspace.  \n                       model_path='cxr_test',                       # Local Tensorflow SavedModel folder to upload and register as a model.  \n                       model_framework=Model.Framework.TENSORFLOW,  # Framework used to create the model.  \n                       model_framework_version='1.15.3',            # Version of Tensorflow used to create the model.  \n                       description='Pneumonia-prediction model')  \n  \nservice_name = 'tensorflow-cxr-service'  \nservice = Model.deploy(ws, service_name, [model])  \n<\/code><\/pre>",
        "Challenge_closed_time":1605125180896,
        "Challenge_comment_count":5,
        "Challenge_created_time":1604939373570,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue with Azure ML where the endpoint for their image classification model is stuck in a 'transitioning' state for over 2 hours. They have used the 'no code' model deployment approach and have successfully authenticated their workspace and registered their TensorFlow model.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/156439\/endpoint-stuck-in-transitioning-state",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":11.2,
        "Challenge_reading_time":13.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":51.6131461111,
        "Challenge_title":"Endpoint stuck in 'transitioning' state",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":114,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>We have created a support ticket for this issue and we will update the solution later. Thanks.  <\/p>\n<p>Regards,  <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.2,
        "Solution_reading_time":1.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":21.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"endpoint stuck in transitioning state"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1513.4666666667,
        "Challenge_answer_count":3,
        "Challenge_body":"Hello everyone\n\nWe use dialogflow cx with the cx phone gateway. But because we aren't based in the US we used a local phone number which we forward to the US-number from the gateway.\n\nThis worked great for some days, but now we are experiencing the following issue:\nThe caller_id from the original caller does not get sent in the webhook response anymore. It only sends the number which forwards the call.\n\nDoes someone know how to prevent this from happening?\n\nThanks for your help!\n\n- Federico",
        "Challenge_closed_time":1670480040000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1665031560000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the CX Phone gateway where the caller_id from the original caller is not being sent in the webhook response when forwarding to a US-number. The user is seeking help to prevent this from happening.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/CX-Phone-gateway-caller-id-lost-when-forwarding-to-US-number\/m-p\/475112#M624",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":6.2,
        "Challenge_reading_time":6.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1513.4666666667,
        "Challenge_title":"CX Phone gateway caller_id lost when forwarding to US-number",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":131.0,
        "Challenge_word_count":94,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"looks like this is a problem with the local network\/phone provider, which does not transfer the caller_id correctly.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":1.84,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":23.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing caller_id"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":572.4141666667,
        "Challenge_answer_count":0,
        "Challenge_body":"In particular:\r\n- Experiments needs to be renamed to Jobs\r\n- Datasets needs to be renamed to Data\r\n\r\nFurther changes probably aren't absolutely necessary right now, but should be considered as well. See #616.",
        "Challenge_closed_time":1663956142000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661895451000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The AzureML extension in VS Code (Insiders) prompts users to login twice when the user is signed out and reloads VS Code. This behavior is disruptive and unnecessary, and no other extensions for AWS or Azure do this.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1691",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.1,
        "Challenge_reading_time":3.19,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":100.0,
        "Challenge_repo_issue_count":2059.0,
        "Challenge_repo_star_count":290.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":572.4141666667,
        "Challenge_title":"Update Treeview asset labels to match Azure ML Studio.",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":40,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"disruptive login prompt"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":14.5543175,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to invoke a MultiModel Endpoint with a RandomCutForest Model. I receive error though, 'Error loading model'. I can invoke the endpoint with models given from the examples.\nAm I missing something e.g. limitations on what models I can use? <\/p>\n\n<p>For MultiModel inspiration I am using below:<\/p>\n\n<blockquote>\n  <p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb<\/a><\/p>\n  \n  <p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/<\/a><\/p>\n<\/blockquote>\n\n<p>I am trying to deploy the outputted 'model.tar.gz' from below RCF example in the MultiModel endpoint:<\/p>\n\n<blockquote>\n  <p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb<\/a><\/p>\n<\/blockquote>\n\n<pre><code>model_name = 'model'\nfull_model_name = '{}.tar.gz'.format(model_name)\nfeatures = data\n\nbody = ','.join(map(str, features)) + '\\n'\nresponse = runtime_sm_client.invoke_endpoint(\n                    EndpointName=endpoint_name,\n                    ContentType='text\/csv',\n                    TargetModel=full_model_name,\n                    Body=body)\nprint(response)\n<\/code><\/pre>\n\n<p><strong>Cloudwatch log Error:<\/strong><\/p>\n\n<pre><code>&gt; 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Error loading model: Unable\n&gt; to load model: invalid load key, '{'. [17:28:59]\n&gt; \/workspace\/src\/learner.cc:334: Check failed: fi-&gt;Read(&amp;mparam_,\n&gt; sizeof(mparam_)) == sizeof(mparam_) (25 vs. 136) : BoostLearner: wrong\n&gt; model format 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Stack trace: 2020-04-27\n&gt; 17:28:59,005 [INFO ] W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (0)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x24)\n&gt; [0x7f37ce1cacb4] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9 com.amazonaws.ml.mms.wlm.WorkerThread\n&gt; - Backend response time: 0 2020-04-27 17:28:59,005 [INFO ] W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (1)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(xgboost::LearnerImpl::Load(dmlc::Stream*)+0x4b5)\n&gt; [0x7f37ce266985] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (2)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(XGBoosterLoadModel+0x37)\n&gt; [0x7f37ce1bf417] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (3)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c)\n&gt; [0x7f37ee993ec0] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (4)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d)\n&gt; [0x7f37ee99387d] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (5)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce)\n&gt; [0x7f37eeba91de] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (6)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12c14)\n&gt; [0x7f37eeba9c14] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (7)\n&gt; \/miniconda3\/bin\/python(_PyObject_FastCallKeywords+0x48b)\n&gt; [0x563d71b4218b] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (8)\n&gt; \/miniconda3\/bin\/python(_PyEval_EvalFrameDefault+0x52cf)\n&gt; [0x563d71b91e8f] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  2020-04-27 17:28:59,005\n&gt; [WARN ] W-9003-b39b888fb4a3fa6cf83bb34a9\n&gt; com.amazonaws.ml.mms.wlm.WorkerThread - Backend worker thread\n&gt; exception. java.lang.IllegalArgumentException: reasonPhrase contains\n&gt; one of the following prohibited characters: \\r\\n: Unable to load\n&gt; model: Unable to load model: invalid load key, '{'. [17:28:59]\n&gt; \/workspace\/src\/learner.cc:334: Check failed: fi-&gt;Read(&amp;mparam_,\n&gt; sizeof(mparam_)) == sizeof(mparam_) (25 vs. 136) : BoostLearner: wrong\n&gt; model format Stack trace:   [bt] (0)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x24)\n&gt; [0x7f37ce1cacb4]   [bt] (1)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(xgboost::LearnerImpl::Load(dmlc::Stream*)+0x4b5)\n&gt; [0x7f37ce266985]   [bt] (2)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(XGBoosterLoadModel+0x37)\n&gt; [0x7f37ce1bf417]   [bt] (3)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c)\n&gt; [0x7f37ee993ec0]   [bt] (4)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d)\n&gt; [0x7f37ee99387d]   [bt] (5)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce)\n&gt; [0x7f37eeba91de]   [bt] (6)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12c14)\n&gt; [0x7f37eeba9c14]   [bt] (7)\n&gt; \/miniconda3\/bin\/python(_PyObject_FastCallKeywords+0x48b)\n&gt; [0x563d71b4218b]   [bt] (8)\n&gt; \/miniconda3\/bin\/python(_PyEval_EvalFrameDefault+0x52cf)\n&gt; [0x563d71b91e8f]\n<\/code><\/pre>",
        "Challenge_closed_time":1588061581360,
        "Challenge_comment_count":0,
        "Challenge_created_time":1588009185817,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to invoke a MultiModel Endpoint with a RandomCutForest Model. The error message says 'Error loading model'. The user is trying to deploy the outputted 'model.tar.gz' from the RCF example in the MultiModel endpoint. The Cloudwatch log shows a stack trace with an error message \"Unable to load model: invalid load key, '{'\". The user is seeking help to understand if there are any limitations on what models can be used in MultiModel Endpoint.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61464960",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":22.8,
        "Challenge_reading_time":86.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":52,
        "Challenge_solved_time":14.5543175,
        "Challenge_title":"SageMaker multimodel and RandomCutForest",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":324.0,
        "Challenge_word_count":394,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1426492930156,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":398.0,
        "Poster_view_count":84.0,
        "Solution_body":"<p>SageMaker Random Cut Forest is part of the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">built-in algorithm library<\/a> and cannot be deployed in multi-model endpoint (MME). Built-in algorithms currently cannot be deployed to MME. XGboost is an exception, since it has an open-source container <a href=\"https:\/\/github.com\/aws\/sagemaker-xgboost-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-xgboost-container<\/a>.<\/p>\n\n<p>If you really need to deploy a RCF to a multi-model endpoint, one option is to find a reasonably similar open-source implementation (for example <a href=\"https:\/\/github.com\/kLabUM\/rrcf\" rel=\"nofollow noreferrer\"><code>rrcf<\/code><\/a> looks reasonably serious: based <a href=\"http:\/\/proceedings.mlr.press\/v48\/guha16.pdf\" rel=\"nofollow noreferrer\">on the same paper from Guha et al<\/a> and with 170+ github stars) and create a custom MME docker container. The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html\" rel=\"nofollow noreferrer\">documentation is here<\/a> and there is <a href=\"https:\/\/github.com\/giuseppeporcelli\/sagemaker-custom-serving-containers\/blob\/master\/multi-model-server-container\/notebook\/multi-model-server-container.ipynb\" rel=\"nofollow noreferrer\">an excellent tuto here<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":19.2,
        "Solution_reading_time":17.98,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":116.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"error loading model"
    },
    {
        "Answerer_created_time":1285219808283,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Perth WA, Australia",
        "Answerer_reputation_count":6770.0,
        "Answerer_view_count":1127.0,
        "Challenge_adjusted_solved_time":0.0213536111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm currently trying to integrate an ML model currently deployed as a webservice on AzureML with PowerBI.<\/p>\n<p>I see that it can be <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#invoking-the-azure-ml-model-in-power-bi\" rel=\"nofollow noreferrer\">integrated<\/a> but the model requires the addition of a schema file when it is <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#schema-discovery-for-machine-learning-models\" rel=\"nofollow noreferrer\">being deployed as a webservice<\/a>. Without this, the model can't be viewed in PowerBI.<\/p>\n<p>The problem that I have come up against is that I use MLflow to log ML model performances and subsequently to deploy a selected model onto AzureML as a webservice using MLflow's AzureML integration - mlflow.azureml.deploy(). This unfortunately doesn't have the option to define a schema file before the model is deployed, thus resulting in no model being available in PowerBI as it lacks the required schema file.<\/p>\n<p>My options seem to be:<\/p>\n<ol>\n<li>Find a workaround, possibly using the working <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/mlflow\/model-serving\" rel=\"nofollow noreferrer\">REST api of the model in a power query<\/a>.<\/li>\n<li>Rewrite the deployment code and handle the webservice deployment steps in Azure instead of MLflow.<\/li>\n<\/ol>\n<p>I thought I would ask to see if I am maybe missing something as I can't find a workaround using my current code to define a schema file in MLflow when deploying with mlflow.azureml.deploy().<\/p>",
        "Challenge_closed_time":1600604920243,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600261190477,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to integrate an ML model deployed as a webservice on AzureML with PowerBI, but the model requires a schema file to be viewed in PowerBI. The user uses MLflow to deploy the model onto AzureML, but MLflow's AzureML integration does not have the option to define a schema file before deployment, resulting in no model being available in PowerBI. The user is considering finding a workaround using the REST API or rewriting the deployment code to handle the webservice deployment steps in Azure instead of MLflow.",
        "Challenge_last_edit_time":1600855880503,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63920599",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":21.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":95.4804905556,
        "Challenge_title":"PowerBI and MLflow integration (through AzureML)",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":405.0,
        "Challenge_word_count":204,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1600260166047,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Point number 2 is the way we solved this issue. Instead of using MLflow to deploy to a scoring service on Azure, we wrote a custom code which load MLflow model when container is initialised.<\/p>\n<p>Scoring code is something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nfrom mlflow.pyfunc import load_model\n\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n\ndef init():\n    global model\n    model = load_model(os.path.join(os.environ.get(&quot;AZUREML_MODEL_DIR&quot;), &quot;awesome_model&quot;))\n\n@input_schema('data', NumpyParameterType(input_sample))\n@output_schema(NumpyParameterType(output_sample))\n\ndef run(data):\n    return model.predict(data)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1600855957376,
        "Solution_link_count":0.0,
        "Solution_readability":16.9,
        "Solution_reading_time":10.68,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":75.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"no schema file available"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.1845747222,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\nCan I use serverless inference as a pricing model for selling a SageMaker Model Package on the AWS Marketplace?",
        "Challenge_closed_time":1673186904128,
        "Challenge_comment_count":0,
        "Challenge_created_time":1673171839659,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of using serverless inference as a pricing model for selling a SageMaker Model Package on the AWS Marketplace.",
        "Challenge_last_edit_time":1673517930936,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUcZwxBuy-SROI7OF3-NFslA\/sagemaker-serverless-on-aws-marketplace",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.3,
        "Challenge_reading_time":1.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":4.1845747222,
        "Challenge_title":"SageMaker Serverless on AWS Marketplace?",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":58.0,
        "Challenge_word_count":24,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"No, quoting from SageMaker documentation:\n\n\n```\n\u2026features currently available for SageMaker Real-time Inference are not supported for Serverless Inference, including GPUs, AWS marketplace model packages\u2026\n```\n\nReference: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints.html#serverless-endpoints-how-it-works-exclusions",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1673186904128,
        "Solution_link_count":1.0,
        "Solution_readability":28.8,
        "Solution_reading_time":4.52,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"serverless inference pricing"
    },
    {
        "Answerer_created_time":1403541426412,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bengaluru, India",
        "Answerer_reputation_count":191.0,
        "Answerer_view_count":22.0,
        "Challenge_adjusted_solved_time":2355.5938513889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created the Azure ML experiment with R script module \nit works fine while we run the experiment but\n when we publish the web service it throws error http 500 \n ( I believe the error is causing in the R script module because other modules are running fine in web service but i can't debug the problem<\/p>\n\n<blockquote>\n  <p>Http status code: 500, Timestamp: Fri, 08 May 2015 04:23:14 GMT<\/p>\n<\/blockquote>\n\n<p>Also is there any limitation in r e.g. some function which wont work in web service<\/p>",
        "Challenge_closed_time":1439539745912,
        "Challenge_comment_count":0,
        "Challenge_created_time":1431059608047,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has created an Azure Machine Learning experiment with an R script module that works fine when run, but encounters an HTTP 500 error when publishing the web service. The user suspects that the error is caused by the R script module, but is unable to debug the problem. The user also asks if there are any limitations in R that may prevent certain functions from working in the web service.",
        "Challenge_last_edit_time":1446192965568,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/30115812",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.7,
        "Challenge_reading_time":7.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":2355.5938513889,
        "Challenge_title":"Error while running Azure Machine Learning web service but the experiment works fine",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":415.0,
        "Challenge_word_count":98,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403541426412,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, India",
        "Poster_reputation_count":191.0,
        "Poster_view_count":22.0,
        "Solution_body":"<p>I found the problem. I was facing this error because the R module in the Azure ML was was taking variable as the other data type and not producing any outputs results which i was checking through for loop which is why i was getting this error.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.9,
        "Solution_reading_time":3.0,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":47.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"HTTP 500 error"
    },
    {
        "Answerer_created_time":1485822323507,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New Delhi, Delhi, India",
        "Answerer_reputation_count":499.0,
        "Answerer_view_count":48.0,
        "Challenge_adjusted_solved_time":185.9970769444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy a model using my own custom inference container on sagemaker. I am following the documentation here <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/adapt-inference-container.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/adapt-inference-container.html<\/a><\/p>\n<p>I have an entrypoint file:<\/p>\n<pre><code>from sagemaker_inference import model_server\n#HANDLER_SERVICE = &quot;\/home\/model-server\/model_handler.py:handle&quot;\nHANDLER_SERVICE = &quot;model_handler.py&quot;\nmodel_server.start_model_server(handler_service=HANDLER_SERVICE)\n<\/code><\/pre>\n<p>I have a model_handler.py file:<\/p>\n<pre><code>from sagemaker_inference.default_handler_service import DefaultHandlerService\nfrom sagemaker_inference.transformer import Transformer\nfrom CustomHandler import CustomHandler\n\n\nclass ModelHandler(DefaultHandlerService):\n    def __init__(self):\n        transformer = Transformer(default_inference_handler=CustomHandler())\n        super(HandlerService, self).__init__(transformer=transformer)\n<\/code><\/pre>\n<p>And I have my CustomHandler.py file:<\/p>\n<pre><code>import os\nimport json\nimport pandas as pd\nfrom joblib import dump, load\nfrom sagemaker_inference import default_inference_handler, decoder, encoder, errors, utils, content_types\n\n\nclass CustomHandler(default_inference_handler.DefaultInferenceHandler):\n\n    def model_fn(self, model_dir: str) -&gt; str:\n        clf = load(os.path.join(model_dir, &quot;model.joblib&quot;))\n        return clf\n\n    def input_fn(self, request_body: str, content_type: str) -&gt; pd.DataFrame:\n        if content_type == &quot;application\/json&quot;:\n            items = json.loads(request_body)\n\n            for item in items:\n                processed_item1 = process_item1(items[&quot;item1&quot;])\n                processed_item2 = process_item2(items[&quot;item2])\n                all_item1 += [processed_item1]\n                all_item2 += [processed_item2]\n            return pd.DataFrame({&quot;item1&quot;: all_item1, &quot;comments&quot;: all_item2})\n\n    def predict_fn(self, input_data, model):\n        return model.predict(input_data)\n<\/code><\/pre>\n<p>Once I deploy the model to an endpoint with these files in the image, I get the following error: <code>ml.mms.wlm.WorkerLifeCycle - ModuleNotFoundError: No module named 'model_handler'<\/code>.<\/p>\n<p>I am really stuck what to do here. I wish there was an example of how to do this in the above way end to end but I don't think there is. Thanks!<\/p>",
        "Challenge_closed_time":1636628150220,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635955837330,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy a model using a custom inference container on SageMaker, following the documentation provided. They have created an entrypoint file, a model_handler.py file, and a CustomHandler.py file. However, when they deploy the model to an endpoint with these files in the image, they receive a \"ModuleNotFoundError: No module named 'model_handler'\" error. The user is unsure of what to do next and is seeking help.",
        "Challenge_last_edit_time":1635958560743,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69828187",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.6,
        "Challenge_reading_time":32.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":186.7535805556,
        "Challenge_title":"sagemaker inference container ModuleNotFoundError: No module named 'model_handler'",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":728.0,
        "Challenge_word_count":209,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521737124656,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Milton Keynes",
        "Poster_reputation_count":738.0,
        "Poster_view_count":69.0,
        "Solution_body":"<p>This is because of the path mismatch. The entrypoint is trying to look for &quot;model_handler.py&quot; in <code>WORKDIR<\/code> directory of the container.\nTo avoid this, always specify absolute path when working with containers.<\/p>\n<p>Moreover your code looks confusing. Please use this sample code as the reference:<\/p>\n<pre><code>import subprocess\nfrom subprocess import CalledProcessError\nimport model_handler\nfrom retrying import retry\nfrom sagemaker_inference import model_server\nimport os\n\n\ndef _retry_if_error(exception):\n    return isinstance(exception, CalledProcessError or OSError)\n\n\n@retry(stop_max_delay=1000 * 50, retry_on_exception=_retry_if_error)\ndef _start_mms():\n    # by default the number of workers per model is 1, but we can configure it through the\n    # environment variable below if desired.\n    # os.environ['SAGEMAKER_MODEL_SERVER_WORKERS'] = '2'\n    print(&quot;Starting MMS -&gt; running &quot;, model_handler.__file__)\n    model_server.start_model_server(handler_service=model_handler.__file__ + &quot;:handle&quot;)\n\n\ndef main():\n    _start_mms()\n    # prevent docker exit\n    subprocess.call([&quot;tail&quot;, &quot;-f&quot;, &quot;\/dev\/null&quot;])\n\nmain()\n<\/code><\/pre>\n<p>Further, notice this line - <code>model_server.start_model_server(handler_service=model_handler.__file__ + &quot;:handle&quot;) <\/code>\nHere we are starting the server, and telling it to call <code>handle()<\/code> function in model_handler.py to invoke your custom logic for all incoming requests.<\/p>\n<p>Also remember that Sagemaker BYOC requires model_handler.py to implement another function <code>ping()<\/code><\/p>\n<p>So your &quot;model_handler.py&quot; should look like this -<\/p>\n<pre><code>custom_handler = CustomHandler()\n\n# define your own health check for the model over here\ndef ping():\n    return &quot;healthy&quot;\n\n\ndef handle(request, context): # context is necessary input otherwise Sagemaker will throw exception\n    if request is None:\n        return &quot;SOME DEFAULT OUTPUT&quot;\n    try:\n        response = custom_handler.predict_fn(request)\n        return [response] # Response must be a list otherwise Sagemaker will throw exception\n\n    except Exception as e:\n        logger.error('Prediction failed for request: {}. \\n'\n                     .format(request) + 'Error trace :: {} \\n'.format(str(e)))\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.4,
        "Solution_reading_time":29.19,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":234.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"module not found error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":25.6642,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>So I'm having an issue with setting up an endpoint for a machine learning model which was trained using Azure AutoML. When I try to test the deployed model, I get an error saying that the service is temporarily unavailable. After looking online, I found that this might happen because of an error in the run() function in the entry script.  <\/p>\n<p>When I try to test the entry script on a notebook in Azure ML studio, on a fresh compute instance, there are two problems:  <br \/>\nFirst I get the error: <em>AttributeError: 'MSIAuthentication' object has no attribute 'get_token'<\/em>  <br \/>\nWhich is solved by running: <em>pip install azureml-core<\/em>  <\/p>\n<p>Then I get the error: <em>ModuleNotFoundError: No module named 'azureml.automl.runtime'<\/em>  <br \/>\nWhich I try to solve using: <em>pip install azureml-automl-runtime<\/em>  <br \/>\nBut this throws a lot of incompatibility errors during the installation. When I then try to run the entry script I get an error with the message: &quot;<em>Failed while applying learned transformations.<\/em>&quot;  <\/p>\n<p>So I setup a new virtual environment on my local machine in which I only installed azure-automl-runtime. Using that setup the entry script works perfectly fine. So I created a custom environment in Azure ML studio using the conda file of that local virtual environment. Unfortunatly I still get the error &quot;service temporarily unavailable&quot; when trying to test the endpoint.  <\/p>\n<p>I have a feeling the default Azure ML containers are incompatible with azureml-automl-runtime, since installing this on a ML studio notebook also throws a lot of errors.   <\/p>\n<p>I feel like there should be an elegant way to deploy an AutoML model, am I doing something wrong here?   <\/p>\n<p><strong>Update:<\/strong> I found out I didn't change the environment for the endpoint, so that is why I was getting the same error probably. When using the custom environment I got errors from gunicorn, so I also added that package to the environment. Now I get the following error:  <\/p>\n<pre><code>      File &quot;\/var\/azureml-server\/entry.py&quot;, line 1, in &lt;module&gt;\n    import create_app\n  File &quot;\/var\/azureml-server\/create_app.py&quot;, line 4, in &lt;module&gt;\n    from routes_common import main\n  File &quot;\/var\/azureml-server\/routes_common.py&quot;, line 39, in &lt;module&gt;\n    from azure.ml.api.exceptions.ClientSideException import ClientSideException\nModuleNotFoundError: No module named 'azure.ml'\n<\/code><\/pre>\n<p>So what do I install to fix this? Is there a list somewhere of required packages for an ML model endpoint?  <\/p>",
        "Challenge_closed_time":1627463996087,
        "Challenge_comment_count":2,
        "Challenge_created_time":1627371604967,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with setting up an endpoint for a machine learning model trained using Azure AutoML. The deployed model is showing a \"service temporarily unavailable\" error. The user tried to solve the issue by testing the entry script on a notebook in Azure ML studio, but encountered errors related to the \"MSIAuthentication\" object and \"ModuleNotFoundError\". The user then created a custom environment in Azure ML studio using the conda file of a local virtual environment, but still faced the same error. The user suspects that the default Azure ML containers are incompatible with azureml-automl-runtime. The user is now trying to fix the error \"No module named 'azure.ml'\" and is looking for a list of required packages for an ML model",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/490809\/automated-machine-learning-model-deployment-issue",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.1,
        "Challenge_reading_time":32.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":25.6642,
        "Challenge_title":"Automated machine learning model deployment issue",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":384,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I managed to fix the issue with the environment by just adding everything that would throw an error. Then I found out the return value has to be a json\/dict object, which if not done throws the exact same 'service temporarily unavailable' error.     <\/p>\n<p>But my issue with the confusing curated environments and azureml-automl-runtime in ML studio notebooks remain. Maybe this is worth looking into <a href=\"\/users\/na\/?userid=0e711f59-976b-4899-a912-2f0dd680421a\">@Ramr-msft  <\/a> .<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":6.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":69.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"endpoint setup error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1111111111,
        "Challenge_answer_count":1,
        "Challenge_body":"A wants to manage Sagemaker resources (such as models and endpoints) via CloudFormation. As part of their model deployment pipeline, they'd like to be able to create or update existing Sagemaker Endpoint with new model data. Customers wants to re-use the same endpoint name for a given workload. \n\n**Question:**\n\nHow to express in CF a following logic:\n1. If Sagemaker endpoint with name \"XYZ\" doesn't exist in customer account, then create a new endpoint;\n2. If Sagemaker endpoint with name \"XYZ\" already exist, then update existing endpoint with new model data.",
        "Challenge_closed_time":1607357193000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607356793000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to manage Sagemaker resources via CloudFormation and create or update existing Sagemaker Endpoint with new model data. They want to re-use the same endpoint name for a given workload and are looking for a way to express the logic in CloudFormation to create a new endpoint if it doesn't exist and update the existing endpoint if it does.",
        "Challenge_last_edit_time":1668615829184,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUXiLSnlxkQHKzQVFj6GKT7w\/create-or-update-sagemaker-endpoint-via-cloudformation",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":7.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.1111111111,
        "Challenge_title":"Create or update Sagemaker Endpoint via CloudFormation",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":437.0,
        "Challenge_word_count":97,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This functionality of \"UPSERT\" type does not exist in CFn natively. You would need to use a Custom Resource to handle this logic.\nOne alternative that is not exactly what you asked for but might be a decent compromise is to use a Parameter to supply the endpoint if it does exist. Then use a condition to check the value. If the paramter is blank then create an endpoint if not use the value supplied.\nI know this is not what you asked for but it allows you to avoid the custom resource solution.\n\nSample of similiar UPSERT example for a VPC:\n\n```\nParameters :\n\n  Vpc:\n    Type: AWS::EC2::VPC::Id\n\nConditions:\n\n  VpcNotSupplied: !Equals [!Ref Vpc, '']\n\nResources:\n\n  NewVpc:\n    Type: AWS::EC2::VPC\n    Condition: VpcNotSupplied\n    Properties:\n      CidrBlock: 10.0.0.0\/16\n\n  SecurityGroup:\n    Type: AWS::EC2::SecurityGroup\n    Properties:\n      GroupDescription: Sample\n      GroupName: Sample\n      VpcId: !If [VpcNotSupplied, !Ref NewVpc, !Ref Vpc ]\n```\n\nHere the `Vpc` input parameter can be supplied if the VPC you wish to use already exists, left blank if you want to create a new one. The NewVPC resource uses the `Condition` to only create if the supplied Vpc parameter value is blank. The Security group then uses the same condition to decide whetehr to use and existing Vpc or the newly created one.\n\nHope this makes sense.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925574694,
        "Solution_link_count":0.0,
        "Solution_readability":9.4,
        "Solution_reading_time":15.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":204.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"manage Sagemaker resources"
    },
    {
        "Answerer_created_time":1626973229036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":199.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":23.1600519444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a pytorch model that i have tested as a real-time endpoint in sagemaker, now i want to test it with batch inference. I am using jsonl data, and setting up a batch transform job as documented in aws documentation, in addition, i'm using my own inference.py (see sample below). I'm getting a json decode error inside the input_fn , function, when i try =&gt; json.loads(request_body).<\/p>\n<p>the error is =&gt; raise JSONDecodeError(&quot;Extra data&quot;, s, end)<\/p>\n<p>has anyone tried this? I sucessfully tested this model and json input with a real time endpoint in sagemaker, but now i'm trying to switch to batch and it is erroring it out.<\/p>\n<p>inference.py<\/p>\n<pre><code>def model_fn(model_dir):\n   ....\n\n\ndef input_fn(request_body, request_content_type):\n    data = json.loads(request_body)\n    return data\n\ndef predict_fn(data, model)\n  ...\n<\/code><\/pre>\n<p>set up for batch job via lambda<\/p>\n<pre><code>response = client.create_transform_job(\n    TransformJobName='some-job',\n    ModelName='mypytorchmodel',\n    ModelClientConfig={\n        'InvocationsTimeoutInSeconds': 3600,\n        'InvocationsMaxRetries': 1\n    },\n    BatchStrategy='MultiRecord',\n    TransformInput={\n        'DataSource': {\n            'S3DataSource': {\n                'S3DataType': 'S3Prefix',\n                'S3Uri': 's3:\/\/inputpath'\n            }\n        },\n        'ContentType': 'application\/json',\n        'SplitType': 'Line'\n    },\n    TransformOutput={\n        'S3OutputPath': 's3:\/\/outputpath',\n        'Accept': 'application\/json',\n        'AssembleWith': 'Line',\n    },\n    TransformResources={\n        'InstanceType': 'ml.g4dn.xlarge'\n        'InstanceCount': 1\n    }\n)\n<\/code><\/pre>\n<p>input file<\/p>\n<pre><code>{&quot;input&quot; : &quot;input line one&quot;}\n{&quot;input&quot; : &quot;input line two&quot;}\n{&quot;input&quot; : &quot;input line three&quot;}\n{&quot;input&quot; : &quot;input line four&quot;}\n{&quot;input&quot; : &quot;input line five&quot;}\n...\n\n<\/code><\/pre>",
        "Challenge_closed_time":1661785882720,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661702506533,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to test a PyTorch model with batch inference using jsonl data in Sagemaker. They have set up a batch transform job and are using their own inference.py file. However, they are encountering a JSON decode error inside the input_fn function when trying to load the request body. The user has successfully tested the model and json input with a real-time endpoint in Sagemaker, but it is failing for batch inference.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73520188",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":23.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":23.1600519444,
        "Challenge_title":"How to get batch predictions with jsonl data in sagemaker?",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":52.0,
        "Challenge_word_count":195,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1584308275360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":365.0,
        "Poster_view_count":94.0,
        "Solution_body":"<p>What is your client side code where you are invoking the endpoint? You should also be properly serializing the data on the client side and handling it in your inference script. Example:<\/p>\n<pre><code>import json\ndata = json.loads(json.dumps(request_body))\npayload = json.dumps(data)\nresponse = client.invoke_endpoint(\n    EndpointName=endpoint_name,\n    ContentType=content_type,\n    Body=payload)\nresult = json.loads(response['Body'].read().decode())['Output']\nresult\n<\/code><\/pre>\n<p>Make sure to also specify your content_type appropriately &quot;application\/jsonlines&quot;.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.7,
        "Solution_reading_time":7.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"JSON decode error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.68,
        "Challenge_answer_count":1,
        "Challenge_body":"Need validation:\n\n+ Once the SageMaker endpoint is deployed. It can be invoked with the *Sagemaker Runtime API* InvokeEndpoint *OR* it can be invoked using the endpoint URL+HTTP AZ headers (below). \n\n+ Successful deployment also exposes a URL (on the console) that has the format: \n\n*https:\/\/**runtime.sagemaker.us-east-1.amazonaws.com**\/endpoints\/ENDPOINT-NAME\/invocations*\n\nWhat is the purpose of this URL (shown on console)?\n\nIn my understanding this URL Cannot be invoked w\/o appropriate headers as then there will be a need to have globally unique  endpoint name!!  THAT IS  to invoke this URL it needs to have the \"HTTP Authorization headers\"   (refer: https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/API\/sig-v4-authenticating-requests.html)\n\nI have a customer who is concerned that anyone can invoke the URL even from the internet. Tried to do it and received the <MissingTokenException> so I know it can't be done but just want to ensure I have the right explanation. *(Test with HTTP\/AZ headers pending)*",
        "Challenge_closed_time":1602171513000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1602169065000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user needs validation on the purpose of the URL exposed on the console after successful deployment of SageMaker endpoint. The user believes that the URL cannot be invoked without appropriate headers and is concerned about anyone invoking it from the internet. The user has tested it and received a MissingTokenException, but wants to ensure that the explanation is correct.",
        "Challenge_last_edit_time":1668628484238,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUFlHNZ7JxTFGIkPHQ75u44w\/please-validate-sagemaker-endpoint-url-authentication-authorization",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.3,
        "Challenge_reading_time":13.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.68,
        "Challenge_title":"Please validate: SageMaker Endpoint URL Authentication\/Authorization",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":750.0,
        "Challenge_word_count":146,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Your understanding is correct. From the docs:\n\nAmazon SageMaker strips all POST headers except those supported by the API. Amazon SageMaker might add additional headers. You should not rely on the behavior of headers outside those enumerated in the request syntax.\n\nCalls to InvokeEndpoint are authenticated by using AWS Signature Version 4.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925564052,
        "Solution_link_count":0.0,
        "Solution_readability":9.7,
        "Solution_reading_time":4.23,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":52.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"purpose of exposed URL"
    },
    {
        "Answerer_created_time":1440024011336,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":335.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":39.2092791667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed a Model to an ACI container and have an endpoint that I can hit in Postman or using python SDK. I use Python to hit the endpoint as well as Postman and I get a response and the Container Instance logging records the event. I now what to use the AZ ML CLI to run the service and pass in some hardcoded JSON:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/cli\/azure\/ext\/azure-cli-ml\/ml\/service?view=azure-cli-latest#ext-azure-cli-ml-az-ml-service-run\" rel=\"nofollow noreferrer\">From the Azure ML CLI docs<\/a>:  <\/p>\n\n<pre><code>az ml service run --name (-n) --input-data (-d)\n<\/code><\/pre>\n\n<p>I run this <\/p>\n\n<pre><code>az ml service run -n \"rj-aci-5\" -d {\\\"input_df\\\": [{\\\"width\\\": 50, \\\"shoe_size\\\": 28}]}\n<\/code><\/pre>\n\n<p>There is no output or error. The logs do not record any invocation. Has anyone used the Azure CLI ML extensions to run a service in the manner above?<\/p>",
        "Challenge_closed_time":1570637042412,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570495889007,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has deployed a model to an ACI container and can hit the endpoint using Postman or Python SDK. They are trying to use the Azure ML CLI to run the service and pass in hardcoded JSON, but there is no output or error, and the logs do not record any invocation. The user is seeking help from anyone who has used the Azure CLI ML extensions to run a service in this manner.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58278844",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":11.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":39.2092791667,
        "Challenge_title":"Does the Azure CLI ML \"service run\" command work?",
        "Challenge_topic":"Lambda Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":63.0,
        "Challenge_word_count":139,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1256089885500,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sydney, Australia",
        "Poster_reputation_count":4947.0,
        "Poster_view_count":531.0,
        "Solution_body":"<p>The az cli is likely failing to parse the provided data input. If I attempt to run the same command I see the following error:<\/p>\n\n<p><code>az: error: unrecognized arguments: [{\"width\": 50, \"shoe_size\": 28}]}<\/code><\/p>\n\n<p>You need to wrap the input in quotes for it to appropriately be taken as a single input parameter:<\/p>\n\n<p><code>az ml service run -n \"rj-aci-5\" -d \"{\\\"input_df\\\": [{\\\"width\\\": 50, \\\"shoe_size\\\": 28}]}\"<\/code><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.6,
        "Solution_reading_time":5.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"no output or error"
    },
    {
        "Answerer_created_time":1334649856036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2091.0,
        "Answerer_view_count":501.0,
        "Challenge_adjusted_solved_time":315.0776647222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In Azure ML, I want to enter data to a model through a published Web Service. \nThe way to tell this to the Web Service, as far as I can tell, it to have an 'Enter Data' box coming into the same input as the Web service. <\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/m1x5x.png\" alt=\"enter image description here\"><\/p>\n\n<p>You can then set you data format in the 'Enter Data' properties:<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/VQJ9V.png\" alt=\"enter image description here\"><\/p>\n\n<p>I want that list to be an arbitrary-length array of samples. This works if your input is:<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"samples\"\n      ],\n      \"Values\": [\n        [\n          1\n        ],\n        [\n          2\n        ],\n        [\n          3\n        ],\n        [\n          4\n        ],\n        [\n          5\n        ]\n      ]\n    }\n  },\n  \"GlobalParameters\": {}\n}\n<\/code><\/pre>\n\n<p>This is ok, but ideally it would be easier, and (more importantly) more network-efficient, if I could send them as:<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"samples\"\n      ],\n      \"Values\": [\n        [\n          1,2,3,4,5\n        ]\n      ]\n    }\n  },\n  \"GlobalParameters\": {}\n}\n<\/code><\/pre>\n\n<p>Is there a correct syntax to implement this? <\/p>",
        "Challenge_closed_time":1438871325836,
        "Challenge_comment_count":2,
        "Challenge_created_time":1437737046243,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to enter data to a model through a published Web Service in Azure ML. They want to send an arbitrary-length array of samples as a list instead of a list of lists to make it easier and more network-efficient. They are looking for the correct syntax to implement this.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/31609319",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.8,
        "Challenge_reading_time":13.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":315.0776647222,
        "Challenge_title":"'Enter Data' as list instead of list of lists in Azure ML Web Service",
        "Challenge_topic":"Web Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":174.0,
        "Challenge_word_count":153,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1266595927520,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":7681.0,
        "Poster_view_count":361.0,
        "Solution_body":"<p>I have worked internally to request a confirmation of your concern - <\/p>\n\n<blockquote>\n  <p>'Enter Data' as list instead of list of lists in Azure ML Web Service<\/p>\n<\/blockquote>\n\n<p>but you expected feature is not available today in Azure ML Studio (The reason behind is Azure ML has to be able to read the input data as a tabular format, rows and columns). Such being the case, I would like to suggest you to submit a new feature request via below option:<\/p>\n\n<p>On Azure ML Studio -> the upper right corner, there is a smiley face, please click that and send the feedback.<\/p>\n\n<p>Should you have any further concerns, please feel free to let me know.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.8,
        "Solution_reading_time":7.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":114.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"send array to web service"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.6372286111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've followed the documentation pretty well as outlined <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-custom-docker-image\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p>I've setup my azure machine learning environment the following way:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace\n\n# Connect to the workspace\nws = Workspace.from_config()\n\nfrom azureml.core import Environment\nfrom azureml.core import ContainerRegistry\n\nmyenv = Environment(name = &quot;myenv&quot;)\n\nmyenv.inferencing_stack_version = &quot;latest&quot;  # This will install the inference specific apt packages.\n\n# Docker\nmyenv.docker.enabled = True\nmyenv.docker.base_image_registry.address = &quot;myazureregistry.azurecr.io&quot;\nmyenv.docker.base_image_registry.username = &quot;myusername&quot;\nmyenv.docker.base_image_registry.password = &quot;mypassword&quot;\nmyenv.docker.base_image = &quot;4fb3...&quot; \nmyenv.docker.arguments = None\n\n# Environment variable (I need python to look at folders \nmyenv.environment_variables = {&quot;PYTHONPATH&quot;:&quot;\/root&quot;}\n\n# python\nmyenv.python.user_managed_dependencies = True\nmyenv.python.interpreter_path = &quot;\/opt\/miniconda\/envs\/myenv\/bin\/python&quot; \n\nfrom azureml.core.conda_dependencies import CondaDependencies\nconda_dep = CondaDependencies()\nconda_dep.add_pip_package(&quot;azureml-defaults&quot;)\nmyenv.python.conda_dependencies=conda_dep\n\nmyenv.register(workspace=ws) # works!\n<\/code><\/pre>\n<p>I have a score.py file configured for inference (not relevant to the problem I'm having)...<\/p>\n<p>I then setup inference configuration<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.model import InferenceConfig\ninference_config = InferenceConfig(entry_script=&quot;score.py&quot;, environment=myenv)\n<\/code><\/pre>\n<p>I setup my compute cluster:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.compute import ComputeTarget, AksCompute\nfrom azureml.exceptions import ComputeTargetException\n\n# Choose a name for your cluster\naks_name = &quot;theclustername&quot; \n\n# Check to see if the cluster already exists\ntry:\n    aks_target = ComputeTarget(workspace=ws, name=aks_name)\n    print('Found existing compute target')\nexcept ComputeTargetException:\n    print('Creating a new compute target...')\n    prov_config = AksCompute.provisioning_configuration(vm_size=&quot;Standard_NC6_Promo&quot;)\n\n    aks_target = ComputeTarget.create(workspace=ws, name=aks_name, provisioning_configuration=prov_config)\n\n    aks_target.wait_for_completion(show_output=True)\n\nfrom azureml.core.webservice import AksWebservice\n\n# Example\ngpu_aks_config = AksWebservice.deploy_configuration(autoscale_enabled=False,\n                                                    num_replicas=3,\n                                                    cpu_cores=4,\n                                                    memory_gb=10)\n<\/code><\/pre>\n<p>Everything succeeds; then I try and deploy the model for inference:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.model import Model\n\nmodel = Model(ws, name=&quot;thenameofmymodel&quot;)\n\n# Name of the web service that is deployed\naks_service_name = 'tryingtodeply'\n\n# Deploy the model\naks_service = Model.deploy(ws,\n                           aks_service_name,\n                           models=[model],\n                           inference_config=inference_config,\n                           deployment_config=gpu_aks_config,\n                           deployment_target=aks_target,\n                           overwrite=True)\n\naks_service.wait_for_deployment(show_output=True)\nprint(aks_service.state)\n<\/code><\/pre>\n<p>And it fails saying that it can't find the environment. More specifically, my environment version is <strong>version 11<\/strong>, but it keeps trying to find an environment with a version number that is 1 higher (i.e., <strong>version 12<\/strong>) than the current environment:<\/p>\n<pre><code>FailedERROR - Service deployment polling reached non-successful terminal state, current service state: Failed\nOperation ID: 0f03a025-3407-4dc1-9922-a53cc27267d4\nMore information can be found here: \nError:\n{\n  &quot;code&quot;: &quot;BadRequest&quot;,\n  &quot;statusCode&quot;: 400,\n  &quot;message&quot;: &quot;The request is invalid&quot;,\n  &quot;details&quot;: [\n    {\n      &quot;code&quot;: &quot;EnvironmentDetailsFetchFailedUserError&quot;,\n      &quot;message&quot;: &quot;Failed to fetch details for Environment with Name: myenv Version: 12.&quot;\n    }\n  ]\n}\n\n<\/code><\/pre>\n<p>I have tried to manually edit the environment JSON to match the version that azureml is trying to fetch, but nothing works. Can anyone see anything wrong with this code?<\/p>\n<h1>Update<\/h1>\n<p>Changing the name of the environment (e.g., <code>my_inference_env<\/code>) and passing it to <code>InferenceConfig<\/code> seems to be on the right track. However, the error now changes to the following<\/p>\n<pre><code>Running..........\nFailed\nERROR - Service deployment polling reached non-successful terminal state, current service state: Failed\nOperation ID: f0dfc13b-6fb6-494b-91a7-de42b9384692\nMore information can be found here: https:\/\/some_long_http_address_that_leads_to_nothing\nError:\n{\n  &quot;code&quot;: &quot;DeploymentFailed&quot;,\n  &quot;statusCode&quot;: 404,\n  &quot;message&quot;: &quot;Deployment not found&quot;\n}\n<\/code><\/pre>\n<h1>Solution<\/h1>\n<p>The answer from Anders below is <strong>indeed correct<\/strong> regarding the use of azure ML environments. However, the last error I was getting was because I was setting the <em>container image<\/em> using the digest value (a sha) and NOT the image name and tag (e.g., <code>imagename:tag<\/code>). Note the line of code in the first block:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>myenv.docker.base_image = &quot;4fb3...&quot; \n<\/code><\/pre>\n<p>I reference the digest value, but it should be changed to<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>myenv.docker.base_image = &quot;imagename:tag&quot;\n<\/code><\/pre>\n<p>Once I made that change, the deployment succeeded! :)<\/p>",
        "Challenge_closed_time":1597702121696,
        "Challenge_comment_count":5,
        "Challenge_created_time":1597699827673,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while deploying a model for inference on Azure-ML. The deployment fails as the environment version number is incorrect. The user has tried to manually edit the environment JSON to match the version that Azure-ML is trying to fetch, but it did not work. The user has also changed the name of the environment and passed it to InferenceConfig, but the error message changed to \"Deployment not found.\" The solution to the problem was to set the container image using the image name and tag instead of the digest value.",
        "Challenge_last_edit_time":1599771558392,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63458904",
        "Challenge_link_count":2,
        "Challenge_participation_count":6,
        "Challenge_readability":17.3,
        "Challenge_reading_time":77.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":44,
        "Challenge_solved_time":0.6372286111,
        "Challenge_title":"Azure-ML Deployment does NOT see AzureML Environment (wrong version number)",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1768.0,
        "Challenge_word_count":509,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432406490590,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Milwaukee, WI",
        "Poster_reputation_count":381.0,
        "Poster_view_count":62.0,
        "Solution_body":"<p>One concept that took me a while to get was the bifurcation of registering and using an Azure ML <code>Environment<\/code>. If you have already registered your env, <code>myenv<\/code>, and none of the details of the your environment have changed, there is no need re-register it with <code>myenv.register()<\/code>. You can simply get the already register env using <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.environment.environment?view=azure-ml-py#get-workspace--name--version-none-\" rel=\"nofollow noreferrer\"><code>Environment.get()<\/code><\/a> like so:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>myenv = Environment.get(ws, name='myenv', version=11)\n<\/code><\/pre>\n<p>My recommendation would be to name your environment something new: like <code>&quot;model_scoring_env&quot;<\/code>. Register it once, then pass it to the <code>InferenceConfig<\/code>.<\/p>",
        "Solution_comment_count":9.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.2,
        "Solution_reading_time":11.94,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"incorrect environment version number"
    },
    {
        "Answerer_created_time":1501163272143,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":118.0,
        "Challenge_adjusted_solved_time":1.6257427778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I wonder if it's possible to export my model to a json file, so I can do some kind of versioning.<\/p>\n\n<p>Building up a model with Azure Machine Learning Studio is easy, but I need to save the previous version anytime I do an update.<\/p>\n\n<p>It's possible to do this?<\/p>",
        "Challenge_closed_time":1544666744367,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544660891693,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to export their Azure Machine Learning model to a JSON file for versioning purposes. They are seeking advice on whether this is possible or not.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53753367",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.7,
        "Challenge_reading_time":3.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.6257427778,
        "Challenge_title":"How to export my azure machine learning model to json",
        "Challenge_topic":"Endpoint Invocation",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":445.0,
        "Challenge_word_count":59,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458558039430,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Trieste, Province of Trieste, Italy",
        "Poster_reputation_count":1657.0,
        "Poster_view_count":164.0,
        "Solution_body":"<p>In Azure ML Studio, the versioning is available as Run History: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/manage-experiment-iterations\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/manage-experiment-iterations<\/a>\nRegards,\nJaya<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":30.3,
        "Solution_reading_time":4.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"export model to JSON"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":141.7067575,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>With an <code>sagemaker.estimator.Estimator<\/code>, I want to re-<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html#sagemaker.estimator.EstimatorBase.deploy\" rel=\"nofollow noreferrer\">deploy<\/a> a model after retraining (calling <code>fit<\/code> with new data).<\/p>\n<p>When I call this<\/p>\n<pre><code>estimator.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')\n<\/code><\/pre>\n<p>I get an error<\/p>\n<pre><code>botocore.exceptions.ClientError: An error occurred (ValidationException) \nwhen calling the CreateEndpoint operation: \nCannot create already existing endpoint &quot;arn:aws:sagemaker:eu-east- \n1:1776401913911:endpoint\/zyx&quot;.\n<\/code><\/pre>\n<p>Apparently I want to use functionality like <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_UpdateEndpoint.html\" rel=\"nofollow noreferrer\">UpdateEndpoint<\/a>. How do I access that functionality from this API?<\/p>",
        "Challenge_closed_time":1601891469683,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601630651783,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to re-deploy a re-trained Sagemaker model using an estimator. The error message indicates that the endpoint already exists, and the user is seeking guidance on how to access the UpdateEndpoint functionality from the API.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64169189",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":20.5,
        "Challenge_reading_time":13.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":72.4494166667,
        "Challenge_title":"How can I deploy a re-trained Sagemaker model to an endpoint?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1958.0,
        "Challenge_word_count":78,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1227171471292,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Israel",
        "Poster_reputation_count":17500.0,
        "Poster_view_count":1561.0,
        "Solution_body":"<p>Yes, under the hood the <code>model.deploy<\/code> creates a model, an endpoint configuration and an endpoint. When you call again the method from an already-deployed, trained estimator it will create an error because a similarly-configured endpoint is already deployed. What I encourage you to try:<\/p>\n<ul>\n<li><p>use the <code>update_endpoint=True<\/code> parameter. From the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\" rel=\"noreferrer\">SageMaker SDK doc<\/a>:\n<em>&quot;Additionally, it is possible to deploy a different endpoint configuration, which links to your model, to an already existing\nSageMaker endpoint. This can be done by specifying the existing\nendpoint name for the <code>endpoint_name<\/code> parameter along with the\n<code>update_endpoint<\/code> parameter as True within your <code>deploy()<\/code> call.&quot;<\/em><\/p>\n<\/li>\n<li><p>Alternatively, if you want to create a separate model you can specify a new <code>model_name<\/code> in your <code>deploy<\/code><\/p>\n<\/li>\n<\/ul>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1602140796110,
        "Solution_link_count":1.0,
        "Solution_readability":14.0,
        "Solution_reading_time":13.16,
        "Solution_score_count":6.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":123.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"endpoint already exists"
    },
    {
        "Answerer_created_time":1435766573232,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":10645.0,
        "Answerer_view_count":1173.0,
        "Challenge_adjusted_solved_time":35082.2519488889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to serve an R model as a web service in Azure ML.<\/p>\n\n<p>The model is trained locally and uses Xgboost and other packages. I have had issues submitting it directly from AzureML package due to size exceeding 130 MB. The workaround was to upload all the packages and the model as zip to Azure and source it from there.<\/p>\n\n<p>The current issue is that the model is loaded from a zip file by Azure ML EVERY time the service is called making the response time very slow (4.5 seconds).\nHow do I restructure the code so that the model is loaded only once from the file. Thank you for your help.<\/p>\n\n<p>Here is how it looks in AzureML <a href=\"https:\/\/i.stack.imgur.com\/I7QRT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/I7QRT.png\" alt=\"enter image description here\"><\/a>\nAnd here is what is inside Execute R script\n<a href=\"https:\/\/i.stack.imgur.com\/qM1OF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qM1OF.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":1519712850203,
        "Challenge_comment_count":0,
        "Challenge_created_time":1519653576907,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in serving an R model as a web service in Azure ML due to its large size. The model is loaded from a zip file by Azure ML every time the service is called, making the response time slow. The user is seeking help to restructure the code so that the model is loaded only once from the file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48990264",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":13.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":16.4648044445,
        "Challenge_title":"Operationalize custom R model in Azure ML without loading it on every call of web service",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":230.0,
        "Challenge_word_count":163,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1363618231430,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":25.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Here is a <a href=\"https:\/\/blogs.technet.microsoft.com\/machinelearning\/2016\/10\/26\/speeding-up-azure-ml-web-services-containing-r-or-python-modules\/?\" rel=\"nofollow noreferrer\">clever trick<\/a> for running initialization steps only the first time and not on every subsequent call.<\/p>\n<p>My understanding is that you would wrap the first 3 statements (that is through line 11) of your script in the following <code>if<\/code> statement:<\/p>\n<pre><code>if (!is.element(&quot;my_env&quot;, search()))\n<\/code><\/pre>\n<p>The <code>if<\/code> statement would also contain the initialization of the <code>my_env<\/code> variable as shown in the example used in that blog:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/S445n.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/S445n.png\" alt=\"R optimization example\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1645949683923,
        "Solution_link_count":3.0,
        "Solution_readability":13.2,
        "Solution_reading_time":11.02,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":80.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"slow response time"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":6.5106594444,
        "Challenge_answer_count":2,
        "Challenge_body":"based on the sample code provided here , https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config\n\nI created a model via lambda, now when i try to create a serverless endpoint config (sample code below) , i keep getting -> parameter validation failed \nunknown parameter in` ProductVariants [ 0 ]:` \"ServerlessConfig\", must be one of : VairantName, ModelName, InitialInstanceCount , Instancetype...\n\n```\nresponse = client.create_endpoint_config(\n   EndpointConfigName=\"endpoint-new\",\n   ProductionVariants=[\n        {\n            \"ModelName\": \"MyModel\",\n            \"VariantName\": \"AllTraffic\",\n            \"ServerlessConfig\": {\n                \"MemorySizeInMB\": 2048,\n                \"MaxConcurrency\": 10\n            }\n        } \n    ]\n)\n```",
        "Challenge_closed_time":1645090644600,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645067206226,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a serverless endpoint configuration using sample code provided by AWS. However, they are encountering an error message stating that the \"ServerlessConfig\" parameter is unknown and must be one of \"VariantName\", \"ModelName\", \"InitialInstanceCount\", or \"InstanceType\".",
        "Challenge_last_edit_time":1668438755896,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUfmAxh_aDQiS2nk0gbDicsg\/how-to-create-a-serverless-endpoint-configuration",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":20.3,
        "Challenge_reading_time":9.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":6.5106594444,
        "Challenge_title":"How to create a serverless endpoint configuration?",
        "Challenge_topic":"Model Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":260.0,
        "Challenge_word_count":66,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The cause might be that your SageMaker Python SDK is not updated to the latest version. Please make sure you update it to the latest version as well as the AWS SDK for Python (boto3). You can use pip:\n\n```\npip install --upgrade boto3\npip install --upgrade sagemaker\n```\n\n\nFor a sample notebook you can have a look [here](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/serverless-inference\/Serverless-Inference-Walkthrough.ipynb). More information on the documentation [page](https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#sagemaker-serverless-inference).",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1645171546007,
        "Solution_link_count":2.0,
        "Solution_readability":14.2,
        "Solution_reading_time":7.48,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":63.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unknown parameter"
    }
]