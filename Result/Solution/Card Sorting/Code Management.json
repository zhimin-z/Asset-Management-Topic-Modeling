[
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.7363888889,
        "Challenge_answer_count":1,
        "Challenge_body":"What value should I set for the **directory_path** parameter in **FileSystemInput** for the Amazon SageMaker SDK?\n\nHere is some information about my Amazon FSx for Lustre file system:\n - My FSx ID is `fs-0684xxxxxxxxxxx`.\n - My FSx has the mount name `lhskdbmv`.\n - The FSx maps to an Amazon S3 bucket with files (without extra prefixes in their keys)\n\nMy attempts to describe the job and the results are the following:\n\n**Attempt 1:**\n\n    fs = FileSystemInput(\n        file_system_id='fs-0684xxxxxxxxxxx',\n        file_system_type='FSxLustre',\n        directory_path='lhskdbmv',\n        file_system_access_mode='ro')\n\n**Result:**\n\n`estimator.fit(fs)` returns `ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: FileSystem DirectoryPath 'lhskdbmv' for channel 'training' is not absolute or normalized. Please ensure you don't have a trailing \"\/\", and\/or \"..\", \".\", \"\/\/\" in the path.`\n\n**Attempt 2:**\n\n    fs = FileSystemInput(\n        file_system_id='fs-0684xxxxxxxxxxx',\n        file_system_type='FSxLustre',\n        directory_path='\/',\n        file_system_access_mode='ro')\n\n**Result:**\n\n`ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: The directory path for FSx Lustre file system fs-068406952bf758bac is invalid. The directory path must begin with mount name of the file system.`\n\n**Attempt 3:**\n\n    fs = FileSystemInput(\n        file_system_id='fs-0684xxxxxxxxxxx',\n        file_system_type='FSxLustre',\n        directory_path='fsx',\n        file_system_access_mode='ro')\n\n**Result:**\n\nClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: FileSystem DirectoryPath 'fsx' for channel 'training' is not absolute or normalized. Please ensure you don't have a trailing \"\/\", and\/or \"..\", \".\", \"\/\/\" in the path.",
        "Challenge_closed_time":1605296508000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605283057000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668165980160,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUHaScKqcfRu-aZ1Cwza63NQ\/what-value-should-i-set-for-directory-path-for-the-amazon-sagemaker-sdk-with-fsx-as-data-source",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.1,
        "Challenge_reading_time":23.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":3.7363888889,
        "Challenge_title":"What value should I set for directory_path for the Amazon SageMaker SDK with FSx as data source?",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":264.0,
        "Challenge_word_count":210,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The **directory_path** parameter must point to \/**mountname**\/path\/to\/specific\/folder\/in-file-system. The value of mountname is returned in the CreateFileSystem API operation response. It is also returned in the response of the describe-file-systems AWS Command Line Interface (AWS CLI) command and the DescribeFileSystems API operation. \n\nFor your use case,  the response might look similar to the following:\n```mountName = lhskdbmv```",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925582735,
        "Solution_link_count":0.0,
        "Solution_readability":11.0,
        "Solution_reading_time":5.55,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0855555556,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nI am currently defining some machines configuration using machine-env1.yaml, machine-env2.yaml which basically contains node selectors and CPU, GPU, and TPU requests configuration, and then running:\n\npolyaxon run -f polyaxonfile.yaml -f machine-env1.yaml\n\nI have two problems with this approach:\n\nI need to copy the env files to all our git repos, which means if I make a change I need to perform several pull requests\nI need to tell the data-scientits to pull the last commit, sometimes that's not possible because they can not merge\/rebase the changes.\n\nBased on those two issues, in the end we tell data-scientists to just use:\n\nenvironment:\n  nodeSelector:\n    nodes: large-pool\n...\nrun:\n  ...\n  container:\n      resources:\n        limits:\n          cpu: 3000m\n          memory: 6000Mi\n        requests:\n          cpu: 2000m\n          memory: 4000Mi\n\nWhich is error prone and confusing for them, and make the files bigger and difficult to change.\n\nAny elegant way to abstract this type of configuration from the data-scientists?",
        "Challenge_closed_time":1649337274000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649336966000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1484",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.6,
        "Challenge_reading_time":13.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.0855555556,
        "Challenge_title":"I would like to configure Polyaxon in a way to avoid asking data-scientists to configure pre-emptible node-pools or request TPUs on their own",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":null,
        "Challenge_word_count":170,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"We have already shared a resource on how to configure the environments in this guide\n\nif you are using multiple git repos and you do not want to replicate the yaml files in all repos you can register those files as presets:\n\nUsers will be able to use --presets machine1 or --presets=env1\n\nNote that in the example in that link, it shows that it defines a queue but you do not have to define a queue, a preset is just any YAML file that can be used with the override operator -f main.yaml -f override1.yaml -f override2.yaml in this case override1.yaml and override2.yaml it can be saved as organization presets using the UI.\n\nMore info from the intro section about presets and the UI section\n\nAlso, when you define presets you can use them directly on the operation or component\n\npresets: [preset1, preset2]\n\nThis is similar to the CLI command\n\npolyaxon run -f polyaxon.yaml --presets preset1,preset2",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.5,
        "Solution_reading_time":10.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":156.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":184.4022222222,
        "Challenge_answer_count":0,
        "Challenge_body":"\r\n*Description:*\r\n\r\nAn apt-get error is seen in `sagemaker-local-test` builds as below. This is because `apt-get` process is already running and in active state.\r\n\r\n```\r\nE: Could not get lock \/var\/lib\/dpkg\/lock-frontend - open (11: Resource temporarily unavailable)\r\n--\r\n294 | E: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), is another process using it?\r\n```\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1598551848000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597888000000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/deep-learning-containers\/issues\/517",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.0,
        "Challenge_reading_time":5.41,
        "Challenge_repo_contributor_count":100.0,
        "Challenge_repo_fork_count":316.0,
        "Challenge_repo_issue_count":2511.0,
        "Challenge_repo_star_count":579.0,
        "Challenge_repo_watch_count":38.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":184.4022222222,
        "Challenge_title":"[bug] apt-get failure in sagemaker-local-test builds",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":null,
        "Challenge_word_count":55,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":5.8410222222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><strong>Background<\/strong>: In my projects I'm using GIT and <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> to keep track of versions:<\/p>\n<ul>\n<li>GIT - only for source codes<\/li>\n<li>DVC - for dataset, model objects and outputs<\/li>\n<\/ul>\n<p>I'm testing different approaches in separate branches, i.e:<\/p>\n<ul>\n<li>random_forest<\/li>\n<li>neural_network_1<\/li>\n<li>...<\/li>\n<\/ul>\n<p>Typically as an output I'm keeping predictions in csv file with standarised name (i.e.: pred_test.csv). As a consequence in different branches I've different pred_test.csv files. The structure of the file is very simple, it contains two columns:<\/p>\n<ul>\n<li>ID<\/li>\n<li>Prediction<\/li>\n<\/ul>\n<p><strong>Question<\/strong>: What is the best way to merge those prediction files into single big file?<\/p>\n<p>I would like to obtain a file with structure:<\/p>\n<ul>\n<li>ID<\/li>\n<li>Prediction_random_forest<\/li>\n<li>Prediction_neural_network_1<\/li>\n<li>Prediction_...<\/li>\n<\/ul>\n<p>My main issue is how to access files with predictions which are in different branches?<\/p>",
        "Challenge_closed_time":1645113091963,
        "Challenge_comment_count":5,
        "Challenge_created_time":1645092064283,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1645132430276,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71155959",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":9.4,
        "Challenge_reading_time":14.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":5.8410222222,
        "Challenge_title":"How to merge data (CSV) files from multiple branches (Git and DVC)?",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":274.0,
        "Challenge_word_count":141,
        "Platform":"Stack Overflow",
        "Poster_created_time":1265742671200,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2735.0,
        "Poster_view_count":552.0,
        "Solution_body":"<p>I would try to use <a href=\"https:\/\/dvc.org\/doc\/command-reference\/get\" rel=\"nofollow noreferrer\"><code>dvc get<\/code><\/a> in this case:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc get -o random_forest_pred.csv --rev random_forest . pred_test.csv\n<\/code><\/pre>\n<p>It should bring the <code>pred_test.csv<\/code> from the <code>random_forest<\/code> branch.<\/p>\n<blockquote>\n<p>Mind the <code>.<\/code> before the <code>pred_test.csv<\/code> please, it's needed and it means that &quot;use the current repo&quot;, since <code>dvc get<\/code> could also be used on other repos (e.g. GitHub URL)<\/p>\n<\/blockquote>\n<p>Then I think you could use some CLI or write a script to join the files:<\/p>\n<p><a href=\"https:\/\/unix.stackexchange.com\/questions\/293775\/merging-contents-of-multiple-csv-files-into-single-csv-file\">https:\/\/unix.stackexchange.com\/questions\/293775\/merging-contents-of-multiple-csv-files-into-single-csv-file<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.7,
        "Solution_reading_time":12.55,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":82.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":109.7675,
        "Challenge_answer_count":0,
        "Challenge_body":"It seems they both assume that the current working dir is where they can find the `.git` and `.dvc` dirs.\r\nWe should correctly detect those paths, as it affects all our logic to e.g. automatically dvc init on behalf of the user.\r\n\r\nRelevant resources:\r\n1. https:\/\/stackoverflow.com\/a\/957978\r\n2. https:\/\/dvc.org\/doc\/command-reference\/root",
        "Challenge_closed_time":1630227884000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629832721000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/92",
        "Challenge_link_count":2,
        "Challenge_participation_count":0,
        "Challenge_readability":9.0,
        "Challenge_reading_time":5.02,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":139.0,
        "Challenge_repo_star_count":357.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":109.7675,
        "Challenge_title":"DVC and Git services don't correctly detect the repo root directory",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":null,
        "Challenge_word_count":58,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1038888889,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nMy job is stack with a warning status, I configured a private bitbucket connection and the cloning fails.",
        "Challenge_closed_time":1649328708000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649328334000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1472",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":7.2,
        "Challenge_reading_time":1.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":0.1038888889,
        "Challenge_title":"How to debug my init git container",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":null,
        "Challenge_word_count":26,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Before updating the connections or changing anything about your current deployment, please perform the following debugging steps:\n\nEnable logs from all containers:\n\n\n\nYou can also inspect the operations from the statuses page to get more information (for distributed runs you can select the correct pod)\n\n\nYou can suspend the init container using :\n\n  - connection: my-connection\n    git: {...}\n    container:\n      command: [\"\/bin\/bash\", \"-c\"]\n      args: [\"sleep 3600\"]\nUse shell to get inside the container (for distributed runs you can select the correct pod and container):",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":20.1,
        "Solution_reading_time":6.86,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":80.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1102.8986563889,
        "Challenge_answer_count":13,
        "Challenge_body":"<!-- Issues are public, they should not contain confidential information -->\n\n### What is the current _bug_ behavior? how can we reproduce it?\nThe requirements.txt file does not have the entire dependency tree defined\n\n### Possible fixes\nModify the requirements.txt file so that it has the complete tree of dependencies and their respective versions\n### Steps\n\n- [x] Make sure that the\n      [code contributions checklist](https:\/\/docs.fluidattacks.com\/development\/contributing#checklist)\n      has been followed.",
        "Challenge_closed_time":1675452134148,
        "Challenge_comment_count":0,
        "Challenge_created_time":1671481698985,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/gitlab.com\/fluidattacks\/universe\/-\/issues\/8382",
        "Challenge_link_count":1,
        "Challenge_participation_count":13,
        "Challenge_readability":11.1,
        "Challenge_reading_time":6.8,
        "Challenge_repo_contributor_count":41.0,
        "Challenge_repo_fork_count":2.0,
        "Challenge_repo_issue_count":5537.0,
        "Challenge_repo_star_count":16.0,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1102.8986563889,
        "Challenge_title":"[Sorts] Add sagemaker dependencies",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":null,
        "Challenge_word_count":66,
        "Platform":"Gitlab",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Added and pinned the additional dependencies. @mriveraatfluid mentioned in commit d54bbb59bc42db6a66848d1cefe4b8ab29f68690 mentioned in merge request !36356 mentioned in commit 24b795e9436ea7c45379486b5541f6a84f967c15 mentioned in commit b3eda78ae28bb6810c052d94edd675a82cff6129 marked the checklist item **Make sure that the** as completed The complete tree of dependencies has been added and pinned properly in the requirements file. @mriveraatfluid mentioned in commit 9ecc0c84c69f7b0422f0a30239d522c031b0d7ab mentioned in merge request !34151 mentioned in commit cf649ea2f6fab7a2e8308aa813e1841bf4d23c95 unassigned @auribeatfluid assigned to @rrodriguezatfluid assigned to @auribeatfluid",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.2,
        "Solution_reading_time":9.12,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":71.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":73.275,
        "Challenge_answer_count":3,
        "Challenge_body":"\r\n\r\nhttps:\/\/github.com\/Azure\/azureml-examples\/runs\/1618089261?check_suite_focus=true\r\n\r\n@trangevi \r\n\r\nhttps:\/\/github.com\/mlflow\/mlflow\/pull\/3419\/files",
        "Challenge_closed_time":1609558021000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609294231000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/azureml-examples\/issues\/318",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":30.8,
        "Challenge_reading_time":2.51,
        "Challenge_repo_contributor_count":135.0,
        "Challenge_repo_fork_count":646.0,
        "Challenge_repo_issue_count":1964.0,
        "Challenge_repo_star_count":873.0,
        "Challenge_repo_watch_count":2758.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":73.275,
        "Challenge_title":"MLflow 1.13 probably broke deployment",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":null,
        "Challenge_word_count":8,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"yeah @trangevi the logging statement is guaranteed to bork out: https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/azureml\/__init__.py#L413\r\n\r\n@eedeleon fyi https:\/\/github.com\/mlflow\/mlflow\/pull\/3922\r\n @akshaya-a @eedeleon looks resolved, thanks for investigating! will close this issue ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.1,
        "Solution_reading_time":3.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":25.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":22.5241808334,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I used  <code>tempfile.mkdtemp() <\/code> to create a temporary directory for my runs (as I don\u2019t want a persistent folder with tons of runs)<\/p>\n<p>For training everything works fine but when resuming the run to do some validation \/ evaluation updates, and using <code>run.summary.update({\"key\": value})<\/code> I got a<\/p>\n<pre><code class=\"lang-auto\">wandb: WARNING Path \/tmp\/tmpq5uafy4d\/wandb\/ wasn't writable, using system temp directory\n<\/code><\/pre>\n<p>with obviously<\/p>\n<pre><code class=\"lang-auto\">File \"\/mnt\/Projets\/nlp\/.venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/internal\/sender.py\", line 855, in _update_summary\n    with open(summary_path, \"w\") as f:\nFileNotFoundError: [Errno 2] No such file or directory: '\/tmp\/tmpq5uafy4d\/wandb\/run-20211102_153311-37264m5k\/files\/wandb-summary.json'\n<\/code><\/pre>\n<p>As in the doc of <a href=\"https:\/\/docs.python.org\/3.9\/library\/tempfile.html#tempfile.mkdtemp\" rel=\"noopener nofollow ugc\"><code>mkdtemp<\/code><\/a> :<\/p>\n<pre><code class=\"lang-auto\"> The directory is readable, writable, and searchable only by the creating user ID.\n<\/code><\/pre>\n<p>So I guess WandB is not using the user ID and thus is not able to write in the directory for updating.<br>\nNote that this directory is different from the training one (as it\u2019s random at each init)<\/p>\n<p>Thanks in advance for any help.<br>\nHave a great day.<\/p>",
        "Challenge_closed_time":1635963680198,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635882593147,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-not-using-user-pid-when-updating\/1204",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":12.0,
        "Challenge_reading_time":18.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":22.5241808334,
        "Challenge_title":"WandB not using user PID when updating",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":330.0,
        "Challenge_word_count":164,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>As of now, there isn\u2019t an option to enable that by default. One issue you should be aware of is that if you are currently logging a run when you call <code>wandb sync --clean<\/code> bad things will happen. We\u2019re working on improving the the robustness of these features and will likely support an automatic clean option in the future.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":4.16,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":59.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1640731722280,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":76.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":0.0418091667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I Can not run <code>apt to install git-lfs<\/code> on sagemaker notebook instance. I want to run git commands in my notebook.<\/p>",
        "Challenge_closed_time":1640732355180,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640732204667,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70513398",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":2.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.0418091667,
        "Challenge_title":"I can not install \"git-lfs\" on aws sagemaker notebook instance",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":489.0,
        "Challenge_word_count":30,
        "Platform":"Stack Overflow",
        "Poster_created_time":1596727881047,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":65.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>use the following commands to install git-lfs<\/p>\n<pre><code>!curl -s https:\/\/packagecloud.io\/install\/repositories\/github\/git-lfs\/script.rpm.sh | sudo bash\n\n!sudo yum install git-lfs -y\n\n!git lfs install\n<\/code><\/pre>\n<p>that should make it work<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.9,
        "Solution_reading_time":3.29,
        "Solution_score_count":6.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646219230528,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":3670.8803794444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to build an Azure ML environment with two python packages that I have in Azure Devops.\nFor this I need a workspace connection to Azure Devops. One package is published to an artifact feed and I can access it using the python SDK using a personal access token:<\/p>\n<pre><code>ws.set_connection(name=&quot;ConnectionName&quot;, \n                  category= &quot;PythonFeed&quot;, \n                  target = &quot;https:\/\/pkgs.dev.azure.com\/&quot;, \n                  authType = &quot;PAT&quot;, \n                  value = PAT_TOKEN)\n<\/code><\/pre>\n<p>However, for the other I need to get the package from the git repository in Azure Devops. The documentation of the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.workspace.workspace?view=azure-ml-py#azureml-core-workspace-workspace-set-connection\" rel=\"nofollow noreferrer\">Python SDK<\/a> and the underlying <a href=\"https:\/\/docs.microsoft.com\/en-us\/rest\/api\/azureml\/workspace-connections\/create\" rel=\"nofollow noreferrer\">REST API<\/a> don't give the options for the arguments, only that they need to be strings (see links).<\/p>\n<p>My question: what are the options for the following arguments:<\/p>\n<ul>\n<li>authType<\/li>\n<li>category<\/li>\n<li>valueFormat<\/li>\n<\/ul>\n<p>And what do I need to set for target argument, so that I can connect to the Azure DevOps repository with potentially different authentication?<\/p>",
        "Challenge_closed_time":1659435013616,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646219844250,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71321757",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":18.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":3670.8803794444,
        "Challenge_title":"What are valid Azure ML Workspace connection argument options?",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":93.0,
        "Challenge_word_count":157,
        "Platform":"Stack Overflow",
        "Poster_created_time":1646219230528,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>To get the package from a Azure DevOps git repository you can change the target to the repository URL:<\/p>\n<pre><code>ws.set_connection(\n    name=&quot;ConnectionName&quot;, \n    category = &quot;PythonFeed&quot;,\n    target = &quot;https:\/\/dev.azure.com\/&lt;MY-ORG&gt;\/&lt;MY-PROJECT&gt;\/_git\/&lt;MY-REPO&gt;&quot;, \n    authType = &quot;PAT&quot;, \n    value = &lt;PAT-TOKEN&gt;)\n<\/code><\/pre>\n<p>Note here that there is no user specified in the URL (the standard &quot;clone&quot; URL in Azure DevOps also contains &quot;DevOps-Vx@&quot;).<\/p>\n<p>As for any other options for &quot;authType&quot;, &quot;category&quot; and &quot;valueFormat&quot;, I don't know.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.3,
        "Solution_reading_time":8.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":68.5333333333,
        "Challenge_answer_count":2,
        "Challenge_body":"Firstly I'd like to apologize if this is a dummy question.\r\nI'm following the tutorial to get introduced to kedro mlflow,; after running the command \"kedro mlflow init\" I tried to run the command \"kedro mlflofw ui\" but I get an error:\r\n\r\nINFO     The 'mlflow_tracking_uri' key in mlflow.yml is relative ('server.mlflow_tracking_uri = mlruns'). It is converted to a valid uri: 'file:\/\/\/C:\/Users\/e107338\/PycharmProjects\/mlflow\/kedro-mlflow-example\/mlruns'                                                   kedro_mlflow_config.py:202\r\n\r\nAfter the Traceback I get an error: FileNotFoundErrror\r\n",
        "Challenge_closed_time":1664786016000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664539296000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/361",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":7.26,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":68.5333333333,
        "Challenge_title":"kedro mlflow ui gets a FileNotFoundError",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":null,
        "Challenge_word_count":74,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi, \r\n\r\nI am sorry to see you are experiencing issues. this is not a dummy question, it sounds like a bug. \r\n\r\nI've just ran this: \r\n\r\n```bash\r\nconda create -n km-361 python=3.9 -y\r\nconda activate km-361\r\npip install kedro==0.18.3\r\npip install mlflow==1.29.0\r\npip install kedro-mlflow==0.11.3\r\nkedro new --starter=pandas-iris\r\ncd iris\r\nkedro mlflow init\r\nkedro mlflow ui\r\n```\r\n\r\nthen I opened ``http:\/\/127.0.0.1:5000`` and th UI opened as expected. \r\n\r\nCan you tell me: \r\n- your python version\r\n- your OS\r\n- your ``kedro`` \/ ``mlflow`` \/ ``kedro-mlflow`` version\r\n- the project using\r\n- the exact error message\r\n- check if you have a ``MLFLOW_TRACKING_URI`` environment set It turned out fine  after trying again! Sorry and thanks for your consideration!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":5.1,
        "Solution_reading_time":8.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":107.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1314097464768,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Warsaw, Poland",
        "Answerer_reputation_count":11056.0,
        "Answerer_view_count":544.0,
        "Challenge_adjusted_solved_time":0.0373211111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In <a href=\"https:\/\/neptune.ml\/\" rel=\"nofollow noreferrer\">Neptune<\/a> (this machine learning experiment tracker) is it possible to make it git-aware? I mean - using <code>.gitignore<\/code> for excluded files and saving commit hashes for each run?<\/p>\n\n<p>In particular, when I review an already finished job, can I go directly to GitHub commit?<\/p>",
        "Challenge_closed_time":1503919410423,
        "Challenge_comment_count":0,
        "Challenge_created_time":1495124614783,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1503919276067,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44053141",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":7.9,
        "Challenge_reading_time":4.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":2442.9987888889,
        "Challenge_title":"Can I make Neptune talk to git?",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":140.0,
        "Challenge_word_count":53,
        "Platform":"Stack Overflow",
        "Poster_created_time":1314097464768,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Warsaw, Poland",
        "Poster_reputation_count":11056.0,
        "Poster_view_count":544.0,
        "Solution_body":"<p>Starting form version 2.0 Neptune provides integration with git, see: <a href=\"https:\/\/docs.neptune.ml\/advanced-topics\/git-integration\/\" rel=\"nofollow noreferrer\">https:\/\/docs.neptune.ml\/advanced-topics\/git-integration\/<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":28.7,
        "Solution_reading_time":3.2,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":14.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.5752777778,
        "Challenge_answer_count":0,
        "Challenge_body":"https:\/\/wandb.ai\/alvarobartt\/resnet-pytorch\/runs\/39mhvmwp\/files\/this\/is\/just\/for\/testing",
        "Challenge_closed_time":1658480572000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658474901000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/alvarobartt\/wandbfsspec\/issues\/7",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":35.5,
        "Challenge_reading_time":2.12,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":11.0,
        "Challenge_repo_star_count":6.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":1,
        "Challenge_solved_time":1.5752777778,
        "Challenge_title":"`WandbFileSystem.ls` not working fine with nested directories",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":null,
        "Challenge_word_count":7,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1614873430827,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":169.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":0.3104436111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm in a team using dvc with git to version-control data files. We are using dvc 1.3.1, with the an S3 bucket remote. I'm getting this error when executing <code>dvc fetch<\/code> or <code>dvc pull<\/code> on a colleague's branch:<\/p>\n<pre><code>ERROR: failed to fetch data from the cloud - DVC-file 'C:\\Users\\blah\\Documents\\repo\\data\\processed_data.dvc' format error: extra keys not allowed @ data['outs'][0]['size']\n<\/code><\/pre>\n<p>When I check the dvc file for a cached file with which I have no problem I see this:<\/p>\n<pre><code>md5: ded591aacbe363f0518ceb9c3bc1836b\nouts:\n- md5: efdab20e8b59903b9523cc188ff727e5\n  path: completion_header.p\n  cache: true\n  metric: false\n  persist: false\n<\/code><\/pre>\n<p>but a problematic file only has this:<\/p>\n<pre><code>outs:\n- md5: f4e15187d9a0bbb328e629eabd8d1784.dir\n  size: 112007\n  nfiles: 3\n  path: processed_data\n<\/code><\/pre>\n<p>In all cases, files are added to dvc with the command <code>dvc add %dirname%<\/code>. This is the second time I've seen this on a colleague's branch (2 different people).<\/p>\n<p>Since posting, I have realized that my colleague dvc'd a directory. I have attempted creating the directory first, then calling <code>dvc fetch<\/code>, but get the same error.<\/p>",
        "Challenge_closed_time":1618566517710,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618565400113,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1618826341416,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67122683",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.1,
        "Challenge_reading_time":15.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.3104436111,
        "Challenge_title":"DVC Files Incomplete",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":548.0,
        "Challenge_word_count":164,
        "Platform":"Stack Overflow",
        "Poster_created_time":1348150034832,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Glasgow, UK",
        "Poster_reputation_count":2400.0,
        "Poster_view_count":263.0,
        "Solution_body":"<blockquote>\n<p>In all cases, files are added to dvc with the command dvc add %filename%.<\/p>\n<\/blockquote>\n<p>It seems like there is a high chance that one of the dvc files created in newer versions of dvc and you are trying to operate with an older version. Are all of your colleagues use the same dvc version when adding new files?<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":4.1,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":60.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1294388296987,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Italy",
        "Answerer_reputation_count":32174.0,
        "Answerer_view_count":3457.0,
        "Challenge_adjusted_solved_time":1.2159166667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I use Azure Machine Learning Workspace Notebooks, connected to a DevOps Repository - using terminal git commands to manage my code. I work on different branches, often has to switch back and forth between them.<\/p>\n<p>I reviewed this thread before: <a href=\"https:\/\/stackoverflow.com\/questions\/18615428\/switching-branches-keeps-new-files-from-other-branch\">switching branches keeps new files from other branch<\/a><\/p>\n<p>In my case it does not only keep the files that should be ignored with the use of the gitignore file, but others too.<\/p>\n<p>I tested it with a totally empty branch, that should not have any files in it, checked it out, and it still has files from the branch that I worked with previously. When I check it manually on DevOps, in the repo, the empty branch is actually empty there.<\/p>\n<p>Has anyone seen similar issues?<\/p>",
        "Challenge_closed_time":1622721091067,
        "Challenge_comment_count":2,
        "Challenge_created_time":1622716713767,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67819912",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":8.8,
        "Challenge_reading_time":11.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1.2159166667,
        "Challenge_title":"Azure ML, DevOps: Switching between branches keeps some files from another branch",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":189.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1438890950452,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Nordhavnen, Copenhagen, Denmark",
        "Poster_reputation_count":570.0,
        "Poster_view_count":187.0,
        "Solution_body":"<p>Some files that are tracked in a branch could be not tracked in another. So when you switch back to the &quot;non tracking&quot; branch, that files remain in the file system. Git does not clean stuff that does not track directly. Do not exchange the term not tracked by ignored. Files are not tracked until we &quot;add&quot; them in stage and commit.\nYou could cleanup the working git by running <code>git clean -f -d<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.4,
        "Solution_reading_time":5.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":74.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4736111111,
        "Challenge_answer_count":7,
        "Challenge_body":"> From https:\/\/github.com\/iterative\/dvc.org\/issues\/1743#issuecomment-730726776\r\n\r\n```console\r\n$ git@github.com:iterative\/example-get-started.git\r\n...\r\n$ cd example-get-started\r\n$ dvc fetch\r\nERROR: failed to fetch data from the cloud - Lockfile 'dvc.lock' is corrupted.\r\n```",
        "Challenge_closed_time":1606074573000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606072868000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/iterative\/example-repos-dev\/issues\/17",
        "Challenge_link_count":1,
        "Challenge_participation_count":7,
        "Challenge_readability":16.4,
        "Challenge_reading_time":4.1,
        "Challenge_repo_contributor_count":17.0,
        "Challenge_repo_fork_count":11.0,
        "Challenge_repo_issue_count":154.0,
        "Challenge_repo_star_count":15.0,
        "Challenge_repo_watch_count":14.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.4736111111,
        "Challenge_title":"example-get-started is broken with latest DVC",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":null,
        "Challenge_word_count":26,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"What DVC version do you use? It should be be fixed in the most recent one. 1.9.1 on Windows (latest) the latest version is 1.10 something. if it's not updated on Windows then we have a problem with Win releases cc @efiop  Ah I was wrong, you're right. Works with 1.10.1 which I got from https:\/\/github.com\/iterative\/dvc\/releases\/\r\n\r\nThe problem is that the dvc.org home page download button is stuck at 1.9.1 for all platforms, it seems. Opened iterative\/dvc.org\/issues\/1964, resolving here. I use the latest dvc version [DVC version: 1.11.16 (pip)] and have got the same issue while following the [installation](https:\/\/github.com\/iterative\/example-get-started) steps:\r\nOS: Mac OS Mojave 10.14.6\r\n```\r\n$ dvc pull\r\nEverything is up to date.                                             \r\nERROR: failed to pull data from the cloud - Lockfile 'dvc.lock' is corrupted.\r\n``` @yakushechkin example-get-started is migrating to dvc 2.0, so it is no longer compatible with older dvc versions. We plan on releasing 2.0 on Wednesday. You could try `pip install --pre dvc` to install dvc pre-release. Sorry for the inconvenience.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":5.0,
        "Solution_reading_time":13.16,
        "Solution_score_count":4.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":163.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1530020182168,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"McLean, VA, USA",
        "Answerer_reputation_count":936.0,
        "Answerer_view_count":153.0,
        "Challenge_adjusted_solved_time":19.5819902778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a Sagemaker notebook that I would like to move to a GitHub repository. I thought perhaps I should download the files locally, then I can easily push to git. But I cannot figure out how to download the folder to my local computer. Then I though, perhaps there is a way using the AWS CLI to move directly from Sagemaker to git? I've made many google searches that are unable to answer my question.<\/p>",
        "Challenge_closed_time":1591101932272,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591031437107,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62137347",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.3,
        "Challenge_reading_time":5.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":19.5819902778,
        "Challenge_title":"How to get a Sagemaker notebook onto GitHub?",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":2743.0,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Poster_created_time":1530020182168,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"McLean, VA, USA",
        "Poster_reputation_count":936.0,
        "Poster_view_count":153.0,
        "Solution_body":"<p>Building off the answer given by @mokugo-devops, I was able to link my existing notebook to my GitHub account.<\/p>\n\n<p>First, I followed the directions posted in the link provided in his answer to set up my GitHub repo with my AWS account on the CLI, then I used the following command to edit my existing notebook:<\/p>\n\n<pre><code>aws sagemaker  update-notebook-instance \\\n--notebook-instance-name &lt;value&gt; \\\n--default-code-repository &lt;saved-github-repo-name-in-AWS&gt;\n<\/code><\/pre>\n\n<p>my notebook instance is now linked my GitHub repo.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":18.7,
        "Solution_reading_time":6.98,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":73.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1512770138847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":542.3006655556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Can I specify SageMaker estimator's entry point script to be in a subdirectory? So far, it fails for me. Here is what I want to do:<\/p>\n\n<pre><code>sklearn = SKLearn(\n    entry_point=\"RandomForest\/my_script.py\",\n    source_dir=\"..\/\",\n    hyperparameters={...\n<\/code><\/pre>\n\n<p>I want to do this so I don't have to break my directory structure. I have some modules, which I use in several sagemaker projects, and each project lives in its own directory:<\/p>\n\n<pre><code>my_git_repo\/\n\n  RandomForest\/\n    my_script.py\n    my_sagemaker_notebook.ipynb\n\n  TensorFlow\/\n    my_script.py\n    my_other_sagemaker_notebook.ipynb\n\nmodule_imported_in_both_scripts.py\n<\/code><\/pre>\n\n<p>If I try to run this, SageMaker fails because it seems to parse the name of the entry point script to make a module name out of it, and it does not do a good job:<\/p>\n\n<pre><code>\/usr\/bin\/python3 -m RandomForest\/my_script --bootstrap True --case nf_2 --max_features 0.5 --min_impurity_decrease 5.323785009485933e-06 --model_name model --n_estimators 455 --oob_score True\n\n...\n\n\/usr\/bin\/python3: No module named RandomForest\/my_script\n\n<\/code><\/pre>\n\n<p>Anyone knows a way around this other than putting <code>my_script.py<\/code> in the <code>source_dir<\/code>?<\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/54314876\/aws-sagemaker-sklearn-entry-point-allow-multiple-script\">Related to this question<\/a><\/p>",
        "Challenge_closed_time":1571939829096,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569987546700,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58194899",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.1,
        "Challenge_reading_time":18.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":542.3006655556,
        "Challenge_title":"AWS SageMaker SKLearn entry point in a subdirectory?",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":843.0,
        "Challenge_word_count":158,
        "Platform":"Stack Overflow",
        "Poster_created_time":1368039192832,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1245.0,
        "Poster_view_count":109.0,
        "Solution_body":"<p>Unfortunately, this is a gap in functionality. There is some related work in <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/pull\/941\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/pull\/941<\/a> which should also solve this issue, but for now, you do need to put <code>my_script.py<\/code> in <code>source_dir<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.8,
        "Solution_reading_time":4.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":34.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":127.7283333333,
        "Challenge_answer_count":0,
        "Challenge_body":"The current implementation of the rename action (to be triggered when the \"rename\" section of the DVC diff contains elements) includes the actual rename of the base image using the shutils' mv command. \r\n\r\n```python\r\nguard_that_base_image_exists(base_filename_old)\r\ncreate_output_folder(base_filename_new)\r\nmove(f\"{base_filename_old}\", f\"{base_filename_new}\")\r\n```\r\n\r\nThis rename is to be committed afterwards so that the rename of the base image is applied to the main branch.\r\n\r\nHowever, this approach is invalid:\r\n\r\n- If only the actual file is renamed, when a DVC pull is performed, the file with the previous name will be pulled. We will get two identical files with different names.\r\n- Nor can we just rename the pointer (.dvc file), as the pointer file name is irrelevant to DVC. The _path_ property inside the pointer is what determines the filename of the pulled file.\r\n\r\nThe right, convenient way to implement the file rename action is using the **dvc rename** command that performs all these actions:\r\n\r\n- Rename the actual file\r\n- Rename the pointer\r\n- Update the _path_ property\r\n\r\nFor consistency, we should use our DVC wrapper. If the move command is not wrapped there, we can do it as part of this issue.",
        "Challenge_closed_time":1643122571000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642662749000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Nautilus-Cyberneering\/nautilus-librarian\/issues\/77",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":9.7,
        "Challenge_reading_time":15.35,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":1.0,
        "Challenge_repo_issue_count":112.0,
        "Challenge_repo_star_count":3.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":127.7283333333,
        "Challenge_title":"Use DVC move instead of system's mv in rename action",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":null,
        "Challenge_word_count":194,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.0219444444,
        "Challenge_answer_count":0,
        "Challenge_body":"When running the `fds add` command for data files it tries to add them to DVC tracking but fails.\r\n\r\nIn my case I tried to add the raw-data directory that contains the following image files:\r\n```\r\n$ tree data\/raw-data\r\ndata\/raw-data\r\n\u251c\u2500\u2500 IM-0001-0001.jpeg\r\n\u251c\u2500\u2500 IM-0003-0001.jpeg\r\n\u251c\u2500\u2500 IM-0005-0001.jpeg\r\n\u251c\u2500\u2500 IM-0006-0001.jpeg\r\n\u251c\u2500\u2500 IM-0007-0001.jpeg\r\n\u251c\u2500\u2500 IM-0009-0001.jpeg\r\n\u251c\u2500\u2500 IM-0010-0001.jpeg\r\n\u251c\u2500\u2500 IM-0011-0001-0001.jpeg\r\n\u251c\u2500\u2500 IM-0011-0001-0002.jpeg\r\n\u251c\u2500\u2500 IM-0011-0001.jpeg\r\n\u251c\u2500\u2500 IM-0013-0001.jpeg\r\n\u251c\u2500\u2500 IM-0015-0001.jpeg\r\n\u251c\u2500\u2500 IM-0016-0001.jpeg\r\n\u251c\u2500\u2500 IM-0017-0001.jpeg\r\n....\r\n```\r\nBut fds failed to execute the add command:\r\n```\r\n$ fds add data\/raw-data\r\n========== Make your selection, Press \"h\" for help ==========\r\n\r\nDVC add failed to execute\r\n```",
        "Challenge_closed_time":1622139051000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622120972000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/39",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":7.7,
        "Challenge_reading_time":9.52,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":139.0,
        "Challenge_repo_star_count":357.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":5.0219444444,
        "Challenge_title":"Fails to add files to DVC tracking",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":null,
        "Challenge_word_count":81,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.6293205556,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>A very bad practice, I know. But for a part of my experiment, the main file wasn\u2019t updated so it ignored some configs. Is it possible to manually add them into the runs, now that the runs are finished?<\/p>\n<p>If it helps, I don\u2019t need to add new entries to the config. I just need to add to an existing string in the config.<\/p>",
        "Challenge_closed_time":1650060023990,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650054158436,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/can-you-edit-config-of-a-run-after-it-finishes\/2247",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":5.7,
        "Challenge_reading_time":4.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.6293205556,
        "Challenge_title":"Can you edit config of a run after it finishes?",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":149.0,
        "Challenge_word_count":73,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/aceticia\">@aceticia<\/a>,<\/p>\n<p>This is absolutely possible! Here is an example in our docs on how to do this : <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide#update-config-for-an-existing-run\">https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide#update-config-for-an-existing-run<\/a><\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":24.1,
        "Solution_reading_time":4.94,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":24.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1618.0322222222,
        "Challenge_answer_count":2,
        "Challenge_body":"Experience is broken since every DVC command changes `.gitignore` now - makes it very annoying to jump between branches.",
        "Challenge_closed_time":1588739140000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582914224000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/iterative\/example-repos-dev\/issues\/12",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.9,
        "Challenge_reading_time":2.2,
        "Challenge_repo_contributor_count":17.0,
        "Challenge_repo_fork_count":11.0,
        "Challenge_repo_issue_count":154.0,
        "Challenge_repo_star_count":15.0,
        "Challenge_repo_watch_count":14.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1618.0322222222,
        "Challenge_title":"need to rebuild get-started with the latest DVC version",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":null,
        "Challenge_word_count":26,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This will be done as part of iterative\/dvc.org\/issues\/599. Shall we close here? Yep.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.6,
        "Solution_reading_time":1.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":10.043535,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I use DVC to track my media files. I use MacOS and I want\".DS_Store\" files to be ignored by DVC. According to DVC documentation I can achieve it with  <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvcignore\" rel=\"nofollow noreferrer\">.dvcignore<\/a>. I created <code>.dvcignore<\/code> file with \".DS_Store\" rule. However every time \".DS_Store\" is created <code>dvc status<\/code> still says that content has changed<\/p>\n\n<p>Here is the little test to reproduce my issue:<\/p>\n\n<pre><code>$ git init\n$ dvc init\n\n# create directory to store data\n# and track it's content with DVC\n$ mkdir data\n$ dvc add data\n\n# Ignore .DS_Store files created by MacOS\n$ echo \".DS_Store\" &gt; .dvcignore\n\n# create .DS_Store in data dir\n$ touch \"data\/.DS_Store\"\n<\/code><\/pre>\n\n<p>If I understand DVC documentation correctly then <code>dvc status<\/code> should print something like \"Pipeline is up to date. Nothing to reproduce\". However <code>dvc status<\/code> gives me:<\/p>\n\n<pre><code>data.dvc:\n        changed outs:\n                modified:           data\n<\/code><\/pre>\n\n<p>How I can really ignore \".DS_Store\" files?<\/p>\n\n<p><strong>UPDATE:<\/strong> The .dvcignore support noticeably improved in latest versions and the problem is no more relevant.<\/p>",
        "Challenge_closed_time":1559758177416,
        "Challenge_comment_count":2,
        "Challenge_created_time":1559722020690,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1568948355943,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56456463",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":9.0,
        "Challenge_reading_time":15.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":10.043535,
        "Challenge_title":"Unable to ignore .DS_Store files in DVC",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":326.0,
        "Challenge_word_count":163,
        "Platform":"Stack Overflow",
        "Poster_created_time":1522254698710,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Russia",
        "Poster_reputation_count":784.0,
        "Poster_view_count":77.0,
        "Solution_body":"<p>The current implementation of <code>.dvcignore<\/code> is very limited. Read more on it <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvcignore\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Please, mention that you are interested in this feature here - <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1876\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc\/issues\/1876<\/a>. That would help our team to prioritize issues properly.<\/p>\n\n<p>The possible workaround for now would be to use one of these approaches - <a href=\"https:\/\/stackoverflow.com\/questions\/18015978\/how-to-stop-creating-ds-store-on-mac\">How to stop creating .DS_Store on Mac?<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":13.9,
        "Solution_reading_time":8.7,
        "Solution_score_count":3.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":60.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1620132280447,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":146.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":13.6342236111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using DAGsHub storage as a remote and running into the following error message (when trying to DVC pull):<\/p>\n<blockquote>\n<p>ERROR: Lockfile 'bias_tagging_model\/dvc.lock' is corrupted.<\/p>\n<\/blockquote>\n<p>I thought I might have messed something up, but when cloning the git repo again and DVC pulling I am still running into this.\nThe data looks ok when viewed in the browser.\nIf you have any ideas, I would appreciate your help!<\/p>",
        "Challenge_closed_time":1620292401492,
        "Challenge_comment_count":1,
        "Challenge_created_time":1620243318287,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67407702",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.6,
        "Challenge_reading_time":5.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":13.6342236111,
        "Challenge_title":"Corrupted dvc.lock",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":362.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1620132740443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"New York, NY, USA",
        "Poster_reputation_count":75.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Usually, the reason for this error is the DVC version.<\/p>\n<p>If the dvc.lock file has a DVC 2.* schema and you are using a lower version, it will throw this error.<\/p>\n<p>Upgrade your DVC version, and it should work.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.8,
        "Solution_reading_time":2.73,
        "Solution_score_count":4.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":39.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1541523185320,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Ottawa, ON, Canada",
        "Answerer_reputation_count":1003.0,
        "Answerer_view_count":56.0,
        "Challenge_adjusted_solved_time":1.5810183334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>What is the difference between these two? We used git-lfs in my previous job and we are starting to use dvc alongside git in my current one. They both place some kind of index instead of file and can be downloaded on demand. Has dvc some improvements over the former one?<\/p>",
        "Challenge_closed_time":1571925277763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1571919586097,
        "Challenge_favorite_count":5.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58541260",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.2,
        "Challenge_reading_time":3.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":27.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.5810183334,
        "Challenge_title":"Difference between git-lfs and dvc",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":6255.0,
        "Challenge_word_count":55,
        "Platform":"Stack Overflow",
        "Poster_created_time":1373630643248,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":382.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>DVC is a better replacement for <code>git-lfs<\/code>. <\/p>\n\n<p>Unlike git-lfs, DVC doesn't require installing a dedicated server; It can be used on-premises (NAS, SSH, for example) or with any major cloud provider (S3, Google Cloud, Azure).<\/p>\n\n<p>For more information: <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-and-model-files-versioning\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/use-cases\/data-and-model-files-versioning<\/a><\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.7,
        "Solution_reading_time":5.71,
        "Solution_score_count":10.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":42.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11.5141666667,
        "Challenge_answer_count":2,
        "Challenge_body":"Is there any standard for ML S3 dataset tracking or versioning?\nBasically, what setup allows to track a given model training execution to a given dataset?\nInterested to hear about proven or state-of-the-art ideas",
        "Challenge_closed_time":1549437509000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1549396058000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668615929200,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUhYC1EJQuSWqpwTByAtB_fg\/s3-dataset-versioning-with-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":3.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":11.5141666667,
        "Challenge_title":"S3 Dataset versioning with SageMaker?",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":754.0,
        "Challenge_word_count":38,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Unfortunately, managing versions of datasets and which models used them is not embedded in SageMaker. But, you can use SageMaker search to manage the differences in data location between experiments. In that case, if your dataset isn't too big, my recommendation will be to create a standard for data structure in S3. i.e. for each new dataset, create a new prefix in S3 with your logic.\nUsing SageMaker search you'll be able to find all your jobs and compare between datasets.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925558875,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":5.83,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":81.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":59.2573102778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p><strong>Issue<\/strong>  <br \/>\nI am trying prepare and then submit a new experiment to Azure Machine Learning from an Azure Function in Python. I therefore register a new dataset for my Azure ML workspace, which contains the training data for my ML model using <code>dataset.register(...<\/code>. However, when I try to create this dataset with the following line of code  <\/p>\n<pre><code>dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths)\n<\/code><\/pre>\n<p>then I get a <code>Failure Exception: OSError: [Errno 30] Read-only file system ...<\/code>.  <\/p>\n<p><strong>Ideas<\/strong>  <\/p>\n<ol>\n<li> I know that I shouldn't write to the file system from within an Azure function if possible. But I actually don't want to write anything to the local file system. I only want to create the dataset as a reference to my blob storage under <code>datastore_path<\/code> and then register this to my Azure Machine Learning workspace. But it seems that the method <code>from_delimited_files<\/code> is trying to write to the file system anyway (maybe some caching?).  <\/li>\n<li> I also know that there is a temp folder in which writing temporary files is permitted. However, I belive I cannot really control where this method is writing data. I already tried changing the current working directory to this temp folder just before the function call using <code>os.chdir(tempfile.gettempdir())<\/code>, but that didn't help.  <\/li>\n<\/ol>\n<p>Any other ideas? I don't think I am doing something particularly unusually...  <\/p>\n<p><strong>Details<\/strong>  <br \/>\nI am using python 3.7 and azureml-sdk 1.9.0 and I can run the python script locally without problems. I currently deploy from VSCode using the Azure Functions extension version 0.23.0 (and an Azure DevOps pipeline for CI\/CD).  <\/p>\n<p>Here is my full stack trace:  <\/p>\n<pre><code>Microsoft.Azure.WebJobs.Host.FunctionInvocationException: Exception while executing function: Functions.HttpTrigger_Train\n ---&gt; Microsoft.Azure.WebJobs.Script.Workers.Rpc.RpcException: Result: Failure\nException: OSError: [Errno 30] Read-only file system: '\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/bin\/deps.lock'\nStack:   File &quot;\/azure-functions-host\/workers\/python\/3.7\/LINUX\/X64\/azure_functions_worker\/dispatcher.py&quot;, line 345, in _handle__invocation_request\n    self.__run_sync_func, invocation_id, fi.func, args)\n  File &quot;\/usr\/local\/lib\/python3.7\/concurrent\/futures\/thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;\/azure-functions-host\/workers\/python\/3.7\/LINUX\/X64\/azure_functions_worker\/dispatcher.py&quot;, line 480, in __run_sync_func\n    return func(**params)\n  File &quot;\/home\/site\/wwwroot\/HttpTrigger_Train\/__init__.py&quot;, line 11, in main\n    train()\n  File &quot;\/home\/site\/wwwroot\/shared_code\/train.py&quot;, line 70, in train\n    dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/data\/_loggerfactory.py&quot;, line 126, in wrapper\n    return func(*args, **kwargs)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/data\/dataset_factory.py&quot;, line 308, in from_delimited_files\n    quoting=support_multi_line)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/readers.py&quot;, line 100, in read_csv\n    df = Dataflow._path_to_get_files_block(path, archive_options)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/dataflow.py&quot;, line 2387, in _path_to_get_files_block\n    return datastore_to_dataflow(path)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_datastore_helper.py&quot;, line 41, in datastore_to_dataflow\n    datastore, datastore_value = get_datastore_value(source)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_datastore_helper.py&quot;, line 83, in get_datastore_value\n    _set_auth_type(workspace)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_datastore_helper.py&quot;, line 134, in _set_auth_type\n    get_engine_api().set_aml_auth(SetAmlAuthMessageArgument(AuthType.SERVICEPRINCIPAL, json.dumps(auth)))\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/api.py&quot;, line 18, in get_engine_api\n    _engine_api = EngineAPI()\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/api.py&quot;, line 55, in __init__\n    self._message_channel = launch_engine()\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/engine.py&quot;, line 300, in launch_engine\n    dependencies_path = runtime.ensure_dependencies()\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/runtime.py&quot;, line 141, in ensure_dependencies\n    with _FileLock(deps_lock_path, raise_on_timeout=timeout_exception):\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/runtime.py&quot;, line 113, in __enter__\n    self.acquire()\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/runtime.py&quot;, line 72, in acquire\n    self.lockfile = os.open(self.lockfile_path, os.O_CREAT | os.O_EXCL | os.O_RDWR)\n\n   at Microsoft.Azure.WebJobs.Script.Description.WorkerFunctionInvoker.InvokeCore(Object[] parameters, FunctionInvocationContext context) in \/src\/azure-functions-host\/src\/WebJobs.Script\/Description\/Workers\/WorkerFunctionInvoker.cs:line 85\n   at Microsoft.Azure.WebJobs.Script.Description.FunctionInvokerBase.Invoke(Object[] parameters) in \/src\/azure-functions-host\/src\/WebJobs.Script\/Description\/FunctionInvokerBase.cs:line 85\n   at Microsoft.Azure.WebJobs.Script.Description.FunctionGenerator.Coerce[T](Task`1 src) in \/src\/azure-functions-host\/src\/WebJobs.Script\/Description\/FunctionGenerator.cs:line 225\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionInvoker`2.InvokeAsync(Object instance, Object[] arguments) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionInvoker.cs:line 52\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.InvokeAsync(IFunctionInvoker invoker, ParameterHelper parameterHelper, CancellationTokenSource timeoutTokenSource, CancellationTokenSource functionCancellationTokenSource, Boolean throwOnTimeout, TimeSpan timerInterval, IFunctionInstance instance) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 587\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithWatchersAsync(IFunctionInstanceEx instance, ParameterHelper parameterHelper, ILogger logger, CancellationTokenSource functionCancellationTokenSource) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 532\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithLoggingAsync(IFunctionInstanceEx instance, ParameterHelper parameterHelper, IFunctionOutputDefinition outputDefinition, ILogger logger, CancellationTokenSource functionCancellationTokenSource) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 470\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithLoggingAsync(IFunctionInstanceEx instance, FunctionStartedMessage message, FunctionInstanceLogEntry instanceLogEntry, ParameterHelper parameterHelper, ILogger logger, CancellationToken cancellationToken) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 278\n   --- End of inner exception stack trace ---\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithLoggingAsync(IFunctionInstanceEx instance, FunctionStartedMessage message, FunctionInstanceLogEntry instanceLogEntry, ParameterHelper parameterHelper, ILogger logger, CancellationToken cancellationToken) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 325\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.TryExecuteAsyncCore(IFunctionInstanceEx functionInstance, CancellationToken cancellationToken) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 117\n<\/code><\/pre>",
        "Challenge_closed_time":1597616477787,
        "Challenge_comment_count":1,
        "Challenge_created_time":1597403151470,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/67126\/failure-exception-oserror-(errno-30)-read-only-fil",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":25.3,
        "Challenge_reading_time":114.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":73,
        "Challenge_solved_time":59.2573102778,
        "Challenge_title":"\u201cFailure Exception: OSError: [Errno 30] Read-only file system\u201d when using AzureML in Python Azure Function",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":null,
        "Challenge_word_count":575,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The issue was an incompatible OS version in my virtual environment.    <\/p>\n<p>A huge thanks goes to <a href=\"https:\/\/learn.microsoft.com\/answers\/users\/111253\/pramodvalavala-msft.html\">PramodValavala-MSFT<\/a> for his idea to create a docker container! Following his suggestion, I suddenly got the following error message for the  <code>dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths)<\/code> command:    <\/p>\n<blockquote>\n<p>Exception: NotImplementedError: Unsupported Linux distribution debian 10.    <\/p>\n<\/blockquote>\n<p>which reminded me of the following warning in the azure machine learning documentation:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/17829-image.png?platform=QnA\" alt=\"17829-image.png\" \/>    <\/p>\n<p>Choosing the predefined docker image <code>2.0-python3.7<\/code> (running Debian 9) instead of  <code>3.0-python3.7<\/code> (running Debian 10) solved the issue (see <a href=\"https:\/\/hub.docker.com\/_\/microsoft-azure-functions-python\">https:\/\/hub.docker.com\/_\/microsoft-azure-functions-python<\/a>).    <\/p>\n<p>I suspect that the default virtual environment, which I was using originally, also ran on an incompatible OS.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":16.0,
        "Solution_reading_time":15.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":113.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2386394444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am looking for sample of notebook\/SDK, but this link is not working at all. Any new repo for reference? <a href=\"https:\/\/learn.microsoft.com\/en-us\/samples\/azure\/azureml-examples\/azure-machine-learning-examples\/\">https:\/\/learn.microsoft.com\/en-us\/samples\/azure\/azureml-examples\/azure-machine-learning-examples\/<\/a><\/p>",
        "Challenge_closed_time":1669743760392,
        "Challenge_comment_count":0,
        "Challenge_created_time":1669742901290,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1109020\/azure-machine-learning-samples-404",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.5,
        "Challenge_reading_time":4.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.2386394444,
        "Challenge_title":"Azure machine learning samples 404",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":null,
        "Challenge_word_count":26,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=79877fab-184b-4267-bfdd-49ea0b34bfe1\">@Yadama Kenzan  <\/a>     <\/p>\n<p>Thanks for reporting this issue, is there any place you got the link or it's from the web search? This link has been deprecated.    <\/p>\n<p>Please see this repo for SDK V2 samples\/ CLI V2 samples - <a href=\"https:\/\/github.com\/Azure\/azureml-examples\">https:\/\/github.com\/Azure\/azureml-examples<\/a>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/265362-image.png?platform=QnA\" alt=\"265362-image.png\" \/>    <\/p>\n<p>Please let me know where this link is from so that I can fix the resource as well.     <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.4,
        "Solution_reading_time":9.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":89.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.3909094444,
        "Challenge_answer_count":2,
        "Challenge_body":"I have an issue while getting Catboost image URI. It is a function for generating ECR image URIs for pre-built SageMaker Docker images. Here is my code catboost_container = sagemaker.image_uris.retrieve(\"catboost\", my_region, \"latest\")",
        "Challenge_closed_time":1658369922828,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658343315554,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668463143111,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU3HFACW88SuKcGZ2izeOsuA\/filenotfounderror-errno-2-no-such-file-or-directory-home-ec2-user-anaconda3-envs-python3-lib-python3-8-site-packages-sagemaker-image-uri-config-catboost-json",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":14.6,
        "Challenge_reading_time":5.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":7.3909094444,
        "Challenge_title":"FileNotFoundError: [Errno 2] No such file or directory: '\/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.8\/site-packages\/sagemaker\/image_uri_config\/catboost.json'",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":164.0,
        "Challenge_word_count":39,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"As illustrated [here in the docs for the algorithm](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/catboost.html#catboost-modes), the parameters for retrieving this URI are a bit different: It's more like using the new JumpStart models (if you're familiar with that) than the old-style pre-built algorithms.\n\n```\ntrain_model_id, train_model_version, train_scope = \"catboost-classification-model\", \"*\", \"training\"\ntraining_instance_type = \"ml.m5.xlarge\"\n\n# Retrieve the docker image\ntrain_image_uri = image_uris.retrieve(\n    region=None,\n    framework=None,\n    model_id=train_model_id,\n    model_version=train_model_version,\n    image_scope=train_scope,\n    instance_type=training_instance_type\n)\n```\n\nI tested the above snippet from the doc page on SageMaker Studio and it worked OK. If you still see errors, it's likely your SageMaker Python SDK version is outdated (which can happen if for example you don't restart SM Studio apps or SM Notebook Instances regularly). Can check with `sagemaker.__version__` and upgrade with `!pip install --upgrade sagemaker` if needed.",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1658369922830,
        "Solution_link_count":1.0,
        "Solution_readability":13.0,
        "Solution_reading_time":13.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":116.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1662022756172,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":0.5066644444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using DVC extension in VScode inside a python project. The problem is that dvc shows files not tracked by dvc in the source control panel! As in the following picture.\nDVC track only data folder and not the src folder. How can I fix it? Have you also encountered these problems?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/sn8YY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sn8YY.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1662022756172,
        "Challenge_comment_count":1,
        "Challenge_created_time":1662017466370,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1662021092696,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73565648",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":6.1,
        "Challenge_reading_time":6.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":1.4693894444,
        "Challenge_title":"DVC shows files not tracked in source control in visual studio code",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":39.0,
        "Challenge_word_count":73,
        "Platform":"Stack Overflow",
        "Poster_created_time":1580668804396,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":498.0,
        "Poster_view_count":66.0,
        "Solution_body":"<p>The files shown are completely untracked. They are shown in both SCM trees so you can add them to either Git or DVC using inline actions.\nOnce the files are tracked by one of the tools they should only show up under the appropriate tree.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1662022916688,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":2.94,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":45.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":105.6016441667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm having a problem trying to run &quot;dvc pull&quot; on Google Colab. I have two repositories (let's call them A and B) where repository A is for my machine learning codes and repository B is for my dataset.<\/p>\n<p>I've successfully pushed my dataset to repository B with DVC (using gdrive as my remote storage) and I also managed to successfully run &quot;dvc import&quot; (as well as &quot;dvc pull\/update&quot;) on my local project of repository A.<\/p>\n<p>The problem comes when I use colab to run my project. So what I did was the following:<\/p>\n<ol>\n<li>Created a new notebook on colab<\/li>\n<li>Successfully git-cloned my machine learning project (repository A)<\/li>\n<li>Ran &quot;!pip install dvc&quot;<\/li>\n<li>Ran &quot;!dvc pull -v&quot; (This is what causes the error)<\/li>\n<\/ol>\n<p>On step 4, I got the error (this is the full stack trace. Note that I changed the repo URL in the stack trace for confidentiality reasons)<\/p>\n<pre><code>2022-03-08 08:53:31,863 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/config.local' to gitignore file.\n2022-03-08 08:53:31,866 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/tmp' to gitignore file.\n2022-03-08 08:53:31,866 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/cache' to gitignore file.\n2022-03-08 08:53:31,916 DEBUG: Creating external repo https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git@3a3f4559efabff8ec74486da39b86688d1b98d75\n2022-03-08 08:53:31,916 DEBUG: erepo: git clone 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to a temporary dir\nEverything is up to date.\n2022-03-08 08:53:32,154 ERROR: failed to pull data from the cloud - Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/backend\/gitpython.py&quot;, line 185, in clone\n    tmp_repo = clone_from()\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/repo\/base.py&quot;, line 1148, in clone_from\n    return cls._clone(git, url, to_path, GitCmdObjectDB, progress, multi_options, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/repo\/base.py&quot;, line 1079, in _clone\n    finalize_process, decode_streams=False)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/cmd.py&quot;, line 176, in handle_process_output\n    return finalizer(process)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/util.py&quot;, line 386, in finalize_process\n    proc.wait(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/cmd.py&quot;, line 502, in wait\n    raise GitCommandError(remove_password_if_present(self.args), status, errstr)\ngit.exc.GitCommandError: Cmd('git') failed due to: exit code(128)\n  cmdline: git clone -v --no-single-branch --progress https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git \/tmp\/tmp2x7y7xgedvc-clone\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/scm.py&quot;, line 104, in clone\n    return Git.clone(url, to_path, progress=pbar.update_git, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/__init__.py&quot;, line 121, in clone\n    backend.clone(url, to_path, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/backend\/gitpython.py&quot;, line 190, in clone\n    raise CloneError(url, to_path) from exc\nscmrepo.exceptions.CloneError: Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/command\/data_sync.py&quot;, line 41, in run\n    glob=self.args.glob,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/pull.py&quot;, line 38, in pull\n    run_cache=run_cache,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/fetch.py&quot;, line 50, in fetch\n    revs=revs,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 437, in used_objs\n    with_deps=with_deps,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/index.py&quot;, line 190, in used_objs\n    filter_info=filter_info,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/stage\/__init__.py&quot;, line 660, in get_used_objs\n    for odb, objs in out.get_used_objs(*args, **kwargs).items():\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/output.py&quot;, line 918, in get_used_objs\n    return self.get_used_external(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/output.py&quot;, line 973, in get_used_external\n    return dep.get_used_objs(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/dependency\/repo.py&quot;, line 94, in get_used_objs\n    used, _ = self._get_used_and_obj(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/dependency\/repo.py&quot;, line 108, in _get_used_and_obj\n    locked=locked, cache_dir=local_odb.cache_dir\n  File &quot;\/usr\/lib\/python3.7\/contextlib.py&quot;, line 112, in __enter__\n    return next(self.gen)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 35, in external_repo\n    path = _cached_clone(url, rev, for_write=for_write)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 155, in _cached_clone\n    clone_path, shallow = _clone_default_branch(url, rev, for_write=for_write)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/decorators.py&quot;, line 45, in wrapper\n    return deco(call, *dargs, **dkwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/flow.py&quot;, line 274, in wrap_with\n    return call()\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/decorators.py&quot;, line 66, in __call__\n    return self._func(*self._args, **self._kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 220, in _clone_default_branch\n    git = clone(url, clone_path)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/scm.py&quot;, line 106, in clone\n    raise CloneError(str(exc))\ndvc.scm.CloneError: Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n------------------------------------------------------------\n2022-03-08 08:53:32,161 DEBUG: Analytics is enabled.\n2022-03-08 08:53:32,192 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmp4x5js0dk']'\n2022-03-08 08:53:32,193 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmp4x5js0dk']'\n<\/code><\/pre>\n<p>And btw this is how I cloned my git repository (repo A)<\/p>\n<pre><code>!git config - global user.name &quot;Zharfan&quot;\n!git config - global user.email &quot;zharfan@myemail.com&quot;\n!git clone https:\/\/&lt;MyTokenName&gt;:&lt;MyToken&gt;@link-to-my-repo-A.git\n<\/code><\/pre>\n<p>Does anyone know why? Any help would be greatly appreciated. Thank you in advance!<\/p>",
        "Challenge_closed_time":1647022114532,
        "Challenge_comment_count":12,
        "Challenge_created_time":1646641948613,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1652856778060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71378280",
        "Challenge_link_count":7,
        "Challenge_participation_count":13,
        "Challenge_readability":13.5,
        "Challenge_reading_time":96.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":76,
        "Challenge_solved_time":105.6016441667,
        "Challenge_title":"Error with DVC on Google Colab - dvc.scm.CloneError: Failed to clone repo",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":707.0,
        "Challenge_word_count":614,
        "Platform":"Stack Overflow",
        "Poster_created_time":1525227015312,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>To summarize the discussion in the comments thread.<\/p>\n<p>Most likely it's happening since DVC can't get access to a private repo on GitLab. (The error message is obscure and should be fixed.)<\/p>\n<p>The same way you would not be able to run:<\/p>\n<pre><code>!git clone https:\/\/gitlab.com\/org\/&lt;private-repo&gt;\n<\/code><\/pre>\n<p>It also returns a pretty obscure error:<\/p>\n<pre><code>Cloning into '&lt;private-repo&gt;'...\nfatal: could not read Username for 'https:\/\/gitlab.com': No such device or address\n<\/code><\/pre>\n<p>(I think it's something related to how tty is setup in Colab?)<\/p>\n<p>The best approach to solve this is to use SSH like described <a href=\"https:\/\/medium.com\/@sadiaafrinpurba\/how-to-clone-private-github-repo-in-google-colab-using-ssh-77384cfef18f\" rel=\"nofollow noreferrer\">here<\/a> for example.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":9.5,
        "Solution_reading_time":10.72,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":99.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":485.9309611111,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I was wondering if it would be possible to have a simple W&amp;B\/wandb badge to display on GitHub repositories, meaning: \u201cThis repository supports experiment tracking with wandb\u201d.<\/p>\n<p>By badge, I mean like below. The official wandb client repository for example uses pypi, codecov and circleci badges.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/2a8eb240dfb428a627280e4311324e7c0ec92188.png\" data-download-href=\"\/uploads\/short-url\/64tMj9Dw36m9P2OBKPlPRcyIuBq.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/2a8eb240dfb428a627280e4311324e7c0ec92188_2_690x151.png\" alt=\"image\" data-base62-sha1=\"64tMj9Dw36m9P2OBKPlPRcyIuBq\" width=\"690\" height=\"151\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/2a8eb240dfb428a627280e4311324e7c0ec92188_2_690x151.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/2a8eb240dfb428a627280e4311324e7c0ec92188.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/2a8eb240dfb428a627280e4311324e7c0ec92188.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/2a8eb240dfb428a627280e4311324e7c0ec92188_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">775\u00d7170 36 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Challenge_closed_time":1650663975454,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648914623994,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/feature-request-w-b-badge-or-shield-for-github-repositories\/2181",
        "Challenge_link_count":6,
        "Challenge_participation_count":4,
        "Challenge_readability":26.2,
        "Challenge_reading_time":24.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":485.9309611111,
        "Challenge_title":"[Feature Request] W&B badge or shield for GitHub repositories",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":275.0,
        "Challenge_word_count":93,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/dealer56\">@dealer56<\/a>,<\/p>\n<p>I discussed this with some folks, and looks like we already have a <a href=\"https:\/\/img.shields.io\/badge\/Weights_&amp;_Biases-FFCC33?style=for-the-badge&amp;logo=WeightsAndBiases&amp;logoColor=black\" rel=\"noopener nofollow ugc\">badge<\/a> for something like this. You should also be able to generate such badges through <a href=\"http:\/\/shields.io\" rel=\"noopener nofollow ugc\">shields.io<\/a>, and we plan to have a tutorial in the future on how to use badges to present a metric on your repo.<\/p>\n<p>I\u2019ll link the tutorial once it is out.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.7,
        "Solution_reading_time":8.21,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":73.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":6.8516666667,
        "Challenge_answer_count":1,
        "Challenge_body":"wandb api key not configured for github ci\r\n\r\nhttps:\/\/github.com\/johannespischinger\/senti_anal\/runs\/4808536333?check_suite_focus=true",
        "Challenge_closed_time":1642173581000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642148915000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/johannespischinger\/senti_anal\/issues\/51",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":2.13,
        "Challenge_repo_contributor_count":2.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":95.0,
        "Challenge_repo_star_count":2.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":6.8516666667,
        "Challenge_title":"wandb api key for github ci",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":null,
        "Challenge_word_count":14,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"\/settings\/secrets\r\n\r\n```\r\njobs:\r\n  weekday_job:\r\n    runs-on: ubuntu-latest\r\n    env:\r\n      DAY_OF_WEEK: Mon\r\n    steps:\r\n      - name: \"Hello world when it's Monday\"\r\n        if: ${{ env.DAY_OF_WEEK == 'Mon' }}\r\n        run: echo \"Hello $FIRST_NAME $middle_name $Last_Name, today is Monday!\"\r\n        env:\r\n          WANDB_API_KEY: $github.SECRETS.WANDB_API\r\n          WANDB_NAME: github_ci_tests\r\n          WANDB_NAME: Octocat\r\n```\r\n\r\n\r\nhttps:\/\/docs.wandb.ai\/guides\/track\/advanced\/environment-variables\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.8,
        "Solution_reading_time":5.33,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":35.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1375058329287,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":762.0,
        "Answerer_view_count":51.0,
        "Challenge_adjusted_solved_time":207.8305247222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following the <a href=\"https:\/\/blog.dataversioncontrol.com\/data-version-control-tutorial-9146715eda46\" rel=\"nofollow noreferrer\">tutorial<\/a> about <a href=\"https:\/\/github.com\/iterative\/dvc\" rel=\"nofollow noreferrer\">Data Version Control<\/a> using <code>mingw32<\/code> on Windows 7.<\/p>\n\n<p>I am getting very strange error when I try to use <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/run\" rel=\"nofollow noreferrer\">run<\/a>:<\/p>\n\n<pre><code>$ dvc run -v echo \"hello\"\nDebug: updater is not old enough to check for updates\nDebug: PRAGMA user_version;\nDebug: fetched: [(2,)]\nDebug: CREATE TABLE IF NOT EXISTS state (inode INTEGER PRIMARY KEY, mtime TEXT NOT NULL, md5 TEXT NOT NULL, timestamp TEXT NOT NULL)\nDebug: CREATE TABLE IF NOT EXISTS state_info (count INTEGER)\nDebug: CREATE TABLE IF NOT EXISTS link_state (path TEXT PRIMARY KEY, inode INTEGER NOT NULL, mtime TEXT NOT NULL)\nDebug: INSERT OR IGNORE INTO state_info (count) SELECT 0 WHERE NOT EXISTS (SELECT * FROM state_info)\nDebug: PRAGMA user_version = 2;\nRunning command:\n        echo hello\n\/c: \/c: Is a directory\nDebug: SELECT count from state_info WHERE rowid=1\nDebug: fetched: [(1,)]\nDebug: UPDATE state_info SET count = 1 WHERE rowid = 1\nError: Traceback (most recent call last):\n  File \"dvc\\command\\run.py\", line 18, in run\n  File \"dvc\\project.py\", line 265, in run\n  File \"dvc\\stage.py\", line 435, in run\nStageCmdFailedError: Stage 'Dvcfile' cmd echo hello failed\n\nError: Failed to run command: Stage 'Dvcfile' cmd echo hello failed\n<\/code><\/pre>\n\n<h3>Question:<\/h3>\n\n<p>Where does the <code>\/c: \/c: Is a directory<\/code> come from?  How can I fix it? <\/p>\n\n<h3>My findings<\/h3>\n\n<ol>\n<li><p>I supposed that it was resolving path to echo, but ech is a builtin.<\/p>\n\n<pre><code>$ type echo\necho is a shell builtin\n<\/code><\/pre>\n\n<p>I tried also with <code>exit<\/code> and <code>cd<\/code> but I am getting the same error.<\/p><\/li>\n<li><p>Calling commands without dvc works fine.<\/p><\/li>\n<li><p><code>dvc<\/code> with <code>--no-exec<\/code> flag works fine, but when later executed with <code>repro<\/code> gives the same error. <\/p><\/li>\n<\/ol>",
        "Challenge_closed_time":1540629268432,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539857086453,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1539881078543,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52871630",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":11.3,
        "Challenge_reading_time":27.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":214.4949941667,
        "Challenge_title":"Resolving paths in mingw fails with Data Version Control",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":109.0,
        "Challenge_word_count":287,
        "Platform":"Stack Overflow",
        "Poster_created_time":1508231047660,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Krak\u00f3w, Poland",
        "Poster_reputation_count":860.0,
        "Poster_view_count":118.0,
        "Solution_body":"<p>I'm one of the dvc developers. Similar error has affected dvc running on cygwin. We've released a fix for it in <code>0.20.0<\/code>. Please upgrade.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.2,
        "Solution_reading_time":1.94,
        "Solution_score_count":4.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":24.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1487197909143,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Brussels, Belgium",
        "Answerer_reputation_count":2206.0,
        "Answerer_view_count":169.0,
        "Challenge_adjusted_solved_time":0.3434,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm new to gunicorn and heroku so I would appreciate any help. I want to deploy my python Dash app on to heroku and I know I need a Procfile. The thing is that my project structure uses the Kedro structure and my structure looks like this:<\/p>\n<pre><code>myproject\n    .... # Kedro-generated files\n    src\/\n        package1\/\n            package2\/\n                __init__.py\n                index.py\n    Procfile\n<\/code><\/pre>\n<p>index.py is a Dash application like so<\/p>\n<pre><code>#imports up here\n\napp = dash.Dash(__name__, external_stylesheets=external_stylesheets)\nserver = app.server\n\n.......  # main code chunk\n\nif __name__ == '__main__':\napp.run_server(debug=True)\n<\/code><\/pre>\n<p>Currently, my Procfile looks like this:<\/p>\n<pre><code>web: gunicorn src frontend.index:app\n<\/code><\/pre>\n<p>My project uploads to heroku just fine but I'm getting this error in my log:<\/p>\n<pre><code>2020-08-21T06:46:46.433935+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 941, in _find_and_load_unlocked\n2020-08-21T06:46:46.433935+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n2020-08-21T06:46:46.433936+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 994, in _gcd_import\n2020-08-21T06:46:46.433936+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load\n2020-08-21T06:46:46.433936+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 953, in _find_and_load_unlocked\n2020-08-21T06:46:46.433962+00:00 app[web.1]: ModuleNotFoundError: No module named 'frontend'\n2020-08-21T06:46:46.434082+00:00 app[web.1]: [2020-08-21 06:46:46 +0000] [11] [INFO] Worker exiting (pid: 11)\n2020-08-21T06:46:46.464346+00:00 app[web.1]: Traceback (most recent call last):\n2020-08-21T06:46:46.464367+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 202, in run\n2020-08-21T06:46:46.464715+00:00 app[web.1]: self.manage_workers()\n2020-08-21T06:46:46.464732+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 545, in manage_workers\n2020-08-21T06:46:46.465049+00:00 app[web.1]: self.spawn_workers()\n2020-08-21T06:46:46.465054+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 617, in spawn_workers\n2020-08-21T06:46:46.465412+00:00 app[web.1]: time.sleep(0.1 * random.random())\n2020-08-21T06:46:46.465417+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 242, in handle_chld\n2020-08-21T06:46:46.465617+00:00 app[web.1]: self.reap_workers()\n2020-08-21T06:46:46.465622+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 525, in reap_workers\n2020-08-21T06:46:46.465905+00:00 app[web.1]: raise HaltServer(reason, self.WORKER_BOOT_ERROR)\n2020-08-21T06:46:46.465950+00:00 app[web.1]: gunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;\n2020-08-21T06:46:46.465964+00:00 app[web.1]: \n2020-08-21T06:46:46.465965+00:00 app[web.1]: During handling of the above exception, another exception occurred:\n2020-08-21T06:46:46.465965+00:00 app[web.1]: \n2020-08-21T06:46:46.465969+00:00 app[web.1]: Traceback (most recent call last):\n2020-08-21T06:46:46.465969+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/bin\/gunicorn&quot;, line 8, in &lt;module&gt;\n2020-08-21T06:46:46.466103+00:00 app[web.1]: sys.exit(run())\n2020-08-21T06:46:46.466107+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in run\n2020-08-21T06:46:46.466254+00:00 app[web.1]: WSGIApplication(&quot;%(prog)s [OPTIONS] [APP_MODULE]&quot;).run()\n2020-08-21T06:46:46.466258+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py&quot;, line 228, in run\n2020-08-21T06:46:46.466464+00:00 app[web.1]: super().run()\n2020-08-21T06:46:46.466470+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py&quot;, line 72, in run\n2020-08-21T06:46:46.466601+00:00 app[web.1]: Arbiter(self).run()\n2020-08-21T06:46:46.466606+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 229, in run\n2020-08-21T06:46:46.466790+00:00 app[web.1]: self.halt(reason=inst.reason, exit_status=inst.exit_status)\n2020-08-21T06:46:46.466794+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 342, in halt\n2020-08-21T06:46:46.467031+00:00 app[web.1]: self.stop()\n2020-08-21T06:46:46.467032+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 393, in stop\n2020-08-21T06:46:46.467262+00:00 app[web.1]: time.sleep(0.1)\n2020-08-21T06:46:46.467267+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 242, in handle_chld\n2020-08-21T06:46:46.467468+00:00 app[web.1]: self.reap_workers()\n2020-08-21T06:46:46.467469+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 525, in reap_workers\n2020-08-21T06:46:46.467750+00:00 app[web.1]: raise HaltServer(reason, self.WORKER_BOOT_ERROR)\n2020-08-21T06:46:46.467754+00:00 app[web.1]: gunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;\n2020-08-21T06:46:46.559947+00:00 heroku[web.1]: Process exited with status 1\n2020-08-21T06:46:46.610907+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T06:47:03.000000+00:00 app[api]: Build succeeded\n2020-08-21T06:49:12.915422+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/&quot; host=protected-coast-07061.herokuapp.com request_id=781ff03f-db0d-40ad-996f-1d25ff3fd026 fwd=&quot;115.66.91.134&quot; dyno= connect= service= status=503 bytes= protocol=https\n2020-08-21T06:49:13.357185+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/&quot; host=protected-coast-07061.herokuapp.com request_id=51bff951-d0fa-4e01-ba38-60f44cbe373b fwd=&quot;18.217.223.118&quot; dyno= connect= service= status=503 bytes= protocol=http\n2020-08-21T06:49:13.955353+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/favicon.ico&quot; host=protected-coast-07061.herokuapp.com request_id=632cc0a9-e052-43c9-a90c-62f99dfbba5c fwd=&quot;115.66.91.134&quot; dyno= connect= service= status=503 bytes= protocol=https\n<\/code><\/pre>\n<pre><code>2020-08-21T06:52:18.372623+00:00 heroku[web.1]: State changed from crashed to starting\n2020-08-21T06:52:32.487313+00:00 heroku[web.1]: Starting process with command `gunicorn src frontend.index`\n2020-08-21T06:52:34.595212+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Starting gunicorn 20.0.4\n2020-08-21T06:52:34.595933+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Listening at: http:\/\/0.0.0.0:17241 (4)\n2020-08-21T06:52:34.596051+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Using worker: sync\n2020-08-21T06:52:34.600183+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [10] [INFO] Booting worker with pid: 10\n2020-08-21T06:52:34.603725+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:52:34.603887+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [10] [INFO] Worker exiting (pid: 10)\n2020-08-21T06:52:34.626625+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [11] [INFO] Booting worker with pid: 11\n2020-08-21T06:52:34.629877+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:52:34.629978+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [11] [INFO] Worker exiting (pid: 11)\n2020-08-21T06:52:34.733270+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Shutting down: Master\n2020-08-21T06:52:34.733356+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Reason: App failed to load.\n2020-08-21T06:52:34.800675+00:00 heroku[web.1]: Process exited with status 4\n2020-08-21T06:52:34.837697+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T06:52:34.839731+00:00 heroku[web.1]: State changed from crashed to starting\n2020-08-21T06:52:49.188229+00:00 heroku[web.1]: Starting process with command `gunicorn src frontend.index`\n2020-08-21T06:52:50.000000+00:00 app[api]: Build succeeded\n2020-08-21T06:52:51.154243+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Starting gunicorn 20.0.4\n2020-08-21T06:52:51.154956+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Listening at: http:\/\/0.0.0.0:46031 (4)\n2020-08-21T06:52:51.155075+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Using worker: sync\n2020-08-21T06:52:51.158999+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [10] [INFO] Booting worker with pid: 10\n2020-08-21T06:52:51.162147+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:52:51.162261+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [10] [INFO] Worker exiting (pid: 10)\n2020-08-21T06:52:51.189291+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Shutting down: Master\n2020-08-21T06:52:51.189375+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Reason: App failed to load.\n2020-08-21T06:52:51.249579+00:00 heroku[web.1]: Process exited with status 4\n2020-08-21T06:52:51.281288+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T06:53:27.313026+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/&quot; host=protected-coast-07061.herokuapp.com request_id=67b1f83d-37a8-4ad1-b522-2e7cc0bd7b7d fwd=&quot;115.66.91.134&quot; dyno= connect= service= status=503 bytes= protocol=https\n2020-08-21T06:53:28.196639+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/favicon.ico&quot; host=protected-coast-07061.herokuapp.com request_id=0d13d80e-ca9b-4857-970e-47cfcf602017 fwd=&quot;115.66.91.134&quot; dyno= connect= service= status=503 bytes= protocol=https\n2020-08-21T06:57:12.000000+00:00 app[api]: Build started by user \n2020-08-21T06:58:51.667324+00:00 app[api]: Deploy 1f77e9e8 by user \n2020-08-21T06:58:51.667324+00:00 app[api]: Release v12 created by user \n2020-08-21T06:58:51.832220+00:00 heroku[web.1]: State changed from crashed to starting\n2020-08-21T06:59:07.062252+00:00 heroku[web.1]: Starting process with command `gunicorn src frontend.index:app`\n2020-08-21T06:59:10.383357+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Starting gunicorn 20.0.4\n2020-08-21T06:59:10.384213+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Listening at: http:\/\/0.0.0.0:54641 (4)\n2020-08-21T06:59:10.384357+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Using worker: sync\n2020-08-21T06:59:10.388913+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [10] [INFO] Booting worker with pid: 10\n2020-08-21T06:59:10.392276+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:59:10.392426+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [10] [INFO] Worker exiting (pid: 10)\n2020-08-21T06:59:10.403239+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [11] [INFO] Booting worker with pid: 11\n2020-08-21T06:59:10.407880+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:59:10.408006+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [11] [INFO] Worker exiting (pid: 11)\n2020-08-21T06:59:10.525402+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Shutting down: Master\n2020-08-21T06:59:10.525558+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Reason: App failed to load.\n2020-08-21T06:59:10.607473+00:00 heroku[web.1]: Process exited with status 4\n2020-08-21T06:59:11.643239+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T06:59:54.000000+00:00 app[api]: Build succeeded\n2020-08-21T07:08:53.300472+00:00 heroku[web.1]: State changed from crashed to starting\n2020-08-21T07:09:16.319403+00:00 heroku[web.1]: Starting process with command `gunicorn src frontend.index:app`\n2020-08-21T07:09:19.182910+00:00 heroku[web.1]: Process exited with status 4\n2020-08-21T07:09:19.228761+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T07:09:19.057971+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Starting gunicorn 20.0.4\n2020-08-21T07:09:19.058760+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Listening at: http:\/\/0.0.0.0:25408 (4)\n2020-08-21T07:09:19.058888+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Using worker: sync\n2020-08-21T07:09:19.063236+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [10] [INFO] Booting worker with pid: 10\n2020-08-21T07:09:19.066629+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T07:09:19.066758+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [10] [INFO] Worker exiting (pid: 10)\n2020-08-21T07:09:19.102247+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Shutting down: Master\n2020-08-21T07:09:19.102349+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Reason: App failed to load.\n<\/code><\/pre>\n<p>Apologies, I am new to this so I am not sure where to even start with the debugging as well. To summarise: I think my gunicorn is not firing as my line may be wrong; and I am not sure what is causing my app to not launch. How do I solve this issue?<\/p>",
        "Challenge_closed_time":1597994468467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597994063423,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63518174",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":180.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":143,
        "Challenge_solved_time":0.1125122222,
        "Challenge_title":"using gunicorn for nested folders",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":586.0,
        "Challenge_word_count":1101,
        "Platform":"Stack Overflow",
        "Poster_created_time":1597993100672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>The logic of gunicorn is the following:\n<code>.<\/code> (dot) for directories, <code>:<\/code> (column) for objects defined inside a file.<\/p>\n<p>Assuming the given structure, you should have something like this:<\/p>\n<pre><code>$ cat Procfile\nweb: gunicorn src.package1.package2.index:app\n<\/code><\/pre>\n<p>[EDIT] If you get an error, you should consider using <code>server<\/code> instead of <code>app<\/code>. As an example, these files are from <a href=\"https:\/\/gitlab.com\/qmeeus\/datathon\" rel=\"nofollow noreferrer\">one of my old projects<\/a> (also a Dash app):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># app.py\n\nimport flask\nfrom src import dashboard\n\nserver = flask.Flask(__name__)\nserver.secret_key = os.environ.get('secret_key', str(randint(0, 1000000)))\napp = dashboard.main(server)\n\nif __name__ == '__main__':\n    app.server.run(debug=True, threaded=True)\n<\/code><\/pre>\n<pre class=\"lang-sh prettyprint-override\"><code># Procfile\nweb: gunicorn app:server --timeout 300\n<\/code><\/pre>\n<pre class=\"lang-sh prettyprint-override\"><code>$ ls *\nProcfile app.py\n\nsrc:\nconfig.py  dashboard.py ...\n \n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1597995299663,
        "Solution_link_count":1.0,
        "Solution_readability":12.0,
        "Solution_reading_time":14.56,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":113.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1379265931347,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Rotterdam, Netherlands",
        "Answerer_reputation_count":6502.0,
        "Answerer_view_count":561.0,
        "Challenge_adjusted_solved_time":0.4821711111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a python script that is written in different files (one for importing, one for calculations, et cetera). These are all in the same folder, and when I need a function from another function I do something like<\/p>\n\n<pre><code>import file_import\nfile_import.do_something_usefull()\n<\/code><\/pre>\n\n<p>where, of course, in the <code>file_import<\/code> there is a function <code>do_something_usefull()<\/code> that, uhm, does something usefull. How can I accomplish the same in Azure?<\/p>",
        "Challenge_closed_time":1457451133736,
        "Challenge_comment_count":4,
        "Challenge_created_time":1457449397920,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1457540530820,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35870839",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":8.0,
        "Challenge_reading_time":6.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.4821711111,
        "Challenge_title":"Is it possible to import python scripts in Azure?",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":1473.0,
        "Challenge_word_count":75,
        "Platform":"Stack Overflow",
        "Poster_created_time":1379265931347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Rotterdam, Netherlands",
        "Poster_reputation_count":6502.0,
        "Poster_view_count":561.0,
        "Solution_body":"<p>I found it out myself. It is documenten on Microsoft's site <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-execute-python-scripts\/\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>The steps, very short, are:<\/p>\n\n<ol>\n<li>Include all the python you want in a .zip<\/li>\n<li>Upload that zip as a dataset<\/li>\n<li>Drag the dataset as the third option parameter in the 'execute python'-block (example below)<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9FuMr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9FuMr.png\" alt=\"Example dragging zip to Python script\"><\/a><\/p>\n\n<ol start=\"4\">\n<li>execute said function by importing <code>import Hello<\/code> (the name of the file, not the zip) and running <code>Hello.do_something_usefull()<\/code><\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1457451850803,
        "Solution_link_count":3.0,
        "Solution_readability":11.5,
        "Solution_reading_time":10.59,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":83.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":6.6123980556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I work on a project with DVC (Data version control). Let's say I make a lot of local commits. Something like this:<\/p>\n\n<pre><code># make changes for experiment 1\ndvc add my_data_file\ngit add my_data_file.dvc\ngit commit -m \"Experiment 1\"\n\n# make changes for experiment 2\n# which change both code and data\ndvc add my_data_file\ngit add my_data_file.dvc\ngit commit -m \"Experiment 2\"\n\n# make changes for experiment 3\n# which change both code and data\ndvc add my_data_file\ngit add my_data_file.dvc\ngit commit -m \"Experiment 3\"\n\n# Finally I'm done\n# push changes:\ndvc push\ngit push\n<\/code><\/pre>\n\n<p>However there is one problem: <code>dvc push<\/code> will only push data from experiment 3. Is there any way to push data from all local commits (i.e. starting from the first commit diverged from remote branch)?<\/p>\n\n<p>Currently I see two options:<\/p>\n\n<ol>\n<li>Tag each commit and push it with <code>dvc push -T<\/code><\/li>\n<li>After \"expermient 3\" commit execute <code>git checkout commit-hash &amp;&amp; dvc push<\/code> for all local commits not yet pushed to remote.<\/li>\n<\/ol>\n\n<p>Both these options seem cumbersome and error-prone. Is there any better way to do it?<\/p>",
        "Challenge_closed_time":1561842649416,
        "Challenge_comment_count":0,
        "Challenge_created_time":1561823734517,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56818930",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":14.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":5.2541386111,
        "Challenge_title":"\"dvc push\" after several local commits",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":916.0,
        "Challenge_word_count":185,
        "Platform":"Stack Overflow",
        "Poster_created_time":1522254698710,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Russia",
        "Poster_reputation_count":784.0,
        "Poster_view_count":77.0,
        "Solution_body":"<p>@NShiny, there is a related ticket:<\/p>\n\n<p><a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1691\" rel=\"nofollow noreferrer\">support push\/pull\/metrics\/gc, etc across different commits<\/a>.<\/p>\n\n<p>Please, give it a vote so that we know how to prioritize it.<\/p>\n\n<p>As a workaround, I would recommend to run <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/install\" rel=\"nofollow noreferrer\"><code>dvc install<\/code><\/a>. It installs a <code>pre-push<\/code> GIt hook and runs <code>dvc push<\/code> automatically:<\/p>\n\n<pre><code>Git pre-push hook executes dvc push before git push to upload files and directories under DVC control to remote.\n<\/code><\/pre>\n\n<p>It means, though you need to run <code>git push<\/code> after every <code>git commit<\/code> :(<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1561847539150,
        "Solution_link_count":2.0,
        "Solution_readability":12.8,
        "Solution_reading_time":9.83,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":86.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2106.4869444444,
        "Challenge_answer_count":0,
        "Challenge_body":"This may lead to strange behaviour when called in interactive mode in another place thant the kedro project root.",
        "Challenge_closed_time":1602948810000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595365457000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/30",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":10.5,
        "Challenge_reading_time":2.66,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":2106.4869444444,
        "Challenge_title":"get_mlflow_config use the working directory instead of given path when called within load_context",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":null,
        "Challenge_word_count":31,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1427953176670,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":145.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":14.9208969444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm having problems pushing files with DVC to DAGsHub.<\/p>\n<p>Workflow:<\/p>\n<ul>\n<li>I used my email to signup to DAGsHub.<\/li>\n<li>I created a repo and clone it to my computer.<\/li>\n<li>I added files to the repo and track them using DVC and Git to track the pointer files.<\/li>\n<li>Running DVC push -r origin, it asks me for my password. When I enter the password and hit enter - nothing happens.<\/li>\n<\/ul>\n<p>It sits and waits, barring me from even canceling the operation with Ctrl+C.\nI'm forced to manually close the terminal, open a new one, ending the &quot;Python&quot; process in task manager and delete the lock file in .dvc\/tmp\/lock.<\/p>",
        "Challenge_closed_time":1620591578072,
        "Challenge_comment_count":2,
        "Challenge_created_time":1620537862843,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1620625906412,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67454531",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":6.0,
        "Challenge_reading_time":8.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":14.9208969444,
        "Challenge_title":"Git bash command prompt hanging when running dvc push to DAGsHub",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":254.0,
        "Challenge_word_count":117,
        "Platform":"Stack Overflow",
        "Poster_created_time":1620537484800,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p><strong>Short answer<\/strong><\/p>\n<p>Do not use <code>ask_password<\/code>.\nInstead, save your token in the local config by running once:<\/p>\n<pre><code>dvc remote modify origin --local --unset ask_password\ndvc remote modify origin --local password &lt;--access token--&gt;\n<\/code><\/pre>\n<p><code>dvc push -r origin<\/code> should work then.<\/p>\n<p><strong>Long answer<\/strong><\/p>\n<p><a href=\"https:\/\/www.atlassian.com\/git\/tutorials\/git-bash#:%7E:text=What%20is%20Git%20Bash%3F,operating%20system%20through%20written%20commands.\" rel=\"nofollow noreferrer\">Git Bash<\/a> is not running the regular Windows command prompt but an emulated Unix-style bash prompt. From the information in your question, I cannot know for sure, but this is probably causing the <code>msvcrt<\/code> package used by DVC to prompt the password on windows machines to fail\/hang.<\/p>\n<p>There are potentially 3 ways to deal with the issue:<\/p>\n<ol>\n<li>Run <code>dvc pull<\/code> from the regular Windows cmd prompt.<\/li>\n<li>Find a way to make Git Bash wrap Python calls with <code>winpty<\/code> - I am not 100% positive about how to do this, but not using <code>winpty<\/code> seems to be the reason <code>msvcrt<\/code> fails at prompting for your password.<\/li>\n<li>The simplest solution - Do not use <code>ask_password<\/code>.\nInstead, save your token in the local config by running once:\n<pre><code>dvc remote modify origin --local --unset ask_password\ndvc remote modify origin --local password &lt;--access token--&gt;\n<\/code><\/pre>\nYou can get your access token by clicking on the question mark beside the DVC\nremote of your DAGsHub repository, then click on &quot;Reveal my token&quot;.<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.6,
        "Solution_reading_time":21.52,
        "Solution_score_count":4.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":211.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":28.6503836111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm doing a pipeline in Azure ML SDK. After I had run the pipeline for some amount of times it reported I had reached the Snapshot limit of 300MB. I followed some of the fixes that was proposed:<\/p>\n<ul>\n<li>   Each step script is moved to a separate subfolder<\/li>\n<li>   I added a datastore to the pipeline<\/li>\n<li>   This line was added: <code>azureml._restclient.snapshots_client.SNAPSHOT_MAX_SIZE_BYTES = 1000<\/code><\/li>\n<\/ul>\n<p>But then a new Snapshot error occurred after I submitted my pipeline:<\/p>\n<pre><code>pipeline1 = Pipeline(default_source_directory=&quot;.&quot;, default_datastore=def_blob_store, workspace=ws, steps=[prep_step, hd_step, register_model_step])\n<\/code><\/pre>\n<p>THE ERROR MESSAGE:<\/p>\n<pre><code>WARNING:root:If 'script' has been provided here and a script file name has been specified in 'run_config', 'script' provided in ScriptRunConfig initialization will take precedence.\n---------------------------------------------------------------------------\nSnapshotException                         Traceback (most recent call last)\n&lt;ipython-input-14-05c5aa4991aa&gt; in &lt;module&gt;\n----&gt; 1 pipeline1 = Pipeline(default_source_directory=&quot;.&quot;, default_datastore=def_blob_store, workspace=ws, steps=[prep_step, hd_step, register_model_step])\n      2 pipeline1.validate()\n      3 pipeline_run = Experiment(ws, 'health_insuarance').submit(pipeline1, regenerate_outputs=False)\n      4 RunDetails(pipeline_run).show()\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/_experiment_method.py in wrapper(self, *args, **kwargs)\n     95             &quot;&quot;&quot;\n     96             ExperimentSubmitRegistrar.register_submit_function(self.__class__, submit_function)\n---&gt; 97             return init_func(self, *args, **kwargs)\n     98         return wrapper\n     99     return real_decorator\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/pipeline.py in __init__(self, workspace, steps, description, default_datastore, default_source_directory, resolve_closure, _workflow_provider, _service_endpoint, **kwargs)\n    175                 raise ValueError('parameter %s is not recognized for Pipeline ' % key)\n    176         self._enable_email_notification = enable_email_notification\n--&gt; 177         self._graph = self._graph_builder.build(self._name, steps, finalize=False)\n    178 \n    179     def _set_experiment_name(self, name):\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/builder.py in build(self, name, steps, finalize, regenerate_outputs)\n   1479                 pass\n   1480 \n-&gt; 1481         graph = self.construct(name, steps)\n   1482         if finalize:\n   1483             graph.finalize(regenerate_outputs=regenerate_outputs)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/builder.py in construct(self, name, steps)\n   1501         self._graph = Graph(name, self._context)\n   1502         self._nodeStack.append([])\n-&gt; 1503         self.process_collection(steps)\n   1504         for builder in self._builderStack[::-1]:\n   1505             builder.apply_rules()\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/builder.py in process_collection(self, collection)\n   1537         self._nodeStack.append([])\n   1538         self._builderStack.append(builder)\n-&gt; 1539         builder.process_collection(collection)\n   1540         added_nodes = self._nodeStack.pop()\n   1541         self._nodeStack[-1].extend(added_nodes)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/builder.py in process_collection(self, collection)\n   1828         &quot;&quot;&quot;\n   1829         for item in collection:\n-&gt; 1830             self._base_builder.process_collection(item)\n   1831 \n   1832     def apply_rules(self):\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/builder.py in process_collection(self, collection)\n   1531         # just a step?\n   1532         if isinstance(collection, PipelineStep):\n-&gt; 1533             return self.process_step(collection)\n   1534 \n   1535         # delegate to correct builder\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/builder.py in process_step(self, step)\n   1575             return self._step2node[step]\n   1576 \n-&gt; 1577         node = step.create_node(self._graph, self._default_datastore, self._context)\n   1578         self.assert_node_valid(step, self._graph, node)\n   1579 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/steps\/hyper_drive_step.py in create_node(self, graph, default_datastore, context)\n    247         &quot;&quot;&quot;\n    248         hyperdrive_config, reuse_hashable_config = self._get_hyperdrive_config(context._workspace,\n--&gt; 249                                                                                context._experiment_name)\n    250         self._params[HyperDriveStep._run_config_param_name] = json.dumps(hyperdrive_config)\n    251         self._params[HyperDriveStep._run_reuse_hashable_config] = json.dumps(reuse_hashable_config)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/steps\/hyper_drive_step.py in _get_hyperdrive_config(self, workspace, experiment_name)\n    323 \n    324         hyperdrive_dto = _search._create_experiment_dto(self._hyperdrive_config, workspace,\n--&gt; 325                                                         experiment_name, telemetry_values)\n    326 \n    327         hyperdrive_config = hyperdrive_dto.as_dict()\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/train\/hyperdrive\/_search.py in _create_experiment_dto(hyperdrive_config, workspace, experiment_name, telemetry_values, activity_logger, **kwargs)\n     41     if hyperdrive_config.source_directory is not None:\n     42         snapshot_client = SnapshotsClient(workspace.service_context)\n---&gt; 43         snapshot_id = snapshot_client.create_snapshot(hyperdrive_config.source_directory)\n     44 \n     45         if activity_logger is not None:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_restclient\/snapshots_client.py in create_snapshot(self, file_or_folder_path, retry_on_failure, raise_on_validation_failure)\n     83         exclude_function = ignore_file.is_file_excluded\n     84 \n---&gt; 85         self._validate_snapshot_size(file_or_folder_path, exclude_function, raise_on_validation_failure)\n     86 \n     87         # Get the previous snapshot for this project\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_restclient\/snapshots_client.py in _validate_snapshot_size(self, file_or_folder_path, exclude_function, raise_on_validation_failure)\n     61                             &quot;\\n&quot;.format(file_or_folder_path, SNAPSHOT_MAX_SIZE_BYTES \/ ONE_MB)\n     62             if raise_on_validation_failure:\n---&gt; 63                 raise SnapshotException(error_message)\n     64             else:\n     65                 self._logger.warning(error_message)\n\nSnapshotException: SnapshotException:\n    Message: ====================================================================\n\nWhile attempting to take snapshot of .\/train\/\nYour total snapshot size exceeds the limit of 0.00095367431640625 MB.\nPlease see http:\/\/aka.ms\/aml-largefiles on how to work with large files.\n\n====================================================================\n\n\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;====================================================================\\n\\nWhile attempting to take snapshot of .\/train\/\\nYour total snapshot size exceeds the limit of 0.00095367431640625 MB.\\nPlease see http:\/\/aka.ms\/aml-largefiles on how to work with large files.\\n\\n====================================================================\\n\\n&quot;\n    }\n}\n\u200b\n<\/code><\/pre>\n<p>Any idea how I fix this?<\/p>",
        "Challenge_closed_time":1612529086248,
        "Challenge_comment_count":3,
        "Challenge_created_time":1612425944867,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/258581\/how-do-i-fix-this-snapshot-exception",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":18.3,
        "Challenge_reading_time":96.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":61,
        "Challenge_solved_time":28.6503836111,
        "Challenge_title":"How do I fix this Snapshot Exception?",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":null,
        "Challenge_word_count":498,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Alright, so I found the fix.  <\/p>\n<p>I changed this line by adding a number equvilant to 1GB: <code>azureml._restclient.snapshots_client.SNAPSHOT_MAX_SIZE_BYTES = 1000000000<\/code>  <\/p>\n<p>For some reason, you have to define the size in BYTES and not megabytes even though the default is 300 MB. Not especially intuitive.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.3,
        "Solution_reading_time":4.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":14.2520105556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have just removed a DVC tracking file by mistake using the command <code>dvc remove training_data.dvc -p<\/code>, which led to all my training dataset gone completely. I know in Git, we can easily revert a deleted branch based on its hash. Does anyone know how to revert all my lost data in DVC?<\/p>",
        "Challenge_closed_time":1592457436920,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592445622650,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62441146",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.5,
        "Challenge_reading_time":4.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":3.2817416667,
        "Challenge_title":"Revert a dvc remove -p command",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":687.0,
        "Challenge_word_count":58,
        "Platform":"Stack Overflow",
        "Poster_created_time":1467943515392,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Danang, H\u1ea3i Ch\u00e2u District, Da Nang, Vietnam",
        "Poster_reputation_count":173.0,
        "Poster_view_count":28.0,
        "Solution_body":"<p>You should be safe (at least data is not gone) most likely. From the <code>dvc remove<\/code> <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remove\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n\n<blockquote>\n  <p>Note that it does not remove files from the DVC cache or remote storage (see dvc gc). However, remember to run <code>dvc push<\/code> to save the files you actually want to use or share in the future.<\/p>\n<\/blockquote>\n\n<p>So, if you created <code>training_data.dvc<\/code> as with <code>dvc add<\/code> and\/or <code>dvc run<\/code> and <code>dvc remove -p<\/code> didn't ask\/warn you about anything, means that data is cached similar to Git in the <code>.dvc\/cache<\/code>. <\/p>\n\n<p>There are ways to retrieve it, but I would need to know a little bit more details - how exactly did you add your dataset? Did you commit <code>training_data.dvc<\/code> or it's completely gone? Was it the only data you have added so far? (happy to help you in comments).<\/p>\n\n<h2>Recovering a directory<\/h2>\n\n<p>First of all, <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvc-files-and-directories#structure-of-cache-directory\" rel=\"nofollow noreferrer\">here<\/a> is the document that describes briefly how DVC stores directories in the cache.<\/p>\n\n<p>What we can do is to find all <code>.dir<\/code> files in the <code>.dvc\/cache<\/code>:<\/p>\n\n<p><code>find .dvc\/cache -type f -name \"*.dir\"<\/code><\/p>\n\n<p>outputs something like:<\/p>\n\n<pre><code>.dvc\/cache\/20\/b786b6e6f80e2b3fcf17827ad18597.dir\n.dvc\/cache\/00\/db872eebe1c914dd13617616bb8586.dir\n.dvc\/cache\/2d\/1764cb0fc973f68f31f5ff90ee0883.dir\n<\/code><\/pre>\n\n<p>(if the local cache is lost and we are restoring data from the remote storage, the same logic applies, commands (e.g. to find files on S3 with .dir extension) look different)<\/p>\n\n<p>Each <code>.dir<\/code> file is a JSON with a content of one version of a directory (file names, hashes, etc). It has all the information needed to restore it. The next thing we need to do is to understand which one do we need. There is no one single rule for that, what I would recommend to check (and pick depending on your use case):<\/p>\n\n<ul>\n<li>Check the date modified (if you remember when this data was added).<\/li>\n<li>Check the content of those files - if you remember a specific file name that was present only in the directory you are looking for - just grep it.<\/li>\n<li>Try to restore them one by one and check the directory content.<\/li>\n<\/ul>\n\n<p>Okay, now let's imagine we decided that we want to restore <code>.dvc\/cache\/20\/b786b6e6f80e2b3fcf17827ad18597.dir<\/code>, (e.g. because content of it looks like:<\/p>\n\n<pre><code>[\n{\"md5\": \"6f597d341ceb7d8fbbe88859a892ef81\", \"relpath\": \"test.tsv\"}, {\"md5\": \"32b715ef0d71ff4c9e61f55b09c15e75\", \"relpath\": \"train.tsv\"}\n]\n<\/code><\/pre>\n\n<p>and we want to get a directory with <code>train.tsv<\/code>).<\/p>\n\n<p>The only thing we need to do is to create a <code>.dvc<\/code> file that references this directory:<\/p>\n\n<pre class=\"lang-yaml prettyprint-override\"><code>outs:\n- md5: 20b786b6e6f80e2b3fcf17827ad18597.dir\n  path: my-directory\n<\/code><\/pre>\n\n<p>(note, that path \/20\/b786b6e6f80e2b3fcf17827ad18597.dir became a hash value: 20b786b6e6f80e2b3fcf17827ad18597.dir)<\/p>\n\n<p>And run <code>dvc pull<\/code> on this file.<\/p>\n\n<p>That should be it.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1592496929888,
        "Solution_link_count":2.0,
        "Solution_readability":7.9,
        "Solution_reading_time":41.6,
        "Solution_score_count":3.0,
        "Solution_sentence_count":36.0,
        "Solution_word_count":420.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1597346132176,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":72.1141575,
        "Challenge_answer_count":1,
        "Challenge_body":"<h2>Issue<\/h2>\n<p>I am trying prepare and then submit a new experiment to Azure Machine Learning from an Azure Function in Python. I therefore register a new dataset for my Azure ML workspace, which contains the training data for my ML model using <code>dataset.register(...<\/code>. However, when I try to create this dataset with the following line of code<\/p>\n<pre><code>dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths)\n<\/code><\/pre>\n<p>then I get a <code>Failure Exception: OSError: [Errno 30] Read-only file system ...<\/code>.<\/p>\n<h2>Ideas<\/h2>\n<ol>\n<li>I know that I shouldn't write to the file system from within an Azure function if possible. But I actually don't want to write anything to the local file system. I only want to create the dataset as a reference to my blob storage under <code>datastore_path<\/code> and then register this to my Azure Machine Learning workspace. But it seems that the method <code>from_delimited_files<\/code> is trying to write to the file system anyway (maybe some caching?).<\/li>\n<li>I also know that there is a temp folder in which writing temporary files is permitted. However, I belive I cannot really control where this method is writing data. I already tried changing the current working directory to this temp folder just before the function call using <code>os.chdir(tempfile.gettempdir())<\/code>, but that didn't help.<\/li>\n<\/ol>\n<p>Any other ideas? I don't think I am doing something particularly unusually...<\/p>\n<h2>Details<\/h2>\n<p>I am using python 3.7 and azureml-sdk 1.9.0 and I can run the python script locally without problems. I currently deploy from VSCode using the Azure Functions extension version 0.23.0 (and an Azure DevOps pipeline for CI\/CD).<\/p>\n<p>Here is my full stack trace:<\/p>\n<pre><code>Microsoft.Azure.WebJobs.Host.FunctionInvocationException: Exception while executing function: Functions.HttpTrigger_Train\n ---&gt; Microsoft.Azure.WebJobs.Script.Workers.Rpc.RpcException: Result: Failure\nException: OSError: [Errno 30] Read-only file system: '\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/bin\/deps.lock'\nStack:   File &quot;\/azure-functions-host\/workers\/python\/3.7\/LINUX\/X64\/azure_functions_worker\/dispatcher.py&quot;, line 345, in _handle__invocation_request\n    self.__run_sync_func, invocation_id, fi.func, args)\n  File &quot;\/usr\/local\/lib\/python3.7\/concurrent\/futures\/thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;\/azure-functions-host\/workers\/python\/3.7\/LINUX\/X64\/azure_functions_worker\/dispatcher.py&quot;, line 480, in __run_sync_func\n    return func(**params)\n  File &quot;\/home\/site\/wwwroot\/HttpTrigger_Train\/__init__.py&quot;, line 11, in main\n    train()\n  File &quot;\/home\/site\/wwwroot\/shared_code\/train.py&quot;, line 70, in train\n    dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/data\/_loggerfactory.py&quot;, line 126, in wrapper\n    return func(*args, **kwargs)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/data\/dataset_factory.py&quot;, line 308, in from_delimited_files\n    quoting=support_multi_line)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/readers.py&quot;, line 100, in read_csv\n    df = Dataflow._path_to_get_files_block(path, archive_options)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/dataflow.py&quot;, line 2387, in _path_to_get_files_block\n    return datastore_to_dataflow(path)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_datastore_helper.py&quot;, line 41, in datastore_to_dataflow\n    datastore, datastore_value = get_datastore_value(source)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_datastore_helper.py&quot;, line 83, in get_datastore_value\n    _set_auth_type(workspace)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_datastore_helper.py&quot;, line 134, in _set_auth_type\n    get_engine_api().set_aml_auth(SetAmlAuthMessageArgument(AuthType.SERVICEPRINCIPAL, json.dumps(auth)))\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/api.py&quot;, line 18, in get_engine_api\n    _engine_api = EngineAPI()\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/api.py&quot;, line 55, in __init__\n    self._message_channel = launch_engine()\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/engine.py&quot;, line 300, in launch_engine\n    dependencies_path = runtime.ensure_dependencies()\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/runtime.py&quot;, line 141, in ensure_dependencies\n    with _FileLock(deps_lock_path, raise_on_timeout=timeout_exception):\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/runtime.py&quot;, line 113, in __enter__\n    self.acquire()\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/runtime.py&quot;, line 72, in acquire\n    self.lockfile = os.open(self.lockfile_path, os.O_CREAT | os.O_EXCL | os.O_RDWR)\n\n   at Microsoft.Azure.WebJobs.Script.Description.WorkerFunctionInvoker.InvokeCore(Object[] parameters, FunctionInvocationContext context) in \/src\/azure-functions-host\/src\/WebJobs.Script\/Description\/Workers\/WorkerFunctionInvoker.cs:line 85\n   at Microsoft.Azure.WebJobs.Script.Description.FunctionInvokerBase.Invoke(Object[] parameters) in \/src\/azure-functions-host\/src\/WebJobs.Script\/Description\/FunctionInvokerBase.cs:line 85\n   at Microsoft.Azure.WebJobs.Script.Description.FunctionGenerator.Coerce[T](Task`1 src) in \/src\/azure-functions-host\/src\/WebJobs.Script\/Description\/FunctionGenerator.cs:line 225\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionInvoker`2.InvokeAsync(Object instance, Object[] arguments) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionInvoker.cs:line 52\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.InvokeAsync(IFunctionInvoker invoker, ParameterHelper parameterHelper, CancellationTokenSource timeoutTokenSource, CancellationTokenSource functionCancellationTokenSource, Boolean throwOnTimeout, TimeSpan timerInterval, IFunctionInstance instance) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 587\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithWatchersAsync(IFunctionInstanceEx instance, ParameterHelper parameterHelper, ILogger logger, CancellationTokenSource functionCancellationTokenSource) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 532\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithLoggingAsync(IFunctionInstanceEx instance, ParameterHelper parameterHelper, IFunctionOutputDefinition outputDefinition, ILogger logger, CancellationTokenSource functionCancellationTokenSource) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 470\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithLoggingAsync(IFunctionInstanceEx instance, FunctionStartedMessage message, FunctionInstanceLogEntry instanceLogEntry, ParameterHelper parameterHelper, ILogger logger, CancellationToken cancellationToken) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 278\n   --- End of inner exception stack trace ---\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithLoggingAsync(IFunctionInstanceEx instance, FunctionStartedMessage message, FunctionInstanceLogEntry instanceLogEntry, ParameterHelper parameterHelper, ILogger logger, CancellationToken cancellationToken) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 325\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.TryExecuteAsyncCore(IFunctionInstanceEx functionInstance, CancellationToken cancellationToken) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 117\n<\/code><\/pre>",
        "Challenge_closed_time":1597616847400,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597357236433,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1613407306920,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63403985",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":25.2,
        "Challenge_reading_time":114.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":73,
        "Challenge_solved_time":72.1141575,
        "Challenge_title":"\"Failure Exception: OSError: [Errno 30] Read-only file system\" when using AzureML in Python Azure Function",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":1092.0,
        "Challenge_word_count":563,
        "Platform":"Stack Overflow",
        "Poster_created_time":1597346132176,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":61.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>The issue was an incompatible OS version in my virtual environment.<\/p>\n<p>A huge thanks goes to <a href=\"https:\/\/docs.microsoft.com\/answers\/users\/111253\/pramodvalavala-msft.html\" rel=\"nofollow noreferrer\">PramodValavala-MSFT<\/a> for his idea to create a docker container! Following his suggestion, I suddenly got the following error message for the  <code>dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths)<\/code> command:<\/p>\n<blockquote>\n<p>Exception: NotImplementedError: Unsupported Linux distribution debian 10.<\/p>\n<\/blockquote>\n<p>which reminded me of the following warning in the azure machine learning documentation:<\/p>\n<blockquote>\n<p>Some dataset classes have dependencies on the azureml-dataprep\npackage, which is only compatible with 64-bit Python. For Linux users,\nthese classes are supported only on the following distributions: Red\nHat Enterprise Linux (7, 8), Ubuntu (14.04, 16.04, 18.04), Fedora (27,\n28), Debian (8, 9), and CentOS (7).<\/p>\n<\/blockquote>\n<p>Choosing the predefined docker image <code>2.0-python3.7<\/code> (running Debian 9) instead of  <code>3.0-python3.7<\/code> (running Debian 10) solved the issue (see <a href=\"https:\/\/hub.docker.com\/_\/microsoft-azure-functions-python\" rel=\"nofollow noreferrer\">https:\/\/hub.docker.com\/_\/microsoft-azure-functions-python<\/a>).<\/p>\n<p>I suspect that the default virtual environment, which I was using originally, also ran on an incompatible OS.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.9,
        "Solution_reading_time":18.86,
        "Solution_score_count":3.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":156.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1541802293200,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":163.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":109.1823416667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The short story is, when I try to submit an azure ML pipeline run (an <em>azure ML pipeline<\/em>, not an <em>Azure pipeline<\/em>) from a jupyter notebook, I get PermissionError: [Errno 13] Permission denied: '.\\NTUSER.DAT'.  More details:<\/p>\n\n<p>Relevant code:<\/p>\n\n<pre><code>from azureml.train.automl import AutoMLConfig\nfrom azureml.train.automl.runtime import AutoMLStep\nautoml_settings = {\n    \"iteration_timeout_minutes\": 20,\n    \"experiment_timeout_minutes\": 30,\n    \"n_cross_validations\": 3,\n    \"primary_metric\": 'r2_score',\n    \"preprocess\": True,\n    \"max_concurrent_iterations\": 3,\n    \"max_cores_per_iteration\": -1,\n    \"verbosity\": logging.INFO,\n    \"enable_early_stopping\": True,\n    'time_column_name': \"DateTime\"\n}\n\nautoml_config = AutoMLConfig(task = 'forecasting',\n                             debug_log = 'automl_errors.log',\n                             path = \".\",\n                             compute_target=compute_target,\n                             run_configuration=conda_run_config,                               \n                             training_data = financeforecast_dataset,\n                             label_column_name = 'TotalUSD',\n                             **automl_settings\n                            )\n\nautoml_step = AutoMLStep(\n    name='automl_module',\n    automl_config=automl_config,\n    allow_reuse=False)\n\ntraining_pipeline = Pipeline(\n    description=\"training_pipeline\",\n    workspace=ws,    \n    steps=[automl_step])\n\ntraining_pipeline_run = Experiment(ws, 'test').submit(training_pipeline)\n<\/code><\/pre>\n\n<p>The training_pipeline step runs for apx 20 seconds, and then I get a long trace, ending in:<\/p>\n\n<pre><code>~\\AppData\\Local\\Continuum\\anaconda2\\envs\\forecasting\\lib\\site- \npackages\\azureml\\pipeline\\core\\_module_builder.py in _hash_from_file_paths(hash_src)\n    100             hasher = hashlib.md5()\n    101             for f in hash_src:\n--&gt; 102                 with open(str(f), 'rb') as afile:\n    103                     buf = afile.read()\n    104                     hasher.update(buf)\n\nPermissionError: [Errno 13] Permission denied: '.\\\\NTUSER.DAT'\n<\/code><\/pre>\n\n<p>According to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-your-first-pipeline\" rel=\"nofollow noreferrer\">Azure's docs on this topic<\/a>, submitting a pipeline uploads a \"snapshot\" of the \"source directory\" you specified.  Initially, I hadn't specified a source directory, so, to test that out, I added: <\/p>\n\n<pre><code>default_source_directory=\"testing\",\n<\/code><\/pre>\n\n<p>as a parameter for the training_pipeline object, but saw the same behavior when I then tried to run it.  Not sure if that is the same source directory the documentation is referring to.  The docs also say that if no source directory is specified, the \"current local directory\" is uploaded.  I used print (os.getcwd()) to get the working directory and gave \"Everyone\" full control permissions on the directory (working in a windows env).<\/p>\n\n<p>All the preceding code works fine, and I can submit an experiment if I use a ScriptRunConfig and run it on attached compute rather than using a pipeline\/training cluster.  <\/p>\n\n<p>Any ideas?  Thanks in advance to anyone who tries to help.  P.S. There is no \"azure-machine-learning-pipelines\" tag, and I can't add one because I don't have enough reputation points.  Someone else could though!  <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-ml-pipelines\" rel=\"nofollow noreferrer\">General<\/a> info on what they are.<\/p>",
        "Challenge_closed_time":1577994637207,
        "Challenge_comment_count":0,
        "Challenge_created_time":1577601580777,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59517355",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.4,
        "Challenge_reading_time":41.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":109.1823416667,
        "Challenge_title":"Permission denied: '.\\NTUSER.DAT' when trying to run an Azure ML Pipeline",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":409.0,
        "Challenge_word_count":336,
        "Platform":"Stack Overflow",
        "Poster_created_time":1541802293200,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":163.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>I resolved this answer by setting the path and the data_script variables in the AutoMLConfig task object, like this (relevant code indicated by -->):<\/p>\n\n<pre><code>automl_config = AutoMLConfig(task = 'forecasting',\n                             debug_log = 'automl_errors.log',\n                             compute_target=compute_target,\n                             run_configuration=conda_run_config,\n                             --&gt;path = \"c:\\\\users\\\\me\",\n                             data_script =\"script.py\",&lt;--\n                             **automl_settings\n                            )\n<\/code><\/pre>\n\n<p>Setting the data_script variable to include the full path, as shown below, did not work.<\/p>\n\n<pre><code>automl_config = AutoMLConfig(task = 'forecasting',\n                             debug_log = 'automl_errors.log',\n                             path = \".\",\n                             --&gt;data_script = \"c:\\\\users\\\\me\\\\script.py\"&lt;--\n                             compute_target=compute_target,\n                             run_configuration=conda_run_config, \n                             **automl_settings\n                            )\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.7,
        "Solution_reading_time":10.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":1.9011638889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>According to <a href=\"https:\/\/dvc.org\/doc\/user-guide\/update-tracked-file\" rel=\"nofollow noreferrer\">this tutorial<\/a> when I update file I should remove file from under DVC control first (i.e. execute <code>dvc unprotect &lt;myfile&gt;.dvc<\/code> or <code>dvc remove &lt;myfile&gt;.dvc<\/code>) and then add it again via <code>dvc add &lt;mifile&gt;<\/code>. However It's not clear if I should apply the same workflow for the directories.<\/p>\n\n<p>I have the directory under DVC control with the following structure:<\/p>\n\n<pre><code>data\/\n    1.jpg\n    2.jpg\n<\/code><\/pre>\n\n<p>Should I run <code>dvc unprotect data<\/code> every time the directory content is updated?<\/p>\n\n<p>More specifically I'm interested if I should run <code>dvc unprotect data<\/code> in the following use cases:<\/p>\n\n<ul>\n<li><strong>New file is added.<\/strong> For example if I put <code>3.jpg<\/code> image in the data dir<\/li>\n<li><strong>File is deleted.<\/strong> For example if I delete <code>2.jpg<\/code> image in the <code>data<\/code> dir<\/li>\n<li><strong>File is updated.<\/strong> For example if I edit <code>1.jpg<\/code> image via graphic editor.<\/li>\n<li>A combination of the previous use cases (i.e. some files are updated, other deleted and new files are added)<\/li>\n<\/ul>",
        "Challenge_closed_time":1558674266680,
        "Challenge_comment_count":0,
        "Challenge_created_time":1558667422490,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1558708772616,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56285351",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.4,
        "Challenge_reading_time":16.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":1.9011638889,
        "Challenge_title":"Updating tracked dir in DVC",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":995.0,
        "Challenge_word_count":163,
        "Platform":"Stack Overflow",
        "Poster_created_time":1522254698710,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Russia",
        "Poster_reputation_count":784.0,
        "Poster_view_count":77.0,
        "Solution_body":"<p>Only when file is updated - i.e. edit <code>1.jpg<\/code> with your editor <strong>AND<\/strong> only if hadrlink or symlink cache type is enabled.<\/p>\n\n<p>Please, check this <a href=\"https:\/\/dvc.org\/doc\/user-guide\/update-tracked-file\" rel=\"nofollow noreferrer\">link<\/a>:<\/p>\n\n<blockquote>\n  <p>updating tracked files has to be carried out with caution to avoid data corruption when the DVC config option cache.type is set to hardlink or\/and symlink<\/p>\n<\/blockquote>\n\n<p>I would strongly recommend reading this document: <a href=\"https:\/\/dvc.org\/doc\/user-guide\/cache-file-linking\" rel=\"nofollow noreferrer\">Performance Optimization for Large Files<\/a> it explains benefits of using hardlinks\/symlinks.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.8,
        "Solution_reading_time":9.18,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":77.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1631019482980,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":234.0,
        "Answerer_view_count":155.0,
        "Challenge_adjusted_solved_time":25.0813055556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Goal: <code>add<\/code> <code>commit<\/code> <code>push<\/code> all contents of <code>project_model\/data\/<\/code> to <strong>dvcstore<\/strong>.<\/p>\n<p>I don't have any <code>.dvc<\/code> files in my project.<\/p>\n<pre><code>$ dvc add .\/project_model\/data\/\nERROR: Cannot add '\/home\/me\/PycharmProjects\/project\/project_model\/data\/images', because it is overlapping with other DVC tracked output: '\/home\/me\/PycharmProjects\/project\/project_model\/data'.\nTo include '\/home\/me\/PycharmProjects\/project\/project_model\/data\/images' in '\/home\/me\/PycharmProjects\/project\/project_model\/data', run 'dvc commit project_model\/data.dvc'\n\n$ dvc commit project_model\/data.dvc\nERROR: failed to commit project_model\/data.dvc - 'project_model\/data.dvc' does not exist\n<\/code><\/pre>\n<p>I've deleted contents from <code>.dvc\/cache\/<\/code> and <strong>S3<\/strong> <code>s3:\/\/foo\/bar\/dvcstore\/<\/code>, with no luck.<\/p>\n<hr \/>\n<pre><code>$ dvc -V\n2.10.2\n<\/code><\/pre>\n<pre><code>$ dvc doctor\nDVC version: 2.10.2 (pip)\n---------------------------------\nPlatform: Python 3.9.12 on Linux-5.15.0-47-generic-x86_64-with-glibc2.35\nSupports:\n        webhdfs (fsspec = 2022.5.0),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.5.2),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.5.2),\n        s3 (s3fs = 2022.5.0, boto3 = 1.21.21)\nCache types: hardlink, symlink\nCache directory: ext4 on \/dev\/nvme0n1p5\nCaches: local\nRemotes: s3\nWorkspace directory: ext4 on \/dev\/nvme0n1p5\nRepo: dvc, git\n<\/code><\/pre>\n<p>Please let me know if there's anything else I can add to post.<\/p>",
        "Challenge_closed_time":1663149911323,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663059618623,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73700203",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.8,
        "Challenge_reading_time":21.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":25.0813055556,
        "Challenge_title":"ERROR: Cannot add 'folder-path', because it is overlapping with other DVC tracked output:",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":31.0,
        "Challenge_word_count":154,
        "Platform":"Stack Overflow",
        "Poster_created_time":1631019482980,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":234.0,
        "Poster_view_count":155.0,
        "Solution_body":"<p>In my case, the problem was in <code>dvc.yaml<\/code>.<\/p>\n<p>For a few <code>stages<\/code>, I had cyclical dependencies, where a file-path was mentioned in both the <code>deps<\/code> and <code>outs<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.4,
        "Solution_reading_time":2.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":27.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":503.9755555556,
        "Challenge_answer_count":0,
        "Challenge_body":"```\r\n$ dvc repro run_benchmarks\r\nERROR: 'dvc.lock' is git-ignored.\r\n```\r\n\r\n`.dvc.lock` in `.gitignore` causes Exceptions at running benchmark. Delete this line solves this problem. And because of #168 maybe we need some better ways to deal with `dvc.lock`.",
        "Challenge_closed_time":1621495987000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619681675000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/iterative\/dvc-bench\/issues\/255",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":5.8,
        "Challenge_reading_time":3.58,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":9.0,
        "Challenge_repo_issue_count":400.0,
        "Challenge_repo_star_count":19.0,
        "Challenge_repo_watch_count":17.0,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":503.9755555556,
        "Challenge_title":"ERROR: 'dvc.lock' is git-ignored.",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":null,
        "Challenge_word_count":39,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1305851487736,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5993.0,
        "Answerer_view_count":457.0,
        "Challenge_adjusted_solved_time":32.1755194445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Suppose I run the following commands:<\/p>\n<pre><code># set up DVC\n\nmkdir foo\ncd foo &amp;&amp; git init\ndvc init\ngit add * &amp;&amp; git commit -m &quot;dvc init&quot;\n\n\n# make a data file\n\nmkdir -p bar\/biz\ntouch bar\/biz\/boz\n\n\n# add the data file\n\ndvc add bar\/biz\/boz\n<\/code><\/pre>\n<p>And DVC outputs the following:<\/p>\n<pre><code>To track the changes with git, run:\n\n  git add bar\/biz\/.gitignore bar\/biz\/boz.dvc\n<\/code><\/pre>\n<hr \/>\n<p>This last part is what I would like to avoid.  Preferably, DVC would only change the top level <code>.gitignore<\/code> (located at the project root, where <code>git init<\/code> was executed), and will change only DVC files at the top level.<\/p>\n<p><strong>And here's why:<\/strong><\/p>\n<p>I have a rather large dataset developed in an original work more or less ad-hoc. This data is not systematically organized, nor do I want to organize it as-is.<\/p>\n<p>Instead, I want to incrementally add this old, bespoke data to the DVC directory tree.  And each time I add some of the data to the tree, I want to check it in with DVC as I would if I were modifying code or mixing one project's code into another.<\/p>\n<p>However, DVC wants to create a local file and gitignore at every location I add.  This creates a mess and I have no reasonable faith that it will be easy to maintain all of these atomic and distributed datastores.<\/p>\n<hr \/>\n<p><strong>The question:<\/strong><\/p>\n<p>What is the preferred way to incrementally add data in DVC so that DVC uses the root gitignore and root DVC files\/items?<\/p>",
        "Challenge_closed_time":1655349731160,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655234204993,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72622280",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":19.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":32.0906019444,
        "Challenge_title":"How does one add individual files with DVC?",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":112.0,
        "Challenge_word_count":254,
        "Platform":"Stack Overflow",
        "Poster_created_time":1405262190020,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Atlanta, GA",
        "Poster_reputation_count":26244.0,
        "Poster_view_count":1383.0,
        "Solution_body":"<p>Assuming bar\/ is the dataset directory you're incrementally adding to, you can instead<\/p>\n<pre><code>dvc add bar\n<\/code><\/pre>\n<p>This creates a bar.dvc file and writes to .gitignore at the top level.<\/p>\n<p>When you update content in bar\/, <code>dvc add<\/code> it again or use <code>dvc commit<\/code> to register the new dataset version. The new files get added to the project cache and the .dvc file gets an updated <code>md5<\/code> hash that identifies to the latest directory structure.<\/p>\n<p>Some docs:<br \/>\n<a href=\"https:\/\/dvc.org\/doc\/start\/data-management#making-changes\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/start\/data-management#making-changes<\/a><br \/>\n<a href=\"https:\/\/dvc.org\/doc\/command-reference\/add\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/add<\/a><br \/>\n<a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1655350036863,
        "Solution_link_count":6.0,
        "Solution_readability":16.8,
        "Solution_reading_time":14.03,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":89.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3357.2136111111,
        "Challenge_answer_count":2,
        "Challenge_body":"After https:\/\/github.com\/iterative\/dvc\/pull\/5265\r\nWe do not allow ignoring lockfile. `dvc-bench` is running currently on some older version of `dvc`, though it would be good to adjust it so that it works with `>2.0.0`.",
        "Challenge_closed_time":1628758546000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616672577000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/iterative\/dvc-bench\/issues\/244",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":7.7,
        "Challenge_reading_time":3.07,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":9.0,
        "Challenge_repo_issue_count":400.0,
        "Challenge_repo_star_count":19.0,
        "Challenge_repo_watch_count":17.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":3357.2136111111,
        "Challenge_title":"requirements: update dvc",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":null,
        "Challenge_word_count":34,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@pared Sorry, not sure I understand what do we need to update here. Could you elaborate, please? Fixed by #267, forgot to close.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.8,
        "Solution_reading_time":1.56,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":23.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2183.8986427778,
        "Challenge_answer_count":2,
        "Challenge_body":"We have a gitlab repo within a private VPN and would like to setup Studio to clone that repo and to push and pull updates. Is that possible yet from within Studio?",
        "Challenge_closed_time":1650994900956,
        "Challenge_comment_count":1,
        "Challenge_created_time":1643132865842,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1668578369590,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QURGs7VOVlTzKCG7H2AFLWww\/how-can-we-connect-a-sagemaker-studio-user-to-a-gitlab-repo-within-a-private-vpn",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.2,
        "Challenge_reading_time":2.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":2183.8986427778,
        "Challenge_title":"How can we connect a Sagemaker Studio user to a gitlab repo within a private VPN?",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":367.0,
        "Challenge_word_count":47,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thank you for your response. For those looking to do the same thing, according to AWS Support AWS SageMakers does NOT support GitLab yet and there is no ETA for that feature.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1650994900956,
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":2.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":32.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1308292969430,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":651.0,
        "Answerer_view_count":99.0,
        "Challenge_adjusted_solved_time":2443.6834486111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>We're currently working with 3 employees in the same notebook-instance, however, since this is a shared workspace this makes version management extra difficult. Is it possible to link aws credentials to your git account from within SageMaker? Or are there any other ways recommended for version management? <\/p>\n\n<p>Right now we're using a single git account for committing the code from within jupyter terminal. <\/p>",
        "Challenge_closed_time":1543573965692,
        "Challenge_comment_count":0,
        "Challenge_created_time":1534776705277,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51933366",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.8,
        "Challenge_reading_time":5.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2443.6834486111,
        "Challenge_title":"Version management in SageMaker",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":836.0,
        "Challenge_word_count":68,
        "Platform":"Stack Overflow",
        "Poster_created_time":1534775846323,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>The situation has changed : Git is now <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/amazon-sagemaker-notebooks-now-support-git-integration-for-increased-persistence-collaboration-and-reproducibility\/\" rel=\"nofollow noreferrer\">available<\/a> in SageMaker<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":40.0,
        "Solution_reading_time":3.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1518706063680,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":95.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":866.1332491667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>To overcome <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-save-write-experiment-files#storage-limits-of-experiment-snapshots\" rel=\"nofollow noreferrer\">300MB snapshot size limit<\/a> I created an .amlignore file in the root of my repository:<\/p>\n\n<pre><code>\/*\n!\/root\n<\/code><\/pre>\n\n<p>The intention is to exclude everything except <code>\/root<\/code> directory where all python code is. The size of the <code>root<\/code> directory is less than 1MB, still I get an error of exceeding snapshot limit size of 300MB. What am I doing wrong?<\/p>",
        "Challenge_closed_time":1573932699567,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570814619870,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58345935",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":8.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":866.1332491667,
        "Challenge_title":"The amlignore file doesn't reduce the size of snapshot",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":522.0,
        "Challenge_word_count":72,
        "Platform":"Stack Overflow",
        "Poster_created_time":1518706063680,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>This is fixed in version <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/azure-machine-learning-release-notes#azure-machine-learning-sdk-for-python-v1074\" rel=\"nofollow noreferrer\">1.0.74 of azureml-sdk<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":28.3,
        "Solution_reading_time":3.35,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":11.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1565528932887,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"United Kingdom",
        "Answerer_reputation_count":1579.0,
        "Answerer_view_count":91.0,
        "Challenge_adjusted_solved_time":28.5942333333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using VS Code to submit a Machine Learning experiment in Azure Portal. When running the experiment I'm obtaining the following error:<\/p>\n\n<p>Run failed: User program failed with ModuleNotFoundError: No module named 'amlrun'<\/p>\n\n<p>This is the code structure:<\/p>\n\n<p>.vscode (json configuration file)<\/p>\n\n<p>aml_config<\/p>\n\n<p>scripts<\/p>\n\n<p>----- amlrun.py (a script with some functions)<\/p>\n\n<p>----- model_training.py (a script creating and saving the model)<\/p>\n\n<p>This is the configuration file:<\/p>\n\n<pre><code>{\n    \"script\": \"model_training.py\",\n    \"framework\": \"Python\",\n    \"communicator\": \"None\",\n    \"target\": \"testazure\",\n    \"environment\": {\n        \"python\": {\n            \"userManagedDependencies\": false,\n            \"condaDependencies\": {\n                \"dependencies\": [\n                    \"python=3.6.2\",\n                    \"scikit-learn\",\n                    \"numpy\",\n                    \"pandas\",\n                    {\n                        \"pip\": [\n                            \"azureml-defaults\"\n                        ]\n                    }\n                ]\n            }\n        },\n        \"docker\": {\n            \"baseImage\": \"mcr.microsoft.com\/azureml\/base:0.2.4\",\n            \"enabled\": true,\n            \"baseImageRegistry\": {\n                \"address\": null,\n                \"username\": null,\n                \"password\": null\n            }\n        }\n    },\n    \"history\": {\n        \"outputCollection\": true,\n        \"snapshotProject\": false,\n        \"directoriesToWatch\": [\n            \"logs\"\n        ]\n    }\n}\n<\/code><\/pre>\n\n<p>Am I missing something?\nThanks<\/p>",
        "Challenge_closed_time":1571837118727,
        "Challenge_comment_count":0,
        "Challenge_created_time":1571735433200,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58500807",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.4,
        "Challenge_reading_time":16.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":28.2459797222,
        "Challenge_title":"Run failed: User program failed with ModuleNotFoundError: No module named 'amlrun' in Azure ML Experiment",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":901.0,
        "Challenge_word_count":125,
        "Platform":"Stack Overflow",
        "Poster_created_time":1547138031703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>When your training script is running in azure, it's not able to find all your local imports i.e. <code>amlrun.py<\/code> script. <\/p>\n\n<p>The submitted training job to azure builds a docker image with your files first and runs the experiment; but in this case the extension hasn't included <code>amlrun.py<\/code>. <\/p>\n\n<p>This is probably because when you have submit the training job with the extension, the visual studio code window opened is not pointing to be in <code>scripts<\/code> folder.<\/p>\n\n<p>Taken from one of the replies to a <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/issues\/24032\" rel=\"nofollow noreferrer\">previously raised github issue<\/a>:<\/p>\n\n<blockquote>\n  <p>The extension currently requires the script you are working on to be\n  in the folder that is open in VS Code and not in a sub-directory.<\/p>\n<\/blockquote>\n\n<hr>\n\n<p>To fix this you can do <strong>either<\/strong> of the following:<\/p>\n\n<ol>\n<li><p>You would need to re-open Visual Studio Code in <code>scripts<\/code> folder instead of parent directory.<\/p><\/li>\n<li><p>Move all files in <code>script<\/code> directory to be in it's parent directory.<\/p><\/li>\n<\/ol>\n\n<hr>\n\n<p>If you're looking for more flexible way to submit training jobs and managing aml - you can use the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/intro?view=azure-ml-py\" rel=\"nofollow noreferrer\">azure machine learning sdk<\/a> for python.<\/p>\n\n<p>Some examples of using the SDK to manage expirements can be found in the links below:<\/p>\n\n<ol>\n<li><p><a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/service\/tutorial-train-models-with-aml.md\" rel=\"nofollow noreferrer\">Scikit Learn Model Training Docs<\/a> <\/p><\/li>\n<li><p><a href=\"https:\/\/github.com\/rithinch\/heartfulness-similar-content-service\" rel=\"nofollow noreferrer\">Basic Pytorch Model Training and Deployment Example Repo<\/a><\/p><\/li>\n<\/ol>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1571838372440,
        "Solution_link_count":4.0,
        "Solution_readability":13.0,
        "Solution_reading_time":24.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":226.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2769444444,
        "Challenge_answer_count":1,
        "Challenge_body":"SageMaker can train on FSx data. One [SageMaker SDK parameter for FSx training][1] is `directory_path`. Where do we find that?\n\n\n  [1]: https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html",
        "Challenge_closed_time":1604679222000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604678225000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668069762163,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUCaemzfoDRIy9AgLRW8suqw\/sagemaker-training-with-fsx-what-is-directory-path",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":3.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.2769444444,
        "Challenge_title":"SageMaker training with FSx: what is \"directory_path\"",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":168.0,
        "Challenge_word_count":28,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"FSx for Lustre is a file system that you can use to provide high performance for ML training workloads. The directory_path should point to the location on your file system where your dataset is stored.\n\nIn the example in the docs:\ndirectory_path='\/fsx\/tensorflow',\n\n\/fsx is the directory you define on your compute instances where you are mounting the file system\n\/tensorflow would represent a folder within the fsx directory\n\nIf you are using an S3-linked FSx for Lustre file system \/tensorflow would be a prefix within your S3-linked bucket.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1614191724716,
        "Solution_link_count":0.0,
        "Solution_readability":14.7,
        "Solution_reading_time":6.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":68.5333333333,
        "Challenge_answer_count":2,
        "Challenge_body":"Firstly I'd like to apologize if this is a dummy question.\r\nI'm following the tutorial to get introduced to kedro mlflow,; after running the command \"kedro mlflow init\" I tried to run the command \"kedro mlflofw ui\" but I get an error:\r\n\r\nINFO     The 'mlflow_tracking_uri' key in mlflow.yml is relative ('server.mlflow_tracking_uri = mlruns'). It is converted to a valid uri: 'file:\/\/\/C:\/Users\/e107338\/PycharmProjects\/mlflow\/kedro-mlflow-example\/mlruns'                                                   kedro_mlflow_config.py:202\r\n\r\nAfter the Traceback I get an error: FileNotFoundErrror\r\n",
        "Challenge_closed_time":1664786016000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664539296000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/361",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":7.26,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":68.5333333333,
        "Challenge_title":"kedro mlflow ui gets a FileNotFoundError",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":null,
        "Challenge_word_count":74,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi, \r\n\r\nI am sorry to see you are experiencing issues. this is not a dummy question, it sounds like a bug. \r\n\r\nI've just ran this: \r\n\r\n```bash\r\nconda create -n km-361 python=3.9 -y\r\nconda activate km-361\r\npip install kedro==0.18.3\r\npip install mlflow==1.29.0\r\npip install kedro-mlflow==0.11.3\r\nkedro new --starter=pandas-iris\r\ncd iris\r\nkedro mlflow init\r\nkedro mlflow ui\r\n```\r\n\r\nthen I opened ``http:\/\/127.0.0.1:5000`` and th UI opened as expected. \r\n\r\nCan you tell me: \r\n- your python version\r\n- your OS\r\n- your ``kedro`` \/ ``mlflow`` \/ ``kedro-mlflow`` version\r\n- the project using\r\n- the exact error message\r\n- check if you have a ``MLFLOW_TRACKING_URI`` environment set It turned out fine  after trying again! Sorry and thanks for your consideration!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":5.1,
        "Solution_reading_time":8.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":107.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1305851487736,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5993.0,
        "Answerer_view_count":457.0,
        "Challenge_adjusted_solved_time":8.7381941667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to use the pipeline functionality of dvc in a git repository. The data is managed otherwise and should not be versioned by dvc. The only functionality which is needed is that dvc reproduces the needed steps of the pipeline when <code>dvc repro<\/code> is called. Checking out the repository on a new system should lead to an 'empty' repository, where none of the pipeline steps are stored.<\/p>\n<p>Thus, - if I understand correctly - there is no need to track the dvc.lock file in the repository. However, adding dvc.lock to the .gitginore file leads to an error message:<\/p>\n<pre><code>ERROR: 'dvc.lock' is git-ignored.\n<\/code><\/pre>\n<p>Is there any way to disable the dvc.lock in .gitignore check for this usecase?<\/p>",
        "Challenge_closed_time":1624393487732,
        "Challenge_comment_count":0,
        "Challenge_created_time":1624362030233,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68082912",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":9.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":8.7381941667,
        "Challenge_title":"git-ignore dvc.lock in repositories where only the DVC pipelines are used",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":493.0,
        "Challenge_word_count":126,
        "Platform":"Stack Overflow",
        "Poster_created_time":1542537900087,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":147.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>This is definitely possible, as DVC features are loosely coupled to one another. You can do pipelining by writing your dvc.yaml file(s), but avoid data management\/versioning by using <code>cache: false<\/code> in the stage outputs (<a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#output-subfields\" rel=\"nofollow noreferrer\"><code>outs<\/code> field<\/a>). See also helper <code>dvc stage add -O<\/code> (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/stage\/add#options\" rel=\"nofollow noreferrer\">big O<\/a>, alias of <code>--outs-no-cache<\/code>).<\/p>\n<p>And the same for initial data dependencies, you can <code>dvc add --no-commit<\/code> them (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#options\" rel=\"nofollow noreferrer\">ref<\/a>).<\/p>\n<p>You do want to track <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#dvclock-file\" rel=\"nofollow noreferrer\">dvc.lock<\/a> in Git though, so that DVC can determine the latest stage of the pipeline associated with the Git commit in every repo copy or branch.<\/p>\n<p>You'll be responsible for placing the right data files\/dirs (matching .dvc files and dvc.lock) in the workspace for <code>dvc repro<\/code> or <code>dvc exp run<\/code> to behave as expected. <code>dvc checkout<\/code> won't be able to help you.<\/p>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":12.9,
        "Solution_reading_time":17.17,
        "Solution_score_count":3.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":141.0,
        "Tool":"DVC"
    }
]